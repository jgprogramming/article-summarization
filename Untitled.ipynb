{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4549\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "data_dir = '/home/jagr/Documents/studia/article-summarization/data/cat_data'\n",
    "\n",
    "categories = os.listdir(data_dir)\n",
    "data = {}\n",
    "\n",
    "for category in categories:\n",
    "    data[category] = []\n",
    "\n",
    "\n",
    "    \n",
    "for category in categories:\n",
    "    for file in os.listdir(os.path.join(data_dir, category)):\n",
    "        doc_path = os.path.join(data_dir, category, file)\n",
    "        d = open(doc_path)\n",
    "        j = json.loads(d.read())\n",
    "        for document in j:\n",
    "            doc = {\"title\":document[\"title\"], \"text\":document[\"text\"]}\n",
    "            data[category].append(doc)\n",
    "\n",
    "corpus_size = 0\n",
    "for category in categories:\n",
    "    corpus_size += len(data[category])\n",
    "            \n",
    "print(corpus_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "stemmer = LancasterStemmer()\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    stems = [word.lower() for word in stems if word.isalpha()]\n",
    "    return stems\n",
    "\n",
    "bayes = {}\n",
    "for category in categories:\n",
    "    bayes[category] = {}\n",
    "    bayes[category][\"prob\"] = len(data[category])/corpus_size\n",
    "    bow = CountVectorizer(input='content', tokenizer=tokenize, stop_words='english')\n",
    "    korpus = \"\"\n",
    "    for entry in data[category]:\n",
    "        korpus += entry[\"text\"]\n",
    "        korpus += \" \"\n",
    "    bow = bow.fit([korpus])\n",
    "    \n",
    "    bayes[category][\"count\"] = bow.transform([korpus]).toarray()[0]\n",
    "    bayes[category][\"features\"] = bow.get_feature_names()\n",
    "    bayes[category][\"sum\"] = sum(bayes[category][\"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "       \n",
    "import sys\n",
    "def calculate_prob(text, category):\n",
    "    prob = math.log(bayes[category][\"prob\"])\n",
    "    \n",
    "    features = bayes[category][\"features\"]\n",
    "    count = bayes[category][\"count\"]\n",
    "    suma = bayes[category][\"sum\"]\n",
    "    \n",
    "    for word in tokenize(text):\n",
    "        try:\n",
    "            indx = features.index(word)\n",
    "            c = count[indx]\n",
    "        except:\n",
    "            c = 1\n",
    "        prob += math.log(c/suma)\n",
    "    return prob\n",
    "\n",
    "def predict_category(text):\n",
    "    value = -sys.maxsize -1\n",
    "    cat = None\n",
    "    \n",
    "    for category in categories:\n",
    "        prob = calculate_prob(text, category)\n",
    "        if  prob > value:\n",
    "            value = prob\n",
    "            cat = category\n",
    "    return cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = \"Winner-take-all (computing)\\n\\nWinner-take-all is a computational principle applied in computational models of neural networks by which neurons in a layer compete with each other for activation. In the classical form, only the neuron with the highest activation stays active while all other neurons shut down; however, other variations allow more than one neuron to be active, for example the soft winner take-all, by which a power function is applied to the neurons.\\n\\nIn the theory of artificial neural networks, winner-take-all networks are a case of competitive learning in recurrent neural networks. Output nodes in the network mutually inhibit each other, while simultaneously activating themselves through reflexive connections. After some time, only one node in the output layer will be active, namely the one corresponding to the strongest input. Thus the network uses nonlinear inhibition to pick out the largest of a set of inputs. Winner-take-all is a general computational primitive that can be implemented using different types of neural network models, including both continuous-time and spiking networks (Grossberg, 1973; Oster et al. 2009).\\n\\nWinner-take-all networks are commonly used in computational models of the brain, particularly for distributed decision-making or action selection in the cortex. Important examples include hierarchical models of vision (Riesenhuber et al. 1999), and models of selective attention and recognition (Carpenter and Grossberg, 1987; Itti et al. 1998). They are also common in artificial neural networks and neuromorphic analog VLSI circuits. It has been formally proven that the winner-take-all operation is computationally powerful compared to other nonlinear operations, such as thresholding (Maass 2000).\\n\\nIn many practical cases, there is not only a single neuron which becomes the only active one but there are exactly \\\"k\\\" neurons which become active for a fixed number \\\"k\\\". This principle is referred to as k-winners-take-all.\\n\\nA simple, but popular CMOS winner-take-all circuit is shown on the right. This circuit was originally proposed by Lazzaro et al. (1989) using MOS transistors biased to operate in the weak-inversion or subthreshold regime. In the particular case shown there are only two inputs (\\\"I\\\" and \\\"I\\\"), but the circuit can be easily extended to multiple inputs in a straightforward way. It operates on continuous-time input signals (currents) in parallel, using only two transistors per input. In addition, the bias current \\\"I\\\" is set by a single global transistor that is common to all the inputs.\\n\\nThe largest of the input currents sets the common potential \\\"V\\\". As a result, the corresponding output carries almost all the bias current, while the other outputs have currents that are close to zero. Thus, the circuit selects the larger of the two input currents, i.e., if \\\"I\\\" > \\\"I\\\", we get \\\"I\\\" = \\\"I\\\" and \\\"I\\\" = 0. Similarly, if \\\"I\\\" > \\\"I\\\", we get \\\"I\\\" = 0 and \\\"I\\\" = \\\"I\\\".\\n\\nA SPICE-based DC simulation of the CMOS winner-take-all circuit in the two-input case is shown on the right. As shown in the top subplot, the input \\\"I\\\" was fixed at 6nA, while \\\"I\\\" was linearly increased from 0 to 10nA. The bottom subplot shows the two output currents. As expected, the output corresponding to the larger of the two inputs carries the entire bias current (10nA in this case), forcing the other output current nearly to zero.\\n\\nIn stereo matching algorithms, following the taxonomy proposed by Scharstein et al. (IJCV 2002), winner-take-all is a local method for disparity computation. Adopting a winner-take-all strategy, the disparity associated with the minimum or maximum cost value is selected at each pixel.\\n\\nIt is axiomatic that in the electronic commerce market, early dominant players such as AOL or Yahoo! get most of the rewards. By 1998, one study found the top 5% of all web sites garnered more than 74% of all traffic.\\n\\nThe winner take all hypothesis suggests that once a technology or a firm gets ahead, it will do better and better over time, whereas lagging technology and firms will fall further behind.\\n\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ai'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_category(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Winner-take-all is a computational principle applied in computational models of neural networks by which neurons in a layer compete with each other for activation.',\n",
       " 'In the classical form, only the neuron with the highest activation stays active while all other neurons shut down; however, other variations allow more than one neuron to be active, for example the soft winner take-all, by which a power function is applied to the neurons.',\n",
       " 'In the theory of artificial neural networks, winner-take-all networks are a case of competitive learning in recurrent neural networks.',\n",
       " 'Output nodes in the network mutually inhibit each other, while simultaneously activating themselves through reflexive connections.',\n",
       " 'After some time, only one node in the output layer will be active, namely the one corresponding to the strongest input.',\n",
       " 'Thus the network uses nonlinear inhibition to pick out the largest of a set of inputs.',\n",
       " 'Winner-take-all is a general computational primitive that can be implemented using different types of neural network models, including both continuous-time and spiking networks (Grossberg, 1973; Oster et al.',\n",
       " '2009).',\n",
       " 'Winner-take-all networks are commonly used in computational models of the brain, particularly for distributed decision-making or action selection in the cortex.',\n",
       " 'Important examples include hierarchical models of vision (Riesenhuber et al.',\n",
       " '1999), and models of selective attention and recognition (Carpenter and Grossberg, 1987; Itti et al.',\n",
       " '1998).',\n",
       " 'They are also common in artificial neural networks and neuromorphic analog VLSI circuits.',\n",
       " 'It has been formally proven that the winner-take-all operation is computationally powerful compared to other nonlinear operations, such as thresholding (Maass 2000).',\n",
       " 'In many practical cases, there is not only a single neuron which becomes the only active one but there are exactly \"k\" neurons which become active for a fixed number \"k\".',\n",
       " 'This principle is referred to as k-winners-take-all.',\n",
       " 'A simple, but popular CMOS winner-take-all circuit is shown on the right.',\n",
       " 'This circuit was originally proposed by Lazzaro et al.',\n",
       " '(1989) using MOS transistors biased to operate in the weak-inversion or subthreshold regime.',\n",
       " 'In the particular case shown there are only two inputs (\"I\" and \"I\"), but the circuit can be easily extended to multiple inputs in a straightforward way.',\n",
       " 'It operates on continuous-time input signals (currents) in parallel, using only two transistors per input.',\n",
       " 'In addition, the bias current \"I\" is set by a single global transistor that is common to all the inputs.',\n",
       " 'The largest of the input currents sets the common potential \"V\".',\n",
       " 'As a result, the corresponding output carries almost all the bias current, while the other outputs have currents that are close to zero.',\n",
       " 'Thus, the circuit selects the larger of the two input currents, i.e., if \"I\" > \"I\", we get \"I\" = \"I\" and \"I\" = 0.',\n",
       " 'Similarly, if \"I\" > \"I\", we get \"I\" = 0 and \"I\" = \"I\".',\n",
       " 'A SPICE-based DC simulation of the CMOS winner-take-all circuit in the two-input case is shown on the right.',\n",
       " 'As shown in the top subplot, the input \"I\" was fixed at 6nA, while \"I\" was linearly increased from 0 to 10nA.',\n",
       " 'The bottom subplot shows the two output currents.',\n",
       " 'As expected, the output corresponding to the larger of the two inputs carries the entire bias current (10nA in this case), forcing the other output current nearly to zero.',\n",
       " 'In stereo matching algorithms, following the taxonomy proposed by Scharstein et al.',\n",
       " '(IJCV 2002), winner-take-all is a local method for disparity computation.',\n",
       " 'Adopting a winner-take-all strategy, the disparity associated with the minimum or maximum cost value is selected at each pixel.',\n",
       " 'It is axiomatic that in the electronic commerce market, early dominant players such as AOL or Yahoo!',\n",
       " 'get most of the rewards.',\n",
       " 'By 1998, one study found the top 5% of all web sites garnered more than 74% of all traffic.',\n",
       " 'The winner take all hypothesis suggests that once a technology or a firm gets ahead, it will do better and better over time, whereas lagging technology and firms will fall further behind.']"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def tokenize_sent(text, title):\n",
    "    tCopy = text\n",
    "    if tCopy.startswith(title):\n",
    "         tCopy = tCopy[len(title):]\n",
    "    return nltk.sent_tokenize(tCopy.lstrip()) \n",
    "\n",
    "tokenize_sent(text, \"Winner-take-all (computing)\")\n",
    "\n",
    "# nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
