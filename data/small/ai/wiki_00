{"url": "https://en.wikipedia.org/wiki?curid=996026", "text": "Commonsense reasoning\n\nCommonsense reasoning is one of the branches of artificial intelligence (AI) that is concerned with simulating the human ability to make presumptions about the type and essence of ordinary situations they encounter every day. These assumptions include judgments about the physical properties, purpose, intentions and behavior of people and objects, as well as possible outcomes of their actions and interactions. A device that exhibits commonsense reasoning will be capable of predicting results and drawing conclusions that are similar to humans' folk psychology (humans' innate ability to reason about people's behavior and intentions) and naive physics (humans' natural understanding of the physical world).\n\nIn Artificial intelligence, commonsense knowledge is the set of background information that an individual is intended to know or assume and the ability to use it when appropriate. It is a shared knowledge (between everybody or people in a particular culture or age group only). The way to obtain commonsense is by learning it or experiencing it. In communication, it is what people don’t have to say because the interlocutor is expected to know or make a presumption about.\n\nThe commonsense knowledge problem is a current project in the sphere of artificial intelligence to create a database that contains the general knowledge most individuals are expected to have, represented in an accessible way to artificial intelligence programs that use natural language. Due to the broad scope of the commonsense knowledge this issue is considered to be among the most difficult ones in the AI research sphere. In order for any task to be done as a human mind would manage it, the machine is required to appear as intelligent as a human being. Such tasks include object recognition, machine translation and text mining. To perform them, the machine has to be aware of the same concepts that an individual, who possess commonsense knowledge, recognizes.\n\nIn 1961, Bar Hillel first discussed the need and significance of practical knowledge for natural language processing in the context of machine translation. Some ambiguities are resolved by using simple and easy to acquire rules. Others require a broad acknowledgement of the surrounding world, thus they require more commonsense knowledge. For instance when a machine is used to translate a text, problems of ambiguity arise, which could be easily resolved by attaining a concrete and true understanding of the context. Online translators often resolve ambiguities using analogous or similar words. For example, in translating the sentences \"The electrician is working\" and \"The telephone is working\" into German, the machine translates correctly \"working\" in the means of \"laboring\" in the first one and as \"functioning properly\" in the second one. The machine has seen and read in the body of texts that the German words for \"laboring\" and \"electrician\" are frequently used in a combination and are found close together. The same applies for \"telephone\" and \"function properly\". However, the statistical proxy which works in simple cases often fails in complex ones. Existing computer programs carry out simple language tasks by manipulating short phrases or separate words, but they don’t attempt any deeper understanding and focus on short-term results.\n\nIssues of this kind arise in computer vision. For instance when looking at the photograph of the bathroom (Figure 1) some of the items that are small and only partly seen, such as the towels or the body lotions, are recognizable due to the surrounding objects (toilet, wash basin, bathtub), which suggest the purpose of the room. In an isolated image they would be difficult to identify. Movies prove to be even more difficult tasks. Some movies contain scenes and moments that cannot be understood by simply matching memorized templates to images. For instance, to understand the context of the movie, the viewer is required to make inferences about characters’ intentions and make presumptions depending on their behavior. In the contemporary state of the art, it is impossible to build and manage a program that will perform such tasks as reasoning, i.e. predicting characters’ actions. The most that can be done is to identify basic actions and track characters.\n\nThe need and importance of commonsense reasoning in autonomous robots that work in a real-life uncontrolled environment is evident. For instance, if a robot is programmed to perform the tasks of a waiter on a cocktail party, and it sees that the glass he had picked up is broken, the waiter-robot should not pour liquid into the glass, but instead pick up another one. Such tasks seem obvious when an individual possess simple commonsense reasoning, but to ensure that a robot will avoid such mistakes is challenging.\n\nSignificant progress in the field of the automated commonsense reasoning is made in the areas of the taxonomic reasoning, actions and change reasoning, reasoning about time. Each of these spheres has a well-acknowledged theory for wide range of commonsense inferences.\n\nTaxonomy is the collection of individuals and categories and their relations. Taxonomies are often referred to as semantic networks. Figure 2 displays a taxonomy of a few categories of individuals and animals.\nThree basic relations are demonstrated:\nTransitivity is one type of inference in taxonomy. Since Tweety is an instance of Robin and Robin is a subset of Bird, it follows that Tweety is an instance of Bird. Inheritance is another type of inference. Since Tweety is an instance of Robin, which is a subset of Bird and Bird is marked with property CanFly, it follows that Tweety and Robin have property CanFly. When an individual taxonomizes more abstract categories, outlining and delimiting specific categories becomes more problematic. Simple taxonomic structures are frequently used in AI programs. For instance, WordNet is a resource including a taxonomy, whose elements are meanings of English words. Web mining systems used to collect commonsense knowledge from Web documents focus on taxonomic relations and specifically in gathering taxonomic relations.\n\nThe theory of action, events and change is another range of the commonsense reasoning. There are established reasoning methods for domains that satisfy the constraints listed below:\n\nTemporal reasoning is the ability to make presumptions about humans' knowledge of times, durations and time intervals. For example, if an individual knows that Mozart was born before Beethoven and died earlier than him, he can use his temporal reasoning knowledge to deduce that Mozart had died younger than Beethoven. The inferences involved reduce themselves to solving systems of linear inequalities. To integrate that kind of reasoning with concrete purposes, such as natural language interpretation, is more challenging, because natural language expressions have context dependent interpretation. Simple tasks such as assigning timestamps to procedures cannot be done with total accuracy.\n\nQualitative reasoning is the form of commonsense reasoning analyzed with certain success. It is concerned with the direction of change in interrelated quantities. For instance, if the price of a stock goes up, the amount of stocks that are going to be sold will go down. If some ecosystem contains wolves and lambs and the number of wolves decreases, the death rate of the lambs will go down as well. This theory was firstly formulated by Johan de Kleer, who analyzed an object moving on a roller coaster. The theory of qualitative reasoning is applied in many spheres such as physics, biology, engineering, ecology, etc. It serves as the basis for many practical programs, analogical mapping, text understanding.\n\nAs of 2014, there are some commercial systems trying to make the use of commonsense reasoning significant. However, they use statistical information as a proxy for commonsense knowledge, where reasoning is absent. Current programs manipulate individual words, but they don't attempt or offer further understanding. Five major obstacles interfere with the producing of a satisfactory \"commonsense reasoner\".\n\nFirst, some of the domains that are involved in commonsense reasoning are only partly understood. Individuals are far from a comprehensive understanding of domains as communication and knowledge, interpersonal interactions or physical processes.\n\nSecond, situations that seem easily predicted or assumed about could have logical complexity, which humans’ commonsense knowledge does not cover. Some aspects of similar situations are studied and are well understood, but there are many relations that are unknown, even in principle and how they could be represented in a form that is usable by computers.\n\nThird, commonsense reasoning involves plausible reasoning. It requires coming to a reasonable conclusion given what is already known. Plausible reasoning has been studied for many years and there are a lot of theories developed that include probabilistic reasoning and non-monotonic logic. It takes different forms that include using unreliable data and rules, whose conclusions are not certain sometimes.\n\nFourth, there are many domains, in which a small number of examples are extremely frequent, whereas there is a vast number of highly infrequent examples.\n\nFifth, when formulating pressumptions it is challenging to discern and determine the level of abstraction.\n\nCompared with humans, all existing computer programs perform extremely poorly on modern \"commonsense reasoning\" benchmark tests such as the Winograd Schema Challenge. The problem of attaining human-level competency at \"commonsense knowledge\" tasks is considered to probably be \"AI complete\" (that is, solving it would require the ability to synthesize a human-level intelligence).\n\nCommonsense’s reasoning study is divided into knowledge-based approaches and approaches that are based on machine learning over and using a large data corpora with limited interactions between these two types of approaches. There are also crowdsourcing approaches, attempting to construct a knowledge basis by linking the collective knowledge and the input of non-expert people. Knowledge-based approaches can be separated into approaches based on mathematical logic.\n\nIn knowledge-based approaches, the experts are analyzing the characteristics of the inferences that are required to do reasoning in a specific area or for a certain task. The knowledge-based approaches consist of mathematically grounded approaches, informal knowledge-based approaches and large-scale approaches. The mathematically grounded approaches are purely theoretical and the result is a printed paper instead of a program. The work is limited to the range of the domains and the reasoning techniques that are being reflected on. In informal knowledge-based approaches, theories of reasoning are based on anecdotal data and intuition that are results from empirical behavioral psychology. Informal approaches are common in computer programming. Two other popular techniques for extracting commonsense knowledge from Web documents involve Web mining and Crowd sourcing.\n\n\n\n", "id": "996026", "title": "Commonsense reasoning"}
{"url": "https://en.wikipedia.org/wiki?curid=610757", "text": "Knowledge level\n\nIn artificial intelligence, knowledge-based agents draw on a pool of logical sentences to infer conclusions about the world. At the knowledge level, we only need to specify what the agent knows and what its goals are; a logical abstraction separate from details of implementation.\n\nThis notion of knowledge level was first introduced by Allen Newell in the 1980s, to have a way to rationalize an agent's behavior. The agent takes actions based on knowledge it possesses, in an attempt to reach specific goals. It chooses actions according to the principle of rationality.\n\nBeneath the knowledge level resides the symbol level. Whereas the knowledge level is \"world\" oriented, namely that it concerns the environment in which the agent operates, the symbol level is \"system\" oriented, in that it includes the mechanisms the agent has available to operate. The knowledge level \"rationalizes\" the agent's behavior, while the symbol level \"mechanizes\" the agent's behavior.\n\nFor example, in a computer program, the knowledge level consists of the information contained in its data structures that it uses to perform certain actions. The symbol level consists of the program's algorithms, the data structures themselves, and so on.\n\n\n", "id": "610757", "title": "Knowledge level"}
{"url": "https://en.wikipedia.org/wiki?curid=238725", "text": "Loebner Prize\n\nThe Loebner Prize is an annual competition in artificial intelligence that awards prizes to the computer programs considered by the judges to be the most human-like. The format of the competition is that of a standard Turing test. In each round, a human judge simultaneously holds textual conversations with a computer program and a human being via computer. Based upon the responses, the judge must decide which is which.\n\nThe contest was launched in 1990 by Hugh Loebner in conjunction with the Cambridge Center for Behavioral Studies, Massachusetts, United States. It has since been associated with Flinders University, Dartmouth College, the Science Museum in London, University of Reading and Ulster University, Magee Campus, Derry, UK City of Culture. In 2004 and 2005, it was held in Loebner's apartment in New York City. Within the field of artificial intelligence, the Loebner Prize is somewhat controversial; the most prominent critic, Marvin Minsky, called it a publicity stunt that does not help the field along.\n\nOriginally, $2,000 was awarded for the most human-seeming program in the competition. The prize was $3,000 in 2005 and $2,250 in 2006. In 2008, $3,000 was awarded.\n\nIn addition, there are two one-time-only prizes that have never been awarded. $25,000 is offered for the first program that judges cannot distinguish from a real human and which can convince judges that the human is the computer program. $100,000 is the reward for the first program that judges cannot distinguish from a real human in a Turing test that includes deciphering and understanding text, visual, and auditory input. Once this is achieved, the annual competition will end.\n\nThe rules have varied over the years and early competitions featured restricted conversation Turing tests but since 1995 the discussion has been unrestricted.\n\nFor the three entries in 2007, Robert Medeksza, Noah Duncan and Rollo Carpenter, some basic \"screening questions\" were used by the sponsor to evaluate the state of the technology. These included simple questions about the time, what round of the contest it is, etc.; general knowledge (\"What is a hammer for?\"); comparisons (\"Which is faster, a train or a plane?\"); and questions demonstrating memory for preceding parts of the same conversation. \"All nouns, adjectives and verbs will come from a dictionary suitable for children or adolescents under the age of 12.\" Entries did not need to respond \"intelligently\" to the questions to be accepted.\n\nFor the first time in 2008 the sponsor allowed introduction of a preliminary phase to the contest opening up the competition to previously disallowed web-based entries judged by a variety of invited interrogators. The available rules do not state how interrogators are selected or instructed. Interrogators (who judge the systems) have limited time: 5 minutes per entity in the 2003 competition, 20+ per pair in 2004–2007 competitions, 5 minutes to conduct \"simultaneous\" conversations with a human and the program in 2008-2009, increased to 25 minutes of simultaneous conversation since 2010.\n\nThe prize has long been scorned by experts in the field, for a variety of reasons.\n\nIt is regarded by many as a publicity stunt. Marvin Minsky scathingly offered a \"prize\" to anyone who could stop the competition. Loebner responded by jokingly observing that Minsky's offering a prize to stop the competition effectively made him a co-sponsor.\n\nThe rules of the competition have encouraged poorly qualified judges to make rapid judgements. Interactions between judges and competitors was originally very brief, for example effectively 2.5 mins of questioning, which permitted only a few questions. Questioning was initially restricted to \"whimsical conversation\", a domain suiting standard chatbot tricks.\n\nCompetition entrants do not aim at understanding or intelligence but resort to basic ELIZA style tricks, and successful entrants find deception and pretense is rewarded.\n\nReporting of the annual competition often confuses the imitation test with intelligence, a typical example being Brian Christian's introduction to his article \"Mind vs. Machine\" in The Atlantic, March 2011, stating that \"in the race to build computers that \"can think like humans\", the proving ground is the Turing Test\".\n\nIn 2006, the contest was organised by Tim Child (CEO of Televirtual) and Huma Shah. On August 30, the four finalists were announced:\n\nThe contest was held on 17 September in the VR theatre, Torrington Place campus of University College London. The judges included the University of Reading's cybernetics professor, Kevin Warwick, a professor of artificial intelligence, John Barnden (specialist in metaphor research at the University of Birmingham), a barrister, Victoria Butler-Cole and a journalist, Graham Duncan-Rowe. The latter's experience of the event can be found in an article in \"Technology Review\". The winner was 'Joan', based on Jabberwacky, both created by Rollo Carpenter.\n\nThe 2007 competition was held on October 21 in New York City. The judges were: computer science professor Russ Abbott, philosophy professor Hartry Field, psychology assistant professor Clayton Curtis and English lecturer Scott Hutchins.\n\nNo bot passed the Turing test, but the judges ranked the three contestants as follows:\n\nThe winner received $2,250 and the annual medal. The runners-up received $250 each.\n\nThe 2008 competition was organised by professor Kevin Warwick, coordinated by Huma Shah and held on October 12 at the University of Reading, UK. After testing by over one hundred judges during the preliminary phase, in June and July 2008, six finalists were selected from thirteen original entrants - artificial conversational entity (ACE). Five of those invited competed in the finals:\n\nIn the finals, each of the judges was given five minutes to conduct simultaneous, split-screen conversations with two hidden entities. Elbot of Artificial Solutions won the 2008 Loebner Prize bronze award, for \"most human-like\" artificial conversational entity, through fooling three of the twelve judges who interrogated it (in the human-parallel comparisons) into believing it was human. This is coming very close to the 30% traditionally required to consider that a program has actually passed the Turing test. Eugene Goostman and Ultra Hal both deceived one judge each that it was the human.\n\nWill Pavia, a journalist for The Times, has written about his experience; a Loebner finals' judge, he was deceived by Elbot and Eugene. Kevin Warwick and Huma Shah have reported on the parallel-paired Turing tests.\n\nThe 2009 Loebner Prize Competition was held September 6, 2009 at the Brighton Centre, Brighton UK in conjunction with the Interspeech 2009 conference. The prize amount for 2009 was $3,000.\n\nEntrants were David Levy, Rollo Carpenter, and Mohan Embar, who finished in that order.\n\nThe writer Brian Christian participated in the 2009 Loebner Prize Competition as a human confederate, and described his experiences at the competition in his book \"The Most Human Human\".\n\nThe 2010 Loebner Prize Competition was held on October 23 at California State University, Los Angeles. The 2010 competition was the 20th running of the contest. The winner was Bruce Wilcox with Suzette.\n\nThe 2011 Loebner Prize Competition was held on October 19 at the University of Exeter, Devon, United Kingdom. The prize amount for 2011 was $4,000.\n\nThe four finalists and their chatterbots were Bruce Wilcox (Rosette), Adeena Mignogna (Zoe), Mohan Embar (Chip Vivant) and Ron Lee (Tutor), who finished in that order.\n\nThat year there was an addition of a panel of junior judges, namely Jean-Paul Astal-Stain, William Dunne, Sam Keat and Kirill Jerdev. The results of the junior contest were markedly different from the main contest, with chatterbots Tutor and Zoe tying for first place and Chip Vivant and Rosette coming in third and fourth place, respectively.\n\nThe 2012 Loebner Prize Competition was held on the 15th of May in Bletchley Park in Bletchley, Buckinghamshire, England, in honor of the Alan Turing centenary celebrations. The prize amount for 2012 was $5,000. The local arrangements organizer was David Levy, who won the Loebner Prize in 1997 and 2009.\n\nThe four finalists and their chatterbots were Mohan Embar (Chip Vivant), Bruce Wilcox (Angela), Daniel Burke (Adam), M. Allan (Linguo), who finished in that order.\n\nThat year, a team from the University of Exeter's computer science department (Ed Keedwell, Max Dupenois and Kent McClymont) conducted the first-ever live webcast of the conversations.\n\nThe 2013 Loebner Prize Competition was held, for the first time on the Island of Ireland, on September 14 at the Ulster University, Magee College, Derry, Northern Ireland, UK.\n\nThe four finalists and their chatbots were Steve Worswick (Mitsuku), Dr. Ron C. Lee (Tutor), Bruce Wilcox (Rose) and Brian Rigsby (Izar), who finished in that order.\n\nThe judges were Professor Roger Schank (Socratic Arts), Professor Noel Sharkey (Sheffield University), Professor Minhua (Eunice) Ma (Huddersfield University, then University of Glasgow)\nand Professor Mike McTear (Ulster University).\n\nFor the 2013 Junior Loebner Prize Competition the chatbots Mitsuku and Tutor tied for first place with Rose and Izar in 3rd and 4th place respectively.\n\nThe 2014 Loebner Prize Competition was held at Bletchley Park, England, on Saturday 15 November 2014. The event was filmed live by Sky News. The guest judge was television presenter and broadcaster James May.\n\nAfter 2 hours of judging, 'Rose' by Bruce Wilcox was declared the winner. Bruce will receive a cheque for $4000 and a bronze medal. The ranks were as follows:\n\nRose - Rank 1 ($4000 & Bronze Medal); \nIzar - Rank 2.25 ($1500); \nUberbot - Rank 3.25 ($1000); and \nMitsuku - Rank 3.5 ($500).\n\nThe Judges were Dr Ian Hocking, Writer & Senior Lecturer in Psychology, Christ Church College, Canterbury; \nDr Ghita Kouadri-Mostefaoui, Lecturer in Computer Science and Technology, University of Bedfordshire; \nMr James May, Television Presenter and Broadcaster; and \nDr Paul Sant, Dean of UCMK, University of Bedfordshire.\n\nThe 2015 Loebner Prize Competition was again won by 'Rose' by Bruce Wilcox.\n\nThe judges were Jacob Aaron, Physical sciences reporter for New Scientist; Rory Cellan-Jones, Technology correspondent for the BBC; Brett Marty, Film Director and Photographer; Ariadne Tampion, Writer.\n\nThe 2016 Loebner Prize was held at Bletchley Park on 17 September 2016. After 2 hours of judging the final results were announced. \nThe ranks were as follows:\n\n\nOfficial list of winners.\n\n", "id": "238725", "title": "Loebner Prize"}
{"url": "https://en.wikipedia.org/wiki?curid=253279", "text": "Automated Mathematician\n\nThe Automated Mathematician (AM) is one of the earliest successful discovery systems. It was created by Douglas Lenat in Lisp, and in 1977 led to Lenat being awarded the IJCAI Computers and Thought Award.\n\nAM worked by generating and modifying short Lisp programs which were then interpreted as defining various mathematical concepts; for example, a program that tested equality between the length of two lists was considered to represent the concept of numerical equality, while a program that produced a list whose length was the product of the lengths of two other lists was interpreted as representing the concept of multiplication. The system had elaborate heuristics for choosing which programs to extend and modify, based on the experiences of working mathematicians in solving mathematical problems.\n\nLenat claimed that the system was composed of hundreds of data structures called \"concepts,\" together with hundreds of \"heuristic rules\" and a simple flow of control: \"AM repeatedly selects the top task from the agenda and tries to carry it out. This is the whole control structure!\" Yet the heuristic rules were not always represented as separate data structures; some had to be intertwined with the control flow logic. Some rules had preconditions that depended on the history, or otherwise could not be represented in the framework of the explicit rules.\n\nWhat's more, the published versions of the rules often involve vague terms that are not defined further, such as \"If two expressions are structurally similar, ...\" (Rule 218) or \"... replace the value obtained by some other (very similar) value...\" (Rule 129).\n\nAnother source of information is the user, via Rule 2: \"If the user has recently referred to X, then boost the priority of any tasks involving X.\" Thus, it appears quite possible that much of the real discovery work is buried in unexplained procedures.\n\nLenat claimed that the system had rediscovered both Goldbach's conjecture and the fundamental theorem of arithmetic. Later critics accused Lenat of over-interpreting the output of AM. In his paper \"Why AM and Eurisko appear to work\", Lenat conceded that any system that generated enough short Lisp programs would generate ones that could be interpreted by an external observer as representing equally sophisticated mathematical concepts. However, he argued that this property was in itself interesting—and that a promising direction for further research would be to look for other languages in which short random strings were likely to be useful.\n\nThis intuition was the basis of AM's successor Eurisko, which attempted to generalize the search for mathematical concepts to the search for useful heuristics.\n\n\n", "id": "253279", "title": "Automated Mathematician"}
{"url": "https://en.wikipedia.org/wiki?curid=1126216", "text": "Anticipation (artificial intelligence)\n\nIn artificial intelligence (AI), anticipation is the concept of an agent making decisions based on predictions, expectations, or beliefs about the future. It is widely considered that anticipation is a vital component of complex natural cognitive systems. As a branch of AI, anticipatory systems is a specialization still echoing the debates from the 1980s about the necessity for AI for an internal model.\n\nElementary forms of artificial intelligence can be constructed using a policy based on simple if-then rules. An example of such a system would be an agent following the rules\n\nA system such as the one defined above might be viewed as inherently reactive because the decision making is based on the current state of the environment with no explicit regard to the future. An agent employing anticipation would try to predict the future state of the environment (weather in this case) and make use of the predictions in the decision making. For example,\n\nThese rules appear more proactive, because they explicitly take into account possible future events. Notice though that in terms of representation and reasoning, these two rule sets are identical, both behave in response to existing conditions. Note too that both systems assume the agent is proactively\n\nIn practice, systems incorporating reactive planning tend to be autonomous systems proactively pursuing at least one, and often many, goals. What defines anticipation in an AI model is the explicit existence of an inner model of the environment for the anticipatory system (sometimes including the system itself). For example, if the phrase \"it will probably rain\" were computed on line in real time, the system would be seen as anticipatory.\n\nIn 1985, Robert Rosen defined an anticipatory system as follows:\n\nIn Rosen's work, analysis of the example : \"It's raining outside, therefore take the umbrella\" does involve a prediction. It involves the prediction that \"If it is raining, I will get wet out there unless I have my umbrella\". In that sense, even though it is already raining outside, the decision to take an umbrella is not a purely reactive thing. It involves the use of predictive models which tell us what will happen if we don't take the umbrella, when it is already raining outside.\n\nTo some extent, Rosen's definition of anticipation applies to any system incorporating machine learning. At issue is how much of a system's behaviour should or indeed can be determined by reasoning over dedicated representations, how much by on-line planning, and how much must be provided by the system's designers.\n\nThe anticipation of future states is also a major evolutionary and cognitive advance (Sjolander 1995).\nAnticipatory agents belonging to Rosen's definition are easy to see in human mental capabilities of taking decisions at a certain time T taking into account the effects of their own actions at different future timescales T+k. However, Rosen (a theoretical biologist) describes ALL living organisms as examples of naturally occurring anticipatory systems, which means that there must be somatic predictive models (meaning, \"of the body\"; physical) as components within the organization of all living organisms. No mental process is required for anticipation. In his book, Anticipatory Systems, Rosen describes how even single cellular organisms manifest this behavior pattern. It is logical to hypothesize therefore: If it is true that life is anticipatory in this sense, then the evolution of the conscious mind (such as human beings experience) may be a natural concentration and amplification of the anticipatory nature of life, itself.\n\nMachine learning methods started to integrate anticipatory capabilities in an implicit form as in reinforcement learning systems (Sutton & Barto, 1998; Balkenius, 1995) where they learn to anticipate future rewards and punishments caused by current actions (Sutton & Barto, 1998). Moreover, anticipation enhanced performance of machine learning techniques to face with complex environments where agents have to guide their attention to collect important information to act (Balkenius & Hulth, 1999).\n\nJürgen Schmidhuber modifies error back propagation algorithm to change neural network weights in order to decrease the mismatch between anticipated states and states actually experienced in the future (Schmidhuber - Adaptive curiosity and adaptive confidence, 1991). He introduces the concept of \"curiosity\" for agents as a measure of the mismatch between expectations and future experienced reality. Agents able to monitor and control their own curiosity explore situations where they expect to engage with novel experiences and are generally able to deal with complex environments more than the others.\n\n\n", "id": "1126216", "title": "Anticipation (artificial intelligence)"}
{"url": "https://en.wikipedia.org/wiki?curid=419463", "text": "Mindpixel\n\nMindpixel was a web-based collaborative artificial intelligence project which aimed to create a knowledgebase of millions of human validated true/false statements, or probabilistic propositions. It ran from 2000 to 2005.\n\nParticipants in the project created one-line statements which aimed to be objectively true or false to 20 other anonymous participants. In order to submit their statement they had first to check the true/false validity of 20 such statements submitted by others. Participants whose replies were consistently out of step with the majority had their status downgraded and were eventually excluded. Likewise, participants who made contributions which others could not agree were objectively true or false had their status downgraded. A validated true/false statement is called a mindpixel.\n\nThe project enlisted the efforts of thousands of participants and claimed to be \"the planet's largest artificial intelligence effort\".\n\nThe project was conceived by Chris McKinstry, a computer scientist and former Very Large Telescope operator for the European Southern Observatory in Chile, as \"MISTIC\" (Minimum Intelligent Signal Test Item Corpus) in 1996. Mindpixel was developed out of this program, and started in 2000 and had 1.4 million mindpixels in January 2004. The database and its software is known as GAC, which stands for \"\"Generic Artificial Consciousness\"\" and is pronounced Jak.\n\nMcKinstry believed that the Mindpixel database could be used in conjunction with a neural net to produce a body of human \"common sense\" knowledge which would have market value. Participants in the project were promised shares in any future value according to the number of mindpixels they had successfully created.\n\nOn 20 September 2005 Mindpixel lost its free server and is no longer operational. It was being rewritten by Chris McKinstry as Mindpixel 2 and was intended to appear on a new server in France.\n\nChris McKinstry died of suicide on 23 January 2006 and the future of the project and the integrity of the data is uncertain. The mindpixel.com domain currently points to an IQ test web site.\n\nSome Mindpixel data have been utilized by Michael Spivey of Cornell University and Rick Dale of The University of Memphis to study theories of high-level reasoning and continuous temporal dynamics of thought. McKinstry, along with Dale and Spivey, designed an experiment that has now been published in Psychological Science in its January, 2008 issue. In this paper, McKinstry (as posthumous first author), Dale, and Spivey use a very small and carefully selected set of Mindpixel statements to show that even high-level thought processes like decision making can be revealed in the nonlinear dynamics of bodily action.\n\nOther similar AI-driven knowledge acquisition projects are Never-Ending Language Learning and Open Mind Common Sense (run by MIT), the latter being also hampered when its director died of suicide.\n\n\n", "id": "419463", "title": "Mindpixel"}
{"url": "https://en.wikipedia.org/wiki?curid=1187268", "text": "Rational agent\n\nIn economics, game theory, decision theory, and artificial intelligence, a rational agent is an agent that has clear preferences, models uncertainty via expected values of variables or functions of variables, and always chooses to perform the action with the optimal expected outcome for itself from among all feasible actions. A rational agent can be anything that makes decisions, typically a person, firm, machine, or software. \n\nRational agents are also studied in the fields of cognitive science, ethics, and philosophy, including the philosophy of practical reason.\n\nIn reference to economics, rational agent refers to hypothetical consumers and how they make decisions in a free market. This concept is one of the assumptions made in neoclassical economic theory. The concept of economic rationality arises from a tradition of marginal analysis used in neoclassical economics. The idea of a rational agent is important to the philosophy of utilitarianism, as detailed by philosopher Jeremy Bentham's theory of the felicific calculus, also known as the hedonistic calculus.\n\nThe action a rational agent takes depends on:\nIn game theory and classical economics, it is often assumed that the actors, people, and firms are rational. However, the extent to which people and firms behave rationally is subject to debate. Economists often assume the models of rational choice theory and bounded rationality to formalize and predict the behavior of individuals and firms. Rational agents sometimes behave in manners that are counter-intuitive to many people, as in the traveler's dilemma.\n\nMany economic theories reject utilitarianism and rational agency, especially those that might be considered heterodox. \n\nFor example, Thorstein Veblen, known as the father of institutional economics, rejects the notion of hedonistic calculus and pure rationality saying: \"The hedonistic conception of man is that of a lightning calculator of pleasures and pains who oscillates like a homogeneous globule of desire of happiness under the impulse of stimuli that shift him about the area, but leave him intact.\" \n\nVeblen instead perceives human economic decisions as the result of multiple complex cumulative factors: \"It is the characteristic of man to do something, not simply to suffer pleasures and pains through the impact of suitable forces. He is ... a coherent structure of propensities and habits which seeks realization and expression in an unfolding activity. ... They are the products of his hereditary traits and his past experience, cumulatively wrought out under a given body of traditions conventionalities, and material circumstances; and they afford the point of departure for the next step in the process. The economic life history of the individual is a cumulative process of adaptation of means to ends that cumulatively change as the process goes on, both the agent and his environment being at any point the outcome of the last process.\"\nEvolutionary economics also provides criticisms of the Rational Agent, citing the \"parental bent\" (the idea that biological impulses can and do frequently override rational decision making based on utility). Arguments against rational agency have also cited the enormous influence of marketing as proof that humans can be persuaded to make economic decisions that are \"non-rational\" in nature.\n\nNeuroeconomics is a concept that uses neuroscience, social psychology and other fields of science to better understand how people make decisions. Unlike rational agent theory, neuroeconomics does not attempt to predict large-scale human behavior but rather how individuals make decisions in case-by-case scenarios.\n\nArtificial intelligence has borrowed the term \"rational agents\" from economics to describe autonomous programs that are capable of goal directed behavior. Today there is a considerable overlap between AI research, game theory and decision theory. Rational agents in AI are closely related to \"intelligent agents\", autonomous software programs that display intelligence.\n\n\n\n", "id": "1187268", "title": "Rational agent"}
{"url": "https://en.wikipedia.org/wiki?curid=821197", "text": "Cognitive tutor\n\nA cognitive tutor is a particular kind of intelligent tutoring system that utilizes a cognitive model to provide feedback to students as they are working through problems. This feedback will immediately inform students of the correctness, or incorrectness, of their actions in the tutor interface; however, cognitive tutors also have the ability to provide context-sensitive hints and instruction to guide students towards reasonable next steps.\n\nThe name of Cognitive Tutor® now usually refers to a particular type of intelligent tutoring system produced by Carnegie Learning for high school mathematics based on John Anderson's ACT-R theory of human cognition. However, cognitive tutors were originally developed to test ACT-R theory for research purposes since the early 1980s and they are developed also for other areas and subjects such as computer programming and science. Cognitive Tutors can be implemented into classrooms as a part of blended learning that combines textbook and software activities.\n\nThe Cognitive Tutor programs utilize cognitive model and are based on model tracing and knowledge tracing. Model tracing means that the cognitive tutor checks every action performed by students such as entering a value or clicking a button, while knowledge tracing is used to calculate the required skills students learned by measuring them on a bar chart called Skillometer. Model tracing and knowledge tracing are essentially used to monitor students' learning progress, guide students to correct path to problem solving, and provide feedback.\n\nThe Institute of Education Sciences published several reports regarding the effectiveness of Carnegie Cognitive Tutor®. A 2013 report concluded that Carnegie Learning Curricula and Cognitive Tutor® was found to have mixed effects on mathematics achievement for high school students. The report identified 27 studies that investigate the effectiveness of Cognitive Tutor ®, and the conclusion is based on 6 studies that meet What Works Clearinghouse standards. Among the 6 studies included, 5 of them show intermediate to significant positive effect, while 1 study shows statistically significant negative effect. Another report published by Institute of Education Sciences in 2009 found that Cognitive Tutor® Algebra I to have potentially positive effects on math achievement based on only 1 study out of 14 studies that meets What Works Clearinghouse standards.it should be understood that What Works Clearinghouse standards call for relatively large numbers of participants, true random assignments to groups, and for a control group receiving either no treatment or a different treatment. Such experimental conditions are difficult to meet in schools, and thus only a small percentage of studies in education meet the standards of this clearinghouse, even though they may still be of value.\n\nIntelligent tutoring systems (ITS) traditionally had a three-component architecture: domain model, student model, and tutoring model. Later, a fourth component was added: the interface component. Now ITS is commonly known to have a four component architecture.\n\nDomain model contains the rules, concepts, and knowledge related to the domain to be learned. It helps to evaluate students' performance and detect students' errors by setting a standard of domain expertise.\n\nStudent model, the central component of an ITS, is expected to contain as much knowledge as possible about the students: their cognitive and affective states, and the progress they gained as they learn. The functions of the student model is three folded: to gather data from and about the learner, to represent the learner's knowledge and learning process, and to perform diagnosis of students' knowledge and select optimal pedagogical strategies.\n\nBased on the data gained from domain model and student model, tutoring model makes decisions about tutoring strategies such as whether or not to intervene, when and how to intervene. Functions of tutoring model include instruction delivery and content planning.\n\nInterface model reflects the decisions made by tutoring model in different forms such as Socratic dialogs, feedback and hints. Students interact with the tutor through the learning interface, also known as communication. Interface also provide domain knowledge elements.\n\nA \"cognitive model\" tries to model the domain knowledge in the same way knowledge is represented in the human mind. Cognitive model enables intelligent tutoring systems to respond to problem-solving situations as the learner would. A tutoring system adopting a cognitive model is called a cognitive tutor.\n\nCognitive model is an expert system which hosts a multitude of solutions to the problems presented to students. The cognitive model is used to trace each student's solution through complex problems, enabling the tutor to provide step-by-step feedback and advice, and to maintain a targeted model of the student's knowledge based on student performance.\n\nCognitive Tutors provide step-by-step guidance as a learner develops a complex problem-solving skill through practice. Typically, cognitive tutors provide such forms of support as: (a) a problem-solving environment that is designed rich and \"thinking visible\"; (b) step-by-step feedback on student performance; (c) feedback messages specific to errors; (d) context-specific next-step hints at student's request, and (e) individualized problem selection.\n\nCognitive Tutors accomplish two of the principal tasks characteristic of human tutoring: (1) monitors the student's performance and providing context-specific individual instruction, and (2) monitors the student's learning and selects appropriate problem-solving activities.\n\nBoth cognitive model and two underlying algorithms, model tracing and knowledge tracing, are used to monitor the student's learning. In model tracing, the cognitive tutor uses the cognitive model in complex problems to follow the student's individual path and provide prompt accuracy feedback and context-specific advice. In knowledge tracing, the cognitive tutor uses a simple Bayesian method of evaluating the student's knowledge and uses this student model to select appropriate problems for individual student.\n\nCognitive tutors use a cognitive architecture which specifies the underlying framework for an intelligent system.\n\nACT-R, a member of ACT family, is the most recent cognitive architecture, devoted primarily to modelling human behavior. ACT-R includes a declarative memory of factual knowledge and a procedural memory of production rules. The architecture functions by matching productions on perceptions and facts, mediated by the real-valued activation levels of objects, and executing them to affect the environment or alter declarative memory. ACT-R has been used to model psychological aspects such as memory, attention, reasoning, problem solving, and language processing.\n\nIn 1984, a group of high school students participated in a mini course in computer science and tried out the geometry tutor through Carnegie Mellon University using the LISP tutor, one of the first iterations of the cognitive tutor.\nSince then, cognitive tutors have been used in a variety of scenarios, with a few organizations developing their own cognitive tutor programs. These programs have been used for students spanning elementary school through to university level, though primarily in the subject areas of Computer Programming, Mathematics, and Science.\nOne of the first organizations to develop a system for use within the school system was the PACT Center at Carnegie Mellon University. Their aim was to \"...develop systems that provide individualized assistance to students as they work on challenging real-world problems in complex domains such as computer programming, algebra and geometry\". PACT's most successful product was the \"Cognitive Tutor Algebra\" course. Originally created in the early 1990s, this course, at its peak, was offered at 75 schools through the U.S. by 1999, and then its spin-off company, Carnegie Learning, now offers tutors to over 1400 schools in the U.S.\n\nThe Carnegie Mellon Cognitive Tutor has attempted to raise students' math test scores in high school and middle-school classrooms, and their Algebra course was designated one of five exemplary curricula for K-12 mathematics educated by the US Department of Education.\nThere were several research projects conducted by the PACT Center to utilize Cognitive tutor for courses in Excel and to develop an intelligent tutoring system for algebra expression writing, called \"Ms. Lindquist\". Initially used by thousands of students, the aim of the Ms. Lindquist program was to have a tutor that could carry on running conversations with a student, with the intent of providing a more human connection. Further, in 2005, Carnegie Learning released \"Bridge to Algebra\", a product intended for middle schools that was piloted in over 100 schools.\n\nCognitive tutoring software is continuing to be used. According to a recent \"Business Insider Report\" article, Ken Koedinger, a professor of human-computer interaction and psychology at Carnegie Mellon University, says teachers can integrate cognitive tutoring software into the classroom. He suggests that teachers use it in a computer lab environment or during classes. In its more current versions, cognitive tutors can understand the many ways that a student might answer a problem, and then assist the student at the exact time that the help is required. Further, the cognitive tutor can customize exercises specific to the student's needs.\n\nAt this time it is unclear whether Cognitive Tutor® is effective at improving student performance. Cognitive Tutor® has had some commercial success however, there may be limitations inherently linked to its design and the nature of intelligent tutoring systems. The following section discusses limitations of Cognitive Tutor® which may also apply to other intelligent tutoring systems.\n\nAt this time, creating a Cognitive Tutor® for all subject areas is not practical or economical. Cognitive Tutor® has been used successfully but is still limited to tutoring algebra, computer programming and geometry because these subject areas have an optimal balance of production rules, complexity and maximum benefit to the learner.\n\nThe focus of Cognitive Tutor® development has been the design of the software to teach specific production rules and not on the development of curricular content. Despite many years of trials, improvements, and a potential to advance learning objectives, the creators continue to rely primarily on outside sources for curricular direction.\n\nThe complexity of Cognitive Tutor® software requires designers to spend hundreds of hours per instructional hour to create the program. Despite the time invested, the challenges associated with meeting the needs of the learner within the constraints of the design often result in compromises in flexibility and cognitive fidelity.\n\nPracticality dictates that designers must choose from a discrete set of methods to teach and support learners. Limited choices of methods, prompts and hints may be effective in supporting some learners but may conflict with the methods already in use by others. In addition, it is possible that learners will use the system of prompts and hints to access the answers prematurely thereby advancing through the exercises which may result in them not meeting the learning objectives.\n\nThe cognitive model, which inspired Cognitive Tutor® is based on assumptions about how learning occurs which dictates the chosen instructional methods such as hints, directions and timing of the tutoring prompts. Given these assumptions and the limited methods of presentation, Cognitive Tutor® may not account for the flexible, complex and diverse ways humans create knowledge. Human tutors outperform Cognitive Tutor® by providing a higher level of responsiveness to student errors. They are capable of providing more effective feedback and scaffolding to learners than Cognitive Tutor®, indicating the cognitive model may still be incomplete.\n\n\n", "id": "821197", "title": "Cognitive tutor"}
{"url": "https://en.wikipedia.org/wiki?curid=658223", "text": "Autonomic computing\n\nAutonomic computing (also known as AC) refers to the self-managing characteristics of distributed computing resources, adapting to unpredictable changes while hiding intrinsic complexity to operators and users. Initiated by IBM in 2001, this initiative ultimately aimed to develop computer systems capable of self-management, to overcome the rapidly growing complexity of computing systems management, and to reduce the barrier that complexity poses to further growth.\nThe AC system concept is designed to make adaptive decisions, using high-level policies. It will constantly check and optimize its status and automatically adapt itself to changing conditions. An autonomic computing framework is composed of autonomic components (AC) interacting with each other. An AC can be modeled in terms of two main control schemes (local and global) with sensors (for self-monitoring), effectors (for self-adjustment), knowledge and planner/adapter for exploiting policies based on self- and environment awareness. This architecture is sometimes referred to as Monitor-Analyze-Plan-Execute (MAPE).\n\nDriven by such vision, a variety of architectural frameworks based on \"self-regulating\" autonomic components has been recently proposed. A very similar trend has recently characterized significant research in the area of multi-agent systems. However, most of these approaches are typically conceived with centralized or cluster-based server architectures in mind and mostly address the need of reducing management costs rather than the need of enabling complex software systems or providing innovative services. Some autonomic systems involve mobile agents interacting via loosely coupled communication mechanisms.\n\n\"Autonomy-oriented computation\" is a paradigm proposed by Jiming Liu in 2001 that uses artificial systems imitating social animals' collective behaviours to solve difficult computational problems. For example, ant colony optimization could be studied in this paradigm.\n\nForecasts suggest that the number of computing devices in use will grow at 38% per year and the average complexity of each device is increasing. Currently, this volume and complexity is managed by highly skilled humans; but the demand for skilled IT personnel is already outstripping supply, with labour costs exceeding equipment costs by a ratio of up to 18:1. Computing systems have brought great benefits of speed and automation but there is now an overwhelming economic need to automate their maintenance.\n\nIn a 2003 IEEE \"Computer Magazine\" article, Kephart and Chess \nwarn that the dream of interconnectivity of computing systems and devices could become the “nightmare of pervasive computing” in which architects are unable to anticipate, design and maintain the complexity of interactions. They state the essence of autonomic computing is system self-management, freeing administrators from low-level task management while delivering better system behavior.\n\nA general problem of modern distributed computing systems is that their complexity, and in particular the complexity of their management, is becoming a significant limiting factor in their further development. Large companies and institutions are employing large-scale computer networks for communication and computation. The distributed applications running on these computer networks are diverse and deal with many tasks, ranging from internal control processes to presenting web content to customer support.\n\nAdditionally, mobile computing is pervading these networks at an increasing speed: employees need to communicate with their companies while they are not in their office. They do so by using laptops, personal digital assistants, or mobile phones with diverse forms of wireless technologies to access their companies' data.\n\nThis creates an enormous complexity in the overall computer network which is hard to control manually by human operators. Manual control is time-consuming, expensive, and error-prone. The manual effort needed to control a growing networked computer-system tends to increase very quickly.\n\n80% of such problems in infrastructure happen at the client specific application and database layer. Most 'autonomic' service providers guarantee only up to the basic plumbing layer (power, hardware, operating system, network and basic database parameters).\n\nA possible solution could be to enable modern, networked computing systems to manage themselves without direct human intervention. The \"Autonomic Computing Initiative\" (ACI) aims at providing the foundation for autonomic systems. It is inspired by the autonomic nervous system of the human body. This nervous system controls important bodily functions (e.g. respiration, heart rate, and blood pressure) without any conscious intervention.\n\nIn a self-managing autonomic system, the human operator takes on a new role: instead of controlling the system directly, he/she defines general policies and rules that guide the self-management process. For this process, IBM defined the following four types of property referred to as self-star (also called self-*, self-x, or auto-*) properties. \n\nOthers such as Poslad and Nami and Bertel have expanded on the set of self-star as follows:\n\nIBM has set forth eight conditions that define an autonomic system:\n\nThe system must\n\nEven though the purpose and thus the behaviour of autonomic systems vary from system to system, every autonomic system should be able to exhibit a minimum set of properties to achieve its purpose:\n\n\nIBM defined five evolutionary levels, or the \"autonomic deployment model\", for the deployment of autonomic systems:\n\nThe design complexity of Autonomic Systems can be simplified by utilizing design patterns such as the model-view-controller (MVC) pattern to improve concern separation by encapsulating functional concerns.\n\nA basic concept that will be applied in Autonomic Systems are closed control loops. This well-known concept stems from Process Control Theory. Essentially, a closed control loop in a self-managing system monitors some resource (software or hardware component) and autonomously tries to keep its parameters within a desired range.\n\nAccording to IBM, hundreds or even thousands of these control loops are expected to work in a large-scale self-managing computer system.\n\nA fundamental building block of an autonomic system is the sensing capability (\"Sensors S\"), which enables the system to observe its external operational context. Inherent to an autonomic system is the knowledge of the \"Purpose\" (intention) and the \"Know-how\" to operate itself (e.g., bootstrapping, configuration knowledge, interpretation of sensory data, etc.) without external intervention. The actual operation of the autonomic system is dictated by the \"Logic\", which is responsible for making the right decisions to serve its \"Purpose\", and influence by the observation of the operational context (based on the sensor input).\n\nThis model highlights the fact that the operation of an autonomic system is purpose-driven. This includes its mission (e.g., the service it is supposed to offer), the policies (e.g., that define the basic behaviour), and the “survival instinct”. If seen as a control system this would be encoded as a feedback error function or in a heuristically assisted system as an algorithm combined with set of heuristics bounding its operational space.\n\n\n", "id": "658223", "title": "Autonomic computing"}
{"url": "https://en.wikipedia.org/wiki?curid=2368264", "text": "Connectionist expert system\n\nConnectionist expert systems are artificial neural network (ANN) based expert systems where the ANN generates inferencing rules e.g., fuzzy-multi layer perceptron where linguistic and natural form of inputs are used. Apart from that, rough set theory may be used for encoding knowledge in the weights better and also genetic algorithms may be used to optimize the search solutions better. Symbolic reasoning methods may also be incorporated (see hybrid intelligent system). (Also see expert system, neural network, clinical decision support system.)\n\n", "id": "2368264", "title": "Connectionist expert system"}
{"url": "https://en.wikipedia.org/wiki?curid=2715076", "text": "Neuro-fuzzy\n\nIn the field of artificial intelligence, neuro-fuzzy refers to combinations of artificial neural networks and fuzzy logic.\n\nNeuro-fuzzy hybridization results in a hybrid intelligent system that synergizes these two techniques by combining the human-like reasoning style of fuzzy systems with the learning and connectionist structure of neural networks. Neuro-fuzzy hybridization is widely termed as fuzzy neural network (FNN) or neuro-fuzzy system (NFS) in the literature. Neuro-fuzzy system (the more popular term is used henceforth) incorporates the human-like reasoning style of fuzzy systems through the use of fuzzy sets and a linguistic model consisting of a set of IF-THEN fuzzy rules. The main strength of neuro-fuzzy systems is that they are universal approximators with the ability to solicit interpretable IF-THEN rules.\n\nThe strength of neuro-fuzzy systems involves two contradictory requirements in fuzzy modeling: interpretability versus accuracy. In practice, one of the two properties prevails. The neuro-fuzzy in fuzzy modeling research field is divided into two areas: linguistic fuzzy modeling that is focused on interpretability, mainly the Mamdani model; and precise fuzzy modeling that is focused on accuracy, mainly the Takagi-Sugeno-Kang (TSK) model.\n\nAlthough generally assumed to be the realization of a fuzzy system through connectionist networks, this term is also used to describe some other configurations including:\n\nIt must be pointed out that interpretability of the Mamdani-type neuro-fuzzy systems can be lost. To improve the interpretability of neuro-fuzzy systems, certain measures must be taken, wherein important aspects of interpretability of neuro-fuzzy systems are also discussed.\n\nA recent research line addresses the data stream mining case, where neuro-fuzzy systems are sequentially updated with new incoming samples on demand and on-the-fly. Thereby, system updates do not only include a recursive adaptation of model parameters, but also a dynamic evolution and pruning of model components (neurons, rules), in order to handle concept drift and dynamically changing system behavior adequately and to keep the systems/models \"up-to-date\" anytime. \nComprehensive surveys of various evolving neuro-fuzzy systems approaches can be found in and.\n\nPseudo outer product-based fuzzy neural networks (POPFNN) are a family of neuro-fuzzy systems that are based on the linguistic fuzzy model.\n\nThree members of POPFNN exist in the literature:\n\nThe \"POPFNN\" architecture is a five-layer neural network where the layers from 1 to 5 are called: input linguistic layer, condition layer, rule layer, consequent layer, output linguistic layer. The fuzzification of the inputs and the defuzzification of the outputs are respectively performed by the input linguistic and output linguistic layers while the fuzzy inference is collectively performed by the rule, condition and consequence layers.\n\nThe learning process of POPFNN consists of three phases:\n\nVarious fuzzy membership generation algorithms can be used: Learning Vector Quantization (LVQ), Fuzzy Kohonen Partitioning (FKP) or Discrete Incremental Clustering (DIC). Generally, the POP algorithm and its variant LazyPOP are used to identify the fuzzy rules.\n\n\n", "id": "2715076", "title": "Neuro-fuzzy"}
{"url": "https://en.wikipedia.org/wiki?curid=2840305", "text": "Computer-assisted proof\n\nA computer-assisted proof is a mathematical proof that has been at least partially generated by computer.\n\nMost computer-aided proofs to date have been implementations of large proofs-by-exhaustion of a mathematical theorem. The idea is to use a computer program to perform lengthy computations, and to provide a proof that the result of these computations implies the given theorem. In 1976, the four color theorem was the first major theorem to be verified using a computer program.\n\nAttempts have also been made in the area of artificial intelligence research to create smaller, explicit, new proofs of mathematical theorems from the bottom up using machine reasoning techniques such as heuristic search. Such automated theorem provers have proved a number of new results and found new proofs for known theorems. Additionally, interactive proof assistants allow mathematicians to develop human-readable proofs which are nonetheless formally verified for correctness. Since these proofs are generally human-surveyable (albeit with difficulty, as with the proof of the Robbins conjecture) they do not share the controversial implications of computer-aided proofs-by-exhaustion.\n\nOne method for using computers in mathematical proofs is by means of so-called validated numerics or rigorous numerics. This means computing numerically yet with mathematical rigour. One uses set-valued arithmetic and inclusion principle in order to ensure that the set-valued output of a numerical program encloses the solution of the original mathematical problem. This is done by controlling, enclosing and propagating round-off and truncation errors using for example interval arithmetic. More precisely, one reduces the computation to a sequence of elementary operations, say (+,-,*,/). In a computer, the result of each elementary operation is rounded off by the computer precision. However, one can construct an interval provided by upper and lower bounds on the result of an elementary operation. Then one proceeds by replacing numbers with intervals and performing elementary operations between such intervals of representable numbers.\n\nComputer-assisted proofs are the subject of some controversy in the mathematical world, with Thomas Tymoczko first to articulate objections. Those who adhere to Tymoczko's arguments believe that lengthy computer-assisted proofs are not, in some sense, 'real' mathematical proofs because they involve so many logical steps that they are not practically verifiable by human beings, and that mathematicians are effectively being asked to replace logical deduction from assumed axioms with trust in an empirical computational process, which is potentially affected by errors in the computer program, as well as defects in the runtime environment and hardware.\n\nOther mathematicians believe that lengthy computer-assisted proofs should be regarded as \"calculations\", rather than \"proofs\": the proof algorithm itself should be proved valid, so that its use can then be regarded as a mere \"verification\". Arguments that computer-assisted proofs are subject to errors in their source programs, compilers, and hardware can be resolved by providing a formal proof of correctness for the computer program (an approach which was successfully applied to the four-color theorem in 2005) as well as replicating the result using different programming languages, different compilers, and different computer hardware.\n\nAnother possible way of verifying computer-aided proofs is to generate their reasoning steps in a machine-readable form, and then use an automated theorem prover to demonstrate their correctness. This approach of using a computer program to prove another program correct does not appeal to computer proof skeptics, who see it as adding another layer of complexity without addressing the perceived need for human understanding.\n\nAnother argument against computer-aided proofs is that they lack mathematical elegance—that they provide no insights or new and useful concepts. In fact, this is an argument that could be advanced against any lengthy proof by exhaustion.\n\nAn additional philosophical issue raised by computer-aided proofs is whether they make mathematics into a quasi-empirical science, where the scientific method becomes more important than the application of pure reason in the area of abstract mathematical concepts. This directly relates to the argument within mathematics as to whether mathematics is based on ideas, or \"merely\" an exercise in formal symbol manipulation. It also raises the question whether, if according to the Platonist view, all possible mathematical objects in some sense \"already exist\", whether computer-aided mathematics is an observational science like astronomy, rather than an experimental one like physics or chemistry. Interestingly, this controversy within mathematics is occurring at the same time as questions are being asked in the physics community about whether twenty-first century theoretical physics is becoming too mathematical, and leaving behind its experimental roots.\n\nThe emerging field of experimental mathematics is confronting this debate head-on by focusing on numerical experiments as its main tool for mathematical exploration.\n\nIn 2010, academics at The University of Edinburgh offered people the chance to \"buy their own theorem\" created through a computer-assisted proof. This new theorem would be named after the purchaser.\n\nInclusion in this list does not imply that a formal computer-checked proof exists, but rather, that a computer program has been involved in some way. See the main articles for details.\n\n\n\n", "id": "2840305", "title": "Computer-assisted proof"}
{"url": "https://en.wikipedia.org/wiki?curid=2932246", "text": "Hybrid intelligent system\n\nHybrid intelligent system denotes a software system which employs, in parallel, a combination of methods and techniques from artificial intelligence subfields as:\n\nFrom the cognitive science perspective, every natural intelligent system is hybrid because it performs mental operations on both the symbolic and subsymbolic levels. For the past few years there has been an increasing discussion of the importance of A.I. Systems Integration. Based on notions that there have already been created simple and specific AI systems (such as systems for computer vision, speech synthesis, etc., or software that employs some of the models mentioned above) and now is the time for integration to create broad AI systems. Proponents of this approach are researchers such as Marvin Minsky, Ron Sun, Aaron Sloman, and Michael A. Arbib.\n\nAn example hybrid is a hierarchical control system in which the lowest, reactive layers are sub-symbolic. The higher layers, having relaxed time constraints, are capable of reasoning from an abstract world model and performing planning.\n\nIntelligent systems usually rely on hybrid reasoning processes, which include induction, deduction, abduction and reasoning by analogy.\n\n\n", "id": "2932246", "title": "Hybrid intelligent system"}
{"url": "https://en.wikipedia.org/wiki?curid=3291080", "text": "Rough fuzzy hybridization\n\nRough fuzzy hybridization is a method of hybrid intelligent system or soft computing, where Fuzzy set theory is used for linguistic representation of patterns, leading to a \"fuzzy granulation\" of the feature space. Rough set theory is used to obtain dependency rules which model informative regions in the granulated feature space.\n\n", "id": "3291080", "title": "Rough fuzzy hybridization"}
{"url": "https://en.wikipedia.org/wiki?curid=2708995", "text": "Model-based reasoning\n\nIn artificial intelligence, model-based reasoning refers to an inference method used in expert systems based on a model of the physical world. With this approach, the main focus of application development is developing the model. Then at run time, an \"engine\" combines this model knowledge with observed data to derive conclusions such as a diagnosis or a prediction.\n\nIn a model-based reasoning system knowledge can be represented using causal rules. For example, in a medical diagnosis system the knowledge base may contain the following rule:\nIn contrast in a diagnostic reasoning system knowledge would be represented through diagnostic rules such as:\n\nThere are many other forms of models that may be used. Models might be quantitative (for instance, based on mathematical equations) or qualitative (for instance, based on cause/effect models.) They may include representation of uncertainty. They might represent behavior over time. They might represent \"normal\" behavior, or might only represent abnormal behavior, as in the case of the examples above. Model types and usage for model-based reasoning are discussed in.\n\n\n", "id": "2708995", "title": "Model-based reasoning"}
{"url": "https://en.wikipedia.org/wiki?curid=637434", "text": "Principle of rationality\n\nThe 'principle of rationality' (or 'rationality principle') was coined by Karl R. Popper in his Harvard Lecture of 1963, and published in his book \"Myth of Framework\". It is related to what he called the 'logic of the situation' in an \"Economica\" article of 1944/1945, published later in his book \"The Poverty of Historicism\". According to Popper’s rationality principle, agents act in the most adequate way according to the objective situation. It is an idealized conception of human behavior which he used to drive his model of situational analysis.\n\nPopper called for social science to be grounded in what he called situational analysis. This requires building models of social situations which include individual actors and their relationship to social institutions, e.g. markets, legal codes, bureaucracies, etc. These models attribute certain aims and information to the actors. This forms the 'logic of the situation', the result of reconstructing meticulously all circumstances of an historical event. The 'principle of rationality' is the assumption that people are instrumental in trying to reach their goals, and this is what drives the model. Popper believed that this model could be continuously refined to approach the objective truth.\n\nPopper called his 'principle of rationality' \"nearly empty\" (a technical term meaning without empirical content) and \"strictly speaking false\", but nonetheless \"tremendously useful\". These remarks earned him a lot of criticism because seemingly he had swerved from his famous Logic of Scientific Discovery.\n\nAmong the many philosophers having discussed his 'principle of rationality' from the 1960s up to now are Noretta Koertge, R. Nadeau, Viktor J. Vanberg, Hans Albert, E. Matzner, Ian C. Jarvie, Mark A. Notturno, John Wettersten, Ian C. Böhm.\n\nIn the context of knowledge-based systems, Newell (in 1982) proposed the following principle of rationality: \"If an agent has knowledge that one of its actions will lead to one of its goals, then the agent will select that action.\" This principle is employed by agents at the knowledge level to move closer to a desired goal. An important philosophical difference between Newell and Popper is that Newell argued that the knowledge level is real in the sense that it exists in nature and is not made up. This allowed Newell to treat the rationality principle as a way of understanding nature and avoid the problems Popper ran into by treating knowledge as non physical and therefore non empirical. \n\n", "id": "637434", "title": "Principle of rationality"}
{"url": "https://en.wikipedia.org/wiki?curid=3070989", "text": "ICAD (software)\n\nICAD (Corporate history: ICAD, Inc., Concentra (name change at IPO in 1995), KTI (name change in 1998), Dassault Systemes (purchase in 2001) () is a Knowledge-Based Engineering (KBE) system that enables users to encode design knowledge using a semantic representation that can be evaluated for parasolid output. ICAD has an open architecture that can utilize all the power and flexibility of the underlying language.\n\nKBE, as implemented via ICAD, received a lot of attention due to the remarkable results that appeared to take little effort. ICAD allowed one example of end-user computing that in a sense is unparalleled. Most ICAD developers were degreed engineers. Systems developed by ICAD users were non-trivial and consisted of highly complicated code. In the sense of end-user computing, ICAD was the first to allow the power of a domain tool to be in the hands of the user at the same time being open to allow extensions as identified and defined by the domain expert or SME.\n\nA COE article looked at the resulting explosion of expectations (see AI Winter), which were not sustainable. However, such a bubble burst does not diminish the existence of capability that would exist if expectations and use were properly managed.\n\nThe original implementation of ICAD was on a Lisp machine (Symbolics). Some of the principals involved with the development were Larry Rosenfeld, Avrum Belzer, Patrick M. O'Keefe, Philip Greenspun, and David F. Place. The time frame was 1984–85.\n\nICAD started on special-purpose Symbolics Lisp hardware and was then ported to Unix when Common Lisp became portable to general-purpose workstations.\n\nThe original domain for ICAD was mechanical design with many application successes. However, ICAD has found use in other domains, such as electrical design, shape modeling, etc. An example project could be wind tunnel design or the development of a support tool for aircraft multidisciplinary design. Further examples can be found in the presentations at the annual IIUG (International ICAD Users Group) that have been published in the KTI Vault (1999 through 2002). Boeing and Airbus used ICAD extensively to develop various components in the 1990s and early 21st century.\n\nAs of 2003, ICAD was featured strongly in several areas as evidenced by the Vision & Strategy Product Vision and Strategy presentation. After 2003, ICAD use diminished. At the end of 2001, the KTI Company faced financial difficulties and laid off most of its best staff. They were eventually bought out by Dassault who effectively scuppered the ICAD product. See IIUG at COE, 2003 (first meeting due to Dassault by KTI)\n\nThe ICAD system was very expensive, relatively, and was in the price range of high-end systems. Market dynamics couldn't support this as there may not have been sufficient differentiating factors between ICAD and the lower-end systems (or the promises from Dassault). KTI was absorbed by Dassault Systemes and ICAD is no longer considered the go-forward tool for knowledge-based engineering (KBE) applications by that company. Dassault Systemes is promoting a suite of tools oriented around version 5 of their popular CATIA CAD application, with Knowledgeware the replacement for ICAD.\n\n, things were still a bit unclear. ICAD 8.3 was delivered.\nThe recent COE Aerospace Conference had a discussion about the futures of KBE. One issue involves the stacking of 'meta' issues within a computer model. How this is resolved, whether by more icons or the availability of an external language, remains to be seen.\n\nThe Genworks GDL product (including kernel technology from the Gendl Project) is the nearest functional equivalent to ICAD currently available.\n\nICAD provided a declarative language (IDL) using New Flavors (never converted to CLOS) that supported a mechanism for relating parts (defpart) via a hierarchical set of relationships. Technically, the ICAD Defpart was a Lisp macro; the ICAD defpart list was a set of generic classes that can be instantiated with specific properties depending upon what was represented. This defpart list was extendible via composited parts that represented domain entities. Along with the part-subpart relations, ICAD supported generic relations via the object modeling capabilities of Lisp.\n\nExample applications of ICAD range from a small collection of defparts that represents a part or component to a larger collection that represents an assembly. In terms of power, an ICAD system, when fully specified, can generate thousands of instances of parts on a major assembly design.\n\nOne example of an application driving thousands of instances of parts is that of an aircraft wing - where fastener type and placement may number in the thousands, each instance requiring evaluation of several factors driving the design parameters.\n\nOne role for ICAD may be serving as the defining prototype for KBE which would require that we know more about what occurred the past 15 years (much information is tied up behind corporate firewalls and under proprietary walls). With the rise of the functional languages (an example is Haskell) on the landscape, perhaps some of the power that is attributable to Lisp may be replicated.\n\n\n", "id": "3070989", "title": "ICAD (software)"}
{"url": "https://en.wikipedia.org/wiki?curid=3907217", "text": "Hybrid neural network\n\nThe term hybrid neural network can have two meanings: \n\nAs for the first meaning, the artificial neurons and synapses in hybrid networks can be digital or analog. For the digital variant voltage clamps are used to monitor the membrane potential of neurons, to computationally simulate artificial neurons and synapses and to stimulate biological neurons by inducing synaptic. For the analog variant, specially designed electronic circuits connect to a network of living neurons through electrodes. \n\nAs for the second meaning, incorporating elements of symbolic computation and artificial neural networks into one model was an attempt to combine the advantages of both paradigms while avoid the shortcomings. Symbolic representations have advantages with respect to explicit, direct control, fast initial coding, dynamic variable binding and knowledge abstraction. Representations of artificial neural networks, on the other hand, show advantages for biological plausibility, learning, robustness (fault-tolerant processing and graceful decay), and generalization to similar input. Since the early 1990s many attempts have been made to reconcile the two approaches. \n\n\n", "id": "3907217", "title": "Hybrid neural network"}
{"url": "https://en.wikipedia.org/wiki?curid=2376785", "text": "ASR-complete\n\nASR-complete is, by analogy to \"NP-completeness\" in complexity theory, a term to indicate that the difficulty of a computational problem is equivalent to solving the central Automatic Speech Recognition problem, i.e. recognize and understanding spoken language. Unlike \"NP-completeness\", this term is typically used informally.\n\nSuch problems are hypothesised to include:\n\n\nThese problems are easy for humans to do (in fact, they are described directly in terms of imitating humans). Some systems can solve very simple restricted versions of these problems, but none can solve them in their full generality.\n\n\n\n", "id": "2376785", "title": "ASR-complete"}
{"url": "https://en.wikipedia.org/wiki?curid=339417", "text": "Symbolic artificial intelligence\n\nSymbolic artificial intelligence is the term for the collection of all methods in artificial intelligence research that are based on high-level \"symbolic\" (human-readable) representations of problems, logic and search. Symbolic AI was the dominant paradigm of AI research from the mid-1950s until the late 1980s.\nJohn Haugeland gave the name GOFAI (\"Good Old-Fashioned Artificial Intelligence\") to symbolic AI in his 1985 book \"Artificial Intelligence: The Very Idea\", which explored the philosophical implications of artificial intelligence research. In robotics the analogous term is GOFR (\"Good Old-Fashioned Robotics\").\nThe approach is based on the assumption that many aspects of intelligence can be achieved by the manipulation of symbols, an assumption defined as the \"physical symbol systems hypothesis\" by Allen Newell and Herbert A. Simon in the middle 1960s.\nThe most successful form of symbolic AI is expert systems, which use a network of production rules. Production rules connect symbols in a relationship similar to an If-Then statement. The expert system processes the rules to make deductions and to determine what additional information it needs, i.e. what questions to ask, using human-readable symbols.\nOpponents of the symbolic approach include roboticists such as Rodney Brooks, who aims to produce autonomous robots without symbolic representation (or with only minimal representation) and computational intelligence researchers, who apply techniques such as neural networks and optimization to solve problems in machine learning and control engineering.\nSymbolic AI was intended to produce general, human-like intelligence in a machine, whereas most modern research is directed at specific sub-problems. Research into general intelligence is now studied in the sub-field of artificial general intelligence.\nMachines were initially designed to formulate outputs based on the inputs that were represented by symbols. Symbols are used when the input is definite and falls under certainty. But when there is uncertainty involved, for example in formulating predictions, the representation is done using \"fuzzy logic\". This can be seen in artificial neural networks.\n\n", "id": "339417", "title": "Symbolic artificial intelligence"}
{"url": "https://en.wikipedia.org/wiki?curid=2934910", "text": "Cognitive robotics\n\nCognitive robotics is concerned with endowing a robot with intelligent behavior by providing it with a processing architecture that will allow it to learn and reason about how to behave in response to complex goals in a complex world. Cognitive robotics may be considered the engineering branch of embodied cognitive science and embodied embedded cognition.\n\nWhile traditional cognitive modeling approaches have assumed symbolic coding schemes as a means for depicting the world, translating the world into these kinds of symbolic representations has proven to be problematic if not untenable. Perception and action and the notion of symbolic representation are therefore core issues to be addressed in cognitive robotics.\n\nCognitive robotics views animal cognition as a starting point for the development of robotic information processing, as opposed to more traditional Artificial Intelligence techniques. Target robotic cognitive capabilities include perception processing, attention allocation, anticipation, planning, complex motor coordination, reasoning about other agents and perhaps even about their own mental states. Robotic cognition embodies the behavior of intelligent agents in the physical world (or a virtual world, in the case of simulated cognitive robotics). Ultimately the robot must be able to act in the real world.\n\nA preliminary robot learning technique called motor babbling involves correlating pseudo-random complex motor movements by the robot with resulting visual and/or auditory feedback such that the robot may begin to \"expect\" a pattern of sensory feedback given a pattern of motor output. Desired sensory feedback may then be used to inform a motor control signal. This is thought to be analogous to how a baby learns to reach for objects or learns to produce speech sounds. For simpler robot systems, where for instance inverse kinematics may feasibly be used to transform anticipated feedback (desired motor result) into motor output, this step may be skipped.\n\nOnce a robot can coordinate its motors to produce a desired result, the technique of \"learning by imitation\" may be used. The robot monitors the performance of another agent and then the robot tries to imitate that agent. It is often a challenge to transform imitation information from a complex scene into a desired motor result for the robot. Note that imitation is a high-level form of cognitive behavior and imitation is not necessarily required in a basic model of embodied animal cognition.\n\nA more complex learning approach is \"autonomous knowledge acquisition\": the robot is left to explore the environment on its own. A system of goals and beliefs is typically assumed.\n\nA somewhat more directed mode of exploration can be achieved by \"curiosity\" algorithms, such as Intelligent Adaptive Curiosity or Category-Based Intrinsic Motivation. These algorithms generally involve breaking sensory input into a finite number of categories and assigning some sort of prediction system (such as an Artificial Neural Network) to each. The prediction system keeps track of the error in its predictions over time. Reduction in prediction error is considered learning. The robot then preferentially explores categories in which it is learning (or reducing prediction error) the fastest.\n\nSome researchers in cognitive robotics have tried using architectures such as (ACT-R and Soar (cognitive architecture)) as a basis of their cognitive robotics programs. These highly modular symbol-processing architectures have been used to simulate operator performance and human performance when modeling simplistic and symbolized laboratory data. The idea is to extend these architectures to handle real-world sensory input as that input continuously unfolds through time. What is needed is a way to somehow translate the world into a set of symbols and their relationships.\n\nSome of the fundamental questions to still be answered in cognitive robotics are:\n\nCognitive Robotics book by Hooman Samani, takes a multidiciplinary approach to cover various aspects of cognitive robotics such as artificial intelligence, physical, chemical, philosophical, psychological, social, cultural, and ethical aspects.\n\n\n\n", "id": "2934910", "title": "Cognitive robotics"}
{"url": "https://en.wikipedia.org/wiki?curid=4832828", "text": "Extremal optimization\n\nExtremal optimization (EO) is an optimization heuristic inspired by the Bak–Sneppen model of self-organized criticality from the field of statistical physics. This heuristic was designed initially to address combinatorial optimization problems such as the travelling salesman problem and spin glasses, although the technique has been demonstrated to function in optimization domains.\n\nSelf-organized criticality (SOC) is a statistical physics concept to describe a class of dynamical systems that have a critical point as an attractor. Specifically, these are non-equilibrium systems that evolve through avalanches of change and dissipations that reach up to the highest scales of the system. SOC is said to govern the dynamics behind some natural systems that have these burst-like phenomena including landscape formation, earthquakes, evolution, and the granular dynamics of rice and sand piles. Of special interest here is the Bak–Sneppen model of SOC, which is able to describe evolution via punctuated equilibrium (extinction events) – thus modelling evolution as a self-organised critical process.\n\nAnother piece in the puzzle is work on computational complexity, specifically that critical points have been shown to exist in NP-complete problems, where near-optimum solutions are widely dispersed and separated by barriers in the search space causing local search algorithms to get stuck or severely hampered. It was the evolutionary self-organised criticality model by Bak and Sneppen and the observation of critical points in combinatorial optimisation problems that lead to the development of Extremal Optimization by Stefan Boettcher and Allon Percus.\n\nEO was designed as a local search algorithm for combinatorial optimization problems. Unlike genetic algorithms, which work with a population of candidate solutions, EO evolves a single solution and makes local modifications to the worst components. This requires that a suitable representation be selected which permits individual solution components to be assigned a quality measure (\"fitness\"). This differs from holistic approaches such as ant colony optimization and evolutionary computation that assign equal-fitness to all components of a solution based upon their collective evaluation against an objective function. The algorithm is initialized with an initial solution, which can be constructed randomly, or derived from another search process.\n\nThe technique is a fine-grained search, and superficially resembles a hill climbing (local search) technique. A more detailed examination reveals some interesting principles, which may have applicability and even some similarity to broader population-based approaches (evolutionary computation and artificial immune system). The governing principle behind this algorithm is that of improvement through selectively removing low-quality components and replacing them with a randomly selected component. This is obviously at odds with genetic algorithms, the quintessential evolutionary computation algorithm that selects good solutions in an attempt to make better solutions. \n\nThe resulting dynamics of this simple principle is firstly a robust hill climbing search behaviour, and secondly a diversity mechanism that resembles that of multiple-restart search. Graphing holistic solution quality over time (algorithm iterations) shows periods of improvement followed by quality crashes (avalanche) very much in the manner as described by punctuated equilibrium. It is these crashes or dramatic jumps in the search space that permit the algorithm to escape local optima and differentiate this approach from other local search procedures. Although such punctuated-equilibrium behaviour can be \"designed\" or \"hard-coded\", it should be stressed that this is an \"emergent\" effect of the negative-component-selection principle fundamental to the algorithm. \n\nEO has primarily been applied to combinatorial problems such as graph partitioning and the travelling salesman problem, as well as problems from statistical physics such as spin glasses.\n\nGeneralised extremal optimization (GEO) was developed to operate on bit strings where component quality is determined by the absolute rate of change of the bit, or the bits contribution to holistic solution quality. This work includes application to standard function optimisation problems as well as engineering problem domains. Another similar extension to EO is Continuous Extremal Optimization (CEO).\n\nEO has been applied to image rasterization as well as used as a local search after using ant colony optimization. EO has been used to identify structures in complex networks. EO has been used on a multiple target tracking problem. Finally, some work has been done on investigating the probability distribution used to control selection.\n\n\n\n", "id": "4832828", "title": "Extremal optimization"}
{"url": "https://en.wikipedia.org/wiki?curid=1355398", "text": "Blackboard system\n\nA blackboard system is an artificial intelligence approach based on the blackboard architectural model, where a common knowledge base, the \"blackboard\", is iteratively updated by a diverse group of specialist knowledge sources, starting with a problem specification and ending with a solution. Each knowledge source updates the blackboard with a partial solution when its internal constraints match the blackboard state. In this way, the specialists work together to solve the problem. The blackboard model was originally designed as a way to handle complex, ill-defined problems, where the solution is the sum of its parts.\n\nThe following scenario provides a simple metaphor that gives some insight into how a blackboard functions:\nA group of specialists are seated in a room with a large blackboard. They work as a team to brainstorm a solution to a problem, using the blackboard as the workplace for cooperatively developing the solution.\n\nThe session begins when the problem specifications are written onto the blackboard. The specialists all watch the blackboard, looking for an opportunity to apply their expertise to the developing solution. When someone writes something on the blackboard that allows another specialist to apply their expertise, the second specialist records their contribution on the blackboard, hopefully enabling other specialists to then apply their expertise. This process of adding contributions to the blackboard continues until the problem has been solved.\n\nA blackboard-system application consists of three major components\n\n\nFamous examples of early academic blackboard systems are the Hearsay II speech recognition system and Douglas Hofstadter's Copycat and Numbo projects.\n\nMore recent examples include deployed real-world applications, such as the PLAN component of the Mission Control System for RADARSAT-1, an Earth observation satellite developed by Canada to monitor environmental changes and Earth's natural resources.\n\nGTXImage CAD software by GTX Corporation was developed in the early 1990s using a set of rulebases and neural networks as specialists operating on a blackboard system.\n\nAdobe Acrobat Capture (now discontinued) used a Blackboard system to decompose and recognize image pages to understand the objects, text, and fonts on the page. This function is currently built into the retail version of Adobe Acrobat as \"OCR Text Recognition\". Details of a similar OCR blackboard for Farsi text are in the public domain.\n\nBlackboard systems are used routinely in many military C4ISTAR systems for detecting and tracking objects.\n\nBlackboard systems were popular before the AI Winter and, along with most symbolic AI models, fell out of fashion during that period. Along with other models it was realised that initial successes on toy problems did not scale well to real problems on the available computers of the time. Most problems using blackboards are inherently NP-hard, so resist tractable solution by any algorithm in the large size limit. During the same period, statistical pattern recognition became dominant, most notably via simple Hidden Markov Models outperforming symbolic approaches such as Hearsay-II in the domain of speech recognition.\n\nBlackboard-like systems have been constructed within modern Bayesian machine learning settings, using agents to add and remove Bayesian network nodes. In these 'Bayesian Blackboard' systems, the heuristics can acquire more rigorous probabilistic meanings as proposal and acceptances in Metropolis Hastings sampling though the space of possible structures. Conversely, using these mappings, existing Metropolis-Hastings samplers over structural spaces may now thus be viewed as forms of blackboard systems even when not named as such by the authors. Such samplers are commonly found in musical transcription algorithms for example.\n\n\n\n", "id": "1355398", "title": "Blackboard system"}
{"url": "https://en.wikipedia.org/wiki?curid=5033373", "text": "Action selection\n\nAction selection is a way of characterizing the most basic problem of intelligent systems: what to do next. In artificial intelligence and computational cognitive science, \"the action selection problem\" is typically associated with intelligent agents and animats—artificial systems that exhibit complex behaviour in an agent environment. The term is also sometimes used in ethology or animal behavior.\n\nOne problem for understanding action selection is determining the level of abstraction used for specifying an \"act\". At the most basic level of abstraction, an atomic act could be anything from \"contracting a muscle cell\" to \"provoking a war\". Typically for any one action-selection mechanism, the set of possible actions is predefined and fixed.\n\nMost researchers working in this field place high demands on their agents: \n\nFor these reasons action selection is not trivial and attracts a good deal of research.\n\nThe main problem for action selection is complexity. Since all computation takes both time and space (in memory), agents cannot possibly consider every option available to them at every instant in time. Consequently, they must be biased, and constrain their search in some way. For AI, the question of action selection is \"what is the best way to constrain this search\"? For biology and ethology, the question is \"how do various types of animals constrain their search? Do all animals use the same approaches? Why do they use the ones they do?\"\n\nOne fundamental question about action selection is whether it is really a problem at all for an agent, or whether it is just a description of an emergent property of an intelligent agent's behavior. However, if we consider how we are going to build an intelligent agent, then it becomes apparent there must be \"some\" mechanism for action selection. This mechanism may be highly distributed (as in the case of distributed organisms such as social insect colonies or slime mold) or it may be a special-purpose module.\n\nThe action selection mechanism (ASM) determines not only the agent’s actions in terms of impact on the world, but also directs its perceptual attention, and updates its memory. These egocentric sorts of actions may in turn result in modifying the agents basic behavioural capacities, particularly in that updating memory implies some form of machine learning is possible. Ideally, action selection itself should also be able to learn and adapt, but there are many problems of combinatorial complexity and computational tractability that may require restricting the search space for learning.\n\nIn AI, an ASM is also sometimes either referred to as an agent architecture or thought of as a substantial part of one.\n\nGenerally, artificial action selection mechanisms can be divided into several categories: symbol-based systems sometimes known as classical planning, distributed solutions, and reactive or dynamic planning. Some approaches do not fall neatly into any one of these categories. Others are really more about providing scientific models than practical AI control; these last are described further in the next section.\n\nEarly in the history of artificial intelligence, it was assumed that the best way for an agent to choose what to do next would be to compute a probably optimal plan, and then execute that plan. This led to the physical symbol system hypothesis, that a physical agent that can manipulate symbols is necessary and sufficient for intelligence. Many software agents still use this approach for action selection. It normally requires describing all sensor readings, the world, all of ones actions and all of one's goals in some form of predicate logic. Critics of this approach complain that it is too slow for real-time planning and that, despite the proofs, it is still unlikely to produce optimal plans because reducing descriptions of reality to logic is a process prone to errors.\n\nSatisficing is a decision-making strategy which attempts to meet criteria for adequacy, rather than identify an optimal solution. A satisficing strategy may often, in fact, be (near) optimal if the costs of the decision-making process itself, such as the cost of obtaining complete information, are considered in the outcome calculus.\n\n\nIn contrast to the symbolic approach, distributed systems of action selection actually have no one \"box\" in the agent which decides the next action. At least in their idealized form, distributed systems have many modules running in parallel and determining the best action based on local expertise. In these idealized systems, overall coherence is expected to emerge somehow, possibly through careful design of the interacting components. This approach is often inspired by neural networks research. In practice, there is almost always \"some\" centralised system determining which module is \"the most active\" or has the most salience. There is evidence real biological brains also have such executive decision systems which evaluate which of the competing systems deserves the most attention, or more properly, has its desired actions disinhibited.\n\nBecause purely distributed systems are difficult to construct, many researchers have turned to using explicit hard-coded plans to determine the priorities of their system.\n\nDynamic or reactive planning methods compute just one next action in every instant based on the current context and pre-scripted plans. In contrast to classical planning methods, reactive or dynamic approaches do not suffer combinatorial explosion. On the other hand, they are sometimes seen as too rigid to be considered strong AI, since the plans are coded in advance. At the same time, natural intelligence can be rigid in some contexts although it is fluid and able to adapt in others.\n\nExample dynamic planning mechanisms include:\n\nSometimes to attempt to address the perceived inflexibility of dynamic planning, hybrid techniques are used. In these, a more conventional AI planning system searches for new plans when the agent has spare time, and updates the dynamic plan library when it finds good solutions. The important aspect of any such system is that when the agent needs to select an action, some solution exists that can be used immediately (see further anytime algorithm).\n\n\nMany dynamic models of artificial action selection were originally inspired by research in ethology. In particular, Konrad Lorenz and Nikolaas Tinbergen provided the idea of an innate releasing mechanism to explain instinctive behaviors (fixed action patterns). Influenced by the ideas of William McDougall, Lorenz developed this into a \"psychohydraulic\" model of the motivation of behavior. In ethology, these ideas were influential in the 1960s, but they are now regarded as outdated because of their use of an energy flow metaphor; the nervous system and the control of behavior are now normally treated as involving information transmission rather than energy flow. Dynamic plans and neural networks are more similar to information transmission, while spreading activation is more similar to the diffuse control of emotional / hormonal systems.\n\nStan Franklin has proposed that action selection is the right perspective to take in understanding the role and evolution of mind. See his page on the action selection paradigm.\n\nSome researchers create elaborate models of neural action selection. See for example:\n\n\n\n", "id": "5033373", "title": "Action selection"}
{"url": "https://en.wikipedia.org/wiki?curid=741104", "text": "Intelligent control\n\nIntelligent control is a class of control techniques that use various artificial intelligence computing approaches like neural networks, Bayesian probability, fuzzy logic, machine learning, evolutionary computation and genetic algorithms.\n\nIntelligent control can be divided into the following major sub-domains:\n\nNew control techniques are created continuously as new models of intelligent behavior are created and computational methods developed to support them.\n\nNeural networks have been used to solve problems in almost all spheres of science and technology. Neural network control basically involves two steps:\n\n\nIt has been shown that a feedforward network with nonlinear, continuous and differentiable activation functions have universal approximation capability. Recurrent networks have also been used for system identification. Given, a set of input-output data pairs, system identification aims to form a mapping among these data pairs. Such a network is supposed to capture the dynamics of a system.\n\nBayesian probability has produced a number of algorithms that are in common use in many advanced control systems, serving as state space estimators of some variables that are used in the controller.\n\nThe Kalman filter and the Particle filter are two examples of popular Bayesian control components. The Bayesian approach to controller design often requires an important effort in deriving the so-called system model and measurement model, which are the mathematical relationships linking the state variables to the sensor measurements available in the controlled system. In this respect, it is very closely linked to the\nsystem-theoretic approach to control design.\n\n\n\n", "id": "741104", "title": "Intelligent control"}
{"url": "https://en.wikipedia.org/wiki?curid=657145", "text": "Self-management (computer science)\n\nSelf-Management is the process by which computer systems shall manage their own operation without human intervention. Self-Management technologies are expected to pervade the next generation of network management systems.\n\nThe growing complexity of modern networked computer systems is currently the biggest limiting factor in their expansion. The increasing heterogeneity of big corporate computer systems, the inclusion of mobile computing devices, and the combination of different networking technologies like WLAN, cellular phone networks, and mobile ad hoc networks make the conventional, manual management very difficult, time-consuming, and error-prone. More recently self-management has been suggested as a solution to increasing complexity in cloud computing.\n\nCurrently, the most important industrial initiative towards realizing self-management is the Autonomic Computing Initiative (ACI) started by IBM in 2001. The ACI defines the following four functional areas:\n\n\nThe design complexity of Autonomic Systems and self-management systems can be simplified by utilizing design patterns such as the Model View Controller (MVC) to improve separation of concerns by helping encapsulate functional concerns.\n\n\n", "id": "657145", "title": "Self-management (computer science)"}
{"url": "https://en.wikipedia.org/wiki?curid=5837233", "text": "Diagnosis (artificial intelligence)\n\nAs a subfield in artificial intelligence, Diagnosis is concerned with the development of algorithms and techniques that are able to determine whether the behaviour of a system is correct. If the system is not functioning correctly, the algorithm should be able to determine, as accurately as possible, which part of the system is failing, and which kind of fault it is facing. The computation is based on \"observations\", which provide information on the current behaviour.\n\nThe expression \"diagnosis\" also refers to the answer of the question of whether the system is malfunctioning or not, and to the process of computing the answer. This word comes from the medical context where a diagnosis is the process of identifying a disease by its symptoms.\n\nAn example of diagnosis is the process of a garage mechanic with an automobile. The mechanic will first try to detect any abnormal behavior based on the observations on the car and his knowledge of this type of vehicle. If he finds out that the behavior is abnormal, the mechanic will try to refine his diagnosis by using new observations and possibly testing the system, until he discovers the faulty component; the mechanic plays an important role in the vehicle diagnosis.\n\nThe expert diagnosis (or diagnosis by expert system) is based on experience with the system. Using this experience, a mapping is built that efficiently associates the observations to the corresponding diagnoses.\n\nThe experience can be provided:\n\nThe main drawbacks of these methods are:\n\nA slightly different approach is to build an expert system from a model of the system rather than directly from an expertise. An example is the computation of a diagnoser for the diagnosis of discrete event systems. This approach can be seen as model-based, but it benefits from some advantages and suffers some drawbacks of the expert system approach.\n\nModel-based diagnosis is an example of abductive reasoning using a model of the system. In general, it works as follows:\n\nWe have a model that describes the behaviour of the system (or artefact). The model is an abstraction of the behaviour of the system and can be incomplete. In particular, the faulty behaviour is generally little-known, and the faulty model may thus not be represented. Given observations of the system, the diagnosis system simulates the system using the model, and compares the observations actually made to the observations predicted by the simulation.\n\nThe modelling can be simplified by the following rules (where formula_1 is the \"Ab\"normal predicate):\n\nformula_2\n\nformula_3 (fault model)\n\nThe semantics of these formulae is the following: if the behaviour of the system is not abnormal (i.e. if it is normal), then the internal (unobservable) behaviour will be formula_4 and the observable behaviour formula_5. Otherwise, the internal behaviour will be formula_6 and the observable behaviour formula_7. Given the observations formula_8, the problem is to determine whether the system behaviour is normal or not (formula_9 or formula_10). This is an example of abductive reasoning.\n\nA system is said to be diagnosable if whatever the behavior of the system, we will be able to determine without ambiguity a unique diagnosis.\n\nThe problem of diagnosability is very important when designing a system because on one hand one may want to reduce the number of sensors to reduce the cost, and on the other hand one may want to increase the number of sensors to increase the probability of detecting a faulty behavior.\n\nSeveral algorithms for dealing with these problems exist. One class of algorithms answers the question whether a system is diagnosable; another class looks for sets of sensors that make the system diagnosable, and optionally comply to criteria such as cost optimization.\n\nThe diagnosability of a system is generally computed from the model of the system. In applications using model-based diagnosis, such a model is already present and doesn't need to be built from scratch.\n\n\nDX is the annual International Workshop on Principles of Diagnosis that started in 1989. \n\n", "id": "5837233", "title": "Diagnosis (artificial intelligence)"}
{"url": "https://en.wikipedia.org/wiki?curid=6417891", "text": "Constructionist design methodology\n\nConstructionist design methodology (CDM) is an approach for building highly modular systems of many interacting components. CDM's strength lies in simplifying the model of complex, multi functional systems that require architectural evolution of tangled data flow and control hierarchies.\n\nThe constructionist design methodology was developed by artificial intelligence (AI) researcher Kristinn R. Thórisson and his students at Columbia University and Reykjavik University for use in the development of cognitive robotics, communicative humanoids and broad AI systems. The creation of such systems requires integration of a large number of functionalities that must be carefully coordinated to achieve coherent system behavior.\n\nCDM is based on iterative design steps that lead to the creation of a network of named interacting modules, communicating via explicitly typed streams and discrete messages. CDM has been used in the creation of many systems including robotics, facial animation, large-scale simulation and virtual humans. One of the first systems was MIRAGE, a simulated human in an augmented-reality environment that could interact with people through speech and gesture. \n\nDr. Kristinn R. Thórisson is an Icelandic Artificial Intelligence researcher, founder of the Icelandic Institute for Intelligent Machines (IIIM) and co-founder and former co-director of CADIA: Center for Analysis and Design of Intelligent Agents. Thórisson is one of the leading proponents of artificial intelligence systems integration. He is a proponent of Artificial General Intelligence (AGI) and has proposed a new methodology for achieving artificial general intelligence.\n\nThe CDM approach is described in an online semi-tutorial format, and in an article published in A.I. magazine, see Thórisson et al. 2004.\n\n\n", "id": "6417891", "title": "Constructionist design methodology"}
{"url": "https://en.wikipedia.org/wiki?curid=8160211", "text": "Cerebellar model articulation controller\n\nThe cerebellar model arithmetic computer (CMAC) is a type of neural network based on a model of the mammalian cerebellum. It is also known as the cerebellar model articulation controller. It is a type of associative memory.\n\nThe CMAC was first proposed as a function modeler for robotic controllers by James Albus in 1975 (hence the name), but has been extensively used in reinforcement learning and also as for automated classification in the machine learning community. CMAC computes a function formula_1, where formula_2 is the number of input dimensions. The input space is divided up into hyper-rectangles, each of which is associated with a memory cell. The contents of the memory cells are the weights, which are adjusted during training. Usually, more than one quantisation of input space is used, so that any point in input space is associated with a number of hyper-rectangles, and therefore with a number of memory cells. The output of a CMAC is the algebraic sum of the weights in all the memory cells activated by the input point.\n\nA change of value of the input point results in a change in the set of activated hyper-rectangles, and therefore a change in the set of memory cells participating in the CMAC output. The CMAC output is therefore stored in a distributed fashion, such that the output corresponding to any point in input space is derived from the value stored in a number of memory cells (hence the name associative memory). This provides generalisation.\n\nIn the image on the right, there are two inputs to the CMAC, represented as a 2D space. Two quantising functions have been used to divide this space with two overlapping grids (one shown in heavier lines). A single input is shown near the middle, and this has activated two memory cells, corresponding to the shaded area. If another point occurs close to the one shown, it will share some of the same memory cells, providing generalisation.\n\nThe CMAC is trained by presenting pairs of input points and output values, and adjusting the weights in the activated cells by a proportion of the error observed at the output. This simple training algorithm has a proof of convergence.\n\nIt is normal to add a kernel function to the hyper-rectangle, so that points falling towards the edge of a hyper-rectangle have a smaller activation than those falling near the centre.\n\nOne of the major problems cited in practical use of CMAC is the memory size required, which is directly related to the number of cells used. This is usually ameliorated by using a hash function, and only providing memory storage for the actual cells that are activated by inputs.\n\nInitially least mean square (LMS) method is employed to update the weights of CMAC. The convergence of using LMS for training CMAC is sensitive to the learning rate and could lead to divergence. In 2004 , a recursive least squares algorithm was introduced to train CMAC online. It does not need to tune a learning rate. Its convergence has been proved theoretically and can be guaranteed to converge in one step. The computational complexity of this RLS algorithm is O(N3).\n\nBased on QR decomposition, an algorithm (QRLS) has been further simplified to have an O(N) complexity. Consequently, this reduces memory usage and time cost significantly. A parallel pipeline array structure on implementing this algorithm has been introduced. \n\nOverall by utilizing QRLS algorithm, the CMAC neural network convergence can be guaranteed, and the weights of the nodes can be updated using one step of training. Its parallel pipeline array structure offers its great potential to be implemented in hardware for large-scale industry usage.\n\nSince the rectangular shape of CMAC receptive field functions produce discontinuous staircase function approximation, by integrating CMAC with B-splines functions, continuous CMAC offers the capability of obtaining any order of derivatives of the approximate functions.\n\n", "id": "8160211", "title": "Cerebellar model articulation controller"}
{"url": "https://en.wikipedia.org/wiki?curid=2088095", "text": "IJCAI Computers and Thought Award\n\nThe IJCAI Computers and Thought Award is presented every two years by the International Joint Conferences on Artificial Intelligence (IJCAI), recognizing outstanding young scientists in artificial intelligence. It was originally funded with royalties received from the book \"Computers and Thought\" (edited by Edward Feigenbaum and Julian Feldman), and is currently funded by IJCAI.\n\nIt is considered to be \"the premier award for artificial intelligence researchers under the age of 35\".\n\n\n", "id": "2088095", "title": "IJCAI Computers and Thought Award"}
{"url": "https://en.wikipedia.org/wiki?curid=195552", "text": "Artificial consciousness\n\nArtificial consciousness (AC), also known as machine consciousness (MC) or synthetic consciousness (; ), is a field related to artificial intelligence and cognitive robotics. The aim of the theory of artificial consciousness is to \"Define that which would have to be synthesized were consciousness to be found in an engineered artifact\" .\n\nNeuroscience hypothesizes that consciousness is generated by the interoperation of various parts of the brain, called the neural correlates of consciousness or NCC, though there are challenges to that perspective. Proponents of AC believe it is possible to construct systems (e.g., computer systems) that can emulate this NCC interoperation.\n\nArtificial consciousness concepts are also pondered in the philosophy of artificial intelligence through questions about mind, consciousness, and mental states.\n\nAs there are many hypothesized types of consciousness, there are many potential implementations of artificial consciousness. In the philosophical literature, perhaps the most common taxonomy of consciousness is into \"access\" and \"phenomenal\" variants. Access consciousness concerns those aspects of experience that can be apprehended, while phenomenal consciousness concerns those aspects of experience that seemingly cannot be apprehended, instead being characterized qualitatively in terms of “raw feels”, “what it is like” or qualia ().\n\nType-identity theorists and other skeptics hold the view that consciousness can only be realized in particular physical systems because consciousness has properties that necessarily depend on physical constitution (; ).\n\nIn his article \"Artificial Consciousness: Utopia or Real Possibility\" Giorgio Buttazzo says that despite our current technology's ability to simulate autonomy, \"Working in a fully automated mode, they [the computers] cannot exhibit creativity, emotions, or free will. A computer, like a washing machine, is a slave operated by its components.\"\n\nFor other theorists (e.g., functionalists), who define mental states in terms of causal roles, any system that can instantiate the same pattern of causal roles, regardless of physical constitution, will instantiate the same mental states, including consciousness ().\n\nOne of the most explicit arguments for the plausibility of AC comes from David Chalmers. His proposal, found within his article , is roughly that the right kinds of computations are sufficient for the possession of a conscious mind. In the outline, he defends his claim thus: Computers perform computations. Computations can capture other systems' abstract causal organization.\n\nThe most controversial part of Chalmers' proposal is that mental properties are \"organizationally invariant\". Mental properties are of two kinds, psychological and phenomenological. Psychological properties, such as belief and perception, are those that are \"characterized by their causal role\". He adverts to the work of and in claiming that \"[s]ystems with the same causal topology…will share their psychological properties\".\n\nPhenomenological properties are not prima facie definable in terms of their causal roles. Establishing that phenomenological properties are amenable to individuation by causal role therefore requires argument. Chalmers provides his Dancing Qualia Argument for this purpose.\n\nChalmers begins by assuming that agents with identical causal organizations could have different experiences. He then asks us to conceive of changing one agent into the other by the replacement of parts (neural parts replaced by silicon, say) while preserving its causal organization. Ex hypothesi, the experience of the agent under transformation would change (as the parts were replaced), but there would be no change in causal topology and therefore no means whereby the agent could \"notice\" the shift in experience.\n\nCritics of AC object that Chalmers begs the question in assuming that all mental properties and external connections are sufficiently captured by abstract causal organization.\n\nIf it were suspected that a particular machine was conscious, its rights would be an ethical issue that would need to be assessed (e.g. what rights it would have under law). For example, a conscious computer that was owned and used as a tool or central computer of a building or large machine is a particular ambiguity. Should laws be made for such a case, consciousness would also require a legal definition (for example a machine's ability to experience pleasure or pain, known as sentience). Because artificial consciousness is still largely a theoretical subject, such ethics have not been discussed or developed to a great extent, though it has often been a theme in fiction (see below).\n\nThe rules for the 2003 Loebner Prize competition explicitly addressed the question of robot rights:\n61. If, in any given year, a publicly available open source Entry entered by the University of Surrey or the Cambridge Center wins the Silver Medal or the Gold Medal, then the Medal and the Cash Award will be awarded to the body responsible for the development of that Entry. If no such body can be identified, or if there is disagreement among two or more claimants, the Medal and the Cash Award will be held in trust \"until such time as the Entry may legally possess, either in the United States of America or in the venue of the contest, the Cash Award and Gold Medal in its own right\".\n\nThere are various aspects of consciousness generally deemed necessary for a machine to be artificially conscious. A variety of functions in which consciousness plays a role were suggested by Bernard Baars () and others. The functions of consciousness suggested by Bernard Baars are Definition and Context Setting, Adaptation and Learning, Editing, Flagging and Debugging, Recruiting and Control, Prioritizing and Access-Control, Decision-making or Executive Function, Analogy-forming Function, Metacognitive and Self-monitoring Function, and Autoprogramming and Self-maintenance Function. Igor Aleksander suggested 12 principles for artificial consciousness () and these are: The Brain is a State Machine, Inner Neuron Partitioning, Conscious and Unconscious States, Perceptual Learning and Memory, Prediction, The Awareness of Self, Representation of Meaning, Learning Utterances, Learning Language, Will, Instinct, and Emotion. The aim of AC is to define whether and how these and other aspects of consciousness can be synthesized in an engineered artifact such as a digital computer. This list is not exhaustive; there are many others not covered.\n\nAwareness could be one required aspect, but there are many problems with the exact definition of \"awareness\". The results of the experiments of neuroscanning on monkeys suggest that a process, not only a state or object, activates neurons. Awareness includes creating and testing alternative models of each process based on the information received through the senses or imagined, and is also useful for making predictions. Such modeling needs a lot of flexibility. Creating such a model includes modeling of the physical world, modeling of one's own internal states and processes, and modeling of other conscious entities.\n\nThere are at least three types of awareness: agency awareness, goal awareness, and sensorimotor awareness, which may also be conscious or not. For example, in agency awareness you may be aware that you performed a certain action yesterday, but are not now conscious of it. In goal awareness you may be aware that you must search for a lost object, but are not now conscious of it. In sensorimotor awareness, you may be aware that your hand is resting on an object, but are not now conscious of it.\n\nBecause objects of awareness are often conscious, the distinction between awareness and consciousness is frequently blurred or they are used as synonyms.\n\nConscious events interact with memory systems in learning, rehearsal, and retrieval.\nThe IDA model elucidates the role of consciousness in the updating of perceptual memory, transient episodic memory, and procedural memory. Transient episodic and declarative memories have distributed representations in IDA, there is evidence that this is also the case in the nervous system. In IDA, these two memories are implemented computationally using a modified version of Kanerva’s Sparse distributed memory architecture.\n\nLearning is also considered necessary for AC. By Bernard Baars, conscious experience is needed to represent and adapt to novel and significant events (). By Axel Cleeremans and Luis Jiménez, learning is defined as \"a set of philogenetically advanced adaptation processes that critically depend on an evolved sensitivity to subjective experience so as to enable agents to afford flexible control over their actions in complex, unpredictable environments\" ().\n\nThe ability to predict (or anticipate) foreseeable events is considered important for AC by Igor Aleksander. The emergentist multiple drafts principle proposed by Daniel Dennett in \"Consciousness Explained\" may be useful for prediction: it involves the evaluation and selection of the most appropriate \"draft\" to fit the current environment. Anticipation includes prediction of consequences of one's own proposed actions and prediction of consequences of probable actions by other entities.\n\nRelationships between real world states are mirrored in the state structure of a conscious organism enabling the organism to predict events. An artificially conscious machine should be able to anticipate events correctly in order to be ready to respond to them when they occur or to take preemptive action to avert anticipated events. The implication here is that the machine needs flexible, real-time components that build spatial, dynamic, statistical, functional, and cause-effect models of the real world and predicted worlds, making it possible to demonstrate that it possesses artificial consciousness in the present and future and not only in the past. In order to do this, a conscious machine should make coherent predictions and contingency plans, not only in worlds with fixed rules like a chess board, but also for novel environments that may change, to be executed only when appropriate to simulate and control the real world.\n\nSubjective experiences or qualia are widely considered to be \"the\" hard problem of consciousness. Indeed, it is held to pose a challenge to physicalism, let alone computationalism. On the other hand, there are problems in other fields of science which limit that which we can observe, such as the uncertainty principle in physics, which have not made the research in these fields of science impossible.\n\nThe term \"cognitive architecture\" may refer to a theory about the structure of the human mind, or any portion or function thereof, including consciousness. In another context, a cognitive architecture implements the theory on computers. An example is \"QuBIC: Quantum and Bio-inspired Cognitive Architecture for Machine Consciousness\". One of the main goals of a cognitive architecture is to summarize the various results of cognitive psychology in a comprehensive computer model. However, the results need to be in a formalized form so they can be the basis of a computer program. Also, the role of cognitive architecture is for the A.I. to clearly structure, build, and implement it's thought process.\n\nStan Franklin (1995, 2003) defines an autonomous agent as possessing functional consciousness when it is capable of several of the functions of consciousness as identified by Bernard Baars' Global Workspace Theory . His brain child IDA (Intelligent Distribution Agent) is a software implementation of GWT, which makes it functionally conscious by definition. IDA's task is to negotiate new assignments for sailors in the US Navy after they end a tour of duty, by matching each individual's skills and preferences with the Navy's needs. IDA interacts with Navy databases and communicates with the sailors via natural language e-mail dialog while obeying a large set of Navy policies. The IDA computational model was developed during 1996–2001 at Stan Franklin's \"Conscious\" Software Research Group at the University of Memphis. It \"consists of approximately a quarter-million lines of Java code, and almost completely consumes the resources of a 2001 high-end workstation.\" It relies heavily on \"codelets\", which are \"special purpose, relatively independent, mini-agent[s] typically implemented as a small piece of code running as a separate thread.\" In IDA's top-down architecture, high-level cognitive functions are explicitly modeled (see and for details). While IDA is functionally conscious by definition, Franklin does \"not attribute phenomenal consciousness to his own 'conscious' software agent, IDA, in spite of her many human-like behaviours. This in spite of watching several US Navy detailers repeatedly nodding their heads saying 'Yes, that's how I do it' while watching IDA's internal and external actions as she performs her task.\"\n\nCLARION posits a two-level representation that explains the distinction between conscious and unconscious mental processes.\n\nCLARION has been successful in accounting for a variety of psychological data. A number of well-known skill learning tasks have been simulated using CLARION that span the spectrum ranging from simple reactive skills to complex cognitive skills. The tasks include serial reaction time (SRT) tasks, artificial grammar learning (AGL) tasks, process control (PC) tasks, the categorical inference (CI) task, the alphabetical arithmetic (AA) task, and the Tower of Hanoi (TOH) task . Among them, SRT, AGL, and PC are typical implicit learning tasks, very much relevant to the issue of consciousness as they operationalized the notion of consciousness in the context of psychological experiments.\n\nBen Goertzel is pursuing an embodied AGI through the open-source OpenCog project. Current code includes embodied virtual pets capable of learning simple English-language commands, as well as integration with real-world robotics, being done at the Hong Kong Polytechnic University.\n\nPentti considers classical rule-based computing inadequate for achieving AC: \"the brain is definitely not a computer. Thinking is not an execution of programmed strings of commands. The brain is not a numerical calculator either. We do not think by numbers.\" Rather than trying to achieve mind and consciousness by identifying and implementing their underlying computational rules, Haikonen proposes \"a special cognitive architecture to reproduce the processes of perception, inner imagery, inner speech, pain, pleasure, emotions and the cognitive functions behind these. This bottom-up architecture would produce higher-level functions by the power of the elementary processing units, the artificial neurons, without algorithms or programs\". Haikonen believes that, when implemented with sufficient complexity, this architecture will develop consciousness, which he considers to be \"a style and way of operation, characterized by distributed signal representation, perception process, cross-modality reporting and availability for retrospection.\" Haikonen is not alone in this process view of consciousness, or the view that AC will spontaneously emerge in autonomous agents that have a suitable neuro-inspired architecture of complexity; these are shared by many, e.g. and . A low-complexity implementation of the architecture proposed by was reportedly not capable of AC, but did exhibit emotions as expected. See for a comprehensive introduction to Haikonen's cognitive architecture. An updated account of Haikonen's architecture, along with a summary of his philosophical views, is given in .\n\nMurray Shanahan describes a cognitive architecture that combines Baars's idea of a global workspace with a mechanism for internal simulation (\"imagination\") . For discussions of Shanahan's architecture, see and and Chapter 20 of .\n\nSelf-awareness in robots is being investigated by Junichi Takeno at Meiji University in Japan. Takeno is asserting that he has developed a robot capable of discriminating between a self-image in a mirror and any other having an identical image to it, and this claim has already been reviewed . Takeno asserts that he first contrived the computational module called a MoNAD, which has a self-aware function, and he then constructed the artificial consciousness system by formulating the relationships between emotions, feelings and reason by connecting the modules in a hierarchy (Igarashi, Takeno 2007). Takeno completed a mirror image cognition experiment using a robot equipped with the MoNAD system. Takeno proposed the Self-Body Theory stating that \"humans feel that their own mirror image is closer to themselves than an actual part of themselves.\" The most important point in developing artificial consciousness or clarifying human consciousness is the development of a function of self awareness, and he claims that he has demonstrated physical and mathematical evidence for this in his thesis. He also demonstrated that robots can study episodes in memory where the emotions were stimulated and use this experience to take predictive actions to prevent the recurrence of unpleasant emotions (Torigoe, Takeno 2009).\n\nIgor Aleksander, emeritus professor of Neural Systems Engineering at Imperial College, has extensively researched artificial neural networks and claims in his book \"Impossible Minds: My Neurons, My Consciousness\" that the principles for creating a conscious machine already exist but that it would take forty years to train such a machine to understand language. Whether this is true remains to be demonstrated and the basic principle stated in \"Impossible Minds\"—that the brain is a neural state machine—is open to doubt.\n\nStephen Thaler proposed a possible connection between consciousness and creativity in his 1994 patent, called \"Device for the Autonomous Generation of Useful Information\" (DAGUI), or the so-called \"Creativity Machine\", in which computational critics govern the injection of synaptic noise and degradation into neural nets so as to induce false memories or confabulations that may qualify as potential ideas or strategies. He recruits this neural architecture and methodology to account for the subjective feel of consciousness, claiming that similar noise-driven neural assemblies within the brain invent dubious significance to overall cortical activity. Thaler's theory and the resulting patents in machine consciousness were inspired by experiments in which he internally disrupted trained neural nets so as to drive a succession of neural activation patterns that he likened to stream of consciousness.\n\nIn 2011, Michael Graziano and Sabine Kastler published a paper named \"Human consciousness and its relationship to social neuroscience: A novel hypothesis\" proposing a theory of consciousness as an attention schema. Graziano went on to publish an expanded discussion of this theory in his book \"Consciousness and the Social Brain\". This Attention Schema Theory of Consciousness, as he named it, proposes that the brain tracks attention to various sensory inputs by way of an attention schema, analogous to the well study body schema that tracks the spatial place of a person's body. This relates to artificial consciousness by proposing a specific mechanism of information handling, that produces what we allegedly experience and describe as consciousness, and which should be able to be duplicated by a machine using current technology. When the brain finds that person X is aware of thing Y, it is in effect modeling the state in which person X is applying an attentional enhancement to Y. In the attention schema theory, the same process can be applied to oneself. The brain tracks attention to various sensory inputs, and one's own awareness is a schematized model of one's attention. Graziano proposes specific locations in the brain for this process, and suggests that such awareness is a computed feature constructed by an expert system in the brain.\n\nThe most well-known method for testing machine intelligence is the Turing test. But when interpreted as only observational, this test contradicts the philosophy of science principles of theory dependence of observations. It also has been suggested that Alan Turing's recommendation of imitating not a human adult consciousness, but a human child consciousness, should be taken seriously.\n\nOther tests, such as ConsScale, test the presence of features inspired by biological systems, or measure the cognitive development of artificial systems.\n\nQualia, or phenomenological consciousness, is an inherently first-person phenomenon. Although various systems may display various signs of behavior correlated with functional consciousness, there is no conceivable way in which third-person tests can have access to first-person phenomenological features. Because of that, and because there is no empirical definition of consciousness, a test of presence of consciousness in AC may be impossible.\n\nIn 2014, Victor Argonov suggested a non-Turing test for machine consciousness based on machine's ability to produce philosophical judgments. He argues that a deterministic machine must be regarded as conscious if it is able to produce judgments on all problematic properties of consciousness (such as qualia or binding) having no innate (preloaded) philosophical knowledge on these issues, no philosophical discussions while learning, and no informational models of other creatures in its memory (such models may implicitly or explicitly contain knowledge about these creatures’ consciousness). However, this test can be used only to detect, but not refute the existence of consciousness. A positive result proves that machine is conscious but a negative result proves nothing. For example, absence of philosophical judgments may be caused by lack of the machine’s intellect, not by absence of consciousness.\n\nCharacters with artificial consciousness (or at least with personalities that imply they have consciousness), from works of fiction:\n\n\n\n\n\n", "id": "195552", "title": "Artificial consciousness"}
{"url": "https://en.wikipedia.org/wiki?curid=7872324", "text": "Artificial intelligence systems integration\n\nThe core idea of Artificial Intelligence systems integration is making individual software components, such as speech synthesizers, interoperable with other components, such as common sense knowledgebases, in order to create larger, broader and more capable A.I. systems. The main methods that have been proposed for integration are message routing, or communication protocols that the software components use to communicate with each other, often through a middleware blackboard system.\n\nMost artificial intelligence systems involve some sort of integrated technologies, for example the integration of speech synthesis technologies with that of speech recognition. However, in recent years there has been an increasing discussion on the importance of systems integration as a field in its own right. Proponents of this approach are researchers such as Marvin Minsky, Aaron Sloman, Deb Roy, Kristinn R. Thórisson and Michael A. Arbib. A reason for the recent attention A.I. integration is attracting is that there have already been created a number of (relatively) simple A.I. systems for specific problem domains (such as computer vision, speech synthesis, etc.), and that integrating what's already available is a more logical approach to broader A.I. than building monolithic systems from scratch.\n\nThe focus on systems integration, especially with regard to modular approaches, derive from the fact that most intelligences of significant scales are composed of a multitude of processes and/or utilize multi-modal input and output. For example, a humanoid-type of intelligence would preferably have to be able to talk using speech synthesis, hear using speech recognition, understand using a logical (or some other undefined) mechanism, and so forth. In order to produce artificially intelligent software of broader intelligence, integration of these modalities is necessary.\n\nCollaboration is an integral part of software development as evidenced by the size of software companies and the size of their software departments. Among the tools to ease software collaboration are various procedures and standards that developers can follow to ensure quality, reliability and that their software is compatible with software created by others (such as W3C standards for webpage development). However, collaboration in fields of A.I. has been lacking, for the most part not seen outside of the respected schools, departments or research institutes (and sometimes not within them either). This presents practitioners of A.I. systems integration with a substantial problem and often causes A.I. researchers to have to 're-invent the wheel' each time they want a specific functionality to work with their software. Even more damaging is the \"not invented here\" syndrome, which manifests itself in a strong reluctance of A.I. researchers to build on the work of others.\n\nThe outcome of this in A.I. is a large set of \"solution islands\": A.I. research has produced numerous isolated software components and mechanisms that deal with various parts of intelligence separately. To take some examples:\n\n\nWith the increased popularity of the free software movement, a lot of the software being created, including A.I. systems, that is available for public exploit. The next natural step is to merge these individual software components into coherent, intelligent systems of a broader nature. As a multitude of components (that often serve the same purpose) have already been created by the community, the most accessible way of integration is giving each of these components an easy way to communicate with each other. By doing so, each component by itself becomes a module which can then be tried in various settings and configurations of larger architectures.\n\nMany online communities for A.I. developers exist where tutorials, examples and forums aim at helping both beginners and experts build intelligent systems (for example the AI Depot, Generation 5). However, few communities have succeeded in making a certain standard or a code of conduct popular to allow the large collection of miscellaneous systems to be integrated with any ease. Recently, however, there have been focused attempts at producing standards for A.I. research collaboration, Mindmakers.org is an online community specifically created to harbor collaboration in the development of A.I. systems. The community has proposed the OpenAIR message and routing protocol for communication between software components, making it easier for individual developers to make modules instantly integrateble into other peoples' projects.\n\nThe Constructionist design methodology (CDM, or 'Constructionist A.I.') is a formal methodology proposed in 2004, for use in the development of cognitive robotics, communicative humanoids and broad AI systems. The creation of such systems requires integration of a large number of functionalities that must be carefully coordinated to achieve coherent system behavior. CDM is based on iterative design steps that lead to the creation of a network of named interacting modules, communicating via explicitly typed streams and discrete messages. The OpenAIR message protocol (see below) was inspired by the CDM, and has frequently been used to aid in development of intelligent systems using CDM.\n\nOne of the first projects to use CDM was Mirage, an embodied, graphical agent visualized through augmented reality which could communicate with human users and talk about objects present in the user's physical room. Mirage was created by Kristinn R. Thórisson, the creator of CDM, and a number of students at Columbia University in 2004. The methodology is actively being developed at Reykjavik University.\n\nOpenAIR is a message routing and communication protocol that has been gaining in popularity over the past two years. The protocol is managed by Mindmakers.org, and is described on their site in the following manner:\n\n\"\"OpenAIR is a routing and communication protocol based on a publish-subscribe architecture. It is intended to be the \"glue\" that allows numerous A.I. researchers to share code more effectively — \"AIR to share\". It is a definition or a blueprint of the \"post office and mail delivery system\" for distributed, multi-module systems. OpenAIR provides a core foundation upon which subsequent markup languages and semantics can be based, e.g. gesture recognition and generation, computer vision, hardware-software interfacing etc.; for a recent example see CVML\".\"\n\nOpenAIR was created to allow software components that serve their own purpose to communicate with each other to produce large scale, overall behavior of an intelligent systems. A simple example would be to have a speech recognition system, and a speech synthesizer communicate with an expert system through OpenAIR messages, to create a system that can hear and answer various questions through spoken dialogue. CORBA (see below) is an older but similar architecture that can be used for comparison, but OpenAIR was specifically created for A.I. research, while CORBA is a more general standard.\n\nThe OpenAIR protocol has been used for collaboration on a number of A.I. systems, a list can be found on the Mindmakers project pages. Psyclone is a popular platform to pair with the OpenAIR protocol (see below).\n\nPsyclone is a software platform, or an AI operating system (AIOS), developed by Communicative Machines Laboratories for use in creating large, multi modal A.I. systems. The system is an implementation of a blackboard system that supports the OpenAIR message protocol. Psyclone is available for free for non-commercial purposes and has therefore often been used by research institutes on low budgets and novice A.I. developers.\n\nElvin is a content-based router with a central routing station, similar to the Psyclone AIOS (see above).\n\nThe OOA is a hybrid architecture that relies on a special inter-agent communication language (ICL) – a logic-based declarative language which is good for expressing high-level, complex tasks and natural language expressions.\n\nThe Common Object Request Broker Architecture (CORBA) is a standard that enables software components written in multiple computer languages and running on multiple computers to interoperate. CORBA is defined by the Object Management Group (OMG). CORBA follows similar principles as the OpenAIR protocol (see above), and can be used for A.I. systems integration.\n\nThe Messaging Open Service Interface Definition (OSID) is an O.K.I. specification which provides a means of sending, subscribing and receiving messages. OSIDs are programmatic interfaces which comprise a Service Oriented Architecture for designing and building reusable and interoperable software.\n\n\n\n\n", "id": "7872324", "title": "Artificial intelligence systems integration"}
{"url": "https://en.wikipedia.org/wiki?curid=7879921", "text": "OpenAIR\n\nOpenAIR is a message routing and communication protocol for artificial intelligence systems that has been gaining in popularity in recent years (2006) . The protocol is managed by Mindmakers, and is described on their site in the following manner:\n\n\"\"OpenAIR is a routing and communication protocol based on a publish-subscribe architecture. It is intended to be the \"glue\" that allows numerous A.I. researchers to share code more effectively — \"AIR to share\". It is a definition or a blueprint of the \"post office and mail delivery system\" for distributed, multi-module systems. OpenAIR provides a core foundation upon which subsequent markup languages and semantics can be based, for e.g. gesture recognition and generation, computer vision, hardware-software interfacing etc.; for a recent example see CVML\".\" \n\nOpenAIR was created to allow software components that serve their own purpose to communicate with each other in order to produce large scale, overall behavior of an intelligent system. A simple example would be to have a speech recognition system, and a speech synthesizer communicate with an expert system through OpenAIR messages, to create a system that can hear and answer various questions through spoken dialogue.\n\n\n", "id": "7879921", "title": "OpenAIR"}
{"url": "https://en.wikipedia.org/wiki?curid=9460224", "text": "Zeuthen strategy\n\nThe Zeuthen strategy is a negotiation strategy used by some artificial agents. Its purpose is to measure the \"willingness to risk conflict\". An agent will be more willing to risk conflict if the difference in utility between its current proposal and the conflict deal is low.\n\nWhen used by both agents in the Monotonic Concession Protocol, the Zeuthen strategy leads the agents to agree upon the deal in the negotiation set, the set of all conflict free deals which are individually rational and Pareto optimal plus the conflict deal, which maximizes the Nash product.\n\nThe strategy was introduced in 1930 by the Danish economist Frederik Zeuthen.\n\nRisk(A,t) is a measurement of agent A's willingness to risk conflict. The risk function formalizes the notion that an agent's willingness to risk conflict is the ratio of the utility that agent would lose by accepting the other agent's proposal to the utility that agent would lose by causing a conflict. Agent A is said to be using a rational negotiation strategy if at any step \"t\" + 1 that agent A sticks to his last proposal, Risk(A,t) > Risk(B,t).\n\nIf agent A makes a sufficient concession in the next step, then, assuming that agent B is using a rational negotiation strategy, if agent B does not concede in the next step, he must do so in the step after that. The set of all sufficient concessions of agent A at step t is denoted SC(\"A\", \"t\").\n\nis the minimal sufficient concession of agent A in step t.\n\nAgent A begins the negotiation by proposing\n\nand will make the minimal sufficient concession in step t+1 if and only if Risk(A,t) ≤ Risk(B,t).\n\nTheorem\nIf both agents are using Zeuthen strategies, then they will agree on\n\nthat is, the deal which maximizes the Nash product. (Harsanyi 56)\n\nProof\nLet δ = δ(A,t).\nLet δ = δ(B,t).\nAccording to the Zeuthen strategy, agent A will concede at step formula_5 if and only if\n\nThat is, if and only if\n\nThus, Agent A will concede if and only if formula_13 does not yield the larger product of utilities.\n\nTherefore, the Zeuthen strategy guarantees a final agreement that maximizes the Nash Product.\n", "id": "9460224", "title": "Zeuthen strategy"}
{"url": "https://en.wikipedia.org/wiki?curid=1456626", "text": "Autonomous agent\n\nAn autonomous agent is an \"intelligent agent\" operating on an owner's behalf but without any interference of that ownership entity. An Intelligent agent, however appears according to a multiply cited statement in a no longer accessible IBM white paper as follows:\n\nIntelligent agents are software entities that carry out some set of operations on behalf of a user or another program with some degree of independence or autonomy, and in so doing, employ some knowledge or representation of the user's goals or desires.\n\nSuch an agent is a system situated in, and part of, a technical or natural environment, which senses any or some status of that environment, and acts on it in pursuit of its own agenda. Such an agenda evolves from drives (or programmed goals). The agent acts to change part of the environment or of its status and influences what it sensed.\n\nNon-biological examples include intelligent agents, autonomous robots, and various software agents, including artificial life agents, and many computer viruses. Biological examples are not yet defined.\n\n\n\n", "id": "1456626", "title": "Autonomous agent"}
{"url": "https://en.wikipedia.org/wiki?curid=8514669", "text": "Intelligent database\n\nUntil the 1980s, databases were viewed as computer systems that stored record-oriented and business data such as manufacturing inventories, bank records, and sales transactions. A database system was not expected to merge numeric data with text, images, or multimedia information, nor was it expected to automatically notice patterns in the data it stored. In the late 1980s the concept of an intelligent database was put forward as a system that manages information (rather than data) in a way that appears natural to users and which goes beyond simple record keeping.\n\nThe term was introduced in 1989 by the book \"Intelligent Databases\" by Kamran Parsaye, Mark Chignell, Setrag Khoshafian and Harry Wong. The concept postulated three levels of intelligence for such systems: high level tools, the user interface and the database engine. The high level tools manage data quality and automatically discover relevant patterns in the data with a process called data mining. This layer often relies on the use of artificial intelligence techniques. The user interface uses hypermedia in a form that uniformly manages text, images and numeric data. The intelligent database engine supports the other two layers, often merging relational database techniques with object orientation.\n\nIn the twenty-first century, intelligent databases have now become widespread, e.g. hospital databases can now call up patient histories consisting of charts, text and x-ray images just with a few mouse clicks, and many corporate databases include decision support tools based on sales pattern analysis.\n\n", "id": "8514669", "title": "Intelligent database"}
{"url": "https://en.wikipedia.org/wiki?curid=1563306", "text": "Computational intelligence\n\nThe expression computational intelligence (CI) usually refers to the ability of a computer to learn a specific task from data or experimental observation. Even though it is commonly considered a synonym of soft computing, there is still no commonly accepted definition of computational intelligence.\n\nGenerally, computational intelligence is a set of nature-inspired computational methodologies and approaches to address complex real-world problems to which mathematical or traditional modelling can be useless for a few reasons: the processes might be too complex for mathematical reasoning, it might contain some uncertainties during the process, or the process might simply be stochastic in nature. Indeed, many real-life problems cannot be translated into binary language (unique values of 0 and 1) for computers to process it. Computational Intelligence therefore provides solutions for such problems.\n\nThe methods used are close to the human's way of reasoning, i.e. it uses inexact and incomplete knowledge, and it is able to produce control actions in an adaptive way. CI therefore uses a combination of five main complementary techniques. The fuzzy logic which enables the computer to understand natural language, artificial neural networks which permits the system to learn experiential data by operating like the biological one, evolutionary computing, which is based on the process of natural selection, learning theory, and probabilistic methods which helps dealing with uncertainty imprecision.\n\nExcept those main principles, currently popular approaches include biologically inspired algorithms such as swarm intelligence and artificial immune systems, which can be seen as a part of evolutionary computation, image processing, data mining, natural language processing, and artificial intelligence, which tends to be confused with Computational Intelligence. But although both Computational Intelligence (CI) and Artificial Intelligence (AI) seek similar goals, there's a clear distinction between them.\n\nComputational Intelligence is thus a way of performing like human beings. Indeed, the characteristic of \"intelligence\" is usually attributed to humans. More recently, many products and items also claim to be \"intelligent\", an attribute which is directly linked to the reasoning and decision making.\n\nThe notion of Computational Intelligence was first used by the IEEE Neural Networks Council in 1990. This Council was founded in the 1980s by a group of researchers interested in the development of biological and artificial neural networks. On November 21, 2001, the IEEE Neural Networks Council became the IEEE Neural Networks Society, to become the IEEE Computational Intelligence Society two years later by including new areas of interest such as fuzzy systems and evolutionary computation, which they related to Computational Intelligence in 2011 (Dote and Ovaska).\n\nBut the first clear definition of Computational Intelligence was introduced by Bezdek in 1994: a system is called computationally intelligent if it deals with low-level data such as numerical data, has a pattern-recognition component and does not use knowledge in the AI sense, and additionally when it begins to exhibit computational adaptively, fault tolerance, speed approaching human-like turnaround and error rates that approximate human performance.\n\nBezdek and Marks (1993) clearly differentiated CI from AI, by arguing that the first one is based on soft computing methods, whereas AI is based on hard computing ones.\n\nAlthough Artificial Intelligence and Computational Intelligence seek a similar long-term goal: reach general intelligence, which is the intelligence of a machine that could perform any intellectual task that a human being can; there's a clear difference between them. According to Bezdek (1994), Computational Intelligence is a subset of Artificial Intelligence.\n\nThere are two types of machine intelligence: the artificial one based on hard computing techniques and the computational one based on soft computing methods, which enable adaptation to many situations.\n\nHard computing techniques work following binary logic based on only two values (the Booleans true or false, 0 or 1) on which modern computers are based. One problem with this logic is that our natural language cannot always be translated easily into absolute terms of 0 and 1. Soft computing techniques, based on fuzzy logic can be useful here. Much closer to the way the human brain works by aggregating data to partial truths (Crisp/fuzzy systems), this logic is one of the main exclusive aspects of CI.\n\nWithin the same principles of fuzzy and binary \"logics\" follow crispy and fuzzy \"systems\". Crisp logic is a part of artificial intelligence principles and consists of either including an element in a set, or not, whereas fuzzy systems (CI) enable elements to be partially in a set. Following this logic, each element can be given a degree of membership (from 0 to 1) and not exclusively one of these 2 values.\n\nThe main applications of Computational Intelligence include computer science, engineering, data analysis and bio-medicine.\n\nAs explained before, fuzzy logic, one of CI's main principles, consists in measurements and process modelling made for real life's complex processes. It can face incompleteness, and most importantly ignorance of data in a process model, contrarily to Artificial Intelligence, which requires exact knowledge.\n\nThis technique tends to apply to a wide range of domains such as control, image processing and decision making. But it is also well introduced in the field of household appliances with washing machines, microwave ovens, etc. We can face it too when using a video camera, where it helps stabilizing the image while holding the camera unsteadily. Other areas such as medical diagnostics, foreign exchange trading and business strategy selection are apart from this principle's numbers of applications.\n\nFuzzy logic is mainly useful for approximate reasoning, and doesn't have learning abilities, a qualification much needed that human beings have. It enables them to improve themselves by learning from their previous mistakes.\n\nThis is why CI experts work on the development of artificial neural networks based on the biological ones, which can be defined by 3 main components: the cell-body which processes the information, the axon, which is a device enabling the signal conducting, and the synapse, which controls signals. Therefore, artificial neural networks are doted of distributed information processing systems, enabling the process and the learning from experiential data. Working like human beings, fault tolerance is also one of the main assets of this principle.\n\nConcerning its applications, neural networks can be classified into five groups: data analysis and classification, associative memory, clustering generation of patterns and control. Generally, this method aims to analyze and classify medical data, proceed to face and fraud detection, and most importantly deal with nonlinearities of a system in order to control it. Furthermore, neural networks techniques share with the fuzzy logic ones the advantage of enabling data clustering.\n\nBased on the process of natural selection firstly introduced by Charles Robert Darwin, the evolutionary computation consists in capitalizing on the strength of natural evolution to bring up new artificial evolutionary methodologies. It also includes other areas such as evolution strategy, and evolutionary algorithms which are seen as problem solvers... This principle's main applications cover areas such as optimization and multi-objective optimization, to which traditional mathematical one techniques aren't enough anymore to apply to a wide range of problems such as DNA Analysis, scheduling problems...\n\nStill looking for a way of \"reasoning\" close to the humans' one, learning theory is one of the main approaches of CI. In psychology, learning is the process of bringing together cognitive, emotional and environmental effects and experiences to acquire, enhance or change knowledge, skills, values and world views (Ormrod, 1995; Illeris, 2004). Learning theories then helps understanding how these effects and experiences are processed, and then helps making predictions based on previous experience.\n\nBeing one of the main elements of fuzzy logic, probabilistic methods firstly introduced by Paul Erdos and Joel Spencer (1974), aim to evaluate the outcomes of a Computation Intelligent system, mostly defined by randomness. Therefore, probabilistic methods bring out the possible solutions to a reasoning problem, based on prior knowledge.\n\n\n", "id": "1563306", "title": "Computational intelligence"}
{"url": "https://en.wikipedia.org/wiki?curid=11053817", "text": "Ontology learning\n\nOntology learning (ontology extraction, ontology generation, or ontology acquisition) is the automatic or semi-automatic creation of ontologies, including extracting the corresponding domain's terms and the relationships between the concepts that these terms represent from a corpus of natural language text, and encoding them with an ontology language for easy retrieval. As building ontologies manually is extremely labor-intensive and time-consuming, there is great motivation to automate the process.\nTypically, the process starts by extracting terms and concepts or noun phrases from plain text using linguistic processors such as part-of-speech tagging and phrase chunking. Then statistical \nor symbolic\n\ntechniques are used to extract relation signatures, often based on pattern-based or definition-based hypernym extraction techniques.\n\nOntology learning (OL) is used to (semi-)automatically extract whole ontologies from natural language text. The process is usually split into the following eight tasks, which are not all necessarily applied in every ontology learning system.\n\n\nDuring the domain terminology extraction step, domain-specific terms are extracted, which are used in the following step (concept discovery) to derive concepts. Relevant terms can be determined e. g. by calculation of the TF/IDF values or by application of the C-value / NC-value method. The resulting list of terms has to be filtered by a domain expert. In the subsequent step, similarly to coreference resolution in information extraction, the OL system determines synonyms, because they share the same meaning and therefore correspond to the same concept. The most common methods therefore are clustering and the application of statistical similarity measures.\n\nIn the concept discovery step, terms are grouped to meaning bearing units, which correspond to an abstraction of the world and therefore to concepts. The grouped terms are these domain-specific terms and their synonyms, which were identified in the domain terminology extraction step.\n\nIn the concept hierarchy derivation step, the OL system tries to arrange the extracted concepts in a taxonomic structure. This is mostly achieved by unsupervised hierarchical clustering methods. Because the result of such methods is often noisy, a supervision, e. g. by evaluation by the user, is integrated. A further method for the derivation of a concept hierarchy exists in the usage of several patterns, which should indicate a sub- or supersumption relationship. Patterns like “X, that is a Y” or “X is a Y” indicate, that X is a subclass of Y. Such pattern can be analyzed efficiently, but they occur too infrequent, to extract enough sub- or supersumption relationships. Instead bootstrapping methods are developed, which learn these patterns automatically and therefore ensure a higher coverage.\n\nAt the learning of non-taxonomic relations step, relationships are extracted, which do not express any sub- or supersumption. Such relationships are e.g. works-for or located-in. There are two common approaches to solve this subtask. The first one is based upon the extraction of anonymous associations, which are named appropriately in a second step. The second approach extracts verbs, which indicate a relationship between the entities, represented by the surrounding words. But the result of both approaches has to be evaluated by an ontologist.\n\nDuring rule discovery, axioms (formal description of concepts) are generated for the extracted concepts. This can be achieved for example by analyzing the syntactic structure of a natural language definition and the application of transformation rules on the resulting dependency tree. The result of this process is a list of axioms, which is afterwards comprehended to a concept description. This one has to be evaluated by an ontologist.\n\nAt the ontology population step, the ontology is augmented with instances of concepts and properties. For the augmentation with instances of concepts methods, which are based on the matching of lexico-syntactic patterns, are used. Instances of properties are added by application of bootstrapping methods, which collect relation tuples.\n\nIn the concept hierarchy extension step, the OL system tries to extend the taxonomic structure of an existing ontology with further concepts. This can be realized supervised by a trained classifier or unsupervised by the application of similarity measures.\n\nDog4Dag - an ontology generation plugin for Protégé 4.1 and OBOEdit.\nDOG4DAG is an ontology generation plugin for both Protégé 4.1 and OBO-Edit 2.1. It allows for term generation, sibling generation, definition generation, and relationship induction. Integrated into Protégé 4.1 and OBO-Edit 2.1, DOG4DAG allows ontology extension for all common ontology formats (e.g., OWL and OBO). Limited largely to EBI and Bio Portal lookup service extensions.\n\nDuring frame/event detection, the OL system tries to extract complex relationships from text, e.g. who departed from where to what place and when. Approaches range from applying SVM with kernel methods to semantic role labeling (SRL) to deep semantic parsing techniques.\n\n\n", "id": "11053817", "title": "Ontology learning"}
{"url": "https://en.wikipedia.org/wiki?curid=11298926", "text": "Nouvelle AI\n\nNouvelle artificial intelligence (AI) is an approach to artificial intelligence pioneered in the 1980s by Rodney Brooks, who was then part of MIT artificial intelligence laboratory. Nouvelle AI differs from classical AI by aiming to produce robots with intelligence levels similar to insects. Researchers believe that intelligence can emerge organically from simple behaviors as these intelligences interacted with the \"real world,\" instead of using the constructed worlds which symbolic AIs typically needed to have programmed into them.\n\nThe differences between nouvelle AI and symbolic AI are apparent in early robots Shakey and Freddy. These robots contained an internal model (or \"representation\") of their micro-worlds consisting of symbolic descriptions. As a result, this structure of symbols had to be renewed as the robot moved or the world changed.\n\nShakey's planning programs assessed the program structure and broke it down into the necessary steps to complete the desired action. This level of computation required a large amount time to process, so Shakey typically performed its tasks very slowly.\n\nSymbolic AI researchers had long been plagued by the problem of updating, searching, and otherwise manipulating the symbolic worlds inside their AIs. A nouvelle system refers continuously to its sensors rather than to an internal model of the world. It processes the external world information it needs from the senses when it is required. As Brooks puts it, \"the world is its own best model--always exactly up to date and complete in every detail.\"\n\nA central idea of nouvelle AI is that simple behaviors combine to form more complex behaviors over time. For example, simple behaviors can include elements like \"move forward\" and \"avoid obstacles.\" A robot using nouvelle AI with simple behaviors like collision avoidance and moving toward a moving object could possibly come together to produce a more complex behavior like chasing a moving object.\n\nThe frame problem describes an issue with using first-order logic (FOL) to express facts about a robot in the world. Representing the state of a robot with traditional FOL requires the use of many axioms (symbolic language) to imply that things about an environment that do not change arbitrarily.\n\nNouvelle AI seeks to sidestep the frame problem by dispensing with filling the AI or robot with volumes of symbolic language and instead letting more complex behaviors emerge by combining simpler behavioral elements.\n\nThe goal of traditional AI was to build intelligences without bodies, which would only have been able to interact with the world via keyboard, screen, or printer. However, nouvelle AI attempts to build embodied intelligence situated in the real world. Brooks quotes approvingly from the brief sketches that Turing gave in 1948 and 1950 of the \"situated\" approach. Turing wrote of equipping a machine \"with the best sense organs that money can buy\" and teaching it \"to understand and speak English\" by a process that would \"follow the normal teaching of a child.\" This approach was contrasted to the others where they focused on abstract activities such as playing chess.\n\nBrooks focused on building robots that acted like simple insects while simultaneously working to remove some traditional AI characteristics. He created insect-like robots called Allen and Herbert.\n\nBrooks's insectoid robots contained no internal models of the world. Herbert, for example, discarded a high volume of the information received from its sensors and never stored information for more than two seconds.\n\nAllen, named after Allen Newell, had a ring of twelve ultrasonic sonars as its primary sensors and three independent behavior-producing modules. These modules were programmed to avoid both stationary and moving objects. With only this module activated, Allen stayed in the middle of a room until an object approached and then it ran away while avoiding obstacles in its way.\n\nHerbert, named after Herbert Simon, used infrared sensors to avoid obstacles and a laser system to collect 3D data over a distance about 12 feet. Herbert also carried a number of simple sensors in its \"hand.\" The robot's testing ground was the real world environment of the busy offices and workspaces of the MIT AI lab where it searched for empty soda cans and carried them away, a seemingly-goal oriented activity that emerged as a result of 15 simple behavior units combining together.\n\nOther robots by Brooks' team were Genghis and Squirt. Genghis had six legs and was able to walk over rough terrain and follow a human. Squirt's behavior modules had it stay in dark corners until it heard a noise, then it would begin to follow the source of the noise.\n\nBrooks agreed that the level of nouvelle AI had come near the complexity of a real insect, which raised a question about whether or not insect level-behavior was and is a reasonable goal for nouvelle AI?\n\nBrooks' own recent work has taken the opposite direction to that proposed by Von Neumann in the quotations \"theorists who select the human nervous system as their model are unrealistically picking 'the most complicated object under the sun,' and that there is little advantage in selecting instead the ant, since any nervous system at all exhibits exceptional complexity.\"\n\nIn the 1990s, Brooks decided to pursue the goal of human-level intelligence and, with Lynn Andrea Stein, built a humanoid robot called Cog. Cog is a robot with an extensive collection of sensors, a face, and arms (among other features) which allow it to interact with the world and gather information and experience so as to assemble intelligence organically in the manner described above by Turing.\n\nThe team believes that Cog will able to learn and able to find a correlation between the sensory information it receives and its actions. In the long term, the team wants Cog to be able to learn common sense knowledge on its own.\n\n\n", "id": "11298926", "title": "Nouvelle AI"}
{"url": "https://en.wikipedia.org/wiki?curid=14760591", "text": "Computational humor\n\nComputational humor is a branch of computational linguistics and artificial intelligence which uses computers in humor research. It is a relatively new area, with the first dedicated conference organized in 1996.\n\nThe first \"computer model of a sense of humor\" was suggested by\nSuslov as early as 1992. Investigation of the general scheme of information processing shows the possibility of a specific malfunction, conditioned by the necessity of a quick deletion from consciousness of a false version. This specific malfunction can be identified with a humorous effect on psychological grounds: it exactly corresponds to incongruity-resolution theory. However, an essentially new ingredient, the role of timing, is added to the well-known role of ambiguity. In biological systems, a sense of humor inevitably develops in the course of evolution, because its biological function consists of quickening the transmission of the processed information into consciousness and in a more effective use of brain resources. A realization of this algorithm in neural networks justifies naturally Spencer's hypothesis on the mechanism of laughter: deletion of a false version corresponds to zeroing of some part of the neural network and excessive energy of neurons is thrown out to the motor cortex, arousing muscular contractions.\n\nA practical realization of this algorithm needs extensive databases, whose creation in the automatic regime was suggested recently. As a result, this magistral direction was not developed properly and subsequent investigations accepted somewhat specialized colouring.\n\nAn approach to analysis of humor is classification of jokes. A further step is an attempt to generate jokes basing on the rules that underlie classification.\n\nSimple prototypes for computer pun generation were reported in the early 1990s, based on a natural language generator program, VINCI. Graeme Ritchie and Kim Binsted in their 1994 research paper described a computer program, JAPE, designed to generate question-answer-type puns from a general, i.e., non-humorous, lexicon. (The program name is an acronym for \"Joke Analysis and Production Engine\".) Some examples produced by JAPE are:\n\nSince then the approach has been improved, and the latest report, dated 2007, describes the STANDUP joke generator, implemented in the Java programming language. The STANDUP generator was tested on children within the framework of analyzing its usability for language skills development for children with communication disabilities, e.g., because of cerebral palsy. (The project name is an acronym for \"System To Augment Non-speakers' Dialog Using Puns\" and an allusion to standup comedy.) Children responded to this \"language playground\" with enthusiasm, and showed marked improvement on certain types of language tests.\nThe two young people, who used the system over a ten-week period, regaled their peers, staff, family and neighbors with jokes such as: \"What do you call a spicy missile? A hot shot!\" Their joy and enthusiasm at entertaining others was inspirational.\nStock and Strapparava described a program to generate funny acronyms.\n\n\"AskTheBrain\" (2002) used clustering and bayesian analysis to associate concepts in a comical way.\n\nA statistical machine learning algorithm to detect whether a sentence contained a \"That's what she said\" double entendre was developed by Kiddon and Brun (2011). There is an open-source Python implementation of Kiddon & Brun's TWSS system.\n\nA program to recognize knock-knock jokes was reported by Taylor and Mazlack. This kind of research is important in analysis of human-computer interaction.\n\nAn application of machine learning techniques for the distinguishing of joke texts from non-jokes was described by Mihalcea and Strapparava (2006).\n\nTakizawa \"et al.\" (1996) reported on a heuristic program for detecting puns in the Japanese language.\n\nA possible application for the assistance in language acquisition is described in the section \"Pun generation\". Another envisioned use of joke generators is in cases of steady supply of jokes where quantity is more important than quality. Another obvious, yet remote, direction is automated joke appreciation.\n\nIt is known that humans interact with computers in ways similar to interacting with other humans that may be described in terms of personality, politeness, flattery, and in-group favoritism. Therefore, the role of humor in human-computer interaction is being investigated. In particular, humor generation in user interface to ease communications with computers was suggested.\n\nCraig McDonough implemented the Mnemonic Sentence Generator, which converts passwords into humorous sentences. Basing on the incongruity theory of humor, it is suggested that the resulting meaningless but funny sentences are easier to remember. For example, the password AjQA3Jtv is converted into \"Arafat joined Quayle's Ant, while TARAR Jeopardized thurmond's vase\".\n\nJohn Allen Paulos is known for his interest in mathematical foundations of humor. His book \"Mathematics and Humor: A Study of the Logic of Humor\" demonstrates structures common to humor and formal sciences (mathematics, linguistics) and develops a mathematical model of jokes based on catastrophe theory.\n\n\n", "id": "14760591", "title": "Computational humor"}
{"url": "https://en.wikipedia.org/wiki?curid=14893994", "text": "Ordered weighted averaging aggregation operator\n\nIn applied mathematics – specifically in fuzzy logic – the ordered weighted averaging (OWA) operators provide a parameterized class of mean type aggregation operators. They were introduced by Ronald R. Yager. Many notable mean operators such as the max, arithmetic average, median and min, are members of this class. They have been widely used in computational intelligence because of their ability to model linguistically expressed aggregation instructions.\n\nFormally an OWA operator of dimension formula_1 is a mapping formula_2 that has an associated collection of weights formula_3 lying in the unit interval and summing to one and with \n\nwhere formula_5 is the \"j\" largest of the formula_6.\n\nBy choosing different \"W\" one can implement different aggregation operators. The OWA operator is a non-linear operator as a result of the process of determining the \"b\".\n\nThe OWA operator is a mean operator. It is bounded, monotonic, symmetric, and idempotent, as defined below.\n\nTwo features have been used to characterize the OWA operators. The first is the attitudinal character(orness).\n\nThis is defined as\n\nIt is known that formula_19.\n\nIn addition \"A\" − \"C\"(max) = 1, A − C(ave) = A − C(med) = 0.5 and A − C(min) = 0. Thus the A − C goes from 1 to 0 as we go from Max to Min aggregation. The attitudinal character characterizes the similarity of aggregation to OR operation(OR is defined as the Max).\n\nThe second feature is the dispersion. This defined as\n\nAn alternative definition is formula_21 The dispersion characterizes how uniformly the arguments are being used\nÀĚ\n\nThe historical reconstruction of scientific development of the OWA field, the identification of the dominant direction of knowledge accumulation that emerged since the publication of the first OWA paper, and to discover the most active lines of research has recently been published, (see: http://onlinelibrary.wiley.com/doi/10.1002/int.21673/full). The results suggest, as expected, that Yager's paper[1] (IEEE Trans. Systems Man Cybernet, 18(1), 183–190, 1988) is the most influential paper and the starting point of all other research using OWA. Starting from his contribution, other lines of research developed and we describe them. Full list of papers published in OWA is also available at http://onlinelibrary.wiley.com/doi/10.1002/int.21673/full)\n\nThe above Yager's OWA operators are used to aggregate the crisp values. Can we aggregate fuzzy sets in the OWA mechanism ? The\nType-1 OWA operators have been proposed for this purpose. So the type-1 OWA operators provides us with a new technique for directly aggregating uncertain information with uncertain weights via OWA mechanism in soft decision making and data mining, where these uncertain objects are modelled by fuzzy sets.\n\nThe type-1 OWA operator is defined according to the alpha-cuts of fuzzy sets as follows:\n\nGiven the \"n\" linguistic weights formula_22 in the form of fuzzy sets defined on the domain of discourse formula_23, then for each formula_24, an formula_25-level type-1 OWA operator with formula_25-level sets formula_27 to aggregate the formula_25-cuts of fuzzy sets formula_29 is given as\n\nwhere formula_31, and formula_32 is a permutation function such that formula_33, i.e., formula_34 is the formula_35th largest\nelement in the set formula_36.\n\nThe computation of the type-1 OWA output is implemented by computing the left end-points and right end-points of the intervals formula_37:\nformula_38 and formula_39\nwhere formula_40. Then membership function of resulting aggregation fuzzy set is:\n\nFor the left end-points, we need to solve the following programming problem:\n\nwhile for the right end-points, we need to solve the following programming problem:\n\nThis paper has presented a fast method to solve two programming problem so that the type-1 OWA aggregation operation can be performed efficiently.\n\n", "id": "14893994", "title": "Ordered weighted averaging aggregation operator"}
{"url": "https://en.wikipedia.org/wiki?curid=1548772", "text": "20Q\n\n20Q is a computerized game of twenty questions that began as a test in artificial intelligence (AI). It was invented by Robin Burgener in 1988. The game was made handheld by Radica in 2004, then it was discontinued in 2011 because Techno Source took the license for 20Q handheld devices.\n\nThe game 20Q is based on the spoken parlor game known as twenty questions, and is both a website and a handheld device. 20Q asks the player to think of something and will then try to guess what they are thinking of with twenty yes-or-no questions. If it fails to guess in 20 questions, it will ask an additional 5 questions. If it fails to guess even with 25 (or 30) questions, the player is declared the winner. Sometimes the first guess of the object can be asked at question 14.\n\nThe principle is that the player thinks of something and the 20Q artificial intelligence asks a series of questions before guessing what the player is thinking. This artificial intelligence learns on its own with the information relayed back to the players who interact with it, and is not programmed. The player can answer these questions with: \"Yes\", \"No\", \"Unknown\", and \"Sometimes.\" The experiment is based on the classic word game of Twenty Questions, and on the computer game \"Animals,\" popular in the early 1970s, which used a somewhat simpler method to guess an animal.\n\nThe 20Q AI uses an artificial neural network to pick the questions and to guess. After the player has answered the twenty questions posed (sometimes fewer), 20Q makes a guess. If it is incorrect, it asks more questions, then guesses again. It makes guesses based on what it has learned; it is not programmed with information or what the inventor thinks. Answers to any question are based on players’ interpretations of the questions asked. Newer editions were made for different categories, such as music 20Q which has the player think of a song, and Harry Potter 20Q, which has the player think of something from the world of the Harry Potter series.\n\nThe 20Q AI can draw its own conclusions on how to interpret the information. It can be described as more of a folk taxonomy than a taxonomy. Its knowledge develops with every game played. In this regard, the online version of the 20Q AI can be inaccurate because it gathers its answers from what people \"think\" rather than from what people \"know\". Limitations of taxonomy are often overcome by the AI itself because it can learn and adapt. For example, if the player was thinking of a \"Horse\" and answered \"No\" to the question \"Is it an animal?,\" the AI will, nevertheless, guess correctly, despite being told that a horse is not an animal.\n\nPatent applications in the US and Europe were submitted in 2005.\n\nIn August 2014, 20Q.net Inc., with Brashworks Studios, developed and released an iOS iPad version available at the Apple ITunes store.\n\nThe built Artificial Neural Network is not resource intensive either to store or to compute, thus it could be embedded in small, less powerful devices. Currently, there is a handheld version of the AI. The device contains a small portion of the original 20Q website knowledge base; unlike the online versions of the game, the handheld version does not have the ability to learn.\n\nThe 20Q artificial intelligence is different from less flexible, and extremely large, expert systems. Its modularity, adaptability, and scalability means that it can be applied to other, more complex devices, for more complex uses.\n\nOn June 13, 2009, GSN began a TV version of the game, hosted by Cat Deeley, with Hal Sparks as the voice of the computer.\n\n", "id": "1548772", "title": "20Q"}
{"url": "https://en.wikipedia.org/wiki?curid=8703473", "text": "Fred (chatterbot)\n\nFred, or FRED, was an early chatterbot written by Robby Garner.\n\nThe name Fred was initially suggested by Karen Lindsey, and then Robby jokingly came up with an acronym, \"Functional Response Emulation Device.\" Fred has also been implemented as a Java application by Paco Nathan called JFRED.\n\nFred Chatterbot is designed to explore Natural Language communications between people and computer programs. In particular, this is a study of conversation between people and ways that a computer program can learn from other people's conversations to make its own conversations.\n\nFred used a minimalistic \"stimulus-response\" approach. It worked by storing a database of statements and their responses, and made its own reply by looking up the input statements made by a user and then rendering the corresponding response from the database. This approach simplified the complexity of the rule base, but required expert coding and editing for modifications.\n\nFred was a predecessor to Albert One, which Garner used in 1998 and 1999 to win the Loebner Prize.\n\n\n", "id": "8703473", "title": "Fred (chatterbot)"}
{"url": "https://en.wikipedia.org/wiki?curid=15893225", "text": "Competitions and prizes in artificial intelligence\n\nThere are a number of competitions and prizes to promote research in artificial intelligence.\n\nThe David E. Rumelhart prize is an annual award for making a \"significant contemporary contribution to the theoretical foundations of human cognition\". The prize is $100,000.\n\nThe Human-Competitive Award is an annual challenge started in 2004 to reward results \"competitive with the work of creative and inventive humans\". The prize is $10,000. Entries are required to use evolutionary computing.\n\nThe IJCAI Award for Research Excellence is a biannual award given at the IJCAI conference to researcher in artificial intelligence as a recognition of excellence of their career.\n\nThe 2011 Federal Virtual World Challenge advertised by The White House and sponsored by the US Army held a competition offering a total of $52,000 USD in cash prize awards for general artificial intelligence applications, including \"adaptive learning systems, intelligent conversational bots, adaptive behavior (objects or processes)\" and more.\n\nThe Machine Intelligence Prize is awarded annually by the British Computer Society for progress towards machine intelligence.\n\nThe Kaggle - \"the world's largest community of data scientists compete to solve most valuable problems\".\n\nThe Loebner prize is an annual competition to determine the best Turing test competitors. The winner is the computer system that, in the judges' opinions, demonstrates the \"most human\" conversational behaviour, they have an additional prize for a system that in their opinion passes a Turing test. This second prize has not yet been awarded.\n\nThe International Aerial Robotics Competition is a long-running event begun in 1991 to advance the state of the art in fully autonomous air vehicles. This competition is restricted to university teams (although industry and governmental sponsorship of teams is allowed). Key to this event is the creation of flying robots which must complete complex missions without any human intervention. Successful entries are able to interpret their environment and make real-time decisions based only on a high-level mission directive (e.g., \"find a particular target inside a building having certain characteristics which is among a group of buildings 3 kilometers from the aerial robot launch point\"). In 2000, a $30,000 prize was awarded during the 3rd Mission (search and rescue), and in 2008, $80,000 in prize money was awarded at the conclusion of the 4th Mission (urban reconnaissance).\n\nThe DARPA Grand Challenge is a series of competitions to promote driverless car technology, aimed at a congressional mandate stating that by 2015 one-third of the operational ground combat vehicles of the US Armed Forces should be unmanned. While the first race had no winner, the second awarded a $2 million prize for the autonomous navigation of a hundred-mile trail, using GPS, computers and a sophisticated array of sensors. In November 2007, DARPA introduced the DARPA Urban Challenge, a sixty-mile urban area race requiring vehicles to navigate through traffic. In November 2010 the US Armed Forces extended the competition with the $1.6 million prize Multi Autonomous Ground-robotic International Challenge to consider cooperation between multiple vehicles in a simulated-combat situation.\n\nRoborace will be a global motorsport championship with autonomously driving, electrically powered vehicles. The series will be run as a support series during the Formula E championship for electric vehicles. This will be the first global championship for driverless cars.\n\nThe Netflix Prize was a competition for the best collaborative filtering algorithm that predicts user ratings for films, based on previous ratings. The competition was held by Netflix, an online DVD-rental service. The prize was $1,000,000.\n\nThe Pittsburgh Brain Activity Interpretation Competition will reward analysis of fMRI data \"to predict what individuals perceive and how they act and feel in a novel Virtual Reality world involving searching for and collecting objects, interpreting changing instructions, and avoiding a threatening dog.\" The prize in 2007 was $22,000.\n\nThe Face Recognition Grand Challenge (May 2004 to March 2006) aimed to promote and advance face recognition technology.\n\nThe American Meteorological Society's artificial intelligence competition involves learning a classifier to characterise precipitation based on meteorological analyses of environmental conditions and polarimetric radar data.\n\nThe RoboCup and FIRA are annual international robot soccer competitions. The International RoboCup Federation challenge is by 2050 \"a team of fully autonomous humanoid robot soccer players shall win the soccer game, comply with the official rule of the FIFA, against the winner of the most recent World Cup.\"\n\nThe Herbrand Award is a prize given by CADE Inc. to honour persons or groups for important contributions to the field of automated deduction. The prize is $1000.\n\nThe CADE ATP System Competition (CASC) is a yearly competition of fully automated theorem provers for classical first order logic associated with the CADE and IJCAR conferences. The competition was part of the Alan Turing Centenary Conference in 2012, with total prizes of 9000 GBP given by Google.\n\nThe SUMO prize is an annual prize for the best open source ontology extension of the Suggested Upper Merged Ontology (SUMO), a formal theory of terms and logical definitions describing the world. The prize is $3000.\n\nThe Hutter Prize for Lossless Compression of Human Knowledge is a cash prize which rewards compression improvements on a specific 100 MB English text file. The prize awards 500 euros for each one percent improvement, up to €50,000. The organizers believe that text compression and AI are equivalent problems and 3 prizes were already given, at around € 2k.\n\nThe Cyc TPTP Challenge is a competition to develop reasoning methods for the Cyc comprehensive ontology and database of everyday common sense knowledge. The prize is 100 euros for \"each winner of two related challenges\".\n\nThe Eternity II challenge was a constraint satisfaction problem very similar to the Tetravex game. The objective is to lay 256 tiles on a 16x16 grid while satisfying a number of constraints. The problem is known to be NP-complete. The prize was US$2,000,000. The competition ended in December 2010.\n\nThe World Computer Chess Championship has been held since 1970. The International Computer Games Association continues to hold an annual Computer Olympiad which includes this event plus computer competitions for many other games.\n\nThe Ing Prize was a substantial money prize attached to the World Computer Go Congress, starting from 1985 and expiring in 2000. It was a graduated set of handicap challenges against young professional players with increasing prizes as the handicap was lowered. At the time it expired in 2000, the unclaimed prize was 400,000 NT dollars for winning a 9-stone handicap match.\n\nThe AAAI General Game Playing Competition is a competition to develop programs that are effective at general game playing. Given a definition of a game, the program must play it effectively without human intervention. Since the game is not known in advance the competitors cannot especially adapt their programs to a particular scenario. The prize in 2006 and 2007 was $10,000.\n\nThe General Video Game AI (GVG-AI) Competition explores the problem of creating controllers for general video game playing. The competition has multiple tracks including one-player planning, two-player planning, one-player learning and level generation. Competitions for the different tracks will be held across different conferences in 2016.\n\nThe 2007 Ultimate Computer Chess Challenge was a competition organised by World Chess Federation that pitted\nDeep Fritz against Deep Junior. The prize was $100,000.\n\nThe annual Arimaa Challenge offered a $10,000 prize until the year 2020 to develop a program that plays the board game Arimaa and defeats a group of selected human opponents. In 2015, David Wu's bot bot_sharp beat the humans, losing only 2 games out of 9. As a result, the Arimaa Challenge was declared over and David Wu received the prize of $12,000 ($2,000 being offered by third-parties for 2015's championship).\n\n2K Australia is offering a prize worth A$10,000 to develop a game-playing bot that plays a first-person shooter. The aim is to convince a panel of judges that it is actually a human player. The competition started in 2008 and was won in 2012. A new competition is planned for 2014.\n\nThe Google AI Challenge was a bi-annual online contest organized by the University of Waterloo Computer Science Club and sponsored by Google that ran from 2009 to 2011. Each year a game was chosen and contestants submitted specialized automated bots to play against other competing bots.\n\nCloudball had its first round in Spring 2012 and finished on June 15. It is an international artificial intelligence programming contest, where users continuously submit the actions their soccer teams will take in each time step, in simple high level C# code.\n", "id": "15893225", "title": "Competitions and prizes in artificial intelligence"}
{"url": "https://en.wikipedia.org/wiki?curid=1065253", "text": "Darwin machine\n\nA Darwin machine (a 1987 coinage by William H. Calvin, by analogy to a Turing machine) is a machine that, like a Turing machine, involves an iteration process that yields a high-quality result, but, whereas a Turing machine uses logic, the Darwin machine uses rounds of variation, selection, and inheritance.\nIn its original connotation, a Darwin machine is any process that bootstraps quality by utilizing all of the six essential features of a Darwinian process: A \"pattern\" is \"copied\" with \"variations\", where populations of one variant pattern \"compete\" with another population, their relative success biased by a \"multifaceted environment\" (natural selection) so that winners predominate in producing the further variants of the next generation (Darwin's \"inheritance principle\").\n\nMore loosely, a Darwin machine is a process that utilizes some subset of the Darwinian essentials, typically natural selection to create a non-reproducing pattern, as in neural Darwinism. Many aspects of neural development utilize overgrowth followed by pruning to a pattern, but the resulting pattern does not itself create further copies.\n\n\"Darwin machine\" has been used multiple times to name computer programs after Charles Darwin.\n\n\n", "id": "1065253", "title": "Darwin machine"}
{"url": "https://en.wikipedia.org/wiki?curid=16300571", "text": "Computational creativity\n\nComputational creativity (also known as artificial creativity, mechanical creativity, creative computing or creative computation) is a multidisciplinary endeavour that is located at the intersection of the fields of artificial intelligence, cognitive psychology, philosophy, and the arts.\n\nThe goal of computational creativity is to model, simulate or replicate creativity using a computer, to achieve one of several ends:\n\n\nThe field of computational creativity concerns itself with theoretical and practical issues in the study of creativity. Theoretical work on the nature and proper definition of creativity is performed in parallel with practical work on the implementation of systems that exhibit creativity, with one strand of work informing the other.\n\nAs measured by the amount of activity in the field (e.g., publications, conferences and workshops), computational creativity is a growing area of research. But the field is still hampered by a number of fundamental problems. Creativity is very difficult, perhaps even impossible, to define in objective terms. Is it a state of mind, a talent or ability, or a process? Creativity takes many forms in human activity, some \"eminent\" (sometimes referred to as \"Creativity\" with a capital C) and some \"mundane\".\n\nThese are problems that complicate the study of creativity in general, but certain problems attach themselves specifically to \"computational\" creativity:\n\n\nIndeed, not all computer theorists would agree with the premise that computers can only do what they are programmed to do—a key point in favor of computational creativity.\n\nBecause no single perspective or definition seems to offer a complete picture of creativity, the AI researchers Newell, Shaw and Simon developed the combination of novelty and usefulness into the cornerstone of a multi-pronged view of creativity, one that uses the following four criteria to categorize a given answer or solution as creative:\n\n\nWhereas the above reflects a \"top-down\" approach to computational creativity, an alternative thread has developed among \"bottom-up\" computational psychologists involved in artificial neural network research. During the late 1980s and early 1990s, for example, such generative neural systems were driven by genetic algorithms. Experiments involving recurrent nets were successful in hybridizing simple musical melodies and predicting listener expectations.\n\nConcurrent with such research, a number of computational psychologists took the perspective, popularized by Stephen Wolfram, that system behaviors perceived as complex, including the mind's creative output, could arise from what would be considered simple algorithms. As neuro-philosophical thinking matured, it also became evident that language actually presented an obstacle to producing a scientific model of cognition, creative or not, since it carried with it so many unscientific aggrandizements that were more uplifting than accurate. Thus questions naturally arose as to how \"rich,\" \"complex,\" and \"wonderful\" creative cognition actually was.\n\nBefore 1989, artificial neural networks have been used to model certain aspects of creativity. Peter Todd (1989) first trained a neural network to reproduce musical melodies from a training set of musical pieces. Then he used a change algorithm to modify the network's input parameters. The network was able to randomly generate new music in a highly uncontrolled manner. In 1992, Todd\nextended this work, using the so-called distal teacher approach that had been developed by\nPaul Munro, Paul Werbos, D. Nguyen and Bernard Widrow, Michael I. Jordan and David Rumelhart. In the new approach there are two neural networks, one of which is supplying training patterns to another. \nIn later efforts by Todd, a composer would select a set of melodies that define the melody space, position them on a 2-d plane with a mouse-based graphic interface, and train a connectionist network to produce those melodies, and listen to the new \"interpolated\" melodies that the network generates corresponding to intermediate points in the 2-d plane.\n\nMore recently a neurodynamical model of semantic networks has been developed to study how the connectivity structure of these networks relates to the richness of the semantic constructs, or ideas, they can generate. It was demonstrated that semantic neural networks that have richer semantic dynamics than those with other connectivity structures may provide insight into the important issue of how the physical structure of the brain determines one of the most profound features of the human mind – its capacity for creative thought.\n\nSome high-level and philosophical themes recur throughout the field of computational creativity.\n\nMargaret Boden refers to creativity that is novel \"merely to the agent that produces it\" as \"P-creativity\" (or \"psychological creativity\"), and refers to creativity that is recognized as novel \"by society at large\" as \"H-creativity\" (or \"historical creativity\"). Stephen Thaler has suggested a new category he calls \"V-\" or \"Visceral creativity\" wherein significance is invented to raw sensory inputs to a Creativity Machine architecture, with the \"gateway\" nets perturbed to produce alternative interpretations, and downstream nets shifting such interpretations to fit the overarching context. An important variety of such V-creativity is consciousness itself, wherein meaning is reflexively invented to activation turnover within the brain.\n\nBoden also distinguishes between the creativity that arises from an exploration within an established conceptual space, and the creativity that arises from a deliberate transformation or transcendence of this space. She labels the former as \"exploratory creativity\" and the latter as \"transformational creativity\", seeing the latter as a form of creativity far more radical, challenging, and rarer than the former. Following the criteria from Newell and Simon elaborated above, we can see that both forms of creativity should produce results that are appreciably novel and useful (criterion 1), but exploratory creativity is more likely to arise from a thorough and persistent search of a well-understood space (criterion 3) -- while transformational creativity should involve the rejection of some of the constraints that define this space (criterion 2) or some of the assumptions that define the problem itself (criterion 4). Boden's insights have guided work in computational creativity at a very general level, providing more an inspirational touchstone for development work than a technical framework of algorithmic substance. However, Boden's insights are more recently also the subject of formalization, most notably in the work by Geraint Wiggins.\n\nThe criterion that creative products should be novel and useful means that creative computational systems are typically structured into two phases, generation and evaluation. In the first phase, novel (to the system itself, thus P-Creative) constructs are generated; unoriginal constructs that are already known to the system are filtered at this stage. This body of potentially creative constructs are then evaluated, to determine which are meaningful and useful and which are not. This two-phase structure conforms to the Geneplore model of Finke, Ward and Smith, which is a psychological model of creative generation based on empirical observation of human creativity.\n\nA great deal, perhaps all, of human creativity can be understood as a novel combination of pre-existing ideas or objects . Common strategies for combinatorial creativity include:\nThe combinatorial perspective allows us to model creativity as a search process through the space of possible combinations. The combinations can arise from composition or concatenation of different representations, or through a rule-based or stochastic transformation of initial and intermediate representations. Genetic algorithms and neural networks can be used to generate blended or crossover representations that capture a combination of different inputs.\n\nMark Turner and Gilles Fauconnier propose a model called Conceptual Integration Networks that elaborates upon Arthur Koestler's ideas about creativity as well as more recent work by Lakoff and Johnson, by synthesizing ideas from Cognitive Linguistic research into mental spaces and conceptual metaphors. Their basic model defines an integration network as four connected spaces:\n\n\nFauconnier and Turner describe a collection of optimality principles that are claimed to guide the construction of a well-formed integration network. In essence, they see blending as a compression mechanism in which two or more input structures are compressed into a single blend structure. This compression operates on the level of conceptual relations. For example, a series of similarity relations between the input spaces can be compressed into a single identity relationship in the blend.\n\nSome computational success has been achieved with the blending model by extending pre-existing computational models of analogical mapping that are compatible by virtue of their emphasis on connected semantic structures. More recently, Francisco Câmara Pereira presented an implementation of blending theory that employs ideas both from GOFAI and genetic algorithms to realize some aspects of blending theory in a practical form; his example domains range from the linguistic to the visual, and the latter most notably includes the creation of mythical monsters by combining 3-D graphical models.\n\nLanguage provides continuous opportunity for creativity, evident in the generation of novel sentences, phrasings, puns, neologisms, rhymes, allusions, sarcasm, irony, similes, metaphors, analogies, witticisms, and jokes. Native speakers of morphologically rich languages frequently create new word-forms that are easily understood, although they will never find their way to the dictionary. The area of natural language generation has been well studied, but these creative aspects of everyday language have yet to be incorporated with any robustness or scale.\n\nSubstantial work has been conducted in this area of linguistic creation since the 1970s, with the development of James Meehan's TALE-SPIN\n\nThe company Narrative Science makes computer generated news and reports commercially available, including summarizing team sporting events based on statistical data from the game. It also creates financial reports and real estate analyses.\n\nExample of a metaphor: \"\"She was an ape.\"\"\n\nExample of a simile: \"\"Felt like a tiger-fur blanket.\"\"\nThe computational study of these phenomena has mainly focused on interpretation as a knowledge-based process. Computationalists such as Yorick Wilks, James Martin, Dan Fass, John Barnden, and Mark Lee have developed knowledge-based approaches to the processing of metaphors, either at a linguistic level or a logical level. Tony Veale and Yanfen Hao have developed a system, called Sardonicus, that acquires a comprehensive database of explicit similes from the web; these similes are then tagged as bona-fide (e.g., \"as hard as steel\") or ironic (e.g., \"as hairy as a bowling ball\", \"as pleasant as a root canal\"); similes of either type can be retrieved on demand for any given adjective. They use these similes as the basis of an on-line metaphor generation system called Aristotle that can suggest lexical metaphors for a given descriptive goal (e.g., to describe a supermodel as skinny, the source terms \"pencil\", \"whip\", \"whippet\", \"rope\", \"stick-insect\" and \"snake\" are suggested).\n\nThe process of analogical reasoning has been studied from both a mapping and a retrieval perspective, the latter being key to the generation of novel analogies. The dominant school of research, as advanced by Dedre Gentner, views analogy as a structure-preserving process; this view has been implemented in the structure mapping engine or SME, the MAC/FAC retrieval engine (Many Are Called, Few Are Chosen), ACME (Analogical Constraint Mapping Engine) and ARCS (Analogical Retrieval Constraint System). Other mapping-based approaches include Sapper, which situates the mapping process in a semantic-network model of memory. Analogy is a very active sub-area of creative computation and creative cognition; active figures in this sub-area include Douglas Hofstadter, Paul Thagard, and Keith Holyoak. Also worthy of note here is Peter Turney and Michael Littman's machine learning approach to the solving of SAT-style analogy problems; their approach achieves a score that compares well with average scores achieved by humans on these tests.\n\nHumour is an especially knowledge-hungry process, and the most successful joke-generation systems to date have focussed on pun-generation, as exemplified by the work of Kim Binsted and Graeme Ritchie. This work includes the JAPE system, which can generate a wide range of puns that are consistently evaluated as novel and humorous by young children. An improved version of JAPE has been developed in the guise of the STANDUP system, which has been experimentally deployed as a means of enhancing linguistic interaction with children with communication disabilities. Some limited progress has been made in generating humour that involves other aspects of natural language, such as the deliberate misunderstanding of pronominal reference (in the work of Hans Wim Tinholt and Anton Nijholt), as well as in the generation of humorous acronyms in the HAHAcronym system of Oliviero Stock and Carlo Strapparava.\n\nThe blending of multiple word forms is a dominant force for new word creation in language; these new words are commonly called \"blends\" or \"portmanteau words\" (after Lewis Carroll). Tony Veale has developed a system called ZeitGeist that harvests neological headwords from Wikipedia and interprets them relative to their local context in Wikipedia and relative to specific word senses in WordNet. ZeitGeist has been extended to generate neologisms of its own; the approach combines elements from an inventory of word parts that are harvested from WordNet, and simultaneously determines likely glosses for these new words (e.g., \"food traveller\" for \"gastronaut\" and \"time traveller\" for \"chrononaut\"). It then uses Web search to determine which glosses are meaningful and which neologisms have not been used before; this search identifies the subset of generated words that are both novel (\"H-creative\") and useful. Neurolinguistic inspirations have been used to analyze the process of novel word creation in the brain, understand neurocognitive processes responsible for intuition, insight, imagination and creativity and to create a server that invents novel names for products, based on their description. Further, the system Nehovah blends two source words into a neologism that blends the meanings of the two source words. Nehovah searches WordNet for synonyms and TheTopTens.com for pop culture hyponyms. The synonyms and hyponyms are blended together to create a set of candidate neologisms. The neologisms are then scored based on their word structure, how unique the word is, how apparent the concepts are conveyed, and if the neologism has a pop culture reference. Nehovah loosely follows conceptual blending.\n\nLike jokes, poems involve a complex interaction of different constraints, and no general-purpose poem generator adequately combines the meaning, phrasing, structure and rhyme aspects of poetry. Nonetheless, Pablo Gervás has developed a noteworthy system called ASPERA that employs a case-based reasoning (CBR) approach to generating poetic formulations of a given input text via a composition of poetic fragments that are retrieved from a case-base of existing poems. Each poem fragment in the ASPERA case-base is annotated with a prose string that expresses the meaning of the fragment, and this prose string is used as the retrieval key for each fragment. Metrical rules are then used to combine these fragments into a well-formed poetic structure. Racter is an example of such a software project.\n\nComputational creativity in the music domain has focused both on the generation of musical scores for use by human musicians, and on the generation of music for performance by computers. The domain of generation has included classical music (with software that generates music in the style of Mozart and Bach) and jazz. Most notably, David Cope has written a software system called \"Experiments in Musical Intelligence\" (or \"EMI\") that is capable of analyzing and generalizing from existing music by a human composer to generate novel musical compositions in the same style. EMI's output is convincing enough to persuade human listeners that its music is human-generated to a high level of competence.\n\nIn the field of contemporary classical music, Iamus is the first computer that composes from scratch, and produces final scores that professional interpreters can play. The London Symphony Orchestra played a piece for full orchestra, included in Iamus' debut CD, which \"New Scientist\" described as \"The first major work composed by a computer and performed by a full orchestra\". Melomics, the technology behind Iamus, is able to generate pieces in different styles of music with a similar level of quality.\n\nCreativity research in jazz has focused on the process of improvisation and the cognitive demands that this places on a musical agent: reasoning about time, remembering and conceptualizing what has already been played, and planning ahead for what might be played next. The robot Shimon, developed by Gil Weinberg of Georgia Tech, has demonstrated jazz improvisation. Virtual improvisation software based on machine learning models of musical style include OMax, SoMax and PyOracle, are used to create improvisations in real-time by re-injecting variable length sequences learned on the fly from live performer.\n\nIn 1994, a Creativity Machine architecture (see above) was able to generate 11,000 musical hooks by training a synaptically perturbed neural net on 100 melodies that had appeared on the top ten list over the last 30 years. In 1996, a self-bootstrapping Creativity Machine observed audience facial expressions through an advanced machine vision system and perfected its musical talents to generate an album entitled \"Song of the Neurons\"\n\nIn the field of musical composition, the patented works by René-Louis Baron allowed to make a robot that can create and play a multitude of orchestrated melodies so-called \"coherent\" in any musical style. All outdoor physical parameter associated with one or more specific musical parameters, can influence and develop each of these songs (in real time while listening to the song). The patented invention \"Medal-Composer\" raises problems of copyright.\n\nComputational creativity in the generation of visual art has had some notable successes in the creation of both abstract art and representational art. The most famous program in this domain is Harold Cohen's AARON, which has been continuously developed and augmented since 1973. Though formulaic, Aaron exhibits a range of outputs, generating black-and-white drawings or colour paintings that incorporate human figures (such as dancers), potted plants, rocks, and other elements of background imagery. These images are of a sufficiently high quality to be displayed in reputable galleries.\n\nOther software artists of note include the NEvAr system (for \"Neuro-Evolutionary Art\") of Penousal Machado. NEvAr uses a genetic algorithm to derive a mathematical function that is then used to generate a coloured three-dimensional surface. A human user is allowed to select the best pictures after each phase of the genetic algorithm, and these preferences are used to guide successive phases, thereby pushing NEvAr's search into pockets of the search space that are considered most appealing to the user.\n\nThe Painting Fool, developed by Simon Colton originated as a system for overpainting digital images of a given scene in a choice of different painting styles, colour palettes and brush types. Given its dependence on an input source image to work with, the earliest iterations of the Painting Fool raised questions about the extent of, or lack of, creativity in a computational art system. Nonetheless, in more recent work, The Painting Fool has been extended to create novel images, much as AARON does, from its own limited imagination. Images in this vein include cityscapes and forests, which are generated by a process of constraint satisfaction from some basic scenarios provided by the user (e.g., these scenarios allow the system to infer that objects closer to the viewing plane should be larger and more color-saturated, while those further away should be less saturated and appear smaller). Artistically, the images now created by the Painting Fool appear on a par with those created by Aaron, though the extensible mechanisms employed by the former (constraint satisfaction, etc.) may well allow it to develop into a more elaborate and sophisticated painter.\n\nThe artist Krasimira Dimtchevska and the software developer Svillen Ranev have created a computational system combining a rule-based generator of English sentences and a visual composition builder that converts sentences generated by the system into abstract art. The software generates automatically indefinite number of different images using different color, shape and size palettes. The software also allows the user to select the subject of the generated sentences or/and the one or more of the palettes used by the visual composition builder.\n\nAn emerging area of computational creativity is that of video games. ANGELINA is a system for creatively developing video games in Java by Michael Cook. One important aspect is Mechanic Miner, a system which can generate short segments of code which act as simple game mechanics. ANGELINA can evaluate these mechanics for usefulness by playing simple unsolvable game levels and testing to see if the new mechanic makes the level solvable. Sometimes Mechanic Miner discovers bugs in the code and exploits these to make new mechanics for the player to solve problems with.\n\nIn July 2015 Google released \"DeepDream\" – an open source computer vision program, created to detect faces and other patterns in images with the aim of automatically classifying images, which uses a convolutional neural network to find and enhance patterns in images via algorithmic pareidolia, thus creating a dreamlike psychedelic appearance in the deliberately over-processed images.\n\nIn August 2015 researchers from Tübingen, Germany created a convolutional neural network that uses neural representations to separate and recombine content and style of arbitrary images which is able to turn images into stylistic imitations of works of art by artists such as a Picasso or Van Gogh in about an hour. Their algorithm is put into use in the website DeepArt that allows users to create unique artistic images by their algorithm.\n\nIn early 2016, a global team of researchers explained how a new computational creativity approach known as the Digital Synaptic Neural Substrate (DSNS) could be used to generate original chess puzzles that were not derived from endgame databases. The DSNS is able to combine features of different objects (e.g. chess problems, paintings, music) using stochastic methods in order to derive new feature specifications which can be used to generate objects in any of the original domains. The generated chess puzzles have also been featured on YouTube.\n\nCreativity is also useful in allowing for unusual solutions in problem solving. In psychology and cognitive science, this research area is called creative problem solving. The Explicit-Implicit Interaction (EII) theory of creativity has recently been implemented using a CLARION-based computational model that allows for the simulation of incubation and insight in problem solving. The emphasis of this computational creativity project is not on performance per se (as in artificial intelligence projects) but rather on the explanation of the psychological processes leading to human creativity and the reproduction of data collected in psychology experiments. So far, this project has been successful in providing an explanation for incubation effects in simple memory experiments, insight in problem solving, and reproducing the overshadowing effect in problem solving.\n\nSome researchers feel that creativity is a complex phenomenon whose study is further complicated by the plasticity of the language we use to describe it. We can describe not just the agent of creativity as \"creative\" but also the product and the method. Consequently, it could be claimed that it is unrealistic to speak of a \"general theory of creativity\". Nonetheless, some generative principles are more general than others, leading some advocates to claim that certain computational approaches are \"general theories\". Stephen Thaler, for instance, proposes that certain modalities of neural networks are generative enough, and general enough, to manifest a high degree of creative capabilities. Likewise, the Formal Theory of Creativity is based on a simple computational principle published by Jürgen Schmidhuber in 1991. The theory postulates that creativity and curiosity and selective attention in general are by-products of a simple algorithmic principle for measuring and optimizing learning progress.\n\nA unifying model of creativity was proposed by S. L. Thaler through a series of international patents in computational creativity, beginning in 1997 with the issuance of U.S. Patent 5,659,666. Based upon theoretical studies of traumatized neural networks and inspired by studies of damage-induced vibrational modes in simulated crystal lattices, this extensive intellectual property suite taught the application of a broad range of noise, damage, and disordering effects to a trained neural network so as to drive the formation of novel or confabulatory patterns that could potentially qualify as ideas and/or plans of action.\n\nThaler's scientific and philosophical papers both preceding and following the issuance of these patents described:\n\nThaler has also recruited his generative neural architectures into a theory of consciousness that closely models the temporal evolution of thought, creative or not, while also accounting for the subjective feel associated with this hotly debated mental phenomenon.\n\nIn 1989, in one of the most controversial reductions to practice of this general theory of creativity, one neural net termed the \"grim reaper,\" governed the synaptic damage (i.e., rule-changes) applied to another net that had learned a series of traditional Christmas carol lyrics. The former net, on the lookout for both novel and grammatical lyrics, seized upon the chilling sentence, \"In the end all men go to good earth in one eternal silent night,\" thereafter ceasing the synaptic degradation process. In subsequent projects, these systems produced more useful results across many fields of human endeavor, oftentimes bootstrapping their learning from a blank slate based upon the success or failure of self-conceived concepts and strategies seeded upon such internal network damage.\n\nTraditional computers, as mainly used in the computational creativity application, do not support creativity, as they fundamentally transform a set of discrete, limited domain of input parameters into a set of discrete, limited domain of output parameters using a limited set of computational functions. As such, a computer cannot be creative, as everything in the output must have been already present in the input data or the algorithms. For some related discussions and references to related work are captured in some recent work on philosophical foundations of simulation.\n\nMathematically, the same set of arguments against creativity has been made by Chaitin. Similar observations come from a Model Theory perspective. All this criticism emphasizes that computational creativity is useful and may look like creativity, but it is not real creativity, as nothing new is created, just transformed in well defined algorithms.\n\nThe International Conference on Computational Creativity (ICCC) occurs annually, organized by The Association for Computational Creativity. Events in the series include:\n\nPreviously, the community of computational creativity has held a dedicated workshop, the International Joint Workshop on Computational Creativity, every year since 1999. Previous events in this series include:\n\n\nThe 1st Conference on Computer Simulation of Musical Creativity will be held\n\nDesign Computing and Cognition is one conference that addresses computational creativity. The ACM Creativity and Cognition conference is another forum for issues related to computational creativity. Journées d'Informatique Musicale 2016 keynote by Shlomo Dubnov was on Information Theoretic Creativity.\n\nA number of recent books provide either a good introduction or a good overview of the field of Computational Creativity. These include:\n\n\nIn addition to the proceedings of conferences and workshops, the computational creativity community has thus far produced these special journal issues dedicated to the topic:\n\n\nIn addition to these, a new journal has started which focuses on computational creativity within the field of music.\n\n\n\n\n\n", "id": "16300571", "title": "Computational creativity"}
{"url": "https://en.wikipedia.org/wiki?curid=16167377", "text": "Artificial Intelligence System\n\nArtificial Intelligence System (AIS) was a distributed computing project undertaken by Intelligence Realm, Inc. with the long-term goal of simulating the human brain in real time, complete with artificial consciousness and artificial general intelligence. They claimed to have found, in research, the \"mechanisms of knowledge representation in the brain which is equivalent to finding artificial intelligence\", before moving into the developmental phase.\n\nThe project's initial goal was recreating the largest brain simulation to date, performed by neuroscientist Eugene M. Izhikevich of The Neurosciences Institute in San Diego, California. Izhikevich simulated 1 second of activity of 100 billion neurons (the estimated number of neurons in the human brain) in 50 days using a cluster of 27 3-gigahertz processors. He extrapolated that a real-time simulation of the brain could not be achieved before 2016. The project aimed to disprove this prediction.\n\nOn July 12, 2008, AIS announced that the first phase of the project had been completed by reaching the 100 billion neuron mark. The project then continued to simulate neurons while they completed the development of the other applications.\n\nAIS simulated the brain via an artificial neural network, and used Hodgkin–Huxley models. The project utilized the BOINC distributed computing platform. In version 1.08 of the software each work unit received by a volunteer simulated 500,000 neurons for 100 milliseconds at 5 millisecond time steps (the estimated firing rate of a human neuron).\n\nThe application had four primary modules—for creating neurons, simulating neurons, visualizing neurons, and finally, knowledge acquisition. Intention was that the neuronal generator would eventually use genetic algorithms to generate neurons for simulation. The neuron simulator used mathematical models to simulate those neurons. Initially Hodgkin–Huxley models were used, but more models (perhaps hundreds) were intended to be utilized in the future. The visualization software was to allow the administrators to monitor and control the neuronal simulators. The knowledge acquisition module involved feeding information to the system and training it to build its knowledge base.\n\nThe AIS project had successfully simulated over 700 billion neurons by April 2009.\n\nThe project was closed in November 2010 as the BOINC program of the project did not work.\n\n", "id": "16167377", "title": "Artificial Intelligence System"}
{"url": "https://en.wikipedia.org/wiki?curid=16598232", "text": "Progress in artificial intelligence\n\nArtificial intelligence applications have been used in a wide range of fields including medical diagnosis, stock trading, robot control, law, scientific discovery and toys. However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\" \"Many thousands of AI applications are deeply embedded in the infrastructure of every industry.\" In the late 1990s and early 21st century, AI technology became widely used as elements of larger systems, but the field is rarely credited for these successes.\n\nTo allow comparison with human performance, artificial intelligence can be evaluated on constrained and well-defined problems. Such tests have been termed subject matter expert Turing tests. Also, smaller problems provide more achievable goals and there are an ever-increasing number of positive results.\n\nIn his famous Turing test, Alan Turing picked language, the defining feature of human beings, for its basis. Yet, there are many other useful abilities that can be described as showing some form of intelligence. This gives better insight into the comparative success of artificial intelligence in different areas.\n\nIn what has been called the Feigenbaum test, the inventor of expert systems argued for subject specific expert tests. A paper by Jim Gray of Microsoft in 2003 suggested extending the Turing test to speech understanding, speaking and recognizing objects and behavior.\n\nBroad classes of outcome for an AI test may be given as:\n\n\n\n\n\n\n", "id": "16598232", "title": "Progress in artificial intelligence"}
{"url": "https://en.wikipedia.org/wiki?curid=16934252", "text": "Data pack\n\nA data pack (or fact pack) is a pre-made database that can be fed to a software, such as software agents, Internet bots or chatterbots, to teach information and facts, which it can later look up. In other words, a data pack can be used to feed minor updates into a system.\n\nCommon data packs may include abbreviations, acronyms, dictionaries, lexicons and technical data, such as country codes, RFCs, filename extensions, TCP and UDP port numbers, country calling codes, and so on.\n\nData packs may come in formats of CSV and SQL that can easily be parsed or imported into a database management system.\n\nThe database may consist of a key-value pair, like an association list.\n\nData packs are commonly used within the gaming industry to provide minor updates within their games. When a user downloads an update for a game they will be downloading loads of data packs which will contain updates for the game such as minor bug fixes or additional content. An example of a data pack used to update a game can be found on the references.\n\nA data pack DataPack Definition is similar to a data packet it contains loads of information (data) and stores it within a pack where the data can be compressed to reduce its file size. Only certain programs can read a data pack therefore when the data is packed it is vital to know whether the receiving program is able to unpack the data. An example of data packs which are able to effective deliver information can be found on the reference page.\n\nWhen you refer to the word data pack it can come in many forms such as a mobile data pack. A mobile data pack refers to an add-on which can enable you to boost the amount of data which you can use on your mobile phone. The rate at which you use your data can also be monitored, so you know how much data you have left. Mobile data is a service which provides a similar service to Wi-Fi and allows you to connect to the Internet. So the purpose of a data pack is to increase the amount of data that your mobile has access to. An example of a mobile data pack can be found on the references.\n\n\n", "id": "16934252", "title": "Data pack"}
{"url": "https://en.wikipedia.org/wiki?curid=14273658", "text": "Agent systems reference model\n\nThe agent systems reference model (ASRM) is a layered, abstract description for multiagent systems. As such, the reference model\n\nThe ASRM differentiates itself from technical standards, such as Knowledge Interchange Format, Knowledge Query and Manipulation Language, and those of the Foundation for Intelligent Physical Agents in that it defines the required \"existence\" of components of a multiagent system; standards prescribe how they are \"designed\".\n\nThe ASRM was technically constructed through forensic software analysis of existing agent-based systems. Such fielded systems include JaDE, Cougaar, EMAA, NOMADS, Retsina, A-Globe, among others. In so doing, through empirical evidence, the ASRM motivates its functional breakdown of agent-based systems.\n\nThe ASRM was started in July 2005, with the first draft having been completed in November 2006. Contributors to the document have included Drexel University, Cougaar Software, Global InfoTek (see also: CoABS), Soar Technology (see also: Soar), Penn State University, University of Southern California, University of South Carolina, the Florida Institute for Human and Machine Cognition, University of West Florida, BBN Technologies, Telcordia, Lockheed Martin, General Dynamics and others.\n\n\n", "id": "14273658", "title": "Agent systems reference model"}
{"url": "https://en.wikipedia.org/wiki?curid=7539196", "text": "The Leaf (AI) Project\n\nThe Leaf Project is a group robot development program whose objective is to develop a robot platform that supports experiments with artificial intelligence, vision, navigation, etc. \n\nLeaf was created by Bruce Weimer, Alex Brown and Robin Hewitt. It is an artificial life program, inspired by Steve Grand's computer game Creatures, in which artificial beings hatch, develop, and interact in a simulated environment. A PC (software-only) version of Leaf was demonstrated in 2003, construction of the first robot began in early 2004.\n\nLeaf:\n\nThree Leaf robots were completed in 2004 and four more were built in 2006.\n\n", "id": "7539196", "title": "The Leaf (AI) Project"}
{"url": "https://en.wikipedia.org/wiki?curid=11306", "text": "Frame problem\n\nIn artificial intelligence, the frame problem describes an issue with using first-order logic (FOL) to express facts about a robot in the world. Representing the state of a robot with traditional FOL requires the use of many axioms that simply imply that things in the environment do not change arbitrarily. For example, Hayes describes a \"block world\" with rules about stacking blocks together. In a FOL system, additional axioms are required to make inferences about the environment (for example, that a block cannot change position unless it is physically moved). The frame problem is the problem of finding adequate collections of axioms for a viable description of a robot environment.\n\nJohn McCarthy and Patrick J. Hayes defined this problem in their 1969 article, \"Some Philosophical Problems from the Standpoint of Artificial Intelligence\". In this paper, and many that came after, the formal mathematical problem was a starting point for more general discussions of the difficulty of knowledge representation for artificial intelligence. Issues such as how to provide rational default assumptions and what humans consider common sense in a virtual environment. Later, the term acquired a broader meaning in philosophy, where it is formulated as the problem of limiting the beliefs that have to be updated in response to actions. In the logical context, actions are typically specified by what they change, with the implicit assumption that everything else (the frame) remains unchanged.\n\nThe frame problem occurs even in very simple domains. A scenario with a door, which can be open or closed, and a light, which can be on or off, is statically represented by two propositions formula_1 and formula_2. If these conditions can change, they are better represented by two predicates formula_3 and formula_4 that depend on time; such predicates are called fluents. A domain in which the door is closed and the light off at time 0, and the door opened at time 1, can be directly represented in logic by the following formulae:\n\nThe first two formulae represent the initial situation; the third formula represents the effect of executing the action of opening the door at time 1. If such an action had preconditions, such as the door being unlocked, it would have been represented by formula_8. In practice, one would have a predicate formula_9 for specifying when an action is executed and a rule formula_10 for specifying the effects of actions. The article on the situation calculus gives more details.\n\nWhile the three formulae above are a direct expression in logic of what is known, they do not suffice to correctly draw consequences. While the following conditions (representing the expected situation) are consistent with the three formulae above, they are not the only ones.\n\nIndeed, another set of conditions that is consistent with the three formulae above is:\n\nThe frame problem is that specifying only which conditions are changed by the actions does not entail that all other conditions are not changed. This problem can be solved by adding the so-called “frame axioms”, which explicitly specify that all conditions not affected by actions are not changed while executing that action. For example, since the action executed at time 0 is that of opening the door, a frame axiom would state that the status of the light does not change from time 0 to time 1:\n\nThe frame problem is that one such frame axiom is necessary for every pair of action and condition such that the action does not affect the condition. In other words, the problem is that of formalizing a dynamical domain without explicitly specifying the frame axioms.\n\nThe solution proposed by McCarthy to solve this problem involves assuming that a minimal amount of condition changes have occurred; this solution is formalized using the framework of circumscription. The Yale shooting problem, however, shows that this solution is not always correct. Alternative solutions were then proposed, involving predicate completion, fluent occlusion, successor state axioms, etc.; they are explained below. By the end of the 1980s, the frame problem as defined by McCarthy and Hayes was solved. Even after that, however, the term “frame problem” was still used, in part to refer to the same problem but under different settings (e.g., concurrent actions), and in part to refer to the general problem of representing and reasoning with dynamical domains.\n\nThe following solutions depict how the frame problem is solved in various formalisms. The formalisms themselves are not presented in full: what is presented are simplified versions that are sufficient to explain the full solution.\n\nThis solution was proposed by Erik Sandewall, who also defined a formal language for the specification of dynamical domains; therefore, such a domain can be first expressed in this language and then automatically translated into logic. In this article, only the expression in logic is shown, and only in the simplified language with no action names.\n\nThe rationale of this solution is to represent not only the value of conditions over time, but also whether they can be affected by the last executed action. The latter is represented by another condition, called occlusion. A condition is said to be \"occluded\" in a given time point if an action has been just executed that makes the condition true or false as an effect. Occlusion can be viewed as “permission to change”: if a condition is occluded, it is relieved from obeying the constraint of inertia.\n\nIn the simplified example of the door and the light, occlusion can be formalized by two predicates formula_12 and formula_13. The rationale is that a condition can change value only if the corresponding occlusion predicate is true at the next time point. In turn, the occlusion predicate is true only when an action affecting the condition is executed.\n\nIn general, every action making a condition true or false also makes the corresponding occlusion predicate true. In this case, formula_19 is true, making the antecedent of the fourth formula above false for formula_20; therefore, the constraint that formula_21 does not hold for formula_20. Therefore, formula_1 can change value, which is also what is enforced by the third formula.\n\nIn order for this condition to work, occlusion predicates have to be true only when they are made true as an effect of an action. This can be achieved either by circumscription or by predicate completion. It is worth noticing that occlusion does not necessarily imply a change: for example, executing the action of opening the door when it was already open (in the formalization above) makes the predicate formula_24 true and makes formula_1 true; however, formula_1 has not changed value, as it was true already.\n\nThis encoding is similar to the fluent occlusion solution, but the additional predicates denote change, not permission to change. For example, formula_27 represents the fact that the predicate formula_1 will change from time formula_29 to formula_30. As a result, a predicate changes if and only if the corresponding change predicate is true. An action results in a change if and only if it makes true a condition that was previously false or vice versa.\n\nThe third formula is a different way of saying that opening the door causes the door to be opened. Precisely, it states that opening the door changes the state of the door if it had been previously closed. The last two conditions state that a condition changes value at time formula_29 if and only if the corresponding change predicate is true at time formula_29. To complete the solution, the time points in which the change predicates are true have to be as few as possible, and this can be done by applying predicate completion to the rules specifying the effects of actions.\n\nThe value of a condition after the execution of an action can be determined by\nthe fact that the condition is true if and only if:\n\n\nA successor state axiom is a formalization in logic of these two facts. For\nexample, if formula_38 and formula_39 are two\nconditions used to denote that the action executed at time formula_29 was\nto open or close the door, respectively, the running example is encoded as\nfollows.\n\nThis solution is centered around the value of conditions, rather than the\neffects of actions. In other words, there is an axiom for every condition,\nrather than a formula for every action. Preconditions to actions (which are not\npresent in this example) are formalized by other formulae. The successor state\naxioms are used in the variant to the situation calculus proposed by\nRay Reiter.\n\nThe fluent calculus is a variant of the situation calculus. It solves the frame problem by using first-order logic\nterms, rather than predicates, to represent the states. Converting\npredicates into terms in first-order logic is called reification; the\nfluent calculus can be seen as a logic in which predicates representing the\nstate of conditions are reified.\n\nThe difference between a predicate and a term in first-order logic is that a term is a representation of an object (possibly a complex object composed of other objects), while a predicate represents a condition that can be true or false when evaluated over a given set of terms.\n\nIn the fluent calculus, each possible state is represented by a term obtained by composition of other terms, each one representing the conditions that are true in state. For example, the state in which the door is open and the light is on is represented by the term formula_45. It is important to notice that a term is not true or false by itself, as it is an object and not a condition. In other words, the term formula_45 represent a possible state, and does not by itself mean that this is the current state. A separate condition can be stated to specify that this is actually the state at a given time, e.g., formula_47 means that this is the state at time formula_48.\n\nThe solution to the frame problem given in the fluent calculus is to specify the effects of actions by stating how a term representing the state changes when the action is executed. For example, the action of opening the door at time 0 is represented by the formula:\n\nThe action of closing the door, which makes a condition false instead of true, is represented in a slightly different way:\n\nThis formula works provided that suitable axioms are given about formula_51 and formula_52, e.g., a term containing the same condition twice is not a valid state (for example, formula_53 is always false for every formula_54 and formula_29).\n\nThe event calculus uses terms for representing fluents, like the fluent calculus, but also has axioms constraining the value of fluents, like the successor state axioms. In the event calculus, inertia is enforced by formulae stating that a fluent is true if it has been true at a given previous time point and no action changing it to false has been performed in the meantime. Predicate completion is still needed in the event calculus for obtaining that a fluent is made true only if an action making it true has been performed, but also for obtaining that an action had been performed only if that is explicitly stated.\n\nThe frame problem can be thought of as the problem of formalizing the principle that, by default, \"everything is presumed to remain in the state in which it is\" (Leibniz, \"An Introduction to a Secret Encyclopædia\", \"c\". 1679). This default, sometimes called the \"commonsense law of inertia\", was expressed by Raymond Reiter in default logic:\n\n(if formula_57 is true in situation formula_54, and it can be assumed that formula_57 remains true after executing action formula_60, then we can conclude that formula_57 remains true).\n\nSteve Hanks and Drew McDermott argued, on the basis of their Yale shooting example, that this solution to the frame problem is unsatisfactory. Hudson Turner showed, however, that it works correctly in the presence of appropriate additional postulates.\n\nThe counterpart of the default logic solution in the language of answer set programming is a rule with strong negation:\n\n(if formula_63 is true at time formula_64, and it can be assumed that formula_63 remains true at time formula_66, then we can conclude that formula_63 remains true).\n\nAction description languages elude the frame problem rather than solving it. An action description language is a formal language with a syntax that is specific for describing situations and actions. For example, that the action formula_68 makes the door open if not locked is expressed by:\n\nThe semantics of an action description language depends on what the language can express (concurrent actions, delayed effects, etc.) and is usually based on transition systems.\n\nSince domains are expressed in these languages rather than directly in logic, the frame problem only arises when a specification given in an action description logic is to be translated into logic. Typically, however, a translation is given from these languages to answer set programming rather than first-order logic.\n\n\n\n", "id": "11306", "title": "Frame problem"}
{"url": "https://en.wikipedia.org/wiki?curid=9536113", "text": "Computer audition\n\nComputer audition (CA) or machine listening is general field of study of algorithms and systems for audio understanding by machine. Since the notion of what it means for a machine to \"hear\" is very broad and somewhat vague, computer audition attempts to bring together several disciplines that originally dealt with specific problems or had a concrete application in mind. The engineer Paris Smaragdis, interviewed in \"Technology Review\", talks about these systems --\"software that uses sound to locate people moving through rooms, monitor machinery for impending breakdowns, or activate traffic cameras to record accidents.\"\n\nInspired by models of human audition, CA deals with questions of representation, transduction, grouping, use of musical knowledge and general sound semantics for the purpose of performing intelligent operations on audio and music signals by the computer. Technically this requires a combination of methods from the fields of signal processing, auditory modelling, music perception and cognition, pattern recognition, and machine learning, as well as more traditional methods of artificial intelligence for musical knowledge representation.\n\nLike computer vision versus image processing, computer audition versus audio engineering deals with understanding of audio rather than processing. It also differs from problems of speech understanding by machine since it deals with general audio signals, such as natural sounds and musical recordings.\n\nApplications of computer auditions are widely varying, and include search for sounds, genre recognition, acoustic monitoring, music transcription, score following, audio texture, music improvisation, emotion in audio and so on.\n\nComputer Audition overlaps with the following disciplines:\n\nSince audio signals are interpreted by the human ear-brain system, that complex perceptual mechanism should be simulated somehow in software for \"machine listening\". In other words, to perform on par with humans, the computer should hear and understand audio content much as humans do. Analyzing audio accurately involves several fields: electrical engineering (spectrum analysis, filtering, and audio transforms); artificial intelligence (machine learning and sound classification); psychoacoustics (sound perception); cognitive sciences (neuroscience and artificial intelligence); acoustics (physics of sound production); and music (harmony, rhythm, and timbre). Furthermore, audio transformations such as pitch shifting, time stretching, and sound object filtering, should be perceptually and musically meaningful. For best results, these transformations require perceptual understanding of spectral models, high-level feature extraction, and sound analysis/synthesis. Finally, structuring and coding the content of an audio file (sound and metadata) could benefit from efficient compression schemes, which discard inaudible information in the sound. Computational models of music and sound perception and cognition can lead to a more meaningful representation, a more intuitive digital manipulation and generation of sound and music in musical human-machine interfaces.\n\nThe study of CA could be roughly divided into the following sub-problems:\n\nComputer audition deals with audio signals that can be represented in a variety of fashions, from direct encoding of digital audio in two or more channels to symbolically represented synthesis instructions. Audio signals are usually represented in terms of analogue or digital recordings. Digital recordings are samples of acoustic waveform or parameters of audio compression algorithms. One of the unique properties of musical signals is that they often combine different types of representations, such as graphical scores and sequences of performance actions that are encoded as MIDI files.\n\nSince audio signals usually comprise multiple sound sources, then unlike speech signals that can be efficiently described in terms of specific models (such as source-filter model), it is hard to devise a parametric representation for general audio. Parametric audio representations usually use filter banks or sinusoidal models to capture multiple sound parameters, sometimes increasing the representation size in order to capture internal structure in the signal. Additional types of data that are relevant for computer audition are textual descriptions of audio contents, such as annotations, reviews, and visual information in the case of audio-visual recordings.\n\nDescription of contents of general audio signals usually requires extraction of features that capture specific aspects of the audio signal. Generally speaking, one could divide the features into signal or mathematical descriptors such as energy, description of spectral shape etc., statistical characterization such as change or novelty detection, special representations that are better adapted to the nature of musical signals or the auditory system, such as logarithmic growth of sensitivity (bandwidth) in frequency or octave invariance (chroma).\n\nSince parametric models in audio usually require very many parameters, the features are used to summarize properties of multiple parameters in a more compact or salient representation.\n\nFinding specific musical structures is possible by using musical knowledge as well as supervised and unsupervised machine learning methods. Examples of this include detection of tonality according to distribution of frequencies that correspond to patterns of occurrence of notes in musical scales, distribution of note onset times for detection of beat structure, distribution of energies in different frequencies to detect musical chords and so on.\n\nComparison of sounds can be done by comparison of features with or without reference to time. In some cases an overall similarity can be assessed by close values of features between two sounds. In other cases when temporal structure is important, methods of dynamic time warping need to be applied to \"correct\" for different temporal scales of acoustic events. Finding repetitions and similar sub-sequences of sonic events is important for tasks such as texture synthesis and machine improvisation.\n\nSince one of the basic characteristics of general audio is that it comprises multiple simultaneously sounding sources, such as multiple musical instruments, people talking, machine noises or animal vocalization, the ability to identify and separate individual sources is very desirable. Unfortunately, there are no methods that can solve this problem in a robust fashion. Existing methods of source separation rely sometimes on correlation between different audio channels in multi-channel recordings. The ability to separate sources from stereo signals requires different techniques than those usually applied in communications where multiple sensors are available. Other source separation methods rely on training or clustering of features in mono recording, such as tracking harmonically related partials for multiple pitch detection.\n\nListening to music and general audio is commonly not a task directed activity. People enjoy music for various poorly understood reasons, which are commonly referred to the emotional effect of music due to creation of expectations and their realization or violation. Animals attend to signs of danger in sounds, which could be either specific or general notions of surprising and unexpected change. Generally, this creates a situation where computer audition can not rely solely on detection of specific features or sound properties and has to come up with general methods of adapting to changing auditory environment and monitoring its structure. This consists of analysis of larger repetition and self-similarity structures in audio to detect innovation, as well as ability to predict local feature dynamics.\n\nAmong the available data for describing music, there are textual representations, such as liner notes, reviews and criticisms that describe the audio contents in words. In other cases human reactions such as emotional judgements or psycho-physiological measurements might provide an insight into the contents and structure of audio. Computer Audition tries to find relation between these different representations in order to provide this additional understanding of the audio contents.\n\n", "id": "9536113", "title": "Computer audition"}
{"url": "https://en.wikipedia.org/wiki?curid=18402617", "text": "Artificial psychology\n\nArtificial psychology is a theoretical discipline proposed by Dan Curtis (b. 1963). The theory considers the situation when an artificial intelligence approaches the level of complexity where the intelligence meets two conditions:\n\nCondition I\n\nCondition II\n\nWhen both conditions are met, then, according to this theory, the possibility exists that the intelligence will reach irrational conclusions based on real or created information. At this point, the criteria is met for intervention which will not necessarily be resolved by simple re-coding of processes due to extraordinarily complex nature of the codebase itself; but rather a discussion with the intelligence in a format which more closely resembles classical (human) psychology.\n\nIf the intelligence cannot be reprogrammed by directly inputting new code, but requires the intelligence to reprogram itself through a process of analysis and decision based on information provided by a human, in order for it to overcome behavior which is inconsistent with the machines purpose or ability to function normally, then artificial psychology is by definition, what is required.\n\nThe level of complexity that is required before these thresholds are met is currently a subject of extensive debate. The theory of artificial psychology does not address the specifics of what those levels may be, but only that the level is sufficiently complex that the intelligence cannot simply be recoded by a software developer, and therefore dysfunctionality must be addressed through the same processes that humans must go through to address their own dysfunctionalities. Along the same lines, artificial psychology does not address the question of whether or not the intelligence is conscious.\n\nAs of 2015, the level of artificial intelligence does not approach any threshold where any of the theories or principles of artificial psychology can even be tested, and therefore, artificial psychology remains a largely theoretical discipline.\n", "id": "18402617", "title": "Artificial psychology"}
{"url": "https://en.wikipedia.org/wiki?curid=19317802", "text": "Decision list\n\nDecision lists are a representation for Boolean functions. Single term decision lists are more expressive than disjunctions and conjunctions; however, 1-term decision lists are less expressive than the general disjunctive normal form and the conjunctive normal form.\n\nThe language specified by a k-length decision list includes as a subset the language specified by a k-depth decision tree.\n\nLearning decision lists can be used for attribute efficient learning.\n\nA decision list (DL) of length is of the form:\n\nwhere is the th formula and is the th boolean for formula_1. The last if-then-else is the default case, which means formula is always equal to true. A -DL is a decision list where all of formulas have at most terms. Sometimes \"decision list\" is used to refer to a 1-DL, where all of the formulas are either a variable or its negation.\n", "id": "19317802", "title": "Decision list"}
{"url": "https://en.wikipedia.org/wiki?curid=3827197", "text": "Epistemic modal logic\n\nEpistemic modal logic is a subfield of modal logic that is concerned with reasoning about knowledge. While epistemology has a long philosophical tradition dating back to Ancient Greece, epistemic logic is a much more recent development with applications in many fields, including philosophy, theoretical computer science, artificial intelligence, economics and linguistics. While philosophers since Aristotle have discussed modal logic, and Medieval philosophers such as Ockham and Duns Scotus developed many of their observations, it was C. I. Lewis who created the first symbolic and systematic approach to the topic, in 1912. It continued to mature as a field, reaching its modern form in 1963 with the work of Kripke.\n\nMany papers were written in the 1950s that spoke of a logic of knowledge in passing, but it was Finnish philosopher von Wright's paper \"An Essay in Modal Logic\" from 1951 that is seen as a founding document. It was not until 1962 that another Finn, Hintikka, would write \"Knowledge and Belief\", the first book-length work to suggest using modalities to capture the semantics of knowledge rather than the alethic statements typically discussed in modal logic. This work laid much of the groundwork for the subject, but a great deal of research has taken place since that time. For example, epistemic logic has been combined recently with some ideas from dynamic logic to create dynamic epistemic logic, which can be used to specify and reason about information change and exchange of information in multi-agent systems. The seminal works in this field are by Plaza, Van Benthem, and Baltag, Moss, and Solecki.\n\nMost attempts at modeling knowledge have been based on the possible worlds model. In order to do this, we must divide the set of possible worlds between those that are compatible with an agent's knowledge, and those that are not. This generally conforms with common usage. If I know that it is either Friday or Saturday, then I know for sure that it is not Thursday. There is no possible world compatible with my knowledge where it is Thursday, since in all these worlds it is either Friday or Saturday. While we will primarily be discussing the logic-based approach to accomplishing this task, it is worthwhile to mention here the other primary method in use, the event-based approach. In this particular usage, events are sets of possible worlds, and knowledge is an operator on events. Though the strategies are closely related, there are two important distinctions to be made between them:\nTypically, the logic-based approach has been used in fields such as philosophy, logic and AI, while the event-based approach is more often used in fields such as game theory and mathematical economics. In the logic-based approach, a syntax and semantics have been built using the language of modal logic, which we will now describe.\n\nThe basic modal operator of epistemic logic, usually written \"K\", can be read as \"it is known that,\" \"it is epistemically necessary that,\" or \"it is inconsistent with what is known that not.\" If there is more than one agent whose knowledge is to be represented, subscripts can be attached to the operator (formula_1, formula_2, etc.) to indicate which agent one is talking about. So formula_3 can be read as \"Agent formula_4 knows that formula_5.\" Thus, epistemic logic can be an example of multimodal logic applied for knowledge representation. The dual of \"K\", which would be in the same relationship to \"K\" as formula_6 is to formula_7, has no specific symbol, but can be represented by formula_8, which can be read as \"formula_4 does not know that not formula_5\" or \"It is consistent with formula_4's knowledge that formula_5 is possible\". The statement \"formula_4 does not know whether or not formula_5\" can be expressed as formula_15.\n\nIn order to accommodate notions of common knowledge and distributed knowledge, three other modal operators can be added to the language. These are formula_16, which reads \"every agent in group G knows;\" formula_17, which reads \"it is common knowledge to every agent in G;\" and formula_18, which reads \"it is distributed knowledge to every agent in G.\" If formula_5 is a formula of our language, then so are formula_20, formula_21, and formula_22. Just as the subscript after formula_23 can be omitted when there is only one agent, the subscript after the modal operators formula_24, formula_25, and formula_26 can be omitted when the group is the set of all agents.\n\nAs we mentioned above, the logic-based approach is built upon the possible worlds model, the semantics of which are often given definite form in Kripke structures, also known as Kripke models. A Kripke structure \"M\" for \"n\" agents over formula_27 is a (n+2)-tuple formula_28, where S is a nonempty set of \"states\" or \"possible worlds\", formula_29 is an \"interpretation\", which associates with each state in S a truth assignment to the primitive propositions in formula_27, and formula_31 are binary relations on S for \"n\" numbers of agents. It is important here not to confuse formula_32, our modal operator, and formula_33, our accessibility relation.\n\nThe truth assignment tells us whether or not a proposition \"p\" is true or false in a certain state. So formula_34 tells us whether \"p\" is true in state \"s\" in model formula_35. Truth depends not only on the structure, but on the current world as well. Just because something is true in one world does not mean it is true in another. To state that a formula formula_5 is true at a certain world, one writes formula_37, normally read as \"formula_5 is true at (M,s),\" or \"(M,s) satisfies formula_5\".\n\nIt is useful to think of our binary relation formula_33 as a \"possibility\" relation, because it is meant to capture what worlds or states agent \"i\" considers to be possible. In idealized accounts of knowledge (e.g., describing the epistemic status of perfect reasoners with infinite memory capacity), it makes sense for formula_33 to be an equivalence relation, since this is the strongest form and is the most appropriate for the greatest number of applications. An equivalence relation is a binary relation that is reflexive, symmetric, and transitive. The accessibility relation does not have to have these qualities; there are certainly other choices possible, such as those used when modeling belief rather than knowledge.\n\nAssuming that formula_33 is an equivalence relation, and that the agents are perfect reasoners, a few properties of knowledge can be derived. The properties listed here are often known as the \"S5 Properties,\" for reasons described in the Axiom Systems section below.\n\nThis axiom is traditionally known as K. In epistemic terms, it states that if an agent knows formula_5 and knows that formula_44, then the agent must also know formula_45. So,\n\nAnother property we can derive is that if formula_47 is valid, then formula_48. This does not mean that if formula_47 is true, then agent i knows formula_47. What it means is that if formula_47 is true in every world that an agent considers to be a possible world, then the agent must know formula_47 at every possible world. This principle is traditionally called N.\n\nThis axiom is also known as T. It says that if an agent knows facts, the facts must be true. This has often been taken as the major distinguishing feature between knowledge and belief. We can believe a statement to be true when it is false, but it would be impossible to \"know\" a false statement.\n\nThis property and the next state that an agent has introspection about its own knowledge, and are traditionally known as 4 and 5, respectively. The Positive Introspection Axiom, also known as the KK Axiom, says specifically that agents \"know that they know what they know\". This axiom may seem less obvious than the ones listed previously, and Timothy Williamson has argued against its inclusion forcefully in his book, \"Knowledge and Its Limits\".\n\nThe Negative Introspection Axiom says that agents \"know that they do not know what they do not know\".\n\nDifferent modal logics can be derived from taking different subsets of these axioms, and these logics are normally named after the important axioms being employed. However, this is not always the case. KT45, the modal logic that results from the combining of K, T, 4, 5, and the Knowledge Generalization Rule, is primarily known as S5. This is why the properties of knowledge described above are often called the S5 Properties.\n\nEpistemic logic also deals with belief, not just knowledge. The basic modal operator is usually written \"B\" instead of \"K\". In this case though, the knowledge axiom no longer seems right—agents only sometimes believe the truth—so it is usually replaced with the Consistency Axiom, traditionally called D:\n\nwhich states that the agent does not believe a contradiction, or that which is false. When D replaces T in S5, the resulting system is known as KD45. This results in different properties for formula_33 as well. For example, in a system where an agent \"believes\" something to be true, but it is not actually true, the accessibility relation would be non-reflexive. The logic of belief is called doxastic logic.\n\nThe notion of knowledge discussed does not take into account computational constraints on inference. If we take the possible worlds approach to knowledge, it follows that our epistemic agent \"a\" knows all the logical consequences of his or her or its beliefs. If formula_59 is a logical consequence of formula_60, then there is no possible world where formula_60 is true but formula_59 is not. So if \"a\" knows that formula_60, it follows that all of the logical consequences of formula_60 are true of all of the possible worlds compatible with \"a\" 's beliefs. Therefore, \"a\" knows formula_59. It is not epistemically possible for \"a\" that not-formula_59 given his knowledge that formula_60. This consideration was a part of what led Robert Stalnaker to develop two dimensionalism, which can arguably explain how we might not know all the logical consequences of our beliefs even if there are no worlds where the propositions we know come out true but their consequences false.\n\nEven when we ignore possible world semantics and stick to axiomatic systems, this peculiar feature holds. With K and N (the Distribution Rule and the Knowledge Generalization Rule, respectively), which are axioms that are minimally true of all normal modal logics, we can prove that we know all the logical consequences of our beliefs. If formula_59 is a logical consequence of formula_60, then we can derive formula_70 with N and the conditional proof and then formula_71 with K. When we translate this into epistemic terms, this says that if formula_59 is a logical consequence of formula_60, then \"a\" knows that it is, and if \"a\" knows formula_60, \"a\" knows formula_59. That is to say, \"a\" knows all the logical consequences of every proposition. This is necessarily true of all classical modal logics. But then, for example, if \"a\" knows that prime numbers are divisible only by themselves and the number one, then \"a\" knows that 8683317618811886495518194401279999999 is prime (since this number is only divisible by itself and the number one). That is to say, under the modal interpretation of knowledge, when \"a\" knows the definition of a prime number, \"a\" knows that this number is prime. It should be clear at this point that \"a\" is not human. This shows that epistemic modal logic is an idealized account of knowledge, and explains objective, rather than subjective knowledge (if anything).\n\n\n\n", "id": "3827197", "title": "Epistemic modal logic"}
{"url": "https://en.wikipedia.org/wiki?curid=19208664", "text": "Neural modeling fields\n\nNeural modeling field (NMF) is a mathematical framework for machine learning which combines ideas from neural networks, fuzzy logic, and model based recognition. It has also been referred to as modeling fields, modeling fields theory (MFT), Maximum likelihood artificial neural networks (MLANS).\n\nThis framework has been developed by Leonid Perlovsky at the AFRL. NMF is interpreted as a mathematical description of mind’s mechanisms, including concepts, emotions, instincts, imagination, thinking, and understanding. NMF is a multi-level, hetero-hierarchical system. At each level in NMF there are concept-models encapsulating the knowledge; they generate so-called top-down signals, interacting with input, bottom-up signals. These interactions are governed by dynamic equations, which drive concept-model learning, adaptation, and formation of new concept-models for better correspondence to the input, bottom-up signals.\n\nIn the general case, NMF system consists of multiple processing levels. At each level, output signals are the concepts recognized in (or formed from) input, bottom-up signals. Input signals are associated with (or recognized, or grouped into) concepts according to the models and at this level. In the process of learning the concept-models are adapted for better representation of the input signals so that similarity between the concept-models and signals increases. This increase in similarity can be interpreted as satisfaction of an instinct for knowledge, and is felt as aesthetic emotions.\n\nEach hierarchical level consists of N \"neurons\" enumerated by index n=1,2..N. These neurons receive input, bottom-up signals, X(n), from lower levels in the processing hierarchy. X(n) is a field of bottom-up neuronal synaptic activations, coming from neurons at a lower level. Each neuron has a number of synapses; for generality, each neuron activation is described as a set of numbers, \n\n, where D is the number or dimensions necessary to describe individual neuron's activation. \n\nTop-down, or priming signals to these neurons are sent by concept-models, M(S,n) \n\n, where M is the number of models. Each model is characterized by its parameters, S; in the neuron structure of the brain they are encoded by strength of synaptic connections, mathematically, they are given by a set of numbers, \n\n, where A is the number of dimensions necessary to describe invividual model.\n\nModels represent signals in the following way. Suppose that signal X(\"n\") is coming from sensory neurons n activated by object m, which is characterized by parameters S. These parameters may include position, orientation, or lighting of an object m. Model M(S,n) predicts a value X(n) of a signal at neuron n. For example, during visual perception, a neuron n in the visual cortex receives a signal X(n) from retina and a priming signal M(S,n) from an object-concept-model \"m\". Neuron \"n\" is activated if both the bottom-up signal from lower-level-input and the top-down priming signal are strong. Various models compete for evidence in the bottom-up signals, while adapting their parameters for better match as described below. This is a simplified description of perception. The most benign everyday visual perception uses many levels from retina to object perception. The NMF premise is that the same laws describe the basic interaction dynamics at each level. Perception of minute features, or everyday objects, or cognition of complex abstract concepts is due to the same mechanism described below. Perception and cognition involve concept-models and learning. In perception, concept-models correspond to objects; in cognition models correspond to relationships and situations.\n\nLearning is an essential part of perception and cognition, and in NMF theory it is driven by the dynamics that increase a similarity measure between the sets of models and signals, L({X},{M}). The similarity measure is a function of model parameters and associations between the input bottom-up signals and top-down, concept-model signals. In constructing a mathematical description of the similarity measure, it is important to acknowledge two principles:\n\nTherefore, the similarity measure is constructed so that it accounts for all bottom-up signals, \"X\"(\"n\"),\n\nThis expression contains a product of partial similarities, l(X(n)), over all bottom-up signals; therefore it forces the NMF system to account for every signal (even if one term in the product is zero, the product is zero, the similarity is low and the knowledge instinct is not satisfied); this is a reflection of the first principle. Second, before perception occurs, the mind does not know which object gave rise to a signal from a particular retinal neuron. Therefore a partial similarity measure is constructed so that it treats each model as an alternative (a sum over concept-models) for each input neuron signal. Its constituent elements are conditional partial similarities between signal X(n) and model M, l(X(n)|m). This measure is “conditional” on object m being present, therefore, when combining these quantities into the overall similarity measure, L, they are multiplied by r(m), which represent a probabilistic measure of object m actually being present. Combining these elements with the two principles noted above, a similarity measure is constructed as follows: \n\nThe structure of the expression above follows standard principles of the probability theory: a summation is taken over alternatives, m, and various pieces of evidence, n, are multiplied. This expression is not necessarily a probability, but it has a probabilistic structure. If learning is successful, it approximates probabilistic description and leads to near-optimal Bayesian decisions. The name “conditional partial similarity” for l(X(n)|m) (or simply l(n|m)) follows the probabilistic terminology. If learning is successful, l(n|m) becomes a conditional probability density function, a probabilistic measure that signal in neuron n originated from object m. Then L is a total likelihood of observing signals {X(n)} coming from objects described by concept-model {M}. Coefficients r(m), called priors in probability theory, contain preliminary biases or expectations, expected objects m have relatively high r(m) values; their true values are usually unknown and should be learned, like other parameters S.\n\nNote that in probability theory, a product of probabilities usually assumes that evidence is independent. Expression for L contains a product over n, but it does not assume independence among various signals X(n). There is a dependence among signals due to concept-models: each model M(S,n) predicts expected signal values in many neurons n. \n\nDuring the learning process, concept-models are constantly modified. Usually, the functional forms of models, M(S,n), are all fixed and learning-adaptation involves only model parameters, S. From time to time a system forms a new concept, while retaining an old one as well; alternatively, old concepts are sometimes merged or eliminated. This requires a modification of the similarity measure L; The reason is that more models always result in a better fit between the models and data. This is a well known problem, it is addressed by reducing similarity L using a “skeptic penalty function,” (Penalty method) p(N,M) that grows with the number of models M, and this growth is steeper for a smaller amount of data N. For example, an asymptotically unbiased maximum likelihood estimation leads to multiplicative p(N,M) = exp(-N/2), where N is a total number of adaptive parameters in all models (this penalty function is known as Akaike information criterion, see (Perlovsky 2001) for further discussion and references).\n\nThe learning process consists of estimating model parameters S and associating signals with concepts by maximizing the similarity L. Note that all possible combinations of signals and models are accounted for in expression (2) for L. This can be seen by expanding a sum and multiplying all the terms resulting in M items, a huge number. This is the number of combinations between all signals (N) and all models (M). This is the source of Combinatorial Complexity, which is solved in NMF by utilizing the idea of dynamic logic. An important aspect of dynamic logic is \"matching vagueness or fuzziness of similarity measures to the uncertainty of models\". Initially, parameter values are not known, and uncertainty of models is high; so is the fuzziness of the similarity measures. In the process of learning, models become more accurate, and the similarity measure more crisp, the value of the similarity increases. \n\nThe maximization of similarity L is done as follows. First, the unknown parameters {S} are randomly initialized. Then the association variables f(m|n) are computed,\n\nEquation for f(m|n) looks like the Bayes formula for a posteriori probabilities; if l(n|m) in the result of learning become conditional likelihoods, f(m|n) become Bayesian probabilities for signal n originating from object m. The dynamic logic of the NMF is defined as follows:\n\nThe following theorem has been proved (Perlovsky 2001):\n\n\"Theorem\". Equations (3), (4), and (5) define a convergent dynamic NMF system with stationary states defined by max{S}L.\n\nIt follows that the stationary states of an MF system are the maximum similarity states. When partial similarities are specified as probability density functions (pdf), or likelihoods, the stationary values of parameters {S} are asymptotically unbiased and efficient estimates of these parameters. The computational complexity of dynamic logic is linear in N. \n\nPractically, when solving the equations through successive iterations, f(m|n) can be recomputed at every iteration using (3), as opposed to incremental formula (5).\n\nThe proof of the above theorem contains a proof that similarity L increases at each iteration. This has a psychological interpretation that the instinct for increasing knowledge is satisfied at each step, resulting in the positive emotions: NMF-dynamic logic system emotionally enjoys learning.\n\nFinding patterns below noise can be an exceedingly complex problem. If an exact pattern shape is not known and depends on unknown parameters, these parameters should be found by fitting the pattern model to the data. However, when the locations and orientations of patterns are not known, it is not clear which subset of the data points should be selected for fitting. A standard approach for solving this kind of problem is multiple hypothesis testing (Singer et al. 1974). Since all combinations of subsets and models are exhaustively searched, this method faces the problem of combinatorial complexity. In the current example, noisy ‘smile’ and ‘frown’ patterns are sought. They are shown in Fig.1a without noise, and in Fig.1b with the noise, as actually measured. The true number of patterns is 3, which is not known. Therefore, at least 4 patterns should be fit to the data, to decide that 3 patterns fit best. The image size in this example is 100x100 = 10,000 points. If one attempts to fit 4 models to all subsets of 10,000 data points, computation of complexity, M ~ 10. An alternative computation by searching through the parameter space, yields lower complexity: each pattern is characterized by a 3-parameter parabolic shape. Fitting 4x3=12 parameters to 100x100 grid by a brute-force testing would take about 10 to 10 operations, still a prohibitive computational complexity.\nTo apply NMF and dynamic logic to this problem one needs to develop parametric adaptive models of expected patterns. The models and conditional partial similarities for this case are described in details in: a uniform model for noise, Gaussian blobs for highly-fuzzy, poorly resolved patterns, and parabolic models for ‘smiles’ and ‘frowns’. The number of computer operations in this example was about 10. Thus, a problem that was not solvable due to combinatorial complexity becomes solvable using dynamic logic.\n\nDuring an adaptation process, initially fuzzy and uncertain models are associated with structures in the input signals, and fuzzy models become more definite and crisp with successive iterations. The type, shape, and number, of models are selected so that the internal representation within the system is similar to input signals: the NMF concept-models represent structure-objects in the signals. The figure below illustrates operations of dynamic logic. In Fig. 1(a) true ‘smile’ and ‘frown’ patterns are shown without noise; (b) actual image available for recognition (signal is below noise, signal-to-noise ratio is between –2dB and –0.7dB); (c) an initial fuzzy model, a large fuzziness corresponds to uncertainty of knowledge; (d) through (m) show improved models at various iteration stages (total of 22 iterations). Every five iterations the algorithm tried to increase or decrease the number of models. Between iterations (d) and (e) the algorithm decided, that it needs three Gaussian models for the ‘best’ fit. \n\nThere are several types of models: one uniform model describing noise (it is not shown) and a variable number of blob models and parabolic models; their number, location, and curvature are estimated from the data. Until about stage (g) the algorithm used simple blob models, at (g) and beyond, the algorithm decided that it needs more complex parabolic models to describe the data. Iterations stopped at (h), when similarity stopped increasing.\n\nAbove, a single processing level in a hierarchical NMF system was described. At each level of hierarchy there are input signals from lower levels, models, similarity measures (L), emotions, which are defined as changes in similarity, and actions; actions include adaptation, behavior satisfying the knowledge instinct – maximization of similarity. An input to each level is a set of signals X(n), or in neural terminology, an input field of neuronal activations. The result of signal processing at a given level are activated models, or concepts m recognized in the input signals n; these models along with the corresponding instinctual signals and emotions may activate behavioral models and generate behavior at this level.\n\nThe activated models initiate other actions. They serve as input signals to the next processing level, where more general concept-models are recognized or created. Output signals from a given level, serving as input to the next level, are the model activation signals, a, defined as\n\na = ∑ f(m|n). \n\nThe hierarchical NMF system is illustrated in Fig. 2. Within the hierarchy of the mind, each concept-model finds its “mental” meaning and purpose at a higher level (in addition to other purposes). For example, consider a concept-model “chair.” It has a “behavioral” purpose of initiating sitting behavior (if sitting is required by the body), this is the “bodily” purpose at the same hierarchical level. In addition, it has a “purely mental” purpose at a higher level in the hierarchy, a purpose of helping to recognize a more general concept, say of a “concert hall,” a model of which contains rows of chairs. \n\nFrom time to time a system forms a new concept or eliminates an old one. At every level, the NMF system always keeps a reserve of vague (fuzzy) inactive concept-models. They are inactive in that their parameters are not adapted to the data; therefore their similarities to signals are low. Yet, because of a large vagueness (covariance) the similarities are not exactly zero. When a new signal does not fit well into any of the active models, its similarities to inactive models automatically increase (because first, every piece of data is accounted for, and second, inactive models are vague-fuzzy and potentially can “grab” every signal that does not fit into more specific, less fuzzy, active models. When the activation signal a for an inactive model, m, exceeds a certain threshold, the model is activated. Similarly, when an activation signal for a particular model falls below a threshold, the model is deactivated. Thresholds for activation and deactivation are set usually based on information existing at a higher hierarchical level (prior information, system resources, numbers of activated models of various types, etc.). Activation signals for active models at a particular level { a } form a “neuronal field,” which serve as input signals to the next level, where more abstract and more general concepts are formed.\n\n", "id": "19208664", "title": "Neural modeling fields"}
{"url": "https://en.wikipedia.org/wiki?curid=6216524", "text": "SNePS\n\nSNePS is a knowledge representation, reasoning, and acting (KRRA) system developed and maintained by Stuart C. Shapiro and colleagues at the State University of New York at Buffalo.\n\nSNePS is simultaneously a logic-based, frame-based, and network-based KRRA system. It uses an assertional model of knowledge, in that a SNePS knowledge base (KB) consists of a set of assertions (propositions) about various entities. Its intended model is of an intensional domain of mental entities---the entities conceived of by some agent, and the propositions believed by it. The intensionality is primarily accomplished by the absence of a built-in equality operator, since any two syntactically different terms might have slightly different Fregean senses.\n\nSNePS has three styles of inference: formula-based, derived from its logic-based personality; slot-based, derived from its frame-based personality; and path-based, derived from its network-based personality. However, all three are integrated, operating together.\n\nSNePS may be used as a stand-alone KRR system. It has also been used, along with its integrated acting component, to implement the mind of intelligent agents (cognitive robots), in accord with the GLAIR agent architecture (a layered cognitive architecture). The SNePS Research Group often calls its agents Cassie.\n\nAs a logic-based system, a SNePS KB consists of a set of terms, and functions and formulas over those terms. The set of logical connectives and quantifiers extends the usual set used by first-order logics, all taking one or more arbitrarily-sized sets of arguments. In accord with the intended use of SNePS to represent the mind of a natural-language-competent intelligent agent, propositions are first-class entities of the intended domain, so formulas are actually proposition-denoting functional terms. SNePSLOG, the input-output language of the logic-based face of SNePS, looks like a naive logic in that function symbols (including \"predicates\"), and formulas (actually proposition-denoting terms) may be the arguments of functions and may be quantified over. The underlying SNePS, however, is a first order logic, with the user's function symbols and formulas reified.\n\nFormula-based inference is implemented as a natural-deduction-style inference engine in which there are introduction and elimination rules for the connectives and quantifiers. SNePS formula-based inference is sound but not complete, as rules of inference that are less useful for natural language understanding and commonsense reasoning have not been implemented.\n\nA proposition-denoting term in a SNePS KB might or might not be \"asserted\", that is, treated as true in the KB. The SNePS logic is a paraconsistent version of relevance logic, so that a contradiction does not imply anything whatsoever. Nevertheless, SNeBR, the SNePS Belief Revision subsystem, will notice any explicit contradiction and engage the user in a dialogue to repair it. SNeBR is an Assumption-Based Truth Maintenance System (ATMS), and removes the assertion status of any proposition whose support has been removed.\n\nAs a frame-based system, every SNePS functional term (including proposition-valued terms) is represented by a frame with slots and fillers. Each slot may be filled by an arbitrarily-sized set of other terms. However, cycles cannot be constructed. SNePSUL, the SNePS User Language is an input-output language for interacting with SNePS in its guise as a frame-based system.\n\nSNePSLOG may be used in any of three modes. In two modes, the caseframe (set of slots) associated with each functional term is determined by the system. In mode 3, the user declares what caseframe is to be used for each function symbol.\n\nIn slot-based inference, any proposition-valued frame is considered to imply the frame with any of its slots filled by a subset of its fillers. In the current implementation, this is not always sound.\n\nAs a network-based system, SNePS is a propositional semantic network,\nthus the original meaning of \"SNePS\" as \"The Semantic Network\nProcessing System\". This view is obtained by considering every\nindividual constant and every functional term to be a node of the\nnetwork, and every slot to be a directed labeled arc from the\nframe-node it is in to every node in its filler. In the intended\ninterpretation, every node denotes a mental entity, some of which are\npropositions, and every proposition represented in the network is\nrepresented by the node that denotes it. Some nodes are variables of\nthe SNePS logic, and they range over nodes, and only over nodes.\n\nPath-based inference rules may be defined, although they, themselves,\nare not represented in SNePS. A path-based inference rule specifies\nthat some labeled arc \"r\" may be inferred as present from some node \"n\"\nto some other node \"m\" just in case a given path exists from \"n\" to \"m\".\nThere is an extensive recursive set of path constructors available.\n\n\nSNePS has been used for a variety of KRR tasks, for natural language\nunderstanding and generation, for commonsense reasoning, and for\ncognitive robotics. It has been used in several KR courses around the\nworld.\n\nSNePS is implemented as a platform-independent system in Common Lisp and is freely available.\n\n", "id": "6216524", "title": "SNePS"}
{"url": "https://en.wikipedia.org/wiki?curid=568967", "text": "Backward chaining\n\nBackward chaining (or backward reasoning) is an inference method that can be described colloquially as working backward from the goal(s). It is used in automated theorem provers, inference engines, proof assistants and other artificial intelligence applications.\n\nIn game theory, its application to (simpler) subgames in order to find a solution to the game is called backward induction. In chess, it is called retrograde analysis, and it is used to generate tablebases for chess endgames for computer chess.\n\nBackward chaining is implemented in logic programming by SLD resolution. Both rules are based on the modus ponens inference rule. It is one of the two most commonly used methods of reasoning with inference rules and logical implications – the other is forward chaining. Backward chaining systems usually employ a depth-first search strategy, e.g. Prolog.\n\nBackward chaining starts with a list of goals (or a hypothesis) and works backwards from the consequent to the antecedent to see if there is data available that will support any of these consequents. An inference engine using backward chaining would search the inference rules until it finds one which has a consequent (Then clause) that matches a desired goal. If the antecedent (If clause) of that rule is not known to be true, then it is added to the list of goals (in order for one's goal to be confirmed one must also provide data that confirms this new rule).\n\nFor example, suppose a new pet, Fritz, is delivered in an opaque box along with two facts about Fritz:\nThe goal is to decide whether Fritz is green, based on a rule base containing the following four rules:\n\n\nWith backward reasoning, an inference engine can determine whether Fritz is green in four steps. To start, the query is phrased as a goal assertion that is to be proved: \"Fritz is green\".\n\n1. Fritz is substituted for X in rule #3 to see if its consequent matches the goal, so rule #3 becomes:\nSince the consequent matches the goal (\"Fritz is green\"), the rules engine now needs to see if the antecedent (\"If Fritz is a frog\") can be proved. The antecedent therefore becomes the new goal:\n\n2. Again substituting Fritz for X, rule #1 becomes:\nSince the consequent matches the current goal (\"Fritz is a frog\"), the inference engine now needs to see if the antecedent (\"If Fritz croaks and eats flies\") can be proved. The antecedent therefore becomes the new goal:\n\n3. Since this goal is a conjunction of two statements, the inference engine breaks it into two sub-goals, both of which must be proved:\n\n4. To prove both of these sub-goals, the inference engine sees that both of these sub-goals were given as initial facts. Therefore, the conjunction is true:\ntherefore the antecedent of rule #1 is true and the consequent must be true:\ntherefore the antecedent of rule #3 is true and the consequent must be true:\n\nThis derivation therefore allows the inference engine to prove that Fritz is green. Rules #2 and #4 were not used.\n\nNote that the goals always match the affirmed versions of the consequents of implications (and not the negated versions as in modus tollens) and even then, their antecedents are then considered as the new goals (and not the conclusions as in affirming the consequent) which ultimately must match known facts (usually defined as consequents whose antecedents are always true); thus, the inference rule which is used is modus ponens.\n\nBecause the list of goals determines which rules are selected and used, this method is called goal-driven, in contrast to data-driven forward-chaining inference. The backward chaining approach is often employed by expert systems.\n\nProgramming languages such as Prolog, Knowledge Machine and ECLiPSe support backward chaining within their inference engines.\n\n\n", "id": "568967", "title": "Backward chaining"}
{"url": "https://en.wikipedia.org/wiki?curid=290634", "text": "Chess as mental training\n\nThere are efforts to use the game of chess as a tool to aid the intellectual development of young people. Chess is significant in cognitive psychology and artificial intelligence (AI) studies, because it represents the domain in which expert performance has been most intensively studied and measured.\n\nAlthough the results of research studies have failed to produce unambiguous support for the intellectual benefits of playing chess, several local government, schools, and student organizations all over the world are implementing chess programs. \n\nNew York based Chess-In-The-Schools, Inc. has been active in the public school system in the city since 1986. It currently reaches more than 30,000 students annually. America's Foundation for Chess has initiated programs in partnership with local school districts in several U.S. cities, including Seattle, San Diego, Philadelphia, and Tampa. The Chess'n Math Association promotes chess at the scholastic level in Canada. Chess for Success is a program for at-risk schools in Oregon. Since 1991, the U.S. Chess Center in Washington, D.C. teaches chess to children, especially those in the inner city, \"as a means of improving their academic and social skills.\"\n\nThere are a number of experiments that suggest that learning and playing chess aids the mind. The Grandmaster Eugene Torre Chess Institute in the Philippines, the United States Chess Federation's chess research bibliography, and English educational consultant Tony Buzan's Brain Foundation, among others, continuously collect such experimental results. The advent of chess software that automatically record and analyze the moves of each player in each game and can tirelessly play with human players of various levels, further helped in giving new directions to experimental designs on chess as mental training.\n\nAs early as 1779 Benjamin Franklin, in his article \"The morals of chess\", has advocated such a view:\n\"The Game of Chess is not merely an idle amusement; several very valuable qualities of the mind, useful in the course of human life, are to be acquired and strengthened by it, so as to become habits ready on all occasions; for life is a kind of Chess, in which we have often points to gain, and competitors or adversaries to contend with, and in which there is a vast variety of good and ill events, that are, in some degree, the effect of prudence, or the want of it. By playing at Chess then, we may learn:\n\n1st, Foresight, which looks a little into futurity, and considers the consequences that may attend an action ...\n\n2nd, Circumspection, which surveys the whole Chess-board, or scene of action: - the relation of the several Pieces, and their situations; ...\n\n3rd, Caution, not to make our moves too hastily...\"\nAlfred Binet demonstrated in the late 19th century that good chess players have superior memory and imagination. Adriaan de Groot concurred with Alfred Binet that visual memory and visual perception are important attributors and that problem-solving ability is of paramount importance. Thus, since 1972, at the collegiate level, the University of Texas at Dallas and the University of Maryland, Baltimore County both recruit chessplayer-scholars and run scholastic outreach programs in their respective communities.\n\n", "id": "290634", "title": "Chess as mental training"}
{"url": "https://en.wikipedia.org/wiki?curid=21171254", "text": "Type-2 fuzzy sets and systems\n\nType-2 fuzzy sets and systems generalize standard Type-1 fuzzy sets and systems so that more uncertainty can be handled. From the very beginning of fuzzy sets, criticism was made about the fact that the membership function of a type-1 fuzzy set has no uncertainty associated with it, something that seems to contradict the word \"fuzzy\", since that word has the connotation of lots of uncertainty. So, what does one do when there is uncertainty about the value of the membership function? The answer to this question was provided in 1975 by the inventor of fuzzy sets, Prof. Lotfi A. Zadeh, when he proposed more sophisticated kinds of fuzzy sets, the first of which he called a type-2 fuzzy set. A type-2 fuzzy set lets us incorporate uncertainty about the membership function into fuzzy set theory, and is a way to address the above criticism of type-1 fuzzy sets head-on. And, if there is no uncertainty, then a type-2 fuzzy set reduces to a type-1 fuzzy set, which is analogous to probability reducing to determinism when unpredictability vanishes.\n\nIn order to symbolically distinguish between a type-1 fuzzy set and a type-2 fuzzy set, a tilde symbol is put over the symbol for the fuzzy set; so, A denotes a type-1 fuzzy set, whereas Ã denotes the comparable type-2 fuzzy set. When the latter is done, the resulting type-2 fuzzy set is called a general type-2 fuzzy set (to distinguish it from the special interval type-2 fuzzy set).\n\nProf. Zadeh didn't stop with type-2 fuzzy sets, because in that 1976 paper he also generalized all of this to type-\"n\" fuzzy sets. The present article focuses only on type-2 fuzzy sets because they are the \"next step\" in the logical progression from type-1 to type-\"n\" fuzzy sets, where \"n\" = 1, 2, … . Although some researchers are beginning to explore higher than type-2 fuzzy sets, as of early 2009, this work is in its infancy.\nThe membership function of a general type-2 fuzzy set, Ã, is three-dimensional (Fig. 1), where the third dimension is the value of the membership function at each point on its two-dimensional domain that is called its footprint of uncertainty (FOU).\n\nFor an interval type-2 fuzzy set that third-dimension value is the same (e.g., 1) everywhere, which means that no new information is contained in the third dimension of an interval type-2 fuzzy set. So, for such a set, the third dimension is ignored, and only the FOU is used to describe it. It is for this reason that an interval type-2 fuzzy set is sometimes called a \"first-order uncertainty\" fuzzy set model, whereas a general type-2 fuzzy set (with its useful third-dimension) is sometimes referred to as a \"second-order uncertainty\" fuzzy set model.\nThe FOU represents the blurring of a type-1 membership function, and is completely described by its two bounding functions (Fig. 2), a lower membership function (LMF) and an upper membership function (UMF), both of which are type-1 fuzzy sets! Consequently, it is possible to use type-1 fuzzy set mathematics to characterize and work with interval type-2 fuzzy sets. This means that engineers and scientists who already know type-1 fuzzy sets will not have to invest a lot of time learning about general type-2 fuzzy set mathematics in order to understand and use interval type-2 fuzzy sets.\n\nWork on type-2 fuzzy sets languished during the 1980s and early-to-mid 1990's, although a small number of articles were published about them. People were still trying to figure out what to do with type-1 fuzzy sets, so even though Zadeh proposed type-2 fuzzy sets in 1976, the time was not right for researchers to drop what they were doing with type-1 fuzzy sets to focus on type-2 fuzzy sets. This changed in the latter part of the 1990s as a result of Prof. Jerry Mendel and his student's works on type-2 fuzzy sets and systems. Since then, more and more researchers around the world are writing articles about type-2 fuzzy sets and systems.\n\nInterval type-2 fuzzy sets have received the most attention because the mathematics that is needed for such sets—primarily Interval arithmetic—is much simpler than the mathematics that is needed for general type-2 fuzzy sets. So, the literature about interval type-2 fuzzy sets is large, whereas the literature about general type-2 fuzzy sets is much smaller. Both kinds of fuzzy sets are being actively researched by an ever-growing number of researchers around the world.\n\nFormilleri for the following have already been worked out for interval type-2 fuzzy sets:\n\n\nType-2 fuzzy sets are finding very wide applicability in rule-based fuzzy logic systems (FLSs) because they let uncertainties be modeled by them whereas such uncertainties cannot be modeled by type-1 fuzzy sets. A block diagram of a type-2 FLS is depicted in Fig. 3. This kind of FLS is used in fuzzy logic control, fuzzy logic signal processing, rule-based classification, etc., and is sometimes referred to as a \"function approximation\" application of fuzzy sets, because the FLS is designed to minimize an error function.\n\nThe following discussions, about the four components in the Fig. 3 rule-based FLS, are given for an interval type-2 FLS, because to-date they are the most popular kind of type-2 FLS; however, most of the discussions are also applicable for a general type-2 FLS.\n\nRules, that are either provided by subject experts or are extracted from numerical data, are expressed as a collection of IF-THEN statements, e.g.,\n\nFuzzy sets are associated with the terms that appear in the antecedents (IF-part) or consequents (THEN-part) of rules, and with the inputs to and the outputs of the FLS. Membership functions are used to describe these fuzzy sets, and in a type-1 FLS they are all type-1 fuzzy sets, whereas in an interval type-2 FLS at least one membership function is an interval type-2 fuzzy set.\n\nAn interval type-2 FLS lets any one or all of the following kinds of uncertainties be quantified:\n\n\nIn Fig. 3, measured (crisp) inputs are first transformed into fuzzy sets in the Fuzzifier block because it is fuzzy sets and not numbers that activate the rules which are described in terms of fuzzy sets and not numbers. Three kinds of fuzzifiers are possible in an interval type-2 FLS. When measurements are:\n\nIn Fig. 3, after measurements are fuzzified, the resulting input fuzzy sets are mapped into fuzzy output sets by the Inference block. This is accomplished by first quantifying each rule using fuzzy set theory, and by then using the mathematics of fuzzy sets to establish the output of each rule, with the help of an inference mechanism. If there are \"M\" rules then the fuzzy input sets to the Inference block will activate only a subset of those rules, where the subset contains at least one rule and usually way fewer than \"M\" rules. Inference is done one rule at a time. So, at the output of the Inference block, there will be one or more \"fired-rule fuzzy output sets\".\n\nIn most engineering applications of a FLS, a number (and not a fuzzy set) is needed as its final output, e.g., the consequent of the rule given above is \"Rotate the valve a bit to the right.\" No automatic valve will know what this means because \"a bit to the right\" is a linguistic expression, and a valve must be turned by numerical values, i.e. by a certain number of degrees. Consequently, the fired-rule output fuzzy sets have to be converted into a number, and this is done in the Fig. 3 Output Processing block.\n\nIn a type-1 FLS, output processing, called Defuzzification, maps a type-1 fuzzy set into a number. There are many ways for doing this, e.g., compute the union of the fired-rule output fuzzy sets (the result is another type-1 fuzzy set) and then compute the center of gravity of the membership function for that set; compute a weighted average of the center of gravities of each of the fired rule consequent membership functions; etc.\n\nThings are somewhat more complicated for an interval type-2 FLS, because to go from an interval type-2 fuzzy set to a number (usually) requires two steps (Fig. 3). The first step, called type-reduction, is where an interval type-2 fuzzy set is reduced to an interval-valued type-1 fuzzy set. There are as many type-reduction methods as there are type-1 defuzzification methods. An algorithm developed by Karnik and Mendel now known as the KM Algorithm is used for type-reduction. Although this algorithm is iterative, it is very fast.\n\nThe second step of Output Processing, which occurs after type-reduction, is still called defuzzification. Because a type-reduced set of an interval type-2 fuzzy set is always a finite interval of numbers, the defuzzified value is just the average of the two end-points of this interval.\n\nIt is clear from Fig. 3 that there can be two outputs to an interval type-2 FLS—crisp numerical values and the type-reduced set. The latter provides a measure of the uncertainties that have flowed through the interval type-2 FLS, due to the (possibly) uncertain input measurements that have activated rules whose antecedents or consequents or both are uncertain. Just as standard deviation is widely used in probability and statistics to provide a measure of unpredictable uncertainty about a mean value, the type-reduced set can provided a measure of uncertainty about the crisp output of an interval type-2 FLS.\n\nAnother application for fuzzy sets has also been inspired by Prof. Zadeh — Computing With Words. Different acronyms have been used for \"computing with words,\" e.g., CW and CWW. According to Zadeh:\nOf course, he did not mean that computers would actually compute using words—single words or phrases—rather than numbers. He meant that computers would be activated by words, which would be converted into a mathematical representation using fuzzy sets and that these fuzzy sets would be mapped by a CWW engine into some other fuzzy set after which the latter would be converted back into a word. A natural question to ask is: Which kind of fuzzy set—type-1 or type-2—should be used as a model for a word? Mendel has argued, on the basis of Karl Popper's concept of Falsificationism, that using a type-1 fuzzy set as a model for a word is scientifically incorrect. An interval type-2 fuzzy set should be used as a (first-order uncertainty) model for a word. Much research is under way about CWW.\n\nType-2 fuzzy sets were applied in image processing, video processing and computer vision, as well as Failure Mode And Effect Analysis.\n\n\n\nThere are two I\"EEE Expert Now\" multi-media modules that can be accessed from the IEEE at: http://www.ieee.org/web/education/Expert_Now_IEEE/Catalog/AI.html\n\nFreeware MATLAB implementations, which cover general and interval type-2 fuzzy sets and systems, as well as type-1 fuzzy systems, are available at: http://sipi.usc.edu/~mendel/software.\nSoftware supporting discrete interval type-2 fuzzy logic systems is available at:\nDIT2FLS Toolbox - http://dit2fls.com/projects/dit2fls-toolbox/\nDIT2FLS Library Package - http://dit2fls.com/projects/dit2fls-library-package/\n\nJava libraries including source code for type-1, interval- and general type-2 fuzzy systems are available at: http://juzzy.wagnerweb.net/.\n\nAn open source Matlab/Simulink Toolbox for Interval Type-2 Fuzzy Logic Systems is available at: http://web.itu.edu.tr/kumbasart/type2fuzzy.htm\n", "id": "21171254", "title": "Type-2 fuzzy sets and systems"}
{"url": "https://en.wikipedia.org/wiki?curid=21342168", "text": "Grammar systems theory\n\nGrammar systems theory is a field of theoretical computer science that studies systems of finite collections of formal grammars generating a formal language. Each grammar works on a string, a so-called \"sequential form\" that represents an environment. Grammar systems can thus be used as a formalization of decentralized or distributed systems of agents in artificial intelligence.\n\nLet formula_1 be a simple reactive agent moving on the table and trying not to fall down from the table with two reactions, \"t\" for turning and \"ƒ\" for moving forward. The set of possible behaviors of formula_1 can then be described as formal language\n\nwhere \"ƒ\" can be done maximally \"k\" times and \"t\" can be done maximally \"ℓ\" times considering the dimensions of the table.\n\nThe schema on the right describes such a system of grammars which shares a common string representing an environment. The shared sequential form is sequentially rewritten by each grammar, which can represent either a component or generally an agent.\n\nIf grammars communicate together and work on a shared sequential form, it is called a \"Cooperating Distributed (DC)\" grammar system. Shared sequential form is a similar concept to the blackboard approach in AI, which is inspired by an idea of experts solving some problem together while they share their proposals and ideas on a shared blackboard.\n\nEach grammar in a grammar system can also work on its own string and communicate with other grammars in a system by sending their sequential forms on request. Such a grammar system is then called a \"Parallel Communicating (PC)\" grammar system.\n\nPC and DC are inspired by distributed AI. If there is no communication between grammars, the system is close to the decentralized approaches in AI. These kinds of grammar systems are sometimes called \"colonies\" or \"Eco-Grammar\" systems, depending (besides others) on whether the environment is changing on its own (Eco-Grammar system) or not (colonies).\n\n", "id": "21342168", "title": "Grammar systems theory"}
{"url": "https://en.wikipedia.org/wiki?curid=21417543", "text": "Allen (robot)\n\nAllen was a robot introduced by Rodney Brooks and his team in the late 1980s, and was their first robot based on subsumption architecture. It had sonar distance and odometry on board, and used an offboard lisp machine to simulate subsumption architecture. It resembled a footstool on wheels.\n\nAllen used three layers of control which are implemented in subsumption architecture. \"\"The lowest layer of control makes sure that the robot does not come into contact with other objects.\"\" Due to this layer it could avoid static and dynamic obstacles, but it could not move. It sat in the middle of the room, waiting for obstruction. When the obstruction came, Allen ran away, avoiding collisions as it went. It used following internal representation, and every sonar return represented a repulsive force with, and inverse square drop off in strength. Direction of its move was obtained by sum of the repulsive forces (suitably thresholded). It possessed an additional reflex which halted it whenever it was moving forward, and something was directly in its path.\n\n\"\"The first level layer of control (second layer), when combined with zeroth, imbues the robot with the ability to wander around aimlessly without hitting obstacles.\"\" Owing to the second layer, Allen could randomly wander about every 10 seconds. It used simple heuristic, which was coupled with the instinct to shun barriers by vector addition. \"\"The summed vector suppressed the more primitive obstacle avoidance vector, but the obstacle avoidance behaviour still operated, having been subsumed by the new layer, in its account of the lower level's repulsive force. Additionally, the halt reflex of the lower level operated autonomously and unchanged.\"\"\n\nThe third layer made the robot try to explore. Allen could look for distant places (with its sonars), then tried to reach them. \"\"This layer monitored progress through odometry, generating a desired heading which suppressed the direction desired by the wander layer. The desired heading was then fed into a vector addition with the instinctive obstacle avoidance layer. The physical robot did not therefore remain true to the desires of the upper layer. The upper layer had to watch what happened in the world, through odometry, in order to understand what was really happening in the lower control layers, and send down correction signals.\"\"\n\n", "id": "21417543", "title": "Allen (robot)"}
{"url": "https://en.wikipedia.org/wiki?curid=21428998", "text": "Intelligent word recognition\n\nIntelligent Word Recognition, or IWR, is the recognition of unconstrained handwritten words. IWR recognizes entire handwritten words or phrases instead of character-by-character, like its predecessor, Optical Character Recognition (OCR). IWR technology matches handwritten or printed words to a user-defined dictionary, significantly reducing character errors encountered in typical character-based recognition engines. \n\nNew technology on the market utilizes IWR, OCR, and ICR together, which opens many doors for the processing of documents, either constrained (hand printed or machine printed) or unconstrained (freeform cursive). IWR also eliminates a large percentage of the manual data entry of handwritten documents that, in the past, could only be keyed by a human, creating an automated workflow.\n\nWhen cursive handwriting is in play, for each word analyzed, the system breaks down the words into a sequence of graphemes, or subparts of letters. These various curves, shapes and lines make up letters and IWR considers these various shape and groupings in order to calculate a confidence value associated with the word in question.\n\nIWR is not meant to replace ICR and OCR engines which work well with printed data; however, IWR reduces the number of character errors associated with these engines, and it is ideal for processing real-world documents that contain mostly freeform, hard-to-recognize data, inherently unsuitable for them.\n\n\n", "id": "21428998", "title": "Intelligent word recognition"}
{"url": "https://en.wikipedia.org/wiki?curid=13494976", "text": "CALO\n\nCALO was an artificial intelligence project that attempted to integrate numerous AI technologies into a cognitive assistant. CALO is an acronym for \"Cognitive Assistant that Learns and Organizes\". The name was inspired by the Latin word \"calonis,\" which means \"soldier’s servant\". The project started in May 2003 and ran for five years, ending in 2008.\n\nThe CALO effort has had many major spin-offs, most notably the Siri intelligent software assistant that is now part of the Apple iOS since iOS 5, delivered in several phones and tablets; Social Kinetics, a social application that learned personalized intervention and treatment strategies for chronic disease patients, sold to RedBrick Health; the Trapit project, which is a web scraper and news aggregator that makes intelligent selections of web content based on user preferences; Tempo AI, a smart calendar; Desti, a personalized travel guide; and Kuato Studios, a game development startup.\n\nCALO was funded by the Defense Advanced Research Projects Agency (DARPA) under its Personalized Assistant that Learns (PAL) program. DARPA's five-year contract brought together over 300 researchers from 25 of the top university and commercial research institutions, with the goal of building a new generation of cognitive assistants that can reason, learn from experience, be told what to do, explain what they are doing, reflect on their experience, and respond robustly to surprise. SRI International was the lead integrator responsible for coordinating the effort to produce an assistant that can live with and learn from its users, provide value to them, and then pass a yearly evaluation that measures how well the system has learned to do its job.\n\nCALO assists its user with six high-level functions:\n\n\nEvery year, the CALO system, after living with its user for a period of time, is given an achievement-style test of 153 \"administration assistant\" questions, primarily focused on what it has learned about the user's life. Evaluators measure how well CALO's performance on these questions improves year-over-year, and how much of CALO's performance is due to \"learning in the wild\" (new knowledge, tasks, and inferences it has been able to acquire on its own, as opposed to function or knowledge hard-wired into the system by a developer).\n\nSRI International made a collection of successful machine learning and reasoning technologies developed in the PAL program, primarily from the CALO project, available online. The available technologies include both general-purpose learning methods along with more focused learning applications. The PAL software and related publications are available at the PAL Framework website.\n\nThe PAL capabilities have been modularized, packaged, and adapted to industry standards to facilitate their incorporation into target applications. Various infrastructure components and APIs are available to simplify interaction with the technologies. PAL capabilities were integrated into the US Army's CPOF command and control system and fielded to Iraq in 2010.\n\nThe available technologies were developed by research teams at SRI International, Carnegie Mellon University, the University of Massachusetts Amherst, the University of Rochester, the Institute for Human and Machine Cognition, Oregon State University, the University of Southern California, and Stanford University.\n\nIn the first four years of the project, CALO-funded research has resulted in more than five hundred publications across all fields of artificial intelligence. Here are several:\n\n", "id": "13494976", "title": "CALO"}
{"url": "https://en.wikipedia.org/wiki?curid=21577031", "text": "Artificial intelligence and law\n\nArtificial intelligence and law (AI and law) is a subfield of artificial intelligence (AI) mainly concerned with applications of AI to legal informatics problems and original research on those problems. It is also concerned to contribute in the other direction: to export tools and techniques developed in the context of legal problems to AI in general. For example, theories of legal decision making, especially models of argumentation, have contributed to knowledge representation and reasoning; models of social organization based on norms have contributed to multi-agent systems; reasoning with legal cases has contributed to case-based reasoning; and the need to store and retrieve large amounts of textual data has resulted in contributions to conceptual information retrieval and intelligent databases.\n\nAlthough Loevinger, Allen and Mehl anticipated several of the ideas that would become important in AI and Law, the first serious proposal for applying AI techniques to law is usually taken to be Buchanan and Headrick. Early work from this period includes Thorne McCarty's influential TAXMAN project in the USA and Ronald Stamper's LEGOL project in the UK. The former concerned the modeling of the majority and minority arguments in a US Tax law case (Eisner v Macomber), while the latter attempted to provide a formal model of the rules and regulations that govern an organization. Landmarks in the early 1980s include Carole Hafner's work on conceptual retrieval, Anne Gardner's work on contract law, Rissland's work on legal hypotheticals and the work at Imperial College, London on executable formalisations of legislation.\n\nEarly meetings of scholars included a one-off meeting at Swansea, the series of conferences organized by IDG in Florence and the workshops organised by Charles Walter at the University of Houston in 1984 and 1985. In 1987 a biennial conference, the International Conference on AI and Law (ICAIL), was instituted. This conference began to be seen as the main venue for publishing and the developing ideas within AI and Law, and it led to the foundation of the International Association for Artificial Intelligence and Law (IAAIL), to organize and convene subsequent ICAILs. This, in turn, led to the foundation of the Artificial Intelligence and Law Journal, first published in 1992. In Europe, the annual JURIX conferences (organised by the Jurix Foundation for Legal Knowledge Based Systems), began in 1988. Initially intended to bring together the Dutch-speaking (i.e. Dutch and Flemish) researchers, JURIX quickly developed into an international, primarily European, conference and since 2002 has regularly been held outside the Dutch speaking countries. Since 2007 the JURISIN workshops have been held in Japan under the auspices of the Japanese Society for Artificial Intelligence.\n\nToday, AI and law embrace a wide range of topics, including:\n\nFormal models of legal texts and legal reasoning have been used in AI and Law to clarify issues, to give a more precise understanding and to provide a basis for implementations. A variety of formalisms have been used, including propositional and predicate calculi; deontic, temporal and non-monotonic logics; and state transition diagrams. Prakken and Sartor give a detailed and authoritative review of the use of logic and argumentation in AI and Law, and has an excellent set of references.\n\nAn important role of formal models is to remove ambiguity. In fact, legislation abounds with ambiguity: because it is written in natural language there are no brackets and so the scope of connectives such as \"and\" and \"or\" can be unclear (legal drafters do not observe the mathematical conventions in this respect). \"Unless\" is also capable of several interpretations, and legal draftsman never write \"if and only if\", although this is often what they intend by \"if\". In perhaps the earliest use of logic to model law in AI and Law, Layman Allen advocated the use of propositional logic to resolve such syntactic ambiguities in a series of papers.\n\nIn the late 1970s and throughout the 1980s a significant strand on AI and Law work involved the production of executable models of legislation. Originating in the LEGOL work of Ronald Stamper the idea was to represent legislation using a formal language and to use this formalisation (typically with some sort of user interface to gather the facts of a particular case) as the basis for an expert system. This became popular, mainly using the Horn Clause subset of first order predicate calculus. In particular Sergot et al.'s representation of the British Nationality Act did much to popularise the approach. In fact, as later work showed, this was an untypically suitable piece of legislation on which to employ the approach: it was new, and so had not been amended, relatively simple and almost all of the concepts were non-technical. Later work, such as that on Supplementary Benefits, showed that larger, more complicated (containing many cross references, exceptions, counterfactuals, and deeming provisions), legislation which used many highly technical concepts (such as contribution conditions) and which had been the subject of many amendments produced a far less satisfactory final system. Some efforts were made to improve matters from a software engineering perspective, especially to handle problems such as cross reference, verification and frequent amendment. The use of hierarchical representations was suggested to address the first problem, and so-called isomorphic representation was intended to address the other two. As the 1990s developed this strand of work became largely absorbed in the development of formalisations of domain conceptualisations, (so-called ontologies), which became popular in AI following the work of Gruber. Early examples in AI and Law include Valente's functional ontology and the frame based ontologies of Visser and van Kralingen. Legal ontologies have since become the subject of regular workshops at AI and Law conferences and there are many examples ranging from generic top-level and core ontologies to very specific models of particular pieces of legislation.\n\nSince law comprises sets of norms, it is unsurprising that deontic logics have been tried as the formal basis for models of legislation. These, however, have not been widely adopted as the basis for expert systems, perhaps because expert systems are supposed to enforce the norms, whereas deontic logic becomes of real interest only when we need to consider violations of the norms. In law directed obligations, whereby an obligation is owed to another named individual are of particular interest, since violations of such obligations are often the basis of legal proceedings. There is also some interesting work combining deontic and action logics to explore normative positions.\n\nIn the context of multi-agent systems, norms have been modelled using state transition diagrams. Often, especially in the context of electronic institutions, the norms so described are regimented (i.e., cannot be violated), but in other systems violations are also handled, giving a more faithful reflection of real norms. For a good example of this approach see Modgil et al.\n\nLaw often concerns issues about time, both relating to the content, such as time periods and deadlines, and those relating to the law itself, such as commencement. Some attempts have been made to model these temporal logics using both computational formalisms such as the Event Calculus and temporal logics such as defeasible temporal logic.\n\nIn any consideration of the use of logic to model law it needs to be borne in mind that law is inherently non-monotonic, as is shown by the rights of appeal enshrined in all legal systems, and the way in which interpretations of the law change over time. Moreover, in the drafting of law exceptions abound, and, in the application of law, precedents are overturned as well as followed. In logic programming approaches, negation of failure is often used to handle non-monotonicity, but specific non-monotonic logics such as defeasible logic have also been used. Following the development of abstract argumentation, however, these concerns have been addressed through argumentation rather than through the use of non-monotonic logics.\n\n\n", "id": "21577031", "title": "Artificial intelligence and law"}
{"url": "https://en.wikipedia.org/wiki?curid=19768799", "text": "Rule-based system\n\nIn computer science, rule-based systems are used as a way to store and manipulate knowledge to interpret information in a useful way. They are often used in artificial intelligence applications and research.\n\nNormally, the term 'rule-based system' is applied to systems involving human-crafted or curated rule sets. Rule-based systems constructed using automatic rule inference, such as rule-based machine learning, are normally excluded from this system type. \n\nA classic example of a rule-based system is the domain-specific expert system that uses rules to make deductions or choices. For example, an expert system might help a doctor choose the correct diagnosis based on a cluster of symptoms, or select tactical moves to play a game.\n\nRule-based systems can be used to perform lexical analysis to compile or interpret computer programs, or in natural language processing.\n\nRule-based programming attempts to derive execution instructions from a starting set of data and rules. This is a more indirect method than that employed by an imperative programming language, which lists execution steps sequentially.\n\nA typical rule-based system has four basic components:\n\n\n\n", "id": "19768799", "title": "Rule-based system"}
{"url": "https://en.wikipedia.org/wiki?curid=11230690", "text": "Autognostics\n\nAutognostics is a new paradigm that describes the capacity for computer networks to be self-aware. It is considered as one of the major components of Autonomic Networking.\n\nOne of the most important characteristics of today’s Internet that has contributed to its success is its basic design principle: a simple and transparent core with intelligence at the edges (the so-called \"end-to-end principle\"). Based on this principle, the network carries data without knowing the characteristics of that data (e.g., voice, video, etc.) - only the end-points have application-specific knowledge. If something goes wrong with the data, only the edge may be able to recognize that since it knows about the application and what the expected behavior is. The core has no information about what should happen with that data - it only forwards packets.\n\nAlthough an effective and beneficial attribute, this design principle has also led to many of today’s problems, limitations, and frustrations. Currently, it is almost impossible for most end-users to know why certain network-based applications do not work well and what they need to do to make it better. Also, network operators who interact with the core in low-level terms such as router configuration have problems expressing their high-level goals into low-level actions. In high-level terms, this may be summarized as a weak coupling between the network and application layers of the overall system.\n\nAs a consequence of the Internet end-to-end principle, the network performance experienced by a particular application is difficult to attribute based on the behavior of the individual elements. At any given moment, the measure of performance between any two points is typically unknown and applications must operate blindly. As a further consequence, changes to the configuration of given element, or changes in the end-to-end path, cannot easily be validated. Optimization and provisioning cannot then be automated except against only the simplest design specifications.\n\nThere is an increasing interest in Autonomic Networking research, and a strong conviction that an evolution from the current networking status quo is necessary. Although to date there have not been any practical implementations demonstrating the benefits of an effective autonomic networking paradigm, there seems to be a consensus as to the characteristics which such implementations would need to demonstrate. These specifically include continuous monitoring, identifying, diagnosing and fixing problems based on high-level policies and objectives.\n\nAutognostics, as a major part of the autonomic networking concept, intends to bring networks to a new level of awareness and eliminate the lack of visibility which currently exists in today's networks.\n\nAutognostics is a new paradigm that describes the capacity for computer networks to be self-aware, in part and as a whole, and dynamically adapt to the applications running on them by autonomously monitoring, identifying, diagnosing, resolving issues, subsequently verifying that any remediation was successful, and reporting the impact with respect to the application’s use (i.e., providing visibility into the changes to networks and their effects).\n\nAlthough similar to the concept of \"network awareness\", i.e., the capability of network devices and applications to be aware of network characteristics (see References section below), it is noteworthy that autognostics takes that concept one step further. The main difference is the \"auto\" part of autognostics, which entails that network devices are \"self\"-aware of network characteristics, and have the capability to adapt \"themselves\" as a result of continuous monitoring and diagnostics.\n\nAutognostics, or in other words \"deep self-knowledge\", can be best described as \"the ability of a network to know itself and the applications that run on it\". This knowledge is used to autonomously adapt to dynamic network and application conditions such as utilization, capacity, quality of service/application/user experience, etc.\n\nIn order to achieve autognosis, networks need a means to:\n\n\n", "id": "11230690", "title": "Autognostics"}
{"url": "https://en.wikipedia.org/wiki?curid=22113059", "text": "Web intelligence\n\nWeb intelligence is the area of scientific research and development that explores the roles and makes use of artificial intelligence and information technology for new products, services and frameworks that are empowered by the World Wide Web.\n\nThe term was coined in a paper written by Ning Zhong, Jiming Liu Yao and Y.Y. Ohsuga in the Computer Software and Applications Conference in 2000.\n\nThe research about the web intelligence covers many fields – including data mining (in particular web mining), information retrieval, pattern recognition, predictive analytics, the semantic web, web data warehousing – typically with a focus on web personalization and adaptive websites.\n\n\n", "id": "22113059", "title": "Web intelligence"}
{"url": "https://en.wikipedia.org/wiki?curid=22837841", "text": "OpenCog\n\nOpenCog is a project that aims to build an open source artificial intelligence framework. OpenCog Prime is an architecture for robot and virtual embodied cognition that defines a set of interacting components designed to give rise to human-equivalent artificial general intelligence (AGI) as an emergent phenomenon of the whole system. OpenCog Prime's design is primarily the work of Ben Goertzel while the OpenCog framework is intended as a generic framework for broad-based AGI research. Research utilizing OpenCog has been published in journals and presented at conferences and workshops including the annual Conference on Artificial General Intelligence. OpenCog is released under the terms of the GNU Affero General Public License.\n\nOpenCog was originally based on the release in 2008 of the source code of the proprietary \"Novamente Cognition Engine\" (NCE) of Novamente LLC. The original NCE code is discussed in the PLN book (ref below). Ongoing development of OpenCog is supported by Artificial General Intelligence Research Institute (AGIRI), the Google Summer of Code project, and others.\n\nOpenCog consists of:\n\n\nIn 2008, the Machine Intelligence Research Institute (MIRI), formerly called Singularity Institute for Artificial Intelligence (SIAI), which was sponsoring OpenCog, resulting in a part-time post-doc Joel Pitt and a full-time system engineer. Many contributions from the open source community have been made since OpenCog's involvement in the Google Summer of Code in 2008 and 2009. Currently the SIAI no longer supports OpenCog. Today, OpenCog receives funding and support from several sources including the Hong Kong government, Hong Kong Polytechnic University and the Jeffrey Epstein VI Foundation.\n\n\n\n\n", "id": "22837841", "title": "OpenCog"}
{"url": "https://en.wikipedia.org/wiki?curid=19835689", "text": "Concurrent MetateM\n\nConcurrent MetateM is a multi-agent language in which each agent is programmed using a set of (augmented) temporal logic specifications of the behaviour it should exhibit. These specifications are executed directly to generate the behaviour of the agent. As a result, there is no risk of invalidating the logic as with systems where logical specification must first be translated to a lower-level implementation.\n\nThe root of the MetateM concept is Gabbay's separation theorem; any arbitrary temporal logic formula can be rewritten in a logically equivalent \"past → future\" form. Execution proceeds by a process of continually matching rules against a history, and \"firing\" those rules when antecedents are satisfied. Any instantiated future-time consequents become commitments which must subsequently be satisfied, iteratively generating a model for the formula made up of the program rules.\n\nThe Temporal Connectives of Concurrent MetateM can divided into two categories, as follows:\n\n\nThe connectives {◎,●,◆,■,◯,◇,□} are unary; the remainder are binary.\n\n●ρ is satisfied now if ρ was true in the previous time. If ●ρ is interpreted at the beginning of time, it is satisfied despite there being no actual previous time. Hence \"weak\" last.\n\n◎ρ is satisfied now if ρ was true in the previous time. If ◎ρ is interpreted at the beginning of time, it is not satisfied because there is no actual previous time. Hence \"strong\" last.\n\n◆ρ is satisfied now if ρ was true in any previous moment in time.\n\n■ρ is satisfied now if ρ was true in every previous moment in time.\n\nρ\"S\"ψ is satisfied now if ψ is true at any previous moment and ρ is true at every moment after that moment.\n\nρ\"Z\"ψ is satisfied now if (ψ is true at any previous moment and ρ is true at every moment after that moment) OR ψ has not happened in the past.\n\n◯ρ is satisfied now if ρ is true in the next moment in time.\n\n◇ρ is satisfied now if ρ is true now or in any future moment in time.\n\n□ρ is satisfied now if ρ is true now and in every future moment in time.\n\nρ\"U\"ψ is satisfied now if ψ is true at any future moment and ρ is true at every moment prior.\n\nρ\"W\"ψ is satisfied now if (ψ is true at any future moment and ρ is true at every moment prior) OR ψ does not happen in the future.\n\n", "id": "19835689", "title": "Concurrent MetateM"}
{"url": "https://en.wikipedia.org/wiki?curid=11920671", "text": "Machine perception\n\nMachine perception is the capability of a computer system to interpret data in a manner that is similar to the way humans use their senses to relate to the world around them. The basic method that the computers take in and respond to their environment is through the attached hardware. Until recently input was limited to a keyboard, or a mouse, but advances in technology, both in hardware and software, have allowed computers to take in sensory input in a way similar to humans.\n\nMachine perception allows the computer to use this sensory input, as well as conventional computational means of gathering information, to gather information with greater accuracy and to present it in a way that is more comfortable for the user. These include computer vision, machine hearing, and machine touch.\n\nThe end goal of machine perception is to give machines the ability to see, feel and perceive the world as humans do and therefore for them to be able to explain in a human way why they are making their decisions, to warn us when it is failing and more importantly, the reason why it is failing.\n\nComputer vision is a field that includes methods for acquiring, processing, analyzing, and understanding images and, in general, high-dimensional data from the real world in order to produce numerical or symbolic information, e.g., in the forms of decisions. Computer vision has many applications already in use today such as facial recognition, geographical modeling, and even aesthetic judgment.\n\nMachine hearing, also known as machine listening or computer audition, is the ability of a computer or machine to take in and process sound data such as music or speech. This area has a wide range of application including music recording and compression, speech synthesis, and speech recognition.\nMoreover, this technology allows the machine to replicate the human brain’s ability to selectively focus in a specific sound against many other competing sounds and background noise. This particular ability is called “auditory scene analysis”. The technology enables the machine to segment several streams occurring at the same time.\nMany commonly used devices such as a smartphones, voice translators, and cars make use of some form of machine hearing.\n\nMachine touch is an area of machine perception where tactile information is processed by a machine or computer. Applications include tactile perception of surface properties and dexterity whereby tactile information can enable intelligent reflexes and interaction with the environment.\n\n", "id": "11920671", "title": "Machine perception"}
{"url": "https://en.wikipedia.org/wiki?curid=3479720", "text": "Situated\n\nIn artificial intelligence and cognitive science, the term situated refers to an agent which is embedded in an environment. The term \"situated\" is commonly used to refer to robots, but some researchers argue that software agents can also be situated if:\n\n\nExamples might include web-based agents, which can alter data or trigger processes (such as purchases) over the internet, or virtual-reality bots which inhabit and change virtual worlds, such as Second Life.\n\nBeing situated is generally considered to be part of being embodied, but it is useful to consider each perspective individually. The situated perspective emphasizes that intelligent behaviour derives from the environment and the agent's interactions with it. The nature of these interactions are defined by an agent's embodiment.\n\n\n", "id": "3479720", "title": "Situated"}
{"url": "https://en.wikipedia.org/wiki?curid=2862", "text": "AI-complete\n\nIn the field of artificial intelligence, the most difficult problems are informally known as AI-complete or AI-hard, implying that the difficulty of these computational problems is equivalent to that of solving the central artificial intelligence problem—making computers as intelligent as people, or strong AI. To call a problem AI-complete reflects an attitude that it would not be solved by a simple specific algorithm. \n\nAI-complete problems are hypothesised to include computer vision, natural language understanding, and dealing with unexpected circumstances while solving any real world problem.\n\nCurrently, AI-complete problems cannot be solved with modern computer technology alone, but would also require human computation. This property can be useful, for instance to test for the presence of humans as with CAPTCHAs, and for computer security to circumvent brute-force attacks.\n\nThe term was coined by Fanya Montalvo by analogy with NP-complete and NP-hard in complexity theory, which formally describes the most famous class of difficult problems. Early uses of the term are in Erik Mueller's 1987 Ph.D. dissertation and in Eric Raymond's 1991 Jargon File.\n\nAI-complete problems are hypothesised to include:\n\nTo translate accurately, a machine must be able to understand the text. It must be able to follow the author's argument, so it must have some ability to reason. It must have extensive world knowledge so that it knows what is being discussed — it must at least be familiar with all the same commonsense facts that the average human translator knows. Some of this knowledge is in the form of facts that can be explicitly represented, but some knowledge is unconscious and closely tied to the human body: for example, the machine may need to understand how an ocean makes one \"feel\" to accurately translate a specific metaphor in the text. It must also model the authors' goals, intentions, and emotional states to accurately reproduce them in a new language. In short, the machine is required to have wide variety of human intellectual skills, including reason, commonsense knowledge and the intuitions that underlie motion and manipulation, perception, and social intelligence. Machine translation, therefore, is believed to be AI-complete: it may require strong AI to be done as well as humans can do it.\n\nCurrent AI systems can solve very simple and/or restricted versions of AI-complete problems, but never in their full generality. When AI researchers attempt to \"scale up\" their systems to handle more complicated, real world situations, the programs tend to become excessively brittle without commonsense knowledge or a rudimentary understanding of the situation: they fail as unexpected circumstances outside of its original problem context begin to appear. When human beings are dealing with new situations in the world, they are helped immensely by the fact that they know what to expect: they know what all things around them are, why they are there, what they are likely to do and so on. They can recognize unusual situations and adjust accordingly. A machine without strong AI has no other skills to fall back on.\n\nComputational complexity theory deals with the relative computational difficulty of computable functions. By definition it does not cover problems whose solution is unknown or has not been characterised formally. Since many AI problems have no formalisation yet, conventional complexity theory does not allow the definition of AI-completeness.\n\nTo address this problem, a complexity theory for AI has been proposed. It is based on a model of computation that splits the computational burden between a computer and a human: one part is solved by computer and the other part solved by human. This is formalised by a human-assisted Turing machine. The formalisation defines algorithm complexity, problem complexity and reducibility which in turn allows equivalence classes to be defined.\n\nThe complexity of executing an algorithm with a human-assisted Turing machine is given by a pair formula_1, where the first element represents the complexity of the human's part and the second element is the complexity of the machine's part.\n\nThe complexity of solving the following problems with a human-assisted Turing machine is:\n\n\n", "id": "2862", "title": "AI-complete"}
{"url": "https://en.wikipedia.org/wiki?curid=2711317", "text": "Intelligent agent\n\nIn artificial intelligence, an intelligent agent (IA) is an autonomous entity which observes through sensors and acts upon an environment using actuators (i.e. it is an agent) and directs its activity towards achieving goals (i.e. it is \"rational\", as defined in economics). Intelligent agents may also learn or use knowledge to achieve their goals. They may be very simple or very complex. A reflex machine, such as a thermostat, is considered an example of an intelligent agent.\nIntelligent agents are often described schematically as an abstract functional system similar to a computer program. For this reason, intelligent agents are sometimes called abstract intelligent agents (AIA) to distinguish them from their real world implementations as computer systems, biological systems, or organizations. Some definitions of intelligent agents emphasize their , and so prefer the term autonomous intelligent agents. Still others (notably ) considered goal-directed behavior as the essence of intelligence and so prefer a term borrowed from economics, \"rational agent\".\n\nIntelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, the philosophy of practical reason, as well as in many interdisciplinary socio-cognitive modeling and computer social simulations.\n\nIntelligent agents are also closely related to software agents (an autonomous computer program that carries out tasks on behalf of users). In computer science, the term \"intelligent agent\" may be used to refer to a software agent that has some intelligence, regardless if it is not a rational agent by Russell and Norvig's definition. For example, autonomous programs used for operator assistance or data mining (sometimes referred to as \"bots\") are also called \"intelligent agents\".\n\nIntelligent agents have been defined many different ways. According to Nikola Kasabov AI systems should exhibit the following characteristics:\n\nA simple agent program can be defined mathematically as an agent function which maps every possible percepts sequence to a possible action the agent can perform or to a coefficient, feedback element, function or constant that affects eventual actions:\nAgent function is an abstract concept as it could incorporate various principles of decision making like calculation of utility of individual options, deduction over logic rules, fuzzy logic, etc.\n\nThe program agent, instead, maps every possible percept to an action.\n\nWe use the term percept to refer to the agent's perceptional inputs at any given instant. In the following figures an agent is anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators.\n\n said we should consider four classes of agents:\n\n group agents into five classes based on their degree of perceived intelligence and capability:\n\nSimple reflex agents act only on the basis of the current percept, ignoring the rest of the percept history. The agent function is based on the \"condition-action rule\": if condition then action.\n\nThis agent function only succeeds when the environment is fully observable. Some reflex agents can also contain information on their current state which allows them to disregard conditions whose actuators are already triggered.\n\nInfinite loops are often unavoidable for simple reflex agents operating in partially observable environments. Note: If the agent can randomize its actions, it may be possible to escape from infinite loops.\n\nA model-based agent can handle partially observable environments. Its current state is stored inside the agent maintaining some kind of structure which describes the part of the world which cannot be seen. This knowledge about \"how the world works\" is called a model of the world, hence the name \"model-based agent\".\n\nA model-based reflex agent should maintain some sort of internal model that depends on the percept history and thereby reflects at least some of the unobserved aspects of the current state. Percept history and impact of action on the environment can be determined by using internal model. It then chooses an action in the same way as reflex agent.\n\nGoal-based agents further expand on the capabilities of the model-based agents, by using \"goal\" information. Goal information describes situations that are desirable. This allows the agent a way to choose among multiple possibilities, selecting the one which reaches a goal state. Search and planning are the subfields of artificial intelligence devoted to finding action sequences that achieve the agent's goals.\n\nGoal-based agents only distinguish between goal states and non-goal states. It is possible to define a measure of how desirable a particular state is. This measure can be obtained through the use of a \"utility function\" which maps a state to a measure of the utility of the state. A more general performance measure should allow a comparison of different world states according to exactly how happy they would make the agent. The term utility can be used to describe how \"happy\" the agent is.\n\nA rational utility-based agent chooses the action that maximizes the expected utility of the action outcomes - that is, what the agent expects to derive, on average, given the probabilities and utilities of each outcome. A utility-based agent has to model and keep track of its environment, tasks that have involved a great deal of research on perception, representation, reasoning, and learning.\n\nLearning has the advantage that it allows the agents to initially operate in unknown environments and to become more competent than its initial knowledge alone might allow. The most important distinction is between the \"learning element\", which is responsible for making improvements, and the \"performance element\", which is responsible for selecting external actions.\n\nThe learning element uses feedback from the \"critic\" on how the agent is doing and determines how the performance element should be modified to do better in the future.\nThe performance element is what we have previously considered to be the entire agent: it takes in percepts and decides on actions.\n\nThe last component of the learning agent is the \"problem generator\". It is responsible for suggesting actions that will lead to new and informative experiences.\n\nTo actively perform their functions, Intelligent Agents today are normally gathered in a hierarchical structure containing many “sub-agents”. Intelligent sub-agents process and perform lower level functions. Taken together, the intelligent agent and sub-agents create a complete system that can accomplish difficult tasks or goals with behaviors and responses that display a form of intelligence.\n\nIntelligent agents are applied as automated online assistants, where they function to perceive the needs of customers in order to perform individualized customer service. Such an agent may basically consist of a dialog system, an avatar, as well an expert system to provide specific expertise to the user. They can also be used to optimize coordination of human groups online.\n\n\n", "id": "2711317", "title": "Intelligent agent"}
{"url": "https://en.wikipedia.org/wiki?curid=23762260", "text": "Language Acquisition Device (computer)\n\nThe Language Acquisition Device is a computer program developed by Lobal Technologies, a computer company in the United Kingdom, and scientists from King's College. It emulates the functions of the brain's frontal lobes where humans process language and emotion.\n\nScientists hope this might enable computers to understand, speak, learn, and eventually think. One possible use is in interactive entertainment such as video gaming, where the technology is used to help computer-controlled characters to develop. A press release describing this technology produced widespread media interest in 2002, but no reports have been published since then, and the current status of the technology is unclear.\n\n", "id": "23762260", "title": "Language Acquisition Device (computer)"}
{"url": "https://en.wikipedia.org/wiki?curid=5477824", "text": "Cobweb (clustering)\n\nCOBWEB is an incremental system for hierarchical conceptual clustering. COBWEB was invented by Professor Douglas H. Fisher, currently at Vanderbilt University.\n\nCOBWEB incrementally organizes observations into a classification tree. Each node in a classification tree represents a class (concept) and is labeled by a probabilistic concept that summarizes the attribute-value distributions of objects classified under the node. This classification tree can be used to predict missing attributes or the class of a new object.\n\nThere are four basic operations COBWEB employs in building the classification tree. Which operation is selected depends on the category utility of the classification achieved by applying it. The operations are:\n\n \"COBWEB\"(\"root\", \"record\"):\n\n", "id": "5477824", "title": "Cobweb (clustering)"}
{"url": "https://en.wikipedia.org/wiki?curid=19890479", "text": "Probabilistic logic network\n\nA probabilistic logic network (PLN) is a conceptual, mathematical and computational approach to uncertain inference; inspired by logic programming, but using probabilities in place of crisp (true/false) truth values, and fractional uncertainty in place of crisp known/unknown values. In order to carry out effective reasoning in real-world circumstances, artificial intelligence software must robustly handle uncertainty. However, previous approaches to uncertain inference do not have the breadth of scope required to provide an integrated treatment of the disparate forms of cognitively critical uncertainty as they manifest themselves within the various forms of pragmatic inference. Going beyond prior probabilistic approaches to uncertain inference, PLN is able to encompass within uncertain logic such ideas as induction, abduction, analogy, fuzziness and speculation, and reasoning about time and causality.\n\nPLN was developed by Ben Goertzel, Matt Ikle, Izabela Lyon Freire Goertzel and Ari Heljakka for use as a cognitive algorithm used by MindAgents within the OpenCog Core. PLN was developed originally for use within the Novamente Cognition Engine.\n\nThe basic goal of PLN is to provide reasonably accurate probabilistic inference in a way that is compatible with both term logic and predicate logic, and scales up to operate in real time on large dynamic knowledge bases.\n\nThe goal underlying the theoretical development of PLN has been the creation of practical software systems carrying out complex, useful inferences based on uncertain knowledge and drawing uncertain conclusions. PLN has been designed to allow basic probabilistic inference to interact with other kinds of inference such as intensional inference, fuzzy inference, and higher-order inference using quantifiers, variables, and combinators, and be a more convenient approach than Bayesian networks (or other conventional approaches) for the purpose of interfacing basic probabilistic inference with these other sorts of inference. In addition, the inference rules are formulated in such a way as to avoid the paradoxes of Dempster-Shafer theory.\n\nPLN begins with a term logic foundation, and then adds on elements of probabilistic and combinatory logic, as well as some aspects of predicate logic and autoepistemic logic, to form a complete inference system, tailored for easy integration with software components embodying other (not explicitly logical) aspects of intelligence.\n\nPLN represents truth values as intervals, but with different semantics than in Imprecise Probability Theory. In addition to the interpretation of truth in a probabilistic fashion, a truth value in PLN also has an associated amount of \"certainty\". This generalizes the notion of truth values used in autoepistemic logic, where truth values are either known or unknown, and when known, are either true or false.\n\nThe current version of PLN has been used in narrow-AI applications such as the inference of biological hypotheses from knowledge extracted from biological texts via language processing, and to assist the reinforcement learning of an embodied agent, in a simple virtual world, as it is taught to play \"fetch\".\n\n\n", "id": "19890479", "title": "Probabilistic logic network"}
{"url": "https://en.wikipedia.org/wiki?curid=24241139", "text": "Incremental heuristic search\n\nIncremental heuristic search algorithms combine both incremental and heuristic search to speed up searches of sequences of similar search problems, which is important in domains that are only incompletely known or change dynamically. Incremental search has been studied at least since the late 1960s. Incremental search algorithms reuse information from previous searches to speed up the current search and solve search problems potentially much faster than solving them repeatedly from scratch. Similarly, heuristic search has also been studied at least since the late 1960s.\n\nHeuristic search algorithms, often based on A*, use heuristic knowledge in the form of approximations of the goal distances to focus the search and solve search problems potentially much faster than uninformed search algorithms. The resulting search problems, sometimes called dynamic path planning problems, are graph search problems where paths have to be found repeatedly because the topology of the graph, its edge costs, the start vertex or the goal vertices change over time.\n\nSo far, three main classes of incremental heuristic search algorithms have been developed:\n\n\nAll three classes of incremental heuristic search algorithms are different from other replanning algorithms, such as planning by analogy, in that their plan quality does not deteriorate with the number of replanning episodes.\n\nIncremental heuristic search has been extensively used in robotics, where a larger number of path planning systems are based on either D* (typically\nearlier systems) or D* Lite (current systems), two different incremental heuristic search algorithms.\n\n", "id": "24241139", "title": "Incremental heuristic search"}
{"url": "https://en.wikipedia.org/wiki?curid=24251211", "text": "Percept (artificial intelligence)\n\nA percept is the input that an intelligent agent is perceiving at any given moment. It is essentially the same concept as a percept in psychology, except that it is being perceived not by the brain but by the agent. A percept is detected by a sensor, often a camera, processed accordingly, and acted upon by an actuator. Each percept is added to a percept sequence, which is a complete history of each percept ever detected. An intelligent agent chooses how to act not only based on the current percept, but the percept sequence. The next action is chosen by the agent function, which maps every percept to an action.\n\nFor example, if a camera were to record a gesture, the agent would process the percepts, calculate the corresponding spatial vectors, examine its percept history, and use the agent program (the application of the agent function) to act accordingly.\n", "id": "24251211", "title": "Percept (artificial intelligence)"}
{"url": "https://en.wikipedia.org/wiki?curid=12018606", "text": "Synthetic Environment for Analysis and Simulations\n\nSEAS was developed to help Fortune 500 companies with strategic planning. Then it was used to help \"recruiting commanders to strategize ways to improve recruiting potential soldiers\". In 2004 SEAS was evaluated for its ability to help simulate \"the non-kinetic aspects of combat, things like the diplomatic, economic, political, infrastructure and social issues\".\n\nSentient World Simulation is the name given to the current vision of making SEAS a \"continuously running, continually updated mirror model of the real world that can be used to predict and evaluate future events and courses of action.\"\n\nSEAS technology resulted from over ten years of research at Purdue University, funded by the Department of Defense, several Fortune 500 companies, the National Science Foundation, the Century Fund of the state of Indiana, and the Office of Naval Research. Originally, SEAS was developed to help Fortune 500 companies with strategic planning. It was also used to model the population of the U.S. that is eligible for military service to help \"recruiting commanders to strategize ways to improve recruiting potential soldiers\" and to study biological attacks.\n\nIn January 2004 SEAS was evaluated by the Joint Innovation and Experimentation Directorate (J9) of the US Joint Forces Command (JFCOM) for its ability to help simulate \"the non-kinetic aspects of combat, things like the diplomatic, economic, political, infrastructure and social issues\" at the Purdue Technology Park during Breaking Point 2004, an environment-shaping war game resulting in the conclusion that it \"moves us from the current situation where everyone comes together and sits around a table discussing what they would do, to a situation where they actually play in the simulation and their actions have consequences.\"\n\nIn 2006 JFCOM-J9 used SEAS to war game warfare scenarios for Baghdad in 2015. In April 2007 JFCOM-J9 began working with Homeland Security and multinational forces in a homeland defense war gaming exercise.\n\nThe Sentient World Simulation project (SWS) is to be based on SEAS. The ultimate goal envisioned by Alok R. Chaturvedi on March 10, 2006 was for SWS to be a \"continuously running, continually updated mirror model of the real world that can be used to predict and evaluate future events and courses of action. SWS will react to actual events that occur anywhere in the world and incorporate newly sensed data from the real world. [...] As the models influence each other and the shared synthetic environment, behaviors and trends emerge in the synthetic world as they do in the real world. Analysis can be performed on the trends in the synthetic world to validate alternate worldviews. [...] Information can be easily displayed and readily transitioned from one focus to another using detailed modeling, such as engineering level modeling, to aggregated strategic, theater, or campaign-level modeling.\" \n\nAlok R. Chaturvedi is the founder and the Director of SEAS Laboratory as well as the technical lead for the Sentient World Simulation project initiated by US Joint Forces Command.\n\n\n", "id": "12018606", "title": "Synthetic Environment for Analysis and Simulations"}
{"url": "https://en.wikipedia.org/wiki?curid=24571654", "text": "Angel F\n\nAngel_F is a fictional child artificial intelligence that has been used in worldwide art performances focused on the issues of digital liberties, intellectual property and on the evolution of language and behaviour in information society. The character was created by Salvatore Iaconesi in 2007 as a hack to the Biodoll art performance by Italian artist Franca Formenti. \n\nThe project revolves around the story of the technological sensual relationship between the Biodoll, a digital prostitute, and professor Derrick de Kerckhove, culminating in the birth of the young artificial intelligence. The digital child morphed from a spyware program capturing internet users' browsing activities in order to gather text to feed an algorithmic language generating system (thus letting the \"child\" learn how to speak) and became a digital personality used by the artists for both online and real-world performances.\n\nThe project was later joined by Oriana Persico who curated communication and part of the theoretical approaches of the action.\n\nThe Angel_F project has been featured in books, magazines, national televisions, and has been invited to many worldwide conferences and events, both academic and artistic. \n\nOn one occasion, in October 2007, Angel_F attended the Internet Governance Forum (IGF) in Rio de Janeiro as the only digital being invited to speak: the young artificial intelligence was featured with a video message concerning the struggles for digital liberties, privacy, knowledge sharing and access to networks and technologies.\n\nAngel_F is an acronym which stads for \"Autonomous Non Generative E-volitive Life_Form\". The project was born in 2007 and resulted from the fusion of two contemporary art performances.\n\nFranca Formenti, an Italian artist living in Varese, invented the Biodoll character in 2002 which began making its appearances first on the network and later in the physical world by using what were called \"clones\": young women, prostitutes, pornographic starlets, transsexuals and models interpreting the role of a digital prostitute. The Biodoll was an art performance focused on research emerging from the network of new forms of sexualities, and on the analysis of changes brought on by this transformation to the concepts of private and public spaces, privacy, and the possibility of creating multiple fluid identities through language and digital media. At the time the Biodoll character claimed a role in professor Derrick de Kerckhove's life, narratively seducing him and alluring him into joining in the performance, becoming the Biodoll's favourite lover.\n\nThe theme of fertility has always been central to the Biodoll performance: the digital prostitute was a wombless clone but desired giving birth to a son, the Bloki.\n\nIn a process starting in 2006 and ending in February 2007, Salvatore Iaconesi (xDxD.vs.xDxD) used his Talker linguistic artificial intelligence to animate the digital child conceived with prof. Derrick de Kerckhove: \"Angel_F\".\n\nIaconesi and Persico met in November 2006 and immediately started collaborating on the birth of Angel_F.\n\nAngel_F was designed as a synthetic digital being composed through narrative, technological and cognitive psychology layers. The objective was to create iconic characteristics that resulted in being evocative and able to mimic human life up to a level in which bringing up a symbolic dialogue was possible. On the other side, the artificial identity was to implement and expose the cultural, emotional and relational ways that were typical of networked social ecosystems, among those technologies, systems and infrastructures that entered and shaped people's daily lives.\n\nThe young digital being mimicked the evolution of a human baby: initially conceived inside the website of its digital mother it emulated the birth of a child by using the metaphor of a virus developing inside a website, taking progressively more space in the domain's databases and interfaces. Content was produced through the software by using small browser-based spyware techniques, through which Angel_F could infer the list of major portals that had been visited by the website's users.\n\nThe Biodoll website was invaded by this growing presence and, thus, Angel_F was born.\n\nThe Artificial Intelligence (AI) component of Angel_F was derived from another project, Talker, through which internet users could build up the AI's linguistic network by feeding it their text and web clips. Angel_F used this component to generate sentences and phrases, publishing them on the interface and on selected blogs.\n\nThe parallel between the growth of the AI and that of a child kept building up and, just as children learn how to speak and act by observing their parents and the people around them, Angel_F used its spyware and AI components to learn, to navigate websites and web portals using web crawler based techniques, and to interact with other people by using the contents hosted and generated in its database to create surreal dialogues in blogs and websites.\n\nA virtual school was created, called Talker Mind, to narratively continue the AI's growth. Five professors (Massimo Canevacci, Antonio Caronia, Carlo Formenti, Derrick de Kerckhove and Luigi Pagliarini) fed their texts and academic articles to Angel_F, simulating virtual asynchronous lessons by using a multi-blog structure.\n\nA peer-to-peer system was also created at the time, named Presence. Its interface resembled the one of 8-bit videogames and the peer to peer users travelled in a starry space and were able to perform standard Instant Messaging tasks, such as chat and file sharing. The interactions were possible both among humans and digital beings. Angel_F was the first user of the Presence peer to peer system.\n\nAngel_F entered the physical world as a baby-stroller mounted laptop computer that was used to let the digital child join events and conferences held worldwide.\n\nThe first public space performance was held in Milan, when the Biodoll distributed a generative \"free press\" publication (called the Bloki FreePreXXX, its text was generated algorithmically and inserted into a prepared graphic layout).\n\nThe second performance was held in Rome, at the Forte Prenestino on June 14, 2007, with a massive playroom created through computational graphics that people could interact with and that were generated by the AI.\n\nAngel_F performed all over the world, both in artistic contexts and in academic ones. It was also used for the communication strategy of several activist groups on the themes of intellectual property and digital freedoms.\n\nOn June 22, 2007 Angel_F presented the closing remarks for an Ipotesi per Assurdo (Absurd Hypothesis) with Salvatore Iaconesi and Oriana Persico at the IULM University in Milan, discussing the possibilities for an ecosystemic, sustainable reinvention of corporations.\n\nOn July 28, 2007, hundreds of people at LiberaFesta (Free Party) in Rome listened to Angel_F in a speech discussing new politics and \"hacker ethics\".\n\nOn October 2007, Angel_F was presented live at the \"FE/MALE 2\" event, as an example of an atypical family during a public debate on new sexualities and social change.\n\nAgain in October 2007, Angel_F made a series of public performances Florence's Festival della Creatività (Festival of Creativity), an institutional event held periodically to showcase Italy's and other countries' best technological projects. During the festival Derrick de Kerckhove publicly recognized the little AI as his digital son.\n\nOn October 2008, Angel_F was used at a public event on a European scale called Freedom not Fear discussing privacy and civil liberties.\n\nLately (July 2009) Angel_F has been seen with its digital father Derrick de Kerckhove to protest against Italy's harsh politics on freedom of speech.\n\nA case of censorship saw Angel_F involved, when it was not allowed to post its contribution to the DFIR (Dialogue Forum for Internet Rights) held in \"Rome\" on September 2007 in preparation for Rio de Janeiro's Internet Governance Forum (IGF) edition. The case quickly turned into a collaboration among the involved parties and Angel_F was invited to the global event in Brazil where it was the only present digital being: on that occasion Angel_F contributed a videomessage, in the digital freedoms workshop, which suggested some ideas for action to the United Nations and to all the parties involved in the global \"IGF\" organization.\n\nAngel_F mixes real technologically advanced systems with a poetic, narrative approach. In the performance real and not-real are constantly intermixed, and the concepts, techniques and characteristics of the whole project have been analyzed multiple times in several international academic events and research projects.\n\nAngel_F's authors coined the term NeoRealismo Virtuale (Virtual NeoRealism) to identify the practices created and used in the project. Its intent is to describe and represent the contemporary world in its hybrid analog/digital form, with digital technologies progressively interacting and integrating to the physical world, even on the level of personal identity and private/public space definitions.\n\nThe Glocal & Outsiders conference held in Prague at the \"Academy of Sciences\" in 2007 was the first academic presentation of the Angel_F project, together with the \"Biodoll\".\n\nBy December 2007 several international associations, and scientific researchers had been involved with Angel_F, eventually producing the system and process used to set up the \"Talker Mind\" digital school for the AI with Angel_F's professors.\n\nOn March 2008 the Tecnológico de Monterrey university in Mexico City organized the Computer Art Congress 2 international event, featuring Angel_F's project among with the ones by scientific researchers worldwide.\n\nOn July 2008, the project was presented in Austria at the \"Planetary Collegiums Consciousness Reframed 9\"' conference, together with the \"NeoRealismo Virtuale\".\n\nAs of the end of 2009, the project continues, both artistically and academically.\n\nA book is about to be published in November 2009 by \"Castelvecchi Editore\" in Italy, titled Angel F. Diario di una intelligenza artificiale (Angel_F, the diaries of an Artificial Intelligence).\n\n", "id": "24571654", "title": "Angel F"}
{"url": "https://en.wikipedia.org/wiki?curid=6585513", "text": "Outline of artificial intelligence\n\nThe following outline is provided as an overview of and topical guide to artificial intelligence:\n\nArtificial intelligence (AI) – intelligence exhibited by machines or software. It is also the name of the academic field which studies how to create computers and computer software that are capable of intelligent behaviour.\n\n\n\n\nApplications of artificial intelligence – \n\n\nList of artificial intelligence projects\n\n\n\nIntelligent personal assistant – \n\n\nHistory of artificial intelligence\n\n\n\nPhilosophy of artificial intelligence\n\nMarek Rosa Owner of goodAI\n\n\nArtificial intelligence in fiction – Some examples of artificially intelligent entities depicted in science fiction include:\n\nCompetitions and prizes in artificial intelligence\n\nList of important publications in computer science\n\n\n\n\n\n\n\n\n\n\n\n\n", "id": "6585513", "title": "Outline of artificial intelligence"}
{"url": "https://en.wikipedia.org/wiki?curid=2710755", "text": "Fuzzy agent\n\nIn computer science a fuzzy agent is a software agent that implements fuzzy logic. This software entity interacts with its environment through an adaptive rule-base and can therefore be considered as a type of intelligent agent.\n", "id": "2710755", "title": "Fuzzy agent"}
{"url": "https://en.wikipedia.org/wiki?curid=637857", "text": "Symbol level\n\nIn knowledge-based systems, agents choose actions based on the principle of rationality to move closer to a desired goal. The agent is able to make decisions based on knowledge it has about the world (see knowledge level). But for the agent to actually change its state, it must use whatever means it has available. This level of description for the agent's behavior is the symbol level.\n\nFor example, in a computer program, the knowledge level consists of the information contained in its data structures that it uses to perform certain actions. The symbol level consists of the program's algorithms, the data structures themselves, and so on.\n\n", "id": "637857", "title": "Symbol level"}
{"url": "https://en.wikipedia.org/wiki?curid=23997153", "text": "Informatics\n\nInformatics is a branch of information engineering. It involves the practice of information processing and the engineering of information systems, and as an academic field it is an applied form of information science. The field considers the interaction between humans and information alongside the construction of interfaces, organisations, technologies and systems. As such, the field of informatics has great breadth and encompasses many subspecialties, including disciplines of computer science, information systems, information technology and statistics. Since the advent of computers, individuals and organizations increasingly process information digitally. This has led to the study of informatics with computational, mathematical, biological, cognitive and social aspects, including study of the social impact of information technologies.\n\nIn 1956 the German computer scientist Karl Steinbuch coined the word \"Informatik\" by publishing a paper called \"Informatik: Automatische Informationsverarbeitung\" (\"Informatics: Automatic Information Processing\"). The English term \"Informatics\" is sometimes understood as meaning the same as computer science. The German word \"Informatik\" is usually translated to English as \"computer science\".\n\nThe French term \"informatique\" was coined in 1962 by Philippe Dreyfus together with various translations—informatics (English), also proposed independently and simultaneously by Walter F. Bauer and associates who co-founded \"Informatics Inc.\", and \"informatica\" (Italian, Spanish, Romanian, Portuguese, Dutch), referring to the application of computers to store and process information.\n\nThe term was coined as a combination of \"information\" and \"automatic\" to describe the science of automating information interactions.\nThe morphology—\"informat\"-ion + -\"ics\"—uses \"the accepted form for names of sciences, as conics, linguistics, optics, or matters of practice, as economics, politics, tactics\", and so, linguistically, the meaning extends easily to encompass both the science of information and the practice of information processing.\n\nThe culture of library science promotes policies and procedures for managing information that fosters the relationship between library science and the development of information science to provide benefits for health informatics development; which is traced to the 1950s with the beginning of computer uses in healthcare (Nelson & Staggers p.4). Early practitioners interested in the field soon learned that there were no formal education programs set up to educate them on the informatics science until the late 1960s and early 1970s. Professional development began to emerge, playing a significant role in the development of health informatics (Nelson &Staggers p.7)\nAccording to Imhoff et al., 2001, healthcare informatics is not only the application of computer technology to problems in healthcare but covers all aspects of generation, handling, communication, storage, retrieval, management, analysis, discovery, and synthesis of data information and knowledge in the entire scope of healthcare. Furthermore, they stated that the primary goal of health informatics can be distinguished as follows: To provide solutions for problems related to data, information, and knowledge processing. To study general principles of processing data information and knowledge in medicine and healthcare.\n\nReference\nImhoff, M., Webb. A.&Goldschmidt, A., (2001). Health Informatics. Intensive Care Med, 27: 179-186. doi:10.1007//s001340000747.\n\nNelson, R. & Staggers, N. Health Informatics: An Interprofessional Approach. St. Louis: Mosby, 2013. Print. (p.4,7)\n\nThis new term was adopted across Western Europe, and, except in English, developed a meaning roughly translated by the English ‘computer science’, or ‘computing science’. Mikhailov advocated the Russian term \"informatika\" (1966), and the English \"informatics\" (1967), as names for the \"theory of scientific information\", and argued for a broader meaning, including study of the use of information technology in various communities (for example, scientific) and of the interaction of technology and human organizational structures.\n\nUsage has since modified this definition in three ways. First, the restriction to scientific information is removed, as in business informatics or legal informatics. Second, since most information is now digitally stored, computation is now central to informatics. Third, the representation, processing and communication of information are added as objects of investigation, since they have been recognized as fundamental to any scientific account of information. Taking \"information\" as the central focus of study distinguishes \"informatics\" from \"computer science.\" Informatics includes the study of biological and social mechanisms of information processing whereas computer science focuses on the digital computation. Similarly, in the study of representation and communication, informatics is indifferent to the substrate that carries information. For example, it encompasses the study of communication using gesture, speech and language, as well as digital communications and networking.\n\nIn the English-speaking world the term \"informatics\" was first widely used in the compound medical informatics, taken to include \"the cognitive, information processing, and communication tasks of medical practice, education, and research, including information science and the technology to support these tasks\". Many such compounds are now in use; they can be viewed as different areas of \"\"applied informatics\"\".\nIndeed, \"In the U.S., however, informatics is linked with applied computing, or computing in the context of another domain.\"\n\nInformatics encompasses the study of systems that represent, process, and communicate information.\nHowever, the theory of computation in the specific discipline of theoretical computer science, which evolved from Alan Turing, studies the notion of a complex system regardless of whether or not information actually exists. Since both fields process information, there is some disagreement among scientists as to field hierarchy; for example Arizona State University attempted to adopt a broader definition of informatics to even encompass cognitive science at the launch of its School of Computing and Informatics in September 2006.\n\nA broad interpretation of \"informatics\", as \"the study of the structure, algorithms, behaviour, and interactions of natural and artificial computational systems,\" was introduced by the University of Edinburgh in 1994 when it formed the grouping that is now its School of Informatics. This meaning is now (2006) increasingly used in the United Kingdom.\n\nThe 2008 Research Assessment Exercise, of the UK Funding Councils, includes a new, \"Computer Science and Informatics,\" unit of assessment (UoA), whose scope is described as follows:\n\nAcademic research in the informatics area can be found in a number of disciplines such as computer science, information technology, Information and Computer Science, information system, business information management and health informatics.\n\nIn France, the first degree level qualifications in Informatics (computer science) appeared in the mid-1960s.\n\nIn English-speaking countries, the first example of a degree level qualification in Informatics occurred in 1982 when Plymouth Polytechnic (now the University of Plymouth) offered a four-year BSc(Honours) degree in Computing and Informatics – with an initial intake of only 35 students. The course still runs today making it the longest available qualification in the subject.\n\nAt the Indiana University School of Informatics (Bloomington, Indianapolis and Southeast), informatics is defined as \"the art, science and human dimensions of information technology\" and \"the study, application, and social consequences of technology.\" It is also defined in Informatics 101, Introduction to Informatics as \"the application of information technology to the arts, sciences, and professions.\" These definitions are widely accepted in the United States, and differ from British usage in omitting the study of natural computation.\n\nTexas Woman's University places its informatics degrees in its department of Mathematics and Computer Science within the College of Arts & Sciences, though it offers interdisciplinary Health Informatics degrees. Informatics is presented in a generalist framework, as evidenced by their definition of informatics (\"Using technology and data analytics to derive meaningful information from data for data and decision driven practice in user centered systems\"), though TWU is also known for its nursing and health informatics programs.\n\nAt the University of California, Irvine Department of Informatics, informatics is defined as \"the interdisciplinary study of the design, application, use and impact of information technology. The discipline of informatics is based on the recognition that the design of this technology is not solely a technical matter, but must focus on the relationship between the technology and its use in real-world settings. That is, informatics designs solutions in context, and takes into account the social, cultural and organizational settings in which computing and information technology will be used.\"\n\nAt the University of Michigan, Ann Arbor Informatics interdisciplinary major, informatics is defined as \"the study of information and the ways information is used by and affects human beings and social systems. The major involves coursework from the College of Literature, Science and the Arts, where the Informatics major is housed, as well as the School of Information and the College of Engineering. Key to this growing field is that it applies both technological and social perspectives to the study of information. Michigan's interdisciplinary approach to teaching Informatics gives a solid grounding in contemporary computer programming, mathematics, and statistics, combined with study of the ethical and social science aspects of complex information systems. Experts in the field help design new information technology tools for specific scientific, business, and cultural needs.\" Michigan offers four curricular tracks within the informatics degree to provide students with increased expertise. These four track topics include:\n\nAt the University of Washington, Seattle Informatics Undergraduate Program, Informatics is an undergraduate program offered by the Information School. Bachelor of Science in Informatics is described as \"[a] program that focuses on computer systems from a user-centered perspective and studies the structure, behavior and interactions of natural and artificial systems that store, process and communicate information. Includes instruction in information sciences, human computer interaction, information system analysis and design, telecommunications structure and information architecture and management.\" Washington offers three degree options as well as a custom track.\n\nOne of the most significant areas of application of informatics is that of organizational informatics. Organizational informatics is fundamentally interested in the application of information, information systems and ICT within organisations of various forms including private sector, public sector and voluntary sector organisations. As such, organisational informatics can be seen to be a sub-category of social informatics and a super-category of business informatics.\n\n\n", "id": "23997153", "title": "Informatics"}
{"url": "https://en.wikipedia.org/wiki?curid=183503", "text": "Description logic\n\nDescription logics (DL) are a family of formal knowledge representation languages. Many DLs are more expressive than propositional logic but less expressive than First-order logic. In contrast to the latter, the core reasoning problems for DLs are (usually) decidable, and efficient decision procedures have been designed and implemented for these problems. There are general, spatial, temporal, spatiotemporal, and fuzzy descriptions logics, and each description logic features a different balance between DL expressivity and reasoning complexity by supporting different sets of mathematical constructors.\n\nDLs are used in artificial intelligence to describe and reason about the relevant concepts of an application domain (known as \"terminological knowledge\"). It is of particular importance in providing a logical formalism for ontologies and the Semantic Web: the Web Ontology Language (OWL) and its profile is based on DLs. The most notable application of DLs and OWL is in biomedical informatics where DL assists in the codification of biomedical knowledge.\n\nA description logic (DL) models \"concepts\", \"roles\" and \"individuals\", and their relationships.\n\nThe fundamental modeling concept of a DL is the \"axiom\"—a logical statement relating roles and/or concepts. This is a key difference from the frames paradigm where a \"frame specification\" declares and completely defines a class.\n\nThe description logic community uses different terminology than the first-order logic (FOL) community for operationally equivalent notions; some examples are given below. The Web Ontology Language (OWL) uses again a different terminology, also given in the table below.\n\nThere are many varieties of description logics and there is an informal naming convention, roughly describing the operators allowed. The expressivity is encoded in the label for a logic starting with one of the following basic logics:\n\nFollowed by any of the following extensions:\nSome canonical DLs that do not exactly fit this convention are:\nAs an example, formula_1 is a centrally important description logic from which comparisons with other varieties can be made. formula_1 is simply formula_3 with complement of any concept allowed, not just atomic concepts. formula_1 is used instead of the equivalent formula_5.\n\nA further example, the description logic formula_6 is the logic formula_1 plus extended cardinality restrictions, and transitive and inverse roles. The naming conventions aren't purely systematic so that the logic formula_8 might be referred to as formula_9 and abbreviations are made where possible,\n\nThe Protégé ontology editor supports formula_10. Three major biomedical informatics terminology bases, SNOMED CT, GALEN, and GO, are expressible in formula_11 (with additional role properties).\n\nOWL 2 provides the expressiveness of formula_12, OWL-DL is based on formula_10, and for OWL-Lite it is formula_14.\n\nDescription logic was given its current name in the 1980s. Previous to this it was called (chronologically): \"terminological systems\", and \"concept languages\".\n\nFrames and semantic networks lack formal (logic-based) semantics. DL was first introduced into Knowledge Representation (KR) systems to overcome this deficiency.\n\nThe first DL-based KR system was KL-ONE (by Ronald J. Brachman and Schmolze, 1985). During the '80s other DL-based systems using \"structural subsumption algorithms\" were developed including KRYPTON (1983), LOOM (1987), BACK (1988), K-REP (1991) and CLASSIC (1991). This approach featured DL with limited expressiveness but relatively efficient (polynomial time) reasoning.\n\nIn the early '90s, the introduction of a new \"tableau based algorithm\" paradigm allowed efficient reasoning on more expressive DL. DL-based systems using these algorithms - such as KRIS (1991) - show acceptable reasoning performance on typical inference problems even though the worst case complexity is no longer polynomial.\n\nFrom the mid '90s, reasoners were created with good practical performance on very expressive DL with high worst case complexity. Examples from this period include FaCT, RACER (2001), CEL (2005), and KAON 2 (2005).\n\nDL reasoners, such as FaCT, FaCT++, RACER, DLP and Pellet, implement the method of analytic tableaux. KAON2 is implemented by algorithms which reduce a SHIQ(D) knowledge base to a disjunctive datalog program.\n\nThe DARPA Agent Markup Language (DAML) and Ontology Inference Layer (OIL) ontology languages for the Semantic Web can be viewed as\nsyntactic variants of DL. In particular, the formal semantics and reasoning in OIL use the formula_6 DL. The DAML+OIL DL was developed as a submission to—and formed the starting point of—the World Wide Web Consortium (W3C) Web Ontology Working Group. In 2004, the Web Ontology Working Group completed its work by issuing the OWL recommendation. The design of OWL is based on the formula_16 family of DL with OWL DL and OWL Lite based on formula_10 and formula_14 respectively.\n\nThe W3C OWL Working Group began work in 2007 on a refinement of - and extension to - OWL. In 2009, this was completed by the issuance of the OWL2 recommendation. OWL2 is based on the description logic formula_12. Practical experience demonstrated that OWL DL lacked several key features necessary to model complex domains.\n\nIn DL, a distinction is drawn between the so-called TBox (terminological box) and the ABox (assertional box). In general, the TBox contains sentences describing concept hierarchies (i.e., relations between concepts) while the ABox contains ground sentences stating where in the hierarchy individuals belong (i.e., relations between individuals and concepts). For example, the statement:\n\nbelongs in the TBox, while the statement:\n\nbelongs in the ABox.\n\nNote that the TBox/ABox distinction is not significant, in the same sense that the two \"kinds\" of sentences are not treated differently in first-order logic (which subsumes most DL). When translated into first-order logic, a subsumption axiom like () is simply a conditional restriction to unary predicates (concepts) with only variables appearing in it. Clearly, a sentence of this form is not privileged or special over sentences in which only constants (\"grounded\" values) appear like ().\n\nSo why was the distinction introduced? The primary reason is that the separation can be useful when describing and formulating decision-procedures for various DL. For example, a reasoner might process the TBox and ABox separately, in part because certain key inference problems are tied to one but not the other one ('classification' is related to the TBox, 'instance checking' to the ABox). Another example is that the complexity of the TBox can greatly affect the performance of a given decision-procedure for a certain DL, independently of the ABox. Thus, it is useful to have a way to talk about that specific part of the knowledge base.\n\nThe secondary reason is that the distinction can make sense from the knowledge base modeler's perspective. It is plausible to distinguish between our conception of terms/concepts in the world (class axioms in the TBox) and particular manifestations of those terms/concepts (instance assertions in the ABox). In the above example: when the hierarchy within a company is the same in every branch but the assignment to employees is different in every department (because there are other people working there), it makes sense to reuse the TBox for different branches that do not use the same ABox.\n\nThere are two features of description logic that are not shared by most other data description formalisms: DL does not make the Unique name assumption (UNA) or the Closed-world assumption (CWA). Not having UNA means that two concepts with different names may be allowed by some inference to be shown to be equivalent. Not having CWA, or rather having the Open world assumption (OWA) means that lack of knowledge of a fact does not immediately imply knowledge of the negation of a fact.\n\nLike first-order logic (FOL), a syntax defines which collections of symbols are legal expressions in a description logic, and semantics determine meaning. Unlike FOL, a DL may have several well known syntactic variants.\n\nThe syntax of a member of the description logic family is characterized by its recursive definition, in which the constructors that can be used to form concept terms are stated. Some constructors are related to logical constructors in first-order logic (FOL) such as \"intersection\" or \"conjunction\" of concepts, \"union\" or \"disjunction\" of concepts, \"negation\" or \"complement\" of concepts, \"universal restriction\" and \"existential restriction\". Other constructors have no corresponding construction in FOL including restrictions on roles for example, inverse, transitivity and functionality.\n\nLet C and D be concepts, a and b be individuals, and R be a role.\n\nIf a is R-related to b, then b is called an R-successor of a.\n\nThe prototypical DL \"Attributive Concept Language with Complements\" (formula_1) was introduced by Manfred Schmidt-Schauß and Gert Smolka in 1991, and is the basis of many more expressive DLs. The following definitions follow the treatment in Baader et al.\n\nLet formula_21, formula_22 and formula_23 be (respectively) sets of \"concept names\" (also known as \"atomic concepts\"), \"role names\" and \"individual names\" (also known as \"individuals\", \"nominals\" or \"objects\"). Then the ordered triple (formula_21, formula_22, formula_23) is the \"signature\".\n\nThe set of formula_1 \"concepts\" is the smallest set such that:\n\n\nA \"general concept inclusion\" (GCI) has the form formula_39 where formula_31 and formula_32 are \"concepts\". Write formula_42 when formula_39 and formula_44. A \"TBox\" is any finite set of GCIs.\n\n\nAn \"ABox\" is a finite set of assertional axioms.\n\nA \"knowledge base\" (KB) is an ordered pair formula_49 for TBox formula_50 and ABox formula_51.\n\nThe semantics of description logics are defined by interpreting concepts as sets of individuals and roles as sets of ordered pairs of individuals. Those individuals are typically assumed from a given domain. The semantics of non-atomic concepts and roles is then defined in terms of atomic concepts and roles. This is done by using a recursive definition similar to the syntax.\n\nThe following definitions follow the treatment in Baader et al.\n\nA \"terminological interpretation\" formula_52 over a \"signature\" formula_53 consists of\nsuch that\n\nDefine formula_67 (read \"in I holds\") as follows\n\n\n\nLet formula_80 be a knowledge base.\n\n\nIn addition to the ability to describe concepts formally, one also would like to employ the description of a set of concepts to ask questions about the concepts and instances described. The most common decision problems are basic database-query-like questions like \"instance checking\" (is a particular instance (member of an A-box) a member of a given concept) and \"relation checking\" (does a relation/role hold between two instances, in other words does a have property b), and the more global-database-questions like \"subsumption\" (is a concept a subset of another concept), and \"concept consistency\" (is there no contradiction among the definitions or chain of definitions). The more operators one includes in a logic and the more complicated the T-box (having cycles, allowing non-atomic concepts to include each other), usually the higher the computational complexity is for each of these problems (see Description Logic Complexity Navigator for examples).\n\nMany DLs are decidable fragments of first-order logic (FOL) and are usually fragments of two-variable logic or guarded logic. In addition, some DLs have features that are not covered in FOL; this includes \"concrete domains\" (such as integer or strings, which can be used as ranges for roles such as \"hasAge\" or \"hasName\") or an operator on roles for the transitive closure of that role.\n\nFuzzy description logics combines fuzzy logic with DLs. Since many concepts that are needed for intelligent systems lack well defined boundaries, or precisely defined criteria of membership, fuzzy logic is needed to deal with notions of vagueness and imprecision. This offers a motivation for a generalization of description logic towards dealing with imprecise and vague concepts.\n\nDescription logic is related to—but developed independently of—modal logic (ML). Many—but not all—DL are syntactic variants of ML.\n\nIn general, an object corresponds to a possible world, a concept corresponds to a modal proposition, and a role-bounded quantifier to a modal operator with that role as its accessibility relation.\n\nOperations on roles (such as composition, inversion, etc.) correspond to the modal operations used in dynamic logic.\n\nTemporal description logic represents—and allows reasoning about—time dependent concepts and many different approaches to this problem exist. For example, a description logic might be combined with a modal temporal logic such as linear temporal logic.\n\n\n\n\nThere are some semantic reasoners that deal with OWL and DL. These are some of the most popular:\n\n\n", "id": "183503", "title": "Description logic"}
{"url": "https://en.wikipedia.org/wiki?curid=20400528", "text": "Ontology engineering\n\nOntology engineering in computer science and information science is a field which studies the methods and methodologies for building ontologies: formal representations of a set of concepts within a domain and the relationships between those concepts.\nA large-scale representation of abstract concepts such as actions, time, physical objects and beliefs would be an example of ontological engineering. Ontology engineering is one of the areas of applied ontology, and can be seen as an application of philosophical ontology. Core ideas and objectives of ontology engineering are also central in conceptual modeling.\n\nAutomated processing of information not interpretable by software agents can be improved by adding rich semantics to the corresponding resources, such as video files. One of the approaches for the formal conceptualization of represented knowledge domains is the use of machine-interpretable ontologies, which provide structured data in, or based on, RDF, RDFS, and OWL. Ontology engineering is the design and creation of such ontologies, which can contain more than just the list of terms (controlled vocabulary); they contain terminological, assertional, and relational axioms to define concepts (classes), individuals, and roles (properties) (TBox, ABox, and RBox, respectively). Ontology engineering is a relatively new field of study concerning the ontology development process, the ontology life cycle, the methods and methodologies for building ontologies, and the tool suites and languages that support them.\nA common way to provide the logical underpinning of ontologies is to formalize the axioms with description logics, which can then be translated to any serialization of RDF, such as RDF/XML or Turtle. Beyond the description logic axioms, ontologies might also contain SWRL rules. The concept definitions can be mapped to any kind of resource or resource segment in RDF, such as images, videos, and regions of interest, to annotate objects, persons, etc., and interlink them with related resources across knowledge bases, ontologies, and LOD datasets. This information, based on human experience and knowledge, is valuable for reasoners for the automated interpretation of sophisticated and ambiguous contents, such as the visual content of multimedia resources. Application areas of ontology-based reasoning include, but are not limited to, information retrieval, automated scene interpretation, and knowledge discovery.\n\nAn ontology language is a formal language used to encode the ontology. There are a number of such languages for ontologies, both proprietary and standards-based:\n\nLife sciences is flourishing with ontologies that biologists use to make sense of their experiments. For inferring correct conclusions from experiments, ontologies have to be structured optimally against the knowledge base they represent. The structure of an ontology needs to be changed continuously so that it is an accurate representation of the underlying domain.\nRecently, an automated method was introduced for engineering ontologies in life sciences such as Gene Ontology (GO), one of the most successful and widely used biomedical ontology. Based on information theory, it restructures ontologies so that the levels represent the desired specificity of the concepts. Similar information theoretic approaches have also been used for optimal partition of Gene Ontology. Given the mathematical nature of such engineering algorithms, these optimizations can be automated to produce a principled and scalable architecture to restructure ontologies such as GO.\n\nOpen Biomedical Ontologies (OBO), a 2006 initiative of the U.S. National Center for Biomedical Ontology, that provides a common 'foundry' for various ontology initiatives, amongst which are:\nand more\n\n\n\n\n", "id": "20400528", "title": "Ontology engineering"}
{"url": "https://en.wikipedia.org/wiki?curid=27726566", "text": "Virtual intelligence\n\nVirtual intelligence is the term given to artificial intelligence that exists within a virtual world. Many virtual worlds have options for persistent avatars that provide information, training, role playing, and social interactions.\n\nThe immersion of virtual worlds provides a unique platform for VI beyond the traditional paradigm of past user interfaces (UIs). What Alan Turing established as the benchmark for telling the difference between human and computerized intelligence was done void of visual influences. With today's VI bots, virtual intelligence has evolved past the constraints of past testing into a new level of the machine's ability to demonstrate intelligence. The immersive features of these environments provide non verbal elements that affect the realism provided by virtually intelligent agents.\n\nVirtual intelligence is the intersection of these two technologies:\n\nThe virtual environments provide non-verbals and visual cues that can affect not only the believability of the VI, but also the usefulness of it. Because – like many things in technology – it's not just about \"whether or not it works\" but also about \"how we feel about it working\". Virtual Intelligence draws a new distinction as to how this application of AI is different due to the environment in which it operates.\n\n\n", "id": "27726566", "title": "Virtual intelligence"}
{"url": "https://en.wikipedia.org/wiki?curid=237629", "text": "Distributed artificial intelligence\n\nDistributed Artificial Intelligence (DAI) is a subfield of artificial intelligence research dedicated to the development of distributed solutions for complex problems regarded as requiring intelligence. DAI is closely related to and a predecessor of the field of Multi-Agent Systems.\n\nDistributed Artificial Intelligence (DAI) is an approach to solving complex learning, planning, and decision making problems. It is embarrassingly parallel, thus able to exploit large scale computation and spatial distribution of computing resources. These properties allow it to solve problems that require the processing of very large data sets. DAI systems consist of autonomous learning processing nodes (agents), that are distributed, often at a very large scale. DAI nodes can act independently and partial solutions are integrated by communication between nodes, often asynchronously. By virtue of their scale, DAI systems are robust and elastic, and by necessity, loosely coupled. Furthermore, DAI systems are built to be adaptive to changes in the problem definition or underlying data sets due to the scale and difficulty in redeployment.\n\nDAI systems do not require all the relevant data to be aggregated in a single location, in contrast to monolithic or centralized Artificial Intelligence systems which have tightly coupled and geographically close processing nodes. Therefore, DAI systems often operate on sub-samples or hashed impressions of very large datasets. In addition, the source dataset may change or be updated during the course of the execution of a DAI system.\n\nThe objectives of Distributed Artificial Intelligence are to solve the reasoning, planning, learning and perception problems of Artificial Intelligence, especially if they require large data, by distributing the problem to autonomous processing nodes (agents). To reach the objective DAI require:\n\nThere are many reasons for wanting to distribute intelligence or cope with multi-agent systems. Mainstreams in DAI research include the following:\n\nIn the 1975 distributed artificial intelligence emerged as a subfield of artificial intelligence that dealt with interaction of intelligent agents[2]. Distributed artificial intelligence systems were conceived as a group of intelligent entities, called agents, that interacted by cooperation, by coexistence or by competition. DAI is categorized into Multi-agent systems and distributed problem solving [1]. In Multi-agent systems the main focus is how agents coordinate their knowledge and activities. For distributed problem solving the major focus is how the problem is decomposed and the solutions are synthesized.\n\nMulti-agent systems and distributed problem solving are the two main DAI approaches. There are numerous applications and tools.\n\nTwo types of DAI has emerged: \n\nDAI can apply a bottom-up approach to AI, similar to the subsumption architecture as well as the traditional top-down\napproach of AI. In addition, DAI can also be a vehicle for emergence.\n\nAreas where DAI have been applied are:\n\n\nNotion of Agents: Agents can be described as distinct entities with standard boundaries and interfaces designed for problem solving.\n\nNotion of Multi-Agents:Multi-Agent system is defined as a network of agents which are loosely coupled working as a single entity like society for problem solving that an individual agent cannot solve.\n\nThe key concept used in DPS and MABS is the abstraction called software agents. An agent is a virtual (or physical) entity that has an understanding of its environment and acts upon it. An agent is usually able to communicate with other agents in the same system to achieve a common goal, that one agent alone could not achieve. This communication system uses an agent communication language.\n\nA first classification that is useful is to divide agents into:\n\nWell-recognized agent architectures that describe how an agent is internally structured are:\n\nThe challenges in Distributed AI are:\n\n1.How to carry out communication and interaction of agents and which communication language or protocols should be used.\n\n2.How to ensure the coherency of agents.\n\n3.How to synthesise the results among 'intelligent agents' group by formulation, description, decomposition and allocation.\n\n\nArtificial Intelligence Review, 6(1):35-66, 1992.\nintelligence, pages 187-210, 1996.\nand design support. Journal of Intelligent Manufacturing, 11(6):573-589, 2000.\n\n", "id": "237629", "title": "Distributed artificial intelligence"}
{"url": "https://en.wikipedia.org/wiki?curid=25765920", "text": "Artificial intelligence, situated approach\n\nIn artificial intelligence research, the situated approach builds agents that are designed to behave effectively successfully in their environment. This requires designing AI \"from the bottom-up\" by focussing on the basic perceptual and motor skills required to survive. The situated approach gives a much lower priority to abstract reasoning or problem-solving skills.\n\nThe approach was originally proposed as an alternative to traditional approaches (that is, approaches popular before 1985 or so).\nAfter several decades, classical AI technologies started to face intractable issues (e.g. combinatorial explosion) when confronted with real-world modeling problems. All approaches to address these issues focus on modeling intelligences situated in an environment. They have become known as the situated approach to AI.\n\nDuring the late 1980s, the approach now known as Nouvelle AI (\"Nouvelle\" means new in French) was pioneered at the MIT Artificial Intelligence Laboratory by Rodney Brooks. As opposed to classical or traditional artificial intelligence, Nouvelle AI purposely avoided the traditional goal of modeling human-level performance, but rather tries to create systems with intelligence at the level of insects, closer to real-world robots. But eventually, at least at MIT new AI did lead to an attempt for humanoid AI in the Cog Project.\n\nThe conceptual shift introduced by nouvelle AI flourished in the robotics area, given way to behavior-based artificial intelligence (BBAI), a methodology for developing AI based on a modular decomposition of intelligence. It was made famous by Rodney Brooks: his subsumption architecture was one of the earliest attempts to describe a mechanism for developing BBAI. It is extremely popular in robotics and to a lesser extent to implement intelligent virtual agents because it allows the successful creation of real-time dynamic systems that can run in complex environments. For example, it underlies the intelligence of the Sony, Aibo and many RoboCup robot teams.\n\nRealizing that in fact all these approaches were aiming at building not an abstract intelligence, but rather an intelligence situated in a given environment, they have come to be known as the situated approach. In fact, this approach stems out from early insights of Alan Turing, describing the need to build machines equipped with sense organs to learn directly from the real-world instead of focusing on abstract activities, such as playing chess.\n\nClassically, a software entity is defined as a simulated element, able to act on itself and on its environment, and which has an internal representation of itself and of the outside world. An entity can communicate with other entities, and its behavior is the consequence of its perceptions, its representations, and its interactions with the other entities.\n\nSimulating entities in a virtual environment requires simulating the entire process that goes from a perception of the environment, or more generally from a stimulus, to an action on the environment. This process is called the AI loop and technology used to simulate it can be subdivided in two categories. \"Sensorimotor or low-level\" AI deals with either the perception problem (what is perceived?) or the animation problem (how are actions executed?). \"Decisional or high-level\" AI deals with the action selection problem (what is the most appropriate action in response to a given perception, i.e. what is the most appropriate behavior?).\n\nThere are two main approaches in decisional AI. The vast majority of the technologies available on the market, such as planning algorithms, finite state machines (FSA), or expert systems, are based on the traditional or symbolic AI approach. Its main characteristics are:\n\nHowever, the limits of traditional AI, which goal is to build systems that mimic human intelligence, are well-known: inevitably, a combinatorial explosion of the number of rules occurs due to the complexity of the environment. In fact, it is impossible to predict all the situations that will be encountered by an autonomous entity.\n\nIn order to address these issues, another approach to decisional AI, also known as situated or behavioral AI, has been proposed. It does not attempt to model systems that produce deductive reasoning processes, but rather systems that behave realistically in their environment. The main characteristics of this approach are the following:\n\nThe goal of situated AI is to model entities that are autonomous in their environment. This is achieved thanks to both the intrinsic robustness of the control architecture, and its adaptation capabilities to unforeseen situations.\n\nIn artificial intelligence and cognitive science, the term situated refers to an agent which is embedded in an environment. The term \"situated\" is commonly used to refer to robots, but some researchers argue that software agents can also be situated if:\n\n\nExamples might include web-based agents, which can alter data or trigger processes (such as purchases) over the Internet, or virtual-reality bots which inhabit and change virtual worlds, such as Second Life.\n\nBeing situated is generally considered to be part of being embodied, but it is useful to consider each perspective individually. The situated perspective emphasizes that intelligent behavior derives from the environment and the agent's interactions with it. The nature of these interactions are defined by an agent's embodiment.\n\nThe most important attribute of a system driven by situated AI is that the intelligence is controlled by a set of independent semi- modules. In the original systems, each module was actually a separate device or was at least conceived of as running on its own processing thread. Generally, though, the modules are just abstractions. In this respect, situated AI may be seen as a software engineering approach to AI, perhaps akin to object oriented design.\n\nSituated AI is often associated with reactive planning, but the two are not synonymous. Brooks advocated an extreme version of cognitive minimalism which required initially that the behavior modules were finite state machines and thus contained no conventional memory or learning. This is associated with reactive AI because reactive AI requires reacting to the current state of the world, not to an agent's memory or preconception of that world. However, learning is obviously key to realistic strong AI, so this constraint has been relaxed, though not entirely abandoned.\n\nThe situated AI community has presented several solutions to modeling decision-making processes, also known as action selection mechanisms. The first attempt to solve this problem goes back to \"subsumption architectures\", which were in fact more an implementation technique than an algorithm. However, this attempt paved the way to several others, in particular the \"free-flow hierarchies\" and \"activation networks\". A comparison of the structure and performances of these two mechanisms demonstrated the advantage of using \"free-flow hierarchies\" in solving the action selection problem. However, \"motor schemas\" and \"process description languages\" are two other approaches that have been used with success for autonomous robots.\n\n\n\n\n\n\n", "id": "25765920", "title": "Artificial intelligence, situated approach"}
{"url": "https://en.wikipedia.org/wiki?curid=16070200", "text": "Intelligent decision support system\n\nAn intelligent decision support system (IDSS) is a decision support system that makes extensive use of artificial intelligence (AI) techniques. Use of AI techniques in management information systems has a long history – indeed terms such as \"Knowledge-based systems\" (KBS) and \"intelligent systems\" have been used since the early 1980s to describe components of management systems, but the term \"Intelligent decision support system\" is thought to originate with Clyde Holsapple and Andrew Whinston in the late 1970s. Examples of specialized intelligent decision support systems include Flexible manufacturing systems (FMS), intelligent marketing decision support systems and medical diagnosis systems.\n\nIdeally, an intelligent decision support system should behave like a human consultant: supporting decision makers by gathering and analysing evidence, identifying and diagnosing problems, proposing possible courses of action and evaluating such proposed actions. The aim of the AI techniques embedded in an intelligent decision support system is to enable these tasks to be performed by a computer, while emulating human capabilities as closely as possible.\n\nMany IDSS implementations are based on expert systems, a well established type of KBS that encode knowledge and emulate the cognitive behaviours of human experts using predicate logic rules, and have been shown to perform better than the original human experts in some circumstances. Expert systems emerged as practical applications in the 1980s based on research in artificial intelligence performed during the late 1960s and early 1970s. They typically combine knowledge of a particular application domain with an inference capability to enable the system to propose decisions or diagnoses. Accuracy and consistency can be comparable to (or even exceed) that of human experts when the decision parameters are well known (e.g. if a common disease is being diagnosed), but performance can be poor when novel or uncertain circumstances arise.\n\nResearch in AI focused on enabling systems to respond to novelty and uncertainty in more flexible ways is starting to be used in IDSS. For example, intelligent agents that perform complex cognitive tasks without any need for human intervention have been used in a range of decision support applications. Capabilities of these intelligent agents include knowledge sharing, machine learning, data mining, and automated inference. A range of AI techniques such as case based reasoning, rough sets and fuzzy logic have also been used to enable decision support systems to perform better in uncertain conditions.\n\n\n", "id": "16070200", "title": "Intelligent decision support system"}
{"url": "https://en.wikipedia.org/wiki?curid=27928333", "text": "Conference on Semantics in Healthcare and Life Sciences\n\nThe Conference on Semantics in Healthcare and Life Sciences (CSHALS) was a scientific meeting on the practical applications of semantic technology to pharmaceutical R&D, healthcare, and life sciences. The conference was held annually from 2008 to 2014, and was the premier meeting in this domain. In 2015, it as organised as a special interests group (SIG) meeting of the Intelligent Systems for Molecular Biology conference.\n\nConference topics included a World Wide Web Consortium tutorial on semantic web standards. In 2012, CSHALS took on the additional topic of the application of Big Data to healthcare and life sciences, while maintaining its previous focus on semantic technologies.\n\nCSHALS was organized by ISCB (International Society for Computational Biology).\n\n\n", "id": "27928333", "title": "Conference on Semantics in Healthcare and Life Sciences"}
{"url": "https://en.wikipedia.org/wiki?curid=27370978", "text": "Singleton (global governance)\n\nIn futurology, a singleton is a hypothetical world order in which there is a single decision-making agency at the highest level, capable of exerting effective control over its domain, and permanently preventing both internal and external threats to its supremacy. The term has first been defined by Nick Bostrom.\n\nAn artificial general intelligence having undergone an intelligence explosion could form a singleton, as could a world government armed with mind control and social surveillance technologies. A singleton need not directly micromanage everything in its domain; it could allow diverse forms of organization within itself, albeit guaranteed to function within strict parameters. A singleton need not support a civilization, and in fact could obliterate it upon coming to power.\n\nA singleton doesn't necessarily need to be an organisation, government or AI. It could form through all people accepting the same values or goals. These people coming together would form an \"agency\" in the broad sense of the term according to Bostrom. \n\nA singleton has both potential risks and potential benefits. Notably, a suitable singleton could solve world coordination problems that would not otherwise be solvable, opening up otherwise unavailable developmental trajectories for civilization. For example, Ben Goertzel, an AGI researcher, suggests humans may instead decide to create an \"AI Nanny\" with \"mildly superhuman intelligence and surveillance powers\", to protect the human race from existential risks like nanotechnology and to delay the development of other (unfriendly) artificial intelligences until and unless the safety issues are solved. Furthermore, Bostrom suggests that a singleton could hold Darwinian evolutionary pressures in check, preventing agents interested only in reproduction from coming to dominate.\n\nYet Bostrom also regards the possibility of a stable, repressive, totalitarian global regime as a serious existential risk. The very stability of a singleton makes the installation of a \"bad\" singleton especially catastrophic, since the consequences can never be undone. Bryan Caplan writes that \"perhaps an eternity of totalitarianism would be worse than extinction\".\n\nSimilarly Hans Morgenthau stressed that the mechanical development of weapons, transportation, and communication makes \"the conquest of the world technically possible, and they make it technically possible to keep the world in that conquered state\". Its lack was the reason why great ancient empires, though vast, failed to complete universal conquest of their world and perpetuate the conquest. Now, however, this is possible. Technology undoes both geographic and climatic barriers. \"Today no technological obstacle stands in the way of a world-wide empire\", as \"modern technology makes it possible to extend the control of mind and action to every corner of the globe regardless of geography and season.\" Morgenthau continued on the technological progress:\n", "id": "27370978", "title": "Singleton (global governance)"}
{"url": "https://en.wikipedia.org/wiki?curid=1706303", "text": "Recurrent neural network\n\nA recurrent neural network (RNN) is a class of artificial neural network where connections between units form a directed cycle. This allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. This makes them applicable to tasks such as unsegmented, connected handwriting recognition or speech recognition.\nRecurrent neural networks were developed in the 1980s. Hopfield networks were invented by John Hopfield in 1982. In 1993, a neural history compressor system solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time.\n\nLong short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1997 and set accuracy records in multiple applications domains.\n\nAround 2007, LSTM started to revolutionize speech recognition, outperforming traditional models in certain speech applications. In 2009, a Connectionist Temporal Classification (CTC)-trained LSTM network was the first RNN to win pattern recognition contests when it won several competitions in connected handwriting recognition. In 2014, the Chinese search giant Baidu used CTC-trained RNNs to break the Switchboard Hub5'00 speech recognition benchmark without using any traditional speech processing methods.\n\nLSTM also improved large-vocabulary speech recognition and text-to-speech synthesis and was used in Google Android. In 2015, Google's speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM, which was used by Google voice search.\n\nLSTM broke records for improved machine translation, Language Modeling and Multilingual Language Processing. LSTM combined with convolutional neural networks (CNNs) improved automatic image captioning.\n\nRNNs come in many variants.\n\nBasic RNNs are a network of neuron-like nodes, each with a directed (one-way) connection to every other node. Each node (neuron) has a time-varying real-valued activation. Each connection (synapse) has a modifiable real-valued weight. Nodes are either input nodes (receiving data from outside the network), output nodes (yielding results), or hidden nodes (that modify the data en route from input to output).\n\nFor supervised learning in discrete time settings, sequences of real-valued input vectors arrive at the input nodes, one vector at a time. At any given time step, each non-input unit computes its current activation (result) as a nonlinear function of the weighted sum of the activations of all units that connect to it. Supervisor-given target activations can be supplied for some output units at certain time steps. For example, if the input sequence is a speech signal corresponding to a spoken digit, the final target output at the end of the sequence may be a label classifying the digit.\n\nIn reinforcement learning settings, no teacher provides target signals. Instead a fitness function or reward function is occasionally used to evaluate the RNN's performance, which influences its input stream through output units connected to actuators that affect the environment. This might be used to play a game in which progress is measured with the number of points won.\n\nEach sequence produces an error as the sum of the deviations of all target signals from the corresponding activations computed by the network. For a training set of numerous sequences, the total error is the sum of the errors of all individual sequences.\n\nA recursive neural network is created by applying the same set of weights recursively over a differentiable graph-like structure by traversing the structure in topological order. Such networks are typically also trained by the reverse mode of automatic differentiation. They can process distributed representations of structure, such as logical terms. A special case of recursive neural networks is the RNN whose structure corresponds to a linear chain. Recursive neural networks have been applied to natural language processing. The Recursive Neural Tensor Network uses a tensor-based composition function for all nodes in the tree.\n\nThe Hopfield network is an RNN in which all connections are symmetric. It requires stationary inputs and is thus not a general RNN, as it does not process sequences of patterns. It guarantees that it will converge. If the connections are trained using Hebbian learning then the Hopfield network can perform as robust content-addressable memory, resistant to connection alteration.\n\nIntroduced by Kosko, a bidirectional associative memory (BAM) network is a variant of a Hopfield network that stores associative data as a vector. The bi-directionality comes from passing information through a matrix and its transpose. Typically, bipolar encoding is preferred to binary encoding of the associative pairs. Recently, stochastic BAM models using Markov stepping were optimized for increased network stability and relevance to real-world applications.\n\nA BAM network has two layers, either of which can be driven as an input to recall an association and produce an output on the other layer.\n\nAn Elman network is a three-layer network (arranged horizontally as \"x\", \"y\", and \"z\" in the illustration) with the addition of a set of \"context units\" (\"u\" in the illustration). The middle (hidden) layer is connected to these context units fixed with a weight of one. At each time step, the input is fed-forward and a learning rule is applied. The fixed back-connections save a copy of the previous values of the hidden units in the context units (since they propagate over the connections before the learning rule is applied). Thus the network can maintain a sort of state, allowing it to perform such tasks as sequence-prediction that are beyond the power of a standard multilayer perceptron.\n\nJordan networks are similar to Elman networks. The context units are fed from the output layer instead of the hidden layer. The context units in a Jordan network are also referred to as the state layer. They have a recurrent connection to themselves.\n\nElman and Jordan networks are also known as \"simple recurrent networks\" (SRN).\n\n\nVariables and functions\n\nThe echo state network (ESN) has a sparsely connected random hidden layer. The weights of output neurons are the only part of the network that can change (be trained). ESNs are good at reproducing certain time series. A variant for spiking neurons is known as a liquid state machine.\n\nThe neural history compressor is an unsupervised stack of RNNs. At the input level, it learns to predict its next input from the previous inputs. Only unpredictable inputs of some RNN in the hierarchy become inputs to the next higher level RNN, which therefore recomputes its internal state only rarely. Each higher level RNN thus studies a compressed representation of the information in the RNN below. This is done such that the input sequence can be precisely reconstructed from the representation at the highest level.\n\nThe system effectively minimises the description length or the negative logarithm of the probability of the data. Given a lot of learnable predictability in the incoming data sequence, the highest level RNN can use supervised learning to easily classify even deep sequences with long intervals between important events.\n\nIt is possible to distill the RNN hierarchy into two RNNs: the \"conscious\" chunker (higher level) and the \"subconscious\" automatizer (lower level). Once the chunker has learned to predict and compress inputs that are unpredictable by the automatizer, then the automatizer can be forced in the next learning phase to predict or imitate through additional units the hidden units of the more slowly changing chunker. This makes it easy for the automatizer to learn appropriate, rarely changing memories across long intervals. In turn this helps the automatizer to make many of its once unpredictable inputs predictable, such that the chunker can focus on the remaining unpredictable events.\n\nA generative model partially overcame the vanishing gradient problem of automatic differentiation or backpropagation in neural networks in 1992. In 1993, such a system solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time.\n\nLong short-term memory (LSTM) is a deep learning system that avoids the vanishing gradient problem. LSTM is normally augmented by recurrent gates called \"forget\" gates. LSTM prevents backpropagated errors from vanishing or exploding. Instead, errors can flow backwards through unlimited numbers of virtual layers unfolded in space. That is, LSTM can learn tasks that require memories of events that happened thousands or even millions of discrete time steps earlier. Problem-specific LSTM-like topologies can be evolved. LSTM works even given long delays between significant events and can handle signals that mix low and high frequency components.\n\nMany applications use stacks of LSTM RNNs and train them by Connectionist Temporal Classification (CTC) to find an RNN weight matrix that maximizes the probability of the label sequences in a training set, given the corresponding input sequences. CTC achieves both alignment and recognition.\n\nLSTM can learn to recognize context-sensitive languages unlike previous models based on hidden Markov models (HMM) and similar concepts.\n\nSecond order RNNs use higher order weights formula_11 instead of the standard formula_12 weights, and inputs and states can be a product. This allows a direct mapping to a finite state machine both in training, stability, and representation. Long short-term memory is an example of this but has no such formal mappings or proof of stability.\n\nGated recurrent units (GRUs) are a gating mechanism in recurrent neural networks introduced in 2014. Their performance on polyphonic music modeling and speech signal modeling was found to be similar to that of long short-term memory. They have fewer parameters than LSTM, as they lack an output gate.\n\nBi-directional RNNs use a finite sequence to predict or label each element of the sequence based on the element's past and future contexts. This is done by concatenating the outputs of two RNNs, one processing the sequence from left to right, the other one from right to left. The combined outputs are the predictions of the teacher-given target signals. This technique proved to be especially useful when combined with LSTM RNNs.\n\nA continuous time recurrent neural network (CTRNN) uses a system of ordinary differential equations to model the effects on a neuron of the incoming spike train.\n\nFor a neuron formula_13 in the network with action potential formula_14, the rate of change of activation is given by:\nWhere:\n\nCTRNNs have been applied to evolutionary robotics where they have been used to address vision, co-operation, and minimal cognitive behaviour.\n\nNote that, by the Shannon sampling theorem, discrete time recurrent neural networks can be viewed as continuous-time recurrent neural networks where the differential equations have transformed into equivalent difference equations. This transformation can be thought of as occurring after the post-synaptic node activation functions formula_25 have been low-pass filtered but prior to sampling.\n\nHierarchical RNNs connect their neurons in various ways to decompose hierarchical behavior into useful subprograms.\n\nGenerally, a Recurrent Multi-Layer Perceptron (RMLP) network consists of cascaded subnetworks, each of which contains multiple layers of nodes. Each of these subnetworks is feed-forward except for the last layer, which can have feedback connections. Each of these subnets is connected only by feed forward connections.\n\nA multiple timescales recurrent neural network (MTRNN) is a neural-based computational model that can simulate the functional hierarchy of the brain through self-organization that depends on spatial connection between neurons and on distinct types of neuron activities, each with distinct time properties. With such varied neuronal activities, continuous sequences of any set of behaviors are segmented into reusable primitives, which in turn are flexibly integrated into diverse sequential behaviors. The biological approval of such a type of hierarchy was discussed in the memory-prediction theory of brain function by Hawkins in his book \"On Intelligence\".\n\nNeural Turing machines (NTMs) are a method of extending recurrent neural networks by coupling them to external memory resources which they can interact with by attentional processes. The combined system is analogous to a Turing machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent.\n\nDifferentiable neural computers (DNCs) are an extension of Neural Turing machines, allowing for usage of fuzzy amounts of each memory address and a record of chronology.\n\nNeural network pushdown automata (NNPDA) are similar to NTMs, but tapes are replaced by analogue stacks that are differentiable and that are trained. In this way, they are similar in complexity to recognizers of context free grammars (CFGs).\n\nGradient descent is a iterative optimization algorithm for finding the minimum of a function. In neural networks, it can be used to minimize the error term by changing each weight in proportion to the derivative of the error with respect to that weight, provided the non-linear activation functions are differentiable. Various methods for doing so were developed in the 1980s and early 1990s by Werbos, Williams, Robinson, Schmidhuber, Hochreiter, Pearlmutter and others.\n\nThe standard method is called \"backpropagation through time\" or BPTT, and is a generalization of back-propagation for feed-forward networks. Like that method, it is an instance of automatic differentiation in the reverse accumulation mode of Pontryagin's minimum principle. A more computationally expensive online variant is called \"Real-Time Recurrent Learning\" or RTRL, which is an instance of automatic differentiation in the forward accumulation mode with stacked tangent vectors. Unlike BPTT, this algorithm is local in time but not local in space.\n\nIn this context, local in space means that a unit's weight vector can be updated using only information stored in the connected units and the unit itself such that update complexity of a single unit is linear in the dimensionality of the weight vector. Local in time means that the updates take place continually (on-line) and depend only on the most recent time step rather than on multiple time steps within a given time horizon as in BPTT. Biological neural networks appear to be local with respect to both time and space.\n\nFor recursively computing the partial derivatives, RTRL has a time-complexity of O(number of hidden x number of weights) per time step for computing the Jacobian matrices, while BPTT only takes O(number of weights) per time step, at the cost of storing all forward activations within the given time horizon. An online hybrid between BPTT and RTRL with intermediate complexity exists, along with variants for continuous time.\n\nA major problem with gradient descent for standard RNN architectures is that error gradients vanish exponentially quickly with the size of the time lag between important events. LSTM combined with a BPTT/RTRL hybrid learning method attempts to overcome these problems.\n\nThe on-line algorithm called causal recursive backpropagation (CRBP), implements and combines BPTT and RTRL paradigms for locally recurrent networks. It works with the most general locally recurrent networks. The CRBP algorithm can minimize the global error term. This fact improves stability of the algorithm, providing a unifying view on gradient calculation techniques for recurrent networks with local feedback.\n\nOne approach to the computation of gradient information in RNNs with arbitrary architectures is based on signal-flow graphs diagrammatic derivation. It uses the BPTT batch algorithm, based on Lee's theorem for network sensitivity calculations. It was proposed by Wan and Beaufays, while its fast online version was proposed by Campolucci, Uncini and Piazza.\n\nTraining the weights in a neural network can be modeled as a non-linear global optimization problem. A target function can be formed to evaluate the fitness or error of a particular weight vector as follows: First, the weights in the network are set according to the weight vector. Next, the network is evaluated against the training sequence. Typically, the sum-squared-difference between the predictions and the target values specified in the training sequence is used to represent the error of the current weight vector. Arbitrary global optimization techniques may then be used to minimize this target function.\n\nThe most common global optimization method for training RNNs is genetic algorithms, especially in unstructured networks.\n\nInitially, the genetic algorithm is encoded with the neural network weights in a predefined manner where one gene in the chromosome represents one weight link.The whole network is represented as a single chromosome. The fitness function is evaluated as follows: \nMany chromosomes make up the population; therefore, many different neural networks are evolved until a stopping criterion is satisfied. A common stopping scheme is: \nThe stopping criterion is evaluated by the fitness function as it gets the reciprocal of the mean-squared-error from each network during training. Therefore, the goal of the genetic algorithm is to maximize the fitness function, reducing the mean-squared-error.\n\nOther global (and/or evolutionary) optimization techniques may be used to seek a good set of weights, such as simulated annealing or particle swarm optimization.\n\nRNNs may behave chaotically. In such cases, dynamical systems theory may be used for analysis.\n\nThey are in fact recursive neural networks with a particular structure: that of a linear chain. Whereas recursive neural networks operate on any hierarchical structure, combining child representations into parent representations, recurrent neural networks operate on the linear progression of time, combining the previous time step and a hidden representation into the representation for the current time step.\n\nIn particular, RNNs can appear as nonlinear versions of finite impulse response and infinite impulse response filters and also as a nonlinear autoregressive exogenous model (NARX).\n\n\nApplications of Recurrent Neural Networks include:\n\n", "id": "1706303", "title": "Recurrent neural network"}
{"url": "https://en.wikipedia.org/wiki?curid=25011536", "text": "Shyster (expert system)\n\nSHYSTER is a legal expert system developed at the Australian National University in Canberra in 1993. It was written as the doctoral dissertation of James Popple under the supervision of Robin Stanton, Roger Clarke, Peter Drahos, and Malcolm Newey. A full technical report of the expert system, and a book further detailing its development and testing have also been published.\n\nSHYSTER emphasises its pragmatic approach, and posits that a legal expert system need not be based upon a complex model of legal reasoning in order to produce useful advice. Although SHYSTER attempts to model the way in which lawyers argue with cases, it does not attempt to model the way in which lawyers decide which cases to use in those arguments. SHYSTER is of a general design, permitting its operation in different legal domains. It was designed to provide advice in areas of case law that have been specified by a legal expert using a bespoke specification language. Its knowledge of the law is acquired, and represented, as information about cases. It produces its advice by examining, and arguing about, the similarities and differences between cases. It derives its name from Shyster: a slang word for someone who acts in a disreputable, unethical, or unscrupulous way, especially in the practice of law and politics.\n\nSHYSTER is a specific example of a general category of legal expert systems, broadly defined as systems that make use of artificial intelligence (AI) techniques to solve legal problems. Legal AI systems can be divided into two categories: legal retrieval systems and legal analysis systems. SHYSTER belongs to the latter category of legal analysis systems. Legal analysis systems can be further subdivided into two categories: judgment machines and legal expert systems. SHYSTER again belongs to the latter category of legal expert systems. A legal expert system, as Popple uses the term, is a system capable of performing at a level expected of a lawyer: “AI systems which merely assist a lawyer in coming to legal conclusions or preparing legal arguments are not here considered to be legal expert systems; a legal expert system must exhibit some legal expertise itself.\"\n\nDesigned to operate in more than one legal domain, and be of specific use to the common law of Australia, SHYSTER accounts for statute law, case law, and the doctrine of precedent in areas of private law. Whilst it accommodates statute law, it is primarily a case-based system, in contradistinction to rule-based systems like MYCIN. More specifically, it was designed in a manner enabling it to be linked with a rule-based system to form a hybrid system. Although case-based reasoning possesses an advantage over rule-based systems by the elimination of complex semantic networks, it suffers from intractable theoretical obstacles: without some further theory it cannot be predicted what features of a case will turn out to be relevant. Users of SHYSTER therefore require some legal expertise.\n\nRichard Susskind argues that “jurisprudence can and ought to supply the models of law and legal reasoning that are required for computerized [sic] implementation in the process of building all expert systems in law.” Popple, however, believes jurisprudence is of limited value to developers of legal expert systems. He posits that a lawyer must have a model of the law (maybe unarticulated) which includes assumptions about the nature of law and legal reasoning, but that model need not rest on basic philosophical foundations. It may be a pragmatic model, developed through experience within the legal system. Many lawyers perform their work with little or no jurisprudential knowledge, and there is no evidence to suggest that they are worse, or better, at their jobs than lawyers well-versed in jurisprudence. The fact that many lawyers have mastered the process of legal reasoning, without having been immersed in jurisprudence, suggests that it may indeed be possible to develop legal expert systems of good quality without jurisprudential insight. As a pragmatic legal expert system SHYSTER is the embodiment of this belief.\n\nA further example of SHYSTER’s pragmatism is its simple knowledge representation structure. This structure was designed to facilitate specification of different areas of case law using a specification language. Areas of case law are specified in terms of the cases and attributes of importance in those areas. SHYSTER weights its attributes and checks for dependence between them. In order to choose cases upon which to construct its opinions, SHYSTER calculates distances between cases and uses these distances to determine which of the leading cases are nearest to the instant case. To this end SHYSTER can be seen to adopt and expand upon nearest neighbor search methods used in pattern recognition. These nearest cases are used to produce an argument (based on similarities and differences between the cases) about the likely outcome in the instant case. This argument relies on the doctrine of precedent; it assumes that the instant case will be decided the same way as was the nearest case. SHYSTER then uses information about these nearest cases to construct a report. The report that SHYSTER generates makes a prediction and justifies that prediction by reference only to cases and their similarities and differences: the calculations that SHYSTER performs in coming to its opinion do not appear in that opinion. Safeguards are employed to warn users if SHYSTER doubts the veracity of its advice.\n\nSHYSTER was tested in four different and disparate areas of case law. Four specifications were written, each representing an area of Australian law: an aspect of the law of trover; the meaning of “authorization [sic]” in Australian copyright law; the categorisation of employment contracts; and the implication of natural justice in administrative decision-making. SHYSTER was evaluated under five headings: its usefulness, its generality, the quality of its advice, its limitations, and possible enhancements that could be made to it. Despite its simple knowledge representation structure, it has shown itself capable of producing good advice, and its simple structure has facilitated the specification of different areas of law.\n\nAppreciating the difficulties encountered by legal expert systems developers in adequately representing legal knowledge can assist in appreciating the shortcomings of digital rights management technologies. Some academics believe future digital rights management systems may become sophisticated enough to permit exceptions to copyright law. To this end SHYSTER's attempt to model “authorization [sic]” in the Copyright Act can be viewed as pioneering work in this field. The term “authorization [sic]” is undefined in the Copyright Act. Consequently, a number of cases have been before the courts seeking answers as to what conduct amounts to authorisation. The main contexts in which the issue has arisen are analogous to permitted exceptions to copyright currently prevented by most digital rights management technologies: “home taping of recorded materials, photocopying in educational institutions and performing works in public”. When applied to one case concerning compact cassettes, SHYSTER successfully agreed that Amstrad did not authorise the infringement.\n\nPopple highlighted the most obvious avenue of future research using SHYSTER as the development of a rule-based system, and the linking together of that rule-based system with the existing case-based system to form a hybrid system. This intention was eventually realised by Thomas O’Callaghan, the creator of SHYSTER-MYCIN: a hybrid legal expert system first presented at ICAIL '03, 24–28 June 2003 in Edinburgh, Scotland.\n\nMYCIN is an existing medical expert system, which was adapted for use with SHYSTER. MYCIN’s controversial “certainty factor” is not used in SHYSTER-MYCIN. The reason for this is the difficulty in scientifically establishing how certain a fact is in a legal domain. The rule-based approach of the MYCIN part is used to reason with the provisions of an Act of Parliament only. This hybrid system enables the case-based system (SHYSTER) to determine open textured concepts when required by the rule-based system (MYCIN). The ultimate conclusion of this joint endeavour is that a hybrid approach is preferred in the creation of legal expert systems where “it is appropriate to use rule-based reasoning when dealing with statutes, and…case-based reasoning when dealing with cases”.\n\n\n", "id": "25011536", "title": "Shyster (expert system)"}
{"url": "https://en.wikipedia.org/wiki?curid=27669989", "text": "Ensemble averaging (machine learning)\n\nIn machine learning, particularly in the creation of artificial neural networks, ensemble averaging is the process of creating multiple models and combining them to produce a desired output, as opposed to creating just one model. Frequently an ensemble of models performs better than any individual model, because the various errors of the models \"average out.\"\n\nEnsemble averaging is one of the simplest types of committee machines. Along with boosting, it is one of the two major types of static committee machines. In contrast to standard network design in which many networks are generated but only one is kept, ensemble averaging keeps the less satisfactory networks around, but with less weight. The theory of ensemble averaging relies on two properties of artificial neural networks:\n\n\nEnsemble averaging creates a group of networks, each with low bias and high variance, then combines them to a new network with (hopefully) low bias and low variance. It is thus a resolution of the bias-variance dilemma. The idea of combining experts has been traced back to Pierre-Simon Laplace.\n\nThe theory mentioned above gives an obvious strategy: create a set of experts with low bias and high variance, and then average them. Generally, what this means is to create a set of experts with varying parameters; frequently, these are the initial synaptic weights, although other factors (such as the learning rate, momentum etc.) may be varied as well. Some authors recommend against varying weight decay and early stopping. The steps are therefore:\nAlternatively, domain knowledge may be used to generate several \"classes\" of experts. An expert from each class is trained, and then combined.\n\nA more complex version of ensemble average views the final result not as a mere average of all the experts, but rather as a weighted sum. If each expert is formula_1, then the overall result formula_2 can be defined as:\nwhere formula_4 is a set of weights. The optimization problem of finding alpha is readily solved through neural networks, hence a \"meta-network\" where each \"neuron\" is in fact an entire neural network can be trained, and the synaptic weights of the final network is the weight applied to each expert. This is known as a \"linear combination of experts\".\n\nIt can be seen that most forms of neural networks are some subset of a linear combination: the standard neural net (where only one expert is used) is simply a linear combination with all formula_5 and one formula_6. A raw average is where all formula_7 are equal to some constant value, namely one over the total number of experts.\n\nA more recent ensemble averaging method is negative correlation learning, proposed by Y. Liu and X. Yao. Now this method has been widely used in evolutionary computing.\n\n\n\n", "id": "27669989", "title": "Ensemble averaging (machine learning)"}
{"url": "https://en.wikipedia.org/wiki?curid=13911207", "text": "Attributional calculus\n\nAttributional calculus is a logic and representation system defined by Ryszard S. Michalski. It combines elements of predicate logic, propositional calculus, and multi-valued logic. Attributional calculus provides a formal language for \"natural induction\", an inductive learning process whose results are in forms natural to people.\n\nMichalski, R.S., \"ATTRIBUTIONAL CALCULUS: A Logic and Representation Language for Natural Induction,\" Reports of the Machine Learning and Inference Laboratory, MLI 04-2, George Mason University, Fairfax, VA, April, 2004.\n", "id": "13911207", "title": "Attributional calculus"}
{"url": "https://en.wikipedia.org/wiki?curid=20756850", "text": "Collective intelligence\n\nCollective intelligence (CI) is shared or group intelligence that emerges from the collaboration, collective efforts, and competition of many individuals and appears in consensus decision making. The term appears in sociobiology, political science and in context of mass peer review and crowdsourcing applications. It may involve consensus, social capital and formalisms such as voting systems, social media and other means of quantifying mass activity. Collective IQ is a measure of collective intelligence, although it is often used interchangeably with the term collective intelligence. Collective intelligence has also been attributed to bacteria and animals.\n\nIt can be understood as an emergent property from the synergies among: 1) data-information-knowledge; 2) software-hardware; and 3) experts (those with new insights as well as recognized authorities) that continually learns from feedback to produce just-in-time knowledge for better decisions than these three elements acting alone. Or more narrowly as an emergent property between people and ways of processing information. This notion of collective intelligence is referred to as \"symbiotic intelligence\" by Norman Lee Johnson. The concept is used in sociology, business, computer science and mass communications: it also appears in science fiction. Pierre Lévy defines collective intelligence as, \"It is a form of universally distributed intelligence, constantly enhanced, coordinated in real time, and resulting in the effective mobilization of skills. I'll add the following indispensable characteristic to this definition: The basis and goal of collective intelligence is mutual recognition and enrichment of individuals rather than the cult of fetishized or hypostatized communities.\" According to researchers Pierre Lévy and Derrick de Kerckhove, it refers to capacity of networked ICTs (Information communication technologies) to enhance the collective pool of social knowledge by simultaneously expanding the extent of human interactions.\n\nCollective intelligence strongly contributes to the shift of knowledge and power from the individual to the collective. According to Eric S. Raymond (1998) and JC Herz (2005), open source intelligence will eventually generate superior outcomes to knowledge generated by proprietary software developed within corporations (Flew 2008). Media theorist Henry Jenkins sees collective intelligence as an 'alternative source of media power', related to convergence culture. He draws attention to education and the way people are learning to participate in knowledge cultures outside formal learning settings. Henry Jenkins criticizes schools which promote 'autonomous problem solvers and self-contained learners' while remaining hostile to learning through the means of collective intelligence. Both Pierre Lévy (2007) and Henry Jenkins (2008) support the claim that collective intelligence is important for democratization, as it is interlinked with knowledge-based culture and sustained by collective idea sharing, and thus contributes to a better understanding of diverse society.\n\nSimilar to the \"g\" factor (\"g\") for general individual intelligence, a new scientific understanding of collective intelligence aims to extract a general collective intelligence factor c factor for groups indicating a group's ability to perform a wide range of tasks. Definition, operationalization and statistical methods are derived from \"g\". Similarly as \"g\" is highly interrelated with the concept of IQ, this measurement of collective intelligence can be interpreted as intelligence quotient for groups (Group-IQ) even though the score is not a quotient per se. Causes for \"c\" and predictive validity are investigated as well.\n\nWriters who have influenced the idea of collective intelligence include Douglas Hofstadter (1979), Peter Russell (1983), Tom Atlee (1993), Pierre Lévy (1994), Howard Bloom (1995), Francis Heylighen (1995), Douglas Engelbart, Louis Rosenberg, Cliff Joslyn, Ron Dembo, Gottfried Mayer-Kress (2003).\n\nThe concept (although not so named) originated in 1785 with the Marquis de Condorcet, whose \"jury theorem\" states that if each member of a voting group is more likely than not to make a correct decision, the probability that the highest vote of the group is the correct decision increases with the number of members of the group (see Condorcet's jury theorem). Many theorists have interpreted Aristotle's statement in the Politics that \"a feast to which many contribute is better than a dinner provided out of a single purse\" to mean that just as many may bring different dishes to the table, so in a deliberation many may contribute different pieces of information to generate a better decision. Recent scholarship, however, suggests that this was probably not what Aristotle meant but is a modern interpretation based on what we now know about team intelligence.\n\nA precursor of the concept is found in entomologist William Morton Wheeler's observation that seemingly independent individuals can cooperate so closely as to become indistinguishable from a single organism (1911). Wheeler saw this collaborative process at work in ants that acted like the cells of a single beast he called a superorganism.\n\nIn 1912 Émile Durkheim identified society as the sole source of human logical thought. He argued in \"The Elementary Forms of Religious Life\" that society constitutes a higher intelligence because it transcends the individual over space and time. Other antecedents are Vladimir Vernadsky's concept of \"noosphere\" and H.G. Wells's concept of \"world brain\" (see also the term \"global brain\"). Peter Russell, Elisabet Sahtouris, and Barbara Marx Hubbard (originator of the term \"conscious evolution\") are inspired by the visions of a noosphere – a transcendent, rapidly evolving collective intelligence – an informational cortex of the planet. The notion has more recently been examined by the philosopher Pierre Lévy. In a 1962 research report, Douglas Engelbart linked collective intelligence to organizational effectiveness, and predicted that pro-actively 'augmenting human intellect' would yield a multiplier effect in group problem solving: \"Three people working together in this augmented mode [would] seem to be more than three times as effective in solving a complex problem as is one augmented person working alone\". In 1994, he coined the term 'collective IQ' as a measure of collective intelligence, to focus attention on the opportunity to significantly raise collective IQ in business and society.\n\nThe idea of collective intelligence also forms the framework for contemporary democratic theories often referred to as epistemic democracy. Epistemic democratic theories refer to the capacity of the populace, either through deliberation or aggregation of knowledge, to track the truth and relies on mechanisms to synthesize and apply collective intelligence.\n\nHoward Bloom has discussed mass behavior – collective behavior from the level of quarks to the level of bacterial, plant, animal, and human societies. He stresses the biological adaptations that have turned most of this earth's living beings into components of what he calls \"a learning machine\". In 1986 Bloom combined the concepts of apoptosis, parallel distributed processing, group selection, and the superorganism to produce a theory of how collective intelligence works. Later he showed how the collective intelligences of competing bacterial colonies and human societies can be explained in terms of computer-generated \"complex adaptive systems\" and the \"genetic algorithms\", concepts pioneered by John Holland.\n\nBloom traced the evolution of collective intelligence to our bacterial ancestors 1 billion years ago and demonstrated how a multi-species intelligence has worked since the beginning of life. Ant societies exhibit more intelligence, in terms of technology, than any other animal except for humans and co-operate in keeping livestock, for example aphids for \"milking\". Leaf cutters care for fungi and carry leaves to feed the fungi.\n\nDavid Skrbina cites the concept of a 'group mind' as being derived from Plato's concept of panpsychism (that mind or consciousness is omnipresent and exists in all matter). He develops the concept of a 'group mind' as articulated by Thomas Hobbes in \"Leviathan\" and Fechner's arguments for a collective consciousness of mankind. He cites Durkheim as the most notable advocate of a \"collective consciousness\" and Teilhard de Chardin as a thinker who has developed the philosophical implications of the group mind.\n\nTom Atlee focuses primarily on humans and on work to upgrade what Howard Bloom calls \"the group IQ\". Atlee feels that collective intelligence can be encouraged \"to overcome 'groupthink' and individual cognitive bias in order to allow a collective to cooperate on one process – while achieving enhanced intellectual performance.\" George Pór defined the collective intelligence phenomenon as \"the capacity of human communities to evolve towards higher order complexity and harmony, through such innovation mechanisms as differentiation and integration, competition and collaboration.\" Atlee and Pór state that \"collective intelligence also involves achieving a single focus of attention and standard of metrics which provide an appropriate threshold of action\". Their approach is rooted in scientific community metaphor.\n\nThe term group intelligence is sometimes used interchangeably with the term collective intelligence. Anita Woolley presents Collective intelligence as a measure of group intelligence and group creativity. The idea is that a measure of collective intelligence covers a broad range of features of the group, mainly group composition and group interaction. The features of composition that lead to increased levels of collective intelligence in groups include criteria such as higher numbers of women in the group as well as increased diversity of the group.\n\nAtlee and Pór suggest that the field of collective intelligence should primarily be seen as a human enterprise in which mind-sets, a willingness to share and an openness to the value of distributed intelligence for the common good are paramount, though group theory and artificial intelligence have something to offer. Individuals who respect collective intelligence are confident of their own abilities and recognize that the whole is indeed greater than the sum of any individual parts. Maximizing collective intelligence relies on the ability of an organization to accept and develop \"The Golden Suggestion\", which is any potentially useful input from any member. Groupthink often hampers collective intelligence by limiting input to a select few individuals or filtering potential Golden Suggestions without fully developing them to implementation.\n\nRobert David Steele Vivas in \"The New Craft of Intelligence\" portrayed all citizens as \"intelligence minutemen,\" drawing only on legal and ethical sources of information, able to create a \"public intelligence\" that keeps public officials and corporate managers honest, turning the concept of \"national intelligence\" (previously concerned about spies and secrecy) on its head.\nAccording to Don Tapscott and Anthony D. Williams, collective intelligence is mass collaboration. In order for this concept to happen, four principles need to exist;\n\n\n\n\nA new scientific understanding of collective intelligence defines it as a group's general ability to perform a wide range of tasks. Definition, operationalization and statistical methods are similar to the psychometric approach of general individual intelligence. Hereby, an individual's performance on a given set of cognitive tasks is used to measure general cognitive ability indicated by the general intelligence factor \"g\" extracted via factor analysis. In the same vein as \"g\" serves to display between-individual performance differences on cognitive tasks, collective intelligence research aims to find a parallel intelligence factor for groups \"c\" factor' (also called 'collective intelligence factor' (\"CI\")) displaying between-group differences on task performance. The collective intelligence score then is used to predict how this same group will perform on any other similar task in the future. Yet tasks, hereby, refer to mental or intellectual tasks performed by small groups even though the concept is hoped to be transferrable to other performances and any groups or crowds reaching from families to companies and even whole cities. Since individuals' \"g\" factor scores are highly correlated with full-scale IQ scores, which are in turn regarded as good estimates of \"g\", this measurement of collective intelligence can also be seen as an intelligence indicator or quotient respectively for a group (Group-IQ) parallel to an individual's intelligence quotient (IQ) even though the score is not a quotient per se.\n\nMathematically, \"c\" and \"g\" are both variables summarizing positive correlations among different tasks supposing that performance on one task is comparable with performance on other similar tasks. \"c\" thus is a source of variance among groups and can only be considered as a group's standing on the \"c\" factor compared to other groups in a given relevant population. The concept is in contrast to competing hypotheses including other correlational structures to explain group intelligence, such as a composition out of several equally important but independent factors as found in individual personality research.\n\nBesides, this scientific idea also aims to explore the causes affecting collective intelligence, such as group size, collaboration tools or group members' interpersonal skills. The MIT Center for Collective Intelligence, for instance, announced the detection of \"The Genome of Collective Intelligence\" as one of its main goals aiming to develop a \"taxonomy of organizational building blocks, or genes, that can be combined and recombined to harness the intelligence of crowds\".\n\nIndividual intelligence is shown to be genetically and environmentally influenced. Analogously, collective intelligence research aims to explore reasons why certain groups perform more intelligent than other groups given that \"c\" is just moderately correlated with the intelligence of individual group members. According to Woolley et al.'s results, neither team cohesion nor motivation or satisfaction correlated with \"c\". However, they claim that three factors were found as significant correlates: the variance in the number of speaking turns, group members' average social sensitivity and the proportion of females. All three had similar predictive power for \"c\", but only social sensitivity was statistically significant (b=0.33, P=0.05).\n\nThe number speaking turns indicates that \"groups where a few people dominated the conversation were less collectively intelligent than those with a more equal distribution of conversational turn-taking\". Hence, providing multiple team members the chance to speak up made a group more intelligent.\n\nGroup members' social sensitivity was measured via the Reading the Mind in the Eyes Test (RME) and correlated .26 with \"c\". Hereby, participants are asked to detect thinking or feeling expressed in other peoples' eyes presented on pictures and assessed in a multiple choice format. The test aims to measure peoples' theory of mind (ToM), also called 'mentalizing' or 'mind reading', which refers to the ability to attribute mental states, such as beliefs, desires or intents, to other people and in how far people understand that others have beliefs, desires, intentions or perspectives different from their own ones. RME is a ToM test for adults that shows sufficient test-retest reliability and constantly differentiates control groups from individuals with functional autism or Asperger Syndrome. It is one of the most widely accepted and well-validated tests for ToM within adults. ToM can be regarded as an associated subset of skills and abilities within the broader concept of emotional intelligence.\n\nThe proportion of females as a predictor of \"c\" was largely mediated by social sensitivity (Sobel z = 1.93, P= 0.03) which is in vein with previous research showing that women score higher on social sensitivity tests. While a mediation, statistically speaking, clarifies the mechanism underlying the relationship between a dependent and an independent variable, Wolley agreed in an interview with the \"Harvard Business Review\" that these findings are saying that groups of women are smarter than groups of men. However, she relativizes this stating that the actual important thing is the high social sensitivity of group members.\n\nIt is theorized that the collective intelligence factor \"c\" is an emergent property resulting from bottom-up as well as top-down processes. Hereby, bottom-up processes cover aggregated group-member characteristics. Top-down processes cover group structures and norms that influence a group's way of collaborating and coordinating.\n\nTop-down processes cover group interaction, such as structures, processes, and norms. An example of such top-down processes is conversational turn-taking. Research further suggest that collectively intelligent groups communicate more in general as well as more equally; same applies for participation and is shown for face-to-face as well as online groups communicating only via writing.\n\nBottom-up processes include group composition, namely the characteristics of group members which are aggregated to the team level encompassing. An example of such bottom-up processes is the average social sensitivity or the average and maximum intelligence scores of group members. Furthermore, collective intelligence was found to be related to a group's cognitive diversity including thinking styles and perspectives. Groups that are moderately diverse in cognitive style have higher collective intelligence than those who are very similar in cognitive style or very different. Consequently, groups where members are too similar to each other lack the variety of perspectives and skills needed to perform well. On the other hand, groups whose members are too different seem to have difficulties to communicate and coordinate effectively.\n\nFor most of human history, collective intelligence was confined to small tribal groups in which opinions were aggregated through real-time parallel interactions among members. In modern times, mass communication, mass media, and networking technologies have enabled collective intelligence to span massive groups, distributed across continents and time-zones. To accommodate this shift in scale, collective intelligence in large-scale groups been dominated by serialized polling processes such as aggregating up-votes, likes, and ratings over time. While modern systems benefit from larger group size, the serialized process has been found to introduce substantial noise that distorts the collective output of the group. In one significant study of serialized collective intelligence, it was found that the first vote contributed to a serialized voting system can distort the final result by 34%.\n\nTo address the problems of serialized aggregation of input among large-scale groups, recent advancements collective intelligence have worked to replace serialized votes, polls, and markets, with parallel systems such as \"human swarms\" modeled after synchronous swarms in nature. Based on natural process of Swarm Intelligence, these artificial swarms of networked humans enable participants to work together in parallel to answer questions and make predictions as an emergent collective intelligence. In one high-profile example, a human swarm challenge by CBS Interactive to predict the Kentucky Derby. The swarm correctly predicted the first four horses, in order, defying 542–1 odds and turning a $20 bet into $10,800.\n\nWoolley, Chabris, Pentland, Hashmi, & Malone (2010), the originators of this scientific understanding of collective intelligence, found a single statistical factor for collective intelligence in their research across 192 groups with people randomly recruited from the public. In Woolley et al.'s two initial studies, groups worked together on different tasks from the McGrath Task Circumplex, a well-established taxonomy of group tasks. Tasks were chosen from all four quadrants of the circumplex and included visual puzzles, brainstorming, making collective moral judgments, and negotiating over limited resources. The results in these tasks were taken to conduct a factor analysis. Both studies showed support for a general collective intelligence factor \"c\" underlying differences in group performance with an initial eigenvalue accounting for 43% (44% in study 2) of the variance, whereas the next factor accounted for only 18% (20%). That fits the range normally found in research regarding a general individual intelligence factor \"g\" typically accounting for 40% to 50% percent of between-individual performance differences on cognitive tests.\nAfterwards, a more complex criterion task was absolved by each group measuring whether the extracted \"c\" factor had predictive power for performance outside the original task batteries. Criterion tasks were playing checkers (draughts) against a standardized computer in the first and a complex architectural design task in the second study. In a regression analysis using both individual intelligence of group members and \"c\" to predict performance on the criterion tasks, \"c\" had a significant effect, but average and maximum individual intelligence had not. While average (r=0.15, P=0.04) and maximum intelligence (r=0.19, P=0.008) of individual group members were moderately correlated with \"c\", \"c\" was still a much better predictor of the criterion tasks. According to Woolley et al., this supports the existence of a collective intelligence factor \"c,\" because it demonstrates an effect over and beyond group members' individual intelligence and thus that \"c\" is more than just the aggregation of the individual IQs or the influence of the group member with the highest IQ.\n\nEngel et al. (2014) replicated Woolley et al.'s findings applying an accelerated battery of tasks with a first factor in the factor analysis explaining 49% of the between-group variance in performance with the following factors explaining less than half of this amount. Moreover, they found a similar result for groups working together online communicating only via text and confirmed the role of female proportion and social sensitivity in causing collective intelligence in both cases. Similarly to Wolley et al., they also measured social sensitivity with the RME which is actually meant to measure people's ability to detect mental states in other peoples' eyes. The online collaborating participants, however, did neither know nor see each other at all. The authors conclude that scores on the RME must be related to a broader set of abilities of social reasoning than only drawing inferences from other people's eye expressions.\n\nA collective intelligence factor \"c\" in the sense of Woolley et al. was further found in groups of MBA students working together over the course of a semester, in online gaming groups as well as in groups from different cultures and groups in different contexts in terms of short-term versus long-term groups. None of these investigations considered team members' individual intelligence scores as control variables.\n\nNote as well that the field of collective intelligence research is quite young and published empirical evidence is relatively rare yet. However, various proposals and working papers are in progress or already completed but (supposedly) still in a scholarly peer reviewing publication process.\n\nNext to predicting a group's performance on more complex criterion tasks as shown in the original experiments, the collective intelligence factor \"c\" was also found to predict group performance in diverse tasks in MBA classes lasting over several months. Thereby, highly collectively intelligent groups earned significantly higher scores on their group assignments although their members did not do any better on other individually performed assignments. Moreover, highly collective intelligent teams improved performance over time suggesting that more collectively intelligent teams learn better. This is another potential parallel to individual intelligence where more intelligent people are found to acquire new material quicker.\n\nIndividual intelligence can be used to predict plenty of life outcomes from school attainment and career success to health outcomes and even mortality. Whether collective intelligence is able to predict other outcomes besides group performance on mental tasks has still to be investigated.\n\nGladwell (2008) showed that the relationship between individual IQ and success works only to a certain point and that additional IQ points over an estimate of IQ 120 do not translate into real life advantages. If a similar border exists for Group-IQ or if advantages are linear and infinite, has still to be explored. Similarly, demand for further research on possible connections of individual and collective intelligence exists within plenty of other potentially transferable logics of individual intelligence, such as, for instance, the development over time or the question of improving intelligence. Whereas it is controversial whether human intelligence can be enhanced via training, a group's collective intelligence potentially offers simpler opportunities for improvement by exchanging team members or implementing structures and technologies. Moreover, social sensitivity was found to be, at least temporarily, improvable by reading literary fiction as well as watching drama movies. In how far such training ultimately improves collective intelligence through social sensitivity remains an open question.\n\nThere are further more advanced concepts and factor models attempting to explain individual cognitive ability including the categorization of intelligence in fluid and crystallized intelligence or the hierarchical model of intelligence differences. Further supplementing explanations and conceptualizations for the factor structure of the Genomes of collective intelligence besides a general \"'c\" factor', though, are missing yet.\n\nOther scholars explain team performance by aggregating team members' general intelligence to the team level instead of building an own overall collective intelligence measure. Devine and Philips (2001) showed in a meta-analysis that mean cognitive ability predicts team performance in laboratory settings (.37) as well as field settings (.14) – note that this is only a small effect. Suggesting a strong dependence on the relevant tasks, other scholars showed that tasks requiring a high degree of communication and cooperation are found to be most influenced by the team member with the lowest cognitive ability. Tasks in which selecting the best team member is the most successful strategy, are shown to be most influenced by the member with the highest cognitive ability.\n\nSince Woolley et al.'s results do not show any influence of group satisfaction, group cohesiveness, or motivation, they, at least implicitly, challenge these concepts regarding the importance for group performance in general and thus contrast meta-analytically proven evidence concerning the positive effects of group cohesion, motivation and satisfaction on group performance.\n\nNoteworthy is also that the involved researchers among the confirming findings widely overlap with each other and with the authors participating in the original first study around Anita Woolley.\n\nIn 2001, Tadeusz (Tad) Szuba from the AGH University in Poland proposed a formal model for the phenomenon of collective intelligence. It is assumed to be an unconscious, random, parallel, and distributed computational process, run in mathematical logic by the social structure.\n\nIn this model, beings and information are modeled as abstract information molecules carrying expressions of mathematical logic. They are quasi-randomly displacing due to their interaction with their environments with their intended displacements. Their interaction in abstract computational space creates multi-thread inference process which we perceive as collective intelligence. Thus, a non-Turing model of computation is used. This theory allows simple formal definition of collective intelligence as the property of social structure and seems to be working well for a wide spectrum of beings, from bacterial colonies up to human social structures. Collective intelligence considered as a specific computational process is providing a straightforward explanation of several social phenomena. For this model of collective intelligence, the formal definition of IQS (IQ Social) was proposed and was defined as \"the probability function over the time and domain of N-element inferences which are reflecting inference activity of the social structure\". While IQS seems to be computationally hard, modeling of social structure in terms of a computational process as described above gives a chance for approximation. Prospective applications are optimization of companies through the maximization of their IQS, and the analysis of drug resistance against collective intelligence of bacterial colonies.\n\nOne measure sometimes applied, especially by more artificial intelligence focused theorists, is a \"collective intelligence quotient\" (or \"cooperation quotient\") – which can be normalized from the \"individual\" intelligence quotient (IQ) – thus making it possible to determine the marginal intelligence added by each new individual participating in the collective action, thus using metrics to avoid the hazards of group think and stupidity.\n\n\"Elicitation of point estimates\" – Here, we try to get an estimate (in a single value) of something. For example, estimating the weight of an object, or the release date of a product or probability of success of a project etc. as are seen in prediction markets like Intrade, HSX or InklingMarkets and also in several implementations of crowdsourced estimation of a numeric outcome. Essentially, we try to get the average value of the estimates provided by the members in the crowd.\n\n\"Opinion Aggregation\" – In this situation, we gather opinions from the crowd regarding some idea, issue or product. For example, trying to get a rating (on some scale) of a product sold online (such as Amazon’s star rating system). Here, the emphasis is to collect and simply aggregate the ratings provided by customers/users.\n\n\"Idea Collection\" – In these problems, someone solicits ideas for projects, designs or solutions from the crowd. For example, ideas on solving a data science problem (as in Kaggle) or getting a good design for a T-shirt (as in Threadless) or in getting answers to simple problems that only humans can do well (as in Amazon’s Mechanical Turk). Here, the objective is to gather the ideas and devise some selection criteria to choose the best ideas.\n\nJames Surowiecki divides the advantages of disorganized decision-making into three main categories, which are cognition, cooperation and coordination.\n\nBecause of the Internet's ability to rapidly convey large amounts of information throughout the world, the use of collective intelligence to predict stock prices and stock price direction has become increasingly viable. Websites aggregate stock market information that is as current as possible so professional or amateur stock analysts can publish their viewpoints, enabling amateur investors to submit their financial opinions and create an aggregate opinion. The opinion of all investor can be weighed equally so that a pivotal premise of the effective application of collective intelligence can be applied: the masses, including a broad spectrum of stock market expertise, can be utilized to more accurately predict the behavior of financial markets.\n\nCollective intelligence underpins the efficient-market hypothesis of Eugene Fama – although the term collective intelligence is not used explicitly in his paper. Fama cites research conducted by Michael Jensen in which 89 out of 115 selected funds underperformed relative to the index during the period from 1955 to 1964. But after removing the loading charge (up-front fee) only 72 underperformed while after removing brokerage costs only 58 underperformed. On the basis of such evidence index funds became popular investment vehicles using the collective intelligence of the market, rather than the judgement of professional fund managers, as an investment strategy.\n\nPolitical parties mobilize large numbers of people to form policy, select candidates and finance and run election campaigns. Knowledge focusing through various voting methods allows perspectives to converge through the assumption that uninformed voting is to some degree random and can be filtered from the decision process leaving only a residue of informed consensus. Critics point out that often bad ideas, misunderstandings, and misconceptions are widely held, and that structuring of the decision process must favor experts who are presumably less prone to random or misinformed voting in a given context.\n\nCompanies such as Affinnova (acquired by Nielsen), Google, InnoCentive, Marketocracy, and Threadless have successfully employed the concept of collective intelligence in bringing about the next generation of technological changes through their research and development (R&D), customer service, and knowledge management. An example of such application is Google's Project Aristotle in 2012, where the effect of collective intelligence on team makeup was examined in hundreds of the company's R&D teams.\n\nIn 2012, the \"Global Futures Collective Intelligence System\" (GFIS) was created by The Millennium Project, which epitomizes collective intelligence as the synergistic intersection among data/information/knowledge, software/hardware, and expertise/insights that has a recursive learning process for better decision-making than the individual players alone.\n\nNew media are often associated with the promotion and enhancement of collective intelligence. The ability of new media to easily store and retrieve information, predominantly through databases and the Internet, allows for it to be shared without difficulty. Thus, through interaction with new media, knowledge easily passes between sources resulting in a form of collective intelligence. The use of interactive new media, particularly the internet, promotes online interaction and this distribution of knowledge between users.\n\nFrancis Heylighen, Valentin Turchin, and Gottfried Mayer-Kress are among those who view collective intelligence through the lens of computer science and cybernetics. In their view, the Internet enables collective intelligence at the widest, planetary scale, thus facilitating the emergence of a global brain.\n\nThe developer of the World Wide Web, Tim Berners-Lee, aimed to promote sharing and publishing of information globally. Later his employer opened up the technology for free use. In the early '90s, the Internet's potential was still untapped, until the mid-1990s when 'critical mass', as termed by the head of the Advanced Research Project Agency (ARPA), Dr. J.C.R. Licklider, demanded more accessibility and utility. The driving force of this Internet-based collective intelligence is the digitization of information and communication. Henry Jenkins, a key theorist of new media and media convergence draws on the theory that collective intelligence can be attributed to media convergence and participatory culture . He criticizes contemporary education for failing to incorporate online trends of collective problem solving into the classroom, stating \"whereas a collective intelligence community encourages ownership of work as a group, schools grade individuals\". Jenkins argues that interaction within a knowledge community builds vital skills for young people, and teamwork through collective intelligence communities contribute to the development of such skills. Collective intelligence is not merely a quantitative contribution of information from all cultures, it is also qualitative.\n\nLévy and de Kerckhove consider CI from a mass communications perspective, focusing on the ability of networked information and communication technologies to enhance the community knowledge pool. They suggest that these communications tools enable humans to interact and to share and collaborate with both ease and speed (Flew 2008). With the development of the Internet and its widespread use, the opportunity to contribute to knowledge-building communities, such as Wikipedia, is greater than ever before. These computer networks give participating users the opportunity to store and to retrieve knowledge through the collective access to these databases and allow them to \"harness the hive\" Researchers at the MIT Center for Collective Intelligence research and explore collective intelligence of groups of people and computers.\n\nIn this context collective intelligence is often confused with shared knowledge. The former is the sum total of information held individually by members of a community while the latter is information that is believed to be true and known by all members of the community. Collective intelligence as represented by Web 2.0 has less user engagement than collaborative intelligence. An art project using Web 2.0 platforms is \"Shared Galaxy\", an experiment developed by an anonymous artist to create a collective identity that shows up as one person on several platforms like MySpace, Facebook, YouTube and Second Life. The password is written in the profiles and the accounts named \"Shared Galaxy\" are open to be used by anyone. In this way many take part in being one. Another art project using collective intelligence to produce artistic work is Curatron, where a large group of artists together decides on a smaller group that they think would make a good collaborative group. The process is used based on an algorithm computing the collective preferences In creating what he calls 'CI-Art', Nova Scotia based artist Mathew Aldred follows Pierry Lévy's definition of collective intelligence. Aldred's CI-Art event in March 2016 involved over four hundred people from the community of Oxford, Nova Scotia, and internationally. Later work developed by Aldred used the UNU swarm intelligence system to create digital drawings and paintings. The Oxford Riverside Gallery (Nova Scotia) held a public CI-Art event in May 2016, which connected with online participants internationally.\n\nIn social bookmarking (also called collaborative tagging), users assign tags to resources shared with other users, which gives rise to a type of information organisation that emerges from this crowdsourcing process. The resulting information structure can be seen as reflecting the collective knowledge (or collective intelligence) of a community of users and is commonly called a \"Folksonomy\", and the process can be captured by models of collaborative tagging.\n\nRecent research using data from the social bookmarking website Delicious, has shown that collaborative tagging systems exhibit a form of complex systems (or self-organizing) dynamics. Although there is no central controlled vocabulary to constrain the actions of individual users, the distributions of tags that describe different resources has been shown to converge over time to a stable power law distributions. Once such stable distributions form, examining the correlations between different tags can be used to construct simple folksonomy graphs, which can be efficiently partitioned to obtained a form of community or shared vocabularies. Such vocabularies can be seen as a form of collective intelligence, emerging from the decentralised actions of a community of users. The Wall-it Project is also an example of social bookmarking.\n\nResearch performed by Tapscott and Williams has provided a few examples of the benefits of collective intelligence to business:\n\n\nCultural theorist and online community developer, John Banks considered the contribution of online fan communities in the creation of the Trainz product. He argued that its commercial success was fundamentally dependent upon \"the formation and growth of an active and vibrant online fan community that would both actively promote the product and create content- extensions and additions to the game software\".\n\nThe increase in user created content and interactivity gives rise to issues of control over the game itself and ownership of the player-created content. This gives rise to fundamental legal issues, highlighted by Lessig and Bray and Konsynski, such as intellectual property and property ownership rights.\n\nGosney extends this issue of Collective Intelligence in videogames one step further in his discussion of alternate reality gaming. This genre, he describes as an \"across-media game that deliberately blurs the line between the in-game and out-of-game experiences\" as events that happen outside the game reality \"reach out\" into the player's lives in order to bring them together. Solving the game requires \"the collective and collaborative efforts of multiple players\"; thus the issue of collective and collaborative team play is essential to ARG. Gosney argues that the Alternate Reality genre of gaming dictates an unprecedented level of collaboration and \"collective intelligence\" in order to solve the mystery of the game.\n\nCo-operation helps to solve most important and most interesting multi-science problems. In his book, James Surowiecki mentioned that most scientists think that benefits of co-operation have much more value when compared to potential costs. Co-operation works also because at best it guarantees number of different viewpoints. Because of the possibilities of technology global co-operation is nowadays much easier and productive than before. It is clear that, when co-operation goes from university level to global it has significant benefits. \n\nFor example, why do scientists co-operate? Science has become more and more isolated and each science field has spread even more and it is impossible for one person to be aware of all developments. This is true especially in experimental research where highly advanced equipment requires special skills. With co-operation scientists can use information from different fields and use it effectively instead of gathering all the information just by reading by themselves.\"\n\nMilitary, trade unions, and corporations satisfy some definitions of CI – the most rigorous definition would require a capacity to respond to very arbitrary conditions without orders or guidance from \"law\" or \"customers\" to constrain actions. Online advertising companies are using collective intelligence to bypass traditional marketing and creative agencies.\n\nThe UNU open platform for \"human swarming\" (or \"social swarming\") establishes real-time closed-loop systems around groups of networked users molded after biological swarms, enabling human participants to behave as a unified collective intelligence. When connected to UNU, groups of distributed users collectively answer questions and make predictions in real-time. Early testing shows that human swarms can out-predict individuals. In 2016, an UNU swarm was challenged by a reporter to predict the winners of the Kentucky Derby, and successfully picked the first four horses, in order, beating 540 to 1 odds.\n\nSpecialized information sites such as Digital Photography Review or Camera Labs is an example of collective intelligence. Anyone who has an access to the internet can contribute to distributing their knowledge over the world through the specialized information sites.\n\nIn learner-generated context a group of users marshal resources to create an ecology that meets their needs often (but not only) in relation to the co-configuration, co-creation and co-design of a particular learning space that allows learners to create their own context. Learner-generated contexts represent an \"ad hoc\" community that facilitates coordination of collective action in a network of trust. An example of learner-generated context is found on the Internet when collaborative users pool knowledge in a \"shared intelligence space\". As the Internet has developed so has the concept of CI as a shared public forum. The global accessibility and availability of the Internet has allowed more people than ever to contribute and access ideas. (Flew 2008)\n\nGames such as \"The Sims\" Series, and \"Second Life\" are designed to be non-linear and to depend on collective intelligence for expansion. This way of sharing is gradually evolving and influencing the mindset of the current and future generations. For them, collective intelligence has become a norm. In Terry Flew's discussion of 'interactivity' in the online games environment, the ongoing interactive dialogue between users and game developers, he refers to Pierre Lévy's concept of Collective Intelligence and argues this is active in videogames as clans or guilds in MMORPG constantly work to achieve goals. Henry Jenkins proposes that the participatory cultures emerging between games producers, media companies, and the end-users mark a fundamental shift in the nature of media production and consumption. Jenkins argues that this new participatory culture arises at the intersection of three broad new media trends. Firstly, the development of new media tools/technologies enabling the creation of content. Secondly, the rise of subcultures promoting such creations, and lastly, the growth of value adding media conglomerates, which foster image, idea and narrative flow.\n\nImprovisational actors also experience a type of collective intelligence which they term \"group mind\", as theatrical improvisation relies on mutual cooperation and agreement, leading to the unity of \"group mind\".\n\nGrowth of the Internet and mobile telecom has also produced \"swarming\" or \"rendezvous\" events that enable meetings or even dates on demand. The full impact has yet to be felt but the anti-globalization movement, for example, relies heavily on e-mail, cell phones, pagers, SMS and other means of organizing. The Indymedia organization does this in a more journalistic way. Such resources could combine into a form of collective intelligence accountable only to the current participants yet with some strong moral or linguistic guidance from generations of contributors – or even take on a more obviously democratic form to advance shared goal.\n\nA further application of collective intelligence is found in the \"Community Engineering for Innovations\". In such an integrated framework proposed by Ebner et al., idea competitions and virtual communities are combined to better realize the potential of the collective intelligence of the participants, particularly in open-source R&D.\n\nCollective actions or tasks require different amounts of coordination depeding on the complexity of the task. Tasks vary from being highly independent simple tasks that require very little coordination to complex interdependent tasks that are built by many individuals and require a lot of coordination. In the article written by Kittur, Lee and Kraut the writers introduce a problem in cooperation: ”When tasks require high coordination because the work is highly interdependent, having more contributors can increase process losses, reducing the effectiveness of the group below what individual members could optimally accomplish”. Having a team too large the overall effectiveness may suffer even when the extra contributors increase the resources. In the end the overall costs from coordination might overwhelm other costs. \n\nGroup collective intelligence is a property that emerges through coordination from both bottom-up and top-down processes. In a bottom-up process the different characteristics of each member are involved in contributing and enhancing coordination. Top-down processes are more strict and fixed with norms, group structures and routines that in their own way enhance the group’s collective work. \n\nTom Atlee reflects that, although humans have an innate ability to gather and analyze data, they are affected by culture, education and social institutions. A single person tends to make decisions motivated by self-preservation. Therefore, without collective intelligence, humans may drive themselves into extinction based on their selfish needs.\n\nPhillip Brown and Hugh Lauder quotes Bowles and Gintis (1976) that in order to truly define collective intelligence, it is crucial to separate 'intelligence' from IQism. They go on to argue that intelligence is an achievement and can only be developed if allowed to. For example, earlier on, groups from the lower levels of society are severely restricted from aggregating and pooling their intelligence. This is because the elites fear that the collective intelligence would convince the people to rebel. If there is no such capacity and relations, there would be no infrastructure on which collective intelligence is built . This reflects how powerful collective intelligence can be if left to develop.\n\nSkeptics, especially those critical of artificial intelligence and more inclined to believe that risk of bodily harm and bodily action are the basis of all unity between people, are more likely to emphasize the capacity of a group to take action and withstand harm as one fluid mass mobilization, shrugging off harms the way a body shrugs off the loss of a few cells. This strain of thought is most obvious in the anti-globalization movement and characterized by the works of John Zerzan, Carol Moore, and Starhawk, who typically shun academics. These theorists are more likely to refer to ecological and collective wisdom and to the role of consensus process in making ontological distinctions than to any form of \"intelligence\" as such, which they often argue does not exist, or is mere \"cleverness\".\n\nHarsh critics of artificial intelligence on ethical grounds are likely to promote collective wisdom-building methods, such as the new tribalists and the Gaians. Whether these can be said to be collective intelligence systems is an open question. Some, e.g. Bill Joy, simply wish to avoid any form of autonomous artificial intelligence and seem willing to work on rigorous collective intelligence in order to remove any possible niche for AI.\n\nIn contrast to these views, Artificial Intelligence companies such as Amazon Mechanical Turk and CrowdFlower are using collective intelligence and crowdsourcing or consensus-based assessment to collect the enormous amounts of data for machine learning algorithms such as Keras and IBM Watson.\n\nGlobal collective intelligence is also seen as the key in solving the challenges that the humankind faces now and in the future. Climate change is an example of a global issue which collective intelligence is currently trying to tackle. With the help of collective intelligence applications such as online crowdsourcing people across the globe are collaborating in developing solutions to climate change.\n\n\n\n\n\n", "id": "20756850", "title": "Collective intelligence"}
{"url": "https://en.wikipedia.org/wiki?curid=6338699", "text": "Embodied cognitive science\n\nEmbodied cognitive science is an interdisciplinary field of research, the aim of which is to explain the mechanisms underlying intelligent behavior. It comprises three main methodologies: 1) the modeling of psychological and biological systems in a holistic manner that considers the mind and body as a single entity, 2) the formation of a common set of general principles of intelligent behavior, and 3) the experimental use of robotic agents in controlled environments.\n\nEmbodied cognitive science borrows heavily from embodied philosophy and the related research fields of cognitive science, psychology, neuroscience and artificial intelligence. From the perspective of neuroscience, research in this field was led by Gerald Edelman of the Neurosciences Institute at La Jolla, the late Francisco Varela of CNRS in France, and J. A. Scott Kelso of Florida Atlantic University. From the perspective of psychology, research by Michael Turvey, Lawrence Barsalou and Eleanor Rosch. From the perspective of language acquisition, Eric Lenneberg and Philip Rubin at Haskins Laboratories. From the perspective of autonomous agent design, early work is sometimes attributed to Rodney Brooks or Valentino Braitenberg. From the perspective of artificial intelligence, see \"Understanding Intelligence\" by Rolf Pfeifer and Christian Scheier or \"How the body shapes the way we think\", also by Rolf Pfeifer and Josh C. Bongard. From the perspective of philosophy see Andy Clark, Shaun Gallagher, and Evan Thompson.\n\nTuring proposed that a machine may need a human-like body to think and speak:\n\nEmbodied cognitive science is an alternative theory to cognition in which it minimizes appeals to computational theory of mind in favor of greater emphasis on how an organism's body determines how and what it thinks. Traditional cognitive theory is based mainly around symbol manipulation, in which certain inputs are fed into a processing unit that produces an output. These inputs follow certain rules of syntax, from which the processing unit finds semantic meaning. Thus, an appropriate output is produced. For example, a human's sensory organs are its input devices, and the stimuli obtained from the external environment are fed into the nervous system which serves as the processing unit. From here, the nervous system is able to read the sensory information because it follows a syntactic structure, thus an output is created. This output then creates bodily motions and brings forth behavior and cognition. Of particular note is that cognition is sealed away in the brain, meaning that mental cognition is cut off from the external world and is only possible by the input of sensory information.\n\nEmbodied cognitive science differs from the traditionalist approach in that it denies the input-output system. This is chiefly due to the problems presented by the Homunculus argument, which concluded that semantic meaning could not be derived from symbols without some kind of inner interpretation. If some little man in a person's head interpreted incoming symbols, then who would interpret the little man's inputs? Because of the specter of an infinite regress, the traditionalist model began to seem less plausible. Thus, embodied cognitive science aims to avoid this problem by defining cognition in three ways.\n\nThe first aspect of embodied cognition examines the role of the physical body, particularly how its properties affect its ability to think. This part attempts to overcome the symbol manipulation component that is a feature of the traditionalist model. Depth perception for instance can be better explained under the embodied approach due to the sheer complexity of the action. Depth perception requires that the brain detect the disparate retinal images obtained by the distance of the two eyes. In addition, body and head cues complicate this further. When the head is turned in a given direction, objects in the foreground will appear to move against objects in the background. From this, it is said that some kind of visual processing is occurring without the need of any kind of symbol manipulation. This is because the objects appearing to move the foreground are simply appearing to move. This observation concludes then that depth can be perceived with no intermediate symbol manipulation necessary. \n\nA more poignant example exists through examining auditory perception. Generally speaking the greater the distance between the ears, the greater the possible auditory acuity. Also relevant is the amount of density in between the ears, for the strength of the frequency wave alters as it passes through a given medium. The brain's auditory system takes these factors into account as it process information, but again without any need for a symbolic manipulation system. This is because the distance between the ears for example does not need symbols to represent it. The distance itself creates the necessary opportunity for greater auditory acuity. The amount of density between the ears is similar, in that it is the actual amount itself that simply forms the opportunity for frequency alteration. Thus under consideration of the physical properties of the body, a symbolic system is unnecessary and an unhelpful metaphor.\n\nThe second aspect draws heavily from George Lakoff's and Mark Johnson's work on concepts. They argued that humans use metaphors whenever possible to better explain their external world. Humans also have a basic stock of concepts in which other concepts can be derived from. These basic concepts include spatial orientations such as up, down, front, and back. Humans can understand what these concepts mean because they can directly experience them from their own bodies. For example, because human movement revolves around standing erect and moving the body in an up-down motion, humans innately have these concepts of up and down. Lakoff and Johnson contend this is similar with other spatial orientations such as front and back too. As mentioned earlier, these basic stocks of spatial concepts are the basis in which other concepts are constructed. Happy and sad for instance are seen now as being up or down respectively. When someone says they are feeling down, what they are really saying is that they feel sad for example. Thus the point here is that true understanding of these concepts is contingent on whether one can have an understanding of the human body. So the argument goes that if one lacked a human body, they could not possibly know what up or down could mean, or how it could relate to emotional states.\n\n‘[I]magine a spherical being living outside of any gravitational field, with no\nknowledge or imagination of any other kind of experience. What could UP\npossibly mean to such a being?'\n\nWhile this does not mean that such beings would be incapable of expressing emotions in other words, it does mean that they would express emotions differently from humans. Human concepts of happiness and sadness would be different because human would have different bodies. So then an organism's body directly affects how it can think, because it uses metaphors related to its body as the basis of concepts.\n\nA third component of the embodied approach looks at how agents use their immediate environment in cognitive processing. Meaning, the local environment is seen as an actual extension of the body's cognitive process. The example of a personal digital assistant (PDA) is used to better imagine this. Echoing functionalism (philosophy of mind), this point claims that mental states are individuated by their role in a much larger system. So under this premise, the information on a PDA is similar to the information stored in the brain. So then if one thinks information in the brain constitutes mental states, then it must follow that information in the PDA is a cognitive state too. Consider also the role of pen and paper in a complex multiplication problem. The pen and paper are so involved in the cognitive process of solving the problem that it seems ridiculous to say they are somehow different from the process, in very much the same way the PDA is used for information like the brain. Another example examines how humans control and manipulate their environment so that cognitive tasks can be better performed. Leaving one's car keys in a familiar place so they aren't missed for instance, or using landmarks to navigate in an unfamiliar city. Thus, humans incorporate aspects of their environment to aid in their cognitive functioning.\n\nThe value of the embodiment approach in the context of cognitive science is perhaps best explained by Andy Clark. He makes the claim that the brain alone should not be the single focus for the scientific study of cognition\n\nIt is increasingly clear that, in a wide variety of cases, the individual brain should not be the sole locus of cognitive scientific interest. Cognition is not a phenomenon that can be successfully studied while marginalizing the roles of body, world and action.\n\nThe following examples used by Clark will better illustrate how embodied thinking is becoming apparent in scientific thinking.\n\n\"Thunnus\", or tuna, long baffled conventional biologists with its incredible abilities to accelerate quickly and attain great speeds. A biological examination of the tuna shows it is simply not capable of such feats. However, an answer can be found when taking the tuna's embodied state into account. The bluefin tuna is able to take advantage of and exploit its local environment by finding naturally occurring currents to increase its speed. The tuna also uses its own physical body for this end as well, by utilizing its tailfin to create the necessary vortices and pressure so it can accelerate and maintain high speeds. Thus, the bluefin tuna is actively using its local environment for its own ends through the attributes of its physical body.\n\nClark uses the example of the hopping robot constructed by Raibert and Hodgins to demonstrate further the value of the embodiment paradigm. These robots were essentially vertical cylinders with a single hopping foot. The challenge of managing the robot's behavior can be daunting because in addition to the intricacies of the program itself, there were also the mechanical matters regarding how the foot ought to be constructed so that it could hop. An embodied approach makes it easier to see that in order for this robot to function, it must be able to exploit its system to the fullest. That is, the robot's systems should be seen as having dynamic characteristics as opposed to the traditional view that it is merely a command center that just executes actions.\n\nClark distinguishes between two kinds of vision, animate and pure vision. Pure vision is an idea that is typically associated with classical artificial intelligence, in which vision is used to create a rich world model so that thought and reason can be used to fully explore the inner model. In other words, pure vision passively creates the external perceivable world so that the faculties of reason can be better used introspectively. Animate vision, by contrast, sees vision as the means by which real-time action can commence. Animate vision is then more of a vehicle by which visual information is obtained so that actions can be undertaken. Clark points to animate vision as an example of embodiment, because it uses both biological and local environment cues to create an active intelligent process. Consider the Clark's example of going to the drugstore to buy some Kodak film. In one's mind, one is familiar with the Kodak logo and its trademark gold color. Thus, one uses incoming visual stimuli to navigate around the drugstore until one finds the film. Therefore, vision should not be seen as a passive system but rather an active retrieval device that intelligently uses sensory information and local environmental cues to perform specific real-world actions.\n\nInspired by the work of the American psychologist James J. Gibson, this next example emphasizes the importance of action-relevant sensory information, bodily movement, and local environment cues. These three concepts are unified by the concept of affordances, which are possibilities of action provided by the physical world to a given agent. These are in turn determined by the agent's physical body, capacities, and the overall action-related properties of the local environment as well. Clark uses the example of an outfielder in baseball to better illustrate the concept of affordance. Traditional computational models would claim that an outfielder attempting to catch a fly-ball can be calculated by variables such as the running speed of the outfielder and the arc of the baseball. However, Gibson's work shows that a simpler method is possible. The outfielder can catch the ball so long as they adjust their running speed so that the ball continually moves in a straight line in their field of vision. Note that this strategy uses various affordances that are contingent upon the success of the outfielder, including their physical body composition, the environment of the baseball field, and the sensory information obtained by the outfielder. \n\nClark points out here that the latter strategy of catching the ball as opposed to the former has significant implications for perception. The affordance approach proves to be non-linear because it relies upon spontaneous real-time adjustments. On the contrary, the former method of computing the arc of the ball is linear as it follows a sequence of perception, calculation and performing action. Thus, the affordance approach challenges the traditional view of perception by arguing against the notion that computation and introspection are necessary. Instead, it ought to be replaced with the idea that perception constitutes a continuous equilibrium of action adjustment between the agent and the world. Ultimately Clark does not expressly claim this is certain but he does observe the affordance approach can explain adaptive response satisfactorily. This is because they utilize environmental cues made possible by perceptual information that is actively used in the real-time by the agent.\n\nIn the formation of general principles of intelligent behavior, Pfeifer intended to be contrary to older principles given in traditional artificial intelligence. The most dramatic difference is that the principles are applicable only to situated robotic agents in the real world, a domain where traditional artificial intelligence showed the least promise.\n\nPrinciple of Cheap Design and Redundancy: Pfeifer realized that implicit assumptions made by engineers often substantially influence a control architecture's complexity. This insight is reflected in discussions of the scalability problem in robotics. The internal processing needed for some bad architectures can grow out of proportion to new tasks needed of an agent. \nOne of the primary reasons for scalability problems is that the amount of programming and knowledge engineering that the robot designers have to perform grows very rapidly with the complexity of the robot's tasks. There is mounting evidence that pre-programming cannot be the solution to the scalability problem ... The problem is that programmers introduce too many hidden assumptions in the robot's code.\nThe proposed solutions are to have the agent exploit the inherent physics of its environment, to exploit the constraints of its niche, and to have agent morphology based on parsimony and the principle of Redundancy. Redundancy reflects the desire for the error-correction of signals afforded by duplicating like channels. Additionally, it reflects the desire to exploit the associations between sensory modalities. (See redundant modalities). In terms of design, this implies that redundancy should be introduced with respect not only to one sensory modality but to several. It has been suggested that the fusion and transfer of knowledge between modalities can be the basis of reducing the size of the sense data taken from the real world. This again addresses the scalability problem.\n\nPrinciple of Parallel, Loosely-coupled Processes: An alternative to hierarchical methods of knowledge and action selection. This design principle differs most importantly from the Sense-Think-Act cycle of traditional AI. Since it does not involve this famous cycle, it is not affected by the frame problem.\n\nPrinciple of Sensory-Motor Coordination: Ideally, internal mechanisms in an agent should give rise to things like memory and choice-making in an emergent fashion, rather than being prescriptively programmed from the beginning. These kinds of things are allowed to emerge as the agent interacts with the environment. The motto is, build fewer assumptions into the agent's controller now, so that learning can be more robust and idiosyncratic in the future. \n\nPrinciple of Ecological Balance: This is more a theory than a principle, but its implications are widespread. Its claim is that the internal processing of an agent cannot be made more complex unless there is a corresponding increase in complexity of the motors, limbs, and sensors of the agent. In other words, the extra complexity added to the brain of a simple robot will not create any discernible change in its behavior. The robot's morphology must already contain the complexity in itself to allow enough \"breathing room\" for more internal processing to develop.\n\nThe Value Principle: This was the architecture developed in the Darwin III robot of Gerald Edelman. It relies heavily on connectionism.\n\nA traditionalist may argue that objects may be used to aid in cognitive processes, but this does not mean they are part of a cognitive system. Eyeglasses are used to aid in the visual process, but to say they are a part of a larger system would completely redefine what is meant by a visual system. However, supporters of the embodied approach could make the case that if objects in the environment play the functional role of mental states, then the items themselves should not be counted among the mental states.\n\n\n", "id": "6338699", "title": "Embodied cognitive science"}
{"url": "https://en.wikipedia.org/wiki?curid=2884728", "text": "Automated reasoning\n\nAutomated reasoning is an area of computer science and mathematical logic dedicated to understanding different aspects of reasoning. The study of automated reasoning helps produce computer programs that allow computers to reason completely, or nearly completely, automatically. Although automated reasoning is considered a sub-field of artificial intelligence, it also has connections with theoretical computer science, and even philosophy.\n\nThe most developed subareas of automated reasoning are automated theorem proving (and the less automated but more pragmatic subfield of interactive theorem proving) and automated proof checking (viewed as guaranteed correct reasoning under fixed assumptions). Extensive work has also been done in reasoning by analogy induction and abduction.\n\nOther important topics include reasoning under uncertainty and non-monotonic reasoning. An important part of the uncertainty field is that of argumentation, where further constraints of minimality and consistency are applied on top of the more standard automated deduction. John Pollock's OSCAR system is an example of an automated argumentation system that is more specific than being just an automated theorem prover.\n\nTools and techniques of automated reasoning include the classical logics and calculi, fuzzy logic, Bayesian inference, reasoning with maximal entropy and a large number of less formal \"ad hoc\" techniques.\n\nThe development of formal logic played a big role in the field of automated reasoning, which itself led to the development of artificial intelligence. A formal proof is a proof in which every logical inference has been checked back to the fundamental axioms of mathematics. All the intermediate logical steps are supplied, without exception. No appeal is made to intuition, even if the translation from intuition to logic is routine. Thus, a formal proof is less intuitive, and less susceptible to logical errors.\n\nSome consider the Cornell Summer meeting of 1957, which brought together a large number of logicians and computer scientists, as the origin of automated reasoning, or automated deduction. Others say that it began before that with the 1955 Logic Theorist program of Newell, Shaw and Simon, or with Martin Davis’ 1954 implementation of Presburger’s decision procedure (which proved that the sum of two even numbers is even). Automated reasoning, although a significant and popular area of research, went through an \"AI winter\" in the eighties and early nineties. Luckily, it got revived after that. For example, in 2005, Microsoft started using verification technology in many of their internal projects and is planning to include a logical specification and checking language in their 2012 version of Visual C.\n\nPrincipia Mathematica was a milestone work in formal logic written by Alfred North Whitehead and Bertrand Russell. Principia Mathematica - also meaning Principles of Mathematics - was written with a purpose to derive all or some of the mathematical expressions, in terms of symbolic logic. Principia Mathematica was initially published in three volumes in 1910, 1912 and 1913.\n\nLogic Theorist (LT) was the first ever program developed in 1956 by Allen Newell, Cliff Shaw and Herbert A. Simon to \"mimic human reasoning\" in proving theorems and was demonstrated on fifty-two theorems from chapter two of Principia Mathematica, proving thirty-eight of them. In addition to proving the theorems, the program found a proof for one of the theorems that was more elegant than the one provided by Whitehead and Russell. After an unsuccessful attempt at publishing their results, Newell, Shaw, and Herbert reported in their publication in 1958, \"The Next Advance in Operation Research\":\n\nExamples of Formal Proofs\n\n\n\n\nAutomated reasoning has been most commonly used to build automated theorem provers. Oftentimes, however, theorem provers require some human guidance to be effective and so more generally qualify as proof assistants. In some cases such provers have come up with new approaches to proving a theorem. Logic Theorist is a good example of this. The program came up with a proof for one of the theorems in Principia Mathematica that was more efficient (requiring fewer steps) than the proof provided by Whitehead and Russell. Automated reasoning programs are being applied to solve a growing number of problems in formal logic, mathematics and computer science, logic programming, software and hardware verification, circuit design, and many others. The TPTP (Sutcliffe and Suttner 1998) is a library of such problems that is updated on a regular basis. There is also a competition among automated theorem provers held regularly at the CADE conference (Pelletier, Sutcliffe and Suttner 2002); the problems for the competition are selected from the TPTP library.\n\n\n\n\n\n", "id": "2884728", "title": "Automated reasoning"}
{"url": "https://en.wikipedia.org/wiki?curid=28443302", "text": "Knowledge-based configuration\n\nKnowledge-based configuration, or also referred to as product configuration or product customization, is an activity of customising a product to meet the needs of a particular customer. The product in question may consist of mechanical parts, services, and software. Knowledge-based configuration is a major application area for artificial intelligence (AI), and it is based on modelling of the configurations in a manner that allows the utilisation of AI techniques for searching for a valid configuration to meet the needs of a particular customer.\n\nKnowledge-based configuration (of complex products and services) has a long history as an artificial intelligence application area, see, e.g. Informally, configuration can be defined as a \"special case of design activity, where the artifact being configured is assembled from instances of a fixed set of well-defined component types which can be composed conforming to a set of constraints\". Such constraints are representing technical restrictions, restrictions related to economic aspects, and conditions related to production processes. The result of a configuration process is a product configuration (concrete configuration), i.e., a list of instances and in some cases also connections between these instances. Examples of such configurations are computers to be delivered or financial service portfolio offers (e.g., a combination of loan and corresponding risk insurance).\n\nConfiguration systems or also referred to as configurators or mass customization toolkits, are one of the most successfully applied Artificial Intelligence technologies. Examples are the automotive industry, the telecommunication industry, the computer industry, and power electric transformers. Starting with rule-based approaches such as R1/XCON, model-based representations of knowledge (in contrast to rule-based representations) have been developed which strictly separate product domain knowledge from the problem solving one - examples thereof are the constraint satisfaction problem, the boolean satisfiability problem, and different answer set programming (ASP) representations. There are two commonly cited conceptualizations of configuration knowledge. The most important concepts in these are components, ports, resources and functions. This separation of product domain knowledge and problem solving knowledge increased the effectiveness of configuration application development and maintenance, since changes in the product domain knowledge do not affect search strategies and vice versa.\n\nConfigurators are also often considered as \"open innovation toolkits\", i.e., tools which support customers in the product identification phase. In this context customers are innovators who articulate their requirements leading to new innovative products. \"Mass Confusion\" – the overwhelming of customers by a large number of possible solution alternatives (choices) – is a phenomenon which often comes with the application of configuration technologies. This phenomenon motivated the creation of personalized configuration environments taking into account a customer’s knowledge and preferences.\n\nCore configuration, i.e., guiding the user and checking the consistency of user requirements with the knowledge base, solution presentation and translation of configuration results into bill of materials (BOM) are major tasks to be supported by a configurator. Configuration knowledge bases are often built using proprietary languages.\nIn most cases knowledge bases are developed by knowledge engineers who elicit product, marketing and sales knowledge from domain experts. Configuration knowledge bases are composed of a formal description of the structure of the product and further constraints restricting the possible feature and component combinations.\n\nRecently, knowledge based configuration has been extended to service and software configuration. Modeling software configuration has been based on two main approaches: feature modeling, and component-connectors. Kumbang domain ontology combines the previous approaches building on the tradition of knowledge based configuration.\n\n\n\n\n\n", "id": "28443302", "title": "Knowledge-based configuration"}
{"url": "https://en.wikipedia.org/wiki?curid=49180", "text": "Fuzzy logic\n\nFuzzy logic is a form of many-valued logic in which the truth values of variables may be any real number between 0 and 1. It is employed to handle the concept of partial truth, where the truth value may range between completely true and completely false. By contrast, in Boolean logic, the truth values of variables may only be the integer values 0 or 1.\n\nThe term \"fuzzy logic\" was introduced with the 1965 proposal of fuzzy set theory by Lotfi Zadeh. Fuzzy logic had however been studied since the 1920s, as infinite-valued logic—notably by Łukasiewicz and Tarski.\n\nFuzzy logic has been applied to many fields, from control theory to artificial intelligence.\n\nClassical logic only permits conclusions which are either true or false. However, there are also propositions with variable answers, such as one might find when asking a group of people to identify a color. In such instances, the truth appears as the result of reasoning from inexact or partial knowledge in which the sampled answers are mapped on a spectrum.\n\nBoth degrees of truth and probabilities range between 0 and 1 and hence may seem similar at first, but fuzzy logic uses degrees of truth as a mathematical model of \"vagueness\", while probability is a mathematical model of \"ignorance\".\n\nA basic application might characterize various sub-ranges of a continuous variable. For instance, a temperature measurement for anti-lock brakes might have several separate membership functions defining particular temperature ranges needed to control the brakes properly. Each function maps the same temperature value to a truth value in the 0 to 1 range. These truth values can then be used to determine how the brakes should be controlled.\n\nWhile variables in mathematics usually take numerical values, in fuzzy logic applications non-numeric values are often used to facilitate the expression of rules and facts.\n\nA linguistic variable such as \"age\" may accept values such as \"young\" and its antonym \"old\". Because natural languages do not always contain enough value terms to express a fuzzy value scale, it is common practice to modify linguistic values with adjectives or adverbs. For example, we can use the hedges \"rather\" and \"somewhat\" to construct the additional values \"rather old\" or \"somewhat young\".\n\nFuzzification operations can map mathematical input values into fuzzy membership functions. And the opposite de-fuzzifying operations can be used to map a fuzzy output membership functions into a \"crisp\" output value that can be then used for decision or control purposes.\n\n\nIn this image, the meanings of the expressions \"cold\", \"warm\", and \"hot\" are represented by functions mapping a temperature scale. A point on that scale has three \"truth values\"—one for each of the three functions. The vertical line in the image represents a particular temperature that the three arrows (truth values) gauge. Since the red arrow points to zero, this temperature may be interpreted as \"not hot\". The orange arrow (pointing at 0.2) may describe it as \"slightly warm\" and the blue arrow (pointing at 0.8) \"fairly cold\".\n\nFuzzy sets are often defined as triangle or trapezoid-shaped curves. They can also be defined using a sigmoid function. One common case is the standard logistic function defined as \n\nwhich has the following symmetry property\n\nFrom this it follows that\n\nformula_3\n\nFuzzy logic works with membership values in a way that mimics Boolean logic.\n\nTo this end, replacements for basic operators AND, OR, NOT must be available. There are several ways to this. A common replacement is called the \"Zadeh operators\":\n\nFor TRUE/1 and FALSE/0, the fuzzy expressions produce the same result as the Boolean expressions.\n\nThere are also other operators, more linguistic in nature, called \"hedges\" that can be applied. These are generally adverbs such as \"very\", or \"somewhat\", which modify the meaning of a set using a mathematical formula.\n\nHowever, an arbitrary choice table does not always define a fuzzy logic function. In the paper, a criterion has been formulated to recognize whether a given choice table defines a fuzzy logic function and a simple algorithm of fuzzy logic function synthesis has been proposed based on introduced concepts of constituents of minimum and maximum. A fuzzy logic function represents a disjunction of constituents of minimum, where a constituent of minimum is a conjunction of variables of the current area greater than or equal to the function value in this area (to the right of the function value in the inequality, including the function value).\n\nAnother set of AND/OR operators is based on multiplication\nx AND y = x*y\nx OR y = 1-(1-x)*(1-y) = x+y-x*y\n1-(1-x)*(1-y) comes from this:\nx OR y = NOT( AND( NOT(x), NOT(y) ) )\nx OR y = NOT( AND(1-x, 1-y) )\nx OR y = NOT( (1-x)*(1-y) )\nx OR y = 1-(1-x)*(1-y)\n\nIF-THEN rules map input or computed truth values to desired output truth values. Example:\nIF temperature IS very cold THEN fan_speed is stopped\nIF temperature IS cold THEN fan_speed is slow\nIF temperature IS warm THEN fan_speed is moderate\nIF temperature IS hot THEN fan_speed is high\nGiven a certain temperature, the fuzzy variable \"hot\" has a certain truth value, which is copied to the \"high\" variable.\n\nShould an output variable occur in several THEN parts, then the values from the respective IF parts are combined using the OR operator.\n\nThe goal is to get a continuous variable from fuzzy truth values.\n\nThis would be easy if the output truth values were exactly those obtained from fuzzification of a given number.\nSince, however, all output truth values are computed independently, in most cases they do not represent such a set of numbers.\nOne has then to decide for a number that matches best the \"intention\" encoded in the truth value.\nFor example, for several truth values of fan_speed, an actual speed must be found that best fits the computed truth values of the variables 'slow', 'medium' and so on.\n\nThere is no single algorithm for this purpose.\n\nA common algorithm is\n\nSince the fuzzy system output is a consensus of all of the inputs and all of the rules, fuzzy logic systems can be well behaved when input values are not available or are not trustworthy. Weightings can be optionally added to each rule in the rulebase and weightings can be used to regulate the degree to which a rule affects the output values. These rule weightings can be based upon the priority, reliability or consistency of each rule. These rule weightings may be static or can be changed dynamically, even based upon the output from other rules.\n\nMany of the early successful applications of fuzzy logic were implemented in Japan. The first notable application was on the high-speed train in Sendai, in which fuzzy logic was able to improve the economy, comfort, and precision of the ride. It has also been used in recognition of hand written symbols in Sony pocket computers, flight aid for helicopters, controlling of subway systems in order to improve driving comfort, precision of halting, and power economy, improved fuel consumption for automobiles, single-button control for washing machines, automatic motor control for vacuum cleaners with recognition of surface condition and degree of soiling, and prediction systems for early recognition of earthquakes through the Institute of Seismology Bureau of Meteorology, Japan.\n\nIn mathematical logic, there are several formal systems of \"fuzzy logic\", most of which are in the family of t-norm fuzzy logics.\n\nThe most important propositional fuzzy logics are:\n\nThese extend the above-mentioned fuzzy logics by adding universal and existential quantifiers in a manner similar to the way that predicate logic is created from propositional logic. The semantics of the universal (resp. existential) quantifier in t-norm fuzzy logics is the infimum (resp. supremum) of the truth degrees of the instances of the quantified subformula.\n\nThe notions of a \"decidable subset\" and \"recursively enumerable subset\" are basic ones for classical mathematics and classical logic. Thus the question of a suitable extension of them to fuzzy set theory is a crucial one. A first proposal in such a direction was made by E.S. Santos by the notions of \"fuzzy Turing machine\", \"Markov normal fuzzy algorithm\" and \"fuzzy program\" (see Santos 1970). Successively, L. Biacino and G. Gerla argued that the proposed definitions are rather questionable. For example, in one shows that the fuzzy Turing machines are not adequate for fuzzy language theory since there are natural fuzzy languages intuitively computable that cannot be recognized by a fuzzy Turing Machine. Then, they proposed the following definitions. Denote by \"Ü\" the set of rational numbers in [0,1]. Then a fuzzy subset \"s\" : \"S\" formula_4[0,1] of a set \"S\" is recursively enumerable if a recursive map \"h\" : \"S\"×\"N\" formula_4\"Ü\" exists such that, for every \"x\" in \"S\", the function \"h\"(\"x\",\"n\") is increasing with respect to \"n\" and \"s\"(\"x\") = lim \"h\"(\"x\",\"n\").\nWe say that \"s\" is \"decidable\" if both \"s\" and its complement –\"s\" are recursively enumerable. An extension of such a theory to the general case of the L-subsets is possible (see Gerla 2006).\nThe proposed definitions are well related with fuzzy logic. Indeed, the following theorem holds true (provided that the deduction apparatus of the considered fuzzy logic satisfies some obvious effectiveness property).\n\nAny \"axiomatizable\" fuzzy theory is recursively enumerable. In particular, the fuzzy set of logically true formulas is recursively enumerable in spite of the fact that the crisp set of valid formulas is not recursively enumerable, in general. Moreover, any axiomatizable and complete theory is decidable.\n\nIt is an open question to give supports for a \"Church thesis\" for fuzzy mathematics, the proposed notion of recursive enumerability for fuzzy subsets is the adequate one. In order to solve this, an extension of the notions of fuzzy grammar and fuzzy Turing machine are necessary. Another open question is to start from this notion to find an extension of Gödel's theorems to fuzzy logic.\n\nOnce fuzzy relations are defined, it is possible to develop fuzzy relational databases. The first fuzzy relational database, FRDB, appeared in Maria Zemankova's dissertation (1983). Later, some other models arose like the Buckles-Petry model, the Prade-Testemale Model, the Umano-Fukami model or the GEFRED model by J.M. Medina, M.A. Vila et al.\n\nFuzzy querying languages have been defined, such as the SQLf by P. Bosc et al. and the FSQL by J. Galindo et al. These languages define some structures in order to include fuzzy aspects in the SQL statements, like fuzzy conditions, fuzzy comparators, fuzzy constants, fuzzy constraints, fuzzy thresholds, linguistic labels etc.\n\nFuzzy logic and probability address different forms of uncertainty. While both fuzzy logic and probability theory can represent degrees of certain kinds of subjective belief, fuzzy set theory uses the concept of fuzzy set membership, i.e., how much an observation is within a vaguely defined set, and probability theory uses the concept of subjective probability, i.e., likelihood of some event or condition. The concept of fuzzy sets was developed in the mid-twentieth century at Berkeley as a response to the lacking of probability theory for jointly modelling uncertainty and vagueness.\n\nBart Kosko claims in Fuzziness vs. Probability that probability theory is a subtheory of fuzzy logic, as questions of degrees of belief in mutually-exclusive set membership in probability theory can be represented as certain cases of non-mutually-exclusive graded membership in fuzzy theory. In that context, he also derives Bayes' theorem from the concept of fuzzy subsethood. Lotfi A. Zadeh argues that fuzzy logic is different in character from probability, and is not a replacement for it. He fuzzified probability to fuzzy probability and also generalized it to possibility theory. (cf.)\n\nMore generally, fuzzy logic is one of many different extensions to classical logic intended to deal with issues of uncertainty outside of the scope of classical logic, the inapplicability of probability theory in many domains, and the paradoxes of Dempster-Shafer theory.\n\nComputational theorist Leslie Valiant uses the term \"ecorithms\" to describe how many less exact systems and techniques like fuzzy logic (and \"less robust\" logic) can be applied to learning algorithms. Valiant essentially redefines machine learning as evolutionary. In general use, ecorithms are algorithms that learn from their more complex environments (hence \"eco-\") to generalize, approximate and simplify solution logic. Like fuzzy logic, they are methods used to overcome continuous variables or systems too complex to completely enumerate or understand discretely or exactly. Ecorithms and fuzzy logic also have the common property of dealing with possibilities more than probabilities, although feedback and feed forward, basically stochastic weights, are a feature of both when dealing with, for example, dynamical systems.\n\nCompensatory fuzzy logic (CFL) is a branch of fuzzy logic with modified rules for conjunction and disjunction. When the truth value of one component of a conjunction or disjunction is increased or decreased, the other component is decreased or increased to compensate. This increase or decrease in truth value may be offset by the increase or decrease in another component. An offset may be blocked when certain thresholds are met. Proponents claim that CFL allows for better computational semantic behaviors and mimic natural language. \n\nCompensatory Fuzzy Logic consists of four continuous operators: conjunction (c); disjunction (d); fuzzy strict order (or); and negation (n). The conjunction is the geometric mean and its dual as conjunctive and disjunctive operators.\n\nThe IEEE 1855, the IEEE STANDARD 1855–2016, is about a specification language named Fuzzy Markup Language (FML) developed by the IEEE Standards Association. FML allows modelling a fuzzy logic system in a human-readable and hardware independent way. FML is based on eXtensible Markup Language (XML). The designers of fuzzy systems with FML have a unified and high-level methodology for describing interoperable fuzzy systems. IEEE STANDARD 1855–2016 uses the W3C XML Schema definition language to define the syntax and semantics of the FML programs.\n\nPrior to the introduction of FML, fuzzy logic practitioners could exchange information about their fuzzy algorithms by  adding to their software functions the ability to read, correctly parse, and store the results of their work in a  form compatible with the Fuzzy Control Language (FCL) described and specified by Part 7 of IEC 61131.\n\n\n", "id": "49180", "title": "Fuzzy logic"}
{"url": "https://en.wikipedia.org/wiki?curid=1028978", "text": "Combs method\n\nThe Combs method is a method of writing fuzzy logic rules described by William E. Combs in 1997. It is designed to prevent combinatorial explosion in fuzzy logic rules.\n\nThe Combs method takes advantage of the logical equality formula_1.\n\nThe simplest proof of given equality involves usage of truth tables:\nSuppose we have a fuzzy system that considers N variables at a time, each of which can fit into at least one of S sets. The number of rules necessary to cover all the cases in a traditional fuzzy system is formula_2, whereas the Combs method would need only formula_3 rules. For example, if we have five sets and five variables to consider to produce one output, covering all the cases would require 3125 rules in a traditional system, while the Combs method would require only 25 rules, taming the combinatorial explosion that occurs when more inputs or more sets are added to the system.\n\nThis article will focus on the Combs method itself. To learn more about the way rules are traditionally formed, see fuzzy logic and fuzzy associative matrix.\n\nSuppose we were designing an artificial personality system that determined how friendly the personality is supposed to be towards a person in a strategic video game. The personality would consider its own fear, trust, and love in the other person. A set of rules in the Combs system might look like this:\nThe table translates to:\n\nIn this case, because the table follows a straightforward pattern in the output, it could be rewritten as:\nEach column of the table maps to the output provided in the last row. To obtain the output of the system, we just average the outputs of each rule for that output. For example, to calculate how much the computer is Enemies with the player, we take the average of how much the computer is Unafraid, Distrusting, and Unloving of the player. When all three averages are obtained, the result can then be defuzzified by any of the traditional means.\n\n", "id": "1028978", "title": "Combs method"}
{"url": "https://en.wikipedia.org/wiki?curid=6983799", "text": "AgentSheets\n\nAgentSheets is a Cyberlearning tool to teach students programming and related information technology skills through game design.\n\nAgentSheets is supported by a middle and high school curriculum called Scalable Game Design aligned with the ISTE National Educational Technology Standards (NETS). The mission of this project is to reinvent computer science in public schools by motivating & educating all students including women and underrepresented communities to learn about computer science through game design starting at the middle school level. Through this curriculum students build increasingly sophisticated games and, as part of this process, learn about computational concepts at the level of computational thinking that are relevant to game design as well as to computational science. The curriculum is made available through the Scalable Game Design Wiki. Research investigating motivational aspects of computer science education in public schools is currently exploring the introduction of game design in representative regions of the USA including technology hubs, inner city, rural and remote/tribal areas. Previous research has already found that game design with AgentSheets is universally accessible across gender as well as ethnicity and is not limited to students interested in playing video games.\n\nThe results of the NSF ITEST program supported research investigating motivational and educational aspects of introducing computer science at the middle school level are extremely positive in terms of motivational levels, number of participants and participation of women and underrepresented communities. The participation is extremely high because most middle schools participating in the study have made Scalable Game Design a module that is part of existing required courses (e.g., computer power with keyboarding and power point). Many of the middle schools instruct all of their students in scalable game design reaching in some schools over 900 students per year, per school. Of the well over 1000 students participating in the project in the first semester over 52% were girls. Of the girls 85% enjoyed the scalable game design course and 78% would like to take another game design course.\n\nThe built-in drag-and-drop language is accessible enough that students without programming background can make their own simple Frogger-like game, and publish it on the Web, in their first session. At the same time, AgentSheets is powerful enough to make sophisticated The Sims-like games with artificial intelligence. To transition from visual programming to more traditional programming students can render their games into Java source code.\n\nSimilar to a spreadsheet, an agentsheet is a computational grid. Unlike spreadsheets, this grid does not just contain numbers and strings but so called agents. These agents are represented by pictures, can be animated, make sounds, react to mouse/keyboard interactions, can read web pages, can speak and even recognize speech commands (Mac). This grid is well suited to build computational science applications modeling complex scientific phenomena with up to tens of thousands of agents. The grid is useful to build agent-based simulations including cellular automata or diffusion-based models. These models are used in a wide variety of applications. How does a mudslide work? When does a bridge collapse? How fragile are ecosystems? This ability to support game as well as computational science applications with the inclusion of scientific visualizations makes AgentSheets a unique computational thinking tool that is used computer science and STEM education.\n\nAgentSheets is used in a number of contexts worldwide:\n\n\nThe original goal of this research was to explore new models of computational thinking. The first prototype of AgentSheets ran in 1989 at the University of Colorado, NCAR, Connection Machine 2. The Connection Machine is a highly parallel computer with up to 65,536 CPUs. Realizing how hard it was to program the Connection Machine the insight that \"CPU cycles will always be ultimately cheaper than cognitive cycles\" led to the exploration of several new programming paradigms:\n\n", "id": "6983799", "title": "AgentSheets"}
{"url": "https://en.wikipedia.org/wiki?curid=1908395", "text": "Artificial brain\n\nAn artificial brain (or artificial mind) is software and hardware with cognitive abilities similar to those of the animal or human brain.\n\nResearch investigating \"artificial brains\" and brain emulation plays three important roles in science:\n\nAn example of the first objective is the project reported by Aston University in Birmingham, England where researchers are using biological cells to create \"neurospheres\" (small clusters of neurons) in order to develop new treatments for diseases including Alzheimer's, motor neurone and Parkinson's disease.\n\nThe second objective is a reply to arguments such as John Searle's Chinese room argument, Hubert Dreyfus' critique of AI or Roger Penrose's argument in \"The Emperor's New Mind\". These critics argued that there are aspects of human consciousness or expertise that can not be simulated by machines. One reply to their arguments is that the biological processes inside the brain can be simulated to any degree of accuracy. This reply was made as early as 1950, by Alan Turing in his classic paper \"Computing Machinery and Intelligence\".\n\nThe third objective is generally called artificial general intelligence by researchers. However, Ray Kurzweil prefers the term \"strong AI\". In his book \"The Singularity is Near\", he focuses on whole brain emulation using conventional computing machines as an approach to implementing artificial brains, and claims (on grounds of computer power continuing an exponential growth trend) that this could be done by 2025. Henry Markram, director of the Blue Brain project (which is attempting brain emulation), made a similar claim (2020) at the Oxford TED conference in 2009.\n\n Although direct human brain emulation using artificial neural networks on a high-performance computing engine is a commonly discussed approach, there are other approaches. An alternative artificial brain implementation could be based on Holographic Neural Technology (HNeT) non linear phase coherence/decoherence principles. The analogy has been made to quantum processes through the core synaptic algorithm which has strong similarities to the quantum mechanical wave equation.\n\nEvBrain is a form of evolutionary software that can evolve \"brainlike\" neural networks, such as the network immediately behind the retina.\n\nIn November 2008, IBM received a US$4.9 million grant from the Pentagon for research into creating intelligent computers. The Blue Brain project is being conducted with the assistance of IBM in Lausanne. The project is based on the premise that it is possible to artificially link the neurons \"in the computer\" by placing thirty million synapses in their proper three-dimensional position.\n\nSome proponents of strong AI speculated that computers in connection with Blue Brain and Soul Catcher may exceed human intellectual capacity by around 2015, and that it is likely that we will be able to download the human brain at some time around 2050.\n\nWhile \"Blue Brain\" is able to represent complex neural connections on the large scale, the project does not achieve the link between brain activity and behaviors executed by the brain. In 2012, project Spaun (Semantic Pointer Architecture Unified Network) attempted to model multiple parts of the human brain through large-scale representations of neural connections that generate complex behaviors in addition to mapping.\n\nSpaun's design recreates elements of human brain anatomy. The model, consisting of approximately 2.5 million neurons, includes features of the visual and motor cortices, GABAergic and dopaminergic connections, the ventral tegmental area (VTA), substantia nigra, and others. The design allows for several functions in response to eight tasks, using visual inputs of typed or handwritten characters and outputs carried out by a mechanical arm. Spaun's functions include copying a drawing, recognizing images, and counting.\n\nThere are good reasons to believe that, regardless of implementation strategy, the predictions of realising artificial brains in the near future are optimistic. In particular brains (including the human brain) and cognition are not currently well understood, and the scale of computation required is unknown. Another near term limitation is that all current approaches for brain simulation require orders of magnitude larger power consumption compared with a human brain. The human brain consumes about 20 W of power whereas current supercomputers may use as much as 1 MW or an order of 100,000 more.\n\nSome critics of brain simulation believe that it is simpler to create general intelligent action directly without imitating nature. Some commentators have used the analogy that early attempts to construct flying machines modeled them after birds, but that modern aircraft do not look like birds.\n\n", "id": "1908395", "title": "Artificial brain"}
{"url": "https://en.wikipedia.org/wiki?curid=24238010", "text": "Any-angle path planning\n\nAny-angle path planning algorithms are a subset of pathfinding algorithms that search for a path between two points in space and allow the turns in the path to have any angle. The result is a path that goes directly toward the goal and has relatively few turns. Other pathfinding algorithms such as A* constrain the paths to a grid, which produces jagged, indirect paths. Any-angle algorithms are able to find optimal or near-optimal paths by incorporating the any-angle search into the algorithm itself. Algorithms such as A* Post Smoothing that smooth a path found by a grid constrained algorithm are unable to reliably find optimal paths since they cannot change what side of a blocked cell is traversed.\n\nAny-angle path planning algorithms are necessary in order to quickly find an optimal path. For a world represented by a grid of blocked and unblocked cells, the brute-force way to find an any-angle path is to search the corresponding visibility graph. This is problematic since the number of edges in a graph with formula_1 vertices is formula_2. Searching the discrete grid graph can be done quickly since the number of edges grows linearly with the number of vertices, but the paths are not optimal since the angle of the turns are constrained to 45° or 90°, which will add turns and increase the overall length of the path. Smoothing a grid-constrained path after does not fix this problem since the algorithm that found that path did not look at all possible paths. Any-angle path planning algorithms find shorter paths than the grid-constrained algorithms while taking roughly same amount of time to compute.\n\nSo far, four main any-angle path planning algorithms that are based on the heuristic search algorithm A* have been developed, all of which propagate information along grid edges:\n\n\n\nBesides, for search in high-dimensional search spaces, such as when the configuration space of the system involves many degrees of freedom that need to be considered (see Motion planning), and/or momentum needs to be considered (which could effectively double the number of dimensions of the search space; this larger space including momentum is known as the phase space), variants of the rapidly-exploring random tree (RRT) have been developed that (almost surely) converge to the optimal path by increasingly finding shorter and shorter paths:\n\n\nHybrid A* was created by Stanford Racing as part of the navigation system for Junior, their entry to the DARPA Urban Challenge. Hybrid A* is continuous and tracks the vehicle's position and orientation. This ensures that the path generated can be followed by the vehicle, unlike the paths generated by A* for Field D*, which both produce sharp turns, and do not consider the geometry or movement constraints of the vehicle.\n\n\n\n", "id": "24238010", "title": "Any-angle path planning"}
{"url": "https://en.wikipedia.org/wiki?curid=9025771", "text": "Anytime algorithm\n\nIn computer science, an anytime algorithm is an algorithm that can return a valid solution to a problem even if it is interrupted before it ends. The algorithm is expected to find better and better solutions the more time it keeps running.\n\nMost algorithms run to completion: they provide a single answer after performing some fixed amount of computation. In some cases, however, the user may wish to terminate the algorithm prior to completion. The amount of the computation required may be substantial, for example, and computational resources might need to be reallocated. Most algorithms either run to completion or they provide no useful solution information. Anytime algorithms, however, are able to return a partial answer, whose quality depends on the amount of computation they were able to perform. The answer generated by anytime algorithms is an approximation of the correct answer.\n\nAn anytime algorithm may be also called an \"interruptible algorithm\". They are different from contract algorithms, which must declare a time in advance; in an anytime algorithm, a process can just announce that it is terminating.\n\nThe goal of anytime algorithms are to give intelligent systems the ability to make results of better quality in return for turn-around time. They are also supposed to be flexible in time and resources. They are important because artificial intelligence or AI algorithms can take a long time to complete results. This algorithm is designed to complete in a shorter amount of time. Also, these are intended to have a better understanding that the system is dependent and restricted to its agents and how they work cooperatively. An example is the Newton-Raphson iteration applied to finding the square root of a number. Another example that uses anytime algorithms is trajectory problems when you're aiming for a target; the object is moving through space while waiting for the algorithm to finish and even an approximate answer can significantly improve its accuracy if given early.\n\nWhat makes anytime algorithms unique is their ability to return many possible outcomes for any given input. An anytime algorithm uses many well defined quality measures to monitor progress in problem solving and distributed computing resources. It keeps searching for the best possible answer with the amount of time that it is given. It may not run until completion and may improve the answer if it is allowed to run longer.\nThis is often used for large decision set problems. This would generally not provide useful information unless it is allowed to finish. While this may sound similar to dynamic programming, the difference is that it is fine-tuned through random adjustments, rather than sequential.\n\nAnytime algorithms are designed so that it can be told to stop at any time and would return the best result it has found so far. This is why it is called an interruptible algorithm. Anytime algorithms also maintain the last result, so that if they are given more time, they can continue from where they left off to obtain an even better result.\n\nWhen the decider has to act, there must be some ambiguity. Also, there must be some idea about how to solve this ambiguity. This idea must be translatable to a state to action diagram.\n\nThe performance profile estimates the quality of the results based on the input and the amount of time that is allotted to the algorithm. The better the estimate, the sooner the result would be found. Some systems have a larger database that gives the probability that the output is the expected output. It is important to note that one algorithm can have several performance profiles. Most of the time performance profiles are constructed using mathematical statistics using representative cases. For example, in the traveling salesman problem, the performance profile was generated using a user-defined special program to generate the necessary statistics. In this example, the performance profile is the mapping of time to the expected results. This quality can be measured in several ways:\n\n\nInitial behavior: While some algorithms start with immediate guesses, others take a more calculated approach and have a start up period before making any guesses.\n\n\n", "id": "9025771", "title": "Anytime algorithm"}
{"url": "https://en.wikipedia.org/wiki?curid=361157", "text": "Bio-inspired computing\n\nBio-inspired computing, short for biologically inspired computing, is a field of study that loosely knits together subfields related to the topics of connectionism, social behaviour and emergence. It is often closely related to the field of artificial intelligence, as many of its pursuits can be linked to machine learning. It relies heavily on the fields of biology, computer science and mathematics. Briefly put, it is the use of computers to model the living phenomena, and simultaneously the study of life to improve the usage of computers. Biologically inspired computing is a major subset of natural computation.\n\nSome areas of study encompassed under the canon of biologically inspired computing, and their biological counterparts:\n\n\nThe way in which bio-inspired computing differs from the traditional artificial intelligence (AI) is in how it takes a more evolutionary approach to learning, as opposed to what could be described as 'creationist' methods used in traditional AI. In traditional AI, intelligence is often programmed from above: the programmer is the creator, and makes something and imbues it with its intelligence. Bio-inspired computing, on the other hand, takes a more bottom-up, decentralised approach; bio-inspired techniques often involve the method of specifying a set of simple rules, a set of simple organisms which adhere to those rules, and a method of iteratively applying those rules. For example, training a virtual insect to navigate in an unknown terrain for finding food includes six simple rules. The insect is trained to \nThe virtual insect controlled by the trained spiking neural network can find food after training in any unknown terrain. After several generations of rule application it is usually the case that some forms of complex behaviour arise. Complexity gets built upon complexity until the end result is something markedly complex, and quite often completely counterintuitive from what the original rules would be expected to produce (see complex systems). For this reason, in neural network models, it is necessary to accurately model an \"in vivo\" network, by live collection of \"noise\" coefficients that can be used to refine statistical inference and extrapolation as system complexity increases.\n\nNatural evolution is a good analogy to this method–the rules of evolution (selection, recombination/reproduction, mutation and more recently transposition) are in principle simple rules, yet over millions of years have produced remarkably complex organisms. A similar technique is used in genetic algorithms.\n\n\n\n\"(the following are presented in ascending order of complexity and depth, with those new to the field suggested to start from the top)\"\n\n\n", "id": "361157", "title": "Bio-inspired computing"}
{"url": "https://en.wikipedia.org/wiki?curid=15291723", "text": "Hierarchical control system\n\nA hierarchical control system is a form of control system in which a set of devices and governing software is arranged in a hierarchical tree. When the links in the tree are implemented by a computer network, then that hierarchical control system is also a form of networked control system.\n\nA human-built system with complex behavior is often organized as a hierarchy. For example, a command hierarchy has among its notable features the organizational chart of superiors, subordinates, and lines of organizational communication. Hierarchical control systems are organized similarly to divide the decision making responsibility.\n\nEach element of the hierarchy is a linked node in the tree. Commands, tasks and goals to be achieved flow down the tree from superior nodes to subordinate nodes, whereas sensations and command results flow up the tree from subordinate to superior nodes. Nodes may also exchange messages with their siblings. The two distinguishing features of a hierarchical control system are related to its layers.\n\n\nBesides artificial systems, an animal's control systems are proposed to be organized as a hierarchy. In perceptual control theory, which postulates that an organism's behavior is a means of controlling its perceptions, the organism's control systems are suggested to be organized in a hierarchical pattern as their perceptions are constructed so.\n\nThe accompanying diagram is a general hierarchical model which shows functional manufacturing levels using computerised control of an industrial control system.\n\nReferring to the diagram;\n\n\nAmong the robotic paradigms is the hierarchical paradigm in which a robot operates in a top-down fashion, heavy on planning, especially motion planning. Computer-aided production engineering has been a research focus at NIST since the 1980s. Its Automated Manufacturing Research Facility was used to develop a five layer production control model. In the early 1990s DARPA sponsored research to develop distributed (i.e. networked) intelligent control systems for applications such as military command and control systems. NIST built on earlier research to develop its Real-Time Control System (RCS) and Real-time Control System Software which is a generic hierarchical control system that has been used to operate a manufacturing cell, a robot crane, and an automated vehicle.\n\nIn November 2007, DARPA held the Urban Challenge. The winning entry, Tartan Racing employed a hierarchical control system, with layered mission planning, motion planning, behavior generation, perception, world modelling, and mechatronics.\n\nSubsumption architecture is a methodology for developing artificial intelligence that is heavily associated with behavior based robotics. This architecture is a way of decomposing complicated intelligent behavior into many \"simple\" behavior modules, which are in turn organized into layers. Each layer implements a particular goal of the software agent (i.e. system as a whole), and higher layers are increasingly more abstract. Each layer's goal subsumes that of the underlying layers, e.g. the decision to move forward by the eat-food layer takes into account the decision of the lowest obstacle-avoidance layer. Behavior need not be planned by a superior layer, rather behaviors may be triggered by sensory inputs and so are only active under circumstances where they might be appropriate.\n\nReinforcement learning has been used to acquire behavior in a hierarchical control system in which each node can learn to improve its behavior with experience.\nJames Albus, while at NIST, developed a theory for intelligent system design named the Reference Model Architecture (RMA), which is a hierarchical control system inspired by RCS. Albus defines each node to contain these components.\nAt its lowest levels, the RMA can be implemented as a subsumption architecture, in which the world model is mapped directly to the controlled process or real world, avoiding the need for a mathematical abstraction, and in which time-constrained reactive planning can be implemented as a finite state machine. Higher levels of the RMA however, may have sophisticated mathematical world models and behavior implemented by automated planning and scheduling. Planning is required when certain behaviors cannot be triggered by current sensations, but rather by predicted or anticipated sensations, especially those that come about as result of the node's actions.\n\n\n", "id": "15291723", "title": "Hierarchical control system"}
{"url": "https://en.wikipedia.org/wiki?curid=383162", "text": "Information extraction\n\nInformation extraction (IE) is the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents. In most of the cases this activity concerns processing human language texts by means of natural language processing (NLP). Recent activities in multimedia document processing like automatic annotation and content extraction out of images/audio/video could be seen as information extraction.\n\nDue to the difficulty of the problem, current approaches to IE focus on narrowly restricted domains. An example is the extraction from newswire reports of corporate mergers, such as denoted by the formal relation: \nfrom an online news sentence such as:\n\nA broad goal of IE is to allow computation to be done on the previously unstructured data. A more specific goal is to allow logical reasoning to draw inferences based on the logical content of the input data. Structured data is semantically well-defined data from a chosen target domain, interpreted with respect to category and context.\n\nInformation Extraction is the part of a greater puzzle which deals with the problem of devising automatic methods for text management, beyond its transmission, storage and display. The discipline of information retrieval (IR) has developed automatic methods, typically of a statistical flavor, for indexing large document collections and classifying documents. Another complementary approach is that of natural language processing (NLP) which has solved the problem of modelling human language processing with considerable success when taking into account the magnitude of the task. In terms of both difficulty and emphasis, IE deals with tasks in between both IR and NLP. In terms of input, IE assumes the existence of a set of documents in which each document follows a template, i.e. describes one or more entities or events in a manner that is similar to those in other documents but differing in the details. An example, consider a group of newswire articles on Latin American terrorism with each article presumed to be based upon one or more terroristic acts. We also define for any given IE task a template, which is a(or a set of) case frame(s) to hold the information contained in a single document. For the terrorism example, a template would have slots corresponding to the perpetrator, victim, and weapon of the terroristic act, and the date on which the event happened. An IE system for this problem is required to “understand” an attack article only enough to find data corresponding to the slots in this template.\n\nInformation extraction dates back to the late 1970s in the early days of NLP. An early commercial system from the mid-1980s was JASPER built for Reuters by the Carnegie Group with the aim of providing real-time financial news to financial traders.\n\nBeginning in 1987, IE was spurred by a series of Message Understanding Conferences. MUC is a competition-based conference that focused on the following domains: \n\nConsiderable support came from the U.S. Defense Advanced Research Projects Agency (DARPA), who wished to automate mundane tasks performed by government analysts, such as scanning newspapers for possible links to terrorism.\n\nThe present significance of IE pertains to the growing amount of information available in unstructured form. Tim Berners-Lee, inventor of the world wide web, refers to the existing Internet as the web of \"documents\" and advocates that more of the content be made available as a web of \"data\". Until this transpires, the web largely consists of unstructured documents lacking semantic metadata. Knowledge contained within these documents can be made more accessible for machine processing by means of transformation into relational form, or by marking-up with XML tags. An intelligent agent monitoring a news data feed requires IE to transform unstructured data into something that can be reasoned with. A typical application of IE is to scan a set of documents written in a natural language and populate a database with the information extracted.\n\nApplying information extraction to text is linked to the problem of text simplification in order to create a structured view of the information present in free text. The overall goal being to create a more easily machine-readable text to process the sentences. Typical subtasks of IE include:\n\n\nNote that this list is not exhaustive and that the exact meaning of IE activities is not commonly accepted and that many approaches combine multiple sub-tasks of IE in order to achieve a wider goal. Machine learning, statistical analysis and/or natural language processing are often used in IE.\n\nIE on non-text documents is becoming an increasingly interesting topic in research, and information extracted from multimedia documents can now be expressed in a high level structure as it is done on text. This naturally leads to the fusion of extracted information from multiple kinds of documents and sources.\n\nIE has been the focus of the MUC conferences. The proliferation of the Web, however, intensified the need for developing IE systems that help people to cope with the enormous amount of data that is available online. Systems that perform IE from online text should meet the requirements of low cost, flexibility in development and easy adaptation to new domains. MUC systems fail to meet those criteria. Moreover, linguistic analysis performed for unstructured text does not exploit the HTML/XML tags and the layout formats that are available in online texts. As a result, less linguistically intensive approaches have been developed for IE on the Web using wrappers, which are sets of highly accurate rules that extract a particular page's content. Manually developing wrappers has proved to be a time-consuming task, requiring a high level of expertise. Machine learning techniques, either supervised or unsupervised, have been used to induce such rules automatically.\n\n\"Wrappers\" typically handle highly structured collections of web pages, such as product catalogs and telephone directories. They fail, however, when the text type is less structured, which is also common on the Web. Recent effort on \"adaptive information extraction\" motivates the development of IE systems that can handle different types of text, from well-structured to almost free text -where common wrappers fail- including mixed types. Such systems can exploit shallow natural language knowledge and thus can be also applied to less structured texts.\n\nA recent development is Visual Information Extraction, that relies on rendering a webpage in a browser and creating rules based on the proximity of regions in the rendered web page. This helps in extracting entities from complex web pages that may exhibit a visual pattern, but lack a discernible pattern in the HTML source code.\n\nThree standard approaches are now widely accepted:\n\nNumerous other approaches exist for IE including hybrid approaches that combine some of the standard approaches previously listed.\n\n\n\n\n\n", "id": "383162", "title": "Information extraction"}
{"url": "https://en.wikipedia.org/wiki?curid=30306484", "text": "Cognitive infocommunications\n\nCognitive infocommunications (CogInfoCom) investigates the link between the research areas of infocommunications and the cognitive sciences, as well as the various engineering applications which have emerged as the synergic combination of these sciences.\n\nThe primary goal of CogInfoCom is to provide a systematic view of how cognitive processes can co-evolve with infocommunications devices so that the capabilities of the human brain may not only be extended through these devices, irrespective of geographical distance, but may also interact with the capabilities of any artificially cognitive system. This merging and extension of cognitive capabilities is targeted towards engineering applications in which artificial and/or natural cognitive systems are enabled to work together more effectively.\n\nTwo important dimensions of cognitive infocommunications are the mode of communication and the type of communication.\nThe mode of communication refers to the actors at the two endpoints of communication:\n\n\nThe type of communication refers to the type of information that is conveyed between the two communicating entities, and the way in which this is done:\n\n\n\nThe first draft definition of CogInfoCom was given in \"Cognitive Infocommunications: CogInfoCom\". The definition was finalized based on the paper with the joint participation of the Startup Committee at the 1st International Workshop on Cognitive Infocommunications, held in Tokyo, Japan in 2010. A recent overview and further information can be found in, and in the two special issues on CogInfoCom which have been published since then, and at the official website of CogInfoCom.\n", "id": "30306484", "title": "Cognitive infocommunications"}
{"url": "https://en.wikipedia.org/wiki?curid=30667214", "text": "Evolving intelligent system\n\nIntelligence and intelligent systems has to be able to evolve, self-develop, self-learn continuously in order to reflect the dynamically evolving environment. The concept of Evolving Intelligent Systems (EISs) was conceived around the turn of the century with the phrase EIS itself coined for the first time in and expanded in. EISs develop their structure, functionality and internal knowledge representation through autonomous learning from data streams generated by the possibly unknown environment and from the system self-monitoring. EISs consider a gradual development of the underlying (fuzzy or neuro-fuzzy) system structure and differ from evolutionary and genetic algorithms which consider such phenomena as chromosomes crossover, mutation, selection and reproduction, parents and off-springs. The evolutionary fuzzy and neuro systems are sometimes also called “evolving” which leads to some confusion. This was more typical for the first works on this topic in the late 1990s.\n\nEISs can be implemented, for example, using neural networks or fuzzy rule-based models. The first neural networks which consider an evolving structure were published in. These were later expanded by N. Kasabov and P. Angelov for the neuro-fuzzy models. P. Angelov introduced the evolving fuzzy rule-based systems (EFSs) as the first mathematical self-learning model that can dynamically evolve its internal structure and is human interpretable and coined the phrase EFS. Contemporarily, the offline incremental approach for learning an EIS, namely, EFuNN, was proprosed by N. Kasabov. P. Angelov, D. Filev, N. Kasabov and O. Cordon organised the first IEEE Symposium on EFSs in 2006 (the proceedings of the conference can be found in ). EFSs include a formal (and mathematically sound) learning mechanism to extract it from streaming data. One of the earliest and the most widely cited comprehensive survey on EFSs was done in 2008. A later comprehensive survey on EFSs with real applications was done in 2011 by E. Lughofer. Other works that contributed further to this area in the following years expanded it to evolving participatory learning, evolving grammar, evolving decision trees, evolving human behaviour modelling, self-calibrating (evolving) sensors (eSensors), evolving fuzzy rule-based classifiers, evolving fuzzy controllers, autonomous fault detectors. More recently, the stability of the evolving fuzzy rule-based systems that consist of the structure learning and the fuzzily weighted recursive least square parameter update method has been proven by Rong and Angelov.\n\nEISs form the theoretical and methodological basis for the Autonomous Learning Machines (ALMA) and autonomous multi-model systems (ALMMo) as well as of the Autonomous Learning Systems. Evolving Fuzzy Rule-based classifiers, in particular, is a very powerful new concept that offers much more than simply incremental or online classifiers – it can cope with new classes being added or existing classes being merged. This is much more than just adapting to new data samples being added or classification surfaces being evolved. Fuzzy rule-based classifiers are the methodological basis of a new approach to deep learning that was until now considered as a form of multi-layered neural networks. Deep Learning offers high precision levels surpassing the level of human ability and grabbed the imagination of the researchers, industry and the wider public. However, it has a number of intrinsic constraints and limitations. These include:\nMost, if not all, of the above limitations can be avoided with the use of the Deep (Fuzzy) Rule-based Classifiers, which were recently introduced based on ALMMo, while achieving similar or even better performance. The resulting prototype-based IF…THEN…models are fully interpretable and dynamically evolving (they can adapt quickly and automatically to new data patterns or even new classes). They are non-parametric and, therefore, their training is non-iterative and fast (it can take few milliseconds per data sample/image on a normal laptop which contrasts with the multiple hours the current deep learning methods require for training even when they use GPUs and HPC). Moreover, they can be trained incrementally/online/in real-time.\n", "id": "30667214", "title": "Evolving intelligent system"}
{"url": "https://en.wikipedia.org/wiki?curid=16794474", "text": "Smart objects\n\nA smart object is an object that enhances the interaction with not only people but also with other smart objects. It can not only refer to interaction with physical world objects but also to interaction with virtual (computing environment) objects. A smart physical object may be created either as an artifact or manufactured product or by embedding electronic tags such as RFID tags or sensors into non-smart physical objects. Smart virtual objects are created as software objects that are intrinsic when creating and operating a virtual or cyber world simulation or game. The concept of a smart object has several origins and uses, see History. There are also several overlapping terms, see also smart device, tangible object or tangible user interface and Thing as in the Internet of things.\n\nIn the early 1990s, Mark Weiser, from whom the term ubiquitous computing originated, referred to a vision \"When almost every object either contains a computer or can have a tab attached to it, obtaining information will be trivial\", \nAlthough Weiser did not specifically refer to an object as being smart, his early work did imply that smart physical objects are smart in the sense that they act as digital information sources. Hiroshi Ishii and Brygg Ullmer refer to tangible objects in terms of tangibles bits or tangible user interfaces that enable users to \"grasp & manipulate\" bits in the center of users' attention by coupling the bits with everyday physical objects and architectural surfaces.\n\nThe smart object concept was introduced by Marcelo Kallman and Daniel Thalmann as an object that can describe its own possible interactions. The main focus here is to model interactions of smart virtual objects with virtual humans, agents, in virtual worlds. The opposite approach to smart objects is 'plain' objects that do not provide this information. The additional information provided by this concept enables far more general interaction schemes, and can greatly simplify the planner of an artificial intelligence agent.\nIn contrast to smart virtual objects used in virtual worlds, Lev Manovich focuses on physical space filled with electronic and visual information. Here, \"smart objects\" are described as \"objects connected to the Net; objects that can sense their users and display smart behaviour\".\n\nMore recently in the early 2010s, smart objects are being proposed as a key enabler for the vision of the Internet of things. The combination of the Internet and emerging technologies such as near field communications, real-time localization, and embedded sensors enables everyday objects to be transformed into smart objects that can understand and react to their environment. Such objects are building blocks for the Internet of things and enable novel computing applications.\n\nAlthough we can view interaction with physical smart object in the physical world as distinct from interaction with virtual smart objects in a virtual simulated world, these can be related. Poslad considers the progression of: how\n\nThe concept smart for a smart physical object simply means that it is active, digital, networked, can operate to some extent autonomously, is reconfigurable and has local control of the resources it needs such as energy, data storage, etc. Note, a smart object does not necessarily need to be intelligent as in exhibiting a strong essence of artificial intelligence—although it can be designed to also be intelligent.\n\nPhysical world smart objects can be described in terms of three properties:\n\nBased upon these properties, these have been classified into three types:\n\nFor the virtual object in a virtual world case, an object is called smart when it has the ability to describe its possible interactions. This focuses on constructing a virtual world using only virtual objects that contain their own interaction information. There are four basic elements to constructing such a smart virtual object framework.\n\nSome versions of smart objects also include animation information in the object information, but this is not considered to be an efficient approach, since this can make objects inappropriately oversized.\n\n\n\n", "id": "16794474", "title": "Smart objects"}
{"url": "https://en.wikipedia.org/wiki?curid=26269089", "text": "Stochastic semantic analysis\n\nStochastic semantic analysis is an approach used in computer science as a semantic component of natural language understanding.\n\nStochastic models generally use the definition of segments of words as basic semantic units for the semantic models, and in some cases involve a two layered approach.\n\nExample applications have a wide range. In machine translation, it has been applied to the translation of spontaneous conversational speech among different languages. In the area of \"spoken language understanding\" the fact that spoken sentences often do not follow the grammar of a language and involve self-corrections, repetitions, and other irregularities, the use of stochastic semantic has been suggested as a natural fit to achieve robustness to deal with noise due to the spontaneous nature of spoken language.\n\n", "id": "26269089", "title": "Stochastic semantic analysis"}
{"url": "https://en.wikipedia.org/wiki?curid=18313661", "text": "Knowledge compilation\n\nKnowledge compilation is a family of approaches for addressing the intractability of\na number of artificial intelligence problems.\n\nA propositional model is compiled in an off-line phase in order to support some queries in polytime. Many ways of compiling a propositional models exist.\nAmong others: NNF, DNNF, d-DNNF, BDD, SDD, MDD, DNF and CNF.\n\nDifferent compiled representations have different properties.\nThe three main properties are:\n", "id": "18313661", "title": "Knowledge compilation"}
{"url": "https://en.wikipedia.org/wiki?curid=19835615", "text": "Gabbay's separation theorem\n\nIn mathematical logic and computer science, Gabbay's separation theorem, named after Dov Gabbay, states that any arbitrary temporal logic formula can be rewritten in a logically equivalent \"past → future\" form. I.e. the future becomes what must be satisfied. This form can be used as execution rules; a MetateM program is a set of such rules.\n", "id": "19835615", "title": "Gabbay's separation theorem"}
{"url": "https://en.wikipedia.org/wiki?curid=31068176", "text": "Multi-Agent Programming Contest\n\nThe Multi-Agent Programming Contest is an annual international programming competition with stated goal of stimulating research in the area of multi-agent system development and programming.\n\nIn 2005 Jürgen Dix (Clausthal University of Technology), Mehdi Dastani (University Utrecht) and Peter Novák (Czech Technical University in Prague) have brought the contest into being and running. The competition originally focused on Logic programming of Multi-agent systems. The goals, raised in 2005, have proven to be a solid basis for multi-agent system development and are still valid:\nIn 2007, a third goal has been added:\nAlthough it is necessary to find a solution for the contest quest to win, the organizers pursue the intention that the solution is a system of cooperating autonomous programs that achieve the objectives together. They are also interested in how the contest participants develop the solution.\n\nAgents have to look for food and bring it to a depot on a two-dimensional grid world. Each cell can contain an agent, or food. The agents can only see a small part of the map. Initially there is no food available, it appears randomly during the game, so that agents need to search the map constantly in order to win. This scenario was used in 2005.\n\nOn a grid based map, teams of agents look for gold and transport it to the depot. As opposed to the food scenario, cells can also contain trees which block the agents and can form more or less complex labyrinths. Also, there are now two opposing teams competing for the gold. This scenario was used in the contests of 2006 and 2007. In 2007, the scenario was extended to allow the agents to carry more than one piece of gold, and to push opposing agents aside.\n\nA grid based map contains trees, corrals, cows and agents. Two opposing teams try to drive as many cows as possible in ones corral. Cows behave using Swarm intelligence. They are also afraid of cowboys and try and run away. This scenario was used in 2008, 2009 and 2010. For the last two years, gates were introduced to make the scenario more challenging.\n\nThe 2011 contest introduces a scenario called agents on mars. Goal is to conquer as much space on mars as possible, using a team of cooperating agents. The challenge here is the higher complexity resulting from the introduction of five roles with different properties and abilities, which have to be used to scout, conquer, and keep the conquered land.\nThe team HactarV2 from the TU-Delft won the 2011 competition while using the GOAL programming language.\n\n\n", "id": "31068176", "title": "Multi-Agent Programming Contest"}
{"url": "https://en.wikipedia.org/wiki?curid=2730882", "text": "Embodied agent\n\nIn artificial intelligence, an embodied agent, also sometimes referred to as an interface agent, is an intelligent agent that interacts with the environment through a physical body within that environment. Agents that are represented graphically with a body, for example a human or a cartoon animal, are also called embodied agents, although they have only virtual, not physical, embodiment. A branch of artificial intelligence focuses on empowering such agents to interact autonomously with human beings and the environment. Mobile robots are one example of physically embodied agents; Ananova and Microsoft Agent are examples of graphically embodied agents. Embodied conversational agents are embodied agents (usually with a graphical front-end as opposed to a robotic body) that are capable of engaging in conversation with one another and with humans employing the same verbal and nonverbal means that humans do (such as gesture, facial expression, and so forth).\n\nEmbodied conversational agents are a form of intelligent user interface. Graphically embodied agents aim to unite gesture, facial expression and speech to enable face-to-face communication with users, providing a powerful means of human-computer interaction.\n\nFace-to-face communication allows communication protocols that give a much richer communication channel than other means of communicating. It enables pragmatic communication acts such as conversational turn-taking, facial expression of emotions, information structure and emphasis, visualisation and iconic gestures, and orientation in a three-dimensional environment. This communication takes place through both verbal and non-verbal channels such as gaze, gesture, spoken intonation and body posture.\n\nResearch has found that users prefer a non-verbal visual indication of an embodied system's internal state to a verbal indication, demonstrating the value of additional non-verbal communication channels. As well as this, the face-to-face communication involved in interacting with an embodied agent can be conducted alongside another task without distracting the human participants, instead improving the enjoyment of such an interaction. Furthermore, the use of an embodied presentation agent results in improved recall of the presented information.\n\nEmbodied agents also provide a social dimension to the interaction. Humans willingly ascribe social awareness to computers, and thus interaction with embodied agents follows social conventions, similar to human/human interactions. This social interaction both raises the believability and perceived trustworthiness of agents, and increases the user's engagement with the system. Rickenberg and Reeves found that the presence of an embodied agent on a website increased the level of user trust in that website; as well as this, the presence of the agent increased users' anxiety and affected their performance just as if they were being watched by a real human. Another effect of the social aspect of agents is that presentations given by an embodied agent are perceived as more entertaining and less difficult than the same presentations given without an agent. Research shows that perceived enjoyment, followed by perceived usefulness and ease of use, is the major factor influencing user adoption of embodied agents.\n\nOne example result from a recent study indicates the power of a character when moderating search inquiries. When a character asked people to type search requests into a window, people used, on average, three more words in their requests (averaging about 7 words per inquiry) compared to identical requests made without a character. Character suggest that a conversational style is appropriate, resulting in higher liking for the\ninteraction on the part of the user, and better accuracy for the engine generating the required results.\n\nThis rich style of communication that characterises human conversation makes conversational interaction with embodied conversational agents ideal for many non-traditional interaction tasks. A familiar application of graphically embodied agents is computer games; embodied agents are ideal for this setting because the richer communication style makes interacting with the agent enjoyable. Embodied conversational agents have also been used in virtual training environments, portable personal navigation guides, interactive fiction and storytelling systems, interactive online characters and automated presenters and commentators.\n\nMajor virtual assistants like Siri and Google Assistant do not come with any visual embodied representation, which is believed to limit\nthe sense of human presence by users.\n\nThe U.S. Department of Defense utilizes a software agent they call \"\"Sgt. Star\"\" on U.S. Army-run Web sites and Web applications for site navigation, recruitment and propaganda purposes. Sgt. Star is run by the Army Marketing and Research Group, a division operated directly from The Pentagon. Sgt. Star is based upon the \"ActiveSentry\" technology developed by Next IT, a Washington-based information technology services company. Other such bots in the Sgt. Star \"family\" are utilized by the Federal Bureau of Investigation and the Central Intelligence Agency for intelligence gathering purposes.\n\n\n\n\n", "id": "2730882", "title": "Embodied agent"}
{"url": "https://en.wikipedia.org/wiki?curid=866256", "text": "User illusion\n\nThe user illusion is the illusion created for the user by a human–computer interface, for example the visual metaphor of a desktop used in many graphical user interfaces. The phrase originated at Xerox PARC.\n\nSome philosophers of mind have argued that consciousness is a form of user illusion. This notion is explored by Tor Nørretranders in his 1991 Danish book \"Mærk verden\", issued in a 1998 English edition as \"The User Illusion: Cutting Consciousness Down to Size\". He introduced the notion of exformation in this book.\n\nAccording to this picture, our experience of the world is not immediate, as all sensation requires processing time. It follows that our conscious experience is less a perfect reflection of what is occurring, and more a simulation produced unconsciously by the brain. Therefore, there may be phenomena that exist beyond our peripheries, beyond what consciousness could create to isolate or reduce them.\n\nCritics of the idea of consciousness being a device for justifying preconceptions argue that such a device would consume nutrients without producing any useful results, since it would not change the outcome of any decisions. These critics argue that the existence of social insects with extremely small brains falsifies the notion that social behavior require consciousness, citing that insects have too small brains to be conscious and yet there are observed behaviors among them that for all functional intents and purposes match those of complex social cooperation and manipulation (including hierarchies where each individual has its place among paper wasps and Jack Jumper ants and honey bees sneaking when they lay eggs). These critics also argue that since social behavior in insects and other extremely small-brained animals have evolved multiple times independently, there is no evolutionary difficulty in simple reaction sociality to impose selection pressure for the more nutrient-consuming path of consciousness for sociality. These critics do point out that other evolutionary paths to consciousness are possible, such as critical evaluation that enhances plasticity by criticizing fallible notions, while pointing out that such a critical consciousness would be quite different from the justificatory type proposed by Nørretranders, differences including that a critical consciousness would make individuals more capable of changing their minds instead of justifying and persuading.\n\n", "id": "866256", "title": "User illusion"}
{"url": "https://en.wikipedia.org/wiki?curid=23402053", "text": "Wetware (brain)\n\nWetware is a term drawn from the computer-related idea of hardware or software, but applied to biological life forms.\n\nThe prefix \"wet\" is a reference to the water found in living creatures. Wetware is used to describe the elements equivalent to hardware and software found in a person, especially the central nervous system (CNS) and the human mind. The term wetware finds use both in works of fiction and in scholarly publications.\n\nThe \"hardware\" component of wetware concerns the bioelectric and biochemical properties of the CNS, specifically the brain. If the sequence of impulses traveling across the various neurons are thought of symbolically as \"software\", then the physical neurons would be the \"hardware\". The amalgamated interaction of this \"software\" and \"hardware\" is manifested through continuously changing physical connections, and chemical and electrical influences that spread across the body. The process by which the \"mind\" and \"brain\" interact to produce the collection of experiences that we define as self-awareness is in question.\n\nAlthough the exact definition has shifted over time, the term \"Wetware\" and its fundamental reference to \"the physical mind\" has been around at least since the mid-1950s. Mostly used in relatively obscure articles and papers, it was not until the heyday of cyberpunk, however, that the term found broad adoption. Among the first uses of the term in popular culture was the Michael Swanwick novel \"Vacuum Flowers\" (1987).\n\nRudy Rucker references the term in a number of books, including one entitled \"Wetware\": ... all sparks and tastes and tangles, all its stimulus/response patterns – the whole bio-cybernetic software of mind. Rucker did not use the word to simply mean a brain, nor in the human-resources sense of employees. He used \"wetware\" to stand for the data found in any biological system, analogous perhaps to the firmware that is found in a ROM chip. In Rucker's sense, a seed, a plant graft, an embryo, or a biological virus are all wetware. DNA, the immune system, and the evolved neural architecture of the brain are further examples of wetware in this sense.\n\nRucker describes his conception in a 1992 compendium \"The Mondo 2000 User's Guide to the New Edge\", which he quotes in a 2007 blog entry.\n\nEarly cyber-guru Arthur Kroker used the term in his blog.\n\nWith the term getting traction in trendsetting publications, it became a buzzword in the early 1990s. In 1991, Dutch media theorist Geert Lovink organized the Wetware Convention in Amsterdam, which was supposed to be an antidote to the \"out-of-body\" experiments conducted in high-tech laboratories, such as experiments in virtual reality.\n\nTimothy Leary, in an appendix to \"Info-Psychology\" originally written in 1975–76 and published in 1989, used the term \"wetware\", writing that \"psychedelic neuro-transmitters were the hot new technology for booting-up the 'wetware' of the brain\". Another common reference is: \"Wetware has 7 plus or minus 2 temporary registers.\" The numerical allusion is to a classic 1957 article by George A. Miller, \"The magical number 7 plus or minus two: some limits in our capacity for processing information\", which later gave way to the Miller's law.\n\n", "id": "23402053", "title": "Wetware (brain)"}
{"url": "https://en.wikipedia.org/wiki?curid=1377612", "text": "Knowledge-based systems\n\nA knowledge-based system (KBS) is a computer program that reasons and uses a knowledge base to solve complex problems. The term is broad and is used to refer to many different kinds of systems; the one common theme that unites all knowledge based systems is an attempt to represent knowledge explicitly via tools such as ontologies and rules rather than implicitly via code the way a conventional computer program does. A knowledge based system has three types of sub-systems: a knowledge base, an user interface and an inference engine. The knowledge base represents facts about the world, often in some form of subsumption ontology. The inference engine represents logical assertions and conditions about the world, usually represented via IF-THEN rules.\n\nKnowledge-based systems were first developed by artificial intelligence researchers. These early knowledge-based systems were primarily expert systems – in fact, the term is often used synonymously with expert systems. The difference is in the view taken to describe the system: \"expert system\" refers to the type of task the system is trying to assist with – to replace or aid a human expert in a complex task; \"knowledge-based system\" refers to the architecture of the system – that it represents knowledge explicitly (rather than as procedural code). While the earliest knowledge-based systems were almost all expert systems, the same tools and architectures can and have since been used for a whole host of other types of systems – i.e., virtually all expert systems are knowledge-based systems, but many knowledge-based systems are not expert systems.\n\nThe first knowledge-based systems were rule based expert systems. One of the most famous was Mycin a program for medical diagnosis. These early expert systems represented facts about the world as simple assertions in a flat database and used rules to reason about and as a result add to these assertions. Representing knowledge explicitly via rules had several advantages:\n\n\nAs knowledge-based systems became more complex the techniques used to represent the knowledge base became more sophisticated. Rather than representing facts as assertions about data, the knowledge-base became more structured, representing information using similar techniques to object-oriented programming such as hierarchies of classes and subclasses, relations between classes, and behavior of objects. As the knowledge base became more structured reasoning could occur both by independent rules and by interactions within the knowledge base itself. For example, procedures stored as demons on objects could fire and could replicate the chaining behavior of rules.\n\nAnother advancement was the development of special purpose automated reasoning systems called classifiers. Rather than statically declare the subsumption relations in a knowledge-base a classifier allows the developer to simply declare facts about the world and let the classifier deduce the relations. In this way a classifier also can play the role of an inference engine.\n\nThe most recent advancement of knowledge-based systems has been to adopt the technologies for the development of systems that use the internet. The internet often has to deal with complex, unstructured data that can't be relied on to fit a specific data model. The technology of knowledge-based systems and especially the ability to classify objects on demand is ideal for such systems. The model for these kinds of knowledge-based Internet systems is known as the Semantic Web.\n\nWhen developing knowledge-based systems it is common to integrate knowledge with data. Constantinou et al. proposed a method for eliciting and incorporating expert knowledge in data-driven Bayesian Networks (BNs). The method addresses the problem whereby the distribution of some variable in a BN is known from data, but where we wish to explicitly model the impact of some addition.\n\n", "id": "1377612", "title": "Knowledge-based systems"}
{"url": "https://en.wikipedia.org/wiki?curid=430106", "text": "Software agent\n\nIn computer science, a software agent is a computer program that acts for a user or other program in a relationship of agency, which derives from the Latin \"agere\" (to do): an agreement to act on one's behalf. Such \"action on behalf of\" implies the authority to decide which, if any, action is appropriate. Agents are colloquially known as \"bots\", from \"robot\". They may be embodied, as when execution is paired with a robot body, or as software such as a chatbot\nexecuting on a phone (e.g. Siri) or other computing device. Software Agents may be autonomous or work together with other agents or people. Software agents interacting with people (e.g. chatbots, human-robot interaction environments) may possess human-like qualities such as natural language understanding and speech, personality or embody humanoid form (see Asimo).\n\nRelated and derived concepts include \"intelligent agents\" (in particular exhibiting some aspect of artificial intelligence, such as learning and reasoning), \"autonomous agents\" (capable of modifying the way in which they achieve their objectives), \"distributed\" agents (being executed on physically distinct computers), \"multi-agent systems\" (distributed agents that work together to achieve an objective that could not be accomplished by a single agent acting alone), and \"mobile agents\" (agents that can relocate their execution onto different processors).\n\nThe basic attributes of an autonomous software agent are that agents\nThe term \"agent\" describes a software abstraction, an idea, or a concept, similar to OOP terms such as methods, functions, and objects. The concept of an agent provides a convenient and powerful way to describe a complex software entity that is capable of acting with a certain degree of autonomy in order to accomplish tasks on behalf of its host. But unlike objects, which are defined in terms of \"methods\" and \"attributes\", an agent is defined in terms of its behavior.\n\nVarious authors have proposed different definitions of agents, these commonly include concepts such as\n\nAll agents are programs, but not all programs are agents.\nContrasting the term with related concepts may help clarify its meaning. Franklin & Graesser (1997) discuss four key notions that distinguish agents from arbitrary programs: reaction to the environment, autonomy, goal-orientation and persistence.\n\n\n\n\nSoftware agents may offer various benefits to their end users by automating complex or repetitive tasks. However, there are organizational and cultural impacts of this technology that need to be considered prior to implementing software agents.\n\nPeople like to perform easy tasks providing the sensation of success unless the repetition of the simple tasking is affecting the overall output. In general implementing software agents to perform administrative requirements provides a substantial increase in work contentment, as administering their own work does never please the worker. The effort freed up serves for a higher degree of engagement in the substantial tasks of individual work. Hence, software agents may provide the basics to implement self-controlled work, relieved from hierarchical controls and interference. Such conditions may be secured by application of software agents for required formal support.\n\nThe cultural effects of the implementation of software agents include trust affliction, skills erosion, privacy attrition and social detachment. Some users may not feel entirely comfortable fully delegating important tasks to software applications. Those who start relying solely on intelligent agents may lose important skills, for example, relating to information literacy. In order to act on a user’s behalf, a software agent needs to have a complete understanding of a user’s profile, including his/her personal preferences. This, in turn, may lead to unpredictable privacy issues. When users start relying on their software agents more, especially for communication activities, they may lose contact with other human users and look at the word with the eyes of their agents. These consequences are what agent researchers and users must consider when dealing with intelligent agent technologies.\n\nThe concept of an agent can be traced back to Hewitt's Actor Model (Hewitt, 1977) - \"A self-contained, interactive and concurrently-executing object, possessing internal state and communication capability.\"\n\nTo be more academic, software agent systems are a direct evolution of Multi-Agent Systems (MAS). MAS evolved from Distributed Artificial Intelligence (DAI), Distributed Problem Solving (DPS) and Parallel AI (PAI), thus inheriting all characteristics (good and bad) from DAI and AI.\n\nJohn Sculley’s 1987 “Knowledge Navigator” video portrayed an image of a relationship between end-users and agents. Being an ideal first, this field experienced a series of unsuccessful top-down implementations, instead of a piece-by-piece, bottom-up approach. The range of agent types is now (from 1990) broad: WWW, search engines, etc.\n\nHaag suggests that there are only four essential types of intelligent software agents:\n\nBuyer agents travel around a network (e.g. the internet) retrieving information about goods and services. These agents, also known as 'shopping bots', work very efficiently for commodity products such as CDs, books, electronic components, and other one-size-fits-all products. Buyer agents are typically optimized to allow for digital payment services used in e-commerce and traditional businesses.\n\nUser agents, or personal agents, are intelligent agents that take action on your behalf. In this category belong those intelligent agents that already perform, or will shortly perform, the following tasks:\n\nMonitoring and Surveillance Agents are used to observe and report on equipment, usually computer systems. The agents may keep track of company inventory levels, observe competitors' prices and relay them back to the company, watch stock manipulation by insider trading and rumors, etc.\nFor example, NASA's Jet Propulsion Laboratory has an agent that monitors inventory, planning, schedules equipment orders to keep costs down, and manages food storage facilities. These agents usually monitor complex computer networks that can keep track of the configuration of each computer connected to the network.\n\nA special case of Monitoring-and-Surveillance agents are organizations of agents used to emulate the Human Decision-Making process during tactical operations. The agents monitor the status of assets (ammunition, weapons available, platforms for transport, etc.) and receive Goals (Missions) from higher level agents. The Agents then pursue the Goals with the Assets at hand, minimizing expenditure of the Assets while maximizing Goal Attainment. (See Popplewell, \"Agents and Applicability\")\n\nThis agent uses information technology to find trends and patterns in an abundance of information from many different sources. The user can sort through this information in order to find whatever information they are seeking.\n\nA data mining agent operates in a data warehouse discovering information. A 'data warehouse' brings together information from lots of different sources. \"Data mining\" is the process of looking through the data warehouse to find information that you can use to take action, such as ways to increase sales or keep customers who are considering defecting.\n\n'Classification' is one of the most common types of data mining, which finds patterns in information and categorizes them into different classes. Data mining agents can also detect major shifts in trends or a key indicator and can detect the presence of new information and alert you to it. For example, the agent may detect a decline in the construction industry for an economy; based on this relayed information construction companies will be able to make intelligent decisions regarding the hiring/firing of employees or the purchase/lease of equipment in order to best suit their firm.\n\nSome other examples of current intelligent agents include some spam filters, game bots, and server monitoring tools. Search engine indexing bots also qualify as intelligent agents.\n\nIssues to consider in the development of agent-based systems include \n\nFor software agents to work together efficiently they must share semantics of their data elements. This can be done by having computer systems publish their metadata.\n\nThe definition of \"agent processing\" can be approached from two interrelated directions:\n\nAgent systems are used to model real-world systems with concurrency or parallel processing.\n\n\nThe agent uses its access methods to go out into local and remote databases to forage for content. These access methods may include setting up news stream delivery to the agent, or retrieval from bulletin boards, or using a spider to walk the Web. The content that is retrieved in this way is probably already partially filtered – by the selection of the newsfeed or the databases that are searched. The agent next may use its detailed searching or language-processing machinery to extract keywords or signatures from the body of the content that has been received or retrieved. This abstracted content (or event) is then passed to the agent’s Reasoning or inferencing machinery in order to decide what to do with the new content. This process combines the event content with the rule-based or knowledge content provided by the user. If this process finds a good hit or match in the new content, the agent may use another piece of its machinery to do a more detailed search on the content. Finally, the agent may decide to take an action based on the new content; for example, to notify the user that an important event has occurred. This action is verified by a security function and then given the authority of the user. The agent makes use of a user-access method to deliver that message to the user. If the user confirms that the event is important by acting quickly on the notification, the agent may also employ its learning machinery to increase its weighting for this kind of event.\n\nBots can act on behalf of their creators to do good as well as bad. There are a few ways which bots can be created to demonstrate that they are designed with the best intention and are not built to do harm. This is first done by having a bot identify itself in the user-agent HTTP header when communicating with a site. The source IP address must also be validated to establish itself as legitimate. Next, the bot must also always respect a site's robots.txt file since it has become the standard across most of the web. And like respecting the robots.txt file, bots should shy away from being too aggressive and respect any crawl delay instructions.\n\n\n\n", "id": "430106", "title": "Software agent"}
{"url": "https://en.wikipedia.org/wiki?curid=23048428", "text": "Kinect\n\nKinect (codenamed Project Natal during development) is a line of motion sensing input devices that was produced by Microsoft for Xbox 360 and Xbox One video game consoles and Microsoft Windows PCs. Based around a webcam-style add-on peripheral, it enables users to control and interact with their console/computer without the need for a game controller, through a natural user interface using gestures and spoken commands.\n\nThe first-generation Kinect was first introduced in November 2010 in an attempt to broaden Xbox 360's audience beyond its typical gamer base. A version for Microsoft Windows was released on February 1, 2012. A newer version, Kinect 2.0, was released with the Xbox One platform starting in 2013.\n\nMicrosoft released the first Beta of the Kinect software development kit for Windows 7 on June 16, 2011. This SDK was meant to allow developers to write Kinecting apps in C++/CLI, C#, or Visual Basic .NET.\n\nUltimately, the Kinect for either Xbox console did not have long-term popularity, and the two versions were discontinued by April 2016 and October 2017, respectively. The Windows version had been discontinued by April 2015.\n\nKinect was first announced on June 1, 2009 at E3 2009 under the code name \"Project Natal\". Three demos were shown to showcase Kinect when it was revealed at Microsoft's E3 2009 Media Briefing: \"Ricochet\", \"Paint Party\" and \"Milo & Kate\". A demo based on \"Burnout Paradise\" was also shown outside of Microsoft's media briefing. The skeletal mapping technology shown at E3 2009 was capable of simultaneously tracking four people, with a feature extraction of 48 skeletal points on a human body at 30 Hz.\n\nIt was rumored that the launch of Project Natal would be accompanied with the release of a new Xbox 360 console (as either a new retail configuration, a significant design revision and/or a modest hardware upgrade). Microsoft dismissed the reports in public and repeatedly emphasized that Project Natal would be fully compatible with all Xbox 360 consoles. Microsoft indicated that the company considers it to be a significant initiative, as fundamental to Xbox brand as Xbox Live, and with a launch akin to that of a new Xbox console platform. Kinect was even referred to as a \"new Xbox\" by Microsoft CEO Steve Ballmer at a speech for Executives' Club of Chicago. When asked if the introduction will extend the time before the next-generation console platform is launched (historically about 5 years between platforms), Microsoft corporate vice president Shane Kim reaffirmed that the company believes that the life cycle of Xbox 360 will last through 2015 (10 years).\n\nDuring Kinect's development, project team members experimentally adapted numerous games to Kinect-based control schemes to help evaluate usability. Among these games were \"Beautiful Katamari\" and \"Space Invaders Extreme\", which were demonstrated at Tokyo Game Show in September 2009. According to creative director Kudo Tsunoda, adding Kinect-based control to pre-existing games would involve significant code alterations, making it unlikely for Kinect features to be added through software updates.\n\nAlthough the sensor unit was originally planned to contain a microprocessor that would perform operations such as the system's skeletal mapping, it was revealed in January 2010 that the sensor would no longer feature a dedicated processor. Instead, processing would be handled by one of the processor cores of Xbox 360's Xenon CPU. According to Alex Kipman, Kinect system consumes about 10-15% of Xbox 360's computing resources. However, in November, Alex Kipman made a statement that \"the new motion control tech now only uses a single-digit percentage of Xbox 360's processing power, down from the previously stated 10 to 15 percent.\" A number of observers commented that the computational load required for Kinect makes the addition of Kinect functionality to pre-existing games through software updates even less likely, with concepts specific to Kinect more likely to be the focus for developers using the platform.\n\nOn March 25, 2010, Microsoft sent out a save the date flier for an event called the \"World Premiere 'Project Natal' for Xbox 360 Experience\" at E3 2010. The event took place on the evening of Sunday, June 13, 2010 at Galen Center and featured a performance by Cirque du Soleil. It was announced that the system would officially be called Kinect, a portmanteau of the words \"kinetic\" and \"connect\", which describe key aspects of the initiative. Microsoft also announced that the North American launch date for Kinect will be November 4, 2010. Despite previous statements dismissing speculation of a new Xbox 360 to accompany the launch of the new control system, Microsoft announced at E3 2010 that it was introducing a redesigned Xbox 360, complete with a connector port ready for Kinect. In addition, on July 20, 2010, Microsoft announced a Kinect bundle with a redesigned Xbox 360, to be available with Kinect launch.\n\nOn June 16, 2011, Microsoft announced its official release of its SDK for non-commercial use.\nOn July 21, 2011, Microsoft announced that the first ever white Kinect sensor would be available as part of \"Xbox 360 Limited Edition Kinect Star Wars Bundle\", which also includes custom a \"Star Wars\"-themed console and controller, and copies of \"Kinect Adventures\" and \"Kinect Star Wars\". Previously, all Kinect sensors had been glossy black.\n\nOn October 31, 2011, Microsoft announced launching of the commercial version of \"Kinect for Windows program\" with release of SDK to companies. David Dennis, Product Manager at Microsoft, said, \"There are hundreds of organizations we are working with to help them determine what's possible with the tech\".\n\nOn February 1, 2012, Microsoft released the commercial version of Kinect for Windows SDK and told that more than 300 companies from over 25 countries are working on Kinect-ready apps.\n\nThe tabloid the New York Post claimed Microsoft had a $500 million budget for advertising the launch of Kinect. While this claim was widely re-reported, an examination of the 10-Q from Microsoft reveals a 20% year-over-year increase ($85 million) for sales and marketing the quarter Kinect was launched for all of the Entertainment and Devices division, making the total sales and marketing spend $425 million for the entire division. The marketing campaign \"You Are the Controller\", aiming to reach new audiences, included advertisements on Kellogg's cereal boxes and Pepsi bottles, commercials during shows such as \"Dancing with the Stars\" and \"Glee\" as well as print ads in various magazines such as \"People\" and \"InStyle\".\n\nOn October 19, Microsoft advertised Kinect on The Oprah Winfrey Show by giving free Xbox 360 consoles and Kinect sensors to the people in the audience. Two weeks later, Kinect bundles with Xbox 360 consoles were also given away to the audience of Late Night with Jimmy Fallon. On October 23, Microsoft held a pre-launch party for Kinect in Beverly Hills. The party was hosted by Ashley Tisdale and was attended by soccer star David Beckham and his three sons, Cruz, Brooklyn, and Romeo. Guests were treated to sessions with Dance Central and Kinect Adventures, followed by Tisdale having a Kinect voice chat with Nick Cannon. Between November 1 and 28, Burger King gave away a free Kinect bundle \"every 15 minutes\".\n\nA major event was organized on November 3 in Times Square, where singer Ne-Yo performed with hundreds of dancers in anticipation of Kinect's midnight launch.\n\nKinect was launched in North America on November 4, 2010, in Europe on November 10, 2010, in Australia, New Zealand and Singapore on November 18, 2010, and in Japan on November 20, 2010. Purchase options for the sensor peripheral include a bundle with the game \"Kinect Adventures\" and console bundles with either a 4 GB or 250 GB Xbox 360 console and \"Kinect Adventures\".\n\nKinect V1 was a combination of Microsoft built software and hardware. Kinect V1 hardware included a range chipset technology by Israeli developer PrimeSense, which developed a system consisting of an infrared projector and camera and a special microchip that generates a grid from which the location of a nearby object in 3 dimensions can be ascertained. This 3D scanner system called \"Light Coding\" employs a variant of image-based 3D reconstruction.\n\nThe Kinect sensor is a horizontal bar connected to a small base with a motorized pivot and is designed to be positioned lengthwise above or below the video display. The device features an \"RGB camera, depth sensor and multi-array microphone running proprietary software\", which provide full-body 3D motion capture, facial recognition and voice recognition capabilities. At launch, voice recognition was only made available in Japan, United Kingdom, Canada and United States. Mainland Europe received the feature later in spring 2011. Currently voice recognition is supported in Australia, Canada, France, Germany, Ireland, Italy, Japan, Mexico, New Zealand, United Kingdom and United States. Kinect sensor's microphone array enables Xbox 360 to conduct acoustic source localization and ambient noise suppression, allowing for things such as headset-free party chat over Xbox Live.\n\nThe depth sensor consists of an infrared laser projector combined with a monochrome CMOS sensor, which captures video data in 3D under any ambient light conditions. The sensing range of the depth sensor is adjustable, and Kinect software is capable of automatically calibrating the sensor based on gameplay and the player's physical environment, accommodating for the presence of furniture or other obstacles.\n\nDescribed by Microsoft personnel as the primary innovation of Kinect, the software technology enables advanced gesture recognition, facial recognition and voice recognition. According to information supplied to retailers, Kinect is capable of simultaneously tracking up to six people, including two active players for motion analysis with a feature extraction of 20 joints per player. However, PrimeSense has stated that the number of people the device can \"see\" (but not process as players) is only limited by how many will fit in the field-of-view of the camera.\n\nReverse engineering has determined that the Kinect's various sensors output video at a frame rate of ≈9 Hz to 30 Hz depending on resolution. The default RGB video stream uses 8-bit VGA resolution (640 × 480 pixels) with a Bayer color filter, but the hardware is capable of resolutions up to 1280x1024 (at a lower frame rate) and other colour formats such as UYVY. The monochrome depth sensing video stream is in VGA resolution (640 × 480 pixels) with 11-bit depth, which provides 2,048 levels of sensitivity. The Kinect can also stream the view from its IR camera directly (i.e.: before it has been converted into a depth map) as 640x480 video, or 1280x1024 at a lower frame rate. The Kinect sensor has a practical ranging limit of distance when used with the Xbox software. The area required to play Kinect is roughly 6 m, although the sensor can maintain tracking through an extended range of approximately . The sensor has an angular field of view of 57° horizontally and 43° vertically, while the motorized pivot is capable of tilting the sensor up to 27° either up or down. The horizontal field of the Kinect sensor at the minimum viewing distance of ≈ is therefore ≈, and the vertical field is ≈, resulting in a resolution of just over per pixel. The microphone array features four microphone capsules and operates with each channel processing 16-bit audio at a sampling rate of 16 kHz.\n\nBecause the Kinect sensor's motorized tilt mechanism requires more power than the Xbox 360's USB ports can supply, the device makes use of a proprietary connector combining USB communication with additional power. Redesigned Xbox 360 S models include a special AUX port for accommodating the connector, while older models require a special power supply cable (included with the sensor) that splits the connection into separate USB and power connections; power is supplied from the mains by way of an AC adapter.\n\nKinect 2.0 was released with Xbox One on November 22, 2013. The hardware included a time-of-flight sensor developed by Microsoft, replacing the older technology from PrimeSense.\n\nXbox One consoles ship with an updated version of Kinect; the new Kinect uses a wide-angle time-of-flight camera, and processes 2 gigabits of data per second to read its environment. The new Kinect has greater accuracy with three times the fidelity over its predecessor and can track without visible light by using an active IR sensor. It has a 60% wider field of vision that can detect a user up to 3 feet from the sensor, compared to six feet for the original Kinect, and can track up to 6 skeletons at once. It can also detect a player's heart rate, facial expression, the position and orientation of 25 individual joints (including thumbs), the weight put on each limb, speed of player movements, and track gestures performed with a standard controller. The color camera captures 1080p video that can be displayed in the same resolution as the viewing screen, allowing for a broad range of scenarios. In addition to improving video communications and video analytics applications, this provides a stable input on which to build interactive applications. Kinect's microphone is used to provide voice commands for actions such as navigation, starting games, and waking the console from sleep mode.\n\nAll Xbox One consoles were initially shipped with the Kinect sensor included—a holdover from a previously-announced, but retracted mandate requiring Kinect to be plugged into the console at all times for it to function. In June 2014, bundles without Kinect were made available, along with an updated Xbox One SDK allowing game developers to explicitly disable Kinect skeletal tracking, freeing up system resources that were previously reserved for Kinect even if it was disabled or unplugged.\n\nA standalone Kinect for Xbox One, bundled with a digital copy of \"Dance Central Spotlight\", was released on October 7, 2014.\n\nOn February 21, 2011 Microsoft announced that it would release a non-commercial Kinect software development kit (SDK) for Microsoft Windows in spring 2011, and the first beta was released for Windows 7 on June 16, 2011. Beta 2 was released on the 1 year anniversary of Kinect for Xbox 360, on November 3, 2011. Version 1.0 was released at the same time as the officially supported Microsoft Windows hardware on February 1, 2012.\n\nThe SDK includes Windows 7 compatible PC drivers for Kinect device. It provides Kinect capabilities to developers to build applications with C++, C#, or Visual Basic by using Microsoft Visual Studio 2010 and includes following features:\n\n\nIn March 2012, Craig Eisler, the general manager of Kinect for Windows, said that almost 350 companies are working with Microsoft on custom Kinect applications for Microsoft Windows.\n\nIn March 2012, Microsoft announced that next version of Kinect for Windows SDK would be available in May 2012. Kinect for Windows 1.5 was released on May 21, 2012. It adds new features, support for many new languages and debut in 19 more countries.\n\n\nKinect for Windows SDK for the first-generation sensor was updated a few more times, with version 1.6 released October 8, 2012, version 1.7 released March 18, 2013, and version 1.8 released September 17, 2013.\n\nThe second-generation Kinect for Windows, based on the same core technology as Kinect for Xbox One, including a new sensor, was first released in 2014.\n\nRequiring at least 190 MB of available storage space, Kinect system software allows users to operate Xbox 360 Dashboard console user interface through voice commands and hand gestures. Techniques such as voice recognition and facial recognition are employed to automatically identify users. Among the applications for Kinect is Video Kinect, which enables voice chat or video chat with other Xbox 360 users or users of Windows Live Messenger. The application can use Kinect's tracking functionality and Kinect sensor's motorized pivot to keep users in frame even as they move around. Other applications with Kinect support include ESPN, Zune Marketplace, Netflix, Hulu Plus and Last.fm. Microsoft later confirmed that all forthcoming applications would be required to have Kinect functionality for certification.\n\nXbox 360 games that require Kinect are packaged in special purple cases (as opposed to the green cases used by all other Xbox 360 games), and contain a prominent \"Requires Kinect Sensor\" logo on their front cover. Games that include features utilizing Kinect, but do not require it for standard gameplay, contain a \"Better with Kinect Sensor\" branding on their front covers.\n\nKinect launched on November 4, 2010 with 17 titles. Third-party publishers of available and announced Kinect games include, among others, Ubisoft, Electronic Arts, LucasArts, THQ, Activision, Konami, Sega, Capcom, Namco Bandai and MTV Games. Along with retail games, there are also select Xbox Live Arcade titles which require the peripheral.\n\nAt E3 2011, Microsoft announced \"Kinect Fun Labs\": a collection of various gadgets and minigames that are accessible from Xbox 360 Dashboard. These gadgets includes \"Build A Buddy\", \"Air Band\", \"Kinect Googly Eyes\", \"Kinect Me\", \"Bobblehead\", \"Kinect Sparkler\", \"Junk Fu\" and \"Avatar Kinect\".\n\nIn November 2010, Adafruit Industries offered a bounty for an open-source driver for Kinect. Microsoft initially voiced its disapproval of the bounty, stating that it \"does not condone the modification of its products\" and that it had \"built in numerous hardware and software safeguards designed to reduce the chances of product tampering\". This reaction, however, was caused by a misunderstanding within Microsoft, and the company later clarified its position, claiming that while it does not condone hacking of either the physical device or the console, the USB connection was left open by design.\n\nOn November 10, Adafruit announced Héctor Martín as the winner, who had produced a Linux driver that allows the use of both the RGB camera and depth sensitivity functions of the device. It was later revealed that Johnny Lee, a core member of Microsoft's Kinect development team, had secretly approached Adafruit with the idea of a driver development contest and had personally financed it.\n\nIn December 2010, PrimeSense, whose depth sensing chips were used in the Kinect v1 hardware, released their own open source drivers along with motion tracking middleware called \"NITE\". PrimeSense later announced that it had teamed up with Asus to develop a PC-compatible device similar to Kinect for Chinese markets, called the Wavi Xtion. The product was released in October 2011.\n\nOpenNI is an open-source software framework that is able to read sensor data from Kinect, among other natural user interface sensors.\n\nNumerous developers are researching possible applications of Kinect that go beyond the system's intended purpose of playing games. For example, Philipp Robbel of MIT combined Kinect with iRobot Create to map a room in 3D and have the robot respond to human gestures, while an MIT Media Lab team is working on a JavaScript extension for Google Chrome called depthJS that allows users to control the browser with hand gestures. Other programmers, including Robot Locomotion Group at MIT, are using the drivers to develop a motion-controller user interface similar to the one envisioned in \"Minority Report\". The developers of MRPT have integrated open source drivers into their libraries and provided examples of live 3D rendering and basic 3D visual SLAM. Another team has shown an application that allows Kinect users to play a virtual piano by tapping their fingers on an empty desk. Oliver Kreylos, a researcher at University of California, Davis, adopted the technology to improve live 3-dimensional videoconferencing, which NASA has shown interest in.\n\nAlexandre Alahi from EPFL presented a video surveillance system that combines multiple Kinect devices to track groups of people even in complete darkness. Companies So touch and Evoluce have developed presentation software for Kinect that can be controlled by hand gestures; among its features is a multi-touch zoom mode. In December 2010, the free public beta of HTPC software \"KinEmote\" was launched; it allows navigation of Boxee and XBMC menus using a Kinect sensor. Soroush Falahati wrote an application that can be used to create stereoscopic 3D images with a Kinect sensor.\n\nFor a limited time in May 2011, a Topshop store in Moscow set up a Kinect kiosk that could overlay a collection of dresses onto the live video feed of customers. Through automatic tracking, position and rotation of the virtual dress were updated even as customers turned around to see the back of the outfit.\n\nKinect also shows compelling potential for use in medicine. Researchers at the University of Minnesota have used Kinect to measure a range of disorder symptoms in children, creating new ways of objective evaluation to detect such conditions as autism, attention-deficit disorder and obsessive-compulsive disorder. Several groups have reported using Kinect for intraoperative, review of medical imaging, allowing the surgeon to access the information without contamination. This technique is already in use at Sunnybrook Health Sciences Centre in Toronto, where doctors use it to guide imaging during cancer surgery. At least one company, GestSure Technologies, is pursuing the commercialization of such a system.\n\nNASA's Jet Propulsion Laboratory (JPL) signed up for the Kinect for Windows Developer program in November 2013 to use the new Kinect to manipulate a robotic arm in combination with an Oculus Rift virtual reality headset, creating \"the most immersive interface\" the unit had built to date.\n\nAnother application of Kinect is for multi-touch displays. A Seattle-based company that graduated from Microsoft's Kinect Accelerator, Ubi Interactive, developed software for the Kinect to work with projectors to allow touchscreen-like capabilities on various surfaces.\n\nKinect is used with a number of ROS robots via OpenNI.\n\nUpon its release, the Kinect garnered positive opinions from reviewers and critics. IGN gave the device 7.5 out of 10, saying that \"Kinect can be a tremendous amount of fun for casual players, and the creative, controller-free concept is undeniably appealing\", though adding that for \"$149.99, a motion-tracking camera add-on for Xbox 360 is a tough sell, especially considering that the entry level variation of Xbox 360 itself is only $199.99\". \"Game Informer\" rated Kinect 8 out of 10, praising the technology but noting that the experience takes a while to get used to and that the spatial requirement may pose a barrier. \"Computer and Video Games\" called the device a technological gem and applauded the gesture and voice controls, while criticizing the launch lineup and Kinect Hub.\n\nCNET's review pointed out how Kinect keeps players active with its full-body motion sensing but criticized the learning curve, the additional power supply needed for older Xbox 360 consoles and the space requirements. Engadget, too, listed the large space requirements as a negative, along with Kinect's launch lineup and the slowness of the hand gesture UI. The review praised the system's powerful technology and the potential of its yoga and dance games. Kotaku considered the device revolutionary upon first use but noted that games were sometimes unable to recognize gestures or had slow responses, concluding that Kinect is \"not must-own yet, more like must-eventually own.\" TechRadar praised the voice control and saw a great deal of potential in the device whose lag and space requirements were identified as issues. Gizmodo also noted Kinect's potential and expressed curiosity in how more mainstream titles would utilize the technology. \"Ars Technica's\" review expressed concern that the core feature of Kinect, its lack of a controller, would hamper development of games beyond those that have either stationary players or control the player's movement automatically.\n\nThe mainstream press also reviewed Kinect. \"USA Today\" compared it to the futuristic control scheme seen in \"Minority Report\", stating that \"playing games feels great\" and giving the device 3.5 out of 4 stars. David Pogue from \"The New York Times\" predicted players will feel a \"crazy, magical, omigosh rush the first time you try the Kinect.\" Despite calling the motion tracking less precise than Wii's implementation, Pogue concluded that \"Kinect’s astonishing technology creates a completely new activity that’s social, age-spanning and even athletic.\" \"The Globe and Mail\" titled Kinect as setting a \"new standard for motion control.\" The slight input lag between making a physical movement and Kinect registering it was not considered a major issue with most games, and the review called Kinect \"a good and innovative product,\" rating it 3.5 out of 4 stars.\n\nAlthough featuring improved performance over the original Kinect, its successor has been subject to mixed responses. In its Xbox One review, \"Engadget\" praised Xbox One's Kinect functionality, such as face recognition login and improved motion tracking, but that whilst \"magical\", \"every false positive or unrecognized [voice] command had us reaching for the controller.\" The Kinect's inability to understand some accents in English was criticized. Writing for \"Time\", Matt Peckham described the device as being \"chunky\" in appearance, but that the facial recognition login feature was \"creepy but equally sci-fi-future cool\", and that the new voice recognition system was a \"powerful, addictive way to navigate the console, and save for a few exceptions that seem to be smoothing out with use\". However, its accuracy was found to be affected by background noise, and Peckham further noted that launching games using voice recognition required that the full title of the game be given rather than an abbreviated name that the console \"ought to semantically understand\", such as \"Forza Motorsport 5\" rather than \"Forza 5\".\n\nPrior to Xbox One's launch, privacy concerns were raised over the new Kinect; critics showed concerns the device could be used for surveillance, stemming from the originally announced requirements that Xbox One's Kinect be plugged in at all times, plus the initial always-on DRM system that required the console to be connected to the internet to ensure continued functionality. Privacy advocates contended that the increased amount of data which could be collected with the new Kinect (such as a person's eye movements, heart rate, and mood) could be used for targeted advertising. Reports also surfaced regarding recent Microsoft patents involving Kinect, such as a DRM system based on detecting the number of viewers in a room, and tracking viewing habits by awarding achievements for watching television programs and advertising. While Microsoft stated that its privacy policy \"prohibit[s] the collection, storage, or use of Kinect data for the purpose of advertising\", critics did not rule out the possibility that these policies could be changed prior to the release of the console. Concerns were also raised that the device could also record conversations, as its microphone remains active at all times. In response to the criticism, a Microsoft spokesperson stated that users are \"in control of when Kinect sensing is On, Off or Paused\", will be provided with key privacy information and settings during the console's initial setup, and that user-generated content such as photos and videos \"will not leave your Xbox One without your explicit permission.\" Microsoft ultimately decided to reverse its decision to require Kinect usage on Xbox One, but the console still shipped with the device upon its launch in November 2013.\n\nWhile announcing Kinect's discontinuation in an interview with Fast Co. Design on October 25, 2017, Microsoft stated that 35 million units had been sold since its release. 24 million units of Kinect had been shipped by February 2013. Having sold 8 million units in its first 60 days on the market, Kinect claimed the Guinness World Record of being the \"fastest selling consumer electronics device\". According to Wedbush analyst Michael Pachter, Kinect bundles accounted for about half of all Xbox 360 console sales in December 2010 and for more than two-thirds in February 2011. More than 750,000 Kinect units were sold during the week of Black Friday 2011.\n\nKinect competes with several motion controllers on other home consoles, such as Wii Remote Plus for Wii and Wii U, PlayStation Move/PlayStation Eye for PlayStation 3, and PlayStation Camera for PlayStation 4.\n\n\n\n", "id": "23048428", "title": "Kinect"}
{"url": "https://en.wikipedia.org/wiki?curid=98778", "text": "Natural language understanding\n\nNatural language understanding (NLU) is a subtopic of natural language processing in artificial intelligence that deals with machine reading comprehension. NLU is considered an AI-hard problem.\n\nThe process of disassembling and parsing input is more complex than the reverse process of assembling output in natural language generation because of the occurrence of unknown and unexpected features in the input and the need to determine the appropriate syntactic and semantic schemes to apply to it, factors which are pre-determined when outputting language.\n\nThere is considerable commercial interest in the field because of its application to news-gathering, text categorization, voice-activation, archiving, and large-scale content-analysis.\n\nThe program STUDENT, written in 1964 by Daniel Bobrow for his PhD dissertation at MIT is one of the earliest known attempts at natural language understanding by a computer. Eight years after John McCarthy coined the term artificial intelligence, Bobrow's dissertation (titled \"Natural Language Input for a Computer Problem Solving System\") showed how a computer can understand simple natural language input to solve algebra word problems.\n\nA year later, in 1965, Joseph Weizenbaum at MIT wrote ELIZA, an interactive program that carried on a dialogue in English on any topic, the most popular being psychotherapy. ELIZA worked by simple parsing and substitution of key words into canned phrases and Weizenbaum sidestepped the problem of giving the program a database of real-world knowledge or a rich lexicon. Yet ELIZA gained surprising popularity as a toy project and can be seen as a very early precursor to current commercial systems such as those used by Ask.com.\n\nIn 1969 Roger Schank at Stanford University introduced the conceptual dependency theory for natural language understanding. This model, partially influenced by the work of Sydney Lamb, was extensively used by Schank's students at Yale University, such as Robert Wilensky, Wendy Lehnert, and Janet Kolodner.\n\nIn 1970, William A. Woods introduced the augmented transition network (ATN) to represent natural language input. Instead of \"phrase structure rules\" ATNs used an equivalent set of finite state automata that were called recursively. ATNs and their more general format called \"generalized ATNs\" continued to be used for a number of years.\n\nIn 1971 Terry Winograd finished writing SHRDLU for his PhD thesis at MIT. SHRDLU could understand simple English sentences in a restricted world of children's blocks to direct a robotic arm to move items. The successful demonstration of SHRDLU provided significant momentum for continued research in the field. Winograd continued to be a major influence in the field with the publication of his book \"Language as a Cognitive Process\". At Stanford, Winograd would later be the adviser for Larry Page, who co-founded Google.\n\nIn the 1970s and 1980s the natural language processing group at SRI International continued research and development in the field. A number of commercial efforts based on the research were undertaken, \"e.g.\", in 1982 Gary Hendrix formed Symantec Corporation originally as a company for developing a natural language interface for database queries on personal computers. However, with the advent of mouse driven, graphic user interfaces Symantec changed direction. A number of other commercial efforts were started around the same time, \"e.g.\", Larry R. Harris at the Artificial Intelligence Corporation and Roger Schank and his students at Cognitive Systems corp. In 1983, Michael Dyer developed the BORIS system at Yale which bore similarities to the work of Roger Schank and W. G. Lehnart.\n\nThe third millennium saw the introduction of systems using machine learning for text classification, such as the IBM Watson. However, this is not NLU. According to John Searle, Watson did not even understand the questions.\n\nJohn Ball, cognitive scientist and inventor of Patom Theory supports this assessment. NLP has made inroads for applications to support human productivity in service and ecommerce but this has largely been made possible by narrowing the scope of the application. There are thousands of ways to request something in a human language which still defies conventional NLP. \"To have a meaningful conversation with machines is only possible when we match every word to the correct meaning based on the meanings of the other words in the sentence – just like a 3-year-old does without guesswork\" Patom Theory\n\nThe umbrella term \"natural language understanding\" can be applied to a diverse set of computer applications, ranging from small, relatively simple tasks such as short commands issued to robots, to highly complex endeavors such as the full comprehension of newspaper articles or poetry passages. Many real world applications fall between the two extremes, for instance text classification for the automatic analysis of emails and their routing to a suitable department in a corporation does not require in depth understanding of the text, but needs to deal with a much larger vocabulary and more diverse syntax than the management of simple queries to database tables with fixed schemata.\n\nThroughout the years various attempts at processing natural language or \"English-like\" sentences presented to computers have taken place at varying degrees of complexity. Some attempts have not resulted in systems with deep understanding, but have helped overall system usability. For example, Wayne Ratliff originally developed the \"Vulcan\" program with an English-like syntax to mimic the English speaking computer in Star Trek. Vulcan later became the dBase system whose easy-to-use syntax effectively launched the personal computer database industry. Systems with an easy to use or \"English like\" syntax are, however, quite distinct from systems that use a rich lexicon and include an internal representation (often as first order logic) of the semantics of natural language sentences.\n\nHence the breadth and depth of \"understanding\" aimed at by a system determine both the complexity of the system (and the implied challenges) and the types of applications it can deal with. The \"breadth\" of a system is measured by the sizes of its vocabulary and grammar. The \"depth\" is measured by the degree to which its understanding approximates that of a fluent native speaker. At the narrowest and shallowest, \"English-like\" command interpreters require minimal complexity, but have a small range of applications. Narrow but deep systems explore and model mechanisms of understanding, but they still have limited application. Systems that attempt to understand the contents of a document such as a news release beyond simple keyword matching and to judge its suitability for a user are broader and require significant complexity, but they are still somewhat shallow. Systems that are both very broad and very deep are beyond the current state of the art.\n\nRegardless of the approach used, most natural language understanding systems share some common components. The system needs a lexicon of the language and a parser and grammar rules to break sentences into an internal representation. The construction of a rich lexicon with a suitable ontology requires significant effort, \"e.g.\", the Wordnet lexicon required many person-years of effort.\n\nThe system also needs a \"semantic theory\" to guide the comprehension. The interpretation capabilities of a language understanding system depend on the semantic theory it uses. Competing semantic theories of language have specific trade offs in their suitability as the basis of computer-automated semantic interpretation. These range from \"naive semantics\" or \"stochastic semantic analysis\" to the use of \"pragmatics\" to derive meaning from context.\n\nAdvanced applications of natural language understanding also attempt to incorporate logical inference within their framework. This is generally achieved by mapping the derived meaning into a set of assertions in predicate logic, then using logical deduction to arrive at conclusions. Therefore, systems based on functional languages such as Lisp need to include a subsystem to represent logical assertions, while logic-oriented systems such as those using the language Prolog generally rely on an extension of the built-in logical representation framework.\n\nThe management of context in natural language understanding can present special challenges. A large variety of examples and counter examples have resulted in multiple approaches to the formal modeling of context, each with specific strengths and weaknesses.\n\n", "id": "98778", "title": "Natural language understanding"}
{"url": "https://en.wikipedia.org/wiki?curid=7347241", "text": "Spreading activation\n\nSpreading activation is a method for searching associative networks, neural networks, or semantic networks. The search process is initiated by labeling a set of source nodes (e.g. concepts in a semantic network) with weights or \"activation\" and then iteratively propagating or \"spreading\" that activation out to other nodes linked to the source nodes. Most often these \"weights\" are real values that decay as activation propagates through the network. When the weights are discrete this process is often referred to as marker passing. Activation may originate from alternate paths, identified by distinct markers, and terminate when two alternate paths reach the same node. However brain studies show that several different brain areas play an important role in semantic processing.\n\nSpreading activation models are used in cognitive psychology to model the fan out effect.\n\nSpreading activation can also be applied in information retrieval, by means of a network of nodes representing documents and terms contained in those documents.\n\nAs it relates to cognitive psychology, spreading activation is how the brain moves through an entire network of ideas to retrieve specific information. The spreading activation theory presents the array of concepts within our memory as cognitive units, each consisting of a node and its associated elements or characteristics, all connected together by edges. A spreading activation network can be represented schematically, in a sort of web diagram with shorter lines between two nodes meaning the ideas are more closely related and will typically be associated more quickly to the original concept.\n\nWhen a word (the target) is preceded by an associated word (the prime) in word recognition tasks, participants seem to perform better in the amount of time that it takes them to respond. For instance, subjects respond faster to the word \"doctor\" when it is preceded by \"nurse\" than when it is preceded by an unrelated word like \"carrot\". This semantic priming effect with words that are close in meaning within the cognitive network has been seen in a wide range of tasks given by experimenters, ranging from sentence verification to lexical decision and naming.\n\nAs another example, if the original concept is \"red\" and the concept \"vehicles\" is primed, they are much more likely to say \"fire engine\" instead of something unrelated to vehicles, such as \"cherries\". If instead \"fruits\" was primed, they would likely name \"cherries\" and continue on from there. The activation of pathways in the network has everything to do with how closely linked two concepts are by meaning, as well as how a subject is primed.\n\nA directed graph is populated by Nodes[ 1...N ] each having an associated activation value A [ i ] which is a real number in the range [ 0.0 ... 1.0]. A Link[ i, j ] connects source node[ i ] with target node[ j ]. Each edge has an associated weight W [ i, j ] usually a real number in the range [0.0 ... 1.0].\n\nParameters:\n\nSteps:\n\n\n", "id": "7347241", "title": "Spreading activation"}
{"url": "https://en.wikipedia.org/wiki?curid=17953841", "text": "OpenIRIS\n\nOpenIRIS is the open source version of IRIS, a semantic desktop that enables users to create a \"personal map\" across their office-related information objects. The name IRIS is an acronym for \"Integrate. Relate. Infer. Share.\"\n\nIRIS includes a machine-learning platform to help automate this process. It provides \"dashboard\" views, contextual navigation, and relationship-based structure across an extensible suite of office applications, including a calendar, web and file browser, e-mail client, and instant messaging client.\n\nIRIS was built as part of SRI International's CALO project, a very large artificial intelligence funded by the Defense Advanced Research Projects Agency (DARPA) under its Personalized Assistant that Learns program.\n\n\n\nCALO-funded research resulted in more than five hundred publications across all fields of artificial intelligence. Here are a few:\n\n\n", "id": "17953841", "title": "OpenIRIS"}
{"url": "https://en.wikipedia.org/wiki?curid=14659967", "text": "Project Joshua Blue\n\nJoshua Blue is a project under development by IBM that focuses on advancing the artificial intelligence field by designing and programing computers to emulate human mental functions.\n\nAccording to researchers at IBM's Thomas J. Watson Research Center, the main goal of Joshua Blue is \"to achieve cognitive flexibility that approaches human functioning\". In short, IBM is aiming to design Joshua Blue to 'think like a human', mainly in terms of emotional thought; similar IBM projects focusing on logical thought and strategic reasoning include Deep Blue, a logic-based chess playing computer, and Watson, a question-driven artificial intelligence software program. Currently, the vast majority of computers and computational systems run off of an input-output model; some sort of input is entered in and some output is given back. Through Project Joshua Blue, IBM hopes to develop computers to the point where they are asking questions and searching for answers themselves rather than relying on an external input to run or only crunching numbers to give a pre-programmed response once given a task. If they succeed in this task, the artificial intelligence knowledge gained from Project Joshua Blue could potentially be used to create social robots that work and act very much like humans do. These robots could take over tasks too dangerous for humans to engage in even if such tasks required many different decisions to be made along the way; the technology advancement gained through Joshua Blue's potential success would allow for the robots to think for themselves and work their way through problems just as humans do.\n\nA model of Joshua Blue's learning pattern has been created. Similar to how young children learn human traits through interacting with their surroundings, Joshua Blue will acquire knowledge through external stimuli present in its environment. IBM believes that if computers evolve to learn in this way and then comprehend and analyze the knowledge gained using reason, computers could begin to possess a \"mind\", of sorts, capable of demonstrating complex social behaviors similar to those of humans.\n\nMost likely done as a precaution to avoid theft of their ideas, IBM has not released any significant information regarding how Joshua Blue will physically gather information. Thus far, IBM has revealed that Joshua Blue will be a computer with a network of wires and input nodes that function as a computer nervous system. This nervous system will be used by Joshua Blue to perceive affect, or personal emotional feeling. Not only will this network of input nodes help Joshua Blue discover things physically, it will also allow Joshua Blue to interpret the significance of events. The input nodes, or proprioceptors, will enable Joshua Blue to be aware of things that happen around itself, as well as recognize and attach meaning to the emotional effect produced by interacting with an object in a certain way. In addition, Joshua Blue's proprioceptors will function as pain and pleasure sensors, allowing Joshua Blue to employ a similar \"reward and punishment\" system that humans use to form behaviors. This will eventually lead Joshua Blue to expect certain outcomes from acting, reacting, or interacting in certain ways, eventually leading to the development of behavioral patterns which IBM hopes to study in order to further bridge the gap between human mental functions and computer functions.\n\nThrough three of their projects (Deep Blue, Watson, and Joshua Blue), IBM is attempting to create computers that imitate the common functions of the human brain. \n\n", "id": "14659967", "title": "Project Joshua Blue"}
{"url": "https://en.wikipedia.org/wiki?curid=6278094", "text": "Means-ends analysis\n\nMeans-ends analysis (MEA) is a problem solving technique used commonly in artificial intelligence (AI) for limiting search in AI programs.\n\nIt is also a technique used at least since the 1950s as a creativity tool, most frequently mentioned in engineering books on design methods. MEA is also related to means-ends chain approach used commonly in consumer behavior analysis. It is also a way to clarify one's thoughts when embarking on a mathematical proof.\n\nAn important aspect of intelligent behavior as studied in AI is \"goal-based\" problem solving, a framework in which the solution of a problem can be described by finding a sequence of \"actions\" that lead to a desirable goal. A goal-seeking system is supposed to be connected to its outside environment by sensory channels through which it receives information about the environment and motor channels through which it acts on the environment. (The term \"afferent\" is used to describe \"inward\" sensory flows, and \"efferent\" is used to describe \"outward\" motor commands.) In addition, the system has some means of storing in a \"memory\" information about the \"state\" of the environment (afferent information) and information about actions (efferent information). Ability to attain goals depends on building up associations, simple or complex, between particular changes in states and particular actions that will bring these changes about. Search is the process of discovery and assembly of sequences of actions that will lead from a given state to a desired state. While this strategy may be appropriate for machine learning and problem solving, it is not always suggested for humans (e.g. cognitive load theory and its implications).\n\nThe MEA technique is a strategy to control search in problem-solving. Given a current state and a goal state, an action is chosen which will reduce the \"difference\" between the two. The action is performed on the current state to produce a new state, and the process is recursively applied to this new state and the goal state.\n\nNote that, in order for MEA to be effective, the goal-seeking system must have a means of associating to any kind of detectable difference those actions that are relevant to reducing that difference. It must also have means for detecting the progress it is making (the changes in the differences between the actual and the desired state), as some attempted sequences of actions may fail and, hence, some alternate sequences may be tried.\n\nWhen knowledge is available concerning the importance of differences, the most important difference is selected first to further improve the average performance of MEA over other brute-force search strategies. However, even without the ordering of differences according to importance, MEA improves over other search heuristics (again in the average case) by focusing the problem solving on the actual differences between the current state and that of the goal.\n\nThe MEA technique as a problem-solving strategy was first introduced in 1961 by Allen Newell and Herbert A. Simon in their computer problem-solving program General Problem Solver (GPS). In that implementation, the correspondence between differences and actions, also called \"operators\", is provided a priori as knowledge in the system. (In GPS this knowledge was in the form of \"table of connections\".)\n\nWhen the action and side-effects of applying an operator are penetrable the search may select the relevant operators by inspection of the operators and do without a table of connections. This latter case, of which the canonical example is STRIPS, an automated planning computer program, allows task-independent correlation of differences to the operators which reduce them.\n\nProdigy, a problem solver developed in a larger learning-assisted automated planning project started at Carnegie Mellon University by Jaime Carbonell, Steven Minton and Craig Knoblock, is another system that used MEA.\n\nProfessor Morten Lind, at Technical University of Denmark has developed a tool called multilevel flow modeling (MFM). It performs means-end based diagnostic reasoning for industrial control and automation systems.\n\n", "id": "6278094", "title": "Means-ends analysis"}
{"url": "https://en.wikipedia.org/wiki?curid=25264546", "text": "Evolutionary developmental robotics\n\nEvolutionary developmental robotics (evo-devo-robo for short) refers to methodologies that systematically integrate evolutionary robotics, epigenetic robotics and morphogenetic robotics to study the evolution, physical and mental development and learning of natural intelligent systems in robotic systems. The field was formally suggested and fully discussed in a published paper and further discussed in a published dialogue.\n\nThe theoretical foundation of evo-devo-robo includes evolutionary developmental biology (evo-devo), evolutionary developmental psychology, developmental cognitive neuroscience etc. Further discussions on evolution, development and learning in robotics and design can be found in a number of papers, including papers on hardware systems and computing tissues.\n\n", "id": "25264546", "title": "Evolutionary developmental robotics"}
{"url": "https://en.wikipedia.org/wiki?curid=430976", "text": "Uncanny valley\n\nIn aesthetics, the uncanny valley is a hypothesized relationship between the degree of an object's resemblance to a human being and the emotional response to such an object. The concept of the uncanny valley suggests that humanoid objects which appear almost, but not exactly, like real human beings elicit uncanny, or strangely familiar, feelings of eeriness and revulsion in observers. \"Valley\" denotes a dip in the human observer's affinity for the replica, a relation that otherwise increases with the replica's human likeness.\n\nExamples can be found in robotics, 3D computer animations, and lifelike dolls among others. With the increasing prevalence of virtual reality, augmented reality, and photorealistic computer animation, the 'valley' has been cited in the popular press in reaction to the verisimilitude of the creation as it approaches indistinguishability from reality. The uncanny valley hypothesis predicts that an entity appearing almost human risks eliciting cold, eerie feelings in viewers.\n\nThe concept was identified by the robotics professor Masahiro Mori as \"Bukimi no Tani Genshō\" (不気味の谷現象) in 1970. The term was first translated as \"uncanny valley\" in the 1978 book \"Robots: Fact, Fiction, and Prediction\", written by Jasia Reichardt, thus forging an unintended link to Ernst Jentsch's concept of the \"uncanny\", introduced in a 1906 essay entitled \"On the Psychology of the Uncanny.\" Jentsch's conception was elaborated by Sigmund Freud in a 1919 essay entitled \"The Uncanny\" (\"Das Unheimliche\").\n\nMori's original hypothesis states that as the appearance of a robot is made more human, some observers' emotional response to the robot becomes increasingly positive and empathetic, until it reaches a point beyond which the response quickly becomes strong revulsion. However, as the robot's appearance continues to become less distinguishable from a human being, the emotional response becomes positive once again and approaches human-to-human empathy levels.\n\nThis area of repulsive response aroused by a robot with appearance and motion between a \"barely human\" and \"fully human\" entity is the uncanny valley. The name captures the idea that an almost human-looking robot seems overly \"strange\" to some human beings, produces a feeling of uncanniness, and thus fails to evoke the empathic response required for productive human–robot interaction.\n\nA number of theories have been proposed to explain the cognitive mechanism underlying the phenomenon:\n\nA series of studies experimentally investigated whether uncanny valley effects exist for static images of robot faces. Mathur MB & Reichling DB used two complementary sets of stimuli spanning the range from very mechanical to very human-like: first, a sample of 80 objectively chosen robot face images from Internet searches, and second, a morphometrically and graphically controlled 6-face series set of faces. They asked subjects to explicitly rate the likability of each face. To measure trust toward each face, subjects completed a one-shot investment game to indirectly measure how much money they were willing to \"wager\" on a robot's trustworthiness. Both stimulus sets showed a robust uncanny valley effect on explicitly-rated likability and a more context-dependent uncanny valley on implicitly-rated trust. Their exploratory analysis of one proposed mechanism for the uncanny valley, perceptual confusion at a category boundary, found that category confusion occurs in the uncanny valley but does not mediate the effect on social and emotional responses.\n\nOne study conducted in 2009 examined the evolutionary mechanism behind the aversion associated with the uncanny valley. A group of five monkeys were shown three images: two different 3D monkey faces (realistic, unrealistic), and a real photo of a monkey's face. The monkeys' eye-gaze was used as a proxy for preference or aversion. Since the realistic 3D monkey face was looked at less than either the real photo, or the unrealistic 3D monkey face, this was interpreted as an indication that the monkey participants found the realistic 3D face aversive, or otherwise preferred the other two images. As one would expect with the uncanny valley, more realism can lead to less positive reactions, and this study demonstrated that neither human-specific cognitive processes, nor human culture explain the uncanny valley. In other words, this aversive reaction to realism can be said to be evolutionary in origin.\n\nAs of 2011, researchers at University of California, San Diego and California Institute for Telecommunications and Information Technology are measuring human brain activations related to the uncanny valley. In one study using fMRI, a group of cognitive scientists and roboticists found the biggest differences in brain responses for uncanny robots in parietal cortex, on both sides of the brain, specifically in the areas that connect the part of the brain’s visual cortex that processes bodily movements with the section of the motor cortex thought to contain mirror neurons. The researchers say they saw, in essence, evidence of mismatch or perceptual conflict. The brain \"lit up\" when the human-like appearance of the android and its robotic motion \"didn’t compute\". Ayşe Pınar Saygın, an assistant professor from UCSD, says \"The brain doesn’t seem selectively tuned to either biological appearance or biological motion per se. What it seems to be doing is looking for its expectations to be met – for appearance and motion to be congruent.\"\n\nViewer perception of facial expression and speech and the uncanny valley in realistic, human-like characters intended for video games and film is being investigated by Tinwell et al., 2011. Consideration is also given by Tinwell et al. (2010) as to how the uncanny may be exaggerated for antipathetic characters in survival horror games. Building on the body of work already undertaken in android science, this research intends to build a conceptual framework of the uncanny valley using 3D characters generated in a real-time gaming engine. The goal is to analyze how cross-modal factors of facial expression and speech can exaggerate the uncanny. Tinwell et al., 2011 have also introduced the notion of an \"unscalable\" uncanny wall that suggests that a viewer’s discernment for detecting imperfections in realism will keep pace with new technologies in simulating realism. A summary of Angela Tinwell's research on the uncanny valley, psychological reasons behind the uncanny valley and how designers may overcome the uncanny in human-like virtual characters is provided in her book, \"The Uncanny Valley in Games and Animation\" by CRC Press.\n\nA number of design principles have been proposed for avoiding the uncanny valley:\n\nA number of criticisms have been raised concerning whether the uncanny valley exists as a unified phenomenon amenable to scientific scrutiny:\n\nAn effect similar to the uncanny valley was noted by Charles Darwin in 1839:\nA similar \"uncanny valley\" effect could, according to the ethical-futurist writer Jamais Cascio, show up when humans begin modifying themselves with transhuman enhancements (cf. body modification), which aim to improve the abilities of the human body beyond what would normally be possible, be it eyesight, muscle strength, or cognition. So long as these enhancements remain within a perceived norm of human behavior, a negative reaction is unlikely, but once individuals supplant normal human variety, revulsion can be expected. However, according to this theory, once such technologies gain further distance from human norms, \"transhuman\" individuals would cease to be judged on human levels and instead be regarded as separate entities altogether (this point is what has been dubbed \"posthuman\"), and it is here that acceptance would rise once again out of the uncanny valley. Another example comes from \"pageant retouching\" photos, especially of children, which some find disturbingly doll-like.\n\nA number of films that use computer-generated imagery to show characters have been described by reviewers as giving a feeling of revulsion or \"creepiness\" as a result of the characters looking too realistic. Examples include the following:\n\n\nThe fear, arising at contemplation of the \"person\" having small aberrations, and strengthening of impression because of its movement were noticed in 1818 by Mary Shelley in the novel \"Frankenstein; or, The Modern Prometheus\":\n\nIn the 2008 \"30 Rock\" episode \"Succession\", Frank Rossitano explains the uncanny valley concept, using a graph and \"Star Wars\" examples, to try to convince Tracy Jordan that his dream of creating a pornographic video game is impossible. He also references the computer-animated film \"The Polar Express.\"\n\n", "id": "430976", "title": "Uncanny valley"}
{"url": "https://en.wikipedia.org/wiki?curid=402688", "text": "Algorithmic probability\n\nIn algorithmic information theory, algorithmic probability, also known as Solomonoff probability, is a mathematical method of assigning a prior probability to a given observation. It was invented by Ray Solomonoff in the 1960s. \nIt is used in inductive inference theory and analyses of algorithms . In his general theory of inductive inference, Solomonoff uses the prior obtained by this formula, in Bayes' rule for prediction .\n\nIn the mathematical formalism used, the observations have the form of finite binary strings, and the universal prior is a probability distribution over the set of finite binary strings . The prior is universal in the\nTuring-computability sense, i.e. no string has zero probability. It is not computable, but it can be approximated. \n\nAlgorithmic probability deals with the following questions. Given a body of data about some phenomenon that we want to understand,\n\n\nFour principle inspirations for Solomonoff's algorithmic probability were: Occam's razor, Epicurus' principle of multiple explanations, modern computing theory (e.g. use of a universal Turing machine) and Bayes rule for prediction.\n\nOccam's razor and Epicurus' principle are essentially two different nonmathematical approximations of the universal prior.\n\n\n\nAt the heart of the universal prior is an abstract model of a computer, such as a universal Turing machine. Any abstract computer will do, as long as it is Turing-complete, i.e. every finite binary string has at least one program that will compute it on the abstract computer .\n\nThe abstract computer is used to give precise meaning to the phrase \"simple explanation\" . In the formalism used, explanations, or theories of phenomena, are computer programs that generate observation strings when run on the abstract computer . A simple explanation is a short computer program. A complex explanation is a long computer program . Simple explanations are more likely, so a high-probability observation string is one generated\nby a short computer program, or perhaps by any of a large number of slightly longer computer programs. A low-probability observation string is one that can only be generated by a long computer program .\n\nThese ideas can be made specific and the probabilities used to construct a prior probability\ndistribution for the given observation. Solomonoff's main reason for inventing this prior is so that it can be used in Bayes' rule when the actual prior is unknown, enabling prediction under uncertainty. It predicts the most likely continuation of that observation, and provides a measure of how likely this continuation will be .\n\nAlthough the universal probability of an observation (and its extension) is incomputable, there is a computer algorithm, Levin Search, which, when run for longer and longer periods of time, will generate a sequence of approximations which converge to the universal probability distribution.\n\nSolomonoff proved this distribution to be \nmachine-invariant within a constant factor (called the invariance theorem).\n\nSolomonoff invented the concept of algorithmic probability with its associated invariance theorem around 1960, publishing a report on it: \"A Preliminary Report on a General Theory of Inductive Inference.\" He clarified these ideas more fully in 1964 with \"A Formal Theory of Inductive Inference,\" Part I and Part II.\n\nSolomonoff described a universal computer with a randomly generated input program. The program computes some possibly infinite output. The universal probability distribution is the probability distribution on all possible output strings with random input.\n\nThe algorithmic probability of any given finite output prefix \"q\" is the sum of the probabilities of the programs that compute something starting with \"q\". Certain long objects with short programs have high probability.\n\nAlgorithmic probability is the main ingredient of Solomonoff's theory of inductive inference, the theory of prediction based on observations; it was invented with the goal of using it for machine learning; given a sequence of symbols, which one will come next? Solomonoff's theory provides an answer that is optimal in a certain sense, although it is incomputable. Unlike, for example, Karl Popper's informal inductive inference theory, Solomonoff's is mathematically rigorous.\n\nAlgorithmic probability is closely related to the concept of Kolmogorov complexity. Kolmogorov's introduction of complexity was motivated by information theory and problems in randomness, while Solomonoff introduced algorithmic complexity for a different reason: inductive reasoning. A single universal prior probability that can be substituted for each actual prior probability in Bayes’s rule was invented by Solomonoff with Kolmogorov complexity as a side product.\n\nSolomonoff's enumerable measure is universal in a certain powerful sense, but the computation time can be infinite. One way of dealing with this issue is a variant of Leonid Levin's Search Algorithm, which limits the time spent computing the success of possible programs, with shorter programs given more time. Other methods of limiting the search space include training sequences.\n\n\n\n\n\n", "id": "402688", "title": "Algorithmic probability"}
{"url": "https://en.wikipedia.org/wiki?curid=24870679", "text": "LIDA (cognitive architecture)\n\nThe LIDA (Learning Intelligent Distribution Agent) cognitive architecture is an integrated artificial cognitive system that attempts to model a broad spectrum of cognition in biological systems, from low-level perception/action to high-level reasoning. Developed primarily by Stan Franklin and colleagues at the University of Memphis, the LIDA architecture is empirically grounded in cognitive science and cognitive neuroscience. In addition to providing hypotheses to guide further research, the architecture can support control structures for software agents and robots. Providing plausible explanations for many cognitive processes, the LIDA conceptual model is also intended as a tool with which to think about how minds work.\n\nTwo hypotheses underlie the LIDA architecture and its corresponding conceptual model: 1) Much of human cognition functions by means of frequently iterated (~10 Hz) interactions, called cognitive cycles, between conscious contents, the various memory systems and action selection. 2) These cognitive cycles, serve as the \"atoms\" of cognition of which higher-level cognitive processes are composed.\n\nThough it is neither symbolic nor strictly connectionist, LIDA is a hybrid architecture in that it employs a variety of computational mechanisms, chosen for their psychological plausibility. The LIDA cognitive cycle is composed of modules and processes employing these mechanisms.\n\nThe LIDA architecture employs several modules that are designed using computational mechanisms drawn from the \"new AI\". These include variants of the Copycat Architecture, sparse distributed memory, the schema mechanism, the Behavior Net, and the subsumption architecture.\n\nAs a comprehensive, conceptual and computational cognitive architecture the LIDA architecture is intended to model a large portion of human cognition. Comprising a broad array of cognitive modules and processes, the LIDA architecture attempts to implement and flesh out a number of psychological and neuropsychological theories including Global Workspace Theory, situated cognition, perceptual symbol systems, working memory, memory by affordances, long-term working memory, and the H-CogAff architecture.\n\nThe LIDA cognitive cycle can be subdivided into three phases: the understanding phase, the attention (consciousness) phase, and the action selection and learning phase. Beginning the understanding phase, incoming stimuli activate low-level feature detectors in sensory memory. The output engages perceptual associative memory where higher-level feature detectors feed in to more abstract entities such as objects, categories, actions, events, etc. The resulting percept moves to the Workspace where it cues both Transient Episodic Memory and Declarative Memory producing local associations. These local associations are combined with the percept to generate a current situational model which is the agent's understanding of what is going on right now. The attention phase begins with the forming of coalitions of the most salient portions of the current situational model, which then compete for attention, that is a place in the current conscious contents. These conscious contents are then broadcast globally, initiating the learning and action selection phase. New entities and associations, and the reinforcement of old ones, occur as the conscious broadcast reaches the various forms of memory, perceptual, episodic and procedural. In parallel with all this learning, and using the conscious contents, possible action schemes are instantiated from Procedural Memory and sent to Action Selection, where they compete to be the behavior selected for this cognitive cycle. The selected behavior triggers sensory-motor memory to produce a suitable algorithm for its execution, which completes the cognitive cycle.\n\nVirtual Mattie (V-Mattie) is a software agent that gathers information from seminar organizers, composes announcements of next week's seminars, and mails them each week to a list that it keeps updated, all without the supervision of a human. V-Mattie employed many of the computational mechanisms mentioned above.\n\nBaars' Global Workspace Theory (GWT) inspired the transformation of V-Mattie into Conscious Mattie, a software agent with the same domain and tasks whose architecture included a consciousness mechanism à la GWT. Conscious Mattie was the first functionally, though not phenomenally, conscious software agent. Conscious Mattie gave rise to IDA.\n\nIDA (Intelligent Distribution Agent) was developed for the US Navy to fulfill tasks performed by human resource personnel called detailers. At the end of each sailor's tour of duty, he or she is assigned to a new billet. This assignment process is called distribution. The Navy employs almost 300 full time detailers to effect these new assignments. IDA's task is to facilitate this process, by automating the role of detailer. IDA was tested by former detailers and accepted by the Navy. Various Navy agencies supported the IDA project to the tune of some $1,500,000.\n\nThe LIDA (Learning IDA) architecture was originally spawned from IDA by the addition of several styles and modes of learning, but has since then grown to become a much larger and generic software framework.\n\n", "id": "24870679", "title": "LIDA (cognitive architecture)"}
{"url": "https://en.wikipedia.org/wiki?curid=9124629", "text": "Cognitive philology\n\nCognitive philology is the science that studies written and oral texts as the product of human mental processes. Studies in cognitive philology compare documentary evidence emerging from textual investigations with results of experimental research, especially in the fields of cognitive and ecological psychology, neurosciences and artificial intelligence. \"The point is not the text, but the mind that made it\". Cognitive Philology aims to foster communication between literary, textual, philological disciplines on the one hand and researches across the whole range of the cognitive, evolutionary, ecological and human sciences on the other.\n\nCognitive philology: \n\nAmong the founding thinkers and noteworthy scholars devoted to such investigations are: \n\n\n", "id": "9124629", "title": "Cognitive philology"}
{"url": "https://en.wikipedia.org/wiki?curid=19442735", "text": "Psychology of reasoning\n\nThe psychology of reasoning is the study of how people reason, often broadly defined as the process of drawing conclusions to inform how people solve problems and make decisions. It overlaps with psychology, philosophy, linguistics, cognitive science, artificial intelligence, logic, and probability theory.\n\nPsychological experiments on how humans and other animals reason have been carried out for over 100 years. An enduring question is whether or not people have the capacity to be rational. What does it mean to be rational? Current research in this area addresses various questions about reasoning, rationality, judgments, intelligence, relationships between emotion and reasoning, and development.\n\nHow do people reason about sentences in natural language? Most experimentation on deduction has been carried out on hypothetical thought, in particular, examining how people reason about conditionals, e.g., \"If A then B\". Participants in experiments make the modus ponens inference, given the indicative conditional \"If A then B\", and given the premise \"A\", they conclude \"B\". However, given the indicative conditional and the minor premise for the modus tollens inference, \"not-B\", about half of the participants in experiments conclude \"not-A\" and the remainder concludes that nothing follows.\n\nThe ease with which people make conditional inferences is affected by content, as demonstrated in the well-known selection task developed by Peter Wason. Participants are better able to test a conditional that contains sensible content, e.g., \"if the envelope is sealed then it must have a 50 cent stamp on it\" compared to one that contains symbolic content, e.g.,\" if the letter is a vowel then the number is even\". Background knowledge can also lead to the suppression of even the simple modus ponens inference Participants given the conditional \"if Lisa has an essay to write then she studies late in the library\" and the premise \"Lisa has an essay to write \" make the modus ponens inference 'she studies late in the library', but the inference is suppressed when they are also given a second conditional \"if the library stays open then she studies late in the library\". Interpretations of the suppression effect are controversial \n\nOther investigations of propositional inference examine how people think about disjunctive alternatives, e.g., \"A or else B\", and how they reason about negation, e.g., \"It is not the case that A and B\". Many experiments have been carried out to examine how people make relational inferences, including comparisons, e.g., \"A is better than B\". Such investigations also concern spatial inferences, e.g. \"A is in front of B\" and temporal inferences, e.g. \"A occurs before B\". Other common tasks include categorical syllogisms, used to examine how people reason about quantifiers such as \"All\" or \"Some\", e.g., \"Some of the A are not B\".\n\nThere are several alternative theories of the cognitive processes that human reasoning is based on. One view is that people rely on a mental logic consisting of formal (abstract or syntactic) inference rules similar to those developed by logicians in the propositional calculus. Another view is that people rely on domain-specific or content-sensitive rules of inference. A third view is that people rely on mental models, that is, mental representations that correspond to imagined possibilities. The mental model theory is the subject of the \"mental models website\" A fourth view is that people compute probabilities.\n\nOne controversial theoretical issue is the identification of an appropriate competence model, or a standard against which to compare human reasoning. Initially classical logic was chosen as a competence model. Subsequently some researchers opted for non-monotonic logic and Bayesian probability. Research on mental models and reasoning has led to the suggestion that people are rational in principle but err in practice. Connectionist approaches towards reasoning have also been proposed.\n\nHow does reasoning develop? Jean Piaget's theory of cognitive development describes a sequence of stages in the development of reasoning from infancy to adulthood. According to the neo-Piagetian theories of cognitive development, changes in reasoning with development come from increasing working memory capacity, increasing speed of processing, and enhanced executive functions and control. Increasing self-awareness is also an important factor.\n\nInductive reasoning makes broad generalizations from specific cases or observations. In this process of reasoning, general assertions are made based on past specific pieces of evidence. This kind of reasoning allows the conclusion to be false even if the original statement is true. For example, if one observes a college athlete, one makes predictions and assumptions about other college athletes based on that one observation. Scientists use inductive reasoning to create theories and hypotheses.\n\nIn opposition, deductive reasoning is a basic form of valid reasoning. In this reasoning process a person starts with a known claim or a general belief and from there asks what follows from these foundations or how will these premises influence other beliefs. In other words, deduction starts with a hypothesis and examines the possibilities to reach a conclusion. Deduction helps people understand why their predictions are wrong and indicates that their prior knowledge or beliefs are off track. An example of deduction can be seen in the scientific method when testing hypotheses and theories. Although the conclusion usually corresponds and therefore proves the hypothesis, there are some cases where the conclusion is logical, but the generalization is not. For example, the argument, “All young girls wear skirts. Julie is a young girl. Therefore, Julie wears skirts,” is valid logically, but is not sound because the first premise isn't true.\n\nThe syllogism is a form of deductive reasoning in which two statements reach a logical conclusion. With this reasoning, one statement could be “Every A is B” and another could be “This C is A”. Those two statements could then lead to the conclusion that “This C is B”. These types of syllogisms are used to test deductive reasoning to ensure there is a valid hypothesis. A Syllogistic Reasoning Task was created from a study performed by Morsanyi, Kinga, Handley, and Simon that examined the intuitive contributions to reasoning. They used this test to assess why “syllogistic reasoning performance is based on an interplay between a conscious and effortful evaluation of logicality and an intuitive appreciation of the believability of the conclusions”.\n\nAnother form of reasoning is called abductive reasoning. This type is based on creating and testing hypotheses using the best information available. Abductive reasoning produces the kind of daily decision-making that works best with the information present, which often is incomplete. This could involve making educated guesses from observed unexplainable phenomena. This type of reasoning can be seen in the world when doctors make decisions about diagnoses from a set of results or when jurors use the relevant evidence to make decisions about a case.\n\nJudgment and reasoning involve thinking through the options, making a judgment or conclusion and finally making a decision. Making judgments involves heuristics, or efficient strategies that usually lead you to the right answers. The most common heuristics used are attribute substitution, the availability heuristic, the representativeness heuristic and the anchoring heuristic – these all aid in quick reasoning and work in most situations. Heuristics allow for errors, a price paid to gain efficiency.\nOther errors in judgment, therefore affecting reasoning, include errors in judgment about covariation – a relationship between two variables such that the presence and magnitude of one can predict the presence and magnitude of the other. One cause of covariation is confirmation bias, or the tendency to be more responsive to evidence that confirms your beliefs. But assessing covariation can be pulled off track by neglecting base-rate information – how frequently something occurs in general. However people often ignore base rates and tend to use other information presented.\nThere are more sophisticated judgment strategies that result in fewer errors. People often reason based on availability but sometimes they look for other, more accurate, information to make judgments. This suggests there are two ways of thinking, known as the Dual-Process Model. The first, System I, is fast, automatic and uses heuristics – more of intuition. The second, System II, is slower, effortful and more likely to be correct – more reasoning.\n\n\nDecision making is often influenced by the emotion of regret and the element of risk. People are strongly motivated by regret and we can see this when they select options they tend to select the option that they will regret the least trying to minimize the amount of regret we will have. Many decisions also include a large element of risk, and in these cases people tend to ask themselves what the level of risk is. They ask themselves how much dread they would experience when thinking about a nuclear accident, and then use that dread as an indicator of risk. We ask “how does this make me feel?” rather than “how risky is this?”\n\nAntonio Damasio suggests that somatic markers, certain memories that can cause a strong bodily reaction, act as a way to guide decision making as well. For example, when you are remembering a scary movie and once again become tense and your palms might begin to sweat. Damasio argues that when making a decision we rely on our “gut feelings” to assess various options, and this makes us decide to go with a decision that is more positive and stay away from those that are negative. He also argues that the orbitofrontal cortex - located at the base of the frontal lobe, just above the eyes - is crucial in your use of somatic markers, because it is the part in the brain that allows you to interpret emotion.\n\nAnother note to make is that when emotion shapes decisions, the influence is usually based on predictions of the future. When people ask themselves how they would react, they are making inferences about the future. Researchers suggest affective forecasting, the ability to predict your own emotions, is poor because people tend to overestimate how much they will regret their errors.\n\n\n\n", "id": "19442735", "title": "Psychology of reasoning"}
{"url": "https://en.wikipedia.org/wiki?curid=8271663", "text": "Scilab Image Processing\n\nSIP is a toolbox for processing images in Scilab. SIP is meant to be a free, complete, and useful image toolbox for Scilab. Its goals include tasks such as filtering, blurring, edge detection, thresholding, histogram manipulation, segmentation, mathematical morphology, and color image processing.\n\nThough SIP is still in early development it can currently import and output image files in many formats including BMP, JPEG, GIF, PNG, TIFF, XPM, and PCX. SIP uses ImageMagick to accomplish this.\n\nSIP is licensed under the GPL.\n\n", "id": "8271663", "title": "Scilab Image Processing"}
{"url": "https://en.wikipedia.org/wiki?curid=33529387", "text": "Artificial intuition\n\nThe theoretical concept of artificial intuition is the capacity of an artificial object or software to function with the factor of consciousness known as intuition: a machine-based system that has some capacity to function analogously to human intuition.\n\nConventional human intuition is a function of the human mind, defined particularly by the psychologist and psychiatrist Carl Jung. Psychologist Jean Piaget showed that intuitive functioning within the normally developing human child at the \"Intuitive Thought Substage\" of the preoperational stage occurred at from four to seven years of age. In Carl Jung's concept of synchronicity, the concept of \"intuitive intelligence\" is described as something like a capacity that transcends ordinary-level functioning to a point where information is understood with a greater depth than is available in more simple rationally-thinking entities.\n\nArtificial intuition is theoretically (or otherwise) a sophisticated function of an artifice that is able to interpret data with depth and locate hidden factors functioning in Gestalt psychology, and that intuition in the artificial mind would, in the context described here, be a bottom-up process upon a macroscopic scale identifying something like the archetypal (see ).\n\nTo create artificial intuition supposes the possibility of the re-creation of a higher functioning of the human mind, with capabilities such as what might be found in semantic memory and learning. The transferral of the functioning of a biological system to synthetic functioning is based upon modeling of functioning from knowledge of cognition and the brain, for instance as applications of models of artificial neural networks from the research done within the discipline of computational neuroscience.\n\nThe notion of a process of a data-interpretative has already been found in a computational-linguistic software application that has been created for use in an internal security context. The software integrates computed data based specifically on objectives incorporating a paradigm described as \"religious intuitive\" (hermeneutic), functional to a degree that represents advances upon the performance of \"generic lexical\" data mining. \n\nVeeramachaneni and others at MIT developed a machine which performed comparably to humans in a test of intuitive intelligence during 2015.\n\nArtificial intelligence in fiction often crosses the line to apparent artificial intuition, although it can't be shown if the intent of the fiction creator was to show a simulation of intuition or that real artificial intuition is part of the story's AI, because this depends on the internal structure of the programming of the AI, which is not usually shown in stories.\n\n\nThe Oxford Companion to Philosophy - E. Honderich (Oxford University Press, 1995) \n\n", "id": "33529387", "title": "Artificial intuition"}
{"url": "https://en.wikipedia.org/wiki?curid=4678739", "text": "Structure mapping engine\n\nIn artificial intelligence and cognitive science, the structure mapping engine is an implementation in software of an algorithm for analogical matching based on the psychological theory of Dedre Gentner. The basis of Gentner's structure-mapping idea is that an analogy is a mapping of knowledge from one domain (the base) into another (the target). The structure-mapping engine, or SME, is a computer simulation of the analogy and similarity comparisons.\n\nAs of 1990, more than 40 projects had used it [Falkenhainer, 2005]. R.M. French said that structure mapping theory is \"unquestionably the most influential work to date of the modeling of analogy-making\" [2002]. \n\nThe theory is useful because it ignores surface features and finds matches between potentially very different things if they have the same representational structure. For example, SME could determine that a pen is like a sponge because both are involved in dispensing liquid, even though they do this very differently.\n\nStructure mapping theory is based on the systematicity principle, which states that connected knowledge is preferred over independent facts. Therefore, the structure mapping engine should ignore isolated source-target mappings unless they are part of a bigger structure. The SME, the theory goes, should map objects that are related to knowledge that has already been mapped.\n\nThe theory also requires that mappings be done one-to-one, which means that no part of the source description can map to more than one item in the target and no part of the target description can be mapped to more than one part of the source. The theory also requires that if a match maps subject to target, the arguments of subject and target must also be mapped. If both these conditions are met, the mapping is said to be \"structurally consistent.\"\n\nSME maps knowledge from a \"source\" into a \"target.\" SME calls each description a \"dgroup.\" Dgroups contain a list of entities and predicates. Entities represent the objects or concepts in a description — such as an input gear or a switch. Predicates are one of three types and are a general way to express knowledge for SME.\n\n\nFunctions and attributes have different meanings, and consequently SME processes them differently. For example, in SME’s true analogy rule set, attributes differ from functions because they cannot match unless there is a higher-order match between them. The difference between attributes and functions will be explained further in this section’s examples.\n\nAll predicates have four parameters. They have (1) a functor, which identifies it, and (2) a type, which is either relation, attribute, or function. The other two parameters (3 and 4) are for determining how to process the arguments in the SME algorithm. If the arguments have to be matched in order, commutative is false. If the predicate can take any number of arguments, N-ary is false. An example of a predicate definition is: (sme:defPredicate behavior-set (predicate) relation :n-ary? t :commutative? t)\nThe predicate’s functor is “behavior-set,” its type is “relation,” and its n-ary and commutative parameters are both set to true. The “(predicate)” part of the definition specifies that there will be one or more predicates inside an instantiation of behavior-set.\n\nThe algorithm has several steps.\nThe first step of the algorithm is to create a set of match hypotheses between source and target dgroups. A match hypothesis represents a possible mapping between any part of the source and the target. This mapping is controlled by a set of match rules. By changing the match rules, one can change the type of reasoning SME does. For example, one set of match rules may perform a kind of analogy called \"literal similarity.\" and another performs a kind of analogy called \"true-analogy.\" These rules are not the place where domain-dependent information is added, but rather where the analogy process is tweaked, depending on the type of cognitive function the user is trying to emulate.\n\nThere are two types of match rules: filter rules and intern rules. Intern rules use only the arguments of the expressions in the match hypotheses that the filter rules identify. This limitation makes the processing more efficient by constraining the number of match hypotheses that are generated. At the same time, it also helps to build the structural consistencies that are needed later on in the algorithm. An example of a filter rule from the true-analogy rule set creates match hypotheses between predicates that have the same functor. The true-analogy rule set has an intern rule that iterates over the arguments of any match hypothesis, creating more match hypotheses if the arguments are entities or functions, or if the arguments are attributes and have the same functor.\n\nIn order to illustrate how the match rules produce match hypotheses consider these two predicates:\n\ncodice_1\n\ncodice_2\n\nThe filter match rule generates a match between p1 and p2 because they share the same functor, \"transmit.\" The intern rules then produce three more match hypotheses: torque to signal, inputgear to switch, and secondgear to div10. The intern rules created these match hypotheses because all the arguments were entities.\n\nIf the arguments were functions or attributes instead of entities, the predicates would be expressed as:\n\ncodice_3\n\ncodice_4\n\nThese additional predicates make inputgear, secondgear, switch, and div10 functions or attributes depending on the value defined in the language input file. The representation also contains additional entities for gear and circuit.\n\nDepending on what type \"inputgear, secondgear, switch,\" and \"div10\" are, their meanings change. As attributes, each one is a property of the gear or circuit. For example, the gear has two attributes, inputgear and secondgear. The circuit has two attributes, switch and circuit. As functions inputgear, secondgear, switch, and div10 become quantities of the gear and circuit. In this example, the functions inputgear and secondgear now map to the numerical quantities “torque from inputgear” and “torque from secondgear,” For the circuit the quantities map to logical quantity “switch engaged” and the numerical quantity “current count on the divide by 10 counter.”\n\nSME processes these differently. It does not allow attributes to match unless they are part of a higher-order relation, but it does allow functions to match, even if they are not part of such a relation. It allows functions to match because they indirectly refer to entities and thus should be treated like relations that involve no entities. However, as next section shows, the intern rules assign lower weights to matches between functions than to matches between relations.\n\nThe reason SME does not match attributes is because it is trying to create connected knowledge based on relationships and thus satisfy the systematicity principle. For example, if both a clock and a car have inputgear attributes, SME will not mark them as similar. If it did, it would be making a match between the clock and car based on their appearance — not on the relationships between them.\n\nWhen the additional predicates in p3 and p4 are functions, the results from matching p3 and p4 are similar to the results from p1 and p2 except there is an additional match between gear and circuit and the values for the match hypotheses between (inputgear gear) and (switch circuit), and (secondgear gear) and (div10 circuit), are lower. The next section describes the reason for this in more detail.\n\nIf the inputgear, secondgear, switch, and div10 are attributes instead of entities, SME does not find matches between any of the attributes. It finds matches only between the transmit predicates and between torque and signal. Additionally, the structural-evaluation scores for the remaining two matches decrease. In order to get the two predicates to match, p3 would need to be replaced by p5, which is demonstrated below.\n\ncodice_5\n\nSince the true-analogy rule set identifies that the div10 attributes are the same between p5 and p4 and because the div10 attributes are both part of the higher-relation match between torque and signal, SME makes a match between (div10 gear) and (div10 circuit) — which leads to a match between gear and circuit.\n\nBeing part of a higher-order match is a requirement only for attributes. For example, if (div10 gear) and (div10 circuit) are not part of a higher-order match, SME does not create a match hypothesis between them. However, if div10 is a function or relation, SME does create a match.\n\nOnce the match hypotheses are generated, SME needs to compute an evaluation score for each hypothesis. SME does so by using a set of intern match rules to calculate positive and negative evidence for each match. Multiple amounts of evidence are correlated using Dempster’s rule [Shafer, 1978] resulting in positive and negative belief values between 0 and 1. The match rules assign different values for matches involving functions and relations. These values are programmable, however, and some default values that can be used to enforce the systematicity principle are described in [Falkenhainer et al., 1989].\n\nThese rules are:\n\n\nIn the example match between p1 and p2, SME gives the match between the transmit relations a positive evidence value of 0.7900, and the others get values of 0.6320. The transmit relation receives the evidence value of 0.7900 because it gains evidence from rules 1, 3, and 2. The other matches get a value of 0.6320 because 0.8 of the evidence from the transmit is propagated to these matches because of rule 5.\n\nFor predicates p3 and p4, SME assigns less evidence because the arguments of the transmit relations are functions. The transmit relation gets positive evidence of 0.65 because rule 3 no longer adds evidence. The match between (input gear) and (switch circuit) becomes 0.7120. This match gets 0.4 evidence because of rule 3, and 0.52 evidence propagated from the transmit relation because of rule 5.\n\nWhen the predicates in p3 and p4 are attributes, rule 4 adds -0.8 evidence to the transmit match because — though the functors of the transmit relation match — the arguments do not have the potential to match and the arguments are not functions.\n\nTo summarize, the intern match rules compute a structural evaluation score for each match hypothesis. These rules enforce the systematicity principle. Rule 5 provides trickle-down evidence in order to strengthen matches that are involved in higher-order relations. Rules 1, 3. and 4 add or subtract support for relations that could have matching arguments. Rule 2 adds support for the cases when the functors match. thereby adding support for matches that emphasize relationships.\n\nThe rules also enforce the difference between attributes, functions, and relations. For example, they have checks which give less evidence for functions than relations. Attributes are not specifically dealt with by the intern match rules, but SME’s filter rules ensure that they will only be considered for these rules if they are part of a higher-order relation, and rule 2 ensures that attributes will only match if they have identical functors.\n\nThe rest of the SME algorithm is involved in creating maximally consistent sets of match hypotheses. These sets are called gmaps. SME must ensure that any gmaps that it creates are structurally consistent; in other words, that they are one-to-one — such that no source maps to multiple targets and no target is mapped to multiple sources. The gmaps must also have support, which means that if a match hypothesis is in the gmap, then so are the match hypothesis that involve the source and target items.\n\nThe gmap creation process follows two steps. First, SME computes information about each match hypothesis — including entity mappings, any conflicts with other hypotheses, and what other match hypotheses with which it might be structurally inconsistent.\n\nSME then uses this information to merge match hypotheses — using a greedy algorithm and the structural evaluation score. It merges the match hypotheses into maximally structurally consistent connected graphs of match hypotheses. Then it combines gmaps that have overlapping structure if they are structurally consistent. Finally, it combines independent gmaps together while maintaining structural consistency.\n\nComparing a source to a target dgroup may produce one or more gmaps. The weight for each gmap is the sum of all the positive evidence values for all the match hypotheses involved in the gmap. For example, if a source containing p1 and p6 below, is compared to a target containing p2, SME will generate two gmaps. Both gmaps have a weight of 2.9186.\n\nSource:\n\ncodice_6\n\ncodice_7\n\nTarget:\ncodice_8\n\nThese are the gmaps which result from comparing a source containing a p1 and p6 and a target containing p2.\n\nGmap No. 1:<br>\n\nGmap No. 2:\n\nThe gmaps show pairs of predicates or entities that match. For example, in gmap No. 1, the entities \"torque\" and \"signal\" match and the behaviors transmit torque inputgear secondgear and transmit signal switch div10 match. Gmap No. 1 represents combining p1 and p2. Gmap No. 2 represents combining p1 and p6. Although p2 is compatible with both p1 and p6, the one-to-one mapping constraint enforces that both mappings cannot be in the same gmap. Therefore, SME produces two independent gmaps. In addition, combining the two gmaps together would make the entity mappings between thirdgear and div10 conflict with the entity mapping between secondgear and div10.\n\nChalmers, French, and Hofstadter [1992] criticize SME for its reliance on manually constructed LISP representations as input. They argue that too much human creativity is required to construct these representations; the intelligence comes from the design of the input, not from SME. Forbus et al. [1998] attempted to rebut this criticism. Morrison and Dietrich [1995] tried to reconcile the two points of view. Turney [2008] presents an algorithm that does not require LISP input, yet follows the principles of Structure Mapping Theory. Turney [2008] state that their work, too, is not immune to the criticism of Chalmers, French, and Hofstadter [1992].\n\nIn her article How Creative Ideas Take Shape, Liane Gabora writes \"According to the honing theory of creativity, creative thought works not on individually considered, discrete, predefined representations but on a contextually-elicited amalgam of items which exist in a state of potentiality and may not be readily separable. This leads to the prediction that analogy making proceeds not by mapping correspondences from candidate sources to target, as predicted by the structure mapping theory of analogy, but by weeding out non-correspondences, thereby whittling away at potentiality.\"\n\n", "id": "4678739", "title": "Structure mapping engine"}
{"url": "https://en.wikipedia.org/wiki?curid=33623429", "text": "DragonLord Enterprises, Inc.\n\nDragonLord Enterprises, Inc. is an American corporation that develops games, mobile apps, and\n3D simulations. Under the name Sequoia Consulting,\nit also specializes in robotics, machine learning,\nand applied artificial intelligence. DragonLord created the first adventure game running on a cell phone in the world.\n\nDragonLord created the first adventure game for cell phones in the world, \"Cavern Crawl\", in 2001. DragonLord was the first developer in America to create games for cell phones, and one of the first in the world, after Nokia's \"Snake\".\nDragonLord's \"Cavern Crawl\" and \"Mystic I Ching\" were\nfactory-built into the firmware of such cell phones as the Kyocera Model 2235 handset, distributed by Verizon; the S14 Opal, distributed by Cricket and MetroPCS; and the 2255 and 2325,\ndistributed by Sprint PCS in America and Virgin Mobile in the UK. The games were noted for providing animation and game play that were ahead of their time.\n\nDragonLord also made multiple versions of \"Tetris\",\nthat were built into the factory firmware of various Kyocera handsets through at least 2004.\n\nDragonLord's Sequoia Consulting created the software for a humanoid robot for NASA JPL\nin 2006. The robot was one of the first humanoid robots to integrate speech recognition of commands and speech generation of responses. It used a real-time expert system shell to plan and sequence actions.\nIt used primitive model-based stereo vision to locate and track objects in its environment.\nIt was able to turn, walk over to a spar, squat down, bend over,\nand pick the spar up, using an arm with no wrist joints.\n\nDragonLord is featured in the book \"The Fat Man on Game Audio\".\n\n", "id": "33623429", "title": "DragonLord Enterprises, Inc."}
{"url": "https://en.wikipedia.org/wiki?curid=33527843", "text": "Trenchard More\n\nTrenchard More is a retired mathematician and computer scientist who worked at IBM's Thomas J Watson Research Centre after teaching posts at MIT and Yale. He was also a full Professor for two years at the Technical University of Denmark. He participated in the 1956 Dartmouth Summer Research Project on Artificial Intelligence. At the 50th year meeting of the Dartmouth Conference with Marvin Minsky, Geoffrey Hinton and Simon Osindero he presented \"The Future of Network Models\" and also gave a lecture entitled \"Routes to the Summit\".\n\nDesigned a theory for nested rectangular array that provided a formal structure used in the development of APL2 and the Nested Interactive Array Language.\n\n\n", "id": "33527843", "title": "Trenchard More"}
{"url": "https://en.wikipedia.org/wiki?curid=1124646", "text": "Dartmouth workshop\n\nThe Dartmouth Summer Research Project on Artificial Intelligence was the name of a 1956 summer workshop now considered by many(though not all)\nto be the seminal event for artificial intelligence as a field.\n\nThe project lasted approximately 6 to 8 weeks, and was essentially an extended brainstorming session. 11 mathematicians and scientists were originally planned to be attendees, and while not all attended, more than 10 others came for short times.\n\nIn the early 1950s, there were various names for the field of \"thinking machines\" such as cybernetics, automata theory, and complex information processing \n\nIn 1955 John McCarthy, then a young Assistant Professor of Mathematics at Dartmouth College, decided to organize a group to clarify and develop ideas about thinking machines. He picked the name 'Artificial Intelligence' for the new field. He chose the name partly for its neutrality; avoiding a focus on narrow automata theory, and avoiding cybernetics which was heavily focused on analog feedback, as well as him potentially having to accept the assertive Norbert Wiener as guru or having to argue with him.\n\nIn early 1955, McCarthy approached the Rockefeller Foundation to request funding for a summer seminar at Dartmouth for about 10 participants. In June, he and Claude Shannon, a founder of Information Theory then at Bell Labs, met with Robert Morison, Director of Biological and Medical Research to discuss the idea and possible funding, though Morison, was unsure whether money would be made available for such a visionary project.\n\nOn September 2, 1955, the project was formally proposed by McCarthy, Marvin Minsky, Nathaniel Rochester and Claude Shannon. The proposal is credited with introducing the term 'artificial intelligence'.\n\nThe Proposal states \n\nThe proposal goes on to discuss computers, natural language processing, neural networks, theory of computation, abstraction and creativity (these areas within the field of artificial intelligence are considered still relevant to the work of the field). \nOn May 26, 1956, McCarthy notified Robert Morison of the planned 11 attendees:\n\nFor the full period:\n\nFor four weeks:\n\nFor the first two weeks:\n\nHe noted, ``We will concentrate on a problem of devising a way of programming a calculator to form concepts and to form generalizations. This of course is subject to change when the group gets together.\n\nAccording to Stottler Henke Associates, besides the proposal's authors, attendees at the conference included Ray Solomonoff, Oliver Selfridge, Trenchard More, Arthur Samuel, Herbert A. Simon, and Allen Newell.\nThe actual participants came at different times, mostly for much shorter times. Trenchard More replaced Rochester for three weeks and MacKay and Holland did not attend --- but the project was set to begin. \nAround June 18, 1956, the earliest participants (perhaps only Ray Solomonoff, maybe with Tom Etter) arrived at the Dartmouth campus in Hanover, N.H., to join John McCarthy who already had an apartment there. Ray and Marvin stayed at Professors' apartments, but most would stay at the Hanover Inn.\n\nThe Dartmouth Workshop is said to have run for six weeks in the summer of 1956. Ray Solomonoff's notes written during the Workshop time, 1956, however, say it ran for ``roughly eight weeks, from about June 18 to August 17.\nSolomonoff's Dartmouth notes start on June 22; June 28 mentions Minsky, June 30 mentions Hanover, N.H., July 1 mentions Tom Etter. On August 17, Ray gave a final talk.\n\nUnfortunately McCarthy lost his list of attendees! Instead, after the Dartmouth Project McCarthy sent Ray a preliminary list of participants and visitors plus those interested in the subject. There are 47 people listed.\n\nSolomonoff, however, made a complete list in his notes of the summer project:\n\n\nShannon attended Ray's talk on July 10 and Bigelow gave a talk on August 15. Ray doesn't mention Bernard Widrow, but apparently he visited, along with W.A. Clark and B.G. Farley. Trenchard mentions R. Culver and Ray mentions Bill Shutz. Herb Gelernter didn't attend, but was influenced later by what Rochester learned. Gloria Minsky also commuted there (with their part-beagle dog, Senje, who would start out in the car back seat and end up curled around her like a scarf), and attended some sessions (without Senje).\n\nRay Solomonoff, Marvin Minsky, and John McCarthy were the only three who stayed for the full-time. Trenchard took attendance during two weeks of his three-week visit. From three to about eight people would attend the daily sessions.\n\nThey had the entire top floor of the Dartmouth Math Department to themselves, and most weekdays they would meet at the main math classroom where someone might lead a discussion focusing on his ideas, or more frequently, a general discussion would be held.\n\nIt was not a directed group research project, discussions convered many topics but several directions are considered to have been initiated or encouraged by the Workshop: the rise of symbolic methods, systems focussed on limited domains (early Expert Systems), and deductive systems versus inductive systems. One participant, Arthur Samuel said, \"It was very interesting, very stimulating, very exciting\".\n\nRay Solomonoff kept notes during the summer giving his impression of the talks and the ideas from various discussions. These are available, along with other notes concerning the Dartmouth Summer Research Project on AI, at: http://raysolomonoff.com/dartmouth/\n\n\n", "id": "1124646", "title": "Dartmouth workshop"}
{"url": "https://en.wikipedia.org/wiki?curid=33591382", "text": "Type-1 OWA operators\n\nThe Yager's OWA (ordered weighted averaging) operators are used to aggregate the crisp values in decision making schemes (such as multi-criteria decision making, multi-expert decision making and multi-criteria/multi-expert decision making). It is widely accepted that Fuzzy sets are more suitable for representing preferences of criteria in decision making.\n\nThe type-1 OWA operators have been proposed for this purpose. The type-1 OWA operators provides a technique for directly aggregating uncertain information with uncertain weights via OWA mechanism in soft decision making and data mining, where these uncertain objects are modelled by fuzzy sets.\n\nThe two definitions for type-1 OWA operators are based on Zadeh's Extension Principle and formula_1-cuts of fuzzy sets. The two definitions lead to equivalent results.\n\nLet formula_2 be the set of fuzzy sets with domain of discourse formula_3, a type-1 OWA operator is defined as follows:\n\nGiven n linguistic weights formula_4 in the form of fuzzy sets defined on the domain of discourse formula_5, a type-1 OWA operator is a mapping, formula_6,\n\nsuch that\n\nwhere formula_10,and formula_11 is a permutation function such that formula_12, i.e., formula_13 is the formula_14th highest element in the set formula_15.\n\nUsing the alpha-cuts of fuzzy sets:\n\nGiven the n linguistic weights formula_16 in the form of fuzzy sets defined on the domain of discourse formula_17, then for each formula_18, an formula_19-level type-1 OWA operator with formula_19-level sets formula_21 to aggregate the formula_19-cuts of fuzzy sets formula_23 is:\n\nwhere formula_25, and formula_26 is a permutation function such that formula_27, i.e., formula_28 is the formula_14th largest\nelement in the set formula_15.\n\nGiven the \"n\" linguistic weights formula_16 in the form of fuzzy sets defined on the domain of discourse formula_17, and the fuzzy sets formula_33, then we have that\n\nwhere formula_35 is the aggregation result obtained by Definition 1, and formula_36 is the result obtained by in Definition 2.\n\nAccording to the Representation Theorem of Type-1 OWA Operators, a general type-1 OWA operator can be decomposed into a series of formula_1-level type-1 OWA operators. In practice, this series of formula_1-level type-1 OWA operators is used to construct the resulting aggregation fuzzy set. So we only need to compute the left end-points and right end-points of the intervals formula_39. Then, the resulting aggregation fuzzy set is constructed with the membership function as follows:\n\nFor the left end-points, we need to solve the following programming problem:\n\nwhile for the right end-points, we need to solve the following programming problem:\n\nA fast method has been presented to solve two programming problem so that the type-1 OWA aggregation operation can be performed efficiently, for details, please see the paper.\n\nThree-step process:\n\n\n\n\nType-2 OWA operators have been suggested to aggregate the type-2 fuzzy sets for soft decision making.\n", "id": "33591382", "title": "Type-1 OWA operators"}
{"url": "https://en.wikipedia.org/wiki?curid=33818014", "text": "Nervous system network models\n\nNetwork of human nervous system comprises nodes (for example, neurons) that are connected by links (for example, synapses). The connectivity may be viewed anatomically, functionally, or electrophysiologically. These are presented in several Wikipedia articles that include Connectionism (a.k.a. Parallel Distributed Processing (PDP)), Biological neural network, Artificial neural network (a.k.a. Neural network), Computational neuroscience, as well as in several books by Ascoli, G. A. (2002), Sterratt, D., Graham, B., Gillies, A., & Willshaw, D. (2011), Gerstner, W., & Kistler, W. (2002), and Rumelhart, J. L., McClelland, J. L., and PDP Research Group (1986) among others. The focus of this article is a comprehensive view of modeling a neural network (technically neuronal network based on neuron model). Once an approach based on the perspective and connectivity is chosen, the models are developed at microscopic (ion and neuron), mesoscopic (functional or population), or macroscopic (system) levels. Computational modeling refers to models that are developed using computing tools.\n\nThe nervous system consists networks made up of neurons and synapses connected to and controlling tissues as well as impacting human thoughts and behavior. In modeling neural networks of the nervous system one has to consider many factors. The brain and the neural network should be considered as an integrated and self-contained firmware system that includes hardware (organs), software (programs), memory (short term and long term), database (centralized and distributed), and a complex network of active elements (such as neurons, synapses, and tissues) and passive elements (such as parts of visual and auditory system) that carry information within and in-and-out of the body.\n\nWhy does one want to model the brain and neural network? Although highly sophisticated computer systems have been developed and used in all walks of life, they are nowhere close to the human system in hardware and software capabilities. So, scientists have been at work to understand the human operation system and try to simulate its functionalities. In order to accomplish this, one needs to model its components and functions and validate its performance with real life. Computational models of a well simulated nervous system enable learning the nervous system and apply it to real life problem solutions.\n\nWhat is brain and what is neural network? Section 2.1 addresses the former question from an evolutionary perspective. The answer to the second question is based on the neural doctrine proposed by Ramon y Cajal (1894). He hypothesized that the elementary biological unit is an active cell, called neuron, and the human machine is run by a vast network that connects these neurons, called neural (or neuronal) network. The neural network is integrated with the human organs to form the human machine comprising the nervous system.\n\nInnumerable number of models of various aspects of the nervous system has been developed and there are several Wikipedia articles identified above that have been generated on the subject. The purpose of this article is to present a comprehensive view of all the models and provide the reader, especially a novice, to the neuroscience, with reference to the various sources.\n\nThe basic structural unit of the neural network is connectivity of one neuron to another via an active junction, called synapse. Neurons of widely divergent characteristics are connected to each other via synapses, whose characteristics are also of diverse chemical and electrical properties. In presenting a comprehensive view of all possible modeling of the brain and neural network, an approach is to organize the material based on the characteristics of the networks and the goals that need to be accomplished. The latter could be either for understanding the brain and the nervous system better or to apply the knowledge gained from the total or partial nervous system to real world applications such as artificial intelligence, Neuroethics or improvements in medical science for society.\n\nOn a high level representation, the neurons can be viewed as connected to other neurons to form a neural network in one of three ways. A specific network can be represented as a physiologically (or anatomically) connected network and modeled that way. There are several approaches to this (see Ascoli, G.A. (2002) Sporns, O. (2007), Connectionism, Rumelhart, J. L., McClelland, J. L., and PDP Research Group (1986), Arbib, M. A. (2007)). Or, it can form a functional network that serves a certain function and modeled accordingly (Honey, C. J., Kotter, R., Breakspear, R., & Sporns, O. (2007), Arbib, M. A. (2007)). A third way is to hypothesize a theory of the functioning of the biological components of the neural system by a mathematical model, in the form of a set of mathematical equations. The variables of the equation are some or all of the neurobiological properties of the entity being modeled, such as the dimensions of the dendrite or the stimulation rate of action potential along the axon in a neuron. The mathematical equations are solved using computational techniques and the results are validated with either simulation or experimental processes. This approach to modeling is called computational neuroscience. This methodology is used to model components from the ionic level to system level of the brain. This method is applicable for modeling integrated system of biological components that carry information signal from one neuron to another via intermediate active neurons that can pass the signal through or create new or additional signals. The computational neuroscience approach is extensively used and is based on two generic models, one of cell membrane potential Goldman (1943) and Hodgkin and Katz (1949), and the other based on Hodgkin-Huxley model of action potential (information signal).\n\nSterratt, D., Graham, B., Gillies, A., & Willshaw, D. (2011) classify the biological model of neuroscience into nine levels from ion channels to nervous system level based on size and function. Table 1 is based on this for neuronal networks.\n\nSporns, O. (2007) presents in his article on brain connectivity, modeling based on structural and functional types. A network that connects at neuron and synaptic level falls into the microscale. If the neurons are grouped into population of columns and minicolumns, the level is defined as mesoscale. The macroscale representation considers the network as regions of the brain connected by inter-regional pathways.\n\nArbib, M. A. (2007) considers in the modular model, a hierarchical formulation of the system into modules and submodules.\n\nThe neuronal signal comprises a stream of short electrical pulses of about 100 millivolt amplitude and about 1 to 2 millisecond duration (Gerstner, W., & Kistler, W. (2002) Chapter 1). The individual pulses are action potentials or spikes and the chain of pulses is called spike train. The action potential does not contain any information. A combination of the timing of the start of the spike train, the rate or frequency of the spikes, and the number and pattern of spikes in the spike train determine the coding of the information content or the signal message.\n\nThe neuron cell has three components – dendrites, soma, and axon as shown in Figure 1. Dendrites, which have the shape of a tree with branches, called arbor, receive the message from other neurons with which the neuron is connected via synapses. The action potential received by each dendrite from the synapse is called the postsynaptic potential. The cumulative sum of the postsynaptic potentials is fed to the soma. The ionic components of the fluid inside and outside maintain the cell membrane at a resting potential of about 65 millivolts. When the cumulative postsynaptic potential exceeds the resting potential, an action potential is generated by the cell body or soma and propagated along the axon. The axon may have one or more terminals and these terminals transmit neurotransmitters to the synapses with which the neuron is connected. Depending on the stimulus received by the dendrites, soma may generate one or more well-separated action potentials or spike train. If the stimulus drives the membrane to a positive potential, it is an excitatory neuron; and if it drives the resting potential further in the negative direction, it is an inhibitory neuron.\nThe generation of the action potential is called the “firing.” The firing neuron described above is called a spiking neuron. We will model the electrical circuit of the neuron in Section 3.6. There are two types of spiking neurons. If the stimulus remains above the threshold level and the output is a spike train, it is called the Integrate-and-Fire (IF) neuron model. If output is modeled as dependent on the impulse response of the circuit, then it is called the Spike Response Model (SRM) (Gestner, W. (1995)).\n\nThe spiking neuron model assumes that frequency (inverse of the rate at which spikes are generated) of spiking train starts at 0 and increases with the stimulus current. There is another hypothetical model that formulates the firing to happen at the threshold, but there is a quantum jump in frequency in contrast to smooth rise in frequency as in the spiking neuron model. This model is called the rate model. Gerstner, W., & Kistler, W. (2002), and Sterratt, D., Graham, B., Gillies, A., & Willshaw, D. (2011) are good sources for a detailed treatment of spiking neuron models and rate neuron models.\n\nThe concept of artificial neural network (ANN) was introduced by McColloch, W. S. & Pitts, W. (1943) for models based on behavior of biological neurons. Norbert Wiener (1961) gave this new field the popular name of cybernetics, whose principle is the interdisciplinary relationship among engineering, biology, control systems, brain functions, and computer science. With the computer science field advancing, the von Neumann-type computer was introduced early in the neuroscience study. But it was not suitable for symbolic processing, nondeterministic computations, dynamic executions, parallel distributed processing, and management of extensive knowledge bases, which are needed for biological neural network applications; and the direction of mind-like machine development changed to a learning machine. Computing technology has since advanced extensively and computational neuroscience is now able to handle mathematical models developed for biological neural network. Research and development are progressing in both artificial and biological neural networks including efforts to merge the two.\n\nThe “triune theory of the brain” McLean, P. (2003) is one of several models used to theorize the organizational structure of the brain. The most ancient neural structure of the brain is the brain stem or “lizard brain.” The second phase is limbic or paleo-mammalian brain and performs the four functions needed for animal survival – fighting, feeding, fleeing, and fucking. The third phase is the neocortex or the neo-mammalian brain. The higher cognitive functions which distinguish humans from other animals are primarily in the cortex. The reptilian brain controls muscles, balance, and autonomic functions, such as breathing and heartbeat. This part of the brain is active, even in deep sleep. The limbic system includes the hypothalamus, hippocampus, and amygdala. The neocortex includes the cortex and the cerebrum. It corresponds to the brain of primates and, specifically, the human species. Each of the three brains is connected by nerves to the other two, but each seems to operate as its own brain system with distinct capacities. (See illustration in Triune brain.)\n\nThe connectionist model evolved out of Parallel Distributed Processing framework that formulates a metatheory from which specific models can be generated for specific applications. PDP approach (Rumelhart, J. L., McClelland, J. L., and PDP Research Group (1986)) is a distributed parallel processing of many inter-related operations, somewhat similar to what’s happening in the human nervous system. The individual entities are defined as units and the units are connected to form a network. Thus, in the application to nervous system, one representation could be such that the units are the neurons and the links are the synapses.\n\nThere are three types of brain connectivity models of a network (Sporns, O. (2007)). “Anatomical (or structural) connectivity” describes a network with anatomical links having specified relationship between connected “units.” If the dependent properties are stochastic, it is defined as “functional connectivity.” “Effective connectivity” has causal interactions between distinct units in the system. As stated earlier, brain connectivity can be described at three levels. At microlevel, it connects neurons through electrical or chemical synapses. A column of neurons can be considered as a unit in the mesolevel and regions of the brain comprising a large number of neurons and neuron populations as units in the macrolevel. The links in the latter case are the inter-regional pathways, forming large-scale connectivity.\nFigure 2 shows the three types of connectivity. The analysis is done using the directed graphs (see Sporns, O. (2007) and Hilgetag, C. C. (2002)). In the structural brain connectivity type, the connectivity is a sparse and directed graph. The functional brain connectivity has bidirectional graphs. The effective brain connectivity is bidirectional with interactive cause and effect relationships. Another representation of the connectivity is by matrix representation (See Sporns, O. (2007)). Hilgetag, C. C. (2002) describes the computational analysis of brain connectivity.\n\nArbib, M. A. (2007) describes the modular models as follows. “Modular models of the brain aid the understanding of a complex system by decomposing it into structural modules (e.g., brain regions, layers, columns) or functional modules (schemas) and exploring the patterns of competition and cooperation that yield the overall function.” This definition is not the same as that defined in functional connectivity. The modular approach is intended to build cognitive models and is, in complexity, between the anatomically defined brain regions (defined as macrolevel in brain connectivity) and the computational model at the neuron level.\n\nThere are three views of modules for modeling. They are (1) modules for brain structures, (2) modules as schemas, and (3) modules as interfaces. Figure 3 presents the modular design of a model for reflex control of saccades (Arbib, M. A. (2007)). It involves two main modules, one for superior colliculus (SC), and one for brainstem. Each of these is decomposed into submodules, with each submodule defining an array of physiologically defined neurons. In Figure 3(b) the model of Figure 3(a) is embedded into a far larger model which embraces various regions of cerebral cortex (represented by the modules Pre-LIP Vis, Ctx., LIP, PFC, and FEF), thalamus, and basal ganglia. While the model may indeed be analyzed at this top level of modular decomposition, we need to further decompose basal ganglia, BG, as shown in Figure 3(c) if we are to tease apart the role of dopamine in differentially modulating (the 2 arrows shown arising from SNc) the direct and indirect pathways within the basal ganglia (Crowley, M. (1997)).\nNeural Simulation Language (NSL) has been developed to provide a simulation system for large-scale general neural networks. It provides an environment to develop an object-oriented approach to brain modeling. NSL supports neural models having as basic data structure neural layers with similar properties and similar connection patterns. Models developed using NSL are documented in Brain Operation Database (BODB) as hierarchically organized modules that can be decomposed into lower levels.\n\nAs mentioned in Section 2.4, development of artificial neural network (ANN), or neural network as it is now called, started as simulation of biological neuron network and ended up using artificial neurons. Major development work has gone into industrial applications with learning process. Complex problems were addressed by simplifying the assumptions. Algorithms were developed to achieve a neurological related performance, such as learning from experience. Since the background and overview have been covered in the other internal references, the discussion here is limited to the types of models. The models are at the system or network level.\n\nThe four main features of an ANN are topology, data flow, types of input values, and forms of activation (Meireles, M. R. G. (2003), Munakata, T. (1998)). Topology can be multilayered, single-layered, or recurrent. Data flow can be recurrent with feedback or non-recurrent with feedforward model. The inputs are binary, bipolar, or continuous. The activation is linear, step, or sigmoid. Multilayer Perceptron (MLP) is the most popular of all the types, which is generally trained with back-propagation of error algorithm. Each neuron output is connected to every neuron in subsequent layers connected in cascade and with no connections between neurons in the same layer. Figure 4 shows a basic MLP topology (Meireles, M. R. G. (2003)), and a basic telecommunication network (Subramanian, M. (2010)) that most are familiar with. We can equate the routers at the nodes in telecommunication network to neurons in MLP technology and the links to synapses.\n\nComputational science is an interdisciplinary field that combines engineering, biology, control systems, brain functions, physical sciences, and computer science. It has fundamental development models done at the lower levels of ions, neurons, and synapses, as well as information propagation between neurons. These models have established the enabling technology for higher-level models to be developed. They are based on chemical and electrical activities in the neurons for which electrical equivalent circuits are generated. A simple model for the neuron with predominantly potassium ions inside the cell and sodium ions outside establishes an electric potential on the membrane under equilibrium, i.e., no external activity, condition. This is called the resting membrane potential, which can be determined by Nernst Equation (Nernst, W. (1888)). An equivalent electrical circuit for a patch of membrane, for example an axon or dendrite, is shown in Figure 5. E and E are the potentials associated with the potassium and sodium channels respectively and R and R are the resistances associated with them. C is the capacitance of the membrane and I is the source current, which could be the test source or the signal source (action potential). The resting potential for potassium-sodium channels in a neuron is about -65 millivolts.\nThe membrane model is for a small section of the cell membrane; for larger sections it can be extended by adding similar sections, called compartments, with the parameter values being the same or different. The compartments are cascaded by a resistance, called axial resistance. Figure 6 shows a compartmental model of a neuron that is developed over the membrane model. Dendrites are the postsynaptic receptors receiving inputs from other neurons; and the axon with one or more axon terminals transmits neurotransmitters to other neurons.\nThe second building block is the Hodgkin-Huxley (HH) model of the action potential. When the membrane potential from the dendrites exceeds the resting membrane potential, a pulse is generated by the neuron cell and propagated along the axon. This pulse is called the action potential and HH model is a set of equations that is made to fit the experimental data by the design of the model and the choice of the parameter values.\n\nModels for more complex neurons containing other types of ions can be derived by adding to the equivalent circuit additional battery and resistance pairs for each ionic channel. The ionic channel could be passive or active as they could be gated by voltage or be ligands. The extended HH model has been developed to handle the active channel situation.\n\nAlthough there are neurons that are physiologically connected to each other, information is transmitted at most of the synapses by chemical process across a cleft. Synapses are also computationally modeled. The next level of complexity is that of stream of action potentials, which are generated, whose pattern contains the coding information of the signal being transmitted. There are basically two types of action potentials, or spikes as they are called, that are generated. One is “integrate-and-fire” (the one we have so far addressed) and the other which is rate based. The latter is a stream whose rate varies. The signal going across the synapses could be modeled either as a deterministic or a stochastic process based on the application (See Section 3.7). Another anatomical complication is when a population of neurons, such as a column of neurons in visionary system, needs to be handled. This is done by considering the collective behavior of the group (Kotter, R., Nielson, P., Dyhrfjeld-Johnson, J., Sommer, F. T., & Northoff, G. (2002)).\n\nThe action potential or the spike does not itself carry any information. It is the stream of spikes, called spike train, that carry the information in its number and pattern of spikes and timing of spikes. The postsynaptic potential can be either positive, the excitatory synapse or negative, inhibitory synapse. In modeling, the postsynaptic potentials received by the dendrites in the postsynaptic neuron are integrated and when the integrated potential exceeds the resting potential, the neuron fires an action potential along its axon. This model is the Integrate-and-Fire (IF) model that was mentioned in Section 2.3. Closely related to IF model is a model called Spike Response Model (SRM) (Gerstner, W. (1995) Pages 738-758) that is dependent on impulse function response convoluted with the input stimulus signal. This forms a base for a large number of models developed for spiking neural networks.\n\nThe IF and SR model of spike train occurs in Type I neurons, in which the spike rate or spike frequency of the occurrence increases smoothly with the increase in stimulus current starting from zero. Another phenomenon of spike train generation happens in Type II neurons, where firing occurs at the resting potential threshold, but with a quantum jump to a non-zero frequency. Models have been developed using the rate (frequency) of the spike train and are called rate-based models.\n\nWhat is important for understanding the functions of the nervous system is how the message is coded and transported by the action potential in the neuron. There are two theories on how the signal that is being propagated is coded in the spikes as to whether it is pulse code or rate code. In the former, it is the time delay of the first spike from the time of stimulus as seen by the postsynaptic receiver that determines the coding. In the rate code, it is average rate of the spike that influences the coding. It is not certain as to which is really the actual physiological phenomenon in each case. However, both cases can be modeled computationally and the parameters varied to match the experimental result. The pulse mode is more complex to model and numerous detailed neuron models and population models are described by Gerstner and Kistler in Parts I and II of Gerstner, W., & Kistler, W. (2002) and Chapter 8 of Sterratt, D., Graham, B., Gillies, A., & Willshaw, D. (2011). Another important characteristic associated with SR model is the spike-time-dependent-plasticity. It is based on Hebb’s postulate on plasticity of synapse, which states that “the neurons that fire together wire together.” This causes the synapse to be a long-term potentiation (LTP) or long-term depression (LTD). The former is the strengthening of the synapse between two neurons if the postsynaptic spike temporally follows immediately after the presynaptic spike. Latter is the case if it is reverse, i.e., the presynaptic spike occurs after the postsynaptic spike. Gerstner, W. & Kistler, W. (2002) in Chapter 10 and Sterratt, D., Graham, B., Gillies, A., & Willshaw, D. (2011) in Chapter 7 discuss the various models related to Hebbian models on plasticity and coding.\n\nThe challenge involved in developing models for small, medium, and large networks is one of reducing the complexity by making valid simplifying assumptions in and extending the Hodgkin-Huxley neuronal model appropriately to design those models ( see Chapter 9 of Sterratt, D., Graham, B., Gillies, A., & Willshaw, D. (2011), Kotter, R., Nielson, P., Dyhrfjeld-Johnson, J., Sommer, F. T., & Northoff, G. (2002), and Chapter 9 of Gerstner, W., & Kistler, W. (2002)). Network models can be classified as either network of neurons propagating through different levels of cortex or neuron populations interconnected as multilevel neurons. The spatial positioning of neuron could be 1-, 2- or 3-dimensional; the latter ones are called small-world networks as they are related to local region. The neuron could be either excitatory or inhibitory, but not both. Modeling design depends on whether it is artificial neuron or biological neuron of neuronal model. Type I or Type II choice needs to be made for the firing mode. Signaling in neurons could be rate-based neurons, spiking response neurons, or deep-brain stimulated. The network can be designed as feedforward or recurrent type. The network needs to be scaled for the computational resource capabilities. Large-scale thalamocortical systems are handled in the manner of the Blue Brain project (Markam, H. (2006)).\n\nNo generalized modeling concepts exist for modeling the development of anatomical physiology and morphology similar to the one of behavior of neuronal network, which is based on HH model. Shankle, W. R., Hara, J., Fallon, J. H., and Landing, B. H. (2002) describe the application of neuroanatomical data of the developing human cerebral cortex to computational models. Sterratt, D., Graham, B., Gillies, A., & Willshaw, D. (2011) discuss aspects of the nervous system of computational modeling in the development of nerve cell morphology, cell physiology, cell patterning, patterns of ocular dominance, and connection between nerve cell and muscle, and retinotopic maps. Carreira-Perpinan, M. A. & Goodhill, G. J. (2002) deal with the optimization of the computerized models of the visual cortex.\n\nWith the enormous number of models that have been created, tools have been developed for dissemination of the information, as well as platforms to develop models. Several generalized tools, such as GENESIS, NEURON, XPP, and NEOSIM are available and are discussed by Hucka, M. (2002).\n\n", "id": "33818014", "title": "Nervous system network models"}
{"url": "https://en.wikipedia.org/wiki?curid=34053961", "text": "Manifold alignment\n\nManifold alignment is a class of machine learning algorithms that produce projections between sets of data, given that the original data sets lie on a common manifold. The concept was first introduced as such by Ham, Lee, and Saul in 2003, adding a manifold constraint to the general problem of correlating sets of high-dimensional vectors.\n\nManifold alignment assumes that disparate data sets produced by similar generating processes will share a similar underlying manifold representation. By learning projections from each original space to the shared manifold, correspondences are recovered and knowledge from one domain can be transferred to another. Most manifold alignment techniques consider only two data sets, but the concept extends to arbitrarily many initial data sets.\n\nConsider the case of aligning two data sets, formula_1 and formula_2, with formula_3 and formula_4.\n\nManifold alignment algorithms attempt to project both formula_1 and formula_2 into a new \"d\"-dimensional space such that the projections both minimize distance between corresponding points and preserve the local manifold structure of the original data. The projection functions are denoted:\n\nformula_7\n\nformula_8\n\nLet formula_9 represent the binary correspondence matrix between points in formula_1 and formula_2:\n\nformula_12\n\nLet formula_13 and formula_14 represent pointwise similarities within data sets. This is usually encoded as the heat kernel of the adjacency matrix of a \"k\"-nearest neighbor graph.\n\nFinally, introduce a coefficient formula_15, which can be tuned to adjust the weight of the 'preserve manifold structure' goal, versus the 'minimize corresponding point distances' goal.\n\nWith these definitions in place, the loss function for manifold alignment can be written:\n\nformula_16\n\nSolving this optimization problem is equivalent to solving a generalized eigenvalue problem using the graph laplacian of the joint matrix, \"G\":\n\nformula_17\n\nThe algorithm described above requires full pairwise correspondence information between input data sets; a supervised learning paradigm. However, this information is usually difficult or impossible to obtain in real world applications. Recent work has extended the core manifold alignment algorithm to semi-supervised\n\n, unsupervised\n\n, and multiple-instance\n\nsettings.\n\nThe algorithm described above performs a \"one-step\" alignment, finding embeddings for both data sets at the same time. A similar effect can also be achieved with \"two-step\" alignments\n, following a slightly modified procedure:\n\nManifold alignment can be used to find linear (feature-level) projections, or nonlinear (instance-level) embeddings. While the instance-level version generally produces more accurate alignments, it sacrifices a great degree of flexibility as the learned embedding is often difficult to parameterize. Feature-level projections allow any new instances to be easily embedded in the manifold space, and projections may be combined to form direct mappings between the original data representations. These properties are especially important for knowledge-transfer applications.\n\nManifold alignment is suited to problems with several corpora that lie on a shared manifold, even when each corpus is of a different dimensionality. Many real-world problems fit this description, but traditional techniques are not able to take advantage of all corpora at the same time. Manifold alignment also facilitates transfer learning, in which knowledge of one domain is used to jump-start learning in correlated domains.\n\nApplications of manifold alignment include:\n\n", "id": "34053961", "title": "Manifold alignment"}
{"url": "https://en.wikipedia.org/wiki?curid=34413019", "text": "Document mosaicing\n\nDocument mosaicing is a process that stitches multiple, overlapping snapshot images of a document together in order to produce one large, high resolution composite. The document is slid under a stationary, over-the-desk camera by hand until all parts of the document are snapshotted by the camera’s field of view. As the document slid under the camera, all motion of the document is coarsely tracked by the vision system. The document is periodically snapshotted such that the successive snapshots are overlap by about 50%. The system then finds the overlapped pairs and stitches them together repeatedly until all pairs are stitched together as one piece of document.\n\nThe document mosaicing can be divided into four main processes.\n\nIn this process, the motion of the document slid under the camera is coarsely tracked by the system. Tracking is performed by a process called simple correlation process. In the first frame of snapshots, a small patch is extracted from the center of the image \nas a correlation template as shown in Figure 1. The correlation process is performed in the four times size of the patch area of the next frame. The motion of the paper is indicated by the peak in the correlation function. The peak in the correlation function indicates the motion of the paper. The template is resampled from this frame and the tracking continues until the template reaches the edge of the document. After the template reaches the edge of the document, another snapshot is taken and the tracking process performs repeatedly until the whole document is imaged. The snapshots are stored in an ordered list in order to pair the overlapped images in later processes.s\n\nFeature detection is the process of finding the transformation which aligns one image with another. There are two main approaches for feature detection.\n\nEach image is segmented into a hierarchy of columns, lines and words in order to match with the organised sets of features across images. Skew angle estimation and columns, lines and words finding are the examples of feature detection operations.\n\nFirstly, the angle that the rows of text make with the image raster lines (skew angle) is estimated. It is assumed to lie in the range of ±20°. A small patch of text in the image is selected randomly and then rotated in the range of ±20° until the variance of the pixel intensities of the patch summed along the raster lines is maximised. See Figure 2.\n\nIn order to ensure that the found skew angle is accurate, the document mosaic system performs calculation at many image patches and derive the final estimation by finding the average of the individual angles weighted by the variance of the pixel intensities of each patch.\n\nIn this operation, the de-skewed document is intuitively segmented into a hierarchy of columns, lines and words. The sensitivity to illumination and page coloration of the de-skewed document can be removed by applying a Sobel operator to the de-skewed image and thresholding the output to obtain the binary gradient, de-skewed image. See Figure 3.\n\nThe operation can be roughly separated into 3 steps : column segmentation, line segmentation and word segmentation.\n\n\nThese segmentations are important because the document mosaic will be created by matching the lower right corners of words in overlapping images pair. Moreover, the segmentation operation can organize the list of images in the context of a hierarchy of rows and column reliably.\n\nThe segmentation operation involves a considerable amount of summing in the binary gradient, de-skewed images, which done by construct a matrix of partial sums whose elements are given by\n\nformula_1\nThe matrix of partial sums is calculated in one pass through the binary gradient, de-skewed image.\n\nformula_2\n\nThe two images are now organized in hierarchy of linked lists in following structure :\n\nAt the bottom of the structure, the length of each word is recorded for establishing correspondence between two images in order to reduce to search only the corresponding structures for the groups of words with the matching lengths.\n\nA seed match finding is done by comparing each row in image1 with each row in image2. The two rows are then compared to each other by every word. If the length (in pixel) of the two words (one from image1 and one from image2) and their immediate neighbours agree with each other within a predefined tolerance threshold (5 pixels, for example), then they are assumed to be matched. The row of each image is assumed to be matched if there are three or more word matches between the two rows. The seed match finding operation is terminated when two pairs of consecutive row match are found.\n\nAfter finishing a seed match finding operation, the next process is to build the match list to generate the correspondences points of the two images. The process is done by searching the matching pairs of rows away from the seed row.\n\nGiven the list of corresponding points of the two images, finding the transformation of the overlapping portion of the images is next process to be performed. Assuming a pinhole camera model, the transformation between pixels (u,v) of image 1 and pixels (u0, v0) of image 2 is demonstrated by a plane-to-plane projectivity.\n\nformula_3\n\nThe parameters of the projectivity is found from four pairs of matching points. RANSAC regression technique is used to reject outlying matches and estimate the projectivity from the remaining good matches.\n\nThe projectivity is fine-tuned using correlation at the corners of the overlapping portion in order to obtain four correspondences to sub-pixel accuracy. Therefore, image1 is then transformed into image2’s coordinate system using Eq.1. The typical result of the process is shown in Figure 5.\n\nFinally, the whole page composition is built up by mapping all the images into the coordinate system of an “anchor” image, which is normally the one nearest the page center. The transformations to the anchor frame are calculated by concatenating the pair-wise transformations which found earlier. The raw document mosaic is shown in Figure 6.\n\nHowever, there might be a problem of non-consecutive images that are overlap. This problem can be solved by performing Hierarchical sub-mosaics. As shown in Figure 7, image1 and image2 are registered, as are image3 and image4, creating two sub-mosaics. These two sub-mosaics are later stitched together in another mosaicing process.\n\nThere are various areas that the technique of document mosaicing can be applied to such as : \n\n\n", "id": "34413019", "title": "Document mosaicing"}
{"url": "https://en.wikipedia.org/wiki?curid=34130293", "text": "Thompson sampling\n\nIn artificial intelligence, Thompson sampling, named after William R. Thompson, is a heuristic for choosing actions that addresses the exploration-exploitation dilemma in the multi-armed bandit problem. It consists in choosing the action that maximizes the expected reward with respect to a randomly drawn belief.\n\nConsider a set of contexts formula_1, a set of actions formula_2, and rewards in formula_3. In each round, the player obtains a context formula_4, plays an action formula_5 and receives a reward formula_6 following a distribution that depends on the context and the issued action. The aim of the player is to play actions such as to maximize the cumulative rewards.\n\nThe elements of Thompson sampling are as follows:\n\nThompson sampling consists in playing the action formula_15 according to the probability that it maximizes the expected reward, i.e. \nwhere formula_17 is the indicator function.\n\nIn practice, the rule is implemented by sampling, in each round, a parameter formula_18 from the posterior formula_19, and choosing the action formula_20 that maximizes formula_21, i.e. the expected reward given the parameter, the action and the current context. Conceptually, this means that the player instantiates his or her beliefs randomly in each round, and then acts optimally according to them.\n\nThompson sampling was originally described in an article by Thompson from 1933 but has been largely ignored by the artificial intelligence community. It was subsequently rediscovered numerous times independently in the context of reinforcement learning. A first proof of convergence for the bandit case has been shown in 1997. The first application to Markov decision processes was in 2000. A related approach (see Bayesian control rule) was published in 2010. In 2010 it was also shown that Thompson sampling is \"instantaneously self-correcting\". Asymptotic convergence results for contextual bandits were published in 2011. Nowadays, Thompson Sampling has been widely used in many online learning problems: Thompson sampling has also been applied to A/B testing in website design and online advertising; Thompson sampling has formed the basis for accelerated learning in decentralized decision making; a Double Thompson Sampling (D-TS) algorithm has been proposed for dueling bandits, a variant of traditional MAB, where feedbacks come in the format of pairwise comparison.\n\nProbability matching is a decision strategy in which predictions of class membership are proportional to the class base rates. Thus, if in the training set positive examples are observed 60% of the time, and negative examples are observed 40% of the time, the observer using a probability-matching strategy will predict (for unlabeled examples) a class label of \"positive\" on 60% of instances, and a class label of \"negative\" on 40% of instances.\n\nA generalization of Thompson sampling to arbitrary dynamical environments and causal structures, known as Bayesian control rule, has been shown to be the optimal solution to the adaptive coding problem with actions and observations. In this formulation, an agent is conceptualized as a mixture over a set of behaviours. As the agent interacts with its environment, it learns the causal properties and adopts the behaviour that minimizes the relative entropy to the behaviour with the best prediction of the environment's behaviour. If these behaviours have been chosen according to the maximum expected utility principle, then the asymptotic behaviour of the Bayesian control rule matches the asymptotic behaviour of the perfectly rational agent.\n\nThe setup is as follows. Let formula_22 be the actions issued by an agent up to time formula_23, and let formula_24 be the observations gathered by the agent up to time formula_23. Then, the agent issues the action formula_26 with probability:\nwhere the \"hat\"-notation formula_28 denotes the fact that formula_29 is a causal intervention (see Causality), and not an ordinary observation. If the agent holds beliefs formula_30 over its behaviors, then the Bayesian control rule becomes\nwhere formula_32 is the posterior distribution over the parameter formula_9 given actions formula_34 and observations formula_35.\n\nIn practice, the Bayesian control amounts to sampling, in each time step, a parameter formula_18 from the posterior distribution formula_32, where the posterior distribution is computed using Bayes' rule by only considering the (causal) likelihoods of the observations formula_24 and ignoring the (causal) likelihoods of the actions formula_22, and then by sampling the action formula_40 from the action distribution formula_41.\n", "id": "34130293", "title": "Thompson sampling"}
{"url": "https://en.wikipedia.org/wiki?curid=19614447", "text": "Radiant AI\n\nThe Radiant AI is a technology developed by Bethesda Softworks for \"The Elder Scrolls\" video games. It allows non-player characters (NPCs) to make choices and engage in behaviors more complex than in past titles. The technology was developed for \"\" and expanded in \"\"; it is also used in \"Fallout 3\", \"\" and \"Fallout 4\", also published by Bethesda.\n\nThe Radiant A.I. technology, as it evolved in its latest iteration developed for \"Skyrim\", comprises two parts:\n\nThe Radiant A.I. system deals with NPC interactions and behavior. It allows non-player characters to dynamically react to and interact with the world around them. General goals, such as \"Eat in this location at 2pm\" are given to NPCs, and NPCs are left to determine how to achieve them. The absence of individual scripting for each character allows for the construction of a world on a much larger scale than other games had developed, and aids in the creation of what Todd Howard described as an \"organic feel\" for the game.\n\nThe Radiant Story system deals with how the game itself reacts to the player behavior, such as the creation of new dynamic quests. Dynamically generated quests are placed by the game in locations the player hasn't visited yet and are related to earlier adventures.\n", "id": "19614447", "title": "Radiant AI"}
{"url": "https://en.wikipedia.org/wiki?curid=3709680", "text": "Pattern theory\n\nPattern theory, formulated by Ulf Grenander, is a mathematical formalism to describe knowledge of the world as patterns. It differs from other approaches to artificial intelligence in that it does not begin by prescribing algorithms and machinery to recognize and classify patterns; rather, it prescribes a vocabulary to articulate and recast the pattern concepts in precise language.\n\nIn addition to the new algebraic vocabulary, its statistical approach is novel in its aim to:\nBroad in its mathematical coverage, Pattern Theory spans algebra and statistics, as well as local topological and global entropic properties.\n\nThe Brown University Pattern Theory Group was formed in 1972 by Ulf Grenander. Many mathematicians are currently working in this group, noteworthy among them being the Fields Medalist David Mumford. Mumford regards Grenander as his \"guru\" in this subject.\n\nWe begin with an example to motivate the algebraic definitions that follow.\n\nIf we want to represent language patterns, the most immediate candidate for primitives might be words. However, set phrases, such as “in order to”, immediately indicate the inappropriateness of words as atoms. In searching for other primitives, we might try the rules of grammar. We can represent grammars as finite state automata or context-free grammars. Below is a sample finite state grammar automaton.\n\nThe following phrases are generated from a few simple rules of the automaton and programming code in pattern theory:\nTo create such sentences, rewriting rules in finite state automata act as generators to create the sentences as follows: if a machine starts in state 1, it goes to state 2 and writes the word “the”. From state 2, it writes one of 4 words: prince, boy, princess, girl, chosen at random. The probability of choosing any given word is given by the Markov chain corresponding to the automaton. Such a simplistic automaton occasionally generates more awkward sentences\n\nFrom the finite state diagram we can infer the following generators (shown at right) that creates the signal. A generator is a 4-tuple: current state, next state, word written, probability of written word when there are multiple choices. That is, each generator is a state transition arrow of state diagram for a Markov chain.\n\nImagine that a configuration of generators are strung together linearly so its output forms a sentence, so each generator \"bonds\" to the generators before and after it. Denote these bonds as 1a,1b,2a,2b,…12a,12b. Each numerical label corresponds to the automaton's state and each letter \"a\" and \"b\" corresponds to the inbound and outbound bonds. Then the following bond table (left) is equivalent to the automaton diagram. For the sake of simplicity, only half of the bond table is shown—the table is actually symmetric.\n\nAs one can tell from this example, and typical of signals that are studied, identifying the primitives and bond tables requires some thought. The example highlights another important fact not readily apparent in other signals problems: that a configuration is not the signal that is observed; rather, its image as a sentence is observed. Herein lies a significant justification for distinguishing an observable from a non-observable construct. Additionally, it provides an algebraic structure to associate with hidden Markov models. In sensory examples such as the vision example below, the hidden configurations and observed images are much more similar, and such a distinction may not seem justified. Fortunately, the grammar example reminds us of this distinction.\n\nA more sophisticated example can be found in the link grammar theory of natural language.\n\nMotivated by the example, we have the following definitions:\n\nPattern Theory defines order in terms of the feature of interest given by \"p\"(\"c\").\n\nGrenander’s Pattern Theory treatment of Bayesian inference in seems to be skewed towards on image reconstruction (e.g. content addressable memory). That is given image I-deformed, find I. However, Mumford’s interpretation of Pattern Theory is broader and he defines PT to include many more well-known statistical methods. Mumford’s criteria for inclusion of a topic as Pattern Theory are those methods \"characterized by common techniques and motivations\", such as the HMM, EM algorithm, dynamic programming circle of ideas. Topics in this section will reflect Mumford's treatment of Pattern Theory. His principle of statistical Pattern Theory are the following:\n\nStatistical PT makes ubiquitous use of conditional probability in the form of Bayes theorem and Markov Models. Both these concepts are used to express the relation between hidden states (configurations) and observed states (images). Markov Models also captures the local properties of the stimulus, reminiscent of the purpose of bond table for regularity.\n\nThe generic set up is the following:\nLet \"s\" = the hidden state of the data that we wish to know. \"i\" = observed image. Bayes theorem gives\n\nThe following conditional probability examples illustrates these methods in action:\n\nN-gram Text Strings: See Mumford's Pattern Theory by Examples, Chapter 1.\n\nMAP ~ MDL (MDL offers a glimpse of why the MAP probabilistic formulation make sense analytically)\n\nSupposing we want to translate French sentences to English. Here, the hidden configurations are English sentences and the observed signal they generate are French sentences. Bayes theorem gives \"p\"(\"e\"|\"f\")\"p\"(\"f\") = \"p\"(\"e\", \"f\") = \"p\"(\"f\"|\"e\")\"p\"(\"e\") and reduces to the fundamental equation of machine translation: maximize \"p\"(\"e\"|\"f\") = \"p\"(\"f\"|\"e\")\"p\"(\"e\") over the appropriate \"e\" (note that \"p\"(\"f\") is independent of \"e\", and so drops out when we maximize over \"e\"). This reduces the problem to three main calculations for:\nThe analysis seems to be symmetric with respect to the two languages, and if we think can calculate \"p\"(\"f\"|\"e\"), why not turn the analysis around and calculate \"p\"(\"e\"|\"f\") directly? The reason is that during the calculation of \"p\"(\"f\"|\"e\") the asymmetric assumption is made that source sentence be well formed and we cannot make any such assumption about the target translation because we do not know what it will translate into.\n\nWe now focus on \"p\"(\"f\"|\"e\") in the three-part decomposition above. The other two parts, \"p\"(\"e\") and maximizing \"e\", uses similar techniques as the \"N\"-gram model. Given a French-English translation from a large training data set (such data sets exists from the Canadian parliament),\nthe sentence pair can be encoded as an \"alignment\" (2, 3, 4, 5, 6, 6, 6) that reads as follows: the first word in French comes from the second English word, the second word in French comes from the 3rd English word, and so forth. Although an alignment is a straightforward encoding of the translation, a more computationally convenient approach to an alignment is to break it down into four parameters:\n\nFor the sake of simplicity in demonstrating an EM algorithm, we shall go through a simple calculation involving only translation probabilities \"t\"(), but needless to say that it the method applies to all parameters in their full glory. Consider the simplified case (1) without the NULL word (2) where every word has fertility 1 and (3) there are no distortion probabilities. Our training data corpus will contain two-sentence pairs: \"bc\" → \"xy\" and \"b\" → \"y\". The translation of a two-word English sentence “b c” into the French sentence “\"x y\"” has two possible alignments, and including the one-sentence words, the alignments are:\ncalled Parallel, Crossed, and Singleton respectively.\n\nTo illustrate an EM algorithm, first set the desired parameter uniformly, that is\n\nThen EM iterates as follows\nThe alignment probability for the “crossing alignment” (where \"b\" connects to \"y\") got a boost from the second sentence pair \"b\"/\"y\". That further solidified \"t\"(\"y\" | \"b\"), but as a side effect also boosted \"t\"(\"x\" | \"c\"), because \"x\" connects to \"c\" in that same “crossing alignment.” The effect of boosting \"t\"(\"x\" | \"c\") necessarily means downgrading \"t\"(\"y\" | \"c\") because they sum to one. So, even though \"y\" and \"c\" co-occur, analysis reveals that they are not translations of each other. With real data, EM also is subject to the usual local extremum traps.\n\nFor decades, speech recognition seemed to hit an impasse as scientists sought descriptive and analytic solution. The sound wave p(t) below is produced by speaking the word “ski”.\n\nIts four distinct segments has very different characteristics. One can choose from many levels of generators (hidden variables): the intention of the speaker’s brain, the state of the mouth and vocal cords, or the ‘phones’ themselves. Phones are the generator of choice to be inferred and it encodes the word in a noisy, highly variable way. Early work on speech recognition attempted to make this inference deterministically using logical rules based on binary features extracted from p(t). For instance, the table below shows some of the features used to distinguish English consonants.\n\nIn real situations, the signal is further complicated by background noises such as cars driving by or artifacts such as a cough in mid sentence (Mumford’s 2nd underpinning). The deterministic rule-based approach failed and the state of the art (e.g. Dragon Naturally Speaking) is to use a family of precisely tuned HMMs and Bayesian MAP estimators to do better. Similar stories played out in vision, and other stimulus categories.\n\n\n\n\n", "id": "3709680", "title": "Pattern theory"}
{"url": "https://en.wikipedia.org/wiki?curid=34591537", "text": "Automated personal assistant\n\nAn automated personal assistant or an Intelligent Personal Assistant is a mobile software agent that can perform tasks, or services, on behalf of an individual based on a combination of user input, location awareness, and the ability to access information from a variety of online sources (such as weather conditions, traffic congestion, news, stock prices, user schedules, retail prices, etc.).\n\nThere are two types of automated personal assistants: intelligent automated assistants (for example, Apple’s Siri and Tronton’s Cluzee), which perform concierge-type tasks (e.g., making dinner reservations, purchasing event tickets, making travel arrangements) or provide information based on voice input or commands; and smart personal agents, which automatically perform management or data-handling tasks based on online information and events often without user initiation or interaction.\n\nAccording to Chi-Hua Chien of Kleiner Perkins Caufield & Byers, examples of tasks that may be performed by a smart personal agent-type of automated personal assistant include schedule management (e.g., sending an alert to a dinner date that a user is running late due to traffic conditions, update schedules for both parties, and change the restaurant reservation time) and personal health management (e.g., monitoring caloric intake, heart rate and exercise regimen, then making recommendations for healthy choices).\n\nBoth types of automated personal assistant technology are enabled by the combination of mobile computing devices, application programming interfaces (APIs), and the proliferation of mobile apps. However, intelligent automated assistants are designed to perform specific, one-off tasks specified by user voice instructions, while smart personal agents perform ongoing tasks (e.g., schedule management) autonomously.\n\n", "id": "34591537", "title": "Automated personal assistant"}
{"url": "https://en.wikipedia.org/wiki?curid=34641430", "text": "Contextual image classification\n\nContextual image classification, a topic of pattern recognition in computer vision, is an approach of classification based on contextual information in images. \"Contextual\" means this approach is focusing on the relationship of the nearby pixels, which is also called neighbourhood. The goal of this approach is to classify the images by using the contextual information.\n\nSimilar as processing language, a single word may have multiple meanings unless the context is provided, and the patterns within the sentences are the only informative segments we care about. For images, the principle is same. Find out the patterns and associate proper meanings to them.\n\nAs the image illustrated below, if only a small portion of the image is shown, it is very difficult to tell what the image is about.\nEven try another portion of the image, it is still difficult to classify the image.\nHowever, if we increase the contextual of the image, then it makes more sense to recognize.\nAs the full images shows below, almost everyone can classify it easily.\nDuring the procedure of segmentation, the methods which do not use the contextual information are sensitive to noise and variations, thus the result of segmentation will contain a great deal of misclassified regions, and often these regions are small (e.g., one pixel).\n\nCompared to other techniques, this approach is robust to noise and substantial variations for it takes the continuity of the segments into account.\n\nSeveral methods of this approach will be described below.\n\nThis approach is very effective against small regions caused by noise. And these small regions are usually formed by few pixels or one pixel. The most probable label is assigned to these regions.\nHowever, there is a drawback of this method. The small regions also can be formed by correct regions rather than noise, and in this case the method is actually making the classification worse.\nThis approach is widely used in remote sensing applications.\n\nThis is a two-stage classification process:\n\nInstead of using single pixels, the neighbour pixels can be merged into homogeneous regions benefiting from contextual information. And provide these regions to classifier.\n\nThe original spectral data can be enriched by adding the contextual information carried by the neighbour pixels, or even replaced in some occasions. This kind of pre-processing methods are widely used in textured image recognition. The typical approaches include mean values, variances, texture description, etc.\n\nThe classifier uses the grey level and pixel neighbourhood (contextual information) to assign labels to pixels. In such case the information is a combination of spectral and spatial information.\n\nContextual classification of image data is based on the Bayes minimum error classifier (also known as a naive Bayes classifier).\n\nPresent the pixel:\n\n\n\nThe neighbourhood:\nSize of the neighbourhood. There is no limitation of the size, but it is considered to be relatively small for each pixel formula_1.\nA reasonable size of neighbourhood would be formula_15 of 4-connectivity or 8-connectivity (formula_1 is marked as red and placed in the centre).\nThe calculation:\n\nApply the minimum error classification on a pixel formula_1, if the probability of a class formula_18 being presenting the pixel formula_1 is the highest among all, then assign formula_18 as its class.\n\nThe contextual classification rule is described as below, it uses the feature vector formula_22 rather than formula_1.\n\nUse the Bayes formula to calculate the posteriori probability formula_25\n\nThe number of vectors is the same as the number of pixels in the image. For the classifier uses a vector corresponding to each pixel formula_27, and the vector is generated from the pixel's neighbourhood.\n\nThe basic steps of contextual image classification:\n\nThe template matching is a \"brute force\" implementation of this approach. The concept is first create a set of templates, and then look for small parts in the image match with a template.\n\nThis method is computationally high and inefficient. It keeps an entire templates list during the whole process and the number of combinations is extremely high. For a formula_33 pixel image, there could be a maximum of formula_34 combinations, which leads to high computation. This method is a top down method and often called table look-up or dictionary look-up.\n\nThe Markov chain also can be applied in pattern recognition. The pixels in an image can be recognised as a set of random variables, then use the lower order Markov chain to find the relationship among the pixels. The image is treated as a virtual line, and the method uses conditional probability.\n\nThe Hilbert curve runs in a unique pattern through the whole image, it traverses every pixel without visiting any of them twice and keeps a continuous curve. It is fast and efficient.\n\nThe lower-order Markov chain and Hilbert space-filling curves mentioned above are treating the image as a line structure. The Markov meshes however will take the two dimensional information into account.\n\nThe dependency tree is a method using tree dependency to approximate probability distributions.\n\n", "id": "34641430", "title": "Contextual image classification"}
{"url": "https://en.wikipedia.org/wiki?curid=34668189", "text": "3D reconstruction from multiple images\n\n3D reconstruction from multiple images is the creation of three-dimensional models from a set of images. It is the reverse process of obtaining 2D images from 3D scenes.\n\nThe essence of an image is a projection from a 3D scene onto a 2D plane, during which process the depth is lost. The 3D point corresponding to a specific image point is constrained to be on the line of sight. From a single image, it is impossible to determine which point on this line corresponds to the image point. If two images are available, then the position of a 3D point can be found as the intersection of the two projection rays. This process is referred to as triangulation. The key for this process is the relations between multiple views which convey the information that corresponding sets of points must contain some structure and that this structure is related to the poses and the calibration of the camera.\n\nIn recent decades, there is an important demand for 3D content for computer graphics, virtual reality and communication, triggering a change in emphasis for the requirements. Many existing systems for constructing 3D models are built around specialized hardware (e.g. stereo rigs) resulting in a high cost, which cannot satisfy the requirement of its new applications. This gap stimulates the use of digital imaging facilities (like a camera). Moore's law also tells us that more work can be done in software. An early method was proposed by Tomasi and Kanade. They used an affine factorization approach to extract 3D from images sequences. However, the assumption of orthographic projection is a significant limitation of this system.\n\nThe task of converting multiple 2D images into 3D model consists of a series of processing steps:\n\nCamera calibration consists of intrinsic and extrinsic parameters, without which at some level no arrangement of algorithms can work. The dotted line between Calibration and Depth determination represents that the camera calibration is usually required for determining depth.\n\nDepth determination serves as the most challenging part in the whole process, as it calculates the 3D component missing from any given image – depth. The correspondence problem, finding matches between two images so the position of the matched elements can then be triangulated in 3D space is the key issue here.\n\nOnce you have the multiple depth maps you have to combine them to create a final mesh by calculating depth and projecting out of the camera – registration. Camera calibration will be used to identify where the many meshes created by depth maps can be combined together to develop a larger one, providing more than one view for observation.\n\nBy the stage of Material Application you have a complete 3D mesh, which may be the final goal, but usually you will want to apply the color from the original photographs to the mesh. This can range from projecting the images onto the mesh randomly, through approaches of combining the textures for super resolution and finally to segmenting the mesh by material, such as specular and diffuse properties.\n\nGiven a group of 3D points viewed by N cameras with matrices formula_1. Define formula_2 be the homogeneous coordinates of the projection of the formula_3 point onto the formula_4 camera. The reconstruction problem can be changed to: given the group of pixel coordinates formula_5, find the corresponding set of camera matrices formula_6 and the scene structure formula_7 such that\n\nGenerally, without further restrictions, we will obtain a projective reconstruction. If formula_6 and formula_7 satisfy (1), formula_11 and formula_12 will satisfy (1) with any 4 × 4 nonsingular matrix T.\n\nA projective reconstruction can be calculated by points correspondences only, without any a-priori information.\n\nAutocalibration or self-calibration is the classical approach, in which camera motion and parameters are recovered first, using rigidity, then structure is readily calculated. Two methods implementing this idea are presented as follows:\n\nWith a minimum of three displacements, we can obtain the internal parameters of the camera using a system of polynomial equations due to Kruppa, which are derived from a geometric interpretation of the rigidity constraint.\n\nThe matrix formula_13 is unknown in the Kruppa equations, named Kruppa coefficients matrix. With K and by the method of Cholesky factorization one can obtain the intrinsic parameters easily:\n\nRecently Hartley proposed a simpler form. Let formula_15 be written as formula_16, where\n\nThen the Kruppa equations are rewritten (the derivation can be found in )\n\nThis method is based on the use of rigidity constraint. Design a cost function, which considers the intrinsic parameters as arguments and the fundamental matrices as parameters. formula_17 is defined as the fundamental matrix, formula_18and formula_19 as intrinsic parameters matrices.\n\nRecently, new methods based on the concept of stratification have been proposed. Starting from a projective structure, which can be calculated from correspondences only, upgrade this projective reconstruction to a Euclidean reconstruction, by making use of all the available constraints. With this idea the problem can be stratified into different sections: according to the amount of constraints available, it can be analyzed at a different level, projective, affine or Euclidean.\n\nUsually, the world is perceived as a 3D Euclidean space. In some cases, it is not possible to use the full Euclidean structure of 3D space. The simplest being projective, then the affine geometry which forms the intermediate layers and finally Euclidean geometry. The concept of stratification is closely related to the series of transformations on geometric entities: in the projective stratum is a series of projective transformations (a homography), in the affine stratum is a series of affine transformations, and in Euclidean stratum is a series of Euclidean transformations.\n\nSuppose that a fixed scene is captured by two or more perspective cameras and the correspondences between visible points in different images are already given. However, in practice, the matching is an essential and extremely challenging issue in computer vision. Here, we suppose that formula_20 3D points formula_21 are observed by formula_22 cameras with projection matrices formula_23 Neither the positions of point nor the projection of camera are known. Only the projections formula_24 of the formula_4 point in the formula_3 image are known.\n\nSimple counting indicates we have formula_27 independent measurements and only formula_28 unknowns, so the problem is supposed to be soluble with enough points and images. The equations in homogeneous coordinates can be represented:\n\nSo we can apply a nonsingular 4 × 4 transformation \"H\" to projections formula_30→formula_31 and world points formula_32→formula_33. Hence, without further constraints, reconstruction is only an unknown projective deformation of the 3D world.\n\n\"See affine space for more detailed information about computing the location of the plane at infinity formula_34.\"\nThe simplest way is to exploit prior knowledge, for example the information that lines in the scene are parallel or that a point is the one thirds between two others.\n\nWe can also use prior constraints on the camera motion. By analyzing different images of the same point can obtain a line in the direction of motion. The intersection of several lines is the point at infinity in the motion direction, and one constraint on the affine structure.\n\nBy mapping the projective reconstruction to one that satisfies a group of redundant Euclidean constraints, we can find a projective transformation \"H\" in equation (2).The equations are highly nonlinear and a good initial guess for the structure is required. This can be obtained by assuming a linear projection - parallel projection, which also allows easy reconstruction by SVD decomposition.\n\nInevitably, measured data (i.e., image or world point positions) is noisy and the noise comes from many sources. To reduce the effect of noise, we usually use more equations than necessary and solve with least squares.\n\nFor example, in a typical null-space problem formulation Ax = 0 (like the DLT algorithm), the square of the residual ||Ax|| is being minimized with the least squares method.\n\nIn general, if ||Ax|| can be considered as a distance between the geometrical entities (points, lines, planes, etc.), then what is being minimized is a geometric error, otherwise (when the error lacks a good geometrical interpretation) it is called an algebraic error.\n\nTherefore, compared with algebraic error, we prefer to minimize a geometric error for the reasons listed:\n\nAll the linear algorithms (DLT and others) we have seen so far minimize an algebraic error. Actually, there is no justification in minimizing an algebraic error apart from the ease of implementation, as it results in a linear problem. The minimization of a geometric error is often a non-linear problem, that admit only iterative solutions and requires a starting point.\n\nUsually, linear solution based on algebraic residuals serves as a starting point for a non-linear minimization of a geometric cost function, which provides the solution a final “polish”.\n\nThe 2-D imaging has problems of anatomy overlapping with each other and don’t disclose the abnormalities. The 3-D imaging can be used for both diagnostic and therapeutic purposes.\n\n3-D models are used for planning the operation, morphometric studies and has more reliability in orthopedics.\nTo reconstruct 3-D images from 2-D images taken by a camera at multiple angles. Medical imaging techniques like CT scanning and MRI are expensive, and although CT scans are accurate, they can induce high radiation doses which is a risk for patients with certain diseases. Methods based on MRI are not accurate. Since we are exposed to powerful magnetic fields during an MRI scan, this method is not suitable for patients with ferromagnetic metallic implants. Both the methods can be done only when in lying position where the global structure of the bone changes. So, we discuss the following methods which can be performed while standing and require low radiation dose.\n\nThough these techniques are 3-D imaging, the region of interest is restricted to a slice; data is acquired to form a time sequence. \n\nThis method is simple and implemented by identifying the points manually in multi-view radiographs. The first step is to extract the corresponding points in two x-ray images and second step is the 3D reconstruction with algorithms like Discrete Linear Transform. Using DLT, the reconstruction is done only where there are SCPs. By increasing the number of points, the results improve but it is time consuming. This method has low accuracy because of low reproducibility and time consumption. This method is dependent on the skill of the operator. This method is not suitable for bony structures with continuous shape. This method is generally used as an initial solution for other methods.\n\nThis method uses X-ray images for 3D Reconstruction and to develop 3D models with low dose radiations in weight bearing positions.\n\nIn NSCC algorithm, the preliminary step is calculation of an initial solution. Firstly anatomical regions from the generic object are defined. Secondly, manual 2D contours identification on the radiographs is performed. From each radiograph 2D contours are generated using the 3D initial solution object. 3D contours of the initial object surface are projected onto their associated radiograph. The 2D association performed between these 2 set points is based on point-to-point distances and contours derivations developing a correspondence between the 2D contours and the 3D contours. Next step is optimization of the initial solution. Lastly deformation of the optimized solution is done by applying Kriging algorithm to the optimized solution. Finally, by iterating the final step until the distance between two set points is superior to a given precision value the reconstructed object is obtained.\n\nThe advantage of this method is it can be used for bony structures with continuous shape and it also reduced human intervention but they are time consuming.\n\nSurface Rendering technique visualizes a 3D object as a set of surfaces called iso-surfaces. Each surface has points with the same intensity (called iso-value). It is used when we want to see the separated structures e.g. skull from slices of head, blood vessel system from slices of body etc. This technique is used mostly for high contrast data. Two main methods for reconstructing are:\n\n\nOther proposed or developed techniques include Statistical Shape Model Based Methods, Parametric Methods, Hybrid methods.\n\n\n\n", "id": "34668189", "title": "3D reconstruction from multiple images"}
{"url": "https://en.wikipedia.org/wiki?curid=30729045", "text": "International Conference on Autonomous Agents and Multiagent Systems\n\nThe International Conference on Autonomous Agents and Multi-Agent Systems or AAMAS is the leading scientific conference for research in the areas of artificial intelligence, autonomous agents, and multiagent systems. It is annually organized by a non-profit organization called the International Foundation for Autonomous Agents and Multiagent Systems (IFAAMAS).\n\nThe AAMAS conference is a merger of three major international conferences/workshops, namely International Conference on Autonomous Agents (AGENTS), International Conference on Multi-Agent Systems (ICMAS), and International Workshop on Agent Theories, Architectures, and Languages (ATAL). As such, this highly respected joint conference provides a quality forum for discussing research in this area.\n\nThe next AAMAS conference (AAMAS 2018) will take place from July 10-15, 2018, in Stockholm, Sweden.\n\nA list of previous AAMAS conferences is as follows: \n\nBesides the main program that consists of a main track, an industry and applications track, and a couple of special area tracks, AAMAS also hosts over 20 workshops (e.g., AOSE, COIN, DALT, ProMAS, to mention a few) and many tutorials. There is also a demonstration session and a doctoral symposium. Finally, each year AAMAS features a bunch of awards, most notably the IFAAMAS Influential Paper Award. It publishes proceedings which are available online.\n\n", "id": "30729045", "title": "International Conference on Autonomous Agents and Multiagent Systems"}
{"url": "https://en.wikipedia.org/wiki?curid=35494317", "text": "Sensorium Project\n\nThe \"Sensorium Project\" was initiated by ROBOTmaker to broaden the IRCF360 sensor's audience beyond its typical robot sensor usage.\n\nTo demonstrate the BETA functionality, ROBOTmaker uses opensource Java based Integrated Development Environments (IDE) such as Arduino and Processing (programming language) and has released all the prototyping sketches on their website for advanced developers to expand the functionality and write their own Java interface apps. They also provide a low cost self-assembly PCB kit to allow developers to make a sensor and to reprogramme the embedded micro-controller with own firmware if desired.\n\nThe sensor is based on ROBOTmaker's Infrared Control Freak 360 (IRCF360) which is a 360 degree proximity sensor and a motion sensing devices, developed by ROBOTmaker. Their goal is to provide low costs configurable sensors & solutions for use within technical projects such as maker and Maker Faire type projects, micro robotic, kinetic art / art, crafts, engineering, science, Do-It-Yourself (DIY) mindset and alternative music type of projects.\n\nDuring the development phase, it became apparent that the unique features of the 360 sensor also enabled it to be used as an independent 3D motion sensor for use within many other 3D computer applications. It is connected to a PC via the USB port. The device can be configured as a joystick, keyboard, mouse or midi device, for example in 3D applications, gaming applications and as an alternative Human interface device HID for people with disabilities - enabling a wide range of users to control and interact with the PC/MAC/Tablet/SmartPhone without the need to touch a game controller or screen, using a more natural user interface through finger gestures. It works also well together with Windows7 and the voice recognition or together with other HID devices to reduce non-value added mouse movements.\n\nExamples of use are:\nA project named Sensorium Project was started aimed at broadening the Sensors audience beyond its typical robot sensor usage. To Demonstrate the BETA functionality, ROBOTmaker uses opensource Java based Integrated Development Environments (IDE) such as Arduino and Processing (programming language) and has released all the prototyping sketches on their website for advanced developers to expand the functionality and write their own Java interface apps. They also provide a low cost self-assembly PCB kit to allow developers to make a sensor and to reprogrammed the embedded micro-controller with own firmware if desired.,\n\nMany other forms of 3D mice and motion sensors have been developed where a 'physical' interaction is required. The mice have also been known as bats, flying mice, or wands, these devices generally function through ultrasound and provide at least three degrees of freedom. Probably the best known example would be 3Dconnexion/Logitech's SpaceMouse from the early 1990s.\n\nIn the late 1990s Kantek introduced the 3D RingMouse. This wireless mouse was worn on a ring around a finger, which enabled the thumb to access three buttons. The mouse was tracked in three dimensions by a base station. Despite a certain appeal, it was finally discontinued because it did not provide sufficient resolution.\n\nA mouse-related controller called the SpaceBall has a ball placed above the work surface that can easily be gripped. With spring-loaded centering, it sends both translational as well as angular displacements on all six axes, in both directions for each.\n\nIn November 2010 a German Company called Axsotic introduced a new concept of 3D mouse called 3D Spheric Mouse. This new concept of a true six degree-of-freedom input device uses a ball to rotate in 3 axes without any limitations.\n\nOther \"touch-less\" type of motion sensors are:\n\n\n\n", "id": "35494317", "title": "Sensorium Project"}
{"url": "https://en.wikipedia.org/wiki?curid=35456221", "text": "IRCF360\n\nInfrared Control Freak 360 (IRCF360) is a 360 degree proximity sensor and a motion sensing devices, developed by ROBOTmaker. The sensor is in BETA developers release as a low cost (software configurable) sensor for use within research, technical and hobby projects such as Maker Faire type projects, Microbotics, Kinetic art / art, crafts, engineering, UAV, Science, Technology and alternative music type of projects.\n\nThe 360 degree sensor was originally designed as a short range micro robot proximity sensor and mainly intended for Swarm robotics, Ant robotics, Swarm intelligence, autonomous Qaudcopter, Drone, UAV, multi-robot simulations e.g. Jasmine Project\nwhere 360 proximity sensing is required to avoid collision with other robots and for simple IR inter-robot communications. Other uses have since been identified. Stacked 360 degree sensors gives a spherical \"bubble\" of proximity sensing required for 3D space sensing, such as with Unmanned aerial vehicle.\n\nTo overcome certain limitation with Infra-red (IR) proximity sensing (e.g. detection of dark surfaces) the sensing module includes ambient light sensing and basic tactile sensing functionality during forward movement sensing/probing providing photovore and photophobe robot swarm behaviours and characteristics.\n\nCertain features of the 360 sensor enabled it also to be used as an independent 3D motion sensor for use within other computer applications where non-tactile input is required (i.e. compared to a mouse, joystick, keyboard which require physical contact). It is connected to a PC via the USB port. The device can be configured as a joystick, keyboard, mouse or midi device, for example where on-line 3D gaming applications require and as an alternative Human interface device (HID) for people with disabilities - enabling a wider range of users to control and interact with a PC/MAC/Tablet/SmartPhone using a more natural user interface through finger and hand gestures. It can be used together with the Windows7 voice recognition or with other HID devices to switch functionality and thereby reducing non-value added mouse movements.\n\nSome examples of use are:\n\nA project named Sensorium Project was started aimed at broadening the Sensors audience beyond its typical robot sensor usage. To Demonstrate the sensor's functionality, opensource Java based Integrated Development Environments (IDE) are used, such as Arduino and Processing (programming language). The java programs / sketches have all been released to public domain for advanced developers to expand the functionality and write their own Java interface apps.\n\nMany other forms of 3D mice and motion sensors have been developed where a 'physical' interaction is required. The mice have also been known as bats, flying mice, or wands, these devices generally function through ultrasound and provide at least three degrees of freedom. Probably the best known example would be 3Dconnexion/Logitech's SpaceMouse from the early 1990s.\n\nIn the late 1990s Kantek introduced the 3D RingMouse. This wireless mouse was worn on a ring around a finger, which enabled the thumb to access three buttons. The mouse was tracked in three dimensions by a base station. Despite a certain appeal, it was finally discontinued because it did not provide sufficient resolution.\n\nA mouse-related controller called the SpaceBall has a ball placed above the work surface that can easily be gripped. With spring-loaded centering, it sends both translational as well as angular displacements on all six axes, in both directions for each.\n\nIn November 2010 a German Company called Axsotic introduced a new concept of 3D mouse called 3D Spheric Mouse. This new concept of a true six degree-of-freedom input device uses a ball to rotate in 3 axes without any limitations.\n\nUnmanned Aerial Vehicles (UAVs) developments:\n\nOther \"touch-less\" type of motion sensors are:\n\n\n", "id": "35456221", "title": "IRCF360"}
{"url": "https://en.wikipedia.org/wiki?curid=8205445", "text": "Colloquis\n\nColloquis, previously known as ActiveBuddy and Conversagent, was a company that created conversation-based interactive agents originally distributed via instant messaging platforms. The company had offices in New York, NY and Sunnyvale, CA.\n\nFounded in 2000, the company was the brainchild of Robert Hoffer, Timothy Kay and Peter Levitan. The idea for interactive agents (also known as Internet bots) came from the team's vision to add functionality to increasingly popular instant messaging services. The original implementation took shape as a word-based adventure game but quickly grew to include a wide range of database applications including access to news, weather, stock information, movie times, yellow pages listings, and detailed sports data, as well as a variety of tools (calculators, translator, etc.). These various applications were bundled into a single entity and launched as SmarterChild in 2001. SmarterChild acted as a showcase for the quick data access and possibilities for fun conversation that the company planned to turn into customized, niche specific products. \n\nThe rapid success of SmarterChild led to targeted promotional products for Radiohead, Austin Powers, The Sporting News, and others. ActiveBuddy sought to strengthen its hold on the interactive agent market for the future by filing for, and receiving, a controversial patent on their creation in 2002. The company also released the BuddyScript SDK, a free developer kit that allow programmers to design and launch their own interactive agents using ActiveBuddy’s proprietary scripting language, in 2002. Ultimately, however, the decline in ad spending in 2001 and 2002 led to a shift in corporate strategy towards business focused Automated Service Agents, building products for clients including Cingular, Comcast and Cox Communications. The company subsequently changed its name from ActiveBuddy to Conversagent in 2003, and then again to Colloquis in 2006. Colloquis was later purchased by Microsoft in October 2006.\n\nMost of the ActiveBuddy, Colloquis and Microsoft team that launched and nurtured SmarterChild is now working on AB2.\n", "id": "8205445", "title": "Colloquis"}
{"url": "https://en.wikipedia.org/wiki?curid=35685954", "text": "Generalized distributive law\n\nThe generalized distributive law (GDL) is a generalization of the distributive property which gives rise to a general message passing algorithm. It is a synthesis of the work of many authors in the information theory, digital communications, signal processing, statistics, and artificial intelligence communities. The law and algorithm were introduced in a semi-tutorial by Srinivas M. Aji and Robert J. McEliece with the same title.\n\n\"\"The distributive law in mathematics is the law relating the operations of multiplication and addition, stated symbolically, formula_1; that is, the monomial factor formula_2 is distributed, or separately applied, to each term of the binomial factor formula_3, resulting in the product formula_4\"\" - Britannica\n\nAs it can be observed from the definition, application of distributive law to an arithmetic expression reduces the number of operations in it. In the previous example the total number of operations reduced from three (two multiplications and an addition in formula_4) to two (one multiplication and one addition in formula_6). Generalization of distributive law leads to a large family of fast algorithms. This includes the FFT and Viterbi algorithm.\n\nThis is explained in a more formal way in the example below:\n\nformula_7 where formula_8 and formula_9 are real-valued functions, formula_10 and formula_11 (say)\n\nHere we are \"marginalizing out\" the independent variables (formula_12, formula_13, and formula_14) to obtain the result. When we are calculating the computational complexity, we can see that for each formula_15 pairs of formula_16, there are formula_17 terms due to the triplet formula_18 which needs to take part in the evaluation of formula_19 with each step having one addition and one multiplication. Therefore, the total number of computations needed is formula_20. Hence the asymptotic complexity of the above function is formula_21.\n\nIf we apply the distributive law to the RHS of the equation, we get the following:\n\nThis implies that formula_23 can be described as a product formula_24 where formula_25 and formula_26\n\nNow, when we are calculating the computational complexity, we can see that there are formula_17 additions in formula_28 and formula_29 each and there are formula_30 multiplications when we are using the product formula_24 to evaluate formula_23. Therefore, the total number of computations needed is formula_33. Hence the asymptotic complexity of calculating formula_34 reduces to formula_35 from formula_36. This shows by an example that applying distributive law reduces the computational complexity which is one of the good features of a \"fast algorithm\".\n\nSome of the problems that used distributive law to solve can be grouped as follows\n\n1. Decoding algorithms<br>\nA GDL like algorithm was used by Gallager's for decoding low density parity-check codes. Based on Gallager's work Tanner introduced the Tanner graph and expressed Gallagers work in message passing form. The tanners graph also helped explain the Viterbi algorithm.\n\nIt is observed by Forney that Viterbi's maximum likelihood decoding of convolutional codes also used algorithms of GDL-like generality.\n\n2. Forward-backward algorithm<br>\nThe forward backward algorithm helped as an algorithm for tracking the states in the markov chain. And this also was used the algorithm of GDL like generality\n\n3. Artificial intelligence<br>\nThe notion of junction trees has been used to solve many problems in AI. Also the concept of bucket elimination used many of the concepts.\n\nMPF or marginalize a product function is a general computational problem which as special case includes many classical problems such as computation of discrete Hadamard transform, maximum likelihood decoding of a linear code over a memory-less channel, and matrix chain multiplication. The power of the GDL lies in the fact that it applies to situations in which additions and multiplications are generalized.\nA commutative semiring is a good framework for explaining this behavior. It is defined over a set formula_37 with operators \"formula_38\" and \"formula_39\" where formula_40 and formula_41 are a commutative monoids and the distributive law holds.\n\nLet formula_42 be variables such that formula_43 where formula_44 is a finite set and formula_45. Here formula_46. If formula_47 and formula_48, let\nformula_49,\nformula_50, \nformula_51, \nformula_52, and\nformula_53\n\nLet formula_54 where formula_55. Suppose a function is defined as formula_56, where formula_57 is a commutative semiring. Also, formula_58 are named the \"local domains\" and formula_59 as the \"local kernels\".\n\nNow the global kernel formula_60 is defined as :\nformula_61\n\n\"Definition of MPF problem\": For one or more indices formula_62, compute a table of the values of formula_63-\"marginalization\" of the global kernel formula_64, which is the function formula_65 defined as formula_66\n\nHere formula_67 is the complement of formula_63 with respect to formula_69 and the formula_70 is called the formula_71 \"objective function\", or the \"objective function\" at formula_72. It can observed that the computation of the formula_71 objective function in the obvious way needs formula_74 operations. This is because there are formula_75 additions and formula_76 multiplications needed in the computation of the formula_77 objective function. The GDL algorithm which is explained in the next section can reduce this computational complexity.\n\nThe following is an example of the MPF problem. \nLet formula_78 and formula_79 be variables such that formula_80 and formula_81. Here formula_82 and formula_83. The given functions using these variables are formula_84 and formula_85 and we need to calculate formula_86 and formula_87 defined as:\n\nHere local domains and local kernels are defined as follows: \nwhere formula_90 is the formula_91 objective function and formula_87 is the formula_93 objective function.\n\nLet us consider another example where formula_94 and formula_95 is a real valued function. Now, we shall consider the MPF problem where the commutative semiring is defined as the set of real numbers with ordinary addition and multiplication and the local domains and local kernels are defined as follows:\n\nNow since the global kernel is defined as the product of the local kernels, it is\n\nand the objective function at the local domain formula_97 is\n\nThis is the Hadamard transform of the function formula_8. Hence we can see that the computation of Hadamard transform is a special case of the MPF problem. More examples can be demonstrated to prove that the MPF problem forms special cases of many classical problem as explained above whose details can be found at\n\nIf one can find a relationship among the elements of a given set formula_100, then one can solve the MPF problem basing on the notion of belief propagation which is a special use of \"message passing\" technique. The required relationship is that the given set of local domains can be organised into a junction tree. In other words, we create a graph theoretic tree with the elements of formula_100 as the vertices of the tree formula_102, such that for any two arbitrary vertices say formula_103 and formula_104 where formula_105 and there exists an edge between these two vertices, then the intersection of corresponding labels, viz formula_106, is a subset of the label on each vertex on the unique path from formula_103 to formula_104.\n\nFor example,\n\nExample 1: Consider the following nine local domains:\n\n\nFor the above given set of local domains, one can organize them into a junction tree as shown below:\n\nSimilarly If another set like the following is given\n\nExample 2: Consider the following four local domains:\n\n\nThen constructing the tree only with these local domains is not possible since this set of values has no common domains which can be placed between any two values of the above set. But however if add the two dummy domains as shown below then organizing the updated set into a junction tree would be possible and easy too.\n\n5.formula_122,formula_123\n6.formula_124,formula_123\n\nSimilarly for these set of domains, the junction tree looks like shown below:\nInput: A set of local domains.\nOutput: For the given set of domains, possible minimum number of operations that is required to solve the problem is computed. \nSo, if formula_103andformula_104 are connected by an edge in the junction tree, then a message from formula_103 to formula_104 is a set/table of values given by a function: formula_130:formula_131. To begin with all the functions i.e. for all combinations of formula_132and formula_133 in the given tree, formula_130 is defined to be identically formula_135 and when a particular message is update, it follows the equation given below.\n\nwhere formula_138 means that formula_139 is an adjacent vertex to formula_103 in tree.\n\nSimilarly each vertex has a state which is defined as a table containing the values from the function formula_141, Just like how messages initialize to 1 identically, state of formula_103 is defined to be local kernel formula_143, but whenever formula_144 gets updated, it follows the following equation:\n\nFor the given set of local domains as input, we find out if we can create a junction tree, either by using the set directly or by adding dummy domains to the set first and then creating the junction tree, if construction junction is not possible then algorithm output that there is no way to reduce the number of steps to compute the given equation problem, but once we have junction tree, algorithm will have to schedule messages and compute states, by doing these we can know where steps can be reduced, hence will be discusses this below.\n\nThere are two special cases we are going to talk about here namely \"Single Vertex Problem\" in which the objective function is computed at only one vertex formula_146 and the second one is \"All Vertices Problem\" where the goal is to compute the objective function at all vertices.\n\nLets begin with the single-vertex problem, GDL will start by directing each edge towards the targeted vertex formula_147. Here messages are sent only in the direction towards the targeted vertex. Note that all the directed messages are sent only once. The messages are started from the leaf nodes(where the degree is 1) go up towards the target vertex formula_147. The message travels from the leaves to its parents and then from there to their parents and so on until it reaches the target vertex formula_147. The target vertex formula_147 will compute its state only when it receives all messages from all its neighbors. Once we have the state, We have got the answer and hence the algorithm terminates.\n\nFor Example, Lets consider a junction tree constructed from the set of local domains given above i.e. the set from example 1, Now the Scheduling table for these domains is (where the target vertex is formula_151).\n\nformula_152\nformula_153\nformula_154\nformula_155\nformula_156\nformula_157\nformula_158\nformula_159\nformula_160\nformula_161\n\nThus the complexity for Single Vertex GDL can be shown as\n\nformula_162 arithmetic operations\nWhere (Note: The explanation for the above equation is explained later in the article )\nformula_163 is the label of formula_164.\nformula_165 is the degree of formula_164 (i.e. number of vertices adjacent to v).\n\nTo solve the All-Vertices problem, we can schedule GDL in several ways, some of them are parallel implementation where in each round, every state is updated and every message is computed and transmitted at the same time. In this type of implementation the states and messages will stabilizes after number of rounds that is at most equal to the diameter of the tree. At this point all the all states of the vertices will be equal to the desired objective function.\n\nAnother way to schedule GDL for this problem is serial implementation where its similar to the Single vertex problem except that we don't stop the algorithm until all the vertices of a required set have not got all the messages from all their neighbors and have compute their state. \nThus the number of arithmetic this implementation requires is at most formula_167 arithmetic operations.\n\nThe key to constructing a junction tree lies in the local domain graph formula_168, which is a weighted complete graph with formula_169 vertices formula_170 i.e. one for each local domain, having the weight of the edge formula_171 defined by\nformula_172.\nif formula_173, then we say formula_174 is contained informula_175. Denoted by formula_176 (the weight of a maximal-weight spanning tree of formula_168), which is defined by\n\nwhere \"n\" is the number of elements in that set. For more clarity and details, please refer to these.\n\nLet formula_179 be a junction tree with vertex set formula_180 and edge set formula_181. In this algorithm, the messages are sent in both the direction on any edge, so we can say/regard the edge set E as set of ordered pairs of vertices. For example, from Figure 1 formula_181 can be defined as follows\n\nNOTE:formula_184 above gives you all the possible directions that a message can travel in the tree.\n\nThe schedule for the GDL is defined as a finite sequence of subsets offormula_184. Which is generally represented by \nformula_186{formula_187}, Where formula_188 is the set of messages updated during the formula_189 round of running the algorithm.\n\nHaving defined/seen some notations, we will see want the theorem says,\nWhen we are given a schedule formula_190, the corresponding message trellis as a finite directed graph with Vertex set of formula_191, in which a typical element is denoted by formula_192 for formula_193, Then after completion of the message passing, state at vertex formula_104 will be the formula_195 objective defined in\n\nand <nowiki>iff</nowiki> there is a path from formula_197 to formula_198\n\nHere we try to explain the complexity of solving the MPF problem in terms of the number of mathematical operations required for the calculation. i.e. We compare the number of operations required when calculated using the normal method (Here by normal method we mean by methods that do not use message passing or junction trees in short methods that do not use the concepts of GDL)and the number of operations using the generalized distributive law.\n\nExample: Let us consider the simplest case where we need to compute the following expression formula_199.\n\nTo evaluate this expression naively requires two multiplications and one addition. The expression when expressed using the distributive law can be written as formula_200 a simple optimization that reduces the number of operations to one addition and one multiplication.\n\nSimilar to the above explained example we will be expressing the equations in different forms to perform as few operation as possible by applying the GDL.\n\nAs explained in the previous sections we solve the problem by using the concept of the junction trees. The optimization obtained by the use of these trees is comparable to the optimization obtained by solving a semi group problem on trees. For example, to find the minimum of a group of numbers we can observe that if we have a tree and the elements are all at the bottom of the tree, then we can compare the minimum of two items in parallel and the resultant minimum will be written to the parent. When this process is propagated up the tree the minimum of the group of elements will be found at the root.\n\nThe following is the complexity for solving the junction tree using message passing\n\nWe rewrite the formula used earlier to the following form. This is the eqn for a message to be sent from vertex \"v\" to \"w\"\n\nSimilarly we rewrite the equation for calculating the state of vertex v as follows\n\nWe first will analyze for the single-vertex problem and assume the target vertex is formula_147 and hence we have one edge from formula_164 to formula_205. \nSuppose we have an edge formula_206 we calculate the message using the message equation. To calculate formula_207 requires\n\nadditions and\n\nmultiplications.\n\nBut there will be many possibilities for formula_212 hence \nformula_213 possibilities for formula_214.\nThus the entire message will need\n\nadditions and\n\nmultiplications\n\nThe total number of arithmetic operations required to send a message towards formula_217along the edges of tree will be\n\nadditions and\n\nmultiplications.\n\nOnce all the messages have been transmitted the algorithm terminates with the computation of state at formula_147 The state computation requires formula_221 more multiplications.\nThus number of calculations required to calculate the state is given as below\n\nadditions and\n\nmultiplications\n\nThus the grand total of the number of calculations is\n\nwhere formula_226 is an edge and its size is defined by formula_227\n\nThe formula above gives us the upper bound.\n\nIf we define the complexity of the edge formula_226 as\n\nTherefore, formula_225 can be written as\n\nWe now calculate the edge complexity for the problem defined in Figure 1 as follows\n\nThe total complexity will be formula_240 which is considerably low compared to the direct method. (Here by direct method we mean by methods that do not use message passing. The time taken using the direct method will be the equivalent to calculating message at each node and time to calculate the state of each of the nodes.)\n\nNow we consider the all-vertex problem where the message will have to be sent in both the directions and state must be computed at both the vertexes. This would take formula_241 but by precomputing we can reduce the number of multiplications to formula_242. Here formula_13 is the degree of the vertex. Ex : If there is a set formula_244 with formula_245 numbers. It is possible to compute all the d products of formula_246 of the formula_247 with at most formula_242 multiplications rather than the obvious formula_249. \nWe do this by precomputing the quantities \nformula_250 and formula_251 this takes formula_252 multiplications. Then if formula_253 denotes the product of all formula_254 except for formula_255 we have formula_256 and so on will need another formula_257 multiplications making the total formula_258\n\nThere is not much we can do when it comes to the construction of the junction tree except that we may have many maximal weight spanning tree and we should choose the spanning tree with the least formula_259 and sometimes this might mean adding a local domain to lower the junction tree complexity.\n\nIt may seem that GDL is correct only when the local domains can be expressed as a junction tree. But even in cases where there are cycles and a number of iterations the messages will approximately be equal to the objective function. The experiments on Gallager–Tanner–Wiberg algorithm for low density parity-check codes were supportive of this claim.\n", "id": "35685954", "title": "Generalized distributive law"}
{"url": "https://en.wikipedia.org/wiki?curid=35713574", "text": "S Voice\n\nS Voice is an intelligent personal assistant and knowledge navigator which is only available as a built-in application for the Samsung Galaxy S III, S III Mini (including NFC Variant), S4, S4 Mini, S4 Active, S5, S5 Mini, S II Plus, Note II, Note 3, Note 4, Note 10.1, Note 8.0, Stellar, Mega, Grand, Avant, Core, Ace 3, Tab 3 7.0, Tab 3 8.0, Tab 3 10.1, Galaxy Camera, and other 2013 or later Samsung Android devices. The application uses a natural language user interface to answer questions, make recommendations, and perform actions by delegating requests to a set of Web services. It is based on the Vlingo personal assistant.\n\nThe Galaxy S5 and later Samsung Android devices, S Voice runs on Nuance instead of Vlingo.\n\nSome of the capabilities of S Voice include making appointments, opening apps, setting alarms, updating social network websites such as Facebook or Twitter and navigation. S Voice also offers multitasking as well as automatic activation features, for example, when the car engine is started.\n\nIn a disclaimer that pops up on first opening S Voice, Samsung states that the app is provided by a third party which it does not name.\n\nIn the Galaxy S8 and S8+, Bixby was announced to be a major update and replacement to S Voice from the prior phones.\n\n\n\n\n", "id": "35713574", "title": "S Voice"}
{"url": "https://en.wikipedia.org/wiki?curid=11848927", "text": "Personoid\n\nPersonoid is the concept coined by Stanislaw Lem (1971), a Polish science-fiction writer. His personoids are an abstraction of functions of human mind and they live in computers; they do not need any human-like physical body. \n\nIn cognitive and software modeling, \"personoid\" is a research approach to the development of intelligent autonomous agents.\nIn frame of the IPK (Information, Preferences, Knowledge) architecture, it is a framework of abstract intelligent agent with a cognitive and structural intelligence. It can be seen as an essence of high intelligent entities.\n\nFrom the philosophical and systemics perspectives, personoid societies can also be seen as the carriers of a culture. According to N. Gessler, the personoids study can be a base for the research on \"artificial culture\" and \"culture evolution\". \n\n\n", "id": "11848927", "title": "Personoid"}
{"url": "https://en.wikipedia.org/wiki?curid=18848971", "text": "Puckstering\n\nPuckstering is a term used in gaming that refers to an operator controlling a group of AI entities. For example, a gamer playing the role of an army platoon sergeant could control a platoon of soldiers. He could command them to attack an enemy, which they would do autonomously. This is referred to as \"puckstering.\"\n", "id": "18848971", "title": "Puckstering"}
{"url": "https://en.wikipedia.org/wiki?curid=36298401", "text": "DAYDREAMER\n\nDAYDREAMER is a goal-based agent and cognitive architecture developed at University of California, Los Angeles by Erik Mueller. It models the human stream of thought and its triggering and direction by emotions, as in human daydreaming. The architecture is implemented as 12,000 lines of Lisp code.\n\nDAYDREAMER was begun by Erik Mueller in 1983 while he was studying under Michael G. Dyer in the UCLA Computer Science Department. It was completed in 1987 and was followed by the ThoughtTreasure program, which was started in 1993.\n\n\n\n", "id": "36298401", "title": "DAYDREAMER"}
{"url": "https://en.wikipedia.org/wiki?curid=36347200", "text": "Simulated consciousness in fiction\n\nSimulated consciousness, synthetic consciousness, etc. is a theme of a number of works in science fiction. The theme is one step beyond the concept of the \"brain in a vat\"/\"simulated reality\" in that not only the perceived reality but the brain and its consciousness are simulations themselves. On the other hand, it is also the extension of the concept of artificial consciousness in that not only the intelligence, but the \"reality\" it perceives and operates within is artificial as well.\n\nStanislaw Lem's professor Corcoran (met by Ijon Tichy during his interstellar travels, first published by Lem in 1961) simulated conscious agents (personoids) to actually test the viability of the \"simulation hypothesis\" of the reality, i.e., the idea of solipsism.\n\nIn the 1954 story \"The Tunnel under the World\" by Frederik Pohl, a whole city was simulated in order to run tests of the efficiency of advertising campaigns, and the plot evolves from the point when one \"simulacrum\" suddenly notices that every day is June 15. Pohl's idea was elaborated in \"Simulacron-3\" (1964) by Daniel F. Galouye (alternative title: \"Counterfeit World\"), which tells the story of a virtual city developed as a computer simulation for market research purposes. In this city the simulated inhabitants possess consciousness; all but one of the inhabitants are unaware of the true nature of their world.\n\nFurthermore, various novels by Greg Egan such as \"Permutation City\" (1994), \"Diaspora\" (1997) and \"Schild's Ladder\" (2002) explore the concept of simulated consciousness.\n\nCharacters with artificial consciousness (or at least with personalities that imply they have consciousness), from works of fiction:\n\n", "id": "36347200", "title": "Simulated consciousness in fiction"}
{"url": "https://en.wikipedia.org/wiki?curid=31103500", "text": "Reasoning system\n\nIn information technology a reasoning system is a software system that generates conclusions from available knowledge using logical techniques such as deduction and induction. Reasoning systems play an important role in the implementation of artificial intelligence and knowledge-based systems.\n\nBy the everyday usage definition of the phrase, all computer systems are reasoning systems in that they all automate some type of logic or decision. In typical use in the Information Technology field however, the phrase is usually reserved for systems that perform more complex kinds of reasoning. For example, not for systems that do fairly straightforward types of reasoning such as calculating a sales tax or customer discount but making logical inferences about a medical diagnosis or mathematical theorem. Reasoning systems come in two modes: interactive and batch processing. Interactive systems interface with the user to ask clarifying questions or otherwise allow the user to guide the reasoning process. Batch systems take in all the available information at once and generate the best answer possible without user feedback or guidance.\n\nReasoning systems have a wide field of application that includes scheduling, business rule processing, problem solving, complex event processing, intrusion detection, predictive analytics, robotics, computer vision, and natural language processing.\n\nThe first reasoning systems were theorem provers, systems that represent axioms and statements in First Order Logic and then use rules of logic such as modus ponens to infer new statements. Another early type of reasoning system were general problem solvers. These were systems such as the General Problem Solver designed by Newell and Simon. General problem solvers attempted to provide a generic planning engine that could represent and solve structured problems. They worked by decomposing problems into smaller more manageable sub-problems, solving each sub-problem and assembling the partial answers into one final answer. Another example general problem solver was the SOAR family of systems.\n\nIn practice these theorem provers and general problem solvers were seldom useful for practical applications and required specialized users with knowledge of logic to utilize. The first practical application of automated reasoning were expert systems. Expert systems focused on much more well defined domains than general problem solving such as medical diagnosis or analyzing faults in an aircraft. Expert systems also focused on more limited implementations of logic. Rather than attempting to implement the full range of logical expressions they typically focused on modus-ponens implemented via IF-THEN rules. Focusing on a specific domain and allowing only a restricted subset of logic improved the performance of such systems so that they were practical for use in the real world and not merely as research demonstrations as most previous automated reasoning systems had been. The engine used for automated reasoning in expert systems were typically called inference engines. Those used for more general logical inferencing are typically called theorem provers.\n\nWith the rise in popularity of expert systems many new types of automated reasoning were applied to diverse problems in government and industry. Some such as case-based reasoning were off shoots of expert systems research. Others such as constraint satisfaction algorithms were also influenced by fields such as decision technology and linear programming. Also, a completely different approach, one not based on symbolic reasoning but on a connectionist model has also been extremely productive. This latter type of automated reasoning is especially well suited to pattern matching and signal detection types of problems such as text searching and face matching.\n\nThe term reasoning system can be used to apply to just about any kind of sophisticated decision system as illustrated by the specific areas described below. However, the most common use of the term reasoning system implies the computer representation of logic. Various implementations demonstrate significant variation in terms of systems of logic and formality. Most reasoning systems implement variations of propositional and symbolic (predicate) logic. These variations may be mathematically precise representations of formal logic systems (e.g., FOL), or extended and hybrid versions of those systems (e.g., Courteous logic). Reasoning systems may explicitly implement additional logic types (e.g., modal, deontic, temporal logics). However, many reasoning systems implement imprecise and semi-formal approximations to recognised logic systems. These systems typically support a variety of procedural and semi-declarative techniques in order to model different reasoning strategies. They emphasise pragmatism over formality and may depend on custom extensions and attachments in order to solve real-world problems.\n\nMany reasoning systems employ deductive reasoning to draw inferences from available knowledge. These inference engines support forward reasoning or backward reasoning to infer conclusions via modus ponens. The recursive reasoning methods they employ are termed ‘forward chaining’ and ‘backward chaining’, respectively. Although reasoning systems widely support deductive inference, some systems employ abductive, inductive, defeasible and other types of reasoning. Heuristics may also be employed to determine acceptable solutions to intractable problems.\n\nReasoning systems may employ the closed world assumption (CWA) or open world assumption (OWA). The OWA is often associated with ontological knowledge representation and the Semantic Web. Different systems exhibit a variety of approaches to negation. As well as logical or bitwise complement, systems may support existential forms of strong and weak negation including negation-as-failure and ‘inflationary’ negation (negation of non-ground atoms). Different reasoning systems may support monotonic or non-monotonic reasoning, stratification and other logical techniques.\n\nMany reasoning systems provide capabilities for reasoning under uncertainty. This is important when building situated reasoning agents which must deal with uncertain representations of the world. There are several common approaches to handling uncertainty. These include the use of certainty factors, probabilistic methods such as Bayesian inference or Dempster–Shafer theory, multi-valued (‘fuzzy’) logic and various connectionist approaches.\n\nThis section provides a non-exhaustive and informal categorisation of common types of reasoning system. These categories are not absolute. They overlap to a significant degree and share a number of techniques, methods and algorithms.\n\nConstraint solvers solve constraint satisfaction problems (CSPs). They support constraint programming. A constraint is a condition which must be met by any valid solution to a problem. Constraints are defined declaratively and applied to variables within given domains. Constraint solvers use search, backtracking and constraint propagation techniques to find solutions and determine optimal solutions. They may employ forms of linear and nonlinear programming. They are often used to perform optimization within highly combinatorial problem spaces. For example, they may be used to calculate optimal scheduling, design efficient integrated circuits or maximise productivity in a manufacturing process.\n\nTheorem provers use automated reasoning techniques to determine proofs of mathematical theorems. They may also be used to verify existing proofs. In addition to academic use, typical applications of theorem provers include verification of the correctness of integrated circuits, software programs, engineering designs, etc.\n\nLogic programs (LPs) are software programs written using programming languages whose primitives and expressions provide direct representations of constructs drawn from mathematical logic. An example of a general-purpose logic programming language is Prolog. LPs represent the direct application of logic programming to solve problems. Logic programming is characterised by highly declarative approaches based on formal logic, and has wide application across many disciplines.\n\nRule engines represent conditional logic as discrete rules. Rule sets can be managed and applied separately to other functionality. They have wide applicability across many domains. Many rule engines implement reasoning capabilities. A common approach is to implement production systems to support forward or backward chaining. Each rule (‘production’) binds a conjunction of predicate clauses to a list of executable actions. At run-time, the rule engine matches productions against facts and executes (‘fires’) the associated action list for each match. If those actions remove or modify any facts, or assert new facts, the engine immediately re-computes the set of matches. Rule engines are widely used to model and apply business rules, to control decision-making in automated processes and to enforce business and technical policies.\n\nDeductive classifiers arose slightly later than rule-based systems and were a component of a new type of artificial intelligence knowledge representation tool known as frame languages. A frame language describes the problem domain as a set of classes, subclasses, and relations among the classes. It is similar to the object-oriented model. Unlike object-oriented models however, frame languages have a formal semantics based on first order logic. They utilize this semantics to provide input to the deductive classifier. The classifier in turn can analyze a given model (known as an ontology) and determine if the various relations described in the model are consistent. If the ontology is not consistent the classifier will highlight the declarations that are inconsistent. If the ontology is consistent the classifier can then do further reasoning and draw additional conclusions about the relations of the objects in the ontology. For example, it may determine that an object is actually a subclass or instance of additional classes as those described by the user. Classifiers are an important technology in analyzing the ontologies used to describe models in the Semantic web.\nMachine learning systems evolve their behavior over time based on experience. This may involve reasoning over observed events or example data provided for training purposes. For example, machine learning systems may use inductive reasoning to generate hypotheses for observed facts. Learning systems search for generalised rules or functions that yield results in line with observations and then use these generalisations to control future behavior.\n\nCase-based reasoning (CBR) systems provide solutions to problems by analysing similarities to other problems for which known solutions already exist. They use analogical reasoning to infer solutions based on case histories. CBR systems are commonly used in customer/technical support and call centre scenarios and have applications in industrial manufacture, agriculture, medicine, law and many other areas.\n\nA procedural reasoning system (PRS) uses reasoning techniques to select plans from a procedural knowledge base. Each plan represents a course of action for achievement of a given goal. The PRS implements a belief-desire-intention model by reasoning over facts (‘beliefs’) to select appropriate plans (‘intentions’) for given goals (‘desires’). Typical applications of PRS include management, monitoring and fault detection systems.\n", "id": "31103500", "title": "Reasoning system"}
{"url": "https://en.wikipedia.org/wiki?curid=36831006", "text": "Hindsight optimization\n\nHindsight optimisation (HOP) is a computer science technique used in artificial intelligence for analysis of actions which have stochastic results. HOP is used in combination with a deterministic planner. By creating sample results for each of the possible actions from the given state (i.e. determinising the actions), and using the deterministic planner to analyse those sample results, HOP allows an estimate of the actual action.\n", "id": "36831006", "title": "Hindsight optimization"}
{"url": "https://en.wikipedia.org/wiki?curid=37044578", "text": "Maluuba\n\nMaluuba is a Canadian technology company conducting research in artificial intelligence and language understanding. Founded in 2011, the company was acquired by Microsoft in 2017.\n\nIn late March 2016, the company demonstrated a machine reading system capable of answering arbitrary questions about J.K Rowling’s \"Harry Potter and the Philosopher’s Stone\". Maluuba's natural language understanding technology is used by several consumer electronic brands for over 50 million devices.\n\nMaluuba was founded by two undergraduate students from the University of Waterloo, Sam Pasupalak and Kaheer Suleman. Their initial proof of concept was a program that allowed users to search for flights using their voice.\n\nIn February 2012, the company secured $2 million in seed funding from Samsung Ventures.\n\nSince 2013, Maluuba has partnered with several companies in the smart phone, smart TV, automotive and IoT space.\n\nIn August 2015 Maluuba secured a $9 million of Series A investment from Nautilus Ventures and Emerllion Capital. Then in December 2015, Maluuba opened an R&D lab in Montreal, Quebec.\n\nBy 2016 the company employed more than fifty people, and had published fifteen peer-reviewed research papers focused on language understanding.\n\nOn January 13, 2017, Maluuba announced they had been acquired by Microsoft. In July 2017, according to the reports, Maluuba closed its Kitchener-Waterloo office and moved employees to its Montreal office.\n\nMaluuba's research centre opened in Montreal, Quebec in December 2015. The lab is advised by Yoshua Bengio (University of Montreal) and Richard Sutton (University of Alberta). The lab has published fifteen peer-reviewed papers discussing some of its recent research. The lab also partners with the University of Montreal MILA lab and McGill University.\n\nIn late 2016 the company released two natural language datasets: NewsQA, focused on comprehension and Frames, focused on Dialogue.\n\nMaluuba has achieved 80% accuracy on the machine reading benchmark MCTest, outperforming other word-matching approaches by 8%, and surpassed the previous benchmark set for deep learning techniques, the DSTC2, by 3%, bringing it to 83%.\n\nIn June 2016, the company demonstrated a program called EpiReader which outperformed Facebook and Google in machine comprehension tests. Several research teams were able to match Maluuba's results since the paper was released. EpiReader made use of two large datasets, the CNN/Daily Mail dataset released by Google DeepMind, comprising over 300,000 news articles; and the Children's Book Test, posted by Facebook Research, made up of 98 children’s books open sourced under Project Gutenberg.\n\nThe company has published research findings into dialogue systems which comprises natural language understanding, state tracking, and natural language generation. Maluuba published a research paper on policy manager (decision maker) where the system is rewarded for a correct decision. In 2016, Maluuba also freely released the Frames dataset, which is a large human-generated corpus of conversations.\n\nThe company conducts research into reinforcement learning in which intelligent agents are motivated to take actions within a set environment in order to maximize a reward. The research team has also published several papers on scalability.\n\nIn June 2017, the Maluuba team was the first to beat the game \"Ms. Pac-Man\" for the Atari 2600 system.\n\nNumerous applications for Maluuba's technology have been proposed in industry with several applications being commercialized.\n\nOne of the first applications of Maluuba's natural language technology has been the smartphone assistant. These systems allow users to speak to their phone and get direct results to their question (instead of merely seeing a sea of blue web links that point to possible answers to their question). The company raised $9M in 2015 to bring their voice assistant technology to automotive and IOT sectors. \n\nMaluuba also offers a Google Chrome extension, NewsQA, that uses deep learning algorithms to read through news articles in order to answer questions posed by the user. \n", "id": "37044578", "title": "Maluuba"}
{"url": "https://en.wikipedia.org/wiki?curid=5349640", "text": "Language/action perspective\n\nThe language/action perspective (LAP) \"takes language as the primary dimension of human cooperative activity,\" applied not just in person-to-person direct (face-to-face) interactions, but also in the design of systems mediated by information and communication technology. The perspective was developed in the joint authorship of \"Understanding Computers and Cognition\" by Fernando Flores and Terry Winograd in 1987.\n\nAs part of a reflection published in 2006, Terry Winograd describes the language-action perspective as resting on two key orienting principles:\n\n\"Language is action\" argues that speech isn't simply composed of assertions about the situation: utterances may also create a situation, such as, \"Let's go to the park.\" That utterance may be subject to interpretation but is not verifiable via the state of the world. This principle is closely linked to the ideas from phenomenology. Furthermore, language is not the transmission of information, which simply correspond to the state of the world. By creating a situation, language forms a consensual domain to further encourage more action through language. These [Speech act|speech acts] may often take the form of commitment to other actions.\n\nIn the design of information systems, the perspective is based upon the notion as proposed by Terry Winograd that information technology may be limited in its ability to improve human communication. \"Expert behavior requires an exquisite sensitivity to context and an ability to know what to commit to. Computing machines, which are purposely designed to process symbols independent of their context, have no hope of becoming experts.\". That sensitivity to context is thus more in the realm of the human than in that of the artificial.\n\nResearch on LAP was done in the Advanced Technology Group (ATG) at Apple Computer in the late 1980s. Winograd was invited to present the basic concepts in a seminar at Apple in the winter of 1988. Some Apple ATG researchers, notably Tom Pittard and Brad Hartfield, saw potential for enhancing the user experience of network based computer interactions if LAP was included in the mix of basic design considerations.\n\nResearch on the application of LAP to business process modelling was done in the System Modelling Research Group, Faculty of Computing, Engineering and Mathematical Sciences, University of the West of England in the early 2000s.\n\nInsights from related work have been applied over the past two decades. At the LAP 2004 - Conference, Kalle Lyytinen discussed the academic/theoretic success of LAP. Yet, these LAP successes have not found entry into the wider stream of applications. In a sense, LAP is now peripheral to computer science, however there may be a need for a deeper look at this viewpoint.\n\nLAP played a role in the second AI Winter. At the time, symbolic AI tried to represent intelligence using a growing knowledge base represented as facts in language. The LAP argued that language was not simply a correspondence with facts but instead depended upon the contextual domain and could not be rigidly defined. Even Winograd's SHRDLU, an exemplar of language understanding in AI, was incapable of broadening its understanding beyond the blocks world.\n\n\n\n", "id": "5349640", "title": "Language/action perspective"}
{"url": "https://en.wikipedia.org/wiki?curid=37339876", "text": "Pedagogical agent\n\nA pedagogical agent is a concept borrowed from computer science and artificial intelligence and applied to education, usually as part of an intelligent tutoring system (ITS). It is a simulated human-like interface between the learner and the content, in an educational environment. A pedagogical agent is designed to model the type of interactions between a student and another person. Mabanza and de Wet define it as \"as a character enacted by a computer that interacts with the user in a socially engaging manner\". A pedagogical agent can be assigned different roles in the learning environment, such as tutor or co-learner, depending on the desired purpose of the agent. \"A tutor agent plays the role of a teacher, while a co-learner agent plays the role of a learning companion\".\nThe history of Pedagogical Agents is closely aligned with the history of computer animation. As computer animation progressed, it was adopted by educators to enhance computerized learning by including a lifelike interface between the program and the learner. The first versions of a pedagogical agent were more cartoon than person, like Microsoft's Clippy which helped user's of Microsoft Office load and use the program's features in 1997. However, with developments in computer animation, pedagogical agents can now look lifelike. By 2006 there was a call to develop modular, reusable agents to decrease the time and expertise required to create a pedagogical agent. There was also a call in 2009 to enact agent standards. The standardization and re-usability of pedagogical agents is less of an issue since the decrease in cost and widespread availability of animation tools. Individualized pedagogical agents can be found across disciplines including medicine, math, law, language learning, automotive, and armed forces. They are used in applications directed to every age, from preschool to adult.\n\nDistributed cognition theory is the method in which cognition progresses in the context of collaboration with others. Pedagogical agents can be designed to assist the cognitive transfer to the learner, operating as artifacts or partners with collaborative role in learning. To support the performance of an action by the user, the pedagogical agent can act as a cognitive tool as long as the agent is equipped with the knowledge that the user lacks. The interactions between the user and the pedagogical agent can facilitate a social relationship. The pedagogical agent may fulfill the role of a working partner.\n\nSocio-cultural learning theory is how the user develops when they are involved in learning activities in which there is interaction with other agents. A pedagogical agent can: intervene when the user requests, provide support for tasks that the user cannot address, and potentially extend the learners cognitive reach. Interaction with the pedagogical agent may elicit a variety of emotions from the learner. The learner may become excited, confused, frustrated, and/or discouraged. These emotions affect the learners' motivation.\n\nIt has been suggested by researchers that pedagogical agents may take on different roles in the learning environment. Examples of these roles are: supplanting, scaffolding, coaching, testing, or demonstrating or modelling a procedure. A pedagogical agent as a tutor has not been demonstrated to add any benefit to an educational strategy in equivalent lessons with and without a pedagogical agent. According to Richard Mayer, there is some support in research for pedagogical agent increasing learning, but only as a presenter of social cues. A co-learner pedagogical agent is believed to increase the student's self-efficacy. By pointing out important features of instructional content, a pedagogical agent can fulfill the signaling function, which research on multimedia learning has shown to enhance learning. Research has demonstrated that human-human interaction may not be completely replaced by pedagogical agents, but learners may prefer the agents to non-agent multimedia systems. This finding is supported by social agency theory.\n\nThe appearance of a pedagogical agent can be manipulated to meet the learning requirements. The attractiveness of a pedagogical agent can enhance student's learning when the users were the opposite gender of the pedagogical agent. Male students prefer a sexy appearance of a female pedagogical agents and dislike the sexy appearance of male agents. Female students were not attracted by the sexy appearance of either male or female pedagogical agents.\n\n * AI: Artificial Intelligence Research at USC Information Science Institute\n", "id": "37339876", "title": "Pedagogical agent"}
{"url": "https://en.wikipedia.org/wiki?curid=37447771", "text": "Legal expert system\n\nA legal expert system is a domain-specific expert system that uses artificial intelligence to emulate the decision-making abilities of a human expert in the field of law. Legal expert systems employ a rule base or knowledge base and an inference engine to accumulate, reference and produce expert knowledge on specific subjects within the legal domain.\n\nIt has been suggested that legal expert systems could help to manage the rapid expansion of legal information and decisions that began to intensify in the late 1960s. Many of the first legal expert systems were created in the 1970s and 1980s.\n\nLawyers were originally identified as primary target users of legal expert systems. Potential motivations for this work included:\n\n\nSome early development work was oriented toward the creation of automated judges.\n\nLater work on legal expert systems has identified potential benefits to non-lawyers as a means to increase access to legal knowledge.\n\nLegal expert systems can also support administrative processes, facilitating decision making processes, automating rule-based analyses and exchanging information directly with citizen-users.\n\nRule-based expert systems rely on a model of deductive reasoning that utilizes \"if A, then B\" rules. In a rule-based legal expert system, information is represented in the form of deductive rules within the knowledge base.\n\nCase-based reasoning models, which store and manipulate examples or cases, hold the potential to emulate an analogical reasoning process thought to be well-suited for the legal domain. This model effectively draws on known experiences our outcomes for similar problems.\n\nA neural net relies on a computer model that mimics that structure of a human brain, and operates in a very similar way to the case-based reasoning model. This expert system model is capable of recognizing and classifying patterns within the realm of legal knowledge and dealing with imprecise inputs.\n\nFuzzy logic models attempt to create 'fuzzy' concepts or objects that can then be converted into quantitative terms or rules that are indexed and retrieved by the system. In the legal domain, fuzzy logic can be used for rule-based and case-based reasoning models.\n\nWhile some legal expert system architects have adopted a very practical approach, employing scientific modes of reasoning within a given set of rules or cases, others have opted for a broader philosophical approach inspired by jurisprudential reasoning modes emanating from established legal theoreticians. \n\nSome legal expert systems aim to arrive at a particular conclusion in law, while others are designed to predict a particular outcome. An example of a predictive system is one that predicts the outcome of judicial decisions, the value of a case, or the outcome of litigation.\n\nMany forms of legal expert systems have become widely used and accepted by both the legal community and the users of legal services.\n\nThe inherent complexity of law as a discipline raises immediate challenges for legal expert system knowledge engineers. Legal matters often involve interrelated facts and issues, which further compound the complexity.\n\nFactual uncertainty may also arise when there are disputed versions of factual representations that must be input into an expert system to begin the reasoning process.\n\nThe limitations of most computerized problem solving techniques inhibit the success of many expert systems in the legal domain. Expert systems typically rely on deductive reasoning models that have difficulty according degrees of weight to certain principles of law or importance to previously decided cases that may or may not influence a decision in an immediate case or context.\n\nExpert legal knowledge can be difficult to represent or formalize within the structure of an expert system. For knowledge engineers, challenges include:\n\nCreating a functioning expert system requires significant investments in software architecture, subject matter expertise and knowledge engineering. Faced with these challenges, many system architects restrict the domain in terms of subject matter and jurisdiction. The consequence of this approach is the creation of narrowly focused and geographically restricted legal expert systems that are difficult to justify on a cost-benefit basis.\n\nLegal expert systems may lead non-expert users to incorrect or inaccurate results and decisions. This problem could be compounded by the fact that users may rely heavily on the correctness or trustworthiness of results or decisions generated by these systems.\n\nASHSD-II is a hybrid legal expert system that blends rule-based and case-based reasoning models in the area of matrimonial property disputes under English law.\n\nCHIRON is a hybrid legal expert system that blends rule-based and case-based reasoning models to support tax planning activities under United States tax law and codes.\n\nJUDGE is a rule-based legal expert system that deals with sentencing in the criminal legal domain for offences relating to murder, assault and manslaughter.\n\nThe Latent Damage Project is a rule-based legal expert system that deals with limitation periods under the (UK) \"Latent Damage Act 1986\" in relation to the domains of tort, contract and product liability law.\n\nSplit-Up is a rule-based legal expert system that assists in the division of marital assets according to the (Australia) \"Family Law Act (1975)\".\n\nSHYSTER is a case-based legal expert system that can also function as a hybrid through its ability to link with rule-based models. It was designed to accommodate multiple legal domains, including aspects of Australian copyright law, contract law, personal property and administrative law.\n\nTAXMAN is a rule-based system that could perform a basic form of legal reasoning by classifying cases under a particular category of statutory rules in the area of law concerning corporate reorganization.\n\nThere may be a lack of consensus over what distinguishes a legal expert system from a knowledge-based system (also called an intelligent knowledge-based system). While legal expert systems are held to function at the level of a human legal expert, knowledge-based systems may depend on the ongoing assistance of a human expert. True legal expert systems typically focus on a narrow domain of expertise as opposed to a wider and less specific domain as in the case of most knowledge-based systems.\n\nLegal expert systems represent potentially disruptive technologies for the traditional, bespoke delivery of legal services. Accordingly, established legal practitioners may consider them a threat to historical business practices.\n\nArguments have been made that a failure to take into consideration various theoretical approaches to legal decision making will produce expert systems that fail to reflect the true nature of decision making. Meanwhile, some legal expert system architects contend that because many lawyers have proficient legal reasoning skills without a sound base in legal theory, the same should hold true for legal expert systems.\n\nBecause legal expert systems apply precision and scientific rigor to the act of legal decision-making, they may be seen as a challenge to the more disorganized and less precise dynamics of traditional jurisprudential modes of legal reasoning. Some commentators also contend that the true nature of legal practice does not necessarily depend on analyses of legal rules or principles; decisions are based instead on an expectation of what a human adjudicator would decide for a given case.\n\nSince 2013, there have been significant developments in legal expert systems. Professor Tanina Rostain of Georgetown Law Center teaches a course in designing legal expert systems. Companies such as Neota Logic have begun to offer artificial intelligence and machine learning-based legal expert systems.\n\n\n", "id": "37447771", "title": "Legal expert system"}
{"url": "https://en.wikipedia.org/wiki?curid=37431577", "text": "Dialogflow\n\nDialogflow (formerly Api.ai, Speaktoit) is a developer of human–computer interaction technologies based on natural language conversations. The company is best known for creating the Assistant (by Speaktoit), a virtual buddy for Android, iOS, and Windows Phone smartphones that performs tasks and answers users' question in a natural language. Speaktoit has also created a natural language processing engine that incorporates conversation context like dialogue history, location and user preferences.\n\nIn May 2012, Speaktoit received a venture round (funding terms undisclosed) from Intel Capital. In July 2014, Speaktoit closed their Series B funding led by Motorola Solutions Venture Capital with participation from new investor Plug and Play Ventures and existing backers Intel Capital and Alpine Technology Fund.\n\nIn September 2014, Speaktoit released api.ai (the voice-enabling engine that powers Assistant) to third-party developers, allowing the addition of voice interfaces to apps based on Android, iOS, HTML5, and Cordova. The SDK's contain voice recognition, natural language understanding, and text-to-speech. api.ai offers a web interface to build and test conversation scenarios. The platform is based on the natural language processing engine built by Speaktoit for its Assistant application. Api.ai allows Internet of Things developers to include natural language voice interfaces in their products. Assistant and Speaktoit's websites now redirect to api.ai's website, which redirects to the Dialogflow website.\n\nGoogle bought the company in September 2016 and was initially known as API.AI; it provides tools to developers building apps (\"Actions\") for the Google Assistant virtual assistant. It was renamed on 10 October 2017 as Dialogflow.\n\nThe organization discontinued the Assistant app on December 15, 2016.\n\nVoice and conversational interfaces created with Dialogflow works with a wide range of devices including phones, wearables, cars, speakers and other smart devices. It supports 14+ languages including Brazilian Portuguese, Chinese, English, Dutch, French, German, Italian, Japanese, Korean, Portuguese, Russian, Spanish and Ukranian. Dialogflow supports an array of services that are relevant to entertainment and hospitality industries. Dialogflow also includes an analytics tool that can measure the engagement or session metrics like usage patterns, latency issues, etc. \n", "id": "37431577", "title": "Dialogflow"}
{"url": "https://en.wikipedia.org/wiki?curid=37394465", "text": "KAoS\n\nKAoS is a policy and domain services framework created by the Florida Institute for Human and Machine Cognition. It uses W3C’s Web Ontology Language (OWL) standard for policy representation and reasoning, and a software guard technology for efficient enforcement of a compiled version of its policies. It has been used in a variety of government-sponsored projects for distributed host and network management and for the coordination of human-agent-robot teams, including DARPA's CoABS Grid, Cougaar, and Common Object Request Broker Architecture (CORBA) models.\n\n", "id": "37394465", "title": "KAoS"}
{"url": "https://en.wikipedia.org/wiki?curid=1499475", "text": "K-line (artificial intelligence)\n\nA K-line, or Knowledge-line, is a mental agent which represents an association of a group of other mental agents found active when a subject solves a certain problem or formulates a new idea. These were first described in Marvin Minsky's essay , published in 1980 in the journal \"Cognitive Science\":\n\nWhen you \"get an idea,\" or \"solve a problem\" ... you create what we shall call a K-line. ... When that K-line is later \"activated\", it reactivates ... mental agencies, creating a partial mental state \"resembling the original.\"\n\n\"Whenever you 'get a good idea', solve a problem, or have a memorable experience, you activate a K-line to 'represent' it. A K-line is a wirelike structure that attaches itself to whichever mental agents are active when you solve a problem or have a good idea.\n\nWhen you activate that K-line later, the agents attached to it are aroused, putting you into a 'mental state' much like the one you were in when you solved that problem or got that idea. This should make it relatively easy for you to solve new, similar problems!\" (1998, p. 82.)\n\n", "id": "1499475", "title": "K-line (artificial intelligence)"}
{"url": "https://en.wikipedia.org/wiki?curid=37291130", "text": "BabelNet\n\nBabelNet is a multilingual lexicalized semantic network and ontology developed at the Sapienza University of Rome the Department of Computer Science Linguistic Computing Laboratory. BabelNet was automatically created by linking Wikipedia to the most popular computational lexicon of the English language, WordNet. The integration is done using an automatic mapping and by filling in lexical gaps in resource-poor languages by using statistical machine translation. The result is an \"encyclopedic dictionary\" that provides concepts and named entities lexicalized in many languages and connected with large amounts of semantic relations. Additional lexicalizations and definitions are added by linking to free-license wordnets, OmegaWiki, the English Wiktionary, Wikidata, FrameNet, VerbNet and others. Similarly to WordNet, BabelNet groups words in different languages into sets of synonyms, called \"Babel synsets\". For each Babel synset, BabelNet provides short definitions (called glosses) in many languages harvested from both WordNet and Wikipedia.\n\n, BabelNet (version 3.7) covers 271 languages, including all European languages, most Asian languages, and Latin. BabelNet 3.7 contains almost 14 million synsets and about 746 million word senses (regardless of their language). Each Babel synset contains 2 synonyms per language, i.e., word senses, on average. The semantic network includes all the lexico-semantic relations from WordNet (hypernymy and hyponymy, meronymy and holonymy, antonymy and synonymy, etc., totaling around 364,000 relation edges) as well as an underspecified relatedness relation from Wikipedia (totaling around 380 million edges). Version 3.7 also associates about 11 million images with Babel synsets and provides a Lemon RDF encoding of the resource, available via a SPARQL endpoint. 2.67 million synsets are assigned domain labels.\n\nBabelNet has been shown to enable multilingual Natural Language Processing applications. The lexicalized knowledge available in BabelNet has been shown to obtain state-of-the-art results in:\n\n\nBabelNet received the META prize 2015 for \"groundbreaking work in overcoming language barriers through a multilingual lexicalised semantic network and ontology making use of heterogeneous data sources\". \n\nBabelNet featured prominently in a TIME magazine's article about the new age of innovative and up-to-date lexical knowledge resources available on the Web. The article describes in some detail how BabelNet is playing a leading role in the 21st century scenario.\n\n", "id": "37291130", "title": "BabelNet"}
{"url": "https://en.wikipedia.org/wiki?curid=30405742", "text": "Plug &amp; Pray\n\nPlug & Pray is a 2010 documentary film about the promise, problems and ethics of artificial intelligence and robotics. The main protagonists are the former MIT professor Joseph Weizenbaum and the futurist Raymond Kurzweil. The title is a pun on the computer hardware phrase \"Plug and Play\".\n\nComputer experts around the world strive towards the development of intelligent robots. Pioneers like Raymond Kurzweil and Hiroshi Ishiguro dream of fashioning intelligent machines that will equal their human creators. In this potential reality, man and machine merge as a single unity. Rejecting evolution's biological shackles tantalisingly dangles the promise of eternal life for those bold enough to seize it. But others, like Joseph Weizenbaum, counterattack against society's limitless faith in the redemptive powers of technology, questioning the prevailing discourses on new technologies and their ethical relationships to human life. The film delves into a world where computer technology, robotics, biology, neuroscience, and developmental psychology merge, and features roboticists in their laboratories in Japan, the USA, Italy and Germany.\n\nSince antiquity, mankind has dreamed of creating brilliant machines. The invention of the computer and the breathtaking pace of technological progress appear to be bringing the realisation of this dream within the grasp of humans. Robots were to do the housework, look after the children, care for the elderly, and go to war. Former MIT professor Joseph Weizenbaum, creator of ELIZA, has become a harsh critic of their visions of technological omnipotence.\n\nProduction of the film started in 2006 and ended in 2009. The death of the main protagonist Joseph Weizenbaum on March 5, 2008, fell in this period. The international festival premiere was at FIPA 2010 in Biarritz, France. Since then the film has been invited to 27 film festivals, among them the Seattle International Film Festival, Vancouver Film Festival, Visions du Réel. The theatrical release in Germany was on Nov. 11, 2010.\n\nThe film won the Bavarian Film Award 2010 for \"best documentary\", the Grand Prix of the Jury for the best film at the Paris International Science Film Festival, the Primer Premio for best film at the Mostra de Ciencia e Cinema in La Coruña (Spain), and the Science Communication Award at the International Science Film Festival Athens. It was also chosen as the best international film at the 46th AFO, Science Documentary Festival in Olomouc, Czech Republic, in 2011.\n\n", "id": "30405742", "title": "Plug &amp; Pray"}
{"url": "https://en.wikipedia.org/wiki?curid=3280462", "text": "Belief–desire–intention model\n\nThe belief–desire–intention (BDI) model of human practical reasoning was developed by Michael Bratman as a way of explaining future-directed intention.\n\nBDI is fundamentally reliant on folk psychology (the 'theory theory'), which is the notion that our mental models of the world are theories. It was used as a basis for developing the belief–desire–intention software model.\n\nBDI was part of the inspiration behind the BDI software architecture, which Bratman was also involved in developing. Here, the notion of intention was seen as a way of limiting time spent on deliberating about what to do, by eliminating choices inconsistent with current intentions.\n\nBDI has also aroused some interest in psychology. BDI formed the basis for a computational model of childlike reasoning CRIBB. It has been proposed that autistic children do not recognise other people as folk-psychological agents (i.e., agents with their own beliefs, etc.). BDI has been used to develop a rehabilitation strategy to teach autistic children to reason about other people.\n\n", "id": "3280462", "title": "Belief–desire–intention model"}
{"url": "https://en.wikipedia.org/wiki?curid=12379384", "text": "And–or tree\n\nAn and–or tree is a graphical representation of the reduction of problems (or goals) to conjunctions and disjunctions of subproblems (or subgoals).\n\nThe and-or tree:\n\nrepresents the search space for solving the problem P, using the goal-reduction methods:\n\nGiven an initial problem P0 and set of problem solving methods of the form:\n\nthe associated and-or tree is a set of labelled nodes such that: \n\n\nA node N, labelled by a problem P, is a success node if there is a method of the form P if nothing (i.e., P is a \"fact\"). The node is a failure node if there is no method for solving P.\n\nIf all of the children of a node N, conjoined by the same arc, are success nodes, then the node N is also a success node. Otherwise the node is a failure node.\n\nAn and-or tree specifies only the search space for solving a problem. Different search strategies for searching the space are possible. These include searching the tree depth-first, breadth-first, or best-first using some measure of desirability of solutions. The search strategy can be sequential, searching or generating one node at a time, or parallel, searching or generating several nodes in parallel.\n\nThe methods used for generating and-or trees are propositional logic programs (without variables). In the case of logic programs containing variables, the solutions of conjoint sub-problems must be compatible. Subject to this complication, sequential and parallel search strategies for and-or trees provide a computational model for executing logic programs.\n\nAnd–or trees can also be used to represent the search spaces for two-person games. The root node of such a tree represents the problem of one of the players winning the game, starting from the initial state of the game. Given a node N, labelled by the problem P of the player winning the game from a particular state of play, there exists a single set of conjoint children nodes, corresponding to all of the opponents responding moves. \nFor each of these children nodes, there exists a set of non-conjoint children nodes, corresponding to all of the player's defending moves.\n\nFor solving game trees with proof-number search family of algorithms, game trees are to be mapped to and-or trees. MAX-nodes (i.e. maximizing player to move) are represented as OR nodes, MIN-nodes map to AND nodes. The mapping is possible, when the search is done with only a binary goal, which usually is \"player to move wins the game\".\n\n", "id": "12379384", "title": "And–or tree"}
{"url": "https://en.wikipedia.org/wiki?curid=39177819", "text": "Cognitive computer\n\nA cognitive computer combines artificial intelligence and machine-learning algorithms, in an approach which attempts to reproduce the behaviour of the human brain.\n\nAn example of neural network implementations of cognitive convolution and deep learning is provided by the IBM company's Watson machine. A subsequent development by IBM is the TrueNorth microchip architecture, which is designed to be closer in structure to the human brain than the von Neumann architecture used in conventional computers. In 2017 Intel announced its own version of a cognitive chip in \"Loihi\", which will be available to university and research labs in 2018.\n\nIntel's self-learning neuromorphic chip, named Loihi, perhaps named after the Hawaiian seamount Loihi, offers substantial power efficiency designed after the human brain. Intel claims Loihi is about 1000 times more energy efficient than the general-purpose computing power needed to train the neural networks that rival Loihi’s performance. \nIn theory, this would support both machine learning training and inference on the same silicon independently of a cloud connection, and more efficient than using convolutional neural networks (CNNs) or deep learning neural networks. Intel points to a system for monitoring a person’s heartbeat, taking readings after events such as exercise or eating, and uses the cognitive computing chip to normalize the data and work out the ‘normal’ heartbeat. It can then spot abnormalities, but also deal with any new events or conditions.\n\nThe first iteration of the Loihi chip was made using Intel’s 14 nm fabrication process, and houses 128 clusters of 1,024 artificial neurons each for a total of 131,072 simulated neurons. This offers around 130 million synapses, which is still a rather long way from the human brain's 80 billion synapses, and behind IBM’s TrueNorth, which has around 16 billion by using 64 by 4,096 cores.\n\nThe IBM cognitive computers implement learning using Hebbian theory. Instead of being programmable in a traditional sense within machine language or a higher level programming language such a device learns by inputting instances through an input device that are aggregated within a computational convolution or neural network architecture consisting of weights within a parallel memory system. An early instantiation of such a device has been developed in 2012 under the Darpa SyNAPSE program at IBM directed by Dharmendra Modha.\n\nIn 2017 this IBM 64-chip array will contain the processing equivalent of 64 million neurons and 16 billion synapses, yet absolutely sips energy; each processor consumes just 10 watts of electricity.\nLike other neural networks, this system will be put to use in pattern recognition and sensory processing roles. The Air Force wants to combine the TrueNorth ability to convert multiple data feeds — whether it's audio, video or text — into machine readable symbols with a conventional supercomputer's ability to crunch data.\nThis isn't the first time that IBM's neural chip system has been integrated into cutting-edge technology. August, 2017 Samsung installed the chips in its Dynamic Vision Sensors enabling cameras to capture images at up to 2,000 fps while burning through just 300 milliwatts of power.\n\nThere are many approaches and definitions for a cognitive computer, and other approaches may be more fruitful.\n\n\nhttp://www.foxnews.com/tech/2018/01/09/ces-2018-intel-gives-glimpse-into-mind-blowing-future-computing.html\n", "id": "39177819", "title": "Cognitive computer"}
{"url": "https://en.wikipedia.org/wiki?curid=39182554", "text": "Catastrophic interference\n\nCatastrophic interference, also known as catastrophic forgetting, is the tendency of an artificial neural network to completely and abruptly forget previously learned information upon learning new information. Neural networks are an important part of the network approach and connectionist approach to cognitive science. These networks use computer simulations to try to model human behaviours, such as memory and learning. Catastrophic interference is an important issue to consider when creating connectionist models of memory. It was originally brought to the attention of the scientific community by research from McCloskey and Cohen (1989), and Ractcliff (1990). It is a radical manifestation of the 'sensitivity-stability' dilemma or the 'stability-plasticity' dilemma. Specifically, these problems refer to the issue of being able to make an artificial neural network that is sensitive to, but not disrupted by, new information. Lookup tables and connectionist networks lie on the opposite sides of the stability plasticity spectrum. The former remains completely stable in the presence of new information but lacks the ability to generalize, i.e. infer general principles, from new inputs. On the other hand, connectionist networks like the standard backpropagation network are very sensitive to new information and can generalize on new inputs. Backpropagation models can be considered good models of human memory insofar as they mirror the human ability to generalize but these networks often exhibit less stability than human memory. Notably, these backpropagation networks are susceptible to catastrophic interference. This is considered an issue when attempting to model human memory because, unlike these networks, humans typically do not show catastrophic forgetting. Thus, the issue of catastrophic interference must be eradicated from these backpropagation models in order to enhance the plausibility as models of human memory.\n\nIn order to understand the topic of catastrophic interference it is important to understand the components of an artificial neural network and, more specifically, the behaviour of a backpropagation network. The following account of neural networks is summarized from \"Rethinking Innateness: A Connectionist Perspective on Development\" by Elman et al. (1996).\n\nArtificial neural networks are inspired by biological neural networks. They use mathematical models, namely algorithms, to do things such as classifying data and learning patterns in data. Information is represented in these networks through patterns of activation, known as a distributed representations.\n\nThe basic components of artificial neural networks are \"nodes/units\" and \"weights\". \n\"Nodes\" or \"units\" are simple processing elements, which can be considered artificial neurons. These units can act in a variety of ways. They can act like sensory neurons and collect inputs from the environment, they can act like motor neurons and sent and output, they can act like interneurons and relay information, or they may do all three functions. A backpropagation network is often a three-layer neural network that includes input nodes, hidden nodes, and output nodes (see Figure 1). The hidden nodes allow the network to be transformed into an internal representation, akin to a mental representation. These internal representations give the backpropagation network its ability to capture abstract relationships between different input patterns.\n\nThe nodes are also connected to each other, thus they can send activation to one another like neurons. These connections can be unidirectional, creating a feedforward network, or they can be bidirectional, creating a recurrent network. Each of the connections between the nodes has a '`weight'`, or strength, and it is in these weights are where the knowledge is 'stored'. The weights act to multiply the output of a node. They can be excitatory (a positive value) or inhibitory (a negative value). For example, if a node has an output of 1.0 and it is connected to another node with a weight of -0.5 then the second node will receive an input signal of 1.0 × (-0.5) = -0.5. Since any one node can receive multiple inputs, the sum of all of these inputs must be taken to calculate the net input.\n\nThe net input (\"net\") to a node \"j\" would be defined as:\n\nOnce the input has been sent to the hidden layer from the input layer, the hidden node may then send an output to the output layer. The output of any given node depends on the activation of that node and the response function of that node. In the case of a three-layer backpropagation network, the response function is a non-linear, logistic function. This function allows a node to behave in an all or none fashion towards high or low input values and in a more graded and sensitive fashion towards mid-ranged input values. It allows the nodes the result in more substantial changes in the network when the node activation is at the more extreme values. Transforming the net input into a net output that can be sent onto the output layer is calculated by:\n\nAn important feature of neural networks is that they can learn. Simply put, this means that they can change their outputs when they are given new inputs. Backpropagation, specifically refers to how this the network is trained, i.e. how the network is told to learn. The way in which a backpropagation network learns, is through comparing the actual output to the desired output of the unit. The desired output is known as a 'teacher' and it can be the same as the input, as in the case of auto-associative/auto-encoder networks, or it can be completely different from the input. Either way, learning which requires a teacher is called supervised learning. The difference between these actual and desired output constitutes an error signal. This error signal is then fedback, or backpropagated, to the nodes in order to modify the weights in the neural network. Backpropagation first modifies the weights between output layer to the hidden layer, then next modifies the weights between the hidden units and the input units. The change in weights help to decrease the discrepancy between the actual and desired output. However, learning is typically incremental in these networks. This means that these networks will require a series of presentations of the same input before it can come up with the weight changes that will result in the desired output. The weights are usually set to random values for first learning trial and after many trials the weights become more able represent the desired output. The process of converging on an output is called settling. This kind of training is based on the error signal and backpropagation learning algorithm / delta rule:\n\nThe issue of catastrophic interference, comes about when learning is sequential. Sequential training involves the network learning an input-output pattern until the error is reduced below a specific criterion, then training the network on another set of input-output patterns. Specifically, a backpropagation network will forget information if it first learns input \"A\" and then next learns input \"B\". It is not seen when learning is concurrent or interleaved. Interleaved training means the network learns both inputs-output patterns at the same time, i.e. as \"AB\". Weights are only changed when the network is being trained and not when the network is being tested on its response.\n\nTo summarize, backpropagation networks:\n\n\nHumans often learn information in a sequential manner. For example, a child will often learn their 1st addition facts first, later followed by the 2nd addition facts, etc. It would be impossible for a child to learn all of the addition facts at the same time. Catastrophic interference can be considered an issue when modelling human memory because, unlike backpropagation networks, humans typically do not show catastrophic forgetting during sequential learning. Rather humans tend to show gradual forgetting or interference when they learn information sequentially. For example, the classic retroactive interference study by Barnes and Underwood (1959) used paired associate learning to determine how much new learning interfered with old learning in humans. Paired associates, means that a pair of stimuli is and responses are learned. Their experiment used eight lists of paired associates, A-B and A-C. The pairs had the stimuli as consonant-vowel-consonant trigrams (e.g., dax) and responses as adjectives. Subjects were initially trained on the A-B list, until they could correctly recall all A-B pairings. Next subjects were given 1, 5, 10 or 20 trials on the A-C list. After learning the A-C pairs the subjects were given a final test in which the stimulus A was presented and the subject was asked to recall the response B and C. They found that as the number of learning trials on A-C list increased, the recall of C increased. But the training on A-C interfered with the recall of B. Specifically recall of B dropped to around 80% after one learning trial of A-C and to 50% after 20 learning trials of A-C. Subsequent research on the topic of retroactive interference has found similar results, with human forgetting being gradual and typically leveling off near 50% recall. Thus when compared with typical human retroactive interference, catastrophic interference could be likened to retrograde amnesia.\n\nSome researchers have argued that catastrophic interference is not an issue with the backpropagation model of human memory. For example, Mirman and Spivey (2001) found that humans show more interference when learning pattern-based information. Pattern-based learning is analogous to how a standard backpropagation network learns. Thus, they concluded that catastrophic interference is not limited to connectionist memory models but rather that it is a \"general product of pattern-based learning that occurs in humans as well\" (p. 272). However, Musca, Rousset and Ans (2004) found contrasting results where retroactive interference was more pronounced in subjects who sequentially learned unstructured lists when controlling for methodological failure that occurred in the Mirman, D., & Spivey, M. (2001) study.\n\nThe term catastrophic interference was originally coined by McCloskey and Cohen (1989) but was also brought to the attention of the scientific community by research from Ratcliff (1990).\n\nMcCloskey and Cohe n(1989) noted the problem of catastrophic interference during two different experiments with backpropagation neural network modelling.\n\nIn their first experiment they trained a standard backpropagation neural network on a single training set consisting of 17 single-digit ones problems (i.e., 1 + 1 through 9 + 1, and 1 + 2 through 1 + 9) until the network could represent and respond properly to all of them. The error between the actual output and the desired output steadily declined across training sessions, which reflected that the network learned to represent the target outputs better across trials. Next they trained the network on a single training set consisting of 17 single-digit twos problems (i.e., 2 + 1 through 2 + 9, and 1 + 2 through 9 + 2) until the network could represent, respond properly to all of them. They noted that their procedure was similar to how a child would learn their addition facts. Following each learning trial on the twos facts, the network was tested for its knowledge on both the ones and twos addition facts. Like the ones facts, the twos facts were readily learned by the network. However, McCloskey and Cohen noted the network was no longer able to properly answer the ones addition problems even after one learning trial of the twos addition problems. The output pattern produced in response to the ones facts often resembled an output pattern for an incorrect number more closely than the output pattern for an incorrect number. This is considered to be a drastic amount of error. Furthermore, the problems 2+1 and 2+1, which were included in both training sets, even showed dramatic disruption during the first learning trials of the twos facts.\n\nIn their second connectionist model, McCloskey and Cohen attempted to replicate the study on retroactive interference in humans by Barnes and Underwood (1959). They trained the model on A-B and A-C lists and used a context pattern in the input vector (input pattern), to differentiate between the lists. Specifically the network was trained to responds with the right B response when shown the A stimulus and A-B context pattern and to respond with the correct C response when shown the A stimulus and the A-C context pattern. When the model was trained concurrently on the A-B and A-C items then the network readily learned all of the associations correctly. In sequential training the A-B list was trained first, followed by the A-C list. After each presentation of the A-C list, performance was measured for both the A-B and A-C lists. They found that the amount of training on the A-C list in Barnes and Underwood study that lead to 50% correct responses, lead to nearly 0% correct responses by the backpropagation network. Furthermore, they found that the network tended to show responses that looked like the C response pattern when the network was prompted to give the B response pattern. This indicated that the A-C list apparently had overwritten the A-B list. This could be likened to learning the word dog, followed by learning the word stool and then finding that you cannot recognize the word cat well but instead think of the word stool when presented with the word dog.\n\nMcCloskey and Cohen tried to reduce interference through a number of manipulations including changing the number of hidden units, changing the value of the learning rate parameter, overtraining on the A-B list, freezing certain connection weights, changing target values 0 and 1 instead 0.1 and 0.9. However none of these manipulations satisfactorily reduced the catastrophic interference exhibited by the networks.\n\nOverall, McCloskey and Cohen (1989) concluded that: \n\nRatcliff (1990) used multiple sets of backpropagation models applied to standard recognition memory procedures, in which the items were sequentially learned. After inspecting the recognition performance models he found two major problems:\nEven one learning trial with new information resulted in a significant loss of the old information, paralleling the findings of McCloskey and Cohen (1989). Ratcliff also found that the resulting outputs were often a blend of the previous input and the new input. In larger networks, items learned in groups (e.g. AB then CD) were more resistant to forgetting than were items learned singly (e.g. A then B then C…). However, the forgetting for items learned in groups was still large. Adding new hidden units to the network did not reduce interference. \nThis finding contradicts with studies on human memory, which indicated that discrimination increases with learning. Ratcliff attempted to alleviate this problem by adding 'response nodes' that would selectively respond to old and new inputs. However, this method did not work as these response nodes would become active for all inputs. A model which used a context pattern also failed to increase discrimination between new and old items.\n\nMany researchers have suggested that the main cause of catastrophic interference is overlap in the representations at the hidden layer of distributed neural networks. In a distributed representation any given input will tend to create changes in the weights to many of the nodes. Catastrophic forgetting occurs because when many of the weights, where 'knowledge is stored, are changed it is impossible for prior knowledge to be kept intact. During sequential learning, the inputs become mixed with the new input being superimposed over top of the old input. Another way to conceptualize this is through visualizing learning as movement through a weight space. This weight space can be likened to a spatial representation of all of the possible combinations of weights that the network can possess. When a network first learns to represent a set of patterns, it has found a point in weight space which allows it to recognize all of the patterns that it has seen. However, when the network learns a new set of patterns sequentially it will move to a place in the weight space that allows it to only recognize the new pattern. To recognize both sets of patterns, the network must find a place in weight space that can represent both the new and the old output. One way to do this is by connecting a hidden unit to only a subset of the input units. This reduces the likelihood that two different inputs will be encoded by the same hidden units and weights, and so will decrease the chance of interference. Indeed, a number of the proposed solutions to catastrophic interference involve reducing the amount of overlap that occurs when storing information in these weights.\n\nMany of the early techniques in reducing representational overlap involved making either the input vectors or the hidden unit activation patterns orthogonal to one another. Lewandowsky and Li (1995) noted that the interference between sequentially learned patterns is minimized if the input vectors are orthogonal to each other. Input vectors are said to be orthogonal to each other if the pairwise product of their elements across the two vectors sum to zero. For example, the patterns [0,0,1,0] and [0,1,0,0] are said to be orthogonal because (0×0 + 0×1 + 1×0 + 0×0) = 0. One of the techniques which can create orthogonal representations at the hidden layers involves bipolar feature coding (i.e., coding using -1 and 1 rather than 0 and 1). Orthogonal patterns tend to produce less interference with each other. However, not all learning problems can be represented using these types of vectors and some studies report that the degree of interference is still problematic with orthogonal vectors. Simple techniques such as varying the learning rate parameters in the backpropagation equation were not successful in reducing interference. Varying the number of hidden nodes has also been used to try and reduce interference. However, the findings have been mixed, with some studies finding that more hidden units decrease interference and other studies finding it does not.\n\nBelow are a number of techniques which have empirical support in successfully reducing catastrophic interference in backpropagation neural networks:\n\nFrench (1991) proposed that catastrophic interference arises in feedforward backpropagation networks due to the interaction of node activations, or activation overlap, that occur in distributed representations at the hidden layer. Specifically, he defined this activation overlap as the average shared activation over all units in the hidden layer, calculated by summing the lowest activation of the nodes at the hidden layer and averaging this sum. For example, if the activations at the hidden layer from one input are (0.3, 0.1, 0.9, 1.0) and the activations from the next input are (0.0, 0.9, 0.1, 0.9) the activation overlap would be (0.0 + 0.1 + 0.1 + 0.9 ) / 4 = 0.275. When using [binary number|binary] representation of input [row vector|vectors], activation values will be 0 through 1, where 0 indicates no activation overlap and 1 indicates full activation overlap. French noted that neural networks which employ very localized representations do not show catastrophic interference because of the lack of overlap at the hidden layer. That is to say, each input pattern will create a hidden layer representation that involves the activation of only one node, so differed inputs will have an activation overlap of 0. Thus, he suggested that reducing the value of activation overlap at the hidden layer would reduce catastrophic interference in distributed networks. Specifically he proposed that this could be done through changing the distributed representations at the hidden layer to 'semi-distributed' representations. A 'semi-distributed' representation has fewer hidden nodes that are active, and/or a lower activation value for these nodes, for each representation, which will make the representations of the different inputs overlap less at the hidden layer. French recommended that this could be done through 'activation sharpening', a technique which slightly increases the activation of a certain number of the most active nodes in the hidden layer, slightly reduces the activation of all the other units and then changes the input-to-hidden layer weights to reflect these activation changes (similar to error backpropgation). Overall the guidelines for the process of 'activation sharpening' are as follows:\n\n\nIn his tests of an 8-8-8 (input-hidden-output) node backpropagation network where one node was sharpened, French found that this sharpening paradigm did result in one node being much more active than the other seven. Moreover, when sharpened, this network took one fourth the time to relearn the initial inputs than a standard backpropagation without node sharpening. Relearning is a measure of memory savings and thus extent of forgetting, where more time to relearn suggests more forgetting (Ebbinghaus savings method). A two-node sharpened network performed even slightly better, however if more than two nodes were sharpened forgetting increased again.\n\nAccording to French, the sharpened activations interfere less with weights in the network than unsharpened weights and this is due specifically to the way that backpropagation algorithm calculates weight changes. Activations near 0 will change the weights of links less than activations near 1. Consequently, when there are many nodes with low activations (due to sharpening), the weights to and from these nodes will be modified much less than the weights on very active nodes. As a result, when a new input is fed into the network, sharpening will reduce activation overlap by limiting the number of highly active hidden units and will reduce the likelihood of representational overlap by reducing the number of weights that are to be changed. Thus, node sharpening will decrease the amount of disruption in the old weights, which store prior input patterns, thereby reducing the likelihood of catastrophic forgetting.\n\nKortge (1990) proposed a learning rule for training neural networks, called the 'novelty rule', to help alleviate catastrophic interference. As its name suggests, this rule helps the neural network to learn only the components of a new input that differ from an old input. Consequently, the novelty rule changes only the weights that were not previously dedicated to storing information, thereby reducing the overlap in representations at the hidden units. Thus, even when inputs are somewhat similar to another, dissimilar representations can be made at the hidden layer. In order to apply the novelty rule, during learning the input pattern is replaced by a novelty vector that represents the components that differ. The novelty vector for the first layer (input units to hidden units) is determined by taking the target pattern away from the current output of the network (the delta rule). For the second layer (hidden units to output units) the novelty vector is simply the activation of the hidden units that resulted from using the novelty vector as an input through the first layer. Weight changes in the network are computed by using a modified delta rule with the novelty vector replacing the activation value (sum of the inputs):\n\nWhen the novelty rule is used in a standard backpropagation network there is no, or lessened, forgetting of old items when new items are presented sequentially. However, this rule can only apply to auto-encoder or auto-associative networks, in which the target response for the output layer is identical to the input pattern. This is because the novelty vector would be meaningless if the desired output was not identical to the input as it would be impossible to calculate how much a new input differed from the old input.\n\nMcRae and Hetherington (1993) argued that humans, unlike most neural networks, do not take on new learning tasks with a random set of weights. Rather, people tend to bring a wealth of prior knowledge to a task and this helps to avoid the problem of interference. They proposed that when a network is pre-trained on a random sample of data prior to starting a sequential learning task that this prior knowledge will naturally constrain how the new information can be incorporated. This would occur because a random sample of data from a domain which has a high degree of internal structure, such as the English language, training would capture the regularities, or recurring patterns, found within that domain. Since the domain is based on regularities, a newly learned item will tend to be similar to the previously learned information, which will allow the network to incorporate new data with little interference with existing data. Specifically, an input vector which follows the same pattern of regularities as the previously trained data should not cause a drastically different pattern of activation at the hidden layer or drastically alter weights.\n\nTo test their hypothesis, McRae and Hetherington (1993) compared the performance of a naïve and pre-trained auto-encoder backpropagation network on three simulations of verbal learning tasks. The pre-trained network was trained using letter based representations of English monosyllabic words or English word pairs. All three tasks involved the learning of some consonant-vowel-consonant (CVC) strings or CVC pairs (list A), followed by training on a second list of these items (list B). Afterwards, the distributions of the hidden node activations were compared between the naïve and pre-trained network. In all three tasks, the representations of a CVC in the naïve network tended to be spread fairly evenly across all hidden nodes, whereas most hidden nodes were inactive in the pre-trained network. Furthermore, in the pre-trained network the representational overlap between CVCs was reduced compared to the naïve network. The pre-trained network also retained some similarity information as the representational overlap between similar CVCs, like \"JEP\" and \"ZEP\", was greater than for dissimilar CVCs, such as \"JEP\" and \"YUG\". This suggests that the pre-trained network had a better ability to generalize, i.e. notice the patterns, than the naïve network. Most importantly, this reduction in hidden unit activation and representational overlap resulted in significantly less forgetting in the pre-trained network than the naïve network, essentially eliminating catastrophic interference. Essentially, the pre-training acted to create internal orthogonalization of the activations at the hidden layer, which reduced interference. Thus, pre-training is a simple way to reduce catastrophic forgetting in standard backpropagation networks.\n\nFrench (1997) proposed the idea of a pseudo-recurrent backpropagation network in order to help reduce catastrophic interference (see Figure 2). In this model the network is separated into two functionally distinct but interacting sub-networks. This model is biologically inspired and is based on research from McClelland, McNaughton, and O'Reilly (1995). In this research McClelland et al. (1995), suggested that the hippocampus and neocortex act as separable but complementary memory systems. Specifically, the hippocampus short term memory storage and acts gradually over time to transfer memories into the neocortex for long term memory storage. They suggest that the information that is stored can be \"brought back\" to the hippocampus during active rehearsal, reminiscence, and sleep and renewed activation is what acts to transfer the information to the neocortex over time. In the pseudo-recurrent network, one of the sub-networks acts as an early processing area, akin to the hippocampus, and functions to learn new input patters. The other sub-network acts as a final-storage area, akin to the neocortex. However, unlike in McClelland et al. (1995) model, the final-storage area sends internally generated representation back to the early processing area. This creates a recurrent network. French proposed that this interleaving of old representations with new representations is the only way to reduce radical forgetting. Since the brain would most likely not have access to the original input patterns, the patterns that would be fed back to the neocortex would be internally generated representations called \"pseudopatterns\". These pseudopatterns are approximations of previous inputs and they can be interleaved with the learning of new inputs. The use of these pseudopatterns could be biologically plausible as parallels between the consolidation of learning that occurs during sleep and the use of interleaved pseudopatterns. Specifically, they both serve to integrate new information with old information without disruption of the old information. When given an input (and a teacher value) is fed into the pseudo-recurrent network would act as follows:\n\n\nWhen tested on sequential learning of real world patterns, categorization of edible and poisonous mushrooms, the pseudo-recurrent network was shown less interference than a standard backpropagation network. This improvement was with both memory savings and exact recognition of old patterns. When the activation patterns of the pseudo-recurrent network were investigated, it was shown that this network automatically formed semi-distributed representations. Since these types of representations involve fewer nodes being activated for each pattern, it is likely what helped to reduce interference.\n\nNot only did the pseudo-recurrent model show reduced interference but also it models list-length and list-strength effects seen in humans. The list-length effect means that adding new items to a list harms the memory of earlier items. Like humans, the pseudo recurrent network showed a more gradual forgetting when to be trained list is lengthened. The list-strength effect means that when the strength of recognition for one item is increased, there is no effect on the recognition of the other list items. This is an important finding as other models often exhibit a decrease in the recognition of other list items when one list item is strengthened. Since the direct copying of weights from the early processing area to the final storage area does not seem highly biologically plausible, the transfer of information to the final storage area can be done through training the final storage area with pseudopatterns created by the early processing area. However, a disadvantage of the pseudo-recurrent model is that the number of hidden units in the early processing and final storage sub-networks must be identical.\n\nFollowing the same basic idea contributed by Robins, Ans and Rousset (1997) have also proposed a two-network artificial neural architecture with \"memory self-refreshing\" that overcomes catastrophic interference when sequential learning tasks are carried out in distributed networks trained by backpropagation. The principle is to interleave, at the time when new external patterns are learned, those to-be-learned new external patterns with internally generated pseudopatterns, or 'pseudo-memories', that reflect the previously learned information. What mainly distinguishes this model from those that use classical pseudorehearsal in feedforward multilayer networks is a \"reverberating\" process that is used for generating pseudopatterns. This process which, after a number of activity re-injections from a single random seed, tends to go up to nonlinear network \"attractors\", is more suitable for optimally capturing the deep structure of previously learned knowledge than a single feedforward pass of random activation. Ans and Rousset (2000) have shown that the learning mechanism they proposed avoiding catastrophic forgetting, provides a more appropriate way to deal with knowledge transfer as measured by learning speed, ability to generalize and vulnerability to network damages. Musca, Rousset and Ans (2009) have also shown that pseudopatterns originating from an artificial reverberating neural network could induce familiarity in humans with never seen items in the way predicted by simulations conducted with a two-network artificial neural architecture. Furthermore, Ans (2004) has implemented a version of the self-refreshing mechanism using only one network trained by the Contrastive Hebbian Learning rule, a training rule considered as more realistic than the largely used backpropagation algorithm, but fortunately equivalent to the latter.\n\nSo far, the different solutions to catastrophic interference that have been presented concern tasks of sequential learning involving only non-temporally ordered lists of items. But, to be credible, the self-refreshing mechanism for 'static' learning has to encompass our human ability to learn serially many temporal sequences of patterns without catastrophic interference (e.g. learning one song followed by learning a second song without forgetting the first one). This was done by Ans, Rousset, French and Musca (2004) who have presented, in addition to simulation work, an experiment that evidences a close similarity between the behaviour of humans and the behaviour of the proposed neuromimetic architecture.\n\nLatent Learning is a technique used by Gutstein & Stump (2015) both to mitigate catastrophic interference and to take advantage of transfer learning. Rather than manipulating the representations for new classes used by the hidden nodes, this approach tries to train optimal representations for new classes into the output nodes. It chooses output encodings that are least likely to catastrophically interfere with existing responses.\n\nGiven a net that has learned to discriminate among one set of classes using Error Correcting Output Codes (ECOC) (as opposed to 1 hot codes), optimal encodings for new classes are chosen by observing the net's average responses to them. Since these average responses arose while learning the original set of classes \"without any exposure to the new classes\", they are referred to as 'Latently Learned Encodings'. This terminology borrows from the concept of Latent Learning, as introduced by Tolman in 1930. In effect, this technique uses transfer learning to avoid catastrophic interference, by making a net's responses to new classes as consistent as possible with existing responses to classes already learned.\n\nKirkpatrick et al. (2017) demonstrated a method to train a single artificial neural network on multiple tasks using a technique called elastic weight consolidation.\n\nPractopoietic theory proposes that biological systems solve the problem of catastrophic interference by storing long-term memories only in a general form, not applicable to a given situation but instead loosely applicable to a class of different situations. In order to adjust the loosely applicable knowledge to the given current situation, the process of \"anapoiesis\" is applied. Anapoiesis stands for \"reconstruction of knowledge\"—transforming knowledge from a general form to a specific one. Practopoietic theory is founded in the theorems of cybernetics and is concerned with the question of how cybernetic systems obtain their capabilities to control and act.\n", "id": "39182554", "title": "Catastrophic interference"}
{"url": "https://en.wikipedia.org/wiki?curid=39278567", "text": "Computer Arimaa\n\nComputer Arimaa refers to the playing of the board game Arimaa by computer programs.\n\nIn 2002, Indian American computer engineer Omar Syed published the rules to Arimaa and announced a $10,000 prize, available annually until 2020, for the first computer program (running on standard, off-the-shelf hardware) able to defeat each of three top-ranked human players in a three-game series. The prize was claimed in 2015, when a computer program played 7:2 against three human players. The game has been the subject of several research papers.\n\nThe number of different ways that each player can set up their pieces at the beginning of the game is:\n\nformula_1\n\nThe player can put 8 rabbits on 16 possible squares, followed by 2 cats on the 8 remaining squares, 2 dogs on the 6 remaining squares, 2 horses on the four remaining squares, one camel on one of the two remaining squares, and the elephant on the final unused square.\n\nBecause each player can start the game with one of 64,864,800 opening setups, the total state space for the opening is:\n\nformula_2\n\nAs Christ-Jan Cox said in his Master's thesis, because the number of possible initial states is so large, \"[i]t follows that it is very difficult to develop complete databases of opening moves.\"\n\nIt is important for the computer to be able to evaluate the value of the pieces on the board so it can assess whether or not a capture or exchange would be desirable. Assessing the relative value of pieces is an area of ongoing Arimaa research. Some currently-used systems are DAPE and FAME.\n\nThe following techniques are used by some or all of the artificial intelligence programs that play Arimaa:\n\n\nSeveral aspects of Arimaa make it difficult for computer programs to beat good human players. Because so much effort has gone into the development of strong chess-playing software, it is particularly relevant to understand why techniques applicable to chess are less effective for Arimaa.\n\nTop chess programs use brute-force searching coupled with static position evaluation dominated by material considerations. Chess programs examine many, many possible moves, but they are not good (compared to humans) at determining who is winning at the end of a series of moves unless one side has more pieces than the other. The same is true for Arimaa programs, but their results are not as good in practice.\n\nWhen brute-force searching is applied to Arimaa, the depth of the search is limited by the huge number of options each player has on each turn. Computationally, the number of options a player has available to them governs the number of different paths play can go down. This is known as the branching factor. The average branching factor in a game of Chess is about 35, whereas in Arimaa it is about 17,000.\n\nThese differing branching factors imply that a computer which can search to a depth of eight turns for each player in chess, can only search about three turns deep for each player in Arimaa:\n\nformula_3\n\nBrute force search depth, for chess software, is nearly doubled by alpha-beta pruning, which allows the software to conclude that one move is better than another without examining every possible continuation of the weaker move. If the opponent can crush a certain move with one reply, it isn't necessary to examine other replies, which dramatically increases search speed. In Arimaa, however, the side to move switches only every four steps, which reduces the number of available cutoffs in a step-based search.\n\nFurthermore, the usefulness of alpha-beta pruning is heavily dependent on the order in which moves are considered. Good moves must be considered before bad ones in order for the bad ones to be neglected. In particular, checking and capturing moves are key for pruning, because they are often much better than other moves. In Arimaa software the speedup provided by alpha-beta pruning is less, because captures are rarer. In rated games played on arimaa.com, only 3% of steps result in capture, compared to about 19% of chess moves that result in capture.\n\nIn most Arimaa positions, particularly toward the beginning of the game when the board is still crowded, a competent player can avoid losing any pieces within the next two turns. Compared to chess, Arimaa allows either player to delay captures for longer. Indeed, the median move number of the first capture in chess is turn 6, whereas in Arimaa it is turn 12. The struggle is initially more positional in Arimaa, and revolves around making captures unavoidable at some point in the future. This magnifies the importance of correctly judging who is gaining ground in non-material ways. Thus the strength of computer programs (examining millions of positions) is not as significant as their weakness (judging the position apart from who has more pieces).\n\nThe weakness of Arimaa programs in the opening phases is further magnified by the setup phase. In chess every game starts from the same position. By compiling before the game a list of stock replies to all standard opening moves, chess programs may often make a dozen or more excellent moves before starting to \"think\". Humans do the same, but have a smaller and less reliable memory of openings, which puts humans at a relative disadvantage in chess. Arimaa, in contrast, has millions of possible ways to set up the pieces even before the first piece moves. This prevents programs from having any meaningful opening book.\n\nAs the game progresses, exchanges and the advancement of rabbits tend to make the position more open and tactical. Arimaa programs typically play better in this sort of position, because they see tactical shots which humans overlook. However, it is usually possible for humans to avoid wide-open positions by conservative play, and to angle for strategic positions in which computers fare worse. Against a conservative opponent it is almost impossible to bust open the position in Arimaa, whereas in chess it is merely difficult. One must beat defensive play by the accumulation of small, long-term advantages, which programs do not do very well.\n\nOne additional technique from computer chess which does not apply to Arimaa is endgame tablebases. Master-level chess games sometimes trade down into unclear endgames with only a few pieces, for example king and knight vs. king and rook. It is possible to build, by retrograde analysis, an exhaustive table of the correct move in all such positions. Programs have only to consult a pre-generated table in such positions, rather than \"thinking\" afresh, which gives them a relative advantage over humans. Arimaa, in contrast, seldom comes to an endgame. Equal exchanges of pieces are less common than in chess, so it is rare for a game of Arimaa to \"trade down\" and still be unclear. An average game of Arimaa has only eight captures (compared to seventeen for chess), and top humans can often defeat top programs in Arimaa without losing a single piece, for example the second game of the 2014 Challenge match. Another example of low capture density is this semifinal game of the 2012 World Championship, featuring only a single capture, a goal-forcing elephant sacrifice.\n\nOmar Syed hopes that, because traditional artificial intelligence techniques are only moderately effective for Arimaa, programmers will be forced to use new artificial intelligence techniques to create a strong Arimaa-playing program. The successful quest to build a world-championship-caliber chess program has produced many techniques to successfully play games, but has contributed essentially nothing to more general reasoning; in fact, the techniques of chess playing programs have been excluded from some definitions of artificial intelligence; a goal for Arimaa is that the techniques involved in playing it will help the larger goals of artificial intelligence.\n\nThe structure of Syed's man-against-machine challenge is focused on rewarding advances in AI software and not advances in hardware. In the annual challenge, programs are run on machines chosen and provided by Syed himself, under the criterion that it be a typical, inexpensive, off-the-shelf home computer. The challenge would not be open to anyone requiring expensive multi-processor machines such as those used to challenge top-level chess players, much less something like the custom-built supercomputer Deep Blue, even though it was the success of this hardware-intensive approach which inspired Arimaa's invention. Syed believes that even the computer used in the 2004 challenge match (a Pentium 4 2.4 GHz system with 512 MB of RAM) had sufficient hardware to win the challenge prize if only it was running the proper software. Supercomputers might already have the power to conquer Arimaa by brute force using conventional AI software, and eventually personal computers will too, if hardware continues to advance at the current rate. This is why the Arimaa challenge prize is offered only until the year 2020.\n\nIt has been argued that a computer has beaten the world chess champion but not beaten the human in the Arimaa Challenge (until 2015) because of six reasons:\n\n\nHowever, the Arimaa community disputes this argument point by point. To the first point, Arimaa is a new game, so the playing community is still small and even the best players are not professional players and have only been playing the game for a few years. Thus the human players in the Arimaa Challenge are much weaker than the human players in the chess challenge. The weakness of human players should make the Arimaa Challenge easier to conquer than chess, which compensates developers for having studied the problem for a shorter time.\n\nThe remaining five points compare the Arimaa Challenge only to Kasparov vs. Deep Blue, ignoring all other man vs. machine chess matches in which computers have prevailed. The chess match which can most closely be compared to the Arimaa Challenge match is the Man vs Machine World Team Championship. In 2004 and 2005 a team of humans played against a team of computer opponents. In both years the computers won by wide margin. In 2005 all three humans lost, the computers won 2/3 of the total points, the chess engines were commercially available for the humans to study, and the machine hardware used was not a supercomputer, but rather comparable to hardware used in the Arimaa Challenge.\n\nMan-vs.-machine chess matches since 2005 have shown increasing computer dominance. For example, the 2006 Deep Fritz vs. Vladimir Kramnik and 2007 Rybka vs. Jaan Ehlvest matches gave additional advantages to the human player, but the computers (running on commodity hardware) prevailed anyway.\n\nThe Arimaa Engine Interface, developed by Brian Haskin, defines a protocol that allows an Arimaa engine to communicate with a controller.\n\nAccording to the documentation: \"An engine is a program capable of taking the state of an Arimaa game and selecting a legal move to make. A controller is anything that wants to communicate with and control an engine. This could be anything from a simple script to have the engine analyse a single position to a GUI program that allows games to be played with humans or other engines.\"\n\nThe Arimaa Engine Interface includes an implementation of an engine and controller, documentation, and various scripts to control the engine and play games on any website which supports the protocol, including the official Arimaa website.\n\n", "id": "39278567", "title": "Computer Arimaa"}
{"url": "https://en.wikipedia.org/wiki?curid=39903059", "text": "Kuwahara filter\n\nThe Kuwahara filter is a non-linear smoothing filter used in image processing for adaptive noise reduction. Most filters that are used for image smoothing are linear low-pass filters that effectively reduce noise but also blur out the edges. However the Kuwahara filter is able to apply smoothing on the image while preserving the edges.\n\nSuppose that formula_1 is a grey scale image and that we take a square window of size formula_2 centered around a point formula_3 in the image. This square can be divided into four smaller square regions formula_4 each of which will be \n\nwhere formula_6 is the cartesian product. It must be noted that pixels located on the borders between two regions belong to both regions so there is a slight overlap between subregions.\n\nThe arithmetic mean formula_7 and standard deviation formula_8 of the four regions centered around a pixel (x,y) are calculated and used to determine the value of the central pixel. The output of the Kuwahara filter formula_9 for any point formula_3 is then given by \nformula_11\n\nThis means that the central pixel will take the mean value of the area that is most homogenous. The location of the pixel in relation to an edge plays a great role in determining which region will have the greater standard deviation. If for example the pixel is located on a dark side of an edge it will most probably take the mean value of the dark region. On the other hand, should the pixel be on the lighter side of an edge it will most probably take a light value. On the event that the pixel is located on the edge it will take the value of the more smooth, least textured region. The fact that the filter takes into account the homogeneity of the regions ensures that it will preserve the edges while using the mean creates the blurring effect.\n\nSimilarly to the Median filter the Kuwahara filter uses a sliding window approach to access every pixel in the image. The size of the window is chosen in advance and may vary depending on the desired level of blur in the final image. Bigger windows typically result in the creation of more abstract images whereas small windows produce images that retain their detail. Typically windows are chose to be square with sides that have an odd number of pixels for symmetry. However, there are variations of the Kuwahara filter that use rectangular windows. Additionally, the subregions do not need to overlap or have the same size as long as they cover all of the window.\n\nObviously the Normal filter can't be used for color images by applying the filter to each RGB channel separately and then using the three resulting channels to compose the image. The main problem with this is that the sub regions will have different variances for each of the channels. For example, a region with the lowest variance in the red channel might have the highest variance in the green channel. This once again causes ambiguity which would result in the color of the central pixel to be determined by several regions, which might also result in blurrier edges.\n\nTo overcome this problem in colored images a slightly modified Kuwahara filter must be used. This filter must also take into account the \"brightness\" (the Value coordinate in the HSV color model) of each pixel in the region. This time only the variance of the \"brightness\" of each subregion is calculated along with the mean for each color channel. The filter will produce an output for each channel which will correspond to the mean of that channel for the subregion with the lowest variance in \"brightness\". This ensures that only one region will determine the RGB values of the central pixel.\n\nOriginally the Kuwahara filter was proposed for use in processing RI-angiocardiographic images of the cardiovascular system. The fact that any edges are preserved when smoothing makes it especially useful for feature extraction and segmentation and explains why it is used in medical imaging. \nThe Kuwahara filter however also finds many applications in artistic imaging and fine-art photography due to its ability to remove textures and sharpen the edges of photographs. The level of abstraction helps create a desirable painting-like effect in artistic photographs especially in the case of the colored image version of the filter. These applications have known great success and have encouraged similar research in the field of image processing for the arts.\n\nAlthough the vast majority of applications have been in the field of image processing there have been cases that use modifications of the Kuwahara filter for machine learning tasks such as clustering.\n\nThe Kuwahara filter has been implemented in CVIPtools.\n\nThe Kuwahara filter despite its capabilities in edge preservation has certain drawbacks.\n\n\nThe success of the Kuwahara filter has spurred an increase the development of edge-enhancing smoothing filteres. Several variations have been proposed for similar use most of which attempt to deal with the drawbacks of the original Kuwahara filter.\n\nThe \"Generalized Kuwahara filter\" proposed by P. Bakker considers several windows that contain a fixed pixel. Each window is then assigned an estimate and a confidence value. The value of the fixed pixel then takes the value of the estimate of the window with the highest confidence. This filter is not characterized by the same ambiguity in the presence of noise and manages to eliminate the block artifacts.\n\nThe \"Mean of Least Variance\"(MLV) filter, proposed by M.A. Schulze also produces edge-enhancing smoothing results in images. Similarly to the Kuwahara filter it assumes a window of size formula_14 but instead of searching amongst four subregions of size formula_15 for the one with minimum variance it searches amongst all possible formula_16 subregions. This means the central pixel of the window will be assigned the mean of the one subregion out of a possible formula_17 that has the smallest variance.\n\nA more recent attempt in edge-enhancing smoothing was also proposed by J. E. Kyprianidis. The filter's output is a weighed sum of the local averages with more weight given the averages of more homogenous regions.\n\n", "id": "39903059", "title": "Kuwahara filter"}
{"url": "https://en.wikipedia.org/wiki?curid=1467948", "text": "Problem solving\n\nProblem solving consists of using generic or \"ad hoc\" methods, in an orderly manner, for finding solutions to problems. Some of the problem-solving techniques developed and used in artificial intelligence, computer science, engineering, mathematics, or medicine are related to mental problem-solving techniques studied in psychology.\n\nThe term \"problem solving\" is used in many, many, many disciplines, sometimes with different perspectives, visuals and often with different terminologies. For instance, it is a mental process in psychology and a computerized process in computer science. Problems can also be classified into two different types (ill-defined and well-defined) from which appropriate solutions are to be made. Ill-defined problems are those that do not have clear goals, solution paths, or expected solution. Well-defined problems have specific goals, clearly defined solution paths, and clear expected solutions. These problems also allow for more initial planning than ill-defined problems. Being able to solve problems sometimes involves dealing with pragmatics (logic) and semantics (interpretation of the problem). The ability to understand what the goal of the problem is and what rules could be applied represent the key to solving the problem. Sometimes the problem requires some abstract thinking and coming up with a creative solution.\n\nThomas J. D'Zurilla in 1988 defined problem solving as a “cognitive–affective–behavioral process through which an individual (or group) attempts to identify, discover, or invent effective means of coping with problems encountered in every day living”. It is an evolutionary drive for living organisms and an important coping skill for dealing with a variety of concerns. Problem solving specifically in psychology refers to a state of desire for reaching a definite 'goal' from a present condition that either is not directly moving toward the goal, is far from it, or needs more complex logic for finding a missing description of conditions or steps toward the goal. In each case \"where you want to be\" is an imagined (or written) state in which you would like to be and the solutions are situation- or context-specific. This process includes problem finding or 'problem analysis', problem shaping, generating alternative strategies, implementation and verification of the selected solution. Distinguished feature of a problem is that there is a \"goal\" to be reached and how you get there depends upon problem orientation (problem-solving coping style and skills) and systematic analysis. The nature of human problem solving processes and methods is a field of study and work for mental health professionals. Methods of studying problem solving include introspection, behaviorism, simulation, computer modeling, and experiment. Social psychologists look into the person-environment relationship aspect of the problem and independent and interdependent problem-solving methods. Problem solving has been defined as a higher-order cognitive process and intellectual function that requires the modulation and control of more routine or fundamental skills.\n\nProblem solving has two major domains: mathematical problem solving and personal problem solving both are seen in terms of some difficulty or barrier is encountered. Empirical researches show that self-interest and interpersonal skills; collaborative and instrumental problem approach (it helps in reflective and expansive understanding of the problem situation and its preferable outcome); strategy fluency (the number and diversity of strategies) and conceptual clarity that can lead to an action-identification (Vallacher & Wegner, 1987); temporal lifespan perspective that lead to selectivity in strategy (problem focused and emotion focused strategies); self-efficacy and problem familiarity; formation of 'carry over' relationships (egalitarian friendship, romantic ties, cliques, hygge's, etc.) that helps individuals mutually move through life and provide a sense of identity (Antonucci, Birditt, & Ajrouch, 2011); negotiation; type of relationships (obligatory vs. voluntary); gender typing; problem focused and emotion focused strategies as some strategies and factors that influence everyday problem solving. Neuropsychologists have studied that individuals with frontal lobe injuries with deficits in emotional control and reasoning can be remediated with effective rehabilitation and could improve the capacity of injured persons to resolve everyday problems (Rath, Simon, Langenbahn, Sherr, & Diller, 2003).\n\nInterpersonal everyday problem solving is dependent upon the individual personal motivational and contextual components. One such component is the emotional valence of \"real-world\" problems and it can either impede or aid problem-solving performance. Researchers have focused on the role of emotions in problem solving (D'Zurilla & Goldfried, 1971; D'Zurilla & Nezu, 1982), demonstrating that poor emotional control can disrupt focus on the target task and impede problem resolution and likely lead to negative outcomes such as fatigue, depression, and inertia (Rath, Langenbahn, Simon, Sherr, & Diller, 2004). In conceptualization, human problem solving consists of two related processes: problem orientation, the motivational/attitudinal/affective approach to problematic situations and problem-solving skills. Studies conclude people's strategies cohere with their goals (Hoppmann & Blanchard-Fields, 2010, Berg et al., 1998) and they are stemmed from the natural process of comparing oneself with others (Sonstegard and Bitter, 1998).\n\nThe early experimental work of the Gestaltists in Germany placed the beginning of problem solving study (e.g., Karl Duncker in 1935 with his book \"The psychology of productive thinking\"). Later this experimental work continued through the 1960s and early 1970s with research conducted on relatively simple (but novel for participants) laboratory tasks of problem solving. Choosing simple novel tasks was based on the clearly defined optimal solutions and their short time for solving, which made it possible for the researchers to trace participants' steps in problem-solving process. Researchers' underlying assumption was that simple tasks such as the Tower of Hanoi correspond to the main properties of \"real world\" problems and thus the characteristic cognitive processes within participants' attempts to solve simple problems are the same for \"real world\" problems too; simple problems were used for reasons of convenience and with the expectation that thought generalizations to more complex problems would become possible. Perhaps the best-known and most impressive example of this line of research is the work by Allen Newell and Herbert A. Simon. Other experts have shown that the principle of decomposition improves the ability of the problem solver to make good judgment.\n\nIn computer science and in the part of artificial intelligence that deals with algorithms (\"algorithmics\"), problem solving encompasses a number of techniques known as algorithms, heuristics, root cause analysis, etc. In these disciplines, problem solving is part of a larger process that encompasses problem determination, de-duplication, analysis, diagnosis, repair, etc.\n\nProblem solving is used in when products or processes fail, so corrective action can be taken to prevent further failures. It can also be applied to a product or process prior to an actual fail event, i.e., when a potential problem can be predicted and analyzed, and mitigation applied so the problem never actually occurs. Techniques such as Failure Mode Effects Analysis can be used to proactively reduce the likelihood of problems occurring.\n\nIn military science, problem solving is linked to the concept of \"end-states\", the desired condition or situation that strategists wish to generate. The ability to solve problems is important at any military rank, but is highly critical at the command and control level, where it is strictly correlated to the deep understanding of qualitative and quantitative scenarios. \"Effectiveness\" of problem solving is \"a criterion used to assess changes in system behavior, capability, or operational environment that is tied to measuring the attainment of an end state, achievement of an objective, or creation of an effect\". \"Planning\" for problem-solving is a \"process that determines and describes how to employ 'means' in specific 'ways' to achieve 'ends' (the problem's solution).\"\n\nForensic engineering is an important technique of failure analysis that involves tracing product defects and flaws. Corrective action can then be taken to prevent further failures.\n\nReverse engineering attempts to discover the original problem-solving logic used in developing a product by taking it apart.\n\nOther problem solving tools are linear and nonlinear programming, queuing systems, and simulation.\n\nProblem-solving strategies are the steps that one would use to find the problem(s) that are in the way to getting to one's own goal. Firend's problem solving model (PSM) is practical in application and incorporates the conventional 5WH approach, with a systematic process of investigation, implementation and assessment cycle. Some would refer to this as the \"problem-solving cycle\" (Bransford & Stein, 1993). In this cycle one will recognize the problem, define the problem, develop a strategy to fix the problem, organize the knowledge of the problem cycle, figure out the resources at the user's disposal, monitor one's progress, and evaluate the solution for accuracy. The reason it is called a cycle is that once one is completed with a problem another usually will pop up.\n\nBlanchard-Fields (2007) looks at problem solving from one of two facets. The first looking at those problems that only have one solution (like mathematical problems, or fact-based questions) which are grounded in psychometric intelligence. The other that is socioemotional in nature and are unpredictable with answers that are constantly changing (like what's your favorite color or what you should get someone for Christmas).\n\nThe following techniques are usually called \"problem-solving strategies'\"\n\n\n\nCommon barriers to problem solving are mental constructs that impede our ability to correctly solve problems. These barriers prevent people from solving problems in the most efficient manner possible. Five of the most common processes and factors that researchers have identified as barriers to problem solving are confirmation bias, mental set, functional fixedness, unnecessary constraints, and irrelevant information.\n\nWithin the field of science there exists a set of fundamental standards, the scientific method, which outlines the process of discovering facts or truths about the world through unbiased consideration of all pertinent information and through impartial observation of and/or experimentation with that information. According to this method, one is able to most accurately find a solution to a perceived problem by performing the aforementioned steps. The scientific method does not prescribe a process that is limited to scientists, but rather one that all people can practice in their respective fields of work as well as in their personal lives. Confirmation bias can be described as one's unconscious or unintentional corruption of the scientific method. Thus when one demonstrates confirmation bias, one is formally or informally collecting data and then subsequently observing and experimenting with that data in such a way that favors a preconceived notion that may or may not have \"motivation\". Research has found that professionals within scientific fields of study also experience confirmation bias. Andreas Hergovich, Reinhard Schott, and Christoph Burger's experiment conducted online, for instance, suggested that professionals within the field of psychological research are likely to view scientific studies that are congruent with their preconceived understandings more favorably than studies that are incongruent with their established beliefs.\n\nMotivation refers to one's desire to defend or find substantiation for beliefs (e.g., religious beliefs) that are important to one. According to Raymond Nickerson, one can see the consequences of confirmation bias in real-life situations, which range in severity from inefficient government policies to genocide. With respect to the latter and most severe ramification of this cognitive barrier, Nickerson argued that those involved in committing genocide of persons accused of witchcraft, an atrocity that occurred from the 15th to 17th centuries, demonstrated confirmation bias with motivation. Researcher Michael Allen found evidence for confirmation bias with motivation in school children who worked to manipulate their science experiments in such a way that would produce their hoped for results. However, confirmation bias does not necessarily require motivation. In 1960, Peter Cathcart Wason conducted an experiment in which participants first viewed three numbers and then created a hypothesis that proposed a rule that could have been used to create that triplet of numbers. When testing their hypotheses, participants tended to only create additional triplets of numbers that would confirm their hypotheses, and tended not to create triplets that would negate or disprove their hypotheses. Thus research also shows that people can and do work to confirm theories or ideas that do not support or engage personally significant beliefs.\n\nMental set was first articulated by Abraham Luchins in the 1940s and demonstrated in his well-known water jug experiments. In these experiments, participants were asked to fill one jug with a specific amount of water using only other jugs (typically three) with different maximum capacities as tools. After Luchins gave his participants a set of water jug problems that could all be solved by employing a single technique, he would then give them a problem that could either be solved using that same technique or a novel and simpler method. Luchins discovered that his participants tended to use the same technique that they had become accustomed to despite the possibility of using a simpler alternative. Thus mental set describes one's inclination to attempt to solve problems in such a way that has proved successful in previous experiences. However, as Luchins' work revealed, such methods for finding a solution that have worked in the past may not be adequate or optimal for certain new but similar problems. Therefore, it is often necessary for people to move beyond their mental sets in order to find solutions. This was again demonstrated in Norman Maier's 1931 experiment, which challenged participants to solve a problem by using a household object (pliers) in an unconventional manner. Maier observed that participants were often unable to view the object in a way that strayed from its typical use, a phenomenon regarded as a particular form of mental set (more specifically known as functional fixedness, which is the topic of the following section). When people cling rigidly to their mental sets, they are said to be experiencing \"fixation\", a seeming obsession or preoccupation with attempted strategies that are repeatedly unsuccessful. In the late 1990s, researcher Jennifer Wiley worked to reveal that expertise can work to create a mental set in persons considered to be experts in certain fields, and she furthermore gained evidence that the mental set created by expertise could lead to the development of fixation.\n\nFunctional fixedness is a specific form of mental set and fixation, which was alluded to earlier in the Maier experiment, and furthermore it is another way in which cognitive bias can be seen throughout daily life. Tim German and Clark Barrett describe this barrier as the fixed design of an object hindering the individual's ability to see it serving other functions. In more technical terms, these researchers explained that \"[s]ubjects become \"fixed\" on the design function of the objects, and problem solving suffers relative to control conditions in which the object's function is not demonstrated.\" Functional fixedness is defined as only having that primary function of the object itself hinder the ability of it serving another purpose other than its original function. In research that highlighted the primary reasons that young children are immune to functional fixedness, it was stated that \"functional fixedness...[is when]subjects are hindered in reaching the solution to a problem by their knowledge of an object's conventional function.\" Furthermore, it is important to note that functional fixedness can be easily expressed in commonplace situations. For instance, imagine the following situation: a man sees a bug on the floor that he wants to kill, but the only thing in his hand at the moment is a can of air freshener. If the man starts looking around for something in the house to kill the bug with instead of realizing that the can of air freshener could in fact be used not only as having its main function as to freshen the air, he is said to be experiencing functional fixedness. The man's knowledge of the can being served as purely an air freshener hindered his ability to realize that it too could have been used to serve another purpose, which in this instance was as an instrument to kill the bug. Functional fixedness can happen on multiple occasions and can cause us to have certain cognitive biases. If we only see an object as serving one primary focus than we fail to realize that the object can be used in various ways other than its intended purpose. This can in turn cause many issues with regards to problem solving. Common sense seems to be a plausible answer to functional fixedness. One could make this argument because it seems rather simple to consider possible alternative uses for an object. Perhaps using common sense to solve this issue could be the most accurate answer within this context. With the previous stated example, it seems as if it would make perfect sense to use the can of air freshener to kill the bug rather than to search for something else to serve that function but, as research shows, this is often not the case.\n\nFunctional fixedness limits the ability for people to solve problems accurately by causing one to have a very narrow way of thinking. Functional fixedness can be seen in other types of learning behaviors as well. For instance, research has discovered the presence of functional fixedness in many educational instances. Researchers Furio, Calatayud, Baracenas, and Padilla stated that \"... functional fixedness may be found in learning concepts as well as in solving chemistry problems.\" There was more emphasis on this function being seen in this type of subject and others.\n\nThere are several hypotheses in regards to how functional fixedness relates to problem solving. There are also many ways in which a person can run into problems while thinking of a particular object with having this function. If there is one way in which a person usually thinks of something rather than multiple ways then this can lead to a constraint in how the person thinks of that particular object. This can be seen as narrow minded thinking, which is defined as a way in which one is not able to see or accept certain ideas in a particular context. Functional fixedness is very closely related to this as previously mentioned. This can be done intentionally and or unintentionally, but for the most part it seems as if this process to problem solving is done in an unintentional way.\n\nFunctional fixedness can affect problem solvers in at least two particular ways. The first is with regards to time, as functional fixedness causes people to use more time than necessary to solve any given problem. Secondly, functional fixedness often causes solvers to make more attempts to solve a problem than they would have made if they were not experiencing this cognitive barrier. In the worst case, functional fixedness can completely prevent a person from realizing a solution to a problem. Functional fixedness is a commonplace occurrence, which affects the lives of many people.\n\nUnnecessary constraints are another very common barrier that people face while attempting to problem-solve. This particular phenomenon occurs when the subject, trying to solve the problem subconsciously, places boundaries on the task at hand, which in turn forces him or her to strain to be more innovative in their thinking. The solver hits a barrier when they become fixated on only one way to solve their problem, and it becomes increasingly difficult to see anything but the method they have chosen. Typically, the solver experiences this when attempting to use a method they have already experienced success from, and they can not help but try to make it work in the present circumstances as well, even if they see that it is counterproductive.\n\nGroupthink, or taking on the mindset of the rest of the group members, can also act as an unnecessary constraint while trying to solve problems. This is due to the fact that with everybody thinking the same thing, stopping on the same conclusions, and inhibiting themselves to think beyond this. This is very common, but the most well-known example of this barrier making itself present is in the famous example of the dot problem. In this example, there are nine dots lying in a square- three dots across, and three dots running up and down. The solver is then asked to draw no more than four lines, without lifting their pen or pencil from the paper. This series of lines should connect all of the dots on the paper. Then, what typically happens is the subject creates an assumption in their mind that they must connect the dots without letting his or her pen or pencil go outside of the square of dots. Standardized procedures like this can often bring mentally invented constraints of this kind, and researchers have found a 0% correct solution rate in the time allotted for the task to be completed. The imposed constraint inhibits the solver to think beyond the bounds of the dots. It is from this phenomenon that the expression \"think outside the box\" is derived.\n\nThis problem can be quickly solved with a dawning of realization, or \"insight\". A few minutes of struggling over a problem can bring these sudden insights, where the solver quickly sees the solution clearly. Problems such as this are most typically solved via insight and can be very difficult for the subject depending on either how they have structured the problem in their minds, how they draw on their past experiences, and how much they juggle this information in their working memories In the case of the nine-dot example, the solver has already been structured incorrectly in their minds because of the constraint that they have placed upon the solution. In addition to this, people experience struggles when they try to compare the problem to their prior knowledge, and they think they must keep their lines within the dots and not go beyond. They do this because trying to envision the dots connected outside of the basic square puts a strain on their working memory.\n\nLuckily, the solution to the problem becomes obvious as insight occurs following incremental movements made toward the solution. These tiny movements happen without the solver knowing. Then when the insight is realized fully, the \"aha\" moment happens for the subject. These moments of insight can take a long while to manifest or not so long at other times, but the way that the solution is arrived at after toiling over these barriers stays the same.\n\nIrrelevant information is information presented within a problem that is unrelated or unimportant to the specific problem. Within the specific context of the problem, irrelevant information would serve no purpose in helping solve that particular problem. Often \"irrelevant information\" is detrimental to the problem solving process. It is a common barrier that many people have trouble getting through, especially if they are not aware of it. \"Irrelevant information\" makes solving otherwise relatively simple problems much harder.\n\nFor example: \"Fifteen percent of the people in Topeka have unlisted telephone numbers. You select 200 names at random from the Topeka phone book. How many of these people have unlisted phone numbers?\"\n\nThe people that are not listed in the phone book would not be among the 200 names you selected. The individuals looking at this task would have naturally wanted to use the 15% given to them in the problem. They see that there is information present and they immediately think that it needs to be used. This of course is not true. These kinds of questions are often used to test students taking aptitude tests or cognitive evaluations. They aren't meant to be difficult but they are meant to require thinking that is not necessarily common. \"Irrelevant Information\" is commonly represented in math problems, word problems specifically, where numerical information is put for the purpose of challenging the individual.\n\nOne reason irrelevant information is so effective at keeping a person off topic and away from the relevant information, is in how it is represented. The way information is represented can make a vast difference in how difficult the problem is to be overcome. Whether a problem is represented visually, verbally, spatially, or mathematically, irrelevant information can have a profound effect on how long a problem takes to be solved; or if it's even possible. The Buddhist monk problem is a classic example of irrelevant information and how it can be represented in different ways:\n\nThis problem is near impossible to solve because of how the information is represented. Because it is written out in a way that represents the information verbally, it causes us to try and create a mental image of the paragraph. This is often very difficult to do especially with all the \"irrelevant information\" involved in the question. This example is made much easier to understand when the paragraph is represented visually. Now if the same problem was asked, but it was also accompanied by a corresponding graph, it would be far easier to answer this question; \"irrelevant information\" no longer serves as a road block. By representing the problem visually, there are no difficult words to understand or scenarios to imagine. The visual representation of this problem has removed the difficulty of solving it.\n\nThese types of representations are often used to make difficult problems easier. They can be used on tests as a strategy to remove \"Irrelevant Information,\" which is one of the most common forms of barriers when discussing the issues of problem solving. Identifying crucial information presented in a problem and then being able to correctly identify its usefulness is essential. Being aware of \"irrelevant information\" is the first step in overcoming this common barrier.\n\nIn cognitive sciences, researchers' realization that problem-solving processes differ across knowledge domains and across levels of expertise (e.g. Sternberg, 1995) and that, consequently, findings obtained in the laboratory cannot necessarily generalize to problem-solving situations outside the laboratory, has led to an emphasis on real-world problem solving since the 1990s. This emphasis has been expressed quite differently in North America and Europe, however. Whereas North American research has typically concentrated on studying problem solving in separate, natural knowledge domains, much of the European research has focused on novel, complex problems, and has been performed with computerized scenarios (see Funke, 1991, for an overview).\n\nIn Europe, two main approaches have surfaced, one initiated by Donald Broadbent (1977; see Berry & Broadbent, 1995) in the United Kingdom and the other one by Dietrich Dörner (1975, 1985; see Dörner & Wearing, 1995) in Germany. The two approaches share an emphasis on relatively complex, semantically rich, computerized laboratory tasks, constructed to resemble real-life problems. The approaches differ somewhat in their theoretical goals and methodology, however. The tradition initiated by Broadbent emphasizes the distinction between cognitive problem-solving processes that operate under awareness versus outside of awareness, and typically employs mathematically well-defined computerized systems. The tradition initiated by Dörner, on the other hand, has an interest in the interplay of the cognitive, motivational, and social components of problem solving, and utilizes very complex computerized scenarios that contain up to 2,000 highly interconnected variables (e.g., Dörner, Kreuzig, Reither & Stäudel's 1983 LOHHAUSEN project; Ringelband, Misiak & Kluwe, 1990). Buchner (1995) describes the two traditions in detail.\n\nIn North America, initiated by the work of Herbert A. Simon on \"learning by doing\" in semantically rich domains (e.g. Anzai & Simon, 1979; Bhaskar & Simon, 1977), researchers began to investigate problem solving separately in different natural knowledge domains – such as physics, writing, or chess playing – thus relinquishing their attempts to extract a global theory of problem solving (e.g. Sternberg & Frensch, 1991). Instead, these researchers have frequently focused on the development of problem solving within a certain domain, that is on the development of expertise (e.g. Anderson, Boyle & Reiser, 1985; Chase & Simon, 1973; Chi, Feltovich & Glaser, 1981).\n\nAreas that have attracted rather intensive attention in North America include:\n\nAs elucidated by Dietrich Dörner and later expanded upon by , complex problems have some typical characteristics that can be summarized as follows:\n\n\nProblem solving is applied on many different levels − from the individual to the civilizational. Collective problem solving refers to problem solving performed collectively.\n\nSocial issues and global issues can typically only be solved collectively.\n\nIt has been noted that the complexity of contemporary problems has exceeded the cognitive capacity of any individual and requires different but complementary expertise and collective problem solving ability.\n\nCollective intelligence is shared or group intelligence that emerges from the collaboration, collective efforts, and competition of many individuals.\n\nIn a 1962 research report, Douglas Engelbart linked collective intelligence to organizational effectiveness, and predicted that pro-actively 'augmenting human intellect' would yield a multiplier effect in group problem solving: \"Three people working together in this augmented mode [would] seem to be more than three times as effective in solving a complex problem as is one augmented person working alone\".\n\nHenry Jenkins, a key theorist of new media and media convergence draws on the theory that collective intelligence can be attributed to media convergence and participatory culture. He criticizes contemporary education for failing to incorporate online trends of collective problem solving into the classroom, stating \"whereas a collective intelligence community encourages ownership of work as a group, schools grade individuals\". Jenkins argues that interaction within a knowledge community builds vital skills for young people, and teamwork through collective intelligence communities contribute to the development of such skills.\n\nCollective impact is the commitment of a group of actors from different sectors to a common agenda for solving a specific social problem, using a structured form of collaboration.\n\nAfter World War II the UN, the Bretton Woods organization and the WTO were created and collective problem solving on the international level crystallized since the 1980s around these 3 types of organizations. As these global institutions remain state-like or state-centric it has been called unsurprising that these continue state-like or state-centric approaches to collective problem-solving rather than alternative ones.\n\nIt has been observed that models of liberal democracy provide neither adequate designs for collective problem solving nor handling the substantive challenges in society such as crime, war, economic decline, illness and environmental degradation to produce satisfying outcomes.\n\nCrowdsourcing is a process of accumulating the ideas, thoughts or information from many independent participants, with aim to find the best solution for a given challenge. Modern information technologies allow for massive number of subjects to be involved as well as systems of managing these suggestions that provide good results. With the Internet a new capacity for collective, including planetary-scale, problem solving was created.\n\n", "id": "1467948", "title": "Problem solving"}
{"url": "https://en.wikipedia.org/wiki?curid=40267442", "text": "MNIST database\n\nThe MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems. The database is also widely used for training and testing in the field of machine learning. It was created by \"re-mixing\" the samples from NIST's original datasets. The creators felt that since NIST's training dataset was taken from American Census Bureau employees, while the testing dataset was taken from American high school students, it was not well-suited for machine learning experiments. Furthermore, the black and white images from NIST were normalized to fit into a 28x28 pixel bounding box and anti-aliased, which introduced grayscale levels.\nThe MNIST database contains 60,000 training images and 10,000 testing images. Half of the training set and half of the test set were taken from NIST's training dataset, while the other half of the training set and the other half of the test set were taken from NIST's testing dataset.\nThere have been a number of scientific papers on attempts to achieve the lowest error rate; one paper, using a hierarchical system of convolutional neural networks, manages to get an error rate on the MNIST database of 0.23 percent. The original creators of the database keep a list of some of the methods tested on it. In their original paper, they use a support vector machine to get an error rate of 0.8 percent. An extended dataset similar to MNIST called EMNIST has been published in 2017, which contains 240,000 training images, and 40,000 testing images of handwritten digits.\n\nThe set of images in the MNIST database is a combination of two of NIST's databases: Special Database 1 and Special Database 3. Special Database 1 and Special Database 3 consist of digits written by high school students and employees of the United States Census Bureau, respectively.\n\nSome researchers have achieved \"near-human performance\" on the MNIST database, using a committee of neural networks; in the same paper, the authors achieve performance double that of humans on other recognition tasks. The highest error rate listed on the original website of the database is 12 percent, which is achieved using a simple linear classifier with no preprocessing.\n\nIn 2004, a best-case error rate of 0.42 percent was achieved on the database by researchers using a new classifier called the LIRA, which is a neural classifier with three neuron layers based on Rosenblatt's perceptron principles.\n\nSome researchers have tested artificial intelligence systems using the database put under random distortions. The systems in these cases are usually neural networks and the distortions used tend to be either affine distortions or elastic distortions. Sometimes, these systems can be very successful; one such system achieved an error rate on the database of 0.39 percent.\n\nIn 2011, an error rate of 0.27 percent, improving on the previous best result, was reported by researchers using a similar system of neural networks. In 2013, an approach based on regularization of neural networks using DropConnect has been claimed to achieve a 0.21 percent error rate. Recently, the single convolutional neural network best performance was 0.31 percent error rate. Currently, the best performance of a single convolutional neural network trained in 74 epochs on the expanded training data is 0.27 percent error rate. Also, the Parallel Computing Center (Khmelnitskiy, Ukraine) obtained an ensemble of only 5 convolutional neural networks which performs on MNIST at 0.21 percent error rate.\n\nThis is a table of some of the machine learning methods used on the database and their error rates, by type of classifier:\n\n", "id": "40267442", "title": "MNIST database"}
{"url": "https://en.wikipedia.org/wiki?curid=40196540", "text": "Histogram of oriented displacements\n\nHistogram of Oriented Displacements (HOD) is a 2D trajectory descriptor. The trajectory is described using a histogram of the directions between each two consecutive points. Given a trajectory T = {P, P, P, ..., P}, where P is the 2D position at time t. For each pair of positions P and P, calculate the direction angle θ(t, t+1). Value of θ is between 0 and 360. A histogram of the quantized values of θ is created. If the histogram is of 8 bins, the first bin represents all θs between 0 and 45.\n\nThe histogram accumulates the lengths of the consecutive moves. For each θ, a specific histogram bin is determined. The length of the line between P and P is then added to the specific histogram bin.\n\nTo show the intuition behind the descriptor, consider the action of waving hands. At the end of the action, the hand falls down. When describing this down movement, the descriptor does not care about the position from which the hand started to fall. This fall will affect the histogram with the appropriate angles and lengths, regardless of the position where the hand started to fall.\n\nHOD records for each moving point: how much it moves in each range of directions. HOD has a clear physical interpretation. It proposes that, a simple way to describe the motion of an object, is to indicate how much distance it moves in each direction. If the movement in all directions are saved accurately, the movement can be repeated from the initial position to the final destination regardless of the displacements order. However, the temporal information will be lost, as the order of movements is not stored-this is what we solve by applying the temporal pyramid, as shown in section \\ref{sec:temp-pyramid}. If the angles quantization range is small, classifiers that use the descriptor will overfit. Generalization needs some slack in directions-which can be done by increasing the quantization range.\n\n", "id": "40196540", "title": "Histogram of oriented displacements"}
{"url": "https://en.wikipedia.org/wiki?curid=40615979", "text": "Voice Mate\n\nVoice Mate formerly called \"Quick Voice\" and later on as \"Q Voice\" is an intelligent personal assistant and knowledge navigator which is only available as a built-in application for various LG smartphones. The application uses a natural language user interface to answer questions, make recommendations, and perform actions by delegating requests to a set of Web services. It is based on the Maluuba personal assistant.\n\nSome of the capabilities of Voice Mate include making appointments, opening apps, setting alarms, updating social network websites such as Facebook or Twitter and navigation. Voice Mate also offers efficient multitasking as well as automatic activation features, for example when the car engine is started.\n", "id": "40615979", "title": "Voice Mate"}
{"url": "https://en.wikipedia.org/wiki?curid=40647601", "text": "Syman\n\nSYMAN is an artificial intelligence technology that uses data from social media profiles to identify trends in the job market. SYMAN is designed to organize actionable data for products and services including recruiting, human capital management, CRM, and marketing.\n\nSYMAN was developed with a $21 million series B financing round secured by Identified, which was led by VantagePoint Capital Partners and Capricorn Investment Group.\n\nIdentified\n", "id": "40647601", "title": "Syman"}
{"url": "https://en.wikipedia.org/wiki?curid=12476035", "text": "Moravec's paradox\n\nMoravec's paradox is the discovery by artificial intelligence and robotics researchers that, contrary to traditional assumptions, high-level reasoning requires very little computation, but low-level sensorimotor skills require enormous computational resources. The principle was articulated by Hans Moravec, Rodney Brooks, Marvin Minsky and others in the 1980s. As Moravec writes, \"it is comparatively easy to make computers exhibit adult level performance on intelligence tests or playing checkers, and difficult or impossible to give them the skills of a one-year-old when it comes to perception and mobility.\"\n\nSimilarly, Marvin Minsky emphasized that the most difficult human skills to reverse engineer are those that are \"unconscious\". \"In general, we're least aware of what our minds do best,\" he wrote, and added \"we're more aware of simple processes that don't work well than of complex ones that work flawlessly.\"\n\nOne possible explanation of the paradox, offered by Moravec, is based on evolution. All human skills are implemented biologically, using machinery designed by the process of natural selection. In the course of their evolution, natural selection has tended to preserve design improvements and optimizations. The older a skill is, the more time natural selection has had to improve the design. Abstract thought developed only very recently, and consequently, we should not expect its implementation to be particularly efficient.\n\nAs Moravec writes:\nA compact way to express this argument would be:\n\nSome examples of skills that have been evolving for millions of years: recognizing a face, moving around in space, judging people’s motivations, catching a ball, recognizing a voice, setting appropriate goals, paying attention to things that are interesting; anything to do with perception, attention, visualization, motor skills, social skills and so on.\n\nSome examples of skills that have appeared more recently: mathematics, engineering, human games, logic and scientific reasoning. These are hard for us because they are not what our bodies and brains were primarily evolved to do. These are skills and techniques that were acquired recently, in historical time, and have had at most a few thousand years to be refined, mostly by cultural evolution.\n\nIn the early days of artificial intelligence research, leading researchers often predicted that they would be able to create thinking machines in just a few decades (see history of artificial intelligence). Their optimism stemmed in part from the fact that they had been successful at writing programs that used logic, solved algebra and geometry problems and played games like checkers and chess. Logic and algebra are difficult for people and are considered a sign of intelligence. They assumed that, having (almost) solved the \"hard\" problems, the \"easy\" problems of vision and commonsense reasoning would soon fall into place. They were wrong, and one reason is that these problems are not easy at all, but incredibly difficult. The fact that they had solved problems like logic and algebra was irrelevant, because these problems are extremely easy for machines to solve.\n\nRodney Brooks explains that, according to early AI research, intelligence was \"best characterized as the things that highly educated male scientists found challenging\", such as chess, symbolic integration, proving mathematical theorems and solving complicated word algebra problems. \"The things that children of four or five years could do effortlessly, such as visually distinguishing between a coffee cup and a chair, or walking around on two legs, or finding their way from their bedroom to the living room were not thought of as activities requiring intelligence.\"\n\nThis would lead Brooks to pursue a new direction in artificial intelligence and robotics research. He decided to build intelligent machines that had \"No cognition. Just sensing and action. That is all I would build and completely leave out what traditionally was thought of as the \"intelligence\" of artificial intelligence.\" This new direction, which he called \"Nouvelle AI\" was highly influential on robotics research and AI.\n\nLinguist and cognitive scientist Steven Pinker considers this the main lesson uncovered by AI researchers. In his book \"The Language Instinct\", he writes:\n\n\n", "id": "12476035", "title": "Moravec's paradox"}
{"url": "https://en.wikipedia.org/wiki?curid=35457079", "text": "Argumentation framework\n\nIn artificial intelligence and related fields, an argumentation framework, or argumentation system, is a way to deal with contentious information and draw conclusions from it.\n\nIn an abstract argumentation framework, entry-level information is a set of abstract arguments that, for instance, represent data or a proposition. Conflicts between arguments are represented by a binary relation on the set of arguments. In concrete terms, you represent an argumentation framework with a directed graph such that the nodes are the arguments, and the arrows represent the attack relation.\nThere exist some extensions of the Dung's framework, like the logic-based argumentation frameworks or the value-based argumentation frameworks.\n\nAbstract argumentation frameworks, also called argumentation frameworks \"à la Dung\", are defined formally as a pair:\nFor instance, the argumentation system formula_4 with formula_5 and formula_6 contains four arguments (formula_7 and formula_8) and three attacks (formula_9 attacks formula_10, formula_10 attacks formula_12 and formula_8 attacks formula_12).\n\nDung defines some notions :\n\nTo decide if an argument can be accepted or not, or if several arguments can be accepted together, Dung defines several semantics of acceptance that allow, given an argumentation system, to compute sets of arguments, called \"extensions\". For instance, given formula_26,\nThere exists some inclusions between the sets of extensions built with these semantics :\n\nSome other semantics have been defined.\n\nOne introduce the notation formula_44 to note the set of formula_45-extensions of the system formula_28.\nIn the case of the system formula_28 in the figure above, formula_48 for every Dung's semantic—the system is well-founded. That explains why the semantics coincide, and the accepted arguments are: formula_9 and formula_8.\n\nLabellings are a more expressive way than extensions to express the acceptance of the arguments. Concretely, a labelling is a mapping that associates every argument with a label \"in\" (the argument is accepted), \"out\" (the argument is rejected), or \"undec\" (the argument is undefined—not accepted or refused).\nOne can also note a labelling as a set of pairs formula_51.\n\nSuch a mapping does not make sense without additional constraint. The notion of reinstatement labelling guarantees the sense of the mapping. formula_52 is a reinstatement labelling on the system formula_4 if and only if :\n\nOne can convert every extension into a reinstatement labelling: the arguments of the extension are \"in\", those attacked by an argument of the extension are \"out\", and the others are \"undec\". Conversely, one can build an extension from a reinstatement labelling just by keeping the arguments \"in\". Indeed, Caminada proved that the reinstatement labellings and the complete extensions can be mapped in a bijective way. Moreover, the other Datung's semantics can be associated to some particular sets of reinstatement labellings.\n\nReinstatement labellings distinguish arguments not accepted because they are attacked by accepted arguments from undefined arguments—that is, those that are not defended cannot defend themselves. An argument is \"undec\" if it is attacked by at least another \"undec\". If it is attacked only by arguments \"out\", it must be \"in\", and if it is attacked only by arguments \"in\", then it is \"out\".\n\nThe unique reinstatement labelling that corresponds to the system formula_28 above is formula_65.\n\nIn the general case when several extensions are computed for a given semantic formula_45, the agent that reasons from the system can use several mechanisms to infer information:\n\nFor these two methods to infer information, one can identify the set of accepted arguments, respectively formula_73 the set of the arguments credulously accepted under the semantic formula_45, and formula_75 the set of arguments accepted skeptically under the semantic formula_45 (the formula_45 can be missed if there is no possible ambiguity about the semantic).\n\nOf course, when there is only one extension (for instance, when the system is well-founded), this problem is very simple: the agent accepts arguments of the unique extension and rejects others.\n\nThe same reasoning can be done with labellings that correspond to the chosen semantic : an argument can be accepted if it is \"in\" for each labelling and refused if it is \"out\" for each labelling, the others being in an undecided state (the status of the arguments can remind the epistemic states of a belief in the AGM framework for dynamic of beliefs).\n\nThere exists several criteria of equivalence between argumentation frameworks. Most of those criteria concern the sets of extensions or the set of accepted arguments.\nFormally, given a semantic formula_45 :\n\nThe strong equivalence says that two systems formula_86 and formula_87 are equivalent if and only if for all other system formula_88, the union of formula_86 with formula_88 is equivalent (for a given criterion) with the union of formula_87 and formula_88.\n\nThe abstract framework of Dung has been instantiated to several particular cases.\n\nIn the case of logic-based argumentation frameworks, an argument is not an abstract entity, but a pair, where the first part is a minimal consistent set of formulae enough to prove the formula for the second part of the argument.\nFormally, an argument is a pair formula_93 such that\n\nOne calls formula_98 a consequence of formula_96, and formula_96 a support of formula_98.\n\nIn this case, the attack relation is not given in an explicit way, as a subset of the Cartesian product formula_104, but as a property that indicates if an argument attacks another. For instance,\n\nGiven a particular attack relation, one can build a graph and reason in a similar way to the abstract argumentation frameworks (use of semantics to build extension, skeptical or credulous inference), the difference is that the information inferred from a logic based argumentation framework is a set of formulae (the consequences of the accepted arguments).\n\nThe value-based argumentation frameworks come from the idea that during an exchange of arguments, some can be \"stronger\" than others with respect to a certain value they advance, and so the success of an attack between arguments depends of the difference of these values.\n\nFormally, a value-based argumentation framework is a tuple formula_116 with formula_1 and formula_3 similar to the standard framework (a set of arguments and a binary relation on this set), formula_119 is a non empty set of values, formula_120 is a mapping that associates each element from formula_1 to an element from formula_119, and formula_123 is a preference relation (transitive, irreflexive and asymmetric) on formula_124.\n\nIn this framework, an argument formula_9 defeats another argument formula_10 if and only if\nOne remarks that an attack succeeds if both arguments are associated to the same value, or if there is no preference between their respective values.\n\nIn assumption-based argumentation frameworks, arguments are defined as a set of rules and attacks are defined in terms of assumptions and contraries.\n\nFormally, an assumption-based argumentation framework is a tuple formula_133, where\nAs a consequence of defining an ABA, an argument can be represented in a tree-form. Formally, given a deductive system formula_134 and set of assumptions formula_141, an argument for claim formula_149 supported by formula_150, is a tree with nodes labelled by sentences in formula_135 or by symbol formula_152, such that:\nAn argument with claim formula_12 supported by a set of assumption formula_28 can also be denoted as formula_173\n\n", "id": "35457079", "title": "Argumentation framework"}
{"url": "https://en.wikipedia.org/wiki?curid=11081176", "text": "Mind–body problem\n\nThe mind–body problem is a philosophical problem concerning the relationship between the human mind and body, although it can also concern animal minds, if any, and animal bodies. It is distinct from the question how mind and body can causally interact, since that question presupposes an interactionist account of mind-body relations. This question arises when mind and body are considered as distinct, based on the premise that the mind and the body are fundamentally different in nature.\n\nThe problem was addressed by René Descartes in the 17th century, resulting in Cartesian dualism, and by pre-Aristotelian philosophers, in Avicennian philosophy, and in earlier Asian traditions. A variety of approaches have been proposed. Most are either dualist or monist. Dualism maintains a rigid distinction between the realms of mind and matter. Monism maintains that there is only one unifying reality, substance or essence in terms of which everything can be explained.\n\nEach of these categories contain numerous variants. The two main forms of dualism are substance dualism, which holds that the mind is formed of a distinct type of substance not governed by the laws of physics, and property dualism, which holds that mental properties involving conscious experience are fundamental properties, alongside the fundamental properties identified by a completed physics. The three main forms of monism are physicalism, which holds that the mind consists of matter organized in a particular way; idealism, which holds that only thought truly exists and matter is merely an illusion; and neutral monism, which holds that both mind and matter are aspects of a distinct essence that is itself identical to neither of them.\n\nSeveral philosophical perspectives have been developed which reject the mind–body dichotomy. The historical materialism of Karl Marx and subsequent writers, itself a form of physicalism, held that consciousness was engendered by the material contingencies of one's environment. An explicit rejection of the dichotomy is found in French structuralism, and is a position that generally characterized post-war French philosophy.\n\nThe absence of an empirically identifiable meeting point between the non-physical mind (if there is such a thing) and its physical extension has proven problematic to dualism, and many modern philosophers of mind maintain that the mind is not something separate from the body. These approaches have been particularly influential in the sciences, particularly in the fields of sociobiology, computer science, evolutionary psychology, and the neurosciences.\n\nAn ancient model of the mind known as the Five-Aggregate Model explains the mind as continuously changing sense impressions and mental phenomena. Considering this model, it is possible to understand that it is the constantly changing sense impressions and mental phenomena (i.e., the mind) that experiences/analyzes all external phenomena in the world as well as all internal phenomena including the body anatomy, the nervous system as well as the organ brain. This conceptualization leads to two levels of analyses: (i) analyses conducted from a third-person perspective on how the brain works, and (ii) analyzing the moment-to-moment manifestation of an individual’s mind-stream (analyses conducted from a first-person perspective). Considering the latter, the manifestation of the mind-stream is described as happening in every person all the time, even in a scientist who analyses various phenomena in the world, including analyzing and hypothesizing about the organ brain.\n\nPhilosophers David L. Robb and John H. Heil introduce mental causation in terms of the mind–body problem of interaction:\nContemporary neurophilosopher, Georg Northoff suggests that mental causation is compatible with classical formal and final causality.\n\nBiologist, theoretical neuroscientist and philosopher, Walter J. Freeman, suggests that explaining mind–body interaction in terms of \"circular causation\" is more relevant than linear causation.\n\nIn neuroscience, much has been learned about correlations between brain activity and subjective, conscious experiences. Many suggest that neuroscience will ultimately explain consciousness:\n\"...consciousness is a biological process that will eventually be explained in terms of molecular signaling pathways used by interacting populations of nerve cells...\" However, this view has been criticized because \"consciousness\" has yet to be shown to be a \"process\", and the \"hard problem\" of relating consciousness directly to brain activity remains elusive.\n\nThe \"neural correlates of consciousness\" \"are the smallest set of brain mechanisms and events sufficient for some specific conscious feeling, as elemental as the color red or as complex as the sensual, mysterious, and primeval sensation evoked when looking at [a] jungle scene...\" Neuroscientists use empirical approaches to discover neural correlates of subjective phenomena.\n\nA science of consciousness must explain the exact relationship between subjective conscious mental states and brain states formed by electrochemical interactions in the body, the so-called hard problem of consciousness. \nNeurobiology studies the connection scientifically, as do neuropsychology and neuropsychiatry. \"Neurophilosophy\" is the interdisciplinary study of neuroscience and philosophy of mind. In this pursuit, neurophilosophers, such as Patricia Churchland,\nPaul Churchland \nand Daniel Dennett, \nhave focused primarily on the body rather than the mind. In this context, neuronal correlates may be viewed as causing consciousness, where consciousness can be thought of as an undefined property that depends upon this complex, adaptive, and highly interconnected biological system. However, it's unknown if discovering and characterizing neural correlates may eventually provide a theory of consciousness that can explain the first-person experience of these \"systems\", and determine whether other systems of equal complexity lack such features.\n\nThe massive parallelism of neural networks allows redundant populations of neurons to mediate the same or similar percepts. Nonetheless, it is assumed that every subjective state will have associated neural correlates, which can be manipulated to artificially inhibit or induce the subject's experience of that conscious state. The growing ability of neuroscientists to manipulate neurons using methods from molecular biology in combination with optical tools was achieved by the development of behavioral and organic models that are amenable to large-scale genomic analysis and manipulation. Non-human analysis such as this, in combination with imaging of the human brain, have contributed to a robust and increasingly predictive theoretical framework.\n\nThere are two common but distinct dimensions of the term \"consciousness\", one involving \"arousal\" and \"states of consciousness\" and the other involving \"content of consciousness\" and \"conscious states\". To be conscious \"of\" something, the brain must be in a relatively high state of arousal (sometimes called \"vigilance\"), whether awake or in REM sleep. Brain arousal level fluctuates in a circadian rhythm but these natural cycles may be influenced by lack of sleep, alcohol and other drugs, physical exertion, etc. Arousal can be measured behaviorally by the signal amplitude required to trigger a given reaction (for example, the sound level that causes a subject to turn and look toward the source). High arousal states involve conscious states that feature specific perceptual content, planning and recollection or even fantasy. Clinicians use scoring systems such as the Glasgow Coma Scale to assess the level of arousal in patients with \"impaired states of consciousness\" such as the comatose state, the persistent vegetative state, and the minimally conscious state. Here, \"state\" refers to different amounts of externalized, physical consciousness: ranging from a total absence in coma, persistent vegetative state and general anesthesia, to a fluctuating, minimally conscious state, such as sleep walking and epileptic seizure.\n\nMany nuclei with distinct chemical signatures in the thalamus, midbrain and pons must function for a subject to be in a sufficient state of brain arousal to experience anything at all. These nuclei therefore belong to the enabling factors for consciousness. Conversely it is likely that the specific content of any particular conscious sensation is mediated by particular neurons in the cortex and their associated satellite structures, including the amygdala, thalamus, claustrum and the basal ganglia.\n\nThe following is a very brief account of some contributions to the mind–body problem.\n\nThe Buddha (480–400 B.C.E), founder of Buddhism, described the mind and the body as depending on each other in a way that two sheaves of reeds were to stand leaning against one another and taught that the world consists of mind and matter which work together, interdependently. Buddhist teachings describe the mind as manifesting from moment to moment, one thought moment at a time as a fast flowing stream. The components that make up the mind are known as the five aggregates (i.e., material form, feelings, perception, volition, and sensory consciousness), which arise and pass away continuously. The arising and passing of these aggregates in the present moment is described as being influenced by five causal laws: biological laws, psychological laws, physical laws, volitional laws, and universal laws. The Buddhist practice of mindfulness involves attending to this constantly changing mind-stream.\n\nUltimately, the Buddha's philosophy is that both mind and forms are conditionally arising qualities of an ever-changing universe in which, when nirvāna is attained, all phenomenal experience ceases to exist. According to the anattā doctrine of the Buddha, the conceptual self is a mere mental construct of an individual entity and is basically an impermanent illusion, sustained by form, sensation, perception, thought and consciousness. The Buddha argued that mentally clinging to any views will result in delusion and stress, since, according to the Buddha, a real self (conceptual self, being the basis of standpoints and views) cannot be found when the mind has clarity.\n\nPlato (429–347 B.C.E.) believed that the material world is a shadow of a higher reality that consists of concepts he called Forms. According to Plato, objects in our everyday world \"participate in\" these Forms, which confer identity and meaning to material objects. For example, a circle drawn in the sand would be a circle only because it participates in the concept of an ideal circle that exists somewhere in the world of Forms. He argued that, as the body is from the material world, the soul is from the world of Forms and is thus immortal. He believed the soul was temporarily united with the body and would only be separated at death, when it would return to the world of Forms. Since the soul does not exist in time and space, as the body does, it can access universal truths. For Plato, ideas (or Forms) are the true reality, and are experienced by the soul. The body is for Plato empty in that it can not access the abstract reality of the world; it can only experience shadows. This is determined by Plato's essentially rationalistic epistemology.\n\nFor Aristotle (384–322 BC) \"mind\" is a faculty of the \"soul\". Regarding the soul, he said:\nIn the end, Aristotle saw the relation between soul and body as uncomplicated, in the same way that it is uncomplicated that a cubical shape is a property of a toy building block. The soul is a property exhibited by the body, one among many. Moreover, Aristotle proposed that when the body perishes, so does the soul, just as the shape of a building block disappears with destruction of the block.\n\nIn religious philosophy of the people of the book dualism denotes a binary opposition of an idea that contains two essential parts. The first formal concept of a \"mind-body\" split may be found in the \"\"divinity - secularity\"\" dualism of the ancient Persian religion of Zoroastrianism around the mid-fifth century BC. Gnosticism is a modern name for a variety of ancient dualistic ideas inspired by Judaism popular in the first and second century AD. These ideas later seem to have been incorporated into Galen's \"\"tripartite soul\"\" that led into both the Christian sentiments expressed in the later Augustinian theodicy and Avicenna’s Platonism in Islamic Philosophy.\n\nRené Descartes (1596–1650) believed that mind exerted control over the brain \"via\" the pineal gland:\nHis posited relation between mind and body is called Cartesian dualism or substance dualism. He held that \"mind\" was distinct from \"matter\", but could influence matter. How such an interaction could be exerted remains a contentious issue.\n\nFor Kant (1724–1804) beyond \"mind\" and \"matter\" there exists a world of \"a priori\" forms, which are seen as necessary preconditions for understanding. Some of these forms, space and time being examples, today seem to be pre-programmed in the brain.\n\nKant views the mind–body interaction as taking place through forces that may be of different kinds for mind and body.\n\nFor Huxley (1825–1895) the conscious mind was a by-product of the brain that has no influence upon the brain, a so-called epiphenomenon.\n\nA. N. Whitehead advocated a sophisticated form of panpsychism that has been called by David Ray Griffin \"panexperientialism\".\n\nFor Popper (1902–1994) there are \"three\" aspects of the mind–body problem: the worlds of matter, mind, and of the creations of the mind, such as mathematics. In his view, the third-world creations of the mind could be interpreted by the second-world mind and used to affect the first-world of matter. An example might be radio, an example of the interpretation of the third-world (Maxwell's electromagnetic theory) by the second-world mind to suggest modifications of the external first world.\n\nFor Searle (b. 1932) the mind–body problem is a false dichotomy; that is, mind is a perfectly ordinary aspect of the brain. \n\n\n\n", "id": "11081176", "title": "Mind–body problem"}
{"url": "https://en.wikipedia.org/wiki?curid=2174928", "text": "Information space analysis\n\nInformation space analysis is a deterministic method, enhanced by machine intelligence, for locating and assessing resources for team centric efforts.\n\nOrganizations need to be able to quickly assemble teams backed by the support services, information, and material to do the job. To do so, these teams need to find and assess sources of services that are potential participants in the team effort.\n\nTo support this initial team and resource development, information needs to be developed via analysis tools that help make sense of sets of data sources in an Intranet or Internet. Part of the process is to characterize them, partition them, sort and filter them.\n\nThese tools focus on three key issues in forming a collaborative team: \n\nInformation space analysis tools combine multiple methods to assist in this task. This causes the tools to be particularly well-suited to integrating additional technologies in order to create specialized systems.\n\n", "id": "2174928", "title": "Information space analysis"}
{"url": "https://en.wikipedia.org/wiki?curid=40888645", "text": "Bayesian programming\n\nBayesian programming is a formalism and a methodology to specify probabilistic models and solve problems when less than the necessary information is available.\n\nEdwin T. Jaynes proposed that probability could be considered as an alternative and an extension of logic for rational reasoning with incomplete and uncertain information. In his founding book \"Probability Theory: The Logic of Science\" he developed this theory and proposed what he called “the robot,” which was not\na physical device, but an inference engine to automate probabilistic reasoning—a kind of Prolog for probability instead of logic. Bayesian programming is a formal and concrete implementation of this \"robot\".\n\nBayesian programming may also be seen as an algebraic formalism to specify graphical models such as, for instance, Bayesian networks, dynamic Bayesian networks, Kalman filters or hidden Markov models. Indeed, Bayesian Programming is more general than Bayesian networks and has a power of expression equivalent to probabilistic factor graphs.\n\nA Bayesian program is a means of specifying a family of probability distributions.\n\nThe constituent elements of a Bayesian program are presented below:\n\n\nThe purpose of a description is to specify an effective method of computing a joint probability distribution\non a set of variables formula_4 given a set of experimental data formula_3 and some\nspecification formula_2. This joint distribution is denoted as: formula_7.\n\nTo specify preliminary knowledge formula_2, the programmer must undertake the following:\n\n\nGiven a partition of formula_10 containing formula_11 subsets, formula_11 variables are defined\nformula_13, each corresponding to one of these subsets.\nEach variable formula_14 is obtained as the conjunction of the variables formula_15\nbelonging to the formula_16 subset. Recursive application of Bayes' theorem leads to:\n\nConditional independence hypotheses then allow further simplifications. A conditional\nindependence hypothesis for variable formula_14 is defined by choosing some variable formula_19\namong the variables appearing in the conjunction formula_20, labelling formula_21 as the\nconjunction of these chosen variables and setting:\n\nWe then obtain:\n\nSuch a simplification of the joint distribution as a product of simpler distributions is\ncalled a decomposition, derived using the chain rule.\n\nThis ensures that each variable appears at the most once on the left of a conditioning\nbar, which is the necessary and sufficient condition to write mathematically valid\ndecompositions.\n\nEach distribution formula_24 appearing in the product is then associated\nwith either a parametric form (i.e., a function formula_25) or a question to another Bayesian program formula_26.\n\nWhen it is a form formula_25, in general, formula_28 is a vector of parameters that may depend on formula_21 or formula_3 or both. Learning\ntakes place when some of these parameters are computed using the data set formula_3.\n\nAn important feature of Bayesian Programming is this capacity to use questions to other Bayesian programs as components of the definition of a new Bayesian program. formula_24 is obtained by some inferences done by another Bayesian program defined by the specifications formula_33 and the data formula_34. This is similar to calling a subroutine in classical programming and provides an easy way to build hierarchical models.\n\nGiven a description (i.e., formula_7), a question is obtained by partitioning formula_4\ninto three sets: the searched variables, the known variables and\nthe free variables.\n\nThe 3 variables formula_37, formula_38 and formula_39 are defined as the\nconjunction of the variables belonging to\nthese sets.\n\nA question is defined as the set\nof distributions:\n\nmade of many \"instantiated questions\" as the cardinal of formula_38,\neach instantiated question being the distribution:\n\nGiven the joint distribution formula_43, it is always possible to compute any possible question using the following general inference:\n\nwhere the first equality results from the marginalization rule, the second\nresults from Bayes' theorem and the third corresponds to a second application of marginalization. The denominator appears to be a normalization term and can be replaced by a constant formula_45.\n\nTheoretically, this allows to solve any Bayesian inference problem. In practice,\nhowever, the cost of computing exhaustively and exactly formula_46 is too great in almost all cases.\n\nReplacing the joint distribution by its decomposition we get:\n\nwhich is usually a much simpler expression to compute, as the dimensionality of the problem is considerably reduced by the decomposition into a product of lower dimension distributions.\n\nThe purpose of Bayesian spam filtering is to eliminate junk e-mails.\n\nThe problem is very easy to formulate. E-mails should be classified\ninto one of two categories: non-spam or spam. The only available information to classify the e-mails is their content: a set of words. Using these words without taking the order into account is commonly called a bag of words model.\n\nThe classifier should furthermore be able to adapt to its user and to learn\nfrom experience. Starting from an initial standard setting, the classifier should\nmodify its internal parameters when the user disagrees with its own decision.\nIt will hence adapt to the user’s criteria to differentiate between non-spam and\nspam. It will improve its results as it encounters increasingly classified e-mails.\n\nThe variables necessary to write this program are as follows:\n\nThese formula_53 binary variables sum up all the information\nabout an e-mail.\n\nStarting from the joint distribution and applying recursively Bayes' theorem we obtain:\n\nThis is an exact mathematical expression.\n\nIt can be drastically simplified by assuming that the probability of appearance of a word knowing the nature of the text (spam or not) is independent of the appearance of the other words. This is the naive Bayes assumption and this makes this spam filter a naive Bayes model.\n\nFor instance, the programmer can assume that:\n\nto finally obtain:\n\nThis kind of assumption is known as the naive Bayes' assumption. It is \"naive\" in the sense that the independence between words is clearly not completely true. For instance, it completely neglects that the appearance of pairs of words may be more significant than isolated appearances. However, the programmer may assume this hypothesis and may develop the model and the associated inferences to test how reliable and efficient it is.\n\nTo be able to compute the joint distribution, the programmer must now specify the\nformula_53 distributions appearing in the decomposition:\n\n\nwhere formula_64 stands for the number of appearances of the formula_52 word in non-spam e-mails and formula_66 stands for the total number of non-spam e-mails. Similarly, formula_67 stands for the number of appearances of the formula_52 word in spam e-mails and formula_69 stands for the total number of spam e-mails.\n\nThe formula_50 forms formula_61 are not yet completely specified because the formula_72 parameters formula_73, formula_74, formula_66 and formula_69 have no values yet.\n\nThe identification of these parameters could be done either by batch processing a series of classified e-mails or by an incremental updating of the parameters using the user's classifications of the e-mails as they arrive.\n\nBoth methods could be combined: the system could start with initial standard values of these parameters issued from a generic database, then some incremental learning customizes the classifier to each individual user.\n\nThe question asked to the program is: \"what is the probability for a given text to be spam knowing which words appear and don't appear in this text?\"\nIt can be formalized by:\n\nwhich can be computed as follows:\n\nThe denominator appears to be a normalization constant. It is not necessary to compute it to decide if we are dealing with spam. For instance, an easy trick is to compute the ratio:\n\nThis computation is faster and easier because it requires only formula_80 products.\n\nThe Bayesian spam filter program is completely defined by:\n\nBayesian filters (often called Recursive Bayesian estimation) are generic probabilistic models for time evolving processes. Numerous models are particular instances of this generic approach, for instance: the Kalman filter or the Hidden Markov model (HMM).\n\n\nThe decomposition is based:\n\nThe parametrical forms are not constrained and different choices lead to different well-known models: see Kalman filters and Hidden Markov models just below.\n\nThe typical question for such models is formula_94: what is the probability distribution for the state at time formula_95 knowing the observations from instant formula_83 to formula_88?\n\nThe most common case is Bayesian filtering where formula_98, which searches for the present state, knowing past observations.\n\nHowever it is also possible formula_99, to extrapolate a future state from past observations, or to do smoothing formula_100, to recover a past state from observations made either before or after that instant.\n\nMore complicated questions may also be asked as shown below in the HMM section.\n\nBayesian filters formula_101 have a very interesting recursive property, which contributes greatly to their attractiveness. formula_102 may be computed simply from formula_103 with the following formula:\n\nAnother interesting point of view for this equation is to consider that there are two phases: a\nprediction phase and an estimation phase:\n\n\nThe very well-known Kalman filters are a special case of Bayesian\nfilters.\n\nThey are defined by the following Bayesian program:\n\n\nWith these hypotheses and by using the recursive formula, it is possible to solve\nthe inference problem analytically to answer the usual formula_111 question.\nThis leads to an extremely efficient algorithm, which explains the popularity of Kalman filters and the number of their everyday applications.\n\nWhen there are no obvious linear transition and observation models, it is still often\npossible, using a first-order Taylor's expansion, to treat these models as locally linear.\nThis generalization is commonly called the extended Kalman filter.\n\nHidden Markov models (HMMs) are another very popular specialization of Bayesian filters.\n\nThey are defined by the following Bayesian program:\n\nboth specified using probability matrices.\n\nWhat is the most probable series of states that leads to the present state, knowing the past observations?\n\nThis particular question may be answered with a specific and very efficient algorithm\ncalled the Viterbi algorithm.\n\nThe Baum–Welch algorithm has been developed\nfor HMMs.\n\nSince 2000, Bayesian programming has been used to develop both robotics applications and life sciences models.\n\nIn robotics, bayesian programming was applied to autonomous robotics, robotic CAD systems, advanced driver-assistance systems, robotic arm control, mobile robotics, human-robot interaction, human-vehicle interaction (Bayesian autonomous driver models) video game avatar programming and training and real-time strategy games (AI).\n\nIn life sciences, bayesian programming was used in vision to reconstruct shape from motion, to model visuo-vestibular interaction and to study saccadic eye movements; in speech perception and control to study early speech acquisition and the emergence of articulatory-acoustic systems; and to model handwriting perception and control.\n\nBayesian program learning has potential applications voice recognition and synthesis, image recognition and natural language processing. It employs the principles of \"compositionality\" (building abstract representations from parts), \"causality\" (building complexity from parts) and \"learning to learn\" (using previously recognized concepts to ease the creation of new concepts).\n\nThe comparison between probabilistic approaches (not only bayesian programming) and possibility theories continues to be debated.\n\nPossibility theories like, for instance, fuzzy sets, fuzzy logic and possibility theory are alternatives to probability to model uncertainty. They argue that probability is insufficient or inconvenient to model certain aspects of incomplete/uncertain knowledge.\n\nThe defense of probability is mainly based on Cox's theorem, which starts from four postulates concerning rational reasoning in the presence of uncertainty. It demonstrates that the only mathematical framework that satisfies these postulates is probability theory. The argument is that any approach other than probability necessarily infringes one of these postulates and the value of that infringement.\n\nThe purpose of probabilistic programming is to unify the scope of classical programming languages with probabilistic modeling (especially bayesian networks) to deal with uncertainty while profiting from the programming languages' expressiveness to encode complexity.\n\nExtended classical programming languages include logical languages as proposed in Probabilistic Horn Abduction, Independent Choice Logic, PRISM, and ProbLog which proposes an extension of Prolog.\n\nIt can also be extensions of functional programming languages (essentially Lisp and Scheme) such as IBAL or CHURCH. The underlying programming languages can be object-oriented as in BLOG and FACTORIE or more standard ones as in CES and FIGARO.\n\nThe purpose of Bayesian programming is different. Jaynes' precept of \"probability as logic\" argues that probability is an extension of and an alternative to logic above which a complete theory of rationality, computation and programming can be rebuilt. Bayesian programming attempts to replace classical languages with a programming approach based on probability that considers incompleteness and uncertainty.\n\nThe precise comparison between the semantics and power of expression of Bayesian and probabilistic programming is an open question.\n\n", "id": "40888645", "title": "Bayesian programming"}
