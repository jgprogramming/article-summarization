[
  {
    "url": "https://en.wikipedia.org/wiki?curid=996026",
    "text": "Commonsense reasoning\n\nCommonsense reasoning is one of the branches of artificial intelligence (AI) that is concerned with simulating the human ability to make presumptions about the type and essence of ordinary situations they encounter every day. These assumptions include judgments about the physical properties, purpose, intentions and behavior of people and objects, as well as possible outcomes of their actions and interactions. A device that exhibits commonsense reasoning will be capable of predicting results and drawing conclusions that are similar to humans' folk psychology (humans' innate ability to reason about people's behavior and intentions) and naive physics (humans' natural understanding of the physical world).\n\nIn Artificial intelligence, commonsense knowledge is the set of background information that an individual is intended to know or assume and the ability to use it when appropriate. It is a shared knowledge (between everybody or people in a particular culture or age group only). The way to obtain commonsense is by learning it or experiencing it. In communication, it is what people don’t have to say because the interlocutor is expected to know or make a presumption about.\n\nThe commonsense knowledge problem is a current project in the sphere of artificial intelligence to create a database that contains the general knowledge most individuals are expected to have, represented in an accessible way to artificial intelligence programs that use natural language. Due to the broad scope of the commonsense knowledge this issue is considered to be among the most difficult ones in the AI research sphere. In order for any task to be done as a human mind would manage it, the machine is required to appear as intelligent as a human being. Such tasks include object recognition, machine translation and text mining. To perform them, the machine has to be aware of the same concepts that an individual, who possess commonsense knowledge, recognizes.\n\nIn 1961, Bar Hillel first discussed the need and significance of practical knowledge for natural language processing in the context of machine translation. Some ambiguities are resolved by using simple and easy to acquire rules. Others require a broad acknowledgement of the surrounding world, thus they require more commonsense knowledge. For instance when a machine is used to translate a text, problems of ambiguity arise, which could be easily resolved by attaining a concrete and true understanding of the context. Online translators often resolve ambiguities using analogous or similar words. For example, in translating the sentences \"The electrician is working\" and \"The telephone is working\" into German, the machine translates correctly \"working\" in the means of \"laboring\" in the first one and as \"functioning properly\" in the second one. The machine has seen and read in the body of texts that the German words for \"laboring\" and \"electrician\" are frequently used in a combination and are found close together. The same applies for \"telephone\" and \"function properly\". However, the statistical proxy which works in simple cases often fails in complex ones. Existing computer programs carry out simple language tasks by manipulating short phrases or separate words, but they don’t attempt any deeper understanding and focus on short-term results.\n\nIssues of this kind arise in computer vision. For instance when looking at the photograph of the bathroom (Figure 1) some of the items that are small and only partly seen, such as the towels or the body lotions, are recognizable due to the surrounding objects (toilet, wash basin, bathtub), which suggest the purpose of the room. In an isolated image they would be difficult to identify. Movies prove to be even more difficult tasks. Some movies contain scenes and moments that cannot be understood by simply matching memorized templates to images. For instance, to understand the context of the movie, the viewer is required to make inferences about characters’ intentions and make presumptions depending on their behavior. In the contemporary state of the art, it is impossible to build and manage a program that will perform such tasks as reasoning, i.e. predicting characters’ actions. The most that can be done is to identify basic actions and track characters.\n\nThe need and importance of commonsense reasoning in autonomous robots that work in a real-life uncontrolled environment is evident. For instance, if a robot is programmed to perform the tasks of a waiter on a cocktail party, and it sees that the glass he had picked up is broken, the waiter-robot should not pour liquid into the glass, but instead pick up another one. Such tasks seem obvious when an individual possess simple commonsense reasoning, but to ensure that a robot will avoid such mistakes is challenging.\n\nSignificant progress in the field of the automated commonsense reasoning is made in the areas of the taxonomic reasoning, actions and change reasoning, reasoning about time. Each of these spheres has a well-acknowledged theory for wide range of commonsense inferences.\n\nTaxonomy is the collection of individuals and categories and their relations. Taxonomies are often referred to as semantic networks. Figure 2 displays a taxonomy of a few categories of individuals and animals.\nThree basic relations are demonstrated:\nTransitivity is one type of inference in taxonomy. Since Tweety is an instance of Robin and Robin is a subset of Bird, it follows that Tweety is an instance of Bird. Inheritance is another type of inference. Since Tweety is an instance of Robin, which is a subset of Bird and Bird is marked with property CanFly, it follows that Tweety and Robin have property CanFly. When an individual taxonomizes more abstract categories, outlining and delimiting specific categories becomes more problematic. Simple taxonomic structures are frequently used in AI programs. For instance, WordNet is a resource including a taxonomy, whose elements are meanings of English words. Web mining systems used to collect commonsense knowledge from Web documents focus on taxonomic relations and specifically in gathering taxonomic relations.\n\nThe theory of action, events and change is another range of the commonsense reasoning. There are established reasoning methods for domains that satisfy the constraints listed below:\n\nTemporal reasoning is the ability to make presumptions about humans' knowledge of times, durations and time intervals. For example, if an individual knows that Mozart was born before Beethoven and died earlier than him, he can use his temporal reasoning knowledge to deduce that Mozart had died younger than Beethoven. The inferences involved reduce themselves to solving systems of linear inequalities. To integrate that kind of reasoning with concrete purposes, such as natural language interpretation, is more challenging, because natural language expressions have context dependent interpretation. Simple tasks such as assigning timestamps to procedures cannot be done with total accuracy.\n\nQualitative reasoning is the form of commonsense reasoning analyzed with certain success. It is concerned with the direction of change in interrelated quantities. For instance, if the price of a stock goes up, the amount of stocks that are going to be sold will go down. If some ecosystem contains wolves and lambs and the number of wolves decreases, the death rate of the lambs will go down as well. This theory was firstly formulated by Johan de Kleer, who analyzed an object moving on a roller coaster. The theory of qualitative reasoning is applied in many spheres such as physics, biology, engineering, ecology, etc. It serves as the basis for many practical programs, analogical mapping, text understanding.\n\nAs of 2014, there are some commercial systems trying to make the use of commonsense reasoning significant. However, they use statistical information as a proxy for commonsense knowledge, where reasoning is absent. Current programs manipulate individual words, but they don't attempt or offer further understanding. Five major obstacles interfere with the producing of a satisfactory \"commonsense reasoner\".\n\nFirst, some of the domains that are involved in commonsense reasoning are only partly understood. Individuals are far from a comprehensive understanding of domains as communication and knowledge, interpersonal interactions or physical processes.\n\nSecond, situations that seem easily predicted or assumed about could have logical complexity, which humans’ commonsense knowledge does not cover. Some aspects of similar situations are studied and are well understood, but there are many relations that are unknown, even in principle and how they could be represented in a form that is usable by computers.\n\nThird, commonsense reasoning involves plausible reasoning. It requires coming to a reasonable conclusion given what is already known. Plausible reasoning has been studied for many years and there are a lot of theories developed that include probabilistic reasoning and non-monotonic logic. It takes different forms that include using unreliable data and rules, whose conclusions are not certain sometimes.\n\nFourth, there are many domains, in which a small number of examples are extremely frequent, whereas there is a vast number of highly infrequent examples.\n\nFifth, when formulating pressumptions it is challenging to discern and determine the level of abstraction.\n\nCompared with humans, all existing computer programs perform extremely poorly on modern \"commonsense reasoning\" benchmark tests such as the Winograd Schema Challenge. The problem of attaining human-level competency at \"commonsense knowledge\" tasks is considered to probably be \"AI complete\" (that is, solving it would require the ability to synthesize a human-level intelligence).\n\nCommonsense’s reasoning study is divided into knowledge-based approaches and approaches that are based on machine learning over and using a large data corpora with limited interactions between these two types of approaches. There are also crowdsourcing approaches, attempting to construct a knowledge basis by linking the collective knowledge and the input of non-expert people. Knowledge-based approaches can be separated into approaches based on mathematical logic.\n\nIn knowledge-based approaches, the experts are analyzing the characteristics of the inferences that are required to do reasoning in a specific area or for a certain task. The knowledge-based approaches consist of mathematically grounded approaches, informal knowledge-based approaches and large-scale approaches. The mathematically grounded approaches are purely theoretical and the result is a printed paper instead of a program. The work is limited to the range of the domains and the reasoning techniques that are being reflected on. In informal knowledge-based approaches, theories of reasoning are based on anecdotal data and intuition that are results from empirical behavioral psychology. Informal approaches are common in computer programming. Two other popular techniques for extracting commonsense knowledge from Web documents involve Web mining and Crowd sourcing.\n\n\n\n",
    "id": "996026",
    "title": "Commonsense reasoning"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=610757",
    "text": "Knowledge level\n\nIn artificial intelligence, knowledge-based agents draw on a pool of logical sentences to infer conclusions about the world. At the knowledge level, we only need to specify what the agent knows and what its goals are; a logical abstraction separate from details of implementation.\n\nThis notion of knowledge level was first introduced by Allen Newell in the 1980s, to have a way to rationalize an agent's behavior. The agent takes actions based on knowledge it possesses, in an attempt to reach specific goals. It chooses actions according to the principle of rationality.\n\nBeneath the knowledge level resides the symbol level. Whereas the knowledge level is \"world\" oriented, namely that it concerns the environment in which the agent operates, the symbol level is \"system\" oriented, in that it includes the mechanisms the agent has available to operate. The knowledge level \"rationalizes\" the agent's behavior, while the symbol level \"mechanizes\" the agent's behavior.\n\nFor example, in a computer program, the knowledge level consists of the information contained in its data structures that it uses to perform certain actions. The symbol level consists of the program's algorithms, the data structures themselves, and so on.\n\n\n",
    "id": "610757",
    "title": "Knowledge level"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=238725",
    "text": "Loebner Prize\n\nThe Loebner Prize is an annual competition in artificial intelligence that awards prizes to the computer programs considered by the judges to be the most human-like. The format of the competition is that of a standard Turing test. In each round, a human judge simultaneously holds textual conversations with a computer program and a human being via computer. Based upon the responses, the judge must decide which is which.\n\nThe contest was launched in 1990 by Hugh Loebner in conjunction with the Cambridge Center for Behavioral Studies, Massachusetts, United States. It has since been associated with Flinders University, Dartmouth College, the Science Museum in London, University of Reading and Ulster University, Magee Campus, Derry, UK City of Culture. In 2004 and 2005, it was held in Loebner's apartment in New York City. Within the field of artificial intelligence, the Loebner Prize is somewhat controversial; the most prominent critic, Marvin Minsky, called it a publicity stunt that does not help the field along.\n\nOriginally, $2,000 was awarded for the most human-seeming program in the competition. The prize was $3,000 in 2005 and $2,250 in 2006. In 2008, $3,000 was awarded.\n\nIn addition, there are two one-time-only prizes that have never been awarded. $25,000 is offered for the first program that judges cannot distinguish from a real human and which can convince judges that the human is the computer program. $100,000 is the reward for the first program that judges cannot distinguish from a real human in a Turing test that includes deciphering and understanding text, visual, and auditory input. Once this is achieved, the annual competition will end.\n\nThe rules have varied over the years and early competitions featured restricted conversation Turing tests but since 1995 the discussion has been unrestricted.\n\nFor the three entries in 2007, Robert Medeksza, Noah Duncan and Rollo Carpenter, some basic \"screening questions\" were used by the sponsor to evaluate the state of the technology. These included simple questions about the time, what round of the contest it is, etc.; general knowledge (\"What is a hammer for?\"); comparisons (\"Which is faster, a train or a plane?\"); and questions demonstrating memory for preceding parts of the same conversation. \"All nouns, adjectives and verbs will come from a dictionary suitable for children or adolescents under the age of 12.\" Entries did not need to respond \"intelligently\" to the questions to be accepted.\n\nFor the first time in 2008 the sponsor allowed introduction of a preliminary phase to the contest opening up the competition to previously disallowed web-based entries judged by a variety of invited interrogators. The available rules do not state how interrogators are selected or instructed. Interrogators (who judge the systems) have limited time: 5 minutes per entity in the 2003 competition, 20+ per pair in 2004–2007 competitions, 5 minutes to conduct \"simultaneous\" conversations with a human and the program in 2008-2009, increased to 25 minutes of simultaneous conversation since 2010.\n\nThe prize has long been scorned by experts in the field, for a variety of reasons.\n\nIt is regarded by many as a publicity stunt. Marvin Minsky scathingly offered a \"prize\" to anyone who could stop the competition. Loebner responded by jokingly observing that Minsky's offering a prize to stop the competition effectively made him a co-sponsor.\n\nThe rules of the competition have encouraged poorly qualified judges to make rapid judgements. Interactions between judges and competitors was originally very brief, for example effectively 2.5 mins of questioning, which permitted only a few questions. Questioning was initially restricted to \"whimsical conversation\", a domain suiting standard chatbot tricks.\n\nCompetition entrants do not aim at understanding or intelligence but resort to basic ELIZA style tricks, and successful entrants find deception and pretense is rewarded.\n\nReporting of the annual competition often confuses the imitation test with intelligence, a typical example being Brian Christian's introduction to his article \"Mind vs. Machine\" in The Atlantic, March 2011, stating that \"in the race to build computers that \"can think like humans\", the proving ground is the Turing Test\".\n\nIn 2006, the contest was organised by Tim Child (CEO of Televirtual) and Huma Shah. On August 30, the four finalists were announced:\n\nThe contest was held on 17 September in the VR theatre, Torrington Place campus of University College London. The judges included the University of Reading's cybernetics professor, Kevin Warwick, a professor of artificial intelligence, John Barnden (specialist in metaphor research at the University of Birmingham), a barrister, Victoria Butler-Cole and a journalist, Graham Duncan-Rowe. The latter's experience of the event can be found in an article in \"Technology Review\". The winner was 'Joan', based on Jabberwacky, both created by Rollo Carpenter.\n\nThe 2007 competition was held on October 21 in New York City. The judges were: computer science professor Russ Abbott, philosophy professor Hartry Field, psychology assistant professor Clayton Curtis and English lecturer Scott Hutchins.\n\nNo bot passed the Turing test, but the judges ranked the three contestants as follows:\n\nThe winner received $2,250 and the annual medal. The runners-up received $250 each.\n\nThe 2008 competition was organised by professor Kevin Warwick, coordinated by Huma Shah and held on October 12 at the University of Reading, UK. After testing by over one hundred judges during the preliminary phase, in June and July 2008, six finalists were selected from thirteen original entrants - artificial conversational entity (ACE). Five of those invited competed in the finals:\n\nIn the finals, each of the judges was given five minutes to conduct simultaneous, split-screen conversations with two hidden entities. Elbot of Artificial Solutions won the 2008 Loebner Prize bronze award, for \"most human-like\" artificial conversational entity, through fooling three of the twelve judges who interrogated it (in the human-parallel comparisons) into believing it was human. This is coming very close to the 30% traditionally required to consider that a program has actually passed the Turing test. Eugene Goostman and Ultra Hal both deceived one judge each that it was the human.\n\nWill Pavia, a journalist for The Times, has written about his experience; a Loebner finals' judge, he was deceived by Elbot and Eugene. Kevin Warwick and Huma Shah have reported on the parallel-paired Turing tests.\n\nThe 2009 Loebner Prize Competition was held September 6, 2009 at the Brighton Centre, Brighton UK in conjunction with the Interspeech 2009 conference. The prize amount for 2009 was $3,000.\n\nEntrants were David Levy, Rollo Carpenter, and Mohan Embar, who finished in that order.\n\nThe writer Brian Christian participated in the 2009 Loebner Prize Competition as a human confederate, and described his experiences at the competition in his book \"The Most Human Human\".\n\nThe 2010 Loebner Prize Competition was held on October 23 at California State University, Los Angeles. The 2010 competition was the 20th running of the contest. The winner was Bruce Wilcox with Suzette.\n\nThe 2011 Loebner Prize Competition was held on October 19 at the University of Exeter, Devon, United Kingdom. The prize amount for 2011 was $4,000.\n\nThe four finalists and their chatterbots were Bruce Wilcox (Rosette), Adeena Mignogna (Zoe), Mohan Embar (Chip Vivant) and Ron Lee (Tutor), who finished in that order.\n\nThat year there was an addition of a panel of junior judges, namely Jean-Paul Astal-Stain, William Dunne, Sam Keat and Kirill Jerdev. The results of the junior contest were markedly different from the main contest, with chatterbots Tutor and Zoe tying for first place and Chip Vivant and Rosette coming in third and fourth place, respectively.\n\nThe 2012 Loebner Prize Competition was held on the 15th of May in Bletchley Park in Bletchley, Buckinghamshire, England, in honor of the Alan Turing centenary celebrations. The prize amount for 2012 was $5,000. The local arrangements organizer was David Levy, who won the Loebner Prize in 1997 and 2009.\n\nThe four finalists and their chatterbots were Mohan Embar (Chip Vivant), Bruce Wilcox (Angela), Daniel Burke (Adam), M. Allan (Linguo), who finished in that order.\n\nThat year, a team from the University of Exeter's computer science department (Ed Keedwell, Max Dupenois and Kent McClymont) conducted the first-ever live webcast of the conversations.\n\nThe 2013 Loebner Prize Competition was held, for the first time on the Island of Ireland, on September 14 at the Ulster University, Magee College, Derry, Northern Ireland, UK.\n\nThe four finalists and their chatbots were Steve Worswick (Mitsuku), Dr. Ron C. Lee (Tutor), Bruce Wilcox (Rose) and Brian Rigsby (Izar), who finished in that order.\n\nThe judges were Professor Roger Schank (Socratic Arts), Professor Noel Sharkey (Sheffield University), Professor Minhua (Eunice) Ma (Huddersfield University, then University of Glasgow)\nand Professor Mike McTear (Ulster University).\n\nFor the 2013 Junior Loebner Prize Competition the chatbots Mitsuku and Tutor tied for first place with Rose and Izar in 3rd and 4th place respectively.\n\nThe 2014 Loebner Prize Competition was held at Bletchley Park, England, on Saturday 15 November 2014. The event was filmed live by Sky News. The guest judge was television presenter and broadcaster James May.\n\nAfter 2 hours of judging, 'Rose' by Bruce Wilcox was declared the winner. Bruce will receive a cheque for $4000 and a bronze medal. The ranks were as follows:\n\nRose - Rank 1 ($4000 & Bronze Medal); \nIzar - Rank 2.25 ($1500); \nUberbot - Rank 3.25 ($1000); and \nMitsuku - Rank 3.5 ($500).\n\nThe Judges were Dr Ian Hocking, Writer & Senior Lecturer in Psychology, Christ Church College, Canterbury; \nDr Ghita Kouadri-Mostefaoui, Lecturer in Computer Science and Technology, University of Bedfordshire; \nMr James May, Television Presenter and Broadcaster; and \nDr Paul Sant, Dean of UCMK, University of Bedfordshire.\n\nThe 2015 Loebner Prize Competition was again won by 'Rose' by Bruce Wilcox.\n\nThe judges were Jacob Aaron, Physical sciences reporter for New Scientist; Rory Cellan-Jones, Technology correspondent for the BBC; Brett Marty, Film Director and Photographer; Ariadne Tampion, Writer.\n\nThe 2016 Loebner Prize was held at Bletchley Park on 17 September 2016. After 2 hours of judging the final results were announced. \nThe ranks were as follows:\n\n\nOfficial list of winners.\n\n",
    "id": "238725",
    "title": "Loebner Prize"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=253279",
    "text": "Automated Mathematician\n\nThe Automated Mathematician (AM) is one of the earliest successful discovery systems. It was created by Douglas Lenat in Lisp, and in 1977 led to Lenat being awarded the IJCAI Computers and Thought Award.\n\nAM worked by generating and modifying short Lisp programs which were then interpreted as defining various mathematical concepts; for example, a program that tested equality between the length of two lists was considered to represent the concept of numerical equality, while a program that produced a list whose length was the product of the lengths of two other lists was interpreted as representing the concept of multiplication. The system had elaborate heuristics for choosing which programs to extend and modify, based on the experiences of working mathematicians in solving mathematical problems.\n\nLenat claimed that the system was composed of hundreds of data structures called \"concepts,\" together with hundreds of \"heuristic rules\" and a simple flow of control: \"AM repeatedly selects the top task from the agenda and tries to carry it out. This is the whole control structure!\" Yet the heuristic rules were not always represented as separate data structures; some had to be intertwined with the control flow logic. Some rules had preconditions that depended on the history, or otherwise could not be represented in the framework of the explicit rules.\n\nWhat's more, the published versions of the rules often involve vague terms that are not defined further, such as \"If two expressions are structurally similar, ...\" (Rule 218) or \"... replace the value obtained by some other (very similar) value...\" (Rule 129).\n\nAnother source of information is the user, via Rule 2: \"If the user has recently referred to X, then boost the priority of any tasks involving X.\" Thus, it appears quite possible that much of the real discovery work is buried in unexplained procedures.\n\nLenat claimed that the system had rediscovered both Goldbach's conjecture and the fundamental theorem of arithmetic. Later critics accused Lenat of over-interpreting the output of AM. In his paper \"Why AM and Eurisko appear to work\", Lenat conceded that any system that generated enough short Lisp programs would generate ones that could be interpreted by an external observer as representing equally sophisticated mathematical concepts. However, he argued that this property was in itself interesting—and that a promising direction for further research would be to look for other languages in which short random strings were likely to be useful.\n\nThis intuition was the basis of AM's successor Eurisko, which attempted to generalize the search for mathematical concepts to the search for useful heuristics.\n\n\n",
    "id": "253279",
    "title": "Automated Mathematician"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1126216",
    "text": "Anticipation (artificial intelligence)\n\nIn artificial intelligence (AI), anticipation is the concept of an agent making decisions based on predictions, expectations, or beliefs about the future. It is widely considered that anticipation is a vital component of complex natural cognitive systems. As a branch of AI, anticipatory systems is a specialization still echoing the debates from the 1980s about the necessity for AI for an internal model.\n\nElementary forms of artificial intelligence can be constructed using a policy based on simple if-then rules. An example of such a system would be an agent following the rules\n\nA system such as the one defined above might be viewed as inherently reactive because the decision making is based on the current state of the environment with no explicit regard to the future. An agent employing anticipation would try to predict the future state of the environment (weather in this case) and make use of the predictions in the decision making. For example,\n\nThese rules appear more proactive, because they explicitly take into account possible future events. Notice though that in terms of representation and reasoning, these two rule sets are identical, both behave in response to existing conditions. Note too that both systems assume the agent is proactively\n\nIn practice, systems incorporating reactive planning tend to be autonomous systems proactively pursuing at least one, and often many, goals. What defines anticipation in an AI model is the explicit existence of an inner model of the environment for the anticipatory system (sometimes including the system itself). For example, if the phrase \"it will probably rain\" were computed on line in real time, the system would be seen as anticipatory.\n\nIn 1985, Robert Rosen defined an anticipatory system as follows:\n\nIn Rosen's work, analysis of the example : \"It's raining outside, therefore take the umbrella\" does involve a prediction. It involves the prediction that \"If it is raining, I will get wet out there unless I have my umbrella\". In that sense, even though it is already raining outside, the decision to take an umbrella is not a purely reactive thing. It involves the use of predictive models which tell us what will happen if we don't take the umbrella, when it is already raining outside.\n\nTo some extent, Rosen's definition of anticipation applies to any system incorporating machine learning. At issue is how much of a system's behaviour should or indeed can be determined by reasoning over dedicated representations, how much by on-line planning, and how much must be provided by the system's designers.\n\nThe anticipation of future states is also a major evolutionary and cognitive advance (Sjolander 1995).\nAnticipatory agents belonging to Rosen's definition are easy to see in human mental capabilities of taking decisions at a certain time T taking into account the effects of their own actions at different future timescales T+k. However, Rosen (a theoretical biologist) describes ALL living organisms as examples of naturally occurring anticipatory systems, which means that there must be somatic predictive models (meaning, \"of the body\"; physical) as components within the organization of all living organisms. No mental process is required for anticipation. In his book, Anticipatory Systems, Rosen describes how even single cellular organisms manifest this behavior pattern. It is logical to hypothesize therefore: If it is true that life is anticipatory in this sense, then the evolution of the conscious mind (such as human beings experience) may be a natural concentration and amplification of the anticipatory nature of life, itself.\n\nMachine learning methods started to integrate anticipatory capabilities in an implicit form as in reinforcement learning systems (Sutton & Barto, 1998; Balkenius, 1995) where they learn to anticipate future rewards and punishments caused by current actions (Sutton & Barto, 1998). Moreover, anticipation enhanced performance of machine learning techniques to face with complex environments where agents have to guide their attention to collect important information to act (Balkenius & Hulth, 1999).\n\nJürgen Schmidhuber modifies error back propagation algorithm to change neural network weights in order to decrease the mismatch between anticipated states and states actually experienced in the future (Schmidhuber - Adaptive curiosity and adaptive confidence, 1991). He introduces the concept of \"curiosity\" for agents as a measure of the mismatch between expectations and future experienced reality. Agents able to monitor and control their own curiosity explore situations where they expect to engage with novel experiences and are generally able to deal with complex environments more than the others.\n\n\n",
    "id": "1126216",
    "title": "Anticipation (artificial intelligence)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=419463",
    "text": "Mindpixel\n\nMindpixel was a web-based collaborative artificial intelligence project which aimed to create a knowledgebase of millions of human validated true/false statements, or probabilistic propositions. It ran from 2000 to 2005.\n\nParticipants in the project created one-line statements which aimed to be objectively true or false to 20 other anonymous participants. In order to submit their statement they had first to check the true/false validity of 20 such statements submitted by others. Participants whose replies were consistently out of step with the majority had their status downgraded and were eventually excluded. Likewise, participants who made contributions which others could not agree were objectively true or false had their status downgraded. A validated true/false statement is called a mindpixel.\n\nThe project enlisted the efforts of thousands of participants and claimed to be \"the planet's largest artificial intelligence effort\".\n\nThe project was conceived by Chris McKinstry, a computer scientist and former Very Large Telescope operator for the European Southern Observatory in Chile, as \"MISTIC\" (Minimum Intelligent Signal Test Item Corpus) in 1996. Mindpixel was developed out of this program, and started in 2000 and had 1.4 million mindpixels in January 2004. The database and its software is known as GAC, which stands for \"\"Generic Artificial Consciousness\"\" and is pronounced Jak.\n\nMcKinstry believed that the Mindpixel database could be used in conjunction with a neural net to produce a body of human \"common sense\" knowledge which would have market value. Participants in the project were promised shares in any future value according to the number of mindpixels they had successfully created.\n\nOn 20 September 2005 Mindpixel lost its free server and is no longer operational. It was being rewritten by Chris McKinstry as Mindpixel 2 and was intended to appear on a new server in France.\n\nChris McKinstry died of suicide on 23 January 2006 and the future of the project and the integrity of the data is uncertain. The mindpixel.com domain currently points to an IQ test web site.\n\nSome Mindpixel data have been utilized by Michael Spivey of Cornell University and Rick Dale of The University of Memphis to study theories of high-level reasoning and continuous temporal dynamics of thought. McKinstry, along with Dale and Spivey, designed an experiment that has now been published in Psychological Science in its January, 2008 issue. In this paper, McKinstry (as posthumous first author), Dale, and Spivey use a very small and carefully selected set of Mindpixel statements to show that even high-level thought processes like decision making can be revealed in the nonlinear dynamics of bodily action.\n\nOther similar AI-driven knowledge acquisition projects are Never-Ending Language Learning and Open Mind Common Sense (run by MIT), the latter being also hampered when its director died of suicide.\n\n\n",
    "id": "419463",
    "title": "Mindpixel"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1187268",
    "text": "Rational agent\n\nIn economics, game theory, decision theory, and artificial intelligence, a rational agent is an agent that has clear preferences, models uncertainty via expected values of variables or functions of variables, and always chooses to perform the action with the optimal expected outcome for itself from among all feasible actions. A rational agent can be anything that makes decisions, typically a person, firm, machine, or software. \n\nRational agents are also studied in the fields of cognitive science, ethics, and philosophy, including the philosophy of practical reason.\n\nIn reference to economics, rational agent refers to hypothetical consumers and how they make decisions in a free market. This concept is one of the assumptions made in neoclassical economic theory. The concept of economic rationality arises from a tradition of marginal analysis used in neoclassical economics. The idea of a rational agent is important to the philosophy of utilitarianism, as detailed by philosopher Jeremy Bentham's theory of the felicific calculus, also known as the hedonistic calculus.\n\nThe action a rational agent takes depends on:\nIn game theory and classical economics, it is often assumed that the actors, people, and firms are rational. However, the extent to which people and firms behave rationally is subject to debate. Economists often assume the models of rational choice theory and bounded rationality to formalize and predict the behavior of individuals and firms. Rational agents sometimes behave in manners that are counter-intuitive to many people, as in the traveler's dilemma.\n\nMany economic theories reject utilitarianism and rational agency, especially those that might be considered heterodox. \n\nFor example, Thorstein Veblen, known as the father of institutional economics, rejects the notion of hedonistic calculus and pure rationality saying: \"The hedonistic conception of man is that of a lightning calculator of pleasures and pains who oscillates like a homogeneous globule of desire of happiness under the impulse of stimuli that shift him about the area, but leave him intact.\" \n\nVeblen instead perceives human economic decisions as the result of multiple complex cumulative factors: \"It is the characteristic of man to do something, not simply to suffer pleasures and pains through the impact of suitable forces. He is ... a coherent structure of propensities and habits which seeks realization and expression in an unfolding activity. ... They are the products of his hereditary traits and his past experience, cumulatively wrought out under a given body of traditions conventionalities, and material circumstances; and they afford the point of departure for the next step in the process. The economic life history of the individual is a cumulative process of adaptation of means to ends that cumulatively change as the process goes on, both the agent and his environment being at any point the outcome of the last process.\"\nEvolutionary economics also provides criticisms of the Rational Agent, citing the \"parental bent\" (the idea that biological impulses can and do frequently override rational decision making based on utility). Arguments against rational agency have also cited the enormous influence of marketing as proof that humans can be persuaded to make economic decisions that are \"non-rational\" in nature.\n\nNeuroeconomics is a concept that uses neuroscience, social psychology and other fields of science to better understand how people make decisions. Unlike rational agent theory, neuroeconomics does not attempt to predict large-scale human behavior but rather how individuals make decisions in case-by-case scenarios.\n\nArtificial intelligence has borrowed the term \"rational agents\" from economics to describe autonomous programs that are capable of goal directed behavior. Today there is a considerable overlap between AI research, game theory and decision theory. Rational agents in AI are closely related to \"intelligent agents\", autonomous software programs that display intelligence.\n\n\n\n",
    "id": "1187268",
    "title": "Rational agent"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=821197",
    "text": "Cognitive tutor\n\nA cognitive tutor is a particular kind of intelligent tutoring system that utilizes a cognitive model to provide feedback to students as they are working through problems. This feedback will immediately inform students of the correctness, or incorrectness, of their actions in the tutor interface; however, cognitive tutors also have the ability to provide context-sensitive hints and instruction to guide students towards reasonable next steps.\n\nThe name of Cognitive Tutor® now usually refers to a particular type of intelligent tutoring system produced by Carnegie Learning for high school mathematics based on John Anderson's ACT-R theory of human cognition. However, cognitive tutors were originally developed to test ACT-R theory for research purposes since the early 1980s and they are developed also for other areas and subjects such as computer programming and science. Cognitive Tutors can be implemented into classrooms as a part of blended learning that combines textbook and software activities.\n\nThe Cognitive Tutor programs utilize cognitive model and are based on model tracing and knowledge tracing. Model tracing means that the cognitive tutor checks every action performed by students such as entering a value or clicking a button, while knowledge tracing is used to calculate the required skills students learned by measuring them on a bar chart called Skillometer. Model tracing and knowledge tracing are essentially used to monitor students' learning progress, guide students to correct path to problem solving, and provide feedback.\n\nThe Institute of Education Sciences published several reports regarding the effectiveness of Carnegie Cognitive Tutor®. A 2013 report concluded that Carnegie Learning Curricula and Cognitive Tutor® was found to have mixed effects on mathematics achievement for high school students. The report identified 27 studies that investigate the effectiveness of Cognitive Tutor ®, and the conclusion is based on 6 studies that meet What Works Clearinghouse standards. Among the 6 studies included, 5 of them show intermediate to significant positive effect, while 1 study shows statistically significant negative effect. Another report published by Institute of Education Sciences in 2009 found that Cognitive Tutor® Algebra I to have potentially positive effects on math achievement based on only 1 study out of 14 studies that meets What Works Clearinghouse standards.it should be understood that What Works Clearinghouse standards call for relatively large numbers of participants, true random assignments to groups, and for a control group receiving either no treatment or a different treatment. Such experimental conditions are difficult to meet in schools, and thus only a small percentage of studies in education meet the standards of this clearinghouse, even though they may still be of value.\n\nIntelligent tutoring systems (ITS) traditionally had a three-component architecture: domain model, student model, and tutoring model. Later, a fourth component was added: the interface component. Now ITS is commonly known to have a four component architecture.\n\nDomain model contains the rules, concepts, and knowledge related to the domain to be learned. It helps to evaluate students' performance and detect students' errors by setting a standard of domain expertise.\n\nStudent model, the central component of an ITS, is expected to contain as much knowledge as possible about the students: their cognitive and affective states, and the progress they gained as they learn. The functions of the student model is three folded: to gather data from and about the learner, to represent the learner's knowledge and learning process, and to perform diagnosis of students' knowledge and select optimal pedagogical strategies.\n\nBased on the data gained from domain model and student model, tutoring model makes decisions about tutoring strategies such as whether or not to intervene, when and how to intervene. Functions of tutoring model include instruction delivery and content planning.\n\nInterface model reflects the decisions made by tutoring model in different forms such as Socratic dialogs, feedback and hints. Students interact with the tutor through the learning interface, also known as communication. Interface also provide domain knowledge elements.\n\nA \"cognitive model\" tries to model the domain knowledge in the same way knowledge is represented in the human mind. Cognitive model enables intelligent tutoring systems to respond to problem-solving situations as the learner would. A tutoring system adopting a cognitive model is called a cognitive tutor.\n\nCognitive model is an expert system which hosts a multitude of solutions to the problems presented to students. The cognitive model is used to trace each student's solution through complex problems, enabling the tutor to provide step-by-step feedback and advice, and to maintain a targeted model of the student's knowledge based on student performance.\n\nCognitive Tutors provide step-by-step guidance as a learner develops a complex problem-solving skill through practice. Typically, cognitive tutors provide such forms of support as: (a) a problem-solving environment that is designed rich and \"thinking visible\"; (b) step-by-step feedback on student performance; (c) feedback messages specific to errors; (d) context-specific next-step hints at student's request, and (e) individualized problem selection.\n\nCognitive Tutors accomplish two of the principal tasks characteristic of human tutoring: (1) monitors the student's performance and providing context-specific individual instruction, and (2) monitors the student's learning and selects appropriate problem-solving activities.\n\nBoth cognitive model and two underlying algorithms, model tracing and knowledge tracing, are used to monitor the student's learning. In model tracing, the cognitive tutor uses the cognitive model in complex problems to follow the student's individual path and provide prompt accuracy feedback and context-specific advice. In knowledge tracing, the cognitive tutor uses a simple Bayesian method of evaluating the student's knowledge and uses this student model to select appropriate problems for individual student.\n\nCognitive tutors use a cognitive architecture which specifies the underlying framework for an intelligent system.\n\nACT-R, a member of ACT family, is the most recent cognitive architecture, devoted primarily to modelling human behavior. ACT-R includes a declarative memory of factual knowledge and a procedural memory of production rules. The architecture functions by matching productions on perceptions and facts, mediated by the real-valued activation levels of objects, and executing them to affect the environment or alter declarative memory. ACT-R has been used to model psychological aspects such as memory, attention, reasoning, problem solving, and language processing.\n\nIn 1984, a group of high school students participated in a mini course in computer science and tried out the geometry tutor through Carnegie Mellon University using the LISP tutor, one of the first iterations of the cognitive tutor.\nSince then, cognitive tutors have been used in a variety of scenarios, with a few organizations developing their own cognitive tutor programs. These programs have been used for students spanning elementary school through to university level, though primarily in the subject areas of Computer Programming, Mathematics, and Science.\nOne of the first organizations to develop a system for use within the school system was the PACT Center at Carnegie Mellon University. Their aim was to \"...develop systems that provide individualized assistance to students as they work on challenging real-world problems in complex domains such as computer programming, algebra and geometry\". PACT's most successful product was the \"Cognitive Tutor Algebra\" course. Originally created in the early 1990s, this course, at its peak, was offered at 75 schools through the U.S. by 1999, and then its spin-off company, Carnegie Learning, now offers tutors to over 1400 schools in the U.S.\n\nThe Carnegie Mellon Cognitive Tutor has attempted to raise students' math test scores in high school and middle-school classrooms, and their Algebra course was designated one of five exemplary curricula for K-12 mathematics educated by the US Department of Education.\nThere were several research projects conducted by the PACT Center to utilize Cognitive tutor for courses in Excel and to develop an intelligent tutoring system for algebra expression writing, called \"Ms. Lindquist\". Initially used by thousands of students, the aim of the Ms. Lindquist program was to have a tutor that could carry on running conversations with a student, with the intent of providing a more human connection. Further, in 2005, Carnegie Learning released \"Bridge to Algebra\", a product intended for middle schools that was piloted in over 100 schools.\n\nCognitive tutoring software is continuing to be used. According to a recent \"Business Insider Report\" article, Ken Koedinger, a professor of human-computer interaction and psychology at Carnegie Mellon University, says teachers can integrate cognitive tutoring software into the classroom. He suggests that teachers use it in a computer lab environment or during classes. In its more current versions, cognitive tutors can understand the many ways that a student might answer a problem, and then assist the student at the exact time that the help is required. Further, the cognitive tutor can customize exercises specific to the student's needs.\n\nAt this time it is unclear whether Cognitive Tutor® is effective at improving student performance. Cognitive Tutor® has had some commercial success however, there may be limitations inherently linked to its design and the nature of intelligent tutoring systems. The following section discusses limitations of Cognitive Tutor® which may also apply to other intelligent tutoring systems.\n\nAt this time, creating a Cognitive Tutor® for all subject areas is not practical or economical. Cognitive Tutor® has been used successfully but is still limited to tutoring algebra, computer programming and geometry because these subject areas have an optimal balance of production rules, complexity and maximum benefit to the learner.\n\nThe focus of Cognitive Tutor® development has been the design of the software to teach specific production rules and not on the development of curricular content. Despite many years of trials, improvements, and a potential to advance learning objectives, the creators continue to rely primarily on outside sources for curricular direction.\n\nThe complexity of Cognitive Tutor® software requires designers to spend hundreds of hours per instructional hour to create the program. Despite the time invested, the challenges associated with meeting the needs of the learner within the constraints of the design often result in compromises in flexibility and cognitive fidelity.\n\nPracticality dictates that designers must choose from a discrete set of methods to teach and support learners. Limited choices of methods, prompts and hints may be effective in supporting some learners but may conflict with the methods already in use by others. In addition, it is possible that learners will use the system of prompts and hints to access the answers prematurely thereby advancing through the exercises which may result in them not meeting the learning objectives.\n\nThe cognitive model, which inspired Cognitive Tutor® is based on assumptions about how learning occurs which dictates the chosen instructional methods such as hints, directions and timing of the tutoring prompts. Given these assumptions and the limited methods of presentation, Cognitive Tutor® may not account for the flexible, complex and diverse ways humans create knowledge. Human tutors outperform Cognitive Tutor® by providing a higher level of responsiveness to student errors. They are capable of providing more effective feedback and scaffolding to learners than Cognitive Tutor®, indicating the cognitive model may still be incomplete.\n\n\n",
    "id": "821197",
    "title": "Cognitive tutor"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=658223",
    "text": "Autonomic computing\n\nAutonomic computing (also known as AC) refers to the self-managing characteristics of distributed computing resources, adapting to unpredictable changes while hiding intrinsic complexity to operators and users. Initiated by IBM in 2001, this initiative ultimately aimed to develop computer systems capable of self-management, to overcome the rapidly growing complexity of computing systems management, and to reduce the barrier that complexity poses to further growth.\nThe AC system concept is designed to make adaptive decisions, using high-level policies. It will constantly check and optimize its status and automatically adapt itself to changing conditions. An autonomic computing framework is composed of autonomic components (AC) interacting with each other. An AC can be modeled in terms of two main control schemes (local and global) with sensors (for self-monitoring), effectors (for self-adjustment), knowledge and planner/adapter for exploiting policies based on self- and environment awareness. This architecture is sometimes referred to as Monitor-Analyze-Plan-Execute (MAPE).\n\nDriven by such vision, a variety of architectural frameworks based on \"self-regulating\" autonomic components has been recently proposed. A very similar trend has recently characterized significant research in the area of multi-agent systems. However, most of these approaches are typically conceived with centralized or cluster-based server architectures in mind and mostly address the need of reducing management costs rather than the need of enabling complex software systems or providing innovative services. Some autonomic systems involve mobile agents interacting via loosely coupled communication mechanisms.\n\n\"Autonomy-oriented computation\" is a paradigm proposed by Jiming Liu in 2001 that uses artificial systems imitating social animals' collective behaviours to solve difficult computational problems. For example, ant colony optimization could be studied in this paradigm.\n\nForecasts suggest that the number of computing devices in use will grow at 38% per year and the average complexity of each device is increasing. Currently, this volume and complexity is managed by highly skilled humans; but the demand for skilled IT personnel is already outstripping supply, with labour costs exceeding equipment costs by a ratio of up to 18:1. Computing systems have brought great benefits of speed and automation but there is now an overwhelming economic need to automate their maintenance.\n\nIn a 2003 IEEE \"Computer Magazine\" article, Kephart and Chess \nwarn that the dream of interconnectivity of computing systems and devices could become the “nightmare of pervasive computing” in which architects are unable to anticipate, design and maintain the complexity of interactions. They state the essence of autonomic computing is system self-management, freeing administrators from low-level task management while delivering better system behavior.\n\nA general problem of modern distributed computing systems is that their complexity, and in particular the complexity of their management, is becoming a significant limiting factor in their further development. Large companies and institutions are employing large-scale computer networks for communication and computation. The distributed applications running on these computer networks are diverse and deal with many tasks, ranging from internal control processes to presenting web content to customer support.\n\nAdditionally, mobile computing is pervading these networks at an increasing speed: employees need to communicate with their companies while they are not in their office. They do so by using laptops, personal digital assistants, or mobile phones with diverse forms of wireless technologies to access their companies' data.\n\nThis creates an enormous complexity in the overall computer network which is hard to control manually by human operators. Manual control is time-consuming, expensive, and error-prone. The manual effort needed to control a growing networked computer-system tends to increase very quickly.\n\n80% of such problems in infrastructure happen at the client specific application and database layer. Most 'autonomic' service providers guarantee only up to the basic plumbing layer (power, hardware, operating system, network and basic database parameters).\n\nA possible solution could be to enable modern, networked computing systems to manage themselves without direct human intervention. The \"Autonomic Computing Initiative\" (ACI) aims at providing the foundation for autonomic systems. It is inspired by the autonomic nervous system of the human body. This nervous system controls important bodily functions (e.g. respiration, heart rate, and blood pressure) without any conscious intervention.\n\nIn a self-managing autonomic system, the human operator takes on a new role: instead of controlling the system directly, he/she defines general policies and rules that guide the self-management process. For this process, IBM defined the following four types of property referred to as self-star (also called self-*, self-x, or auto-*) properties. \n\nOthers such as Poslad and Nami and Bertel have expanded on the set of self-star as follows:\n\nIBM has set forth eight conditions that define an autonomic system:\n\nThe system must\n\nEven though the purpose and thus the behaviour of autonomic systems vary from system to system, every autonomic system should be able to exhibit a minimum set of properties to achieve its purpose:\n\n\nIBM defined five evolutionary levels, or the \"autonomic deployment model\", for the deployment of autonomic systems:\n\nThe design complexity of Autonomic Systems can be simplified by utilizing design patterns such as the model-view-controller (MVC) pattern to improve concern separation by encapsulating functional concerns.\n\nA basic concept that will be applied in Autonomic Systems are closed control loops. This well-known concept stems from Process Control Theory. Essentially, a closed control loop in a self-managing system monitors some resource (software or hardware component) and autonomously tries to keep its parameters within a desired range.\n\nAccording to IBM, hundreds or even thousands of these control loops are expected to work in a large-scale self-managing computer system.\n\nA fundamental building block of an autonomic system is the sensing capability (\"Sensors S\"), which enables the system to observe its external operational context. Inherent to an autonomic system is the knowledge of the \"Purpose\" (intention) and the \"Know-how\" to operate itself (e.g., bootstrapping, configuration knowledge, interpretation of sensory data, etc.) without external intervention. The actual operation of the autonomic system is dictated by the \"Logic\", which is responsible for making the right decisions to serve its \"Purpose\", and influence by the observation of the operational context (based on the sensor input).\n\nThis model highlights the fact that the operation of an autonomic system is purpose-driven. This includes its mission (e.g., the service it is supposed to offer), the policies (e.g., that define the basic behaviour), and the “survival instinct”. If seen as a control system this would be encoded as a feedback error function or in a heuristically assisted system as an algorithm combined with set of heuristics bounding its operational space.\n\n\n",
    "id": "658223",
    "title": "Autonomic computing"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=2368264",
    "text": "Connectionist expert system\n\nConnectionist expert systems are artificial neural network (ANN) based expert systems where the ANN generates inferencing rules e.g., fuzzy-multi layer perceptron where linguistic and natural form of inputs are used. Apart from that, rough set theory may be used for encoding knowledge in the weights better and also genetic algorithms may be used to optimize the search solutions better. Symbolic reasoning methods may also be incorporated (see hybrid intelligent system). (Also see expert system, neural network, clinical decision support system.)\n\n",
    "id": "2368264",
    "title": "Connectionist expert system"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=2715076",
    "text": "Neuro-fuzzy\n\nIn the field of artificial intelligence, neuro-fuzzy refers to combinations of artificial neural networks and fuzzy logic.\n\nNeuro-fuzzy hybridization results in a hybrid intelligent system that synergizes these two techniques by combining the human-like reasoning style of fuzzy systems with the learning and connectionist structure of neural networks. Neuro-fuzzy hybridization is widely termed as fuzzy neural network (FNN) or neuro-fuzzy system (NFS) in the literature. Neuro-fuzzy system (the more popular term is used henceforth) incorporates the human-like reasoning style of fuzzy systems through the use of fuzzy sets and a linguistic model consisting of a set of IF-THEN fuzzy rules. The main strength of neuro-fuzzy systems is that they are universal approximators with the ability to solicit interpretable IF-THEN rules.\n\nThe strength of neuro-fuzzy systems involves two contradictory requirements in fuzzy modeling: interpretability versus accuracy. In practice, one of the two properties prevails. The neuro-fuzzy in fuzzy modeling research field is divided into two areas: linguistic fuzzy modeling that is focused on interpretability, mainly the Mamdani model; and precise fuzzy modeling that is focused on accuracy, mainly the Takagi-Sugeno-Kang (TSK) model.\n\nAlthough generally assumed to be the realization of a fuzzy system through connectionist networks, this term is also used to describe some other configurations including:\n\nIt must be pointed out that interpretability of the Mamdani-type neuro-fuzzy systems can be lost. To improve the interpretability of neuro-fuzzy systems, certain measures must be taken, wherein important aspects of interpretability of neuro-fuzzy systems are also discussed.\n\nA recent research line addresses the data stream mining case, where neuro-fuzzy systems are sequentially updated with new incoming samples on demand and on-the-fly. Thereby, system updates do not only include a recursive adaptation of model parameters, but also a dynamic evolution and pruning of model components (neurons, rules), in order to handle concept drift and dynamically changing system behavior adequately and to keep the systems/models \"up-to-date\" anytime. \nComprehensive surveys of various evolving neuro-fuzzy systems approaches can be found in and.\n\nPseudo outer product-based fuzzy neural networks (POPFNN) are a family of neuro-fuzzy systems that are based on the linguistic fuzzy model.\n\nThree members of POPFNN exist in the literature:\n\nThe \"POPFNN\" architecture is a five-layer neural network where the layers from 1 to 5 are called: input linguistic layer, condition layer, rule layer, consequent layer, output linguistic layer. The fuzzification of the inputs and the defuzzification of the outputs are respectively performed by the input linguistic and output linguistic layers while the fuzzy inference is collectively performed by the rule, condition and consequence layers.\n\nThe learning process of POPFNN consists of three phases:\n\nVarious fuzzy membership generation algorithms can be used: Learning Vector Quantization (LVQ), Fuzzy Kohonen Partitioning (FKP) or Discrete Incremental Clustering (DIC). Generally, the POP algorithm and its variant LazyPOP are used to identify the fuzzy rules.\n\n\n",
    "id": "2715076",
    "title": "Neuro-fuzzy"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=2840305",
    "text": "Computer-assisted proof\n\nA computer-assisted proof is a mathematical proof that has been at least partially generated by computer.\n\nMost computer-aided proofs to date have been implementations of large proofs-by-exhaustion of a mathematical theorem. The idea is to use a computer program to perform lengthy computations, and to provide a proof that the result of these computations implies the given theorem. In 1976, the four color theorem was the first major theorem to be verified using a computer program.\n\nAttempts have also been made in the area of artificial intelligence research to create smaller, explicit, new proofs of mathematical theorems from the bottom up using machine reasoning techniques such as heuristic search. Such automated theorem provers have proved a number of new results and found new proofs for known theorems. Additionally, interactive proof assistants allow mathematicians to develop human-readable proofs which are nonetheless formally verified for correctness. Since these proofs are generally human-surveyable (albeit with difficulty, as with the proof of the Robbins conjecture) they do not share the controversial implications of computer-aided proofs-by-exhaustion.\n\nOne method for using computers in mathematical proofs is by means of so-called validated numerics or rigorous numerics. This means computing numerically yet with mathematical rigour. One uses set-valued arithmetic and inclusion principle in order to ensure that the set-valued output of a numerical program encloses the solution of the original mathematical problem. This is done by controlling, enclosing and propagating round-off and truncation errors using for example interval arithmetic. More precisely, one reduces the computation to a sequence of elementary operations, say (+,-,*,/). In a computer, the result of each elementary operation is rounded off by the computer precision. However, one can construct an interval provided by upper and lower bounds on the result of an elementary operation. Then one proceeds by replacing numbers with intervals and performing elementary operations between such intervals of representable numbers.\n\nComputer-assisted proofs are the subject of some controversy in the mathematical world, with Thomas Tymoczko first to articulate objections. Those who adhere to Tymoczko's arguments believe that lengthy computer-assisted proofs are not, in some sense, 'real' mathematical proofs because they involve so many logical steps that they are not practically verifiable by human beings, and that mathematicians are effectively being asked to replace logical deduction from assumed axioms with trust in an empirical computational process, which is potentially affected by errors in the computer program, as well as defects in the runtime environment and hardware.\n\nOther mathematicians believe that lengthy computer-assisted proofs should be regarded as \"calculations\", rather than \"proofs\": the proof algorithm itself should be proved valid, so that its use can then be regarded as a mere \"verification\". Arguments that computer-assisted proofs are subject to errors in their source programs, compilers, and hardware can be resolved by providing a formal proof of correctness for the computer program (an approach which was successfully applied to the four-color theorem in 2005) as well as replicating the result using different programming languages, different compilers, and different computer hardware.\n\nAnother possible way of verifying computer-aided proofs is to generate their reasoning steps in a machine-readable form, and then use an automated theorem prover to demonstrate their correctness. This approach of using a computer program to prove another program correct does not appeal to computer proof skeptics, who see it as adding another layer of complexity without addressing the perceived need for human understanding.\n\nAnother argument against computer-aided proofs is that they lack mathematical elegance—that they provide no insights or new and useful concepts. In fact, this is an argument that could be advanced against any lengthy proof by exhaustion.\n\nAn additional philosophical issue raised by computer-aided proofs is whether they make mathematics into a quasi-empirical science, where the scientific method becomes more important than the application of pure reason in the area of abstract mathematical concepts. This directly relates to the argument within mathematics as to whether mathematics is based on ideas, or \"merely\" an exercise in formal symbol manipulation. It also raises the question whether, if according to the Platonist view, all possible mathematical objects in some sense \"already exist\", whether computer-aided mathematics is an observational science like astronomy, rather than an experimental one like physics or chemistry. Interestingly, this controversy within mathematics is occurring at the same time as questions are being asked in the physics community about whether twenty-first century theoretical physics is becoming too mathematical, and leaving behind its experimental roots.\n\nThe emerging field of experimental mathematics is confronting this debate head-on by focusing on numerical experiments as its main tool for mathematical exploration.\n\nIn 2010, academics at The University of Edinburgh offered people the chance to \"buy their own theorem\" created through a computer-assisted proof. This new theorem would be named after the purchaser.\n\nInclusion in this list does not imply that a formal computer-checked proof exists, but rather, that a computer program has been involved in some way. See the main articles for details.\n\n\n\n",
    "id": "2840305",
    "title": "Computer-assisted proof"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=2932246",
    "text": "Hybrid intelligent system\n\nHybrid intelligent system denotes a software system which employs, in parallel, a combination of methods and techniques from artificial intelligence subfields as:\n\nFrom the cognitive science perspective, every natural intelligent system is hybrid because it performs mental operations on both the symbolic and subsymbolic levels. For the past few years there has been an increasing discussion of the importance of A.I. Systems Integration. Based on notions that there have already been created simple and specific AI systems (such as systems for computer vision, speech synthesis, etc., or software that employs some of the models mentioned above) and now is the time for integration to create broad AI systems. Proponents of this approach are researchers such as Marvin Minsky, Ron Sun, Aaron Sloman, and Michael A. Arbib.\n\nAn example hybrid is a hierarchical control system in which the lowest, reactive layers are sub-symbolic. The higher layers, having relaxed time constraints, are capable of reasoning from an abstract world model and performing planning.\n\nIntelligent systems usually rely on hybrid reasoning processes, which include induction, deduction, abduction and reasoning by analogy.\n\n\n",
    "id": "2932246",
    "title": "Hybrid intelligent system"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=3291080",
    "text": "Rough fuzzy hybridization\n\nRough fuzzy hybridization is a method of hybrid intelligent system or soft computing, where Fuzzy set theory is used for linguistic representation of patterns, leading to a \"fuzzy granulation\" of the feature space. Rough set theory is used to obtain dependency rules which model informative regions in the granulated feature space.\n\n",
    "id": "3291080",
    "title": "Rough fuzzy hybridization"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=2708995",
    "text": "Model-based reasoning\n\nIn artificial intelligence, model-based reasoning refers to an inference method used in expert systems based on a model of the physical world. With this approach, the main focus of application development is developing the model. Then at run time, an \"engine\" combines this model knowledge with observed data to derive conclusions such as a diagnosis or a prediction.\n\nIn a model-based reasoning system knowledge can be represented using causal rules. For example, in a medical diagnosis system the knowledge base may contain the following rule:\nIn contrast in a diagnostic reasoning system knowledge would be represented through diagnostic rules such as:\n\nThere are many other forms of models that may be used. Models might be quantitative (for instance, based on mathematical equations) or qualitative (for instance, based on cause/effect models.) They may include representation of uncertainty. They might represent behavior over time. They might represent \"normal\" behavior, or might only represent abnormal behavior, as in the case of the examples above. Model types and usage for model-based reasoning are discussed in.\n\n\n",
    "id": "2708995",
    "title": "Model-based reasoning"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=637434",
    "text": "Principle of rationality\n\nThe 'principle of rationality' (or 'rationality principle') was coined by Karl R. Popper in his Harvard Lecture of 1963, and published in his book \"Myth of Framework\". It is related to what he called the 'logic of the situation' in an \"Economica\" article of 1944/1945, published later in his book \"The Poverty of Historicism\". According to Popper’s rationality principle, agents act in the most adequate way according to the objective situation. It is an idealized conception of human behavior which he used to drive his model of situational analysis.\n\nPopper called for social science to be grounded in what he called situational analysis. This requires building models of social situations which include individual actors and their relationship to social institutions, e.g. markets, legal codes, bureaucracies, etc. These models attribute certain aims and information to the actors. This forms the 'logic of the situation', the result of reconstructing meticulously all circumstances of an historical event. The 'principle of rationality' is the assumption that people are instrumental in trying to reach their goals, and this is what drives the model. Popper believed that this model could be continuously refined to approach the objective truth.\n\nPopper called his 'principle of rationality' \"nearly empty\" (a technical term meaning without empirical content) and \"strictly speaking false\", but nonetheless \"tremendously useful\". These remarks earned him a lot of criticism because seemingly he had swerved from his famous Logic of Scientific Discovery.\n\nAmong the many philosophers having discussed his 'principle of rationality' from the 1960s up to now are Noretta Koertge, R. Nadeau, Viktor J. Vanberg, Hans Albert, E. Matzner, Ian C. Jarvie, Mark A. Notturno, John Wettersten, Ian C. Böhm.\n\nIn the context of knowledge-based systems, Newell (in 1982) proposed the following principle of rationality: \"If an agent has knowledge that one of its actions will lead to one of its goals, then the agent will select that action.\" This principle is employed by agents at the knowledge level to move closer to a desired goal. An important philosophical difference between Newell and Popper is that Newell argued that the knowledge level is real in the sense that it exists in nature and is not made up. This allowed Newell to treat the rationality principle as a way of understanding nature and avoid the problems Popper ran into by treating knowledge as non physical and therefore non empirical. \n\n",
    "id": "637434",
    "title": "Principle of rationality"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=3070989",
    "text": "ICAD (software)\n\nICAD (Corporate history: ICAD, Inc., Concentra (name change at IPO in 1995), KTI (name change in 1998), Dassault Systemes (purchase in 2001) () is a Knowledge-Based Engineering (KBE) system that enables users to encode design knowledge using a semantic representation that can be evaluated for parasolid output. ICAD has an open architecture that can utilize all the power and flexibility of the underlying language.\n\nKBE, as implemented via ICAD, received a lot of attention due to the remarkable results that appeared to take little effort. ICAD allowed one example of end-user computing that in a sense is unparalleled. Most ICAD developers were degreed engineers. Systems developed by ICAD users were non-trivial and consisted of highly complicated code. In the sense of end-user computing, ICAD was the first to allow the power of a domain tool to be in the hands of the user at the same time being open to allow extensions as identified and defined by the domain expert or SME.\n\nA COE article looked at the resulting explosion of expectations (see AI Winter), which were not sustainable. However, such a bubble burst does not diminish the existence of capability that would exist if expectations and use were properly managed.\n\nThe original implementation of ICAD was on a Lisp machine (Symbolics). Some of the principals involved with the development were Larry Rosenfeld, Avrum Belzer, Patrick M. O'Keefe, Philip Greenspun, and David F. Place. The time frame was 1984–85.\n\nICAD started on special-purpose Symbolics Lisp hardware and was then ported to Unix when Common Lisp became portable to general-purpose workstations.\n\nThe original domain for ICAD was mechanical design with many application successes. However, ICAD has found use in other domains, such as electrical design, shape modeling, etc. An example project could be wind tunnel design or the development of a support tool for aircraft multidisciplinary design. Further examples can be found in the presentations at the annual IIUG (International ICAD Users Group) that have been published in the KTI Vault (1999 through 2002). Boeing and Airbus used ICAD extensively to develop various components in the 1990s and early 21st century.\n\nAs of 2003, ICAD was featured strongly in several areas as evidenced by the Vision & Strategy Product Vision and Strategy presentation. After 2003, ICAD use diminished. At the end of 2001, the KTI Company faced financial difficulties and laid off most of its best staff. They were eventually bought out by Dassault who effectively scuppered the ICAD product. See IIUG at COE, 2003 (first meeting due to Dassault by KTI)\n\nThe ICAD system was very expensive, relatively, and was in the price range of high-end systems. Market dynamics couldn't support this as there may not have been sufficient differentiating factors between ICAD and the lower-end systems (or the promises from Dassault). KTI was absorbed by Dassault Systemes and ICAD is no longer considered the go-forward tool for knowledge-based engineering (KBE) applications by that company. Dassault Systemes is promoting a suite of tools oriented around version 5 of their popular CATIA CAD application, with Knowledgeware the replacement for ICAD.\n\n, things were still a bit unclear. ICAD 8.3 was delivered.\nThe recent COE Aerospace Conference had a discussion about the futures of KBE. One issue involves the stacking of 'meta' issues within a computer model. How this is resolved, whether by more icons or the availability of an external language, remains to be seen.\n\nThe Genworks GDL product (including kernel technology from the Gendl Project) is the nearest functional equivalent to ICAD currently available.\n\nICAD provided a declarative language (IDL) using New Flavors (never converted to CLOS) that supported a mechanism for relating parts (defpart) via a hierarchical set of relationships. Technically, the ICAD Defpart was a Lisp macro; the ICAD defpart list was a set of generic classes that can be instantiated with specific properties depending upon what was represented. This defpart list was extendible via composited parts that represented domain entities. Along with the part-subpart relations, ICAD supported generic relations via the object modeling capabilities of Lisp.\n\nExample applications of ICAD range from a small collection of defparts that represents a part or component to a larger collection that represents an assembly. In terms of power, an ICAD system, when fully specified, can generate thousands of instances of parts on a major assembly design.\n\nOne example of an application driving thousands of instances of parts is that of an aircraft wing - where fastener type and placement may number in the thousands, each instance requiring evaluation of several factors driving the design parameters.\n\nOne role for ICAD may be serving as the defining prototype for KBE which would require that we know more about what occurred the past 15 years (much information is tied up behind corporate firewalls and under proprietary walls). With the rise of the functional languages (an example is Haskell) on the landscape, perhaps some of the power that is attributable to Lisp may be replicated.\n\n\n",
    "id": "3070989",
    "title": "ICAD (software)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=3907217",
    "text": "Hybrid neural network\n\nThe term hybrid neural network can have two meanings: \n\nAs for the first meaning, the artificial neurons and synapses in hybrid networks can be digital or analog. For the digital variant voltage clamps are used to monitor the membrane potential of neurons, to computationally simulate artificial neurons and synapses and to stimulate biological neurons by inducing synaptic. For the analog variant, specially designed electronic circuits connect to a network of living neurons through electrodes. \n\nAs for the second meaning, incorporating elements of symbolic computation and artificial neural networks into one model was an attempt to combine the advantages of both paradigms while avoid the shortcomings. Symbolic representations have advantages with respect to explicit, direct control, fast initial coding, dynamic variable binding and knowledge abstraction. Representations of artificial neural networks, on the other hand, show advantages for biological plausibility, learning, robustness (fault-tolerant processing and graceful decay), and generalization to similar input. Since the early 1990s many attempts have been made to reconcile the two approaches. \n\n\n",
    "id": "3907217",
    "title": "Hybrid neural network"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=2376785",
    "text": "ASR-complete\n\nASR-complete is, by analogy to \"NP-completeness\" in complexity theory, a term to indicate that the difficulty of a computational problem is equivalent to solving the central Automatic Speech Recognition problem, i.e. recognize and understanding spoken language. Unlike \"NP-completeness\", this term is typically used informally.\n\nSuch problems are hypothesised to include:\n\n\nThese problems are easy for humans to do (in fact, they are described directly in terms of imitating humans). Some systems can solve very simple restricted versions of these problems, but none can solve them in their full generality.\n\n\n\n",
    "id": "2376785",
    "title": "ASR-complete"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=339417",
    "text": "Symbolic artificial intelligence\n\nSymbolic artificial intelligence is the term for the collection of all methods in artificial intelligence research that are based on high-level \"symbolic\" (human-readable) representations of problems, logic and search. Symbolic AI was the dominant paradigm of AI research from the mid-1950s until the late 1980s.\nJohn Haugeland gave the name GOFAI (\"Good Old-Fashioned Artificial Intelligence\") to symbolic AI in his 1985 book \"Artificial Intelligence: The Very Idea\", which explored the philosophical implications of artificial intelligence research. In robotics the analogous term is GOFR (\"Good Old-Fashioned Robotics\").\nThe approach is based on the assumption that many aspects of intelligence can be achieved by the manipulation of symbols, an assumption defined as the \"physical symbol systems hypothesis\" by Allen Newell and Herbert A. Simon in the middle 1960s.\nThe most successful form of symbolic AI is expert systems, which use a network of production rules. Production rules connect symbols in a relationship similar to an If-Then statement. The expert system processes the rules to make deductions and to determine what additional information it needs, i.e. what questions to ask, using human-readable symbols.\nOpponents of the symbolic approach include roboticists such as Rodney Brooks, who aims to produce autonomous robots without symbolic representation (or with only minimal representation) and computational intelligence researchers, who apply techniques such as neural networks and optimization to solve problems in machine learning and control engineering.\nSymbolic AI was intended to produce general, human-like intelligence in a machine, whereas most modern research is directed at specific sub-problems. Research into general intelligence is now studied in the sub-field of artificial general intelligence.\nMachines were initially designed to formulate outputs based on the inputs that were represented by symbols. Symbols are used when the input is definite and falls under certainty. But when there is uncertainty involved, for example in formulating predictions, the representation is done using \"fuzzy logic\". This can be seen in artificial neural networks.\n\n",
    "id": "339417",
    "title": "Symbolic artificial intelligence"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=2934910",
    "text": "Cognitive robotics\n\nCognitive robotics is concerned with endowing a robot with intelligent behavior by providing it with a processing architecture that will allow it to learn and reason about how to behave in response to complex goals in a complex world. Cognitive robotics may be considered the engineering branch of embodied cognitive science and embodied embedded cognition.\n\nWhile traditional cognitive modeling approaches have assumed symbolic coding schemes as a means for depicting the world, translating the world into these kinds of symbolic representations has proven to be problematic if not untenable. Perception and action and the notion of symbolic representation are therefore core issues to be addressed in cognitive robotics.\n\nCognitive robotics views animal cognition as a starting point for the development of robotic information processing, as opposed to more traditional Artificial Intelligence techniques. Target robotic cognitive capabilities include perception processing, attention allocation, anticipation, planning, complex motor coordination, reasoning about other agents and perhaps even about their own mental states. Robotic cognition embodies the behavior of intelligent agents in the physical world (or a virtual world, in the case of simulated cognitive robotics). Ultimately the robot must be able to act in the real world.\n\nA preliminary robot learning technique called motor babbling involves correlating pseudo-random complex motor movements by the robot with resulting visual and/or auditory feedback such that the robot may begin to \"expect\" a pattern of sensory feedback given a pattern of motor output. Desired sensory feedback may then be used to inform a motor control signal. This is thought to be analogous to how a baby learns to reach for objects or learns to produce speech sounds. For simpler robot systems, where for instance inverse kinematics may feasibly be used to transform anticipated feedback (desired motor result) into motor output, this step may be skipped.\n\nOnce a robot can coordinate its motors to produce a desired result, the technique of \"learning by imitation\" may be used. The robot monitors the performance of another agent and then the robot tries to imitate that agent. It is often a challenge to transform imitation information from a complex scene into a desired motor result for the robot. Note that imitation is a high-level form of cognitive behavior and imitation is not necessarily required in a basic model of embodied animal cognition.\n\nA more complex learning approach is \"autonomous knowledge acquisition\": the robot is left to explore the environment on its own. A system of goals and beliefs is typically assumed.\n\nA somewhat more directed mode of exploration can be achieved by \"curiosity\" algorithms, such as Intelligent Adaptive Curiosity or Category-Based Intrinsic Motivation. These algorithms generally involve breaking sensory input into a finite number of categories and assigning some sort of prediction system (such as an Artificial Neural Network) to each. The prediction system keeps track of the error in its predictions over time. Reduction in prediction error is considered learning. The robot then preferentially explores categories in which it is learning (or reducing prediction error) the fastest.\n\nSome researchers in cognitive robotics have tried using architectures such as (ACT-R and Soar (cognitive architecture)) as a basis of their cognitive robotics programs. These highly modular symbol-processing architectures have been used to simulate operator performance and human performance when modeling simplistic and symbolized laboratory data. The idea is to extend these architectures to handle real-world sensory input as that input continuously unfolds through time. What is needed is a way to somehow translate the world into a set of symbols and their relationships.\n\nSome of the fundamental questions to still be answered in cognitive robotics are:\n\nCognitive Robotics book by Hooman Samani, takes a multidiciplinary approach to cover various aspects of cognitive robotics such as artificial intelligence, physical, chemical, philosophical, psychological, social, cultural, and ethical aspects.\n\n\n\n",
    "id": "2934910",
    "title": "Cognitive robotics"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=4832828",
    "text": "Extremal optimization\n\nExtremal optimization (EO) is an optimization heuristic inspired by the Bak–Sneppen model of self-organized criticality from the field of statistical physics. This heuristic was designed initially to address combinatorial optimization problems such as the travelling salesman problem and spin glasses, although the technique has been demonstrated to function in optimization domains.\n\nSelf-organized criticality (SOC) is a statistical physics concept to describe a class of dynamical systems that have a critical point as an attractor. Specifically, these are non-equilibrium systems that evolve through avalanches of change and dissipations that reach up to the highest scales of the system. SOC is said to govern the dynamics behind some natural systems that have these burst-like phenomena including landscape formation, earthquakes, evolution, and the granular dynamics of rice and sand piles. Of special interest here is the Bak–Sneppen model of SOC, which is able to describe evolution via punctuated equilibrium (extinction events) – thus modelling evolution as a self-organised critical process.\n\nAnother piece in the puzzle is work on computational complexity, specifically that critical points have been shown to exist in NP-complete problems, where near-optimum solutions are widely dispersed and separated by barriers in the search space causing local search algorithms to get stuck or severely hampered. It was the evolutionary self-organised criticality model by Bak and Sneppen and the observation of critical points in combinatorial optimisation problems that lead to the development of Extremal Optimization by Stefan Boettcher and Allon Percus.\n\nEO was designed as a local search algorithm for combinatorial optimization problems. Unlike genetic algorithms, which work with a population of candidate solutions, EO evolves a single solution and makes local modifications to the worst components. This requires that a suitable representation be selected which permits individual solution components to be assigned a quality measure (\"fitness\"). This differs from holistic approaches such as ant colony optimization and evolutionary computation that assign equal-fitness to all components of a solution based upon their collective evaluation against an objective function. The algorithm is initialized with an initial solution, which can be constructed randomly, or derived from another search process.\n\nThe technique is a fine-grained search, and superficially resembles a hill climbing (local search) technique. A more detailed examination reveals some interesting principles, which may have applicability and even some similarity to broader population-based approaches (evolutionary computation and artificial immune system). The governing principle behind this algorithm is that of improvement through selectively removing low-quality components and replacing them with a randomly selected component. This is obviously at odds with genetic algorithms, the quintessential evolutionary computation algorithm that selects good solutions in an attempt to make better solutions. \n\nThe resulting dynamics of this simple principle is firstly a robust hill climbing search behaviour, and secondly a diversity mechanism that resembles that of multiple-restart search. Graphing holistic solution quality over time (algorithm iterations) shows periods of improvement followed by quality crashes (avalanche) very much in the manner as described by punctuated equilibrium. It is these crashes or dramatic jumps in the search space that permit the algorithm to escape local optima and differentiate this approach from other local search procedures. Although such punctuated-equilibrium behaviour can be \"designed\" or \"hard-coded\", it should be stressed that this is an \"emergent\" effect of the negative-component-selection principle fundamental to the algorithm. \n\nEO has primarily been applied to combinatorial problems such as graph partitioning and the travelling salesman problem, as well as problems from statistical physics such as spin glasses.\n\nGeneralised extremal optimization (GEO) was developed to operate on bit strings where component quality is determined by the absolute rate of change of the bit, or the bits contribution to holistic solution quality. This work includes application to standard function optimisation problems as well as engineering problem domains. Another similar extension to EO is Continuous Extremal Optimization (CEO).\n\nEO has been applied to image rasterization as well as used as a local search after using ant colony optimization. EO has been used to identify structures in complex networks. EO has been used on a multiple target tracking problem. Finally, some work has been done on investigating the probability distribution used to control selection.\n\n\n\n",
    "id": "4832828",
    "title": "Extremal optimization"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1355398",
    "text": "Blackboard system\n\nA blackboard system is an artificial intelligence approach based on the blackboard architectural model, where a common knowledge base, the \"blackboard\", is iteratively updated by a diverse group of specialist knowledge sources, starting with a problem specification and ending with a solution. Each knowledge source updates the blackboard with a partial solution when its internal constraints match the blackboard state. In this way, the specialists work together to solve the problem. The blackboard model was originally designed as a way to handle complex, ill-defined problems, where the solution is the sum of its parts.\n\nThe following scenario provides a simple metaphor that gives some insight into how a blackboard functions:\nA group of specialists are seated in a room with a large blackboard. They work as a team to brainstorm a solution to a problem, using the blackboard as the workplace for cooperatively developing the solution.\n\nThe session begins when the problem specifications are written onto the blackboard. The specialists all watch the blackboard, looking for an opportunity to apply their expertise to the developing solution. When someone writes something on the blackboard that allows another specialist to apply their expertise, the second specialist records their contribution on the blackboard, hopefully enabling other specialists to then apply their expertise. This process of adding contributions to the blackboard continues until the problem has been solved.\n\nA blackboard-system application consists of three major components\n\n\nFamous examples of early academic blackboard systems are the Hearsay II speech recognition system and Douglas Hofstadter's Copycat and Numbo projects.\n\nMore recent examples include deployed real-world applications, such as the PLAN component of the Mission Control System for RADARSAT-1, an Earth observation satellite developed by Canada to monitor environmental changes and Earth's natural resources.\n\nGTXImage CAD software by GTX Corporation was developed in the early 1990s using a set of rulebases and neural networks as specialists operating on a blackboard system.\n\nAdobe Acrobat Capture (now discontinued) used a Blackboard system to decompose and recognize image pages to understand the objects, text, and fonts on the page. This function is currently built into the retail version of Adobe Acrobat as \"OCR Text Recognition\". Details of a similar OCR blackboard for Farsi text are in the public domain.\n\nBlackboard systems are used routinely in many military C4ISTAR systems for detecting and tracking objects.\n\nBlackboard systems were popular before the AI Winter and, along with most symbolic AI models, fell out of fashion during that period. Along with other models it was realised that initial successes on toy problems did not scale well to real problems on the available computers of the time. Most problems using blackboards are inherently NP-hard, so resist tractable solution by any algorithm in the large size limit. During the same period, statistical pattern recognition became dominant, most notably via simple Hidden Markov Models outperforming symbolic approaches such as Hearsay-II in the domain of speech recognition.\n\nBlackboard-like systems have been constructed within modern Bayesian machine learning settings, using agents to add and remove Bayesian network nodes. In these 'Bayesian Blackboard' systems, the heuristics can acquire more rigorous probabilistic meanings as proposal and acceptances in Metropolis Hastings sampling though the space of possible structures. Conversely, using these mappings, existing Metropolis-Hastings samplers over structural spaces may now thus be viewed as forms of blackboard systems even when not named as such by the authors. Such samplers are commonly found in musical transcription algorithms for example.\n\n\n\n",
    "id": "1355398",
    "title": "Blackboard system"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=5033373",
    "text": "Action selection\n\nAction selection is a way of characterizing the most basic problem of intelligent systems: what to do next. In artificial intelligence and computational cognitive science, \"the action selection problem\" is typically associated with intelligent agents and animats—artificial systems that exhibit complex behaviour in an agent environment. The term is also sometimes used in ethology or animal behavior.\n\nOne problem for understanding action selection is determining the level of abstraction used for specifying an \"act\". At the most basic level of abstraction, an atomic act could be anything from \"contracting a muscle cell\" to \"provoking a war\". Typically for any one action-selection mechanism, the set of possible actions is predefined and fixed.\n\nMost researchers working in this field place high demands on their agents: \n\nFor these reasons action selection is not trivial and attracts a good deal of research.\n\nThe main problem for action selection is complexity. Since all computation takes both time and space (in memory), agents cannot possibly consider every option available to them at every instant in time. Consequently, they must be biased, and constrain their search in some way. For AI, the question of action selection is \"what is the best way to constrain this search\"? For biology and ethology, the question is \"how do various types of animals constrain their search? Do all animals use the same approaches? Why do they use the ones they do?\"\n\nOne fundamental question about action selection is whether it is really a problem at all for an agent, or whether it is just a description of an emergent property of an intelligent agent's behavior. However, if we consider how we are going to build an intelligent agent, then it becomes apparent there must be \"some\" mechanism for action selection. This mechanism may be highly distributed (as in the case of distributed organisms such as social insect colonies or slime mold) or it may be a special-purpose module.\n\nThe action selection mechanism (ASM) determines not only the agent’s actions in terms of impact on the world, but also directs its perceptual attention, and updates its memory. These egocentric sorts of actions may in turn result in modifying the agents basic behavioural capacities, particularly in that updating memory implies some form of machine learning is possible. Ideally, action selection itself should also be able to learn and adapt, but there are many problems of combinatorial complexity and computational tractability that may require restricting the search space for learning.\n\nIn AI, an ASM is also sometimes either referred to as an agent architecture or thought of as a substantial part of one.\n\nGenerally, artificial action selection mechanisms can be divided into several categories: symbol-based systems sometimes known as classical planning, distributed solutions, and reactive or dynamic planning. Some approaches do not fall neatly into any one of these categories. Others are really more about providing scientific models than practical AI control; these last are described further in the next section.\n\nEarly in the history of artificial intelligence, it was assumed that the best way for an agent to choose what to do next would be to compute a probably optimal plan, and then execute that plan. This led to the physical symbol system hypothesis, that a physical agent that can manipulate symbols is necessary and sufficient for intelligence. Many software agents still use this approach for action selection. It normally requires describing all sensor readings, the world, all of ones actions and all of one's goals in some form of predicate logic. Critics of this approach complain that it is too slow for real-time planning and that, despite the proofs, it is still unlikely to produce optimal plans because reducing descriptions of reality to logic is a process prone to errors.\n\nSatisficing is a decision-making strategy which attempts to meet criteria for adequacy, rather than identify an optimal solution. A satisficing strategy may often, in fact, be (near) optimal if the costs of the decision-making process itself, such as the cost of obtaining complete information, are considered in the outcome calculus.\n\n\nIn contrast to the symbolic approach, distributed systems of action selection actually have no one \"box\" in the agent which decides the next action. At least in their idealized form, distributed systems have many modules running in parallel and determining the best action based on local expertise. In these idealized systems, overall coherence is expected to emerge somehow, possibly through careful design of the interacting components. This approach is often inspired by neural networks research. In practice, there is almost always \"some\" centralised system determining which module is \"the most active\" or has the most salience. There is evidence real biological brains also have such executive decision systems which evaluate which of the competing systems deserves the most attention, or more properly, has its desired actions disinhibited.\n\nBecause purely distributed systems are difficult to construct, many researchers have turned to using explicit hard-coded plans to determine the priorities of their system.\n\nDynamic or reactive planning methods compute just one next action in every instant based on the current context and pre-scripted plans. In contrast to classical planning methods, reactive or dynamic approaches do not suffer combinatorial explosion. On the other hand, they are sometimes seen as too rigid to be considered strong AI, since the plans are coded in advance. At the same time, natural intelligence can be rigid in some contexts although it is fluid and able to adapt in others.\n\nExample dynamic planning mechanisms include:\n\nSometimes to attempt to address the perceived inflexibility of dynamic planning, hybrid techniques are used. In these, a more conventional AI planning system searches for new plans when the agent has spare time, and updates the dynamic plan library when it finds good solutions. The important aspect of any such system is that when the agent needs to select an action, some solution exists that can be used immediately (see further anytime algorithm).\n\n\nMany dynamic models of artificial action selection were originally inspired by research in ethology. In particular, Konrad Lorenz and Nikolaas Tinbergen provided the idea of an innate releasing mechanism to explain instinctive behaviors (fixed action patterns). Influenced by the ideas of William McDougall, Lorenz developed this into a \"psychohydraulic\" model of the motivation of behavior. In ethology, these ideas were influential in the 1960s, but they are now regarded as outdated because of their use of an energy flow metaphor; the nervous system and the control of behavior are now normally treated as involving information transmission rather than energy flow. Dynamic plans and neural networks are more similar to information transmission, while spreading activation is more similar to the diffuse control of emotional / hormonal systems.\n\nStan Franklin has proposed that action selection is the right perspective to take in understanding the role and evolution of mind. See his page on the action selection paradigm.\n\nSome researchers create elaborate models of neural action selection. See for example:\n\n\n\n",
    "id": "5033373",
    "title": "Action selection"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=741104",
    "text": "Intelligent control\n\nIntelligent control is a class of control techniques that use various artificial intelligence computing approaches like neural networks, Bayesian probability, fuzzy logic, machine learning, evolutionary computation and genetic algorithms.\n\nIntelligent control can be divided into the following major sub-domains:\n\nNew control techniques are created continuously as new models of intelligent behavior are created and computational methods developed to support them.\n\nNeural networks have been used to solve problems in almost all spheres of science and technology. Neural network control basically involves two steps:\n\n\nIt has been shown that a feedforward network with nonlinear, continuous and differentiable activation functions have universal approximation capability. Recurrent networks have also been used for system identification. Given, a set of input-output data pairs, system identification aims to form a mapping among these data pairs. Such a network is supposed to capture the dynamics of a system.\n\nBayesian probability has produced a number of algorithms that are in common use in many advanced control systems, serving as state space estimators of some variables that are used in the controller.\n\nThe Kalman filter and the Particle filter are two examples of popular Bayesian control components. The Bayesian approach to controller design often requires an important effort in deriving the so-called system model and measurement model, which are the mathematical relationships linking the state variables to the sensor measurements available in the controlled system. In this respect, it is very closely linked to the\nsystem-theoretic approach to control design.\n\n\n\n",
    "id": "741104",
    "title": "Intelligent control"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=657145",
    "text": "Self-management (computer science)\n\nSelf-Management is the process by which computer systems shall manage their own operation without human intervention. Self-Management technologies are expected to pervade the next generation of network management systems.\n\nThe growing complexity of modern networked computer systems is currently the biggest limiting factor in their expansion. The increasing heterogeneity of big corporate computer systems, the inclusion of mobile computing devices, and the combination of different networking technologies like WLAN, cellular phone networks, and mobile ad hoc networks make the conventional, manual management very difficult, time-consuming, and error-prone. More recently self-management has been suggested as a solution to increasing complexity in cloud computing.\n\nCurrently, the most important industrial initiative towards realizing self-management is the Autonomic Computing Initiative (ACI) started by IBM in 2001. The ACI defines the following four functional areas:\n\n\nThe design complexity of Autonomic Systems and self-management systems can be simplified by utilizing design patterns such as the Model View Controller (MVC) to improve separation of concerns by helping encapsulate functional concerns.\n\n\n",
    "id": "657145",
    "title": "Self-management (computer science)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=5837233",
    "text": "Diagnosis (artificial intelligence)\n\nAs a subfield in artificial intelligence, Diagnosis is concerned with the development of algorithms and techniques that are able to determine whether the behaviour of a system is correct. If the system is not functioning correctly, the algorithm should be able to determine, as accurately as possible, which part of the system is failing, and which kind of fault it is facing. The computation is based on \"observations\", which provide information on the current behaviour.\n\nThe expression \"diagnosis\" also refers to the answer of the question of whether the system is malfunctioning or not, and to the process of computing the answer. This word comes from the medical context where a diagnosis is the process of identifying a disease by its symptoms.\n\nAn example of diagnosis is the process of a garage mechanic with an automobile. The mechanic will first try to detect any abnormal behavior based on the observations on the car and his knowledge of this type of vehicle. If he finds out that the behavior is abnormal, the mechanic will try to refine his diagnosis by using new observations and possibly testing the system, until he discovers the faulty component; the mechanic plays an important role in the vehicle diagnosis.\n\nThe expert diagnosis (or diagnosis by expert system) is based on experience with the system. Using this experience, a mapping is built that efficiently associates the observations to the corresponding diagnoses.\n\nThe experience can be provided:\n\nThe main drawbacks of these methods are:\n\nA slightly different approach is to build an expert system from a model of the system rather than directly from an expertise. An example is the computation of a diagnoser for the diagnosis of discrete event systems. This approach can be seen as model-based, but it benefits from some advantages and suffers some drawbacks of the expert system approach.\n\nModel-based diagnosis is an example of abductive reasoning using a model of the system. In general, it works as follows:\n\nWe have a model that describes the behaviour of the system (or artefact). The model is an abstraction of the behaviour of the system and can be incomplete. In particular, the faulty behaviour is generally little-known, and the faulty model may thus not be represented. Given observations of the system, the diagnosis system simulates the system using the model, and compares the observations actually made to the observations predicted by the simulation.\n\nThe modelling can be simplified by the following rules (where formula_1 is the \"Ab\"normal predicate):\n\nformula_2\n\nformula_3 (fault model)\n\nThe semantics of these formulae is the following: if the behaviour of the system is not abnormal (i.e. if it is normal), then the internal (unobservable) behaviour will be formula_4 and the observable behaviour formula_5. Otherwise, the internal behaviour will be formula_6 and the observable behaviour formula_7. Given the observations formula_8, the problem is to determine whether the system behaviour is normal or not (formula_9 or formula_10). This is an example of abductive reasoning.\n\nA system is said to be diagnosable if whatever the behavior of the system, we will be able to determine without ambiguity a unique diagnosis.\n\nThe problem of diagnosability is very important when designing a system because on one hand one may want to reduce the number of sensors to reduce the cost, and on the other hand one may want to increase the number of sensors to increase the probability of detecting a faulty behavior.\n\nSeveral algorithms for dealing with these problems exist. One class of algorithms answers the question whether a system is diagnosable; another class looks for sets of sensors that make the system diagnosable, and optionally comply to criteria such as cost optimization.\n\nThe diagnosability of a system is generally computed from the model of the system. In applications using model-based diagnosis, such a model is already present and doesn't need to be built from scratch.\n\n\nDX is the annual International Workshop on Principles of Diagnosis that started in 1989. \n\n",
    "id": "5837233",
    "title": "Diagnosis (artificial intelligence)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=6417891",
    "text": "Constructionist design methodology\n\nConstructionist design methodology (CDM) is an approach for building highly modular systems of many interacting components. CDM's strength lies in simplifying the model of complex, multi functional systems that require architectural evolution of tangled data flow and control hierarchies.\n\nThe constructionist design methodology was developed by artificial intelligence (AI) researcher Kristinn R. Thórisson and his students at Columbia University and Reykjavik University for use in the development of cognitive robotics, communicative humanoids and broad AI systems. The creation of such systems requires integration of a large number of functionalities that must be carefully coordinated to achieve coherent system behavior.\n\nCDM is based on iterative design steps that lead to the creation of a network of named interacting modules, communicating via explicitly typed streams and discrete messages. CDM has been used in the creation of many systems including robotics, facial animation, large-scale simulation and virtual humans. One of the first systems was MIRAGE, a simulated human in an augmented-reality environment that could interact with people through speech and gesture. \n\nDr. Kristinn R. Thórisson is an Icelandic Artificial Intelligence researcher, founder of the Icelandic Institute for Intelligent Machines (IIIM) and co-founder and former co-director of CADIA: Center for Analysis and Design of Intelligent Agents. Thórisson is one of the leading proponents of artificial intelligence systems integration. He is a proponent of Artificial General Intelligence (AGI) and has proposed a new methodology for achieving artificial general intelligence.\n\nThe CDM approach is described in an online semi-tutorial format, and in an article published in A.I. magazine, see Thórisson et al. 2004.\n\n\n",
    "id": "6417891",
    "title": "Constructionist design methodology"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=8160211",
    "text": "Cerebellar model articulation controller\n\nThe cerebellar model arithmetic computer (CMAC) is a type of neural network based on a model of the mammalian cerebellum. It is also known as the cerebellar model articulation controller. It is a type of associative memory.\n\nThe CMAC was first proposed as a function modeler for robotic controllers by James Albus in 1975 (hence the name), but has been extensively used in reinforcement learning and also as for automated classification in the machine learning community. CMAC computes a function formula_1, where formula_2 is the number of input dimensions. The input space is divided up into hyper-rectangles, each of which is associated with a memory cell. The contents of the memory cells are the weights, which are adjusted during training. Usually, more than one quantisation of input space is used, so that any point in input space is associated with a number of hyper-rectangles, and therefore with a number of memory cells. The output of a CMAC is the algebraic sum of the weights in all the memory cells activated by the input point.\n\nA change of value of the input point results in a change in the set of activated hyper-rectangles, and therefore a change in the set of memory cells participating in the CMAC output. The CMAC output is therefore stored in a distributed fashion, such that the output corresponding to any point in input space is derived from the value stored in a number of memory cells (hence the name associative memory). This provides generalisation.\n\nIn the image on the right, there are two inputs to the CMAC, represented as a 2D space. Two quantising functions have been used to divide this space with two overlapping grids (one shown in heavier lines). A single input is shown near the middle, and this has activated two memory cells, corresponding to the shaded area. If another point occurs close to the one shown, it will share some of the same memory cells, providing generalisation.\n\nThe CMAC is trained by presenting pairs of input points and output values, and adjusting the weights in the activated cells by a proportion of the error observed at the output. This simple training algorithm has a proof of convergence.\n\nIt is normal to add a kernel function to the hyper-rectangle, so that points falling towards the edge of a hyper-rectangle have a smaller activation than those falling near the centre.\n\nOne of the major problems cited in practical use of CMAC is the memory size required, which is directly related to the number of cells used. This is usually ameliorated by using a hash function, and only providing memory storage for the actual cells that are activated by inputs.\n\nInitially least mean square (LMS) method is employed to update the weights of CMAC. The convergence of using LMS for training CMAC is sensitive to the learning rate and could lead to divergence. In 2004 , a recursive least squares algorithm was introduced to train CMAC online. It does not need to tune a learning rate. Its convergence has been proved theoretically and can be guaranteed to converge in one step. The computational complexity of this RLS algorithm is O(N3).\n\nBased on QR decomposition, an algorithm (QRLS) has been further simplified to have an O(N) complexity. Consequently, this reduces memory usage and time cost significantly. A parallel pipeline array structure on implementing this algorithm has been introduced. \n\nOverall by utilizing QRLS algorithm, the CMAC neural network convergence can be guaranteed, and the weights of the nodes can be updated using one step of training. Its parallel pipeline array structure offers its great potential to be implemented in hardware for large-scale industry usage.\n\nSince the rectangular shape of CMAC receptive field functions produce discontinuous staircase function approximation, by integrating CMAC with B-splines functions, continuous CMAC offers the capability of obtaining any order of derivatives of the approximate functions.\n\n",
    "id": "8160211",
    "title": "Cerebellar model articulation controller"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=2088095",
    "text": "IJCAI Computers and Thought Award\n\nThe IJCAI Computers and Thought Award is presented every two years by the International Joint Conferences on Artificial Intelligence (IJCAI), recognizing outstanding young scientists in artificial intelligence. It was originally funded with royalties received from the book \"Computers and Thought\" (edited by Edward Feigenbaum and Julian Feldman), and is currently funded by IJCAI.\n\nIt is considered to be \"the premier award for artificial intelligence researchers under the age of 35\".\n\n\n",
    "id": "2088095",
    "title": "IJCAI Computers and Thought Award"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=195552",
    "text": "Artificial consciousness\n\nArtificial consciousness (AC), also known as machine consciousness (MC) or synthetic consciousness (; ), is a field related to artificial intelligence and cognitive robotics. The aim of the theory of artificial consciousness is to \"Define that which would have to be synthesized were consciousness to be found in an engineered artifact\" .\n\nNeuroscience hypothesizes that consciousness is generated by the interoperation of various parts of the brain, called the neural correlates of consciousness or NCC, though there are challenges to that perspective. Proponents of AC believe it is possible to construct systems (e.g., computer systems) that can emulate this NCC interoperation.\n\nArtificial consciousness concepts are also pondered in the philosophy of artificial intelligence through questions about mind, consciousness, and mental states.\n\nAs there are many hypothesized types of consciousness, there are many potential implementations of artificial consciousness. In the philosophical literature, perhaps the most common taxonomy of consciousness is into \"access\" and \"phenomenal\" variants. Access consciousness concerns those aspects of experience that can be apprehended, while phenomenal consciousness concerns those aspects of experience that seemingly cannot be apprehended, instead being characterized qualitatively in terms of “raw feels”, “what it is like” or qualia ().\n\nType-identity theorists and other skeptics hold the view that consciousness can only be realized in particular physical systems because consciousness has properties that necessarily depend on physical constitution (; ).\n\nIn his article \"Artificial Consciousness: Utopia or Real Possibility\" Giorgio Buttazzo says that despite our current technology's ability to simulate autonomy, \"Working in a fully automated mode, they [the computers] cannot exhibit creativity, emotions, or free will. A computer, like a washing machine, is a slave operated by its components.\"\n\nFor other theorists (e.g., functionalists), who define mental states in terms of causal roles, any system that can instantiate the same pattern of causal roles, regardless of physical constitution, will instantiate the same mental states, including consciousness ().\n\nOne of the most explicit arguments for the plausibility of AC comes from David Chalmers. His proposal, found within his article , is roughly that the right kinds of computations are sufficient for the possession of a conscious mind. In the outline, he defends his claim thus: Computers perform computations. Computations can capture other systems' abstract causal organization.\n\nThe most controversial part of Chalmers' proposal is that mental properties are \"organizationally invariant\". Mental properties are of two kinds, psychological and phenomenological. Psychological properties, such as belief and perception, are those that are \"characterized by their causal role\". He adverts to the work of and in claiming that \"[s]ystems with the same causal topology…will share their psychological properties\".\n\nPhenomenological properties are not prima facie definable in terms of their causal roles. Establishing that phenomenological properties are amenable to individuation by causal role therefore requires argument. Chalmers provides his Dancing Qualia Argument for this purpose.\n\nChalmers begins by assuming that agents with identical causal organizations could have different experiences. He then asks us to conceive of changing one agent into the other by the replacement of parts (neural parts replaced by silicon, say) while preserving its causal organization. Ex hypothesi, the experience of the agent under transformation would change (as the parts were replaced), but there would be no change in causal topology and therefore no means whereby the agent could \"notice\" the shift in experience.\n\nCritics of AC object that Chalmers begs the question in assuming that all mental properties and external connections are sufficiently captured by abstract causal organization.\n\nIf it were suspected that a particular machine was conscious, its rights would be an ethical issue that would need to be assessed (e.g. what rights it would have under law). For example, a conscious computer that was owned and used as a tool or central computer of a building or large machine is a particular ambiguity. Should laws be made for such a case, consciousness would also require a legal definition (for example a machine's ability to experience pleasure or pain, known as sentience). Because artificial consciousness is still largely a theoretical subject, such ethics have not been discussed or developed to a great extent, though it has often been a theme in fiction (see below).\n\nThe rules for the 2003 Loebner Prize competition explicitly addressed the question of robot rights:\n61. If, in any given year, a publicly available open source Entry entered by the University of Surrey or the Cambridge Center wins the Silver Medal or the Gold Medal, then the Medal and the Cash Award will be awarded to the body responsible for the development of that Entry. If no such body can be identified, or if there is disagreement among two or more claimants, the Medal and the Cash Award will be held in trust \"until such time as the Entry may legally possess, either in the United States of America or in the venue of the contest, the Cash Award and Gold Medal in its own right\".\n\nThere are various aspects of consciousness generally deemed necessary for a machine to be artificially conscious. A variety of functions in which consciousness plays a role were suggested by Bernard Baars () and others. The functions of consciousness suggested by Bernard Baars are Definition and Context Setting, Adaptation and Learning, Editing, Flagging and Debugging, Recruiting and Control, Prioritizing and Access-Control, Decision-making or Executive Function, Analogy-forming Function, Metacognitive and Self-monitoring Function, and Autoprogramming and Self-maintenance Function. Igor Aleksander suggested 12 principles for artificial consciousness () and these are: The Brain is a State Machine, Inner Neuron Partitioning, Conscious and Unconscious States, Perceptual Learning and Memory, Prediction, The Awareness of Self, Representation of Meaning, Learning Utterances, Learning Language, Will, Instinct, and Emotion. The aim of AC is to define whether and how these and other aspects of consciousness can be synthesized in an engineered artifact such as a digital computer. This list is not exhaustive; there are many others not covered.\n\nAwareness could be one required aspect, but there are many problems with the exact definition of \"awareness\". The results of the experiments of neuroscanning on monkeys suggest that a process, not only a state or object, activates neurons. Awareness includes creating and testing alternative models of each process based on the information received through the senses or imagined, and is also useful for making predictions. Such modeling needs a lot of flexibility. Creating such a model includes modeling of the physical world, modeling of one's own internal states and processes, and modeling of other conscious entities.\n\nThere are at least three types of awareness: agency awareness, goal awareness, and sensorimotor awareness, which may also be conscious or not. For example, in agency awareness you may be aware that you performed a certain action yesterday, but are not now conscious of it. In goal awareness you may be aware that you must search for a lost object, but are not now conscious of it. In sensorimotor awareness, you may be aware that your hand is resting on an object, but are not now conscious of it.\n\nBecause objects of awareness are often conscious, the distinction between awareness and consciousness is frequently blurred or they are used as synonyms.\n\nConscious events interact with memory systems in learning, rehearsal, and retrieval.\nThe IDA model elucidates the role of consciousness in the updating of perceptual memory, transient episodic memory, and procedural memory. Transient episodic and declarative memories have distributed representations in IDA, there is evidence that this is also the case in the nervous system. In IDA, these two memories are implemented computationally using a modified version of Kanerva’s Sparse distributed memory architecture.\n\nLearning is also considered necessary for AC. By Bernard Baars, conscious experience is needed to represent and adapt to novel and significant events (). By Axel Cleeremans and Luis Jiménez, learning is defined as \"a set of philogenetically advanced adaptation processes that critically depend on an evolved sensitivity to subjective experience so as to enable agents to afford flexible control over their actions in complex, unpredictable environments\" ().\n\nThe ability to predict (or anticipate) foreseeable events is considered important for AC by Igor Aleksander. The emergentist multiple drafts principle proposed by Daniel Dennett in \"Consciousness Explained\" may be useful for prediction: it involves the evaluation and selection of the most appropriate \"draft\" to fit the current environment. Anticipation includes prediction of consequences of one's own proposed actions and prediction of consequences of probable actions by other entities.\n\nRelationships between real world states are mirrored in the state structure of a conscious organism enabling the organism to predict events. An artificially conscious machine should be able to anticipate events correctly in order to be ready to respond to them when they occur or to take preemptive action to avert anticipated events. The implication here is that the machine needs flexible, real-time components that build spatial, dynamic, statistical, functional, and cause-effect models of the real world and predicted worlds, making it possible to demonstrate that it possesses artificial consciousness in the present and future and not only in the past. In order to do this, a conscious machine should make coherent predictions and contingency plans, not only in worlds with fixed rules like a chess board, but also for novel environments that may change, to be executed only when appropriate to simulate and control the real world.\n\nSubjective experiences or qualia are widely considered to be \"the\" hard problem of consciousness. Indeed, it is held to pose a challenge to physicalism, let alone computationalism. On the other hand, there are problems in other fields of science which limit that which we can observe, such as the uncertainty principle in physics, which have not made the research in these fields of science impossible.\n\nThe term \"cognitive architecture\" may refer to a theory about the structure of the human mind, or any portion or function thereof, including consciousness. In another context, a cognitive architecture implements the theory on computers. An example is \"QuBIC: Quantum and Bio-inspired Cognitive Architecture for Machine Consciousness\". One of the main goals of a cognitive architecture is to summarize the various results of cognitive psychology in a comprehensive computer model. However, the results need to be in a formalized form so they can be the basis of a computer program. Also, the role of cognitive architecture is for the A.I. to clearly structure, build, and implement it's thought process.\n\nStan Franklin (1995, 2003) defines an autonomous agent as possessing functional consciousness when it is capable of several of the functions of consciousness as identified by Bernard Baars' Global Workspace Theory . His brain child IDA (Intelligent Distribution Agent) is a software implementation of GWT, which makes it functionally conscious by definition. IDA's task is to negotiate new assignments for sailors in the US Navy after they end a tour of duty, by matching each individual's skills and preferences with the Navy's needs. IDA interacts with Navy databases and communicates with the sailors via natural language e-mail dialog while obeying a large set of Navy policies. The IDA computational model was developed during 1996–2001 at Stan Franklin's \"Conscious\" Software Research Group at the University of Memphis. It \"consists of approximately a quarter-million lines of Java code, and almost completely consumes the resources of a 2001 high-end workstation.\" It relies heavily on \"codelets\", which are \"special purpose, relatively independent, mini-agent[s] typically implemented as a small piece of code running as a separate thread.\" In IDA's top-down architecture, high-level cognitive functions are explicitly modeled (see and for details). While IDA is functionally conscious by definition, Franklin does \"not attribute phenomenal consciousness to his own 'conscious' software agent, IDA, in spite of her many human-like behaviours. This in spite of watching several US Navy detailers repeatedly nodding their heads saying 'Yes, that's how I do it' while watching IDA's internal and external actions as she performs her task.\"\n\nCLARION posits a two-level representation that explains the distinction between conscious and unconscious mental processes.\n\nCLARION has been successful in accounting for a variety of psychological data. A number of well-known skill learning tasks have been simulated using CLARION that span the spectrum ranging from simple reactive skills to complex cognitive skills. The tasks include serial reaction time (SRT) tasks, artificial grammar learning (AGL) tasks, process control (PC) tasks, the categorical inference (CI) task, the alphabetical arithmetic (AA) task, and the Tower of Hanoi (TOH) task . Among them, SRT, AGL, and PC are typical implicit learning tasks, very much relevant to the issue of consciousness as they operationalized the notion of consciousness in the context of psychological experiments.\n\nBen Goertzel is pursuing an embodied AGI through the open-source OpenCog project. Current code includes embodied virtual pets capable of learning simple English-language commands, as well as integration with real-world robotics, being done at the Hong Kong Polytechnic University.\n\nPentti considers classical rule-based computing inadequate for achieving AC: \"the brain is definitely not a computer. Thinking is not an execution of programmed strings of commands. The brain is not a numerical calculator either. We do not think by numbers.\" Rather than trying to achieve mind and consciousness by identifying and implementing their underlying computational rules, Haikonen proposes \"a special cognitive architecture to reproduce the processes of perception, inner imagery, inner speech, pain, pleasure, emotions and the cognitive functions behind these. This bottom-up architecture would produce higher-level functions by the power of the elementary processing units, the artificial neurons, without algorithms or programs\". Haikonen believes that, when implemented with sufficient complexity, this architecture will develop consciousness, which he considers to be \"a style and way of operation, characterized by distributed signal representation, perception process, cross-modality reporting and availability for retrospection.\" Haikonen is not alone in this process view of consciousness, or the view that AC will spontaneously emerge in autonomous agents that have a suitable neuro-inspired architecture of complexity; these are shared by many, e.g. and . A low-complexity implementation of the architecture proposed by was reportedly not capable of AC, but did exhibit emotions as expected. See for a comprehensive introduction to Haikonen's cognitive architecture. An updated account of Haikonen's architecture, along with a summary of his philosophical views, is given in .\n\nMurray Shanahan describes a cognitive architecture that combines Baars's idea of a global workspace with a mechanism for internal simulation (\"imagination\") . For discussions of Shanahan's architecture, see and and Chapter 20 of .\n\nSelf-awareness in robots is being investigated by Junichi Takeno at Meiji University in Japan. Takeno is asserting that he has developed a robot capable of discriminating between a self-image in a mirror and any other having an identical image to it, and this claim has already been reviewed . Takeno asserts that he first contrived the computational module called a MoNAD, which has a self-aware function, and he then constructed the artificial consciousness system by formulating the relationships between emotions, feelings and reason by connecting the modules in a hierarchy (Igarashi, Takeno 2007). Takeno completed a mirror image cognition experiment using a robot equipped with the MoNAD system. Takeno proposed the Self-Body Theory stating that \"humans feel that their own mirror image is closer to themselves than an actual part of themselves.\" The most important point in developing artificial consciousness or clarifying human consciousness is the development of a function of self awareness, and he claims that he has demonstrated physical and mathematical evidence for this in his thesis. He also demonstrated that robots can study episodes in memory where the emotions were stimulated and use this experience to take predictive actions to prevent the recurrence of unpleasant emotions (Torigoe, Takeno 2009).\n\nIgor Aleksander, emeritus professor of Neural Systems Engineering at Imperial College, has extensively researched artificial neural networks and claims in his book \"Impossible Minds: My Neurons, My Consciousness\" that the principles for creating a conscious machine already exist but that it would take forty years to train such a machine to understand language. Whether this is true remains to be demonstrated and the basic principle stated in \"Impossible Minds\"—that the brain is a neural state machine—is open to doubt.\n\nStephen Thaler proposed a possible connection between consciousness and creativity in his 1994 patent, called \"Device for the Autonomous Generation of Useful Information\" (DAGUI), or the so-called \"Creativity Machine\", in which computational critics govern the injection of synaptic noise and degradation into neural nets so as to induce false memories or confabulations that may qualify as potential ideas or strategies. He recruits this neural architecture and methodology to account for the subjective feel of consciousness, claiming that similar noise-driven neural assemblies within the brain invent dubious significance to overall cortical activity. Thaler's theory and the resulting patents in machine consciousness were inspired by experiments in which he internally disrupted trained neural nets so as to drive a succession of neural activation patterns that he likened to stream of consciousness.\n\nIn 2011, Michael Graziano and Sabine Kastler published a paper named \"Human consciousness and its relationship to social neuroscience: A novel hypothesis\" proposing a theory of consciousness as an attention schema. Graziano went on to publish an expanded discussion of this theory in his book \"Consciousness and the Social Brain\". This Attention Schema Theory of Consciousness, as he named it, proposes that the brain tracks attention to various sensory inputs by way of an attention schema, analogous to the well study body schema that tracks the spatial place of a person's body. This relates to artificial consciousness by proposing a specific mechanism of information handling, that produces what we allegedly experience and describe as consciousness, and which should be able to be duplicated by a machine using current technology. When the brain finds that person X is aware of thing Y, it is in effect modeling the state in which person X is applying an attentional enhancement to Y. In the attention schema theory, the same process can be applied to oneself. The brain tracks attention to various sensory inputs, and one's own awareness is a schematized model of one's attention. Graziano proposes specific locations in the brain for this process, and suggests that such awareness is a computed feature constructed by an expert system in the brain.\n\nThe most well-known method for testing machine intelligence is the Turing test. But when interpreted as only observational, this test contradicts the philosophy of science principles of theory dependence of observations. It also has been suggested that Alan Turing's recommendation of imitating not a human adult consciousness, but a human child consciousness, should be taken seriously.\n\nOther tests, such as ConsScale, test the presence of features inspired by biological systems, or measure the cognitive development of artificial systems.\n\nQualia, or phenomenological consciousness, is an inherently first-person phenomenon. Although various systems may display various signs of behavior correlated with functional consciousness, there is no conceivable way in which third-person tests can have access to first-person phenomenological features. Because of that, and because there is no empirical definition of consciousness, a test of presence of consciousness in AC may be impossible.\n\nIn 2014, Victor Argonov suggested a non-Turing test for machine consciousness based on machine's ability to produce philosophical judgments. He argues that a deterministic machine must be regarded as conscious if it is able to produce judgments on all problematic properties of consciousness (such as qualia or binding) having no innate (preloaded) philosophical knowledge on these issues, no philosophical discussions while learning, and no informational models of other creatures in its memory (such models may implicitly or explicitly contain knowledge about these creatures’ consciousness). However, this test can be used only to detect, but not refute the existence of consciousness. A positive result proves that machine is conscious but a negative result proves nothing. For example, absence of philosophical judgments may be caused by lack of the machine’s intellect, not by absence of consciousness.\n\nCharacters with artificial consciousness (or at least with personalities that imply they have consciousness), from works of fiction:\n\n\n\n\n\n",
    "id": "195552",
    "title": "Artificial consciousness"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=7872324",
    "text": "Artificial intelligence systems integration\n\nThe core idea of Artificial Intelligence systems integration is making individual software components, such as speech synthesizers, interoperable with other components, such as common sense knowledgebases, in order to create larger, broader and more capable A.I. systems. The main methods that have been proposed for integration are message routing, or communication protocols that the software components use to communicate with each other, often through a middleware blackboard system.\n\nMost artificial intelligence systems involve some sort of integrated technologies, for example the integration of speech synthesis technologies with that of speech recognition. However, in recent years there has been an increasing discussion on the importance of systems integration as a field in its own right. Proponents of this approach are researchers such as Marvin Minsky, Aaron Sloman, Deb Roy, Kristinn R. Thórisson and Michael A. Arbib. A reason for the recent attention A.I. integration is attracting is that there have already been created a number of (relatively) simple A.I. systems for specific problem domains (such as computer vision, speech synthesis, etc.), and that integrating what's already available is a more logical approach to broader A.I. than building monolithic systems from scratch.\n\nThe focus on systems integration, especially with regard to modular approaches, derive from the fact that most intelligences of significant scales are composed of a multitude of processes and/or utilize multi-modal input and output. For example, a humanoid-type of intelligence would preferably have to be able to talk using speech synthesis, hear using speech recognition, understand using a logical (or some other undefined) mechanism, and so forth. In order to produce artificially intelligent software of broader intelligence, integration of these modalities is necessary.\n\nCollaboration is an integral part of software development as evidenced by the size of software companies and the size of their software departments. Among the tools to ease software collaboration are various procedures and standards that developers can follow to ensure quality, reliability and that their software is compatible with software created by others (such as W3C standards for webpage development). However, collaboration in fields of A.I. has been lacking, for the most part not seen outside of the respected schools, departments or research institutes (and sometimes not within them either). This presents practitioners of A.I. systems integration with a substantial problem and often causes A.I. researchers to have to 're-invent the wheel' each time they want a specific functionality to work with their software. Even more damaging is the \"not invented here\" syndrome, which manifests itself in a strong reluctance of A.I. researchers to build on the work of others.\n\nThe outcome of this in A.I. is a large set of \"solution islands\": A.I. research has produced numerous isolated software components and mechanisms that deal with various parts of intelligence separately. To take some examples:\n\n\nWith the increased popularity of the free software movement, a lot of the software being created, including A.I. systems, that is available for public exploit. The next natural step is to merge these individual software components into coherent, intelligent systems of a broader nature. As a multitude of components (that often serve the same purpose) have already been created by the community, the most accessible way of integration is giving each of these components an easy way to communicate with each other. By doing so, each component by itself becomes a module which can then be tried in various settings and configurations of larger architectures.\n\nMany online communities for A.I. developers exist where tutorials, examples and forums aim at helping both beginners and experts build intelligent systems (for example the AI Depot, Generation 5). However, few communities have succeeded in making a certain standard or a code of conduct popular to allow the large collection of miscellaneous systems to be integrated with any ease. Recently, however, there have been focused attempts at producing standards for A.I. research collaboration, Mindmakers.org is an online community specifically created to harbor collaboration in the development of A.I. systems. The community has proposed the OpenAIR message and routing protocol for communication between software components, making it easier for individual developers to make modules instantly integrateble into other peoples' projects.\n\nThe Constructionist design methodology (CDM, or 'Constructionist A.I.') is a formal methodology proposed in 2004, for use in the development of cognitive robotics, communicative humanoids and broad AI systems. The creation of such systems requires integration of a large number of functionalities that must be carefully coordinated to achieve coherent system behavior. CDM is based on iterative design steps that lead to the creation of a network of named interacting modules, communicating via explicitly typed streams and discrete messages. The OpenAIR message protocol (see below) was inspired by the CDM, and has frequently been used to aid in development of intelligent systems using CDM.\n\nOne of the first projects to use CDM was Mirage, an embodied, graphical agent visualized through augmented reality which could communicate with human users and talk about objects present in the user's physical room. Mirage was created by Kristinn R. Thórisson, the creator of CDM, and a number of students at Columbia University in 2004. The methodology is actively being developed at Reykjavik University.\n\nOpenAIR is a message routing and communication protocol that has been gaining in popularity over the past two years. The protocol is managed by Mindmakers.org, and is described on their site in the following manner:\n\n\"\"OpenAIR is a routing and communication protocol based on a publish-subscribe architecture. It is intended to be the \"glue\" that allows numerous A.I. researchers to share code more effectively — \"AIR to share\". It is a definition or a blueprint of the \"post office and mail delivery system\" for distributed, multi-module systems. OpenAIR provides a core foundation upon which subsequent markup languages and semantics can be based, e.g. gesture recognition and generation, computer vision, hardware-software interfacing etc.; for a recent example see CVML\".\"\n\nOpenAIR was created to allow software components that serve their own purpose to communicate with each other to produce large scale, overall behavior of an intelligent systems. A simple example would be to have a speech recognition system, and a speech synthesizer communicate with an expert system through OpenAIR messages, to create a system that can hear and answer various questions through spoken dialogue. CORBA (see below) is an older but similar architecture that can be used for comparison, but OpenAIR was specifically created for A.I. research, while CORBA is a more general standard.\n\nThe OpenAIR protocol has been used for collaboration on a number of A.I. systems, a list can be found on the Mindmakers project pages. Psyclone is a popular platform to pair with the OpenAIR protocol (see below).\n\nPsyclone is a software platform, or an AI operating system (AIOS), developed by Communicative Machines Laboratories for use in creating large, multi modal A.I. systems. The system is an implementation of a blackboard system that supports the OpenAIR message protocol. Psyclone is available for free for non-commercial purposes and has therefore often been used by research institutes on low budgets and novice A.I. developers.\n\nElvin is a content-based router with a central routing station, similar to the Psyclone AIOS (see above).\n\nThe OOA is a hybrid architecture that relies on a special inter-agent communication language (ICL) – a logic-based declarative language which is good for expressing high-level, complex tasks and natural language expressions.\n\nThe Common Object Request Broker Architecture (CORBA) is a standard that enables software components written in multiple computer languages and running on multiple computers to interoperate. CORBA is defined by the Object Management Group (OMG). CORBA follows similar principles as the OpenAIR protocol (see above), and can be used for A.I. systems integration.\n\nThe Messaging Open Service Interface Definition (OSID) is an O.K.I. specification which provides a means of sending, subscribing and receiving messages. OSIDs are programmatic interfaces which comprise a Service Oriented Architecture for designing and building reusable and interoperable software.\n\n\n\n\n",
    "id": "7872324",
    "title": "Artificial intelligence systems integration"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=7879921",
    "text": "OpenAIR\n\nOpenAIR is a message routing and communication protocol for artificial intelligence systems that has been gaining in popularity in recent years (2006) . The protocol is managed by Mindmakers, and is described on their site in the following manner:\n\n\"\"OpenAIR is a routing and communication protocol based on a publish-subscribe architecture. It is intended to be the \"glue\" that allows numerous A.I. researchers to share code more effectively — \"AIR to share\". It is a definition or a blueprint of the \"post office and mail delivery system\" for distributed, multi-module systems. OpenAIR provides a core foundation upon which subsequent markup languages and semantics can be based, for e.g. gesture recognition and generation, computer vision, hardware-software interfacing etc.; for a recent example see CVML\".\" \n\nOpenAIR was created to allow software components that serve their own purpose to communicate with each other in order to produce large scale, overall behavior of an intelligent system. A simple example would be to have a speech recognition system, and a speech synthesizer communicate with an expert system through OpenAIR messages, to create a system that can hear and answer various questions through spoken dialogue.\n\n\n",
    "id": "7879921",
    "title": "OpenAIR"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=9460224",
    "text": "Zeuthen strategy\n\nThe Zeuthen strategy is a negotiation strategy used by some artificial agents. Its purpose is to measure the \"willingness to risk conflict\". An agent will be more willing to risk conflict if the difference in utility between its current proposal and the conflict deal is low.\n\nWhen used by both agents in the Monotonic Concession Protocol, the Zeuthen strategy leads the agents to agree upon the deal in the negotiation set, the set of all conflict free deals which are individually rational and Pareto optimal plus the conflict deal, which maximizes the Nash product.\n\nThe strategy was introduced in 1930 by the Danish economist Frederik Zeuthen.\n\nRisk(A,t) is a measurement of agent A's willingness to risk conflict. The risk function formalizes the notion that an agent's willingness to risk conflict is the ratio of the utility that agent would lose by accepting the other agent's proposal to the utility that agent would lose by causing a conflict. Agent A is said to be using a rational negotiation strategy if at any step \"t\" + 1 that agent A sticks to his last proposal, Risk(A,t) > Risk(B,t).\n\nIf agent A makes a sufficient concession in the next step, then, assuming that agent B is using a rational negotiation strategy, if agent B does not concede in the next step, he must do so in the step after that. The set of all sufficient concessions of agent A at step t is denoted SC(\"A\", \"t\").\n\nis the minimal sufficient concession of agent A in step t.\n\nAgent A begins the negotiation by proposing\n\nand will make the minimal sufficient concession in step t+1 if and only if Risk(A,t) ≤ Risk(B,t).\n\nTheorem\nIf both agents are using Zeuthen strategies, then they will agree on\n\nthat is, the deal which maximizes the Nash product. (Harsanyi 56)\n\nProof\nLet δ = δ(A,t).\nLet δ = δ(B,t).\nAccording to the Zeuthen strategy, agent A will concede at step formula_5 if and only if\n\nThat is, if and only if\n\nThus, Agent A will concede if and only if formula_13 does not yield the larger product of utilities.\n\nTherefore, the Zeuthen strategy guarantees a final agreement that maximizes the Nash Product.\n",
    "id": "9460224",
    "title": "Zeuthen strategy"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1456626",
    "text": "Autonomous agent\n\nAn autonomous agent is an \"intelligent agent\" operating on an owner's behalf but without any interference of that ownership entity. An Intelligent agent, however appears according to a multiply cited statement in a no longer accessible IBM white paper as follows:\n\nIntelligent agents are software entities that carry out some set of operations on behalf of a user or another program with some degree of independence or autonomy, and in so doing, employ some knowledge or representation of the user's goals or desires.\n\nSuch an agent is a system situated in, and part of, a technical or natural environment, which senses any or some status of that environment, and acts on it in pursuit of its own agenda. Such an agenda evolves from drives (or programmed goals). The agent acts to change part of the environment or of its status and influences what it sensed.\n\nNon-biological examples include intelligent agents, autonomous robots, and various software agents, including artificial life agents, and many computer viruses. Biological examples are not yet defined.\n\n\n\n",
    "id": "1456626",
    "title": "Autonomous agent"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=8514669",
    "text": "Intelligent database\n\nUntil the 1980s, databases were viewed as computer systems that stored record-oriented and business data such as manufacturing inventories, bank records, and sales transactions. A database system was not expected to merge numeric data with text, images, or multimedia information, nor was it expected to automatically notice patterns in the data it stored. In the late 1980s the concept of an intelligent database was put forward as a system that manages information (rather than data) in a way that appears natural to users and which goes beyond simple record keeping.\n\nThe term was introduced in 1989 by the book \"Intelligent Databases\" by Kamran Parsaye, Mark Chignell, Setrag Khoshafian and Harry Wong. The concept postulated three levels of intelligence for such systems: high level tools, the user interface and the database engine. The high level tools manage data quality and automatically discover relevant patterns in the data with a process called data mining. This layer often relies on the use of artificial intelligence techniques. The user interface uses hypermedia in a form that uniformly manages text, images and numeric data. The intelligent database engine supports the other two layers, often merging relational database techniques with object orientation.\n\nIn the twenty-first century, intelligent databases have now become widespread, e.g. hospital databases can now call up patient histories consisting of charts, text and x-ray images just with a few mouse clicks, and many corporate databases include decision support tools based on sales pattern analysis.\n\n",
    "id": "8514669",
    "title": "Intelligent database"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1563306",
    "text": "Computational intelligence\n\nThe expression computational intelligence (CI) usually refers to the ability of a computer to learn a specific task from data or experimental observation. Even though it is commonly considered a synonym of soft computing, there is still no commonly accepted definition of computational intelligence.\n\nGenerally, computational intelligence is a set of nature-inspired computational methodologies and approaches to address complex real-world problems to which mathematical or traditional modelling can be useless for a few reasons: the processes might be too complex for mathematical reasoning, it might contain some uncertainties during the process, or the process might simply be stochastic in nature. Indeed, many real-life problems cannot be translated into binary language (unique values of 0 and 1) for computers to process it. Computational Intelligence therefore provides solutions for such problems.\n\nThe methods used are close to the human's way of reasoning, i.e. it uses inexact and incomplete knowledge, and it is able to produce control actions in an adaptive way. CI therefore uses a combination of five main complementary techniques. The fuzzy logic which enables the computer to understand natural language, artificial neural networks which permits the system to learn experiential data by operating like the biological one, evolutionary computing, which is based on the process of natural selection, learning theory, and probabilistic methods which helps dealing with uncertainty imprecision.\n\nExcept those main principles, currently popular approaches include biologically inspired algorithms such as swarm intelligence and artificial immune systems, which can be seen as a part of evolutionary computation, image processing, data mining, natural language processing, and artificial intelligence, which tends to be confused with Computational Intelligence. But although both Computational Intelligence (CI) and Artificial Intelligence (AI) seek similar goals, there's a clear distinction between them.\n\nComputational Intelligence is thus a way of performing like human beings. Indeed, the characteristic of \"intelligence\" is usually attributed to humans. More recently, many products and items also claim to be \"intelligent\", an attribute which is directly linked to the reasoning and decision making.\n\nThe notion of Computational Intelligence was first used by the IEEE Neural Networks Council in 1990. This Council was founded in the 1980s by a group of researchers interested in the development of biological and artificial neural networks. On November 21, 2001, the IEEE Neural Networks Council became the IEEE Neural Networks Society, to become the IEEE Computational Intelligence Society two years later by including new areas of interest such as fuzzy systems and evolutionary computation, which they related to Computational Intelligence in 2011 (Dote and Ovaska).\n\nBut the first clear definition of Computational Intelligence was introduced by Bezdek in 1994: a system is called computationally intelligent if it deals with low-level data such as numerical data, has a pattern-recognition component and does not use knowledge in the AI sense, and additionally when it begins to exhibit computational adaptively, fault tolerance, speed approaching human-like turnaround and error rates that approximate human performance.\n\nBezdek and Marks (1993) clearly differentiated CI from AI, by arguing that the first one is based on soft computing methods, whereas AI is based on hard computing ones.\n\nAlthough Artificial Intelligence and Computational Intelligence seek a similar long-term goal: reach general intelligence, which is the intelligence of a machine that could perform any intellectual task that a human being can; there's a clear difference between them. According to Bezdek (1994), Computational Intelligence is a subset of Artificial Intelligence.\n\nThere are two types of machine intelligence: the artificial one based on hard computing techniques and the computational one based on soft computing methods, which enable adaptation to many situations.\n\nHard computing techniques work following binary logic based on only two values (the Booleans true or false, 0 or 1) on which modern computers are based. One problem with this logic is that our natural language cannot always be translated easily into absolute terms of 0 and 1. Soft computing techniques, based on fuzzy logic can be useful here. Much closer to the way the human brain works by aggregating data to partial truths (Crisp/fuzzy systems), this logic is one of the main exclusive aspects of CI.\n\nWithin the same principles of fuzzy and binary \"logics\" follow crispy and fuzzy \"systems\". Crisp logic is a part of artificial intelligence principles and consists of either including an element in a set, or not, whereas fuzzy systems (CI) enable elements to be partially in a set. Following this logic, each element can be given a degree of membership (from 0 to 1) and not exclusively one of these 2 values.\n\nThe main applications of Computational Intelligence include computer science, engineering, data analysis and bio-medicine.\n\nAs explained before, fuzzy logic, one of CI's main principles, consists in measurements and process modelling made for real life's complex processes. It can face incompleteness, and most importantly ignorance of data in a process model, contrarily to Artificial Intelligence, which requires exact knowledge.\n\nThis technique tends to apply to a wide range of domains such as control, image processing and decision making. But it is also well introduced in the field of household appliances with washing machines, microwave ovens, etc. We can face it too when using a video camera, where it helps stabilizing the image while holding the camera unsteadily. Other areas such as medical diagnostics, foreign exchange trading and business strategy selection are apart from this principle's numbers of applications.\n\nFuzzy logic is mainly useful for approximate reasoning, and doesn't have learning abilities, a qualification much needed that human beings have. It enables them to improve themselves by learning from their previous mistakes.\n\nThis is why CI experts work on the development of artificial neural networks based on the biological ones, which can be defined by 3 main components: the cell-body which processes the information, the axon, which is a device enabling the signal conducting, and the synapse, which controls signals. Therefore, artificial neural networks are doted of distributed information processing systems, enabling the process and the learning from experiential data. Working like human beings, fault tolerance is also one of the main assets of this principle.\n\nConcerning its applications, neural networks can be classified into five groups: data analysis and classification, associative memory, clustering generation of patterns and control. Generally, this method aims to analyze and classify medical data, proceed to face and fraud detection, and most importantly deal with nonlinearities of a system in order to control it. Furthermore, neural networks techniques share with the fuzzy logic ones the advantage of enabling data clustering.\n\nBased on the process of natural selection firstly introduced by Charles Robert Darwin, the evolutionary computation consists in capitalizing on the strength of natural evolution to bring up new artificial evolutionary methodologies. It also includes other areas such as evolution strategy, and evolutionary algorithms which are seen as problem solvers... This principle's main applications cover areas such as optimization and multi-objective optimization, to which traditional mathematical one techniques aren't enough anymore to apply to a wide range of problems such as DNA Analysis, scheduling problems...\n\nStill looking for a way of \"reasoning\" close to the humans' one, learning theory is one of the main approaches of CI. In psychology, learning is the process of bringing together cognitive, emotional and environmental effects and experiences to acquire, enhance or change knowledge, skills, values and world views (Ormrod, 1995; Illeris, 2004). Learning theories then helps understanding how these effects and experiences are processed, and then helps making predictions based on previous experience.\n\nBeing one of the main elements of fuzzy logic, probabilistic methods firstly introduced by Paul Erdos and Joel Spencer (1974), aim to evaluate the outcomes of a Computation Intelligent system, mostly defined by randomness. Therefore, probabilistic methods bring out the possible solutions to a reasoning problem, based on prior knowledge.\n\n\n",
    "id": "1563306",
    "title": "Computational intelligence"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=11053817",
    "text": "Ontology learning\n\nOntology learning (ontology extraction, ontology generation, or ontology acquisition) is the automatic or semi-automatic creation of ontologies, including extracting the corresponding domain's terms and the relationships between the concepts that these terms represent from a corpus of natural language text, and encoding them with an ontology language for easy retrieval. As building ontologies manually is extremely labor-intensive and time-consuming, there is great motivation to automate the process.\nTypically, the process starts by extracting terms and concepts or noun phrases from plain text using linguistic processors such as part-of-speech tagging and phrase chunking. Then statistical \nor symbolic\n\ntechniques are used to extract relation signatures, often based on pattern-based or definition-based hypernym extraction techniques.\n\nOntology learning (OL) is used to (semi-)automatically extract whole ontologies from natural language text. The process is usually split into the following eight tasks, which are not all necessarily applied in every ontology learning system.\n\n\nDuring the domain terminology extraction step, domain-specific terms are extracted, which are used in the following step (concept discovery) to derive concepts. Relevant terms can be determined e. g. by calculation of the TF/IDF values or by application of the C-value / NC-value method. The resulting list of terms has to be filtered by a domain expert. In the subsequent step, similarly to coreference resolution in information extraction, the OL system determines synonyms, because they share the same meaning and therefore correspond to the same concept. The most common methods therefore are clustering and the application of statistical similarity measures.\n\nIn the concept discovery step, terms are grouped to meaning bearing units, which correspond to an abstraction of the world and therefore to concepts. The grouped terms are these domain-specific terms and their synonyms, which were identified in the domain terminology extraction step.\n\nIn the concept hierarchy derivation step, the OL system tries to arrange the extracted concepts in a taxonomic structure. This is mostly achieved by unsupervised hierarchical clustering methods. Because the result of such methods is often noisy, a supervision, e. g. by evaluation by the user, is integrated. A further method for the derivation of a concept hierarchy exists in the usage of several patterns, which should indicate a sub- or supersumption relationship. Patterns like “X, that is a Y” or “X is a Y” indicate, that X is a subclass of Y. Such pattern can be analyzed efficiently, but they occur too infrequent, to extract enough sub- or supersumption relationships. Instead bootstrapping methods are developed, which learn these patterns automatically and therefore ensure a higher coverage.\n\nAt the learning of non-taxonomic relations step, relationships are extracted, which do not express any sub- or supersumption. Such relationships are e.g. works-for or located-in. There are two common approaches to solve this subtask. The first one is based upon the extraction of anonymous associations, which are named appropriately in a second step. The second approach extracts verbs, which indicate a relationship between the entities, represented by the surrounding words. But the result of both approaches has to be evaluated by an ontologist.\n\nDuring rule discovery, axioms (formal description of concepts) are generated for the extracted concepts. This can be achieved for example by analyzing the syntactic structure of a natural language definition and the application of transformation rules on the resulting dependency tree. The result of this process is a list of axioms, which is afterwards comprehended to a concept description. This one has to be evaluated by an ontologist.\n\nAt the ontology population step, the ontology is augmented with instances of concepts and properties. For the augmentation with instances of concepts methods, which are based on the matching of lexico-syntactic patterns, are used. Instances of properties are added by application of bootstrapping methods, which collect relation tuples.\n\nIn the concept hierarchy extension step, the OL system tries to extend the taxonomic structure of an existing ontology with further concepts. This can be realized supervised by a trained classifier or unsupervised by the application of similarity measures.\n\nDog4Dag - an ontology generation plugin for Protégé 4.1 and OBOEdit.\nDOG4DAG is an ontology generation plugin for both Protégé 4.1 and OBO-Edit 2.1. It allows for term generation, sibling generation, definition generation, and relationship induction. Integrated into Protégé 4.1 and OBO-Edit 2.1, DOG4DAG allows ontology extension for all common ontology formats (e.g., OWL and OBO). Limited largely to EBI and Bio Portal lookup service extensions.\n\nDuring frame/event detection, the OL system tries to extract complex relationships from text, e.g. who departed from where to what place and when. Approaches range from applying SVM with kernel methods to semantic role labeling (SRL) to deep semantic parsing techniques.\n\n\n",
    "id": "11053817",
    "title": "Ontology learning"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=11298926",
    "text": "Nouvelle AI\n\nNouvelle artificial intelligence (AI) is an approach to artificial intelligence pioneered in the 1980s by Rodney Brooks, who was then part of MIT artificial intelligence laboratory. Nouvelle AI differs from classical AI by aiming to produce robots with intelligence levels similar to insects. Researchers believe that intelligence can emerge organically from simple behaviors as these intelligences interacted with the \"real world,\" instead of using the constructed worlds which symbolic AIs typically needed to have programmed into them.\n\nThe differences between nouvelle AI and symbolic AI are apparent in early robots Shakey and Freddy. These robots contained an internal model (or \"representation\") of their micro-worlds consisting of symbolic descriptions. As a result, this structure of symbols had to be renewed as the robot moved or the world changed.\n\nShakey's planning programs assessed the program structure and broke it down into the necessary steps to complete the desired action. This level of computation required a large amount time to process, so Shakey typically performed its tasks very slowly.\n\nSymbolic AI researchers had long been plagued by the problem of updating, searching, and otherwise manipulating the symbolic worlds inside their AIs. A nouvelle system refers continuously to its sensors rather than to an internal model of the world. It processes the external world information it needs from the senses when it is required. As Brooks puts it, \"the world is its own best model--always exactly up to date and complete in every detail.\"\n\nA central idea of nouvelle AI is that simple behaviors combine to form more complex behaviors over time. For example, simple behaviors can include elements like \"move forward\" and \"avoid obstacles.\" A robot using nouvelle AI with simple behaviors like collision avoidance and moving toward a moving object could possibly come together to produce a more complex behavior like chasing a moving object.\n\nThe frame problem describes an issue with using first-order logic (FOL) to express facts about a robot in the world. Representing the state of a robot with traditional FOL requires the use of many axioms (symbolic language) to imply that things about an environment that do not change arbitrarily.\n\nNouvelle AI seeks to sidestep the frame problem by dispensing with filling the AI or robot with volumes of symbolic language and instead letting more complex behaviors emerge by combining simpler behavioral elements.\n\nThe goal of traditional AI was to build intelligences without bodies, which would only have been able to interact with the world via keyboard, screen, or printer. However, nouvelle AI attempts to build embodied intelligence situated in the real world. Brooks quotes approvingly from the brief sketches that Turing gave in 1948 and 1950 of the \"situated\" approach. Turing wrote of equipping a machine \"with the best sense organs that money can buy\" and teaching it \"to understand and speak English\" by a process that would \"follow the normal teaching of a child.\" This approach was contrasted to the others where they focused on abstract activities such as playing chess.\n\nBrooks focused on building robots that acted like simple insects while simultaneously working to remove some traditional AI characteristics. He created insect-like robots called Allen and Herbert.\n\nBrooks's insectoid robots contained no internal models of the world. Herbert, for example, discarded a high volume of the information received from its sensors and never stored information for more than two seconds.\n\nAllen, named after Allen Newell, had a ring of twelve ultrasonic sonars as its primary sensors and three independent behavior-producing modules. These modules were programmed to avoid both stationary and moving objects. With only this module activated, Allen stayed in the middle of a room until an object approached and then it ran away while avoiding obstacles in its way.\n\nHerbert, named after Herbert Simon, used infrared sensors to avoid obstacles and a laser system to collect 3D data over a distance about 12 feet. Herbert also carried a number of simple sensors in its \"hand.\" The robot's testing ground was the real world environment of the busy offices and workspaces of the MIT AI lab where it searched for empty soda cans and carried them away, a seemingly-goal oriented activity that emerged as a result of 15 simple behavior units combining together.\n\nOther robots by Brooks' team were Genghis and Squirt. Genghis had six legs and was able to walk over rough terrain and follow a human. Squirt's behavior modules had it stay in dark corners until it heard a noise, then it would begin to follow the source of the noise.\n\nBrooks agreed that the level of nouvelle AI had come near the complexity of a real insect, which raised a question about whether or not insect level-behavior was and is a reasonable goal for nouvelle AI?\n\nBrooks' own recent work has taken the opposite direction to that proposed by Von Neumann in the quotations \"theorists who select the human nervous system as their model are unrealistically picking 'the most complicated object under the sun,' and that there is little advantage in selecting instead the ant, since any nervous system at all exhibits exceptional complexity.\"\n\nIn the 1990s, Brooks decided to pursue the goal of human-level intelligence and, with Lynn Andrea Stein, built a humanoid robot called Cog. Cog is a robot with an extensive collection of sensors, a face, and arms (among other features) which allow it to interact with the world and gather information and experience so as to assemble intelligence organically in the manner described above by Turing.\n\nThe team believes that Cog will able to learn and able to find a correlation between the sensory information it receives and its actions. In the long term, the team wants Cog to be able to learn common sense knowledge on its own.\n\n\n",
    "id": "11298926",
    "title": "Nouvelle AI"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=14760591",
    "text": "Computational humor\n\nComputational humor is a branch of computational linguistics and artificial intelligence which uses computers in humor research. It is a relatively new area, with the first dedicated conference organized in 1996.\n\nThe first \"computer model of a sense of humor\" was suggested by\nSuslov as early as 1992. Investigation of the general scheme of information processing shows the possibility of a specific malfunction, conditioned by the necessity of a quick deletion from consciousness of a false version. This specific malfunction can be identified with a humorous effect on psychological grounds: it exactly corresponds to incongruity-resolution theory. However, an essentially new ingredient, the role of timing, is added to the well-known role of ambiguity. In biological systems, a sense of humor inevitably develops in the course of evolution, because its biological function consists of quickening the transmission of the processed information into consciousness and in a more effective use of brain resources. A realization of this algorithm in neural networks justifies naturally Spencer's hypothesis on the mechanism of laughter: deletion of a false version corresponds to zeroing of some part of the neural network and excessive energy of neurons is thrown out to the motor cortex, arousing muscular contractions.\n\nA practical realization of this algorithm needs extensive databases, whose creation in the automatic regime was suggested recently. As a result, this magistral direction was not developed properly and subsequent investigations accepted somewhat specialized colouring.\n\nAn approach to analysis of humor is classification of jokes. A further step is an attempt to generate jokes basing on the rules that underlie classification.\n\nSimple prototypes for computer pun generation were reported in the early 1990s, based on a natural language generator program, VINCI. Graeme Ritchie and Kim Binsted in their 1994 research paper described a computer program, JAPE, designed to generate question-answer-type puns from a general, i.e., non-humorous, lexicon. (The program name is an acronym for \"Joke Analysis and Production Engine\".) Some examples produced by JAPE are:\n\nSince then the approach has been improved, and the latest report, dated 2007, describes the STANDUP joke generator, implemented in the Java programming language. The STANDUP generator was tested on children within the framework of analyzing its usability for language skills development for children with communication disabilities, e.g., because of cerebral palsy. (The project name is an acronym for \"System To Augment Non-speakers' Dialog Using Puns\" and an allusion to standup comedy.) Children responded to this \"language playground\" with enthusiasm, and showed marked improvement on certain types of language tests.\nThe two young people, who used the system over a ten-week period, regaled their peers, staff, family and neighbors with jokes such as: \"What do you call a spicy missile? A hot shot!\" Their joy and enthusiasm at entertaining others was inspirational.\nStock and Strapparava described a program to generate funny acronyms.\n\n\"AskTheBrain\" (2002) used clustering and bayesian analysis to associate concepts in a comical way.\n\nA statistical machine learning algorithm to detect whether a sentence contained a \"That's what she said\" double entendre was developed by Kiddon and Brun (2011). There is an open-source Python implementation of Kiddon & Brun's TWSS system.\n\nA program to recognize knock-knock jokes was reported by Taylor and Mazlack. This kind of research is important in analysis of human-computer interaction.\n\nAn application of machine learning techniques for the distinguishing of joke texts from non-jokes was described by Mihalcea and Strapparava (2006).\n\nTakizawa \"et al.\" (1996) reported on a heuristic program for detecting puns in the Japanese language.\n\nA possible application for the assistance in language acquisition is described in the section \"Pun generation\". Another envisioned use of joke generators is in cases of steady supply of jokes where quantity is more important than quality. Another obvious, yet remote, direction is automated joke appreciation.\n\nIt is known that humans interact with computers in ways similar to interacting with other humans that may be described in terms of personality, politeness, flattery, and in-group favoritism. Therefore, the role of humor in human-computer interaction is being investigated. In particular, humor generation in user interface to ease communications with computers was suggested.\n\nCraig McDonough implemented the Mnemonic Sentence Generator, which converts passwords into humorous sentences. Basing on the incongruity theory of humor, it is suggested that the resulting meaningless but funny sentences are easier to remember. For example, the password AjQA3Jtv is converted into \"Arafat joined Quayle's Ant, while TARAR Jeopardized thurmond's vase\".\n\nJohn Allen Paulos is known for his interest in mathematical foundations of humor. His book \"Mathematics and Humor: A Study of the Logic of Humor\" demonstrates structures common to humor and formal sciences (mathematics, linguistics) and develops a mathematical model of jokes based on catastrophe theory.\n\n\n",
    "id": "14760591",
    "title": "Computational humor"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=14893994",
    "text": "Ordered weighted averaging aggregation operator\n\nIn applied mathematics – specifically in fuzzy logic – the ordered weighted averaging (OWA) operators provide a parameterized class of mean type aggregation operators. They were introduced by Ronald R. Yager. Many notable mean operators such as the max, arithmetic average, median and min, are members of this class. They have been widely used in computational intelligence because of their ability to model linguistically expressed aggregation instructions.\n\nFormally an OWA operator of dimension formula_1 is a mapping formula_2 that has an associated collection of weights formula_3 lying in the unit interval and summing to one and with \n\nwhere formula_5 is the \"j\" largest of the formula_6.\n\nBy choosing different \"W\" one can implement different aggregation operators. The OWA operator is a non-linear operator as a result of the process of determining the \"b\".\n\nThe OWA operator is a mean operator. It is bounded, monotonic, symmetric, and idempotent, as defined below.\n\nTwo features have been used to characterize the OWA operators. The first is the attitudinal character(orness).\n\nThis is defined as\n\nIt is known that formula_19.\n\nIn addition \"A\" − \"C\"(max) = 1, A − C(ave) = A − C(med) = 0.5 and A − C(min) = 0. Thus the A − C goes from 1 to 0 as we go from Max to Min aggregation. The attitudinal character characterizes the similarity of aggregation to OR operation(OR is defined as the Max).\n\nThe second feature is the dispersion. This defined as\n\nAn alternative definition is formula_21 The dispersion characterizes how uniformly the arguments are being used\nÀĚ\n\nThe historical reconstruction of scientific development of the OWA field, the identification of the dominant direction of knowledge accumulation that emerged since the publication of the first OWA paper, and to discover the most active lines of research has recently been published, (see: http://onlinelibrary.wiley.com/doi/10.1002/int.21673/full). The results suggest, as expected, that Yager's paper[1] (IEEE Trans. Systems Man Cybernet, 18(1), 183–190, 1988) is the most influential paper and the starting point of all other research using OWA. Starting from his contribution, other lines of research developed and we describe them. Full list of papers published in OWA is also available at http://onlinelibrary.wiley.com/doi/10.1002/int.21673/full)\n\nThe above Yager's OWA operators are used to aggregate the crisp values. Can we aggregate fuzzy sets in the OWA mechanism ? The\nType-1 OWA operators have been proposed for this purpose. So the type-1 OWA operators provides us with a new technique for directly aggregating uncertain information with uncertain weights via OWA mechanism in soft decision making and data mining, where these uncertain objects are modelled by fuzzy sets.\n\nThe type-1 OWA operator is defined according to the alpha-cuts of fuzzy sets as follows:\n\nGiven the \"n\" linguistic weights formula_22 in the form of fuzzy sets defined on the domain of discourse formula_23, then for each formula_24, an formula_25-level type-1 OWA operator with formula_25-level sets formula_27 to aggregate the formula_25-cuts of fuzzy sets formula_29 is given as\n\nwhere formula_31, and formula_32 is a permutation function such that formula_33, i.e., formula_34 is the formula_35th largest\nelement in the set formula_36.\n\nThe computation of the type-1 OWA output is implemented by computing the left end-points and right end-points of the intervals formula_37:\nformula_38 and formula_39\nwhere formula_40. Then membership function of resulting aggregation fuzzy set is:\n\nFor the left end-points, we need to solve the following programming problem:\n\nwhile for the right end-points, we need to solve the following programming problem:\n\nThis paper has presented a fast method to solve two programming problem so that the type-1 OWA aggregation operation can be performed efficiently.\n\n",
    "id": "14893994",
    "title": "Ordered weighted averaging aggregation operator"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1548772",
    "text": "20Q\n\n20Q is a computerized game of twenty questions that began as a test in artificial intelligence (AI). It was invented by Robin Burgener in 1988. The game was made handheld by Radica in 2004, then it was discontinued in 2011 because Techno Source took the license for 20Q handheld devices.\n\nThe game 20Q is based on the spoken parlor game known as twenty questions, and is both a website and a handheld device. 20Q asks the player to think of something and will then try to guess what they are thinking of with twenty yes-or-no questions. If it fails to guess in 20 questions, it will ask an additional 5 questions. If it fails to guess even with 25 (or 30) questions, the player is declared the winner. Sometimes the first guess of the object can be asked at question 14.\n\nThe principle is that the player thinks of something and the 20Q artificial intelligence asks a series of questions before guessing what the player is thinking. This artificial intelligence learns on its own with the information relayed back to the players who interact with it, and is not programmed. The player can answer these questions with: \"Yes\", \"No\", \"Unknown\", and \"Sometimes.\" The experiment is based on the classic word game of Twenty Questions, and on the computer game \"Animals,\" popular in the early 1970s, which used a somewhat simpler method to guess an animal.\n\nThe 20Q AI uses an artificial neural network to pick the questions and to guess. After the player has answered the twenty questions posed (sometimes fewer), 20Q makes a guess. If it is incorrect, it asks more questions, then guesses again. It makes guesses based on what it has learned; it is not programmed with information or what the inventor thinks. Answers to any question are based on players’ interpretations of the questions asked. Newer editions were made for different categories, such as music 20Q which has the player think of a song, and Harry Potter 20Q, which has the player think of something from the world of the Harry Potter series.\n\nThe 20Q AI can draw its own conclusions on how to interpret the information. It can be described as more of a folk taxonomy than a taxonomy. Its knowledge develops with every game played. In this regard, the online version of the 20Q AI can be inaccurate because it gathers its answers from what people \"think\" rather than from what people \"know\". Limitations of taxonomy are often overcome by the AI itself because it can learn and adapt. For example, if the player was thinking of a \"Horse\" and answered \"No\" to the question \"Is it an animal?,\" the AI will, nevertheless, guess correctly, despite being told that a horse is not an animal.\n\nPatent applications in the US and Europe were submitted in 2005.\n\nIn August 2014, 20Q.net Inc., with Brashworks Studios, developed and released an iOS iPad version available at the Apple ITunes store.\n\nThe built Artificial Neural Network is not resource intensive either to store or to compute, thus it could be embedded in small, less powerful devices. Currently, there is a handheld version of the AI. The device contains a small portion of the original 20Q website knowledge base; unlike the online versions of the game, the handheld version does not have the ability to learn.\n\nThe 20Q artificial intelligence is different from less flexible, and extremely large, expert systems. Its modularity, adaptability, and scalability means that it can be applied to other, more complex devices, for more complex uses.\n\nOn June 13, 2009, GSN began a TV version of the game, hosted by Cat Deeley, with Hal Sparks as the voice of the computer.\n\n",
    "id": "1548772",
    "title": "20Q"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=8703473",
    "text": "Fred (chatterbot)\n\nFred, or FRED, was an early chatterbot written by Robby Garner.\n\nThe name Fred was initially suggested by Karen Lindsey, and then Robby jokingly came up with an acronym, \"Functional Response Emulation Device.\" Fred has also been implemented as a Java application by Paco Nathan called JFRED.\n\nFred Chatterbot is designed to explore Natural Language communications between people and computer programs. In particular, this is a study of conversation between people and ways that a computer program can learn from other people's conversations to make its own conversations.\n\nFred used a minimalistic \"stimulus-response\" approach. It worked by storing a database of statements and their responses, and made its own reply by looking up the input statements made by a user and then rendering the corresponding response from the database. This approach simplified the complexity of the rule base, but required expert coding and editing for modifications.\n\nFred was a predecessor to Albert One, which Garner used in 1998 and 1999 to win the Loebner Prize.\n\n\n",
    "id": "8703473",
    "title": "Fred (chatterbot)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=15893225",
    "text": "Competitions and prizes in artificial intelligence\n\nThere are a number of competitions and prizes to promote research in artificial intelligence.\n\nThe David E. Rumelhart prize is an annual award for making a \"significant contemporary contribution to the theoretical foundations of human cognition\". The prize is $100,000.\n\nThe Human-Competitive Award is an annual challenge started in 2004 to reward results \"competitive with the work of creative and inventive humans\". The prize is $10,000. Entries are required to use evolutionary computing.\n\nThe IJCAI Award for Research Excellence is a biannual award given at the IJCAI conference to researcher in artificial intelligence as a recognition of excellence of their career.\n\nThe 2011 Federal Virtual World Challenge advertised by The White House and sponsored by the US Army held a competition offering a total of $52,000 USD in cash prize awards for general artificial intelligence applications, including \"adaptive learning systems, intelligent conversational bots, adaptive behavior (objects or processes)\" and more.\n\nThe Machine Intelligence Prize is awarded annually by the British Computer Society for progress towards machine intelligence.\n\nThe Kaggle - \"the world's largest community of data scientists compete to solve most valuable problems\".\n\nThe Loebner prize is an annual competition to determine the best Turing test competitors. The winner is the computer system that, in the judges' opinions, demonstrates the \"most human\" conversational behaviour, they have an additional prize for a system that in their opinion passes a Turing test. This second prize has not yet been awarded.\n\nThe International Aerial Robotics Competition is a long-running event begun in 1991 to advance the state of the art in fully autonomous air vehicles. This competition is restricted to university teams (although industry and governmental sponsorship of teams is allowed). Key to this event is the creation of flying robots which must complete complex missions without any human intervention. Successful entries are able to interpret their environment and make real-time decisions based only on a high-level mission directive (e.g., \"find a particular target inside a building having certain characteristics which is among a group of buildings 3 kilometers from the aerial robot launch point\"). In 2000, a $30,000 prize was awarded during the 3rd Mission (search and rescue), and in 2008, $80,000 in prize money was awarded at the conclusion of the 4th Mission (urban reconnaissance).\n\nThe DARPA Grand Challenge is a series of competitions to promote driverless car technology, aimed at a congressional mandate stating that by 2015 one-third of the operational ground combat vehicles of the US Armed Forces should be unmanned. While the first race had no winner, the second awarded a $2 million prize for the autonomous navigation of a hundred-mile trail, using GPS, computers and a sophisticated array of sensors. In November 2007, DARPA introduced the DARPA Urban Challenge, a sixty-mile urban area race requiring vehicles to navigate through traffic. In November 2010 the US Armed Forces extended the competition with the $1.6 million prize Multi Autonomous Ground-robotic International Challenge to consider cooperation between multiple vehicles in a simulated-combat situation.\n\nRoborace will be a global motorsport championship with autonomously driving, electrically powered vehicles. The series will be run as a support series during the Formula E championship for electric vehicles. This will be the first global championship for driverless cars.\n\nThe Netflix Prize was a competition for the best collaborative filtering algorithm that predicts user ratings for films, based on previous ratings. The competition was held by Netflix, an online DVD-rental service. The prize was $1,000,000.\n\nThe Pittsburgh Brain Activity Interpretation Competition will reward analysis of fMRI data \"to predict what individuals perceive and how they act and feel in a novel Virtual Reality world involving searching for and collecting objects, interpreting changing instructions, and avoiding a threatening dog.\" The prize in 2007 was $22,000.\n\nThe Face Recognition Grand Challenge (May 2004 to March 2006) aimed to promote and advance face recognition technology.\n\nThe American Meteorological Society's artificial intelligence competition involves learning a classifier to characterise precipitation based on meteorological analyses of environmental conditions and polarimetric radar data.\n\nThe RoboCup and FIRA are annual international robot soccer competitions. The International RoboCup Federation challenge is by 2050 \"a team of fully autonomous humanoid robot soccer players shall win the soccer game, comply with the official rule of the FIFA, against the winner of the most recent World Cup.\"\n\nThe Herbrand Award is a prize given by CADE Inc. to honour persons or groups for important contributions to the field of automated deduction. The prize is $1000.\n\nThe CADE ATP System Competition (CASC) is a yearly competition of fully automated theorem provers for classical first order logic associated with the CADE and IJCAR conferences. The competition was part of the Alan Turing Centenary Conference in 2012, with total prizes of 9000 GBP given by Google.\n\nThe SUMO prize is an annual prize for the best open source ontology extension of the Suggested Upper Merged Ontology (SUMO), a formal theory of terms and logical definitions describing the world. The prize is $3000.\n\nThe Hutter Prize for Lossless Compression of Human Knowledge is a cash prize which rewards compression improvements on a specific 100 MB English text file. The prize awards 500 euros for each one percent improvement, up to €50,000. The organizers believe that text compression and AI are equivalent problems and 3 prizes were already given, at around € 2k.\n\nThe Cyc TPTP Challenge is a competition to develop reasoning methods for the Cyc comprehensive ontology and database of everyday common sense knowledge. The prize is 100 euros for \"each winner of two related challenges\".\n\nThe Eternity II challenge was a constraint satisfaction problem very similar to the Tetravex game. The objective is to lay 256 tiles on a 16x16 grid while satisfying a number of constraints. The problem is known to be NP-complete. The prize was US$2,000,000. The competition ended in December 2010.\n\nThe World Computer Chess Championship has been held since 1970. The International Computer Games Association continues to hold an annual Computer Olympiad which includes this event plus computer competitions for many other games.\n\nThe Ing Prize was a substantial money prize attached to the World Computer Go Congress, starting from 1985 and expiring in 2000. It was a graduated set of handicap challenges against young professional players with increasing prizes as the handicap was lowered. At the time it expired in 2000, the unclaimed prize was 400,000 NT dollars for winning a 9-stone handicap match.\n\nThe AAAI General Game Playing Competition is a competition to develop programs that are effective at general game playing. Given a definition of a game, the program must play it effectively without human intervention. Since the game is not known in advance the competitors cannot especially adapt their programs to a particular scenario. The prize in 2006 and 2007 was $10,000.\n\nThe General Video Game AI (GVG-AI) Competition explores the problem of creating controllers for general video game playing. The competition has multiple tracks including one-player planning, two-player planning, one-player learning and level generation. Competitions for the different tracks will be held across different conferences in 2016.\n\nThe 2007 Ultimate Computer Chess Challenge was a competition organised by World Chess Federation that pitted\nDeep Fritz against Deep Junior. The prize was $100,000.\n\nThe annual Arimaa Challenge offered a $10,000 prize until the year 2020 to develop a program that plays the board game Arimaa and defeats a group of selected human opponents. In 2015, David Wu's bot bot_sharp beat the humans, losing only 2 games out of 9. As a result, the Arimaa Challenge was declared over and David Wu received the prize of $12,000 ($2,000 being offered by third-parties for 2015's championship).\n\n2K Australia is offering a prize worth A$10,000 to develop a game-playing bot that plays a first-person shooter. The aim is to convince a panel of judges that it is actually a human player. The competition started in 2008 and was won in 2012. A new competition is planned for 2014.\n\nThe Google AI Challenge was a bi-annual online contest organized by the University of Waterloo Computer Science Club and sponsored by Google that ran from 2009 to 2011. Each year a game was chosen and contestants submitted specialized automated bots to play against other competing bots.\n\nCloudball had its first round in Spring 2012 and finished on June 15. It is an international artificial intelligence programming contest, where users continuously submit the actions their soccer teams will take in each time step, in simple high level C# code.\n",
    "id": "15893225",
    "title": "Competitions and prizes in artificial intelligence"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1065253",
    "text": "Darwin machine\n\nA Darwin machine (a 1987 coinage by William H. Calvin, by analogy to a Turing machine) is a machine that, like a Turing machine, involves an iteration process that yields a high-quality result, but, whereas a Turing machine uses logic, the Darwin machine uses rounds of variation, selection, and inheritance.\nIn its original connotation, a Darwin machine is any process that bootstraps quality by utilizing all of the six essential features of a Darwinian process: A \"pattern\" is \"copied\" with \"variations\", where populations of one variant pattern \"compete\" with another population, their relative success biased by a \"multifaceted environment\" (natural selection) so that winners predominate in producing the further variants of the next generation (Darwin's \"inheritance principle\").\n\nMore loosely, a Darwin machine is a process that utilizes some subset of the Darwinian essentials, typically natural selection to create a non-reproducing pattern, as in neural Darwinism. Many aspects of neural development utilize overgrowth followed by pruning to a pattern, but the resulting pattern does not itself create further copies.\n\n\"Darwin machine\" has been used multiple times to name computer programs after Charles Darwin.\n\n\n",
    "id": "1065253",
    "title": "Darwin machine"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=16300571",
    "text": "Computational creativity\n\nComputational creativity (also known as artificial creativity, mechanical creativity, creative computing or creative computation) is a multidisciplinary endeavour that is located at the intersection of the fields of artificial intelligence, cognitive psychology, philosophy, and the arts.\n\nThe goal of computational creativity is to model, simulate or replicate creativity using a computer, to achieve one of several ends:\n\n\nThe field of computational creativity concerns itself with theoretical and practical issues in the study of creativity. Theoretical work on the nature and proper definition of creativity is performed in parallel with practical work on the implementation of systems that exhibit creativity, with one strand of work informing the other.\n\nAs measured by the amount of activity in the field (e.g., publications, conferences and workshops), computational creativity is a growing area of research. But the field is still hampered by a number of fundamental problems. Creativity is very difficult, perhaps even impossible, to define in objective terms. Is it a state of mind, a talent or ability, or a process? Creativity takes many forms in human activity, some \"eminent\" (sometimes referred to as \"Creativity\" with a capital C) and some \"mundane\".\n\nThese are problems that complicate the study of creativity in general, but certain problems attach themselves specifically to \"computational\" creativity:\n\n\nIndeed, not all computer theorists would agree with the premise that computers can only do what they are programmed to do—a key point in favor of computational creativity.\n\nBecause no single perspective or definition seems to offer a complete picture of creativity, the AI researchers Newell, Shaw and Simon developed the combination of novelty and usefulness into the cornerstone of a multi-pronged view of creativity, one that uses the following four criteria to categorize a given answer or solution as creative:\n\n\nWhereas the above reflects a \"top-down\" approach to computational creativity, an alternative thread has developed among \"bottom-up\" computational psychologists involved in artificial neural network research. During the late 1980s and early 1990s, for example, such generative neural systems were driven by genetic algorithms. Experiments involving recurrent nets were successful in hybridizing simple musical melodies and predicting listener expectations.\n\nConcurrent with such research, a number of computational psychologists took the perspective, popularized by Stephen Wolfram, that system behaviors perceived as complex, including the mind's creative output, could arise from what would be considered simple algorithms. As neuro-philosophical thinking matured, it also became evident that language actually presented an obstacle to producing a scientific model of cognition, creative or not, since it carried with it so many unscientific aggrandizements that were more uplifting than accurate. Thus questions naturally arose as to how \"rich,\" \"complex,\" and \"wonderful\" creative cognition actually was.\n\nBefore 1989, artificial neural networks have been used to model certain aspects of creativity. Peter Todd (1989) first trained a neural network to reproduce musical melodies from a training set of musical pieces. Then he used a change algorithm to modify the network's input parameters. The network was able to randomly generate new music in a highly uncontrolled manner. In 1992, Todd\nextended this work, using the so-called distal teacher approach that had been developed by\nPaul Munro, Paul Werbos, D. Nguyen and Bernard Widrow, Michael I. Jordan and David Rumelhart. In the new approach there are two neural networks, one of which is supplying training patterns to another. \nIn later efforts by Todd, a composer would select a set of melodies that define the melody space, position them on a 2-d plane with a mouse-based graphic interface, and train a connectionist network to produce those melodies, and listen to the new \"interpolated\" melodies that the network generates corresponding to intermediate points in the 2-d plane.\n\nMore recently a neurodynamical model of semantic networks has been developed to study how the connectivity structure of these networks relates to the richness of the semantic constructs, or ideas, they can generate. It was demonstrated that semantic neural networks that have richer semantic dynamics than those with other connectivity structures may provide insight into the important issue of how the physical structure of the brain determines one of the most profound features of the human mind – its capacity for creative thought.\n\nSome high-level and philosophical themes recur throughout the field of computational creativity.\n\nMargaret Boden refers to creativity that is novel \"merely to the agent that produces it\" as \"P-creativity\" (or \"psychological creativity\"), and refers to creativity that is recognized as novel \"by society at large\" as \"H-creativity\" (or \"historical creativity\"). Stephen Thaler has suggested a new category he calls \"V-\" or \"Visceral creativity\" wherein significance is invented to raw sensory inputs to a Creativity Machine architecture, with the \"gateway\" nets perturbed to produce alternative interpretations, and downstream nets shifting such interpretations to fit the overarching context. An important variety of such V-creativity is consciousness itself, wherein meaning is reflexively invented to activation turnover within the brain.\n\nBoden also distinguishes between the creativity that arises from an exploration within an established conceptual space, and the creativity that arises from a deliberate transformation or transcendence of this space. She labels the former as \"exploratory creativity\" and the latter as \"transformational creativity\", seeing the latter as a form of creativity far more radical, challenging, and rarer than the former. Following the criteria from Newell and Simon elaborated above, we can see that both forms of creativity should produce results that are appreciably novel and useful (criterion 1), but exploratory creativity is more likely to arise from a thorough and persistent search of a well-understood space (criterion 3) -- while transformational creativity should involve the rejection of some of the constraints that define this space (criterion 2) or some of the assumptions that define the problem itself (criterion 4). Boden's insights have guided work in computational creativity at a very general level, providing more an inspirational touchstone for development work than a technical framework of algorithmic substance. However, Boden's insights are more recently also the subject of formalization, most notably in the work by Geraint Wiggins.\n\nThe criterion that creative products should be novel and useful means that creative computational systems are typically structured into two phases, generation and evaluation. In the first phase, novel (to the system itself, thus P-Creative) constructs are generated; unoriginal constructs that are already known to the system are filtered at this stage. This body of potentially creative constructs are then evaluated, to determine which are meaningful and useful and which are not. This two-phase structure conforms to the Geneplore model of Finke, Ward and Smith, which is a psychological model of creative generation based on empirical observation of human creativity.\n\nA great deal, perhaps all, of human creativity can be understood as a novel combination of pre-existing ideas or objects . Common strategies for combinatorial creativity include:\nThe combinatorial perspective allows us to model creativity as a search process through the space of possible combinations. The combinations can arise from composition or concatenation of different representations, or through a rule-based or stochastic transformation of initial and intermediate representations. Genetic algorithms and neural networks can be used to generate blended or crossover representations that capture a combination of different inputs.\n\nMark Turner and Gilles Fauconnier propose a model called Conceptual Integration Networks that elaborates upon Arthur Koestler's ideas about creativity as well as more recent work by Lakoff and Johnson, by synthesizing ideas from Cognitive Linguistic research into mental spaces and conceptual metaphors. Their basic model defines an integration network as four connected spaces:\n\n\nFauconnier and Turner describe a collection of optimality principles that are claimed to guide the construction of a well-formed integration network. In essence, they see blending as a compression mechanism in which two or more input structures are compressed into a single blend structure. This compression operates on the level of conceptual relations. For example, a series of similarity relations between the input spaces can be compressed into a single identity relationship in the blend.\n\nSome computational success has been achieved with the blending model by extending pre-existing computational models of analogical mapping that are compatible by virtue of their emphasis on connected semantic structures. More recently, Francisco Câmara Pereira presented an implementation of blending theory that employs ideas both from GOFAI and genetic algorithms to realize some aspects of blending theory in a practical form; his example domains range from the linguistic to the visual, and the latter most notably includes the creation of mythical monsters by combining 3-D graphical models.\n\nLanguage provides continuous opportunity for creativity, evident in the generation of novel sentences, phrasings, puns, neologisms, rhymes, allusions, sarcasm, irony, similes, metaphors, analogies, witticisms, and jokes. Native speakers of morphologically rich languages frequently create new word-forms that are easily understood, although they will never find their way to the dictionary. The area of natural language generation has been well studied, but these creative aspects of everyday language have yet to be incorporated with any robustness or scale.\n\nSubstantial work has been conducted in this area of linguistic creation since the 1970s, with the development of James Meehan's TALE-SPIN\n\nThe company Narrative Science makes computer generated news and reports commercially available, including summarizing team sporting events based on statistical data from the game. It also creates financial reports and real estate analyses.\n\nExample of a metaphor: \"\"She was an ape.\"\"\n\nExample of a simile: \"\"Felt like a tiger-fur blanket.\"\"\nThe computational study of these phenomena has mainly focused on interpretation as a knowledge-based process. Computationalists such as Yorick Wilks, James Martin, Dan Fass, John Barnden, and Mark Lee have developed knowledge-based approaches to the processing of metaphors, either at a linguistic level or a logical level. Tony Veale and Yanfen Hao have developed a system, called Sardonicus, that acquires a comprehensive database of explicit similes from the web; these similes are then tagged as bona-fide (e.g., \"as hard as steel\") or ironic (e.g., \"as hairy as a bowling ball\", \"as pleasant as a root canal\"); similes of either type can be retrieved on demand for any given adjective. They use these similes as the basis of an on-line metaphor generation system called Aristotle that can suggest lexical metaphors for a given descriptive goal (e.g., to describe a supermodel as skinny, the source terms \"pencil\", \"whip\", \"whippet\", \"rope\", \"stick-insect\" and \"snake\" are suggested).\n\nThe process of analogical reasoning has been studied from both a mapping and a retrieval perspective, the latter being key to the generation of novel analogies. The dominant school of research, as advanced by Dedre Gentner, views analogy as a structure-preserving process; this view has been implemented in the structure mapping engine or SME, the MAC/FAC retrieval engine (Many Are Called, Few Are Chosen), ACME (Analogical Constraint Mapping Engine) and ARCS (Analogical Retrieval Constraint System). Other mapping-based approaches include Sapper, which situates the mapping process in a semantic-network model of memory. Analogy is a very active sub-area of creative computation and creative cognition; active figures in this sub-area include Douglas Hofstadter, Paul Thagard, and Keith Holyoak. Also worthy of note here is Peter Turney and Michael Littman's machine learning approach to the solving of SAT-style analogy problems; their approach achieves a score that compares well with average scores achieved by humans on these tests.\n\nHumour is an especially knowledge-hungry process, and the most successful joke-generation systems to date have focussed on pun-generation, as exemplified by the work of Kim Binsted and Graeme Ritchie. This work includes the JAPE system, which can generate a wide range of puns that are consistently evaluated as novel and humorous by young children. An improved version of JAPE has been developed in the guise of the STANDUP system, which has been experimentally deployed as a means of enhancing linguistic interaction with children with communication disabilities. Some limited progress has been made in generating humour that involves other aspects of natural language, such as the deliberate misunderstanding of pronominal reference (in the work of Hans Wim Tinholt and Anton Nijholt), as well as in the generation of humorous acronyms in the HAHAcronym system of Oliviero Stock and Carlo Strapparava.\n\nThe blending of multiple word forms is a dominant force for new word creation in language; these new words are commonly called \"blends\" or \"portmanteau words\" (after Lewis Carroll). Tony Veale has developed a system called ZeitGeist that harvests neological headwords from Wikipedia and interprets them relative to their local context in Wikipedia and relative to specific word senses in WordNet. ZeitGeist has been extended to generate neologisms of its own; the approach combines elements from an inventory of word parts that are harvested from WordNet, and simultaneously determines likely glosses for these new words (e.g., \"food traveller\" for \"gastronaut\" and \"time traveller\" for \"chrononaut\"). It then uses Web search to determine which glosses are meaningful and which neologisms have not been used before; this search identifies the subset of generated words that are both novel (\"H-creative\") and useful. Neurolinguistic inspirations have been used to analyze the process of novel word creation in the brain, understand neurocognitive processes responsible for intuition, insight, imagination and creativity and to create a server that invents novel names for products, based on their description. Further, the system Nehovah blends two source words into a neologism that blends the meanings of the two source words. Nehovah searches WordNet for synonyms and TheTopTens.com for pop culture hyponyms. The synonyms and hyponyms are blended together to create a set of candidate neologisms. The neologisms are then scored based on their word structure, how unique the word is, how apparent the concepts are conveyed, and if the neologism has a pop culture reference. Nehovah loosely follows conceptual blending.\n\nLike jokes, poems involve a complex interaction of different constraints, and no general-purpose poem generator adequately combines the meaning, phrasing, structure and rhyme aspects of poetry. Nonetheless, Pablo Gervás has developed a noteworthy system called ASPERA that employs a case-based reasoning (CBR) approach to generating poetic formulations of a given input text via a composition of poetic fragments that are retrieved from a case-base of existing poems. Each poem fragment in the ASPERA case-base is annotated with a prose string that expresses the meaning of the fragment, and this prose string is used as the retrieval key for each fragment. Metrical rules are then used to combine these fragments into a well-formed poetic structure. Racter is an example of such a software project.\n\nComputational creativity in the music domain has focused both on the generation of musical scores for use by human musicians, and on the generation of music for performance by computers. The domain of generation has included classical music (with software that generates music in the style of Mozart and Bach) and jazz. Most notably, David Cope has written a software system called \"Experiments in Musical Intelligence\" (or \"EMI\") that is capable of analyzing and generalizing from existing music by a human composer to generate novel musical compositions in the same style. EMI's output is convincing enough to persuade human listeners that its music is human-generated to a high level of competence.\n\nIn the field of contemporary classical music, Iamus is the first computer that composes from scratch, and produces final scores that professional interpreters can play. The London Symphony Orchestra played a piece for full orchestra, included in Iamus' debut CD, which \"New Scientist\" described as \"The first major work composed by a computer and performed by a full orchestra\". Melomics, the technology behind Iamus, is able to generate pieces in different styles of music with a similar level of quality.\n\nCreativity research in jazz has focused on the process of improvisation and the cognitive demands that this places on a musical agent: reasoning about time, remembering and conceptualizing what has already been played, and planning ahead for what might be played next. The robot Shimon, developed by Gil Weinberg of Georgia Tech, has demonstrated jazz improvisation. Virtual improvisation software based on machine learning models of musical style include OMax, SoMax and PyOracle, are used to create improvisations in real-time by re-injecting variable length sequences learned on the fly from live performer.\n\nIn 1994, a Creativity Machine architecture (see above) was able to generate 11,000 musical hooks by training a synaptically perturbed neural net on 100 melodies that had appeared on the top ten list over the last 30 years. In 1996, a self-bootstrapping Creativity Machine observed audience facial expressions through an advanced machine vision system and perfected its musical talents to generate an album entitled \"Song of the Neurons\"\n\nIn the field of musical composition, the patented works by René-Louis Baron allowed to make a robot that can create and play a multitude of orchestrated melodies so-called \"coherent\" in any musical style. All outdoor physical parameter associated with one or more specific musical parameters, can influence and develop each of these songs (in real time while listening to the song). The patented invention \"Medal-Composer\" raises problems of copyright.\n\nComputational creativity in the generation of visual art has had some notable successes in the creation of both abstract art and representational art. The most famous program in this domain is Harold Cohen's AARON, which has been continuously developed and augmented since 1973. Though formulaic, Aaron exhibits a range of outputs, generating black-and-white drawings or colour paintings that incorporate human figures (such as dancers), potted plants, rocks, and other elements of background imagery. These images are of a sufficiently high quality to be displayed in reputable galleries.\n\nOther software artists of note include the NEvAr system (for \"Neuro-Evolutionary Art\") of Penousal Machado. NEvAr uses a genetic algorithm to derive a mathematical function that is then used to generate a coloured three-dimensional surface. A human user is allowed to select the best pictures after each phase of the genetic algorithm, and these preferences are used to guide successive phases, thereby pushing NEvAr's search into pockets of the search space that are considered most appealing to the user.\n\nThe Painting Fool, developed by Simon Colton originated as a system for overpainting digital images of a given scene in a choice of different painting styles, colour palettes and brush types. Given its dependence on an input source image to work with, the earliest iterations of the Painting Fool raised questions about the extent of, or lack of, creativity in a computational art system. Nonetheless, in more recent work, The Painting Fool has been extended to create novel images, much as AARON does, from its own limited imagination. Images in this vein include cityscapes and forests, which are generated by a process of constraint satisfaction from some basic scenarios provided by the user (e.g., these scenarios allow the system to infer that objects closer to the viewing plane should be larger and more color-saturated, while those further away should be less saturated and appear smaller). Artistically, the images now created by the Painting Fool appear on a par with those created by Aaron, though the extensible mechanisms employed by the former (constraint satisfaction, etc.) may well allow it to develop into a more elaborate and sophisticated painter.\n\nThe artist Krasimira Dimtchevska and the software developer Svillen Ranev have created a computational system combining a rule-based generator of English sentences and a visual composition builder that converts sentences generated by the system into abstract art. The software generates automatically indefinite number of different images using different color, shape and size palettes. The software also allows the user to select the subject of the generated sentences or/and the one or more of the palettes used by the visual composition builder.\n\nAn emerging area of computational creativity is that of video games. ANGELINA is a system for creatively developing video games in Java by Michael Cook. One important aspect is Mechanic Miner, a system which can generate short segments of code which act as simple game mechanics. ANGELINA can evaluate these mechanics for usefulness by playing simple unsolvable game levels and testing to see if the new mechanic makes the level solvable. Sometimes Mechanic Miner discovers bugs in the code and exploits these to make new mechanics for the player to solve problems with.\n\nIn July 2015 Google released \"DeepDream\" – an open source computer vision program, created to detect faces and other patterns in images with the aim of automatically classifying images, which uses a convolutional neural network to find and enhance patterns in images via algorithmic pareidolia, thus creating a dreamlike psychedelic appearance in the deliberately over-processed images.\n\nIn August 2015 researchers from Tübingen, Germany created a convolutional neural network that uses neural representations to separate and recombine content and style of arbitrary images which is able to turn images into stylistic imitations of works of art by artists such as a Picasso or Van Gogh in about an hour. Their algorithm is put into use in the website DeepArt that allows users to create unique artistic images by their algorithm.\n\nIn early 2016, a global team of researchers explained how a new computational creativity approach known as the Digital Synaptic Neural Substrate (DSNS) could be used to generate original chess puzzles that were not derived from endgame databases. The DSNS is able to combine features of different objects (e.g. chess problems, paintings, music) using stochastic methods in order to derive new feature specifications which can be used to generate objects in any of the original domains. The generated chess puzzles have also been featured on YouTube.\n\nCreativity is also useful in allowing for unusual solutions in problem solving. In psychology and cognitive science, this research area is called creative problem solving. The Explicit-Implicit Interaction (EII) theory of creativity has recently been implemented using a CLARION-based computational model that allows for the simulation of incubation and insight in problem solving. The emphasis of this computational creativity project is not on performance per se (as in artificial intelligence projects) but rather on the explanation of the psychological processes leading to human creativity and the reproduction of data collected in psychology experiments. So far, this project has been successful in providing an explanation for incubation effects in simple memory experiments, insight in problem solving, and reproducing the overshadowing effect in problem solving.\n\nSome researchers feel that creativity is a complex phenomenon whose study is further complicated by the plasticity of the language we use to describe it. We can describe not just the agent of creativity as \"creative\" but also the product and the method. Consequently, it could be claimed that it is unrealistic to speak of a \"general theory of creativity\". Nonetheless, some generative principles are more general than others, leading some advocates to claim that certain computational approaches are \"general theories\". Stephen Thaler, for instance, proposes that certain modalities of neural networks are generative enough, and general enough, to manifest a high degree of creative capabilities. Likewise, the Formal Theory of Creativity is based on a simple computational principle published by Jürgen Schmidhuber in 1991. The theory postulates that creativity and curiosity and selective attention in general are by-products of a simple algorithmic principle for measuring and optimizing learning progress.\n\nA unifying model of creativity was proposed by S. L. Thaler through a series of international patents in computational creativity, beginning in 1997 with the issuance of U.S. Patent 5,659,666. Based upon theoretical studies of traumatized neural networks and inspired by studies of damage-induced vibrational modes in simulated crystal lattices, this extensive intellectual property suite taught the application of a broad range of noise, damage, and disordering effects to a trained neural network so as to drive the formation of novel or confabulatory patterns that could potentially qualify as ideas and/or plans of action.\n\nThaler's scientific and philosophical papers both preceding and following the issuance of these patents described:\n\nThaler has also recruited his generative neural architectures into a theory of consciousness that closely models the temporal evolution of thought, creative or not, while also accounting for the subjective feel associated with this hotly debated mental phenomenon.\n\nIn 1989, in one of the most controversial reductions to practice of this general theory of creativity, one neural net termed the \"grim reaper,\" governed the synaptic damage (i.e., rule-changes) applied to another net that had learned a series of traditional Christmas carol lyrics. The former net, on the lookout for both novel and grammatical lyrics, seized upon the chilling sentence, \"In the end all men go to good earth in one eternal silent night,\" thereafter ceasing the synaptic degradation process. In subsequent projects, these systems produced more useful results across many fields of human endeavor, oftentimes bootstrapping their learning from a blank slate based upon the success or failure of self-conceived concepts and strategies seeded upon such internal network damage.\n\nTraditional computers, as mainly used in the computational creativity application, do not support creativity, as they fundamentally transform a set of discrete, limited domain of input parameters into a set of discrete, limited domain of output parameters using a limited set of computational functions. As such, a computer cannot be creative, as everything in the output must have been already present in the input data or the algorithms. For some related discussions and references to related work are captured in some recent work on philosophical foundations of simulation.\n\nMathematically, the same set of arguments against creativity has been made by Chaitin. Similar observations come from a Model Theory perspective. All this criticism emphasizes that computational creativity is useful and may look like creativity, but it is not real creativity, as nothing new is created, just transformed in well defined algorithms.\n\nThe International Conference on Computational Creativity (ICCC) occurs annually, organized by The Association for Computational Creativity. Events in the series include:\n\nPreviously, the community of computational creativity has held a dedicated workshop, the International Joint Workshop on Computational Creativity, every year since 1999. Previous events in this series include:\n\n\nThe 1st Conference on Computer Simulation of Musical Creativity will be held\n\nDesign Computing and Cognition is one conference that addresses computational creativity. The ACM Creativity and Cognition conference is another forum for issues related to computational creativity. Journées d'Informatique Musicale 2016 keynote by Shlomo Dubnov was on Information Theoretic Creativity.\n\nA number of recent books provide either a good introduction or a good overview of the field of Computational Creativity. These include:\n\n\nIn addition to the proceedings of conferences and workshops, the computational creativity community has thus far produced these special journal issues dedicated to the topic:\n\n\nIn addition to these, a new journal has started which focuses on computational creativity within the field of music.\n\n\n\n\n\n",
    "id": "16300571",
    "title": "Computational creativity"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=16167377",
    "text": "Artificial Intelligence System\n\nArtificial Intelligence System (AIS) was a distributed computing project undertaken by Intelligence Realm, Inc. with the long-term goal of simulating the human brain in real time, complete with artificial consciousness and artificial general intelligence. They claimed to have found, in research, the \"mechanisms of knowledge representation in the brain which is equivalent to finding artificial intelligence\", before moving into the developmental phase.\n\nThe project's initial goal was recreating the largest brain simulation to date, performed by neuroscientist Eugene M. Izhikevich of The Neurosciences Institute in San Diego, California. Izhikevich simulated 1 second of activity of 100 billion neurons (the estimated number of neurons in the human brain) in 50 days using a cluster of 27 3-gigahertz processors. He extrapolated that a real-time simulation of the brain could not be achieved before 2016. The project aimed to disprove this prediction.\n\nOn July 12, 2008, AIS announced that the first phase of the project had been completed by reaching the 100 billion neuron mark. The project then continued to simulate neurons while they completed the development of the other applications.\n\nAIS simulated the brain via an artificial neural network, and used Hodgkin–Huxley models. The project utilized the BOINC distributed computing platform. In version 1.08 of the software each work unit received by a volunteer simulated 500,000 neurons for 100 milliseconds at 5 millisecond time steps (the estimated firing rate of a human neuron).\n\nThe application had four primary modules—for creating neurons, simulating neurons, visualizing neurons, and finally, knowledge acquisition. Intention was that the neuronal generator would eventually use genetic algorithms to generate neurons for simulation. The neuron simulator used mathematical models to simulate those neurons. Initially Hodgkin–Huxley models were used, but more models (perhaps hundreds) were intended to be utilized in the future. The visualization software was to allow the administrators to monitor and control the neuronal simulators. The knowledge acquisition module involved feeding information to the system and training it to build its knowledge base.\n\nThe AIS project had successfully simulated over 700 billion neurons by April 2009.\n\nThe project was closed in November 2010 as the BOINC program of the project did not work.\n\n",
    "id": "16167377",
    "title": "Artificial Intelligence System"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=16598232",
    "text": "Progress in artificial intelligence\n\nArtificial intelligence applications have been used in a wide range of fields including medical diagnosis, stock trading, robot control, law, scientific discovery and toys. However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\" \"Many thousands of AI applications are deeply embedded in the infrastructure of every industry.\" In the late 1990s and early 21st century, AI technology became widely used as elements of larger systems, but the field is rarely credited for these successes.\n\nTo allow comparison with human performance, artificial intelligence can be evaluated on constrained and well-defined problems. Such tests have been termed subject matter expert Turing tests. Also, smaller problems provide more achievable goals and there are an ever-increasing number of positive results.\n\nIn his famous Turing test, Alan Turing picked language, the defining feature of human beings, for its basis. Yet, there are many other useful abilities that can be described as showing some form of intelligence. This gives better insight into the comparative success of artificial intelligence in different areas.\n\nIn what has been called the Feigenbaum test, the inventor of expert systems argued for subject specific expert tests. A paper by Jim Gray of Microsoft in 2003 suggested extending the Turing test to speech understanding, speaking and recognizing objects and behavior.\n\nBroad classes of outcome for an AI test may be given as:\n\n\n\n\n\n\n",
    "id": "16598232",
    "title": "Progress in artificial intelligence"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=16934252",
    "text": "Data pack\n\nA data pack (or fact pack) is a pre-made database that can be fed to a software, such as software agents, Internet bots or chatterbots, to teach information and facts, which it can later look up. In other words, a data pack can be used to feed minor updates into a system.\n\nCommon data packs may include abbreviations, acronyms, dictionaries, lexicons and technical data, such as country codes, RFCs, filename extensions, TCP and UDP port numbers, country calling codes, and so on.\n\nData packs may come in formats of CSV and SQL that can easily be parsed or imported into a database management system.\n\nThe database may consist of a key-value pair, like an association list.\n\nData packs are commonly used within the gaming industry to provide minor updates within their games. When a user downloads an update for a game they will be downloading loads of data packs which will contain updates for the game such as minor bug fixes or additional content. An example of a data pack used to update a game can be found on the references.\n\nA data pack DataPack Definition is similar to a data packet it contains loads of information (data) and stores it within a pack where the data can be compressed to reduce its file size. Only certain programs can read a data pack therefore when the data is packed it is vital to know whether the receiving program is able to unpack the data. An example of data packs which are able to effective deliver information can be found on the reference page.\n\nWhen you refer to the word data pack it can come in many forms such as a mobile data pack. A mobile data pack refers to an add-on which can enable you to boost the amount of data which you can use on your mobile phone. The rate at which you use your data can also be monitored, so you know how much data you have left. Mobile data is a service which provides a similar service to Wi-Fi and allows you to connect to the Internet. So the purpose of a data pack is to increase the amount of data that your mobile has access to. An example of a mobile data pack can be found on the references.\n\n\n",
    "id": "16934252",
    "title": "Data pack"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=14273658",
    "text": "Agent systems reference model\n\nThe agent systems reference model (ASRM) is a layered, abstract description for multiagent systems. As such, the reference model\n\nThe ASRM differentiates itself from technical standards, such as Knowledge Interchange Format, Knowledge Query and Manipulation Language, and those of the Foundation for Intelligent Physical Agents in that it defines the required \"existence\" of components of a multiagent system; standards prescribe how they are \"designed\".\n\nThe ASRM was technically constructed through forensic software analysis of existing agent-based systems. Such fielded systems include JaDE, Cougaar, EMAA, NOMADS, Retsina, A-Globe, among others. In so doing, through empirical evidence, the ASRM motivates its functional breakdown of agent-based systems.\n\nThe ASRM was started in July 2005, with the first draft having been completed in November 2006. Contributors to the document have included Drexel University, Cougaar Software, Global InfoTek (see also: CoABS), Soar Technology (see also: Soar), Penn State University, University of Southern California, University of South Carolina, the Florida Institute for Human and Machine Cognition, University of West Florida, BBN Technologies, Telcordia, Lockheed Martin, General Dynamics and others.\n\n\n",
    "id": "14273658",
    "title": "Agent systems reference model"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=7539196",
    "text": "The Leaf (AI) Project\n\nThe Leaf Project is a group robot development program whose objective is to develop a robot platform that supports experiments with artificial intelligence, vision, navigation, etc. \n\nLeaf was created by Bruce Weimer, Alex Brown and Robin Hewitt. It is an artificial life program, inspired by Steve Grand's computer game Creatures, in which artificial beings hatch, develop, and interact in a simulated environment. A PC (software-only) version of Leaf was demonstrated in 2003, construction of the first robot began in early 2004.\n\nLeaf:\n\nThree Leaf robots were completed in 2004 and four more were built in 2006.\n\n",
    "id": "7539196",
    "title": "The Leaf (AI) Project"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=11306",
    "text": "Frame problem\n\nIn artificial intelligence, the frame problem describes an issue with using first-order logic (FOL) to express facts about a robot in the world. Representing the state of a robot with traditional FOL requires the use of many axioms that simply imply that things in the environment do not change arbitrarily. For example, Hayes describes a \"block world\" with rules about stacking blocks together. In a FOL system, additional axioms are required to make inferences about the environment (for example, that a block cannot change position unless it is physically moved). The frame problem is the problem of finding adequate collections of axioms for a viable description of a robot environment.\n\nJohn McCarthy and Patrick J. Hayes defined this problem in their 1969 article, \"Some Philosophical Problems from the Standpoint of Artificial Intelligence\". In this paper, and many that came after, the formal mathematical problem was a starting point for more general discussions of the difficulty of knowledge representation for artificial intelligence. Issues such as how to provide rational default assumptions and what humans consider common sense in a virtual environment. Later, the term acquired a broader meaning in philosophy, where it is formulated as the problem of limiting the beliefs that have to be updated in response to actions. In the logical context, actions are typically specified by what they change, with the implicit assumption that everything else (the frame) remains unchanged.\n\nThe frame problem occurs even in very simple domains. A scenario with a door, which can be open or closed, and a light, which can be on or off, is statically represented by two propositions formula_1 and formula_2. If these conditions can change, they are better represented by two predicates formula_3 and formula_4 that depend on time; such predicates are called fluents. A domain in which the door is closed and the light off at time 0, and the door opened at time 1, can be directly represented in logic by the following formulae:\n\nThe first two formulae represent the initial situation; the third formula represents the effect of executing the action of opening the door at time 1. If such an action had preconditions, such as the door being unlocked, it would have been represented by formula_8. In practice, one would have a predicate formula_9 for specifying when an action is executed and a rule formula_10 for specifying the effects of actions. The article on the situation calculus gives more details.\n\nWhile the three formulae above are a direct expression in logic of what is known, they do not suffice to correctly draw consequences. While the following conditions (representing the expected situation) are consistent with the three formulae above, they are not the only ones.\n\nIndeed, another set of conditions that is consistent with the three formulae above is:\n\nThe frame problem is that specifying only which conditions are changed by the actions does not entail that all other conditions are not changed. This problem can be solved by adding the so-called “frame axioms”, which explicitly specify that all conditions not affected by actions are not changed while executing that action. For example, since the action executed at time 0 is that of opening the door, a frame axiom would state that the status of the light does not change from time 0 to time 1:\n\nThe frame problem is that one such frame axiom is necessary for every pair of action and condition such that the action does not affect the condition. In other words, the problem is that of formalizing a dynamical domain without explicitly specifying the frame axioms.\n\nThe solution proposed by McCarthy to solve this problem involves assuming that a minimal amount of condition changes have occurred; this solution is formalized using the framework of circumscription. The Yale shooting problem, however, shows that this solution is not always correct. Alternative solutions were then proposed, involving predicate completion, fluent occlusion, successor state axioms, etc.; they are explained below. By the end of the 1980s, the frame problem as defined by McCarthy and Hayes was solved. Even after that, however, the term “frame problem” was still used, in part to refer to the same problem but under different settings (e.g., concurrent actions), and in part to refer to the general problem of representing and reasoning with dynamical domains.\n\nThe following solutions depict how the frame problem is solved in various formalisms. The formalisms themselves are not presented in full: what is presented are simplified versions that are sufficient to explain the full solution.\n\nThis solution was proposed by Erik Sandewall, who also defined a formal language for the specification of dynamical domains; therefore, such a domain can be first expressed in this language and then automatically translated into logic. In this article, only the expression in logic is shown, and only in the simplified language with no action names.\n\nThe rationale of this solution is to represent not only the value of conditions over time, but also whether they can be affected by the last executed action. The latter is represented by another condition, called occlusion. A condition is said to be \"occluded\" in a given time point if an action has been just executed that makes the condition true or false as an effect. Occlusion can be viewed as “permission to change”: if a condition is occluded, it is relieved from obeying the constraint of inertia.\n\nIn the simplified example of the door and the light, occlusion can be formalized by two predicates formula_12 and formula_13. The rationale is that a condition can change value only if the corresponding occlusion predicate is true at the next time point. In turn, the occlusion predicate is true only when an action affecting the condition is executed.\n\nIn general, every action making a condition true or false also makes the corresponding occlusion predicate true. In this case, formula_19 is true, making the antecedent of the fourth formula above false for formula_20; therefore, the constraint that formula_21 does not hold for formula_20. Therefore, formula_1 can change value, which is also what is enforced by the third formula.\n\nIn order for this condition to work, occlusion predicates have to be true only when they are made true as an effect of an action. This can be achieved either by circumscription or by predicate completion. It is worth noticing that occlusion does not necessarily imply a change: for example, executing the action of opening the door when it was already open (in the formalization above) makes the predicate formula_24 true and makes formula_1 true; however, formula_1 has not changed value, as it was true already.\n\nThis encoding is similar to the fluent occlusion solution, but the additional predicates denote change, not permission to change. For example, formula_27 represents the fact that the predicate formula_1 will change from time formula_29 to formula_30. As a result, a predicate changes if and only if the corresponding change predicate is true. An action results in a change if and only if it makes true a condition that was previously false or vice versa.\n\nThe third formula is a different way of saying that opening the door causes the door to be opened. Precisely, it states that opening the door changes the state of the door if it had been previously closed. The last two conditions state that a condition changes value at time formula_29 if and only if the corresponding change predicate is true at time formula_29. To complete the solution, the time points in which the change predicates are true have to be as few as possible, and this can be done by applying predicate completion to the rules specifying the effects of actions.\n\nThe value of a condition after the execution of an action can be determined by\nthe fact that the condition is true if and only if:\n\n\nA successor state axiom is a formalization in logic of these two facts. For\nexample, if formula_38 and formula_39 are two\nconditions used to denote that the action executed at time formula_29 was\nto open or close the door, respectively, the running example is encoded as\nfollows.\n\nThis solution is centered around the value of conditions, rather than the\neffects of actions. In other words, there is an axiom for every condition,\nrather than a formula for every action. Preconditions to actions (which are not\npresent in this example) are formalized by other formulae. The successor state\naxioms are used in the variant to the situation calculus proposed by\nRay Reiter.\n\nThe fluent calculus is a variant of the situation calculus. It solves the frame problem by using first-order logic\nterms, rather than predicates, to represent the states. Converting\npredicates into terms in first-order logic is called reification; the\nfluent calculus can be seen as a logic in which predicates representing the\nstate of conditions are reified.\n\nThe difference between a predicate and a term in first-order logic is that a term is a representation of an object (possibly a complex object composed of other objects), while a predicate represents a condition that can be true or false when evaluated over a given set of terms.\n\nIn the fluent calculus, each possible state is represented by a term obtained by composition of other terms, each one representing the conditions that are true in state. For example, the state in which the door is open and the light is on is represented by the term formula_45. It is important to notice that a term is not true or false by itself, as it is an object and not a condition. In other words, the term formula_45 represent a possible state, and does not by itself mean that this is the current state. A separate condition can be stated to specify that this is actually the state at a given time, e.g., formula_47 means that this is the state at time formula_48.\n\nThe solution to the frame problem given in the fluent calculus is to specify the effects of actions by stating how a term representing the state changes when the action is executed. For example, the action of opening the door at time 0 is represented by the formula:\n\nThe action of closing the door, which makes a condition false instead of true, is represented in a slightly different way:\n\nThis formula works provided that suitable axioms are given about formula_51 and formula_52, e.g., a term containing the same condition twice is not a valid state (for example, formula_53 is always false for every formula_54 and formula_29).\n\nThe event calculus uses terms for representing fluents, like the fluent calculus, but also has axioms constraining the value of fluents, like the successor state axioms. In the event calculus, inertia is enforced by formulae stating that a fluent is true if it has been true at a given previous time point and no action changing it to false has been performed in the meantime. Predicate completion is still needed in the event calculus for obtaining that a fluent is made true only if an action making it true has been performed, but also for obtaining that an action had been performed only if that is explicitly stated.\n\nThe frame problem can be thought of as the problem of formalizing the principle that, by default, \"everything is presumed to remain in the state in which it is\" (Leibniz, \"An Introduction to a Secret Encyclopædia\", \"c\". 1679). This default, sometimes called the \"commonsense law of inertia\", was expressed by Raymond Reiter in default logic:\n\n(if formula_57 is true in situation formula_54, and it can be assumed that formula_57 remains true after executing action formula_60, then we can conclude that formula_57 remains true).\n\nSteve Hanks and Drew McDermott argued, on the basis of their Yale shooting example, that this solution to the frame problem is unsatisfactory. Hudson Turner showed, however, that it works correctly in the presence of appropriate additional postulates.\n\nThe counterpart of the default logic solution in the language of answer set programming is a rule with strong negation:\n\n(if formula_63 is true at time formula_64, and it can be assumed that formula_63 remains true at time formula_66, then we can conclude that formula_63 remains true).\n\nAction description languages elude the frame problem rather than solving it. An action description language is a formal language with a syntax that is specific for describing situations and actions. For example, that the action formula_68 makes the door open if not locked is expressed by:\n\nThe semantics of an action description language depends on what the language can express (concurrent actions, delayed effects, etc.) and is usually based on transition systems.\n\nSince domains are expressed in these languages rather than directly in logic, the frame problem only arises when a specification given in an action description logic is to be translated into logic. Typically, however, a translation is given from these languages to answer set programming rather than first-order logic.\n\n\n\n",
    "id": "11306",
    "title": "Frame problem"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=9536113",
    "text": "Computer audition\n\nComputer audition (CA) or machine listening is general field of study of algorithms and systems for audio understanding by machine. Since the notion of what it means for a machine to \"hear\" is very broad and somewhat vague, computer audition attempts to bring together several disciplines that originally dealt with specific problems or had a concrete application in mind. The engineer Paris Smaragdis, interviewed in \"Technology Review\", talks about these systems --\"software that uses sound to locate people moving through rooms, monitor machinery for impending breakdowns, or activate traffic cameras to record accidents.\"\n\nInspired by models of human audition, CA deals with questions of representation, transduction, grouping, use of musical knowledge and general sound semantics for the purpose of performing intelligent operations on audio and music signals by the computer. Technically this requires a combination of methods from the fields of signal processing, auditory modelling, music perception and cognition, pattern recognition, and machine learning, as well as more traditional methods of artificial intelligence for musical knowledge representation.\n\nLike computer vision versus image processing, computer audition versus audio engineering deals with understanding of audio rather than processing. It also differs from problems of speech understanding by machine since it deals with general audio signals, such as natural sounds and musical recordings.\n\nApplications of computer auditions are widely varying, and include search for sounds, genre recognition, acoustic monitoring, music transcription, score following, audio texture, music improvisation, emotion in audio and so on.\n\nComputer Audition overlaps with the following disciplines:\n\nSince audio signals are interpreted by the human ear-brain system, that complex perceptual mechanism should be simulated somehow in software for \"machine listening\". In other words, to perform on par with humans, the computer should hear and understand audio content much as humans do. Analyzing audio accurately involves several fields: electrical engineering (spectrum analysis, filtering, and audio transforms); artificial intelligence (machine learning and sound classification); psychoacoustics (sound perception); cognitive sciences (neuroscience and artificial intelligence); acoustics (physics of sound production); and music (harmony, rhythm, and timbre). Furthermore, audio transformations such as pitch shifting, time stretching, and sound object filtering, should be perceptually and musically meaningful. For best results, these transformations require perceptual understanding of spectral models, high-level feature extraction, and sound analysis/synthesis. Finally, structuring and coding the content of an audio file (sound and metadata) could benefit from efficient compression schemes, which discard inaudible information in the sound. Computational models of music and sound perception and cognition can lead to a more meaningful representation, a more intuitive digital manipulation and generation of sound and music in musical human-machine interfaces.\n\nThe study of CA could be roughly divided into the following sub-problems:\n\nComputer audition deals with audio signals that can be represented in a variety of fashions, from direct encoding of digital audio in two or more channels to symbolically represented synthesis instructions. Audio signals are usually represented in terms of analogue or digital recordings. Digital recordings are samples of acoustic waveform or parameters of audio compression algorithms. One of the unique properties of musical signals is that they often combine different types of representations, such as graphical scores and sequences of performance actions that are encoded as MIDI files.\n\nSince audio signals usually comprise multiple sound sources, then unlike speech signals that can be efficiently described in terms of specific models (such as source-filter model), it is hard to devise a parametric representation for general audio. Parametric audio representations usually use filter banks or sinusoidal models to capture multiple sound parameters, sometimes increasing the representation size in order to capture internal structure in the signal. Additional types of data that are relevant for computer audition are textual descriptions of audio contents, such as annotations, reviews, and visual information in the case of audio-visual recordings.\n\nDescription of contents of general audio signals usually requires extraction of features that capture specific aspects of the audio signal. Generally speaking, one could divide the features into signal or mathematical descriptors such as energy, description of spectral shape etc., statistical characterization such as change or novelty detection, special representations that are better adapted to the nature of musical signals or the auditory system, such as logarithmic growth of sensitivity (bandwidth) in frequency or octave invariance (chroma).\n\nSince parametric models in audio usually require very many parameters, the features are used to summarize properties of multiple parameters in a more compact or salient representation.\n\nFinding specific musical structures is possible by using musical knowledge as well as supervised and unsupervised machine learning methods. Examples of this include detection of tonality according to distribution of frequencies that correspond to patterns of occurrence of notes in musical scales, distribution of note onset times for detection of beat structure, distribution of energies in different frequencies to detect musical chords and so on.\n\nComparison of sounds can be done by comparison of features with or without reference to time. In some cases an overall similarity can be assessed by close values of features between two sounds. In other cases when temporal structure is important, methods of dynamic time warping need to be applied to \"correct\" for different temporal scales of acoustic events. Finding repetitions and similar sub-sequences of sonic events is important for tasks such as texture synthesis and machine improvisation.\n\nSince one of the basic characteristics of general audio is that it comprises multiple simultaneously sounding sources, such as multiple musical instruments, people talking, machine noises or animal vocalization, the ability to identify and separate individual sources is very desirable. Unfortunately, there are no methods that can solve this problem in a robust fashion. Existing methods of source separation rely sometimes on correlation between different audio channels in multi-channel recordings. The ability to separate sources from stereo signals requires different techniques than those usually applied in communications where multiple sensors are available. Other source separation methods rely on training or clustering of features in mono recording, such as tracking harmonically related partials for multiple pitch detection.\n\nListening to music and general audio is commonly not a task directed activity. People enjoy music for various poorly understood reasons, which are commonly referred to the emotional effect of music due to creation of expectations and their realization or violation. Animals attend to signs of danger in sounds, which could be either specific or general notions of surprising and unexpected change. Generally, this creates a situation where computer audition can not rely solely on detection of specific features or sound properties and has to come up with general methods of adapting to changing auditory environment and monitoring its structure. This consists of analysis of larger repetition and self-similarity structures in audio to detect innovation, as well as ability to predict local feature dynamics.\n\nAmong the available data for describing music, there are textual representations, such as liner notes, reviews and criticisms that describe the audio contents in words. In other cases human reactions such as emotional judgements or psycho-physiological measurements might provide an insight into the contents and structure of audio. Computer Audition tries to find relation between these different representations in order to provide this additional understanding of the audio contents.\n\n",
    "id": "9536113",
    "title": "Computer audition"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=18402617",
    "text": "Artificial psychology\n\nArtificial psychology is a theoretical discipline proposed by Dan Curtis (b. 1963). The theory considers the situation when an artificial intelligence approaches the level of complexity where the intelligence meets two conditions:\n\nCondition I\n\nCondition II\n\nWhen both conditions are met, then, according to this theory, the possibility exists that the intelligence will reach irrational conclusions based on real or created information. At this point, the criteria is met for intervention which will not necessarily be resolved by simple re-coding of processes due to extraordinarily complex nature of the codebase itself; but rather a discussion with the intelligence in a format which more closely resembles classical (human) psychology.\n\nIf the intelligence cannot be reprogrammed by directly inputting new code, but requires the intelligence to reprogram itself through a process of analysis and decision based on information provided by a human, in order for it to overcome behavior which is inconsistent with the machines purpose or ability to function normally, then artificial psychology is by definition, what is required.\n\nThe level of complexity that is required before these thresholds are met is currently a subject of extensive debate. The theory of artificial psychology does not address the specifics of what those levels may be, but only that the level is sufficiently complex that the intelligence cannot simply be recoded by a software developer, and therefore dysfunctionality must be addressed through the same processes that humans must go through to address their own dysfunctionalities. Along the same lines, artificial psychology does not address the question of whether or not the intelligence is conscious.\n\nAs of 2015, the level of artificial intelligence does not approach any threshold where any of the theories or principles of artificial psychology can even be tested, and therefore, artificial psychology remains a largely theoretical discipline.\n",
    "id": "18402617",
    "title": "Artificial psychology"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=19317802",
    "text": "Decision list\n\nDecision lists are a representation for Boolean functions. Single term decision lists are more expressive than disjunctions and conjunctions; however, 1-term decision lists are less expressive than the general disjunctive normal form and the conjunctive normal form.\n\nThe language specified by a k-length decision list includes as a subset the language specified by a k-depth decision tree.\n\nLearning decision lists can be used for attribute efficient learning.\n\nA decision list (DL) of length is of the form:\n\nwhere is the th formula and is the th boolean for formula_1. The last if-then-else is the default case, which means formula is always equal to true. A -DL is a decision list where all of formulas have at most terms. Sometimes \"decision list\" is used to refer to a 1-DL, where all of the formulas are either a variable or its negation.\n",
    "id": "19317802",
    "title": "Decision list"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=3827197",
    "text": "Epistemic modal logic\n\nEpistemic modal logic is a subfield of modal logic that is concerned with reasoning about knowledge. While epistemology has a long philosophical tradition dating back to Ancient Greece, epistemic logic is a much more recent development with applications in many fields, including philosophy, theoretical computer science, artificial intelligence, economics and linguistics. While philosophers since Aristotle have discussed modal logic, and Medieval philosophers such as Ockham and Duns Scotus developed many of their observations, it was C. I. Lewis who created the first symbolic and systematic approach to the topic, in 1912. It continued to mature as a field, reaching its modern form in 1963 with the work of Kripke.\n\nMany papers were written in the 1950s that spoke of a logic of knowledge in passing, but it was Finnish philosopher von Wright's paper \"An Essay in Modal Logic\" from 1951 that is seen as a founding document. It was not until 1962 that another Finn, Hintikka, would write \"Knowledge and Belief\", the first book-length work to suggest using modalities to capture the semantics of knowledge rather than the alethic statements typically discussed in modal logic. This work laid much of the groundwork for the subject, but a great deal of research has taken place since that time. For example, epistemic logic has been combined recently with some ideas from dynamic logic to create dynamic epistemic logic, which can be used to specify and reason about information change and exchange of information in multi-agent systems. The seminal works in this field are by Plaza, Van Benthem, and Baltag, Moss, and Solecki.\n\nMost attempts at modeling knowledge have been based on the possible worlds model. In order to do this, we must divide the set of possible worlds between those that are compatible with an agent's knowledge, and those that are not. This generally conforms with common usage. If I know that it is either Friday or Saturday, then I know for sure that it is not Thursday. There is no possible world compatible with my knowledge where it is Thursday, since in all these worlds it is either Friday or Saturday. While we will primarily be discussing the logic-based approach to accomplishing this task, it is worthwhile to mention here the other primary method in use, the event-based approach. In this particular usage, events are sets of possible worlds, and knowledge is an operator on events. Though the strategies are closely related, there are two important distinctions to be made between them:\nTypically, the logic-based approach has been used in fields such as philosophy, logic and AI, while the event-based approach is more often used in fields such as game theory and mathematical economics. In the logic-based approach, a syntax and semantics have been built using the language of modal logic, which we will now describe.\n\nThe basic modal operator of epistemic logic, usually written \"K\", can be read as \"it is known that,\" \"it is epistemically necessary that,\" or \"it is inconsistent with what is known that not.\" If there is more than one agent whose knowledge is to be represented, subscripts can be attached to the operator (formula_1, formula_2, etc.) to indicate which agent one is talking about. So formula_3 can be read as \"Agent formula_4 knows that formula_5.\" Thus, epistemic logic can be an example of multimodal logic applied for knowledge representation. The dual of \"K\", which would be in the same relationship to \"K\" as formula_6 is to formula_7, has no specific symbol, but can be represented by formula_8, which can be read as \"formula_4 does not know that not formula_5\" or \"It is consistent with formula_4's knowledge that formula_5 is possible\". The statement \"formula_4 does not know whether or not formula_5\" can be expressed as formula_15.\n\nIn order to accommodate notions of common knowledge and distributed knowledge, three other modal operators can be added to the language. These are formula_16, which reads \"every agent in group G knows;\" formula_17, which reads \"it is common knowledge to every agent in G;\" and formula_18, which reads \"it is distributed knowledge to every agent in G.\" If formula_5 is a formula of our language, then so are formula_20, formula_21, and formula_22. Just as the subscript after formula_23 can be omitted when there is only one agent, the subscript after the modal operators formula_24, formula_25, and formula_26 can be omitted when the group is the set of all agents.\n\nAs we mentioned above, the logic-based approach is built upon the possible worlds model, the semantics of which are often given definite form in Kripke structures, also known as Kripke models. A Kripke structure \"M\" for \"n\" agents over formula_27 is a (n+2)-tuple formula_28, where S is a nonempty set of \"states\" or \"possible worlds\", formula_29 is an \"interpretation\", which associates with each state in S a truth assignment to the primitive propositions in formula_27, and formula_31 are binary relations on S for \"n\" numbers of agents. It is important here not to confuse formula_32, our modal operator, and formula_33, our accessibility relation.\n\nThe truth assignment tells us whether or not a proposition \"p\" is true or false in a certain state. So formula_34 tells us whether \"p\" is true in state \"s\" in model formula_35. Truth depends not only on the structure, but on the current world as well. Just because something is true in one world does not mean it is true in another. To state that a formula formula_5 is true at a certain world, one writes formula_37, normally read as \"formula_5 is true at (M,s),\" or \"(M,s) satisfies formula_5\".\n\nIt is useful to think of our binary relation formula_33 as a \"possibility\" relation, because it is meant to capture what worlds or states agent \"i\" considers to be possible. In idealized accounts of knowledge (e.g., describing the epistemic status of perfect reasoners with infinite memory capacity), it makes sense for formula_33 to be an equivalence relation, since this is the strongest form and is the most appropriate for the greatest number of applications. An equivalence relation is a binary relation that is reflexive, symmetric, and transitive. The accessibility relation does not have to have these qualities; there are certainly other choices possible, such as those used when modeling belief rather than knowledge.\n\nAssuming that formula_33 is an equivalence relation, and that the agents are perfect reasoners, a few properties of knowledge can be derived. The properties listed here are often known as the \"S5 Properties,\" for reasons described in the Axiom Systems section below.\n\nThis axiom is traditionally known as K. In epistemic terms, it states that if an agent knows formula_5 and knows that formula_44, then the agent must also know formula_45. So,\n\nAnother property we can derive is that if formula_47 is valid, then formula_48. This does not mean that if formula_47 is true, then agent i knows formula_47. What it means is that if formula_47 is true in every world that an agent considers to be a possible world, then the agent must know formula_47 at every possible world. This principle is traditionally called N.\n\nThis axiom is also known as T. It says that if an agent knows facts, the facts must be true. This has often been taken as the major distinguishing feature between knowledge and belief. We can believe a statement to be true when it is false, but it would be impossible to \"know\" a false statement.\n\nThis property and the next state that an agent has introspection about its own knowledge, and are traditionally known as 4 and 5, respectively. The Positive Introspection Axiom, also known as the KK Axiom, says specifically that agents \"know that they know what they know\". This axiom may seem less obvious than the ones listed previously, and Timothy Williamson has argued against its inclusion forcefully in his book, \"Knowledge and Its Limits\".\n\nThe Negative Introspection Axiom says that agents \"know that they do not know what they do not know\".\n\nDifferent modal logics can be derived from taking different subsets of these axioms, and these logics are normally named after the important axioms being employed. However, this is not always the case. KT45, the modal logic that results from the combining of K, T, 4, 5, and the Knowledge Generalization Rule, is primarily known as S5. This is why the properties of knowledge described above are often called the S5 Properties.\n\nEpistemic logic also deals with belief, not just knowledge. The basic modal operator is usually written \"B\" instead of \"K\". In this case though, the knowledge axiom no longer seems right—agents only sometimes believe the truth—so it is usually replaced with the Consistency Axiom, traditionally called D:\n\nwhich states that the agent does not believe a contradiction, or that which is false. When D replaces T in S5, the resulting system is known as KD45. This results in different properties for formula_33 as well. For example, in a system where an agent \"believes\" something to be true, but it is not actually true, the accessibility relation would be non-reflexive. The logic of belief is called doxastic logic.\n\nThe notion of knowledge discussed does not take into account computational constraints on inference. If we take the possible worlds approach to knowledge, it follows that our epistemic agent \"a\" knows all the logical consequences of his or her or its beliefs. If formula_59 is a logical consequence of formula_60, then there is no possible world where formula_60 is true but formula_59 is not. So if \"a\" knows that formula_60, it follows that all of the logical consequences of formula_60 are true of all of the possible worlds compatible with \"a\" 's beliefs. Therefore, \"a\" knows formula_59. It is not epistemically possible for \"a\" that not-formula_59 given his knowledge that formula_60. This consideration was a part of what led Robert Stalnaker to develop two dimensionalism, which can arguably explain how we might not know all the logical consequences of our beliefs even if there are no worlds where the propositions we know come out true but their consequences false.\n\nEven when we ignore possible world semantics and stick to axiomatic systems, this peculiar feature holds. With K and N (the Distribution Rule and the Knowledge Generalization Rule, respectively), which are axioms that are minimally true of all normal modal logics, we can prove that we know all the logical consequences of our beliefs. If formula_59 is a logical consequence of formula_60, then we can derive formula_70 with N and the conditional proof and then formula_71 with K. When we translate this into epistemic terms, this says that if formula_59 is a logical consequence of formula_60, then \"a\" knows that it is, and if \"a\" knows formula_60, \"a\" knows formula_59. That is to say, \"a\" knows all the logical consequences of every proposition. This is necessarily true of all classical modal logics. But then, for example, if \"a\" knows that prime numbers are divisible only by themselves and the number one, then \"a\" knows that 8683317618811886495518194401279999999 is prime (since this number is only divisible by itself and the number one). That is to say, under the modal interpretation of knowledge, when \"a\" knows the definition of a prime number, \"a\" knows that this number is prime. It should be clear at this point that \"a\" is not human. This shows that epistemic modal logic is an idealized account of knowledge, and explains objective, rather than subjective knowledge (if anything).\n\n\n\n",
    "id": "3827197",
    "title": "Epistemic modal logic"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=19208664",
    "text": "Neural modeling fields\n\nNeural modeling field (NMF) is a mathematical framework for machine learning which combines ideas from neural networks, fuzzy logic, and model based recognition. It has also been referred to as modeling fields, modeling fields theory (MFT), Maximum likelihood artificial neural networks (MLANS).\n\nThis framework has been developed by Leonid Perlovsky at the AFRL. NMF is interpreted as a mathematical description of mind’s mechanisms, including concepts, emotions, instincts, imagination, thinking, and understanding. NMF is a multi-level, hetero-hierarchical system. At each level in NMF there are concept-models encapsulating the knowledge; they generate so-called top-down signals, interacting with input, bottom-up signals. These interactions are governed by dynamic equations, which drive concept-model learning, adaptation, and formation of new concept-models for better correspondence to the input, bottom-up signals.\n\nIn the general case, NMF system consists of multiple processing levels. At each level, output signals are the concepts recognized in (or formed from) input, bottom-up signals. Input signals are associated with (or recognized, or grouped into) concepts according to the models and at this level. In the process of learning the concept-models are adapted for better representation of the input signals so that similarity between the concept-models and signals increases. This increase in similarity can be interpreted as satisfaction of an instinct for knowledge, and is felt as aesthetic emotions.\n\nEach hierarchical level consists of N \"neurons\" enumerated by index n=1,2..N. These neurons receive input, bottom-up signals, X(n), from lower levels in the processing hierarchy. X(n) is a field of bottom-up neuronal synaptic activations, coming from neurons at a lower level. Each neuron has a number of synapses; for generality, each neuron activation is described as a set of numbers, \n\n, where D is the number or dimensions necessary to describe individual neuron's activation. \n\nTop-down, or priming signals to these neurons are sent by concept-models, M(S,n) \n\n, where M is the number of models. Each model is characterized by its parameters, S; in the neuron structure of the brain they are encoded by strength of synaptic connections, mathematically, they are given by a set of numbers, \n\n, where A is the number of dimensions necessary to describe invividual model.\n\nModels represent signals in the following way. Suppose that signal X(\"n\") is coming from sensory neurons n activated by object m, which is characterized by parameters S. These parameters may include position, orientation, or lighting of an object m. Model M(S,n) predicts a value X(n) of a signal at neuron n. For example, during visual perception, a neuron n in the visual cortex receives a signal X(n) from retina and a priming signal M(S,n) from an object-concept-model \"m\". Neuron \"n\" is activated if both the bottom-up signal from lower-level-input and the top-down priming signal are strong. Various models compete for evidence in the bottom-up signals, while adapting their parameters for better match as described below. This is a simplified description of perception. The most benign everyday visual perception uses many levels from retina to object perception. The NMF premise is that the same laws describe the basic interaction dynamics at each level. Perception of minute features, or everyday objects, or cognition of complex abstract concepts is due to the same mechanism described below. Perception and cognition involve concept-models and learning. In perception, concept-models correspond to objects; in cognition models correspond to relationships and situations.\n\nLearning is an essential part of perception and cognition, and in NMF theory it is driven by the dynamics that increase a similarity measure between the sets of models and signals, L({X},{M}). The similarity measure is a function of model parameters and associations between the input bottom-up signals and top-down, concept-model signals. In constructing a mathematical description of the similarity measure, it is important to acknowledge two principles:\n\nTherefore, the similarity measure is constructed so that it accounts for all bottom-up signals, \"X\"(\"n\"),\n\nThis expression contains a product of partial similarities, l(X(n)), over all bottom-up signals; therefore it forces the NMF system to account for every signal (even if one term in the product is zero, the product is zero, the similarity is low and the knowledge instinct is not satisfied); this is a reflection of the first principle. Second, before perception occurs, the mind does not know which object gave rise to a signal from a particular retinal neuron. Therefore a partial similarity measure is constructed so that it treats each model as an alternative (a sum over concept-models) for each input neuron signal. Its constituent elements are conditional partial similarities between signal X(n) and model M, l(X(n)|m). This measure is “conditional” on object m being present, therefore, when combining these quantities into the overall similarity measure, L, they are multiplied by r(m), which represent a probabilistic measure of object m actually being present. Combining these elements with the two principles noted above, a similarity measure is constructed as follows: \n\nThe structure of the expression above follows standard principles of the probability theory: a summation is taken over alternatives, m, and various pieces of evidence, n, are multiplied. This expression is not necessarily a probability, but it has a probabilistic structure. If learning is successful, it approximates probabilistic description and leads to near-optimal Bayesian decisions. The name “conditional partial similarity” for l(X(n)|m) (or simply l(n|m)) follows the probabilistic terminology. If learning is successful, l(n|m) becomes a conditional probability density function, a probabilistic measure that signal in neuron n originated from object m. Then L is a total likelihood of observing signals {X(n)} coming from objects described by concept-model {M}. Coefficients r(m), called priors in probability theory, contain preliminary biases or expectations, expected objects m have relatively high r(m) values; their true values are usually unknown and should be learned, like other parameters S.\n\nNote that in probability theory, a product of probabilities usually assumes that evidence is independent. Expression for L contains a product over n, but it does not assume independence among various signals X(n). There is a dependence among signals due to concept-models: each model M(S,n) predicts expected signal values in many neurons n. \n\nDuring the learning process, concept-models are constantly modified. Usually, the functional forms of models, M(S,n), are all fixed and learning-adaptation involves only model parameters, S. From time to time a system forms a new concept, while retaining an old one as well; alternatively, old concepts are sometimes merged or eliminated. This requires a modification of the similarity measure L; The reason is that more models always result in a better fit between the models and data. This is a well known problem, it is addressed by reducing similarity L using a “skeptic penalty function,” (Penalty method) p(N,M) that grows with the number of models M, and this growth is steeper for a smaller amount of data N. For example, an asymptotically unbiased maximum likelihood estimation leads to multiplicative p(N,M) = exp(-N/2), where N is a total number of adaptive parameters in all models (this penalty function is known as Akaike information criterion, see (Perlovsky 2001) for further discussion and references).\n\nThe learning process consists of estimating model parameters S and associating signals with concepts by maximizing the similarity L. Note that all possible combinations of signals and models are accounted for in expression (2) for L. This can be seen by expanding a sum and multiplying all the terms resulting in M items, a huge number. This is the number of combinations between all signals (N) and all models (M). This is the source of Combinatorial Complexity, which is solved in NMF by utilizing the idea of dynamic logic. An important aspect of dynamic logic is \"matching vagueness or fuzziness of similarity measures to the uncertainty of models\". Initially, parameter values are not known, and uncertainty of models is high; so is the fuzziness of the similarity measures. In the process of learning, models become more accurate, and the similarity measure more crisp, the value of the similarity increases. \n\nThe maximization of similarity L is done as follows. First, the unknown parameters {S} are randomly initialized. Then the association variables f(m|n) are computed,\n\nEquation for f(m|n) looks like the Bayes formula for a posteriori probabilities; if l(n|m) in the result of learning become conditional likelihoods, f(m|n) become Bayesian probabilities for signal n originating from object m. The dynamic logic of the NMF is defined as follows:\n\nThe following theorem has been proved (Perlovsky 2001):\n\n\"Theorem\". Equations (3), (4), and (5) define a convergent dynamic NMF system with stationary states defined by max{S}L.\n\nIt follows that the stationary states of an MF system are the maximum similarity states. When partial similarities are specified as probability density functions (pdf), or likelihoods, the stationary values of parameters {S} are asymptotically unbiased and efficient estimates of these parameters. The computational complexity of dynamic logic is linear in N. \n\nPractically, when solving the equations through successive iterations, f(m|n) can be recomputed at every iteration using (3), as opposed to incremental formula (5).\n\nThe proof of the above theorem contains a proof that similarity L increases at each iteration. This has a psychological interpretation that the instinct for increasing knowledge is satisfied at each step, resulting in the positive emotions: NMF-dynamic logic system emotionally enjoys learning.\n\nFinding patterns below noise can be an exceedingly complex problem. If an exact pattern shape is not known and depends on unknown parameters, these parameters should be found by fitting the pattern model to the data. However, when the locations and orientations of patterns are not known, it is not clear which subset of the data points should be selected for fitting. A standard approach for solving this kind of problem is multiple hypothesis testing (Singer et al. 1974). Since all combinations of subsets and models are exhaustively searched, this method faces the problem of combinatorial complexity. In the current example, noisy ‘smile’ and ‘frown’ patterns are sought. They are shown in Fig.1a without noise, and in Fig.1b with the noise, as actually measured. The true number of patterns is 3, which is not known. Therefore, at least 4 patterns should be fit to the data, to decide that 3 patterns fit best. The image size in this example is 100x100 = 10,000 points. If one attempts to fit 4 models to all subsets of 10,000 data points, computation of complexity, M ~ 10. An alternative computation by searching through the parameter space, yields lower complexity: each pattern is characterized by a 3-parameter parabolic shape. Fitting 4x3=12 parameters to 100x100 grid by a brute-force testing would take about 10 to 10 operations, still a prohibitive computational complexity.\nTo apply NMF and dynamic logic to this problem one needs to develop parametric adaptive models of expected patterns. The models and conditional partial similarities for this case are described in details in: a uniform model for noise, Gaussian blobs for highly-fuzzy, poorly resolved patterns, and parabolic models for ‘smiles’ and ‘frowns’. The number of computer operations in this example was about 10. Thus, a problem that was not solvable due to combinatorial complexity becomes solvable using dynamic logic.\n\nDuring an adaptation process, initially fuzzy and uncertain models are associated with structures in the input signals, and fuzzy models become more definite and crisp with successive iterations. The type, shape, and number, of models are selected so that the internal representation within the system is similar to input signals: the NMF concept-models represent structure-objects in the signals. The figure below illustrates operations of dynamic logic. In Fig. 1(a) true ‘smile’ and ‘frown’ patterns are shown without noise; (b) actual image available for recognition (signal is below noise, signal-to-noise ratio is between –2dB and –0.7dB); (c) an initial fuzzy model, a large fuzziness corresponds to uncertainty of knowledge; (d) through (m) show improved models at various iteration stages (total of 22 iterations). Every five iterations the algorithm tried to increase or decrease the number of models. Between iterations (d) and (e) the algorithm decided, that it needs three Gaussian models for the ‘best’ fit. \n\nThere are several types of models: one uniform model describing noise (it is not shown) and a variable number of blob models and parabolic models; their number, location, and curvature are estimated from the data. Until about stage (g) the algorithm used simple blob models, at (g) and beyond, the algorithm decided that it needs more complex parabolic models to describe the data. Iterations stopped at (h), when similarity stopped increasing.\n\nAbove, a single processing level in a hierarchical NMF system was described. At each level of hierarchy there are input signals from lower levels, models, similarity measures (L), emotions, which are defined as changes in similarity, and actions; actions include adaptation, behavior satisfying the knowledge instinct – maximization of similarity. An input to each level is a set of signals X(n), or in neural terminology, an input field of neuronal activations. The result of signal processing at a given level are activated models, or concepts m recognized in the input signals n; these models along with the corresponding instinctual signals and emotions may activate behavioral models and generate behavior at this level.\n\nThe activated models initiate other actions. They serve as input signals to the next processing level, where more general concept-models are recognized or created. Output signals from a given level, serving as input to the next level, are the model activation signals, a, defined as\n\na = ∑ f(m|n). \n\nThe hierarchical NMF system is illustrated in Fig. 2. Within the hierarchy of the mind, each concept-model finds its “mental” meaning and purpose at a higher level (in addition to other purposes). For example, consider a concept-model “chair.” It has a “behavioral” purpose of initiating sitting behavior (if sitting is required by the body), this is the “bodily” purpose at the same hierarchical level. In addition, it has a “purely mental” purpose at a higher level in the hierarchy, a purpose of helping to recognize a more general concept, say of a “concert hall,” a model of which contains rows of chairs. \n\nFrom time to time a system forms a new concept or eliminates an old one. At every level, the NMF system always keeps a reserve of vague (fuzzy) inactive concept-models. They are inactive in that their parameters are not adapted to the data; therefore their similarities to signals are low. Yet, because of a large vagueness (covariance) the similarities are not exactly zero. When a new signal does not fit well into any of the active models, its similarities to inactive models automatically increase (because first, every piece of data is accounted for, and second, inactive models are vague-fuzzy and potentially can “grab” every signal that does not fit into more specific, less fuzzy, active models. When the activation signal a for an inactive model, m, exceeds a certain threshold, the model is activated. Similarly, when an activation signal for a particular model falls below a threshold, the model is deactivated. Thresholds for activation and deactivation are set usually based on information existing at a higher hierarchical level (prior information, system resources, numbers of activated models of various types, etc.). Activation signals for active models at a particular level { a } form a “neuronal field,” which serve as input signals to the next level, where more abstract and more general concepts are formed.\n\n",
    "id": "19208664",
    "title": "Neural modeling fields"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=6216524",
    "text": "SNePS\n\nSNePS is a knowledge representation, reasoning, and acting (KRRA) system developed and maintained by Stuart C. Shapiro and colleagues at the State University of New York at Buffalo.\n\nSNePS is simultaneously a logic-based, frame-based, and network-based KRRA system. It uses an assertional model of knowledge, in that a SNePS knowledge base (KB) consists of a set of assertions (propositions) about various entities. Its intended model is of an intensional domain of mental entities---the entities conceived of by some agent, and the propositions believed by it. The intensionality is primarily accomplished by the absence of a built-in equality operator, since any two syntactically different terms might have slightly different Fregean senses.\n\nSNePS has three styles of inference: formula-based, derived from its logic-based personality; slot-based, derived from its frame-based personality; and path-based, derived from its network-based personality. However, all three are integrated, operating together.\n\nSNePS may be used as a stand-alone KRR system. It has also been used, along with its integrated acting component, to implement the mind of intelligent agents (cognitive robots), in accord with the GLAIR agent architecture (a layered cognitive architecture). The SNePS Research Group often calls its agents Cassie.\n\nAs a logic-based system, a SNePS KB consists of a set of terms, and functions and formulas over those terms. The set of logical connectives and quantifiers extends the usual set used by first-order logics, all taking one or more arbitrarily-sized sets of arguments. In accord with the intended use of SNePS to represent the mind of a natural-language-competent intelligent agent, propositions are first-class entities of the intended domain, so formulas are actually proposition-denoting functional terms. SNePSLOG, the input-output language of the logic-based face of SNePS, looks like a naive logic in that function symbols (including \"predicates\"), and formulas (actually proposition-denoting terms) may be the arguments of functions and may be quantified over. The underlying SNePS, however, is a first order logic, with the user's function symbols and formulas reified.\n\nFormula-based inference is implemented as a natural-deduction-style inference engine in which there are introduction and elimination rules for the connectives and quantifiers. SNePS formula-based inference is sound but not complete, as rules of inference that are less useful for natural language understanding and commonsense reasoning have not been implemented.\n\nA proposition-denoting term in a SNePS KB might or might not be \"asserted\", that is, treated as true in the KB. The SNePS logic is a paraconsistent version of relevance logic, so that a contradiction does not imply anything whatsoever. Nevertheless, SNeBR, the SNePS Belief Revision subsystem, will notice any explicit contradiction and engage the user in a dialogue to repair it. SNeBR is an Assumption-Based Truth Maintenance System (ATMS), and removes the assertion status of any proposition whose support has been removed.\n\nAs a frame-based system, every SNePS functional term (including proposition-valued terms) is represented by a frame with slots and fillers. Each slot may be filled by an arbitrarily-sized set of other terms. However, cycles cannot be constructed. SNePSUL, the SNePS User Language is an input-output language for interacting with SNePS in its guise as a frame-based system.\n\nSNePSLOG may be used in any of three modes. In two modes, the caseframe (set of slots) associated with each functional term is determined by the system. In mode 3, the user declares what caseframe is to be used for each function symbol.\n\nIn slot-based inference, any proposition-valued frame is considered to imply the frame with any of its slots filled by a subset of its fillers. In the current implementation, this is not always sound.\n\nAs a network-based system, SNePS is a propositional semantic network,\nthus the original meaning of \"SNePS\" as \"The Semantic Network\nProcessing System\". This view is obtained by considering every\nindividual constant and every functional term to be a node of the\nnetwork, and every slot to be a directed labeled arc from the\nframe-node it is in to every node in its filler. In the intended\ninterpretation, every node denotes a mental entity, some of which are\npropositions, and every proposition represented in the network is\nrepresented by the node that denotes it. Some nodes are variables of\nthe SNePS logic, and they range over nodes, and only over nodes.\n\nPath-based inference rules may be defined, although they, themselves,\nare not represented in SNePS. A path-based inference rule specifies\nthat some labeled arc \"r\" may be inferred as present from some node \"n\"\nto some other node \"m\" just in case a given path exists from \"n\" to \"m\".\nThere is an extensive recursive set of path constructors available.\n\n\nSNePS has been used for a variety of KRR tasks, for natural language\nunderstanding and generation, for commonsense reasoning, and for\ncognitive robotics. It has been used in several KR courses around the\nworld.\n\nSNePS is implemented as a platform-independent system in Common Lisp and is freely available.\n\n",
    "id": "6216524",
    "title": "SNePS"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=568967",
    "text": "Backward chaining\n\nBackward chaining (or backward reasoning) is an inference method that can be described colloquially as working backward from the goal(s). It is used in automated theorem provers, inference engines, proof assistants and other artificial intelligence applications.\n\nIn game theory, its application to (simpler) subgames in order to find a solution to the game is called backward induction. In chess, it is called retrograde analysis, and it is used to generate tablebases for chess endgames for computer chess.\n\nBackward chaining is implemented in logic programming by SLD resolution. Both rules are based on the modus ponens inference rule. It is one of the two most commonly used methods of reasoning with inference rules and logical implications – the other is forward chaining. Backward chaining systems usually employ a depth-first search strategy, e.g. Prolog.\n\nBackward chaining starts with a list of goals (or a hypothesis) and works backwards from the consequent to the antecedent to see if there is data available that will support any of these consequents. An inference engine using backward chaining would search the inference rules until it finds one which has a consequent (Then clause) that matches a desired goal. If the antecedent (If clause) of that rule is not known to be true, then it is added to the list of goals (in order for one's goal to be confirmed one must also provide data that confirms this new rule).\n\nFor example, suppose a new pet, Fritz, is delivered in an opaque box along with two facts about Fritz:\nThe goal is to decide whether Fritz is green, based on a rule base containing the following four rules:\n\n\nWith backward reasoning, an inference engine can determine whether Fritz is green in four steps. To start, the query is phrased as a goal assertion that is to be proved: \"Fritz is green\".\n\n1. Fritz is substituted for X in rule #3 to see if its consequent matches the goal, so rule #3 becomes:\nSince the consequent matches the goal (\"Fritz is green\"), the rules engine now needs to see if the antecedent (\"If Fritz is a frog\") can be proved. The antecedent therefore becomes the new goal:\n\n2. Again substituting Fritz for X, rule #1 becomes:\nSince the consequent matches the current goal (\"Fritz is a frog\"), the inference engine now needs to see if the antecedent (\"If Fritz croaks and eats flies\") can be proved. The antecedent therefore becomes the new goal:\n\n3. Since this goal is a conjunction of two statements, the inference engine breaks it into two sub-goals, both of which must be proved:\n\n4. To prove both of these sub-goals, the inference engine sees that both of these sub-goals were given as initial facts. Therefore, the conjunction is true:\ntherefore the antecedent of rule #1 is true and the consequent must be true:\ntherefore the antecedent of rule #3 is true and the consequent must be true:\n\nThis derivation therefore allows the inference engine to prove that Fritz is green. Rules #2 and #4 were not used.\n\nNote that the goals always match the affirmed versions of the consequents of implications (and not the negated versions as in modus tollens) and even then, their antecedents are then considered as the new goals (and not the conclusions as in affirming the consequent) which ultimately must match known facts (usually defined as consequents whose antecedents are always true); thus, the inference rule which is used is modus ponens.\n\nBecause the list of goals determines which rules are selected and used, this method is called goal-driven, in contrast to data-driven forward-chaining inference. The backward chaining approach is often employed by expert systems.\n\nProgramming languages such as Prolog, Knowledge Machine and ECLiPSe support backward chaining within their inference engines.\n\n\n",
    "id": "568967",
    "title": "Backward chaining"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=290634",
    "text": "Chess as mental training\n\nThere are efforts to use the game of chess as a tool to aid the intellectual development of young people. Chess is significant in cognitive psychology and artificial intelligence (AI) studies, because it represents the domain in which expert performance has been most intensively studied and measured.\n\nAlthough the results of research studies have failed to produce unambiguous support for the intellectual benefits of playing chess, several local government, schools, and student organizations all over the world are implementing chess programs. \n\nNew York based Chess-In-The-Schools, Inc. has been active in the public school system in the city since 1986. It currently reaches more than 30,000 students annually. America's Foundation for Chess has initiated programs in partnership with local school districts in several U.S. cities, including Seattle, San Diego, Philadelphia, and Tampa. The Chess'n Math Association promotes chess at the scholastic level in Canada. Chess for Success is a program for at-risk schools in Oregon. Since 1991, the U.S. Chess Center in Washington, D.C. teaches chess to children, especially those in the inner city, \"as a means of improving their academic and social skills.\"\n\nThere are a number of experiments that suggest that learning and playing chess aids the mind. The Grandmaster Eugene Torre Chess Institute in the Philippines, the United States Chess Federation's chess research bibliography, and English educational consultant Tony Buzan's Brain Foundation, among others, continuously collect such experimental results. The advent of chess software that automatically record and analyze the moves of each player in each game and can tirelessly play with human players of various levels, further helped in giving new directions to experimental designs on chess as mental training.\n\nAs early as 1779 Benjamin Franklin, in his article \"The morals of chess\", has advocated such a view:\n\"The Game of Chess is not merely an idle amusement; several very valuable qualities of the mind, useful in the course of human life, are to be acquired and strengthened by it, so as to become habits ready on all occasions; for life is a kind of Chess, in which we have often points to gain, and competitors or adversaries to contend with, and in which there is a vast variety of good and ill events, that are, in some degree, the effect of prudence, or the want of it. By playing at Chess then, we may learn:\n\n1st, Foresight, which looks a little into futurity, and considers the consequences that may attend an action ...\n\n2nd, Circumspection, which surveys the whole Chess-board, or scene of action: - the relation of the several Pieces, and their situations; ...\n\n3rd, Caution, not to make our moves too hastily...\"\nAlfred Binet demonstrated in the late 19th century that good chess players have superior memory and imagination. Adriaan de Groot concurred with Alfred Binet that visual memory and visual perception are important attributors and that problem-solving ability is of paramount importance. Thus, since 1972, at the collegiate level, the University of Texas at Dallas and the University of Maryland, Baltimore County both recruit chessplayer-scholars and run scholastic outreach programs in their respective communities.\n\n",
    "id": "290634",
    "title": "Chess as mental training"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=21171254",
    "text": "Type-2 fuzzy sets and systems\n\nType-2 fuzzy sets and systems generalize standard Type-1 fuzzy sets and systems so that more uncertainty can be handled. From the very beginning of fuzzy sets, criticism was made about the fact that the membership function of a type-1 fuzzy set has no uncertainty associated with it, something that seems to contradict the word \"fuzzy\", since that word has the connotation of lots of uncertainty. So, what does one do when there is uncertainty about the value of the membership function? The answer to this question was provided in 1975 by the inventor of fuzzy sets, Prof. Lotfi A. Zadeh, when he proposed more sophisticated kinds of fuzzy sets, the first of which he called a type-2 fuzzy set. A type-2 fuzzy set lets us incorporate uncertainty about the membership function into fuzzy set theory, and is a way to address the above criticism of type-1 fuzzy sets head-on. And, if there is no uncertainty, then a type-2 fuzzy set reduces to a type-1 fuzzy set, which is analogous to probability reducing to determinism when unpredictability vanishes.\n\nIn order to symbolically distinguish between a type-1 fuzzy set and a type-2 fuzzy set, a tilde symbol is put over the symbol for the fuzzy set; so, A denotes a type-1 fuzzy set, whereas Ã denotes the comparable type-2 fuzzy set. When the latter is done, the resulting type-2 fuzzy set is called a general type-2 fuzzy set (to distinguish it from the special interval type-2 fuzzy set).\n\nProf. Zadeh didn't stop with type-2 fuzzy sets, because in that 1976 paper he also generalized all of this to type-\"n\" fuzzy sets. The present article focuses only on type-2 fuzzy sets because they are the \"next step\" in the logical progression from type-1 to type-\"n\" fuzzy sets, where \"n\" = 1, 2, … . Although some researchers are beginning to explore higher than type-2 fuzzy sets, as of early 2009, this work is in its infancy.\nThe membership function of a general type-2 fuzzy set, Ã, is three-dimensional (Fig. 1), where the third dimension is the value of the membership function at each point on its two-dimensional domain that is called its footprint of uncertainty (FOU).\n\nFor an interval type-2 fuzzy set that third-dimension value is the same (e.g., 1) everywhere, which means that no new information is contained in the third dimension of an interval type-2 fuzzy set. So, for such a set, the third dimension is ignored, and only the FOU is used to describe it. It is for this reason that an interval type-2 fuzzy set is sometimes called a \"first-order uncertainty\" fuzzy set model, whereas a general type-2 fuzzy set (with its useful third-dimension) is sometimes referred to as a \"second-order uncertainty\" fuzzy set model.\nThe FOU represents the blurring of a type-1 membership function, and is completely described by its two bounding functions (Fig. 2), a lower membership function (LMF) and an upper membership function (UMF), both of which are type-1 fuzzy sets! Consequently, it is possible to use type-1 fuzzy set mathematics to characterize and work with interval type-2 fuzzy sets. This means that engineers and scientists who already know type-1 fuzzy sets will not have to invest a lot of time learning about general type-2 fuzzy set mathematics in order to understand and use interval type-2 fuzzy sets.\n\nWork on type-2 fuzzy sets languished during the 1980s and early-to-mid 1990's, although a small number of articles were published about them. People were still trying to figure out what to do with type-1 fuzzy sets, so even though Zadeh proposed type-2 fuzzy sets in 1976, the time was not right for researchers to drop what they were doing with type-1 fuzzy sets to focus on type-2 fuzzy sets. This changed in the latter part of the 1990s as a result of Prof. Jerry Mendel and his student's works on type-2 fuzzy sets and systems. Since then, more and more researchers around the world are writing articles about type-2 fuzzy sets and systems.\n\nInterval type-2 fuzzy sets have received the most attention because the mathematics that is needed for such sets—primarily Interval arithmetic—is much simpler than the mathematics that is needed for general type-2 fuzzy sets. So, the literature about interval type-2 fuzzy sets is large, whereas the literature about general type-2 fuzzy sets is much smaller. Both kinds of fuzzy sets are being actively researched by an ever-growing number of researchers around the world.\n\nFormilleri for the following have already been worked out for interval type-2 fuzzy sets:\n\n\nType-2 fuzzy sets are finding very wide applicability in rule-based fuzzy logic systems (FLSs) because they let uncertainties be modeled by them whereas such uncertainties cannot be modeled by type-1 fuzzy sets. A block diagram of a type-2 FLS is depicted in Fig. 3. This kind of FLS is used in fuzzy logic control, fuzzy logic signal processing, rule-based classification, etc., and is sometimes referred to as a \"function approximation\" application of fuzzy sets, because the FLS is designed to minimize an error function.\n\nThe following discussions, about the four components in the Fig. 3 rule-based FLS, are given for an interval type-2 FLS, because to-date they are the most popular kind of type-2 FLS; however, most of the discussions are also applicable for a general type-2 FLS.\n\nRules, that are either provided by subject experts or are extracted from numerical data, are expressed as a collection of IF-THEN statements, e.g.,\n\nFuzzy sets are associated with the terms that appear in the antecedents (IF-part) or consequents (THEN-part) of rules, and with the inputs to and the outputs of the FLS. Membership functions are used to describe these fuzzy sets, and in a type-1 FLS they are all type-1 fuzzy sets, whereas in an interval type-2 FLS at least one membership function is an interval type-2 fuzzy set.\n\nAn interval type-2 FLS lets any one or all of the following kinds of uncertainties be quantified:\n\n\nIn Fig. 3, measured (crisp) inputs are first transformed into fuzzy sets in the Fuzzifier block because it is fuzzy sets and not numbers that activate the rules which are described in terms of fuzzy sets and not numbers. Three kinds of fuzzifiers are possible in an interval type-2 FLS. When measurements are:\n\nIn Fig. 3, after measurements are fuzzified, the resulting input fuzzy sets are mapped into fuzzy output sets by the Inference block. This is accomplished by first quantifying each rule using fuzzy set theory, and by then using the mathematics of fuzzy sets to establish the output of each rule, with the help of an inference mechanism. If there are \"M\" rules then the fuzzy input sets to the Inference block will activate only a subset of those rules, where the subset contains at least one rule and usually way fewer than \"M\" rules. Inference is done one rule at a time. So, at the output of the Inference block, there will be one or more \"fired-rule fuzzy output sets\".\n\nIn most engineering applications of a FLS, a number (and not a fuzzy set) is needed as its final output, e.g., the consequent of the rule given above is \"Rotate the valve a bit to the right.\" No automatic valve will know what this means because \"a bit to the right\" is a linguistic expression, and a valve must be turned by numerical values, i.e. by a certain number of degrees. Consequently, the fired-rule output fuzzy sets have to be converted into a number, and this is done in the Fig. 3 Output Processing block.\n\nIn a type-1 FLS, output processing, called Defuzzification, maps a type-1 fuzzy set into a number. There are many ways for doing this, e.g., compute the union of the fired-rule output fuzzy sets (the result is another type-1 fuzzy set) and then compute the center of gravity of the membership function for that set; compute a weighted average of the center of gravities of each of the fired rule consequent membership functions; etc.\n\nThings are somewhat more complicated for an interval type-2 FLS, because to go from an interval type-2 fuzzy set to a number (usually) requires two steps (Fig. 3). The first step, called type-reduction, is where an interval type-2 fuzzy set is reduced to an interval-valued type-1 fuzzy set. There are as many type-reduction methods as there are type-1 defuzzification methods. An algorithm developed by Karnik and Mendel now known as the KM Algorithm is used for type-reduction. Although this algorithm is iterative, it is very fast.\n\nThe second step of Output Processing, which occurs after type-reduction, is still called defuzzification. Because a type-reduced set of an interval type-2 fuzzy set is always a finite interval of numbers, the defuzzified value is just the average of the two end-points of this interval.\n\nIt is clear from Fig. 3 that there can be two outputs to an interval type-2 FLS—crisp numerical values and the type-reduced set. The latter provides a measure of the uncertainties that have flowed through the interval type-2 FLS, due to the (possibly) uncertain input measurements that have activated rules whose antecedents or consequents or both are uncertain. Just as standard deviation is widely used in probability and statistics to provide a measure of unpredictable uncertainty about a mean value, the type-reduced set can provided a measure of uncertainty about the crisp output of an interval type-2 FLS.\n\nAnother application for fuzzy sets has also been inspired by Prof. Zadeh — Computing With Words. Different acronyms have been used for \"computing with words,\" e.g., CW and CWW. According to Zadeh:\nOf course, he did not mean that computers would actually compute using words—single words or phrases—rather than numbers. He meant that computers would be activated by words, which would be converted into a mathematical representation using fuzzy sets and that these fuzzy sets would be mapped by a CWW engine into some other fuzzy set after which the latter would be converted back into a word. A natural question to ask is: Which kind of fuzzy set—type-1 or type-2—should be used as a model for a word? Mendel has argued, on the basis of Karl Popper's concept of Falsificationism, that using a type-1 fuzzy set as a model for a word is scientifically incorrect. An interval type-2 fuzzy set should be used as a (first-order uncertainty) model for a word. Much research is under way about CWW.\n\nType-2 fuzzy sets were applied in image processing, video processing and computer vision, as well as Failure Mode And Effect Analysis.\n\n\n\nThere are two I\"EEE Expert Now\" multi-media modules that can be accessed from the IEEE at: http://www.ieee.org/web/education/Expert_Now_IEEE/Catalog/AI.html\n\nFreeware MATLAB implementations, which cover general and interval type-2 fuzzy sets and systems, as well as type-1 fuzzy systems, are available at: http://sipi.usc.edu/~mendel/software.\nSoftware supporting discrete interval type-2 fuzzy logic systems is available at:\nDIT2FLS Toolbox - http://dit2fls.com/projects/dit2fls-toolbox/\nDIT2FLS Library Package - http://dit2fls.com/projects/dit2fls-library-package/\n\nJava libraries including source code for type-1, interval- and general type-2 fuzzy systems are available at: http://juzzy.wagnerweb.net/.\n\nAn open source Matlab/Simulink Toolbox for Interval Type-2 Fuzzy Logic Systems is available at: http://web.itu.edu.tr/kumbasart/type2fuzzy.htm\n",
    "id": "21171254",
    "title": "Type-2 fuzzy sets and systems"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=21342168",
    "text": "Grammar systems theory\n\nGrammar systems theory is a field of theoretical computer science that studies systems of finite collections of formal grammars generating a formal language. Each grammar works on a string, a so-called \"sequential form\" that represents an environment. Grammar systems can thus be used as a formalization of decentralized or distributed systems of agents in artificial intelligence.\n\nLet formula_1 be a simple reactive agent moving on the table and trying not to fall down from the table with two reactions, \"t\" for turning and \"ƒ\" for moving forward. The set of possible behaviors of formula_1 can then be described as formal language\n\nwhere \"ƒ\" can be done maximally \"k\" times and \"t\" can be done maximally \"ℓ\" times considering the dimensions of the table.\n\nThe schema on the right describes such a system of grammars which shares a common string representing an environment. The shared sequential form is sequentially rewritten by each grammar, which can represent either a component or generally an agent.\n\nIf grammars communicate together and work on a shared sequential form, it is called a \"Cooperating Distributed (DC)\" grammar system. Shared sequential form is a similar concept to the blackboard approach in AI, which is inspired by an idea of experts solving some problem together while they share their proposals and ideas on a shared blackboard.\n\nEach grammar in a grammar system can also work on its own string and communicate with other grammars in a system by sending their sequential forms on request. Such a grammar system is then called a \"Parallel Communicating (PC)\" grammar system.\n\nPC and DC are inspired by distributed AI. If there is no communication between grammars, the system is close to the decentralized approaches in AI. These kinds of grammar systems are sometimes called \"colonies\" or \"Eco-Grammar\" systems, depending (besides others) on whether the environment is changing on its own (Eco-Grammar system) or not (colonies).\n\n",
    "id": "21342168",
    "title": "Grammar systems theory"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=21417543",
    "text": "Allen (robot)\n\nAllen was a robot introduced by Rodney Brooks and his team in the late 1980s, and was their first robot based on subsumption architecture. It had sonar distance and odometry on board, and used an offboard lisp machine to simulate subsumption architecture. It resembled a footstool on wheels.\n\nAllen used three layers of control which are implemented in subsumption architecture. \"\"The lowest layer of control makes sure that the robot does not come into contact with other objects.\"\" Due to this layer it could avoid static and dynamic obstacles, but it could not move. It sat in the middle of the room, waiting for obstruction. When the obstruction came, Allen ran away, avoiding collisions as it went. It used following internal representation, and every sonar return represented a repulsive force with, and inverse square drop off in strength. Direction of its move was obtained by sum of the repulsive forces (suitably thresholded). It possessed an additional reflex which halted it whenever it was moving forward, and something was directly in its path.\n\n\"\"The first level layer of control (second layer), when combined with zeroth, imbues the robot with the ability to wander around aimlessly without hitting obstacles.\"\" Owing to the second layer, Allen could randomly wander about every 10 seconds. It used simple heuristic, which was coupled with the instinct to shun barriers by vector addition. \"\"The summed vector suppressed the more primitive obstacle avoidance vector, but the obstacle avoidance behaviour still operated, having been subsumed by the new layer, in its account of the lower level's repulsive force. Additionally, the halt reflex of the lower level operated autonomously and unchanged.\"\"\n\nThe third layer made the robot try to explore. Allen could look for distant places (with its sonars), then tried to reach them. \"\"This layer monitored progress through odometry, generating a desired heading which suppressed the direction desired by the wander layer. The desired heading was then fed into a vector addition with the instinctive obstacle avoidance layer. The physical robot did not therefore remain true to the desires of the upper layer. The upper layer had to watch what happened in the world, through odometry, in order to understand what was really happening in the lower control layers, and send down correction signals.\"\"\n\n",
    "id": "21417543",
    "title": "Allen (robot)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=21428998",
    "text": "Intelligent word recognition\n\nIntelligent Word Recognition, or IWR, is the recognition of unconstrained handwritten words. IWR recognizes entire handwritten words or phrases instead of character-by-character, like its predecessor, Optical Character Recognition (OCR). IWR technology matches handwritten or printed words to a user-defined dictionary, significantly reducing character errors encountered in typical character-based recognition engines. \n\nNew technology on the market utilizes IWR, OCR, and ICR together, which opens many doors for the processing of documents, either constrained (hand printed or machine printed) or unconstrained (freeform cursive). IWR also eliminates a large percentage of the manual data entry of handwritten documents that, in the past, could only be keyed by a human, creating an automated workflow.\n\nWhen cursive handwriting is in play, for each word analyzed, the system breaks down the words into a sequence of graphemes, or subparts of letters. These various curves, shapes and lines make up letters and IWR considers these various shape and groupings in order to calculate a confidence value associated with the word in question.\n\nIWR is not meant to replace ICR and OCR engines which work well with printed data; however, IWR reduces the number of character errors associated with these engines, and it is ideal for processing real-world documents that contain mostly freeform, hard-to-recognize data, inherently unsuitable for them.\n\n\n",
    "id": "21428998",
    "title": "Intelligent word recognition"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=13494976",
    "text": "CALO\n\nCALO was an artificial intelligence project that attempted to integrate numerous AI technologies into a cognitive assistant. CALO is an acronym for \"Cognitive Assistant that Learns and Organizes\". The name was inspired by the Latin word \"calonis,\" which means \"soldier’s servant\". The project started in May 2003 and ran for five years, ending in 2008.\n\nThe CALO effort has had many major spin-offs, most notably the Siri intelligent software assistant that is now part of the Apple iOS since iOS 5, delivered in several phones and tablets; Social Kinetics, a social application that learned personalized intervention and treatment strategies for chronic disease patients, sold to RedBrick Health; the Trapit project, which is a web scraper and news aggregator that makes intelligent selections of web content based on user preferences; Tempo AI, a smart calendar; Desti, a personalized travel guide; and Kuato Studios, a game development startup.\n\nCALO was funded by the Defense Advanced Research Projects Agency (DARPA) under its Personalized Assistant that Learns (PAL) program. DARPA's five-year contract brought together over 300 researchers from 25 of the top university and commercial research institutions, with the goal of building a new generation of cognitive assistants that can reason, learn from experience, be told what to do, explain what they are doing, reflect on their experience, and respond robustly to surprise. SRI International was the lead integrator responsible for coordinating the effort to produce an assistant that can live with and learn from its users, provide value to them, and then pass a yearly evaluation that measures how well the system has learned to do its job.\n\nCALO assists its user with six high-level functions:\n\n\nEvery year, the CALO system, after living with its user for a period of time, is given an achievement-style test of 153 \"administration assistant\" questions, primarily focused on what it has learned about the user's life. Evaluators measure how well CALO's performance on these questions improves year-over-year, and how much of CALO's performance is due to \"learning in the wild\" (new knowledge, tasks, and inferences it has been able to acquire on its own, as opposed to function or knowledge hard-wired into the system by a developer).\n\nSRI International made a collection of successful machine learning and reasoning technologies developed in the PAL program, primarily from the CALO project, available online. The available technologies include both general-purpose learning methods along with more focused learning applications. The PAL software and related publications are available at the PAL Framework website.\n\nThe PAL capabilities have been modularized, packaged, and adapted to industry standards to facilitate their incorporation into target applications. Various infrastructure components and APIs are available to simplify interaction with the technologies. PAL capabilities were integrated into the US Army's CPOF command and control system and fielded to Iraq in 2010.\n\nThe available technologies were developed by research teams at SRI International, Carnegie Mellon University, the University of Massachusetts Amherst, the University of Rochester, the Institute for Human and Machine Cognition, Oregon State University, the University of Southern California, and Stanford University.\n\nIn the first four years of the project, CALO-funded research has resulted in more than five hundred publications across all fields of artificial intelligence. Here are several:\n\n",
    "id": "13494976",
    "title": "CALO"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=21577031",
    "text": "Artificial intelligence and law\n\nArtificial intelligence and law (AI and law) is a subfield of artificial intelligence (AI) mainly concerned with applications of AI to legal informatics problems and original research on those problems. It is also concerned to contribute in the other direction: to export tools and techniques developed in the context of legal problems to AI in general. For example, theories of legal decision making, especially models of argumentation, have contributed to knowledge representation and reasoning; models of social organization based on norms have contributed to multi-agent systems; reasoning with legal cases has contributed to case-based reasoning; and the need to store and retrieve large amounts of textual data has resulted in contributions to conceptual information retrieval and intelligent databases.\n\nAlthough Loevinger, Allen and Mehl anticipated several of the ideas that would become important in AI and Law, the first serious proposal for applying AI techniques to law is usually taken to be Buchanan and Headrick. Early work from this period includes Thorne McCarty's influential TAXMAN project in the USA and Ronald Stamper's LEGOL project in the UK. The former concerned the modeling of the majority and minority arguments in a US Tax law case (Eisner v Macomber), while the latter attempted to provide a formal model of the rules and regulations that govern an organization. Landmarks in the early 1980s include Carole Hafner's work on conceptual retrieval, Anne Gardner's work on contract law, Rissland's work on legal hypotheticals and the work at Imperial College, London on executable formalisations of legislation.\n\nEarly meetings of scholars included a one-off meeting at Swansea, the series of conferences organized by IDG in Florence and the workshops organised by Charles Walter at the University of Houston in 1984 and 1985. In 1987 a biennial conference, the International Conference on AI and Law (ICAIL), was instituted. This conference began to be seen as the main venue for publishing and the developing ideas within AI and Law, and it led to the foundation of the International Association for Artificial Intelligence and Law (IAAIL), to organize and convene subsequent ICAILs. This, in turn, led to the foundation of the Artificial Intelligence and Law Journal, first published in 1992. In Europe, the annual JURIX conferences (organised by the Jurix Foundation for Legal Knowledge Based Systems), began in 1988. Initially intended to bring together the Dutch-speaking (i.e. Dutch and Flemish) researchers, JURIX quickly developed into an international, primarily European, conference and since 2002 has regularly been held outside the Dutch speaking countries. Since 2007 the JURISIN workshops have been held in Japan under the auspices of the Japanese Society for Artificial Intelligence.\n\nToday, AI and law embrace a wide range of topics, including:\n\nFormal models of legal texts and legal reasoning have been used in AI and Law to clarify issues, to give a more precise understanding and to provide a basis for implementations. A variety of formalisms have been used, including propositional and predicate calculi; deontic, temporal and non-monotonic logics; and state transition diagrams. Prakken and Sartor give a detailed and authoritative review of the use of logic and argumentation in AI and Law, and has an excellent set of references.\n\nAn important role of formal models is to remove ambiguity. In fact, legislation abounds with ambiguity: because it is written in natural language there are no brackets and so the scope of connectives such as \"and\" and \"or\" can be unclear (legal drafters do not observe the mathematical conventions in this respect). \"Unless\" is also capable of several interpretations, and legal draftsman never write \"if and only if\", although this is often what they intend by \"if\". In perhaps the earliest use of logic to model law in AI and Law, Layman Allen advocated the use of propositional logic to resolve such syntactic ambiguities in a series of papers.\n\nIn the late 1970s and throughout the 1980s a significant strand on AI and Law work involved the production of executable models of legislation. Originating in the LEGOL work of Ronald Stamper the idea was to represent legislation using a formal language and to use this formalisation (typically with some sort of user interface to gather the facts of a particular case) as the basis for an expert system. This became popular, mainly using the Horn Clause subset of first order predicate calculus. In particular Sergot et al.'s representation of the British Nationality Act did much to popularise the approach. In fact, as later work showed, this was an untypically suitable piece of legislation on which to employ the approach: it was new, and so had not been amended, relatively simple and almost all of the concepts were non-technical. Later work, such as that on Supplementary Benefits, showed that larger, more complicated (containing many cross references, exceptions, counterfactuals, and deeming provisions), legislation which used many highly technical concepts (such as contribution conditions) and which had been the subject of many amendments produced a far less satisfactory final system. Some efforts were made to improve matters from a software engineering perspective, especially to handle problems such as cross reference, verification and frequent amendment. The use of hierarchical representations was suggested to address the first problem, and so-called isomorphic representation was intended to address the other two. As the 1990s developed this strand of work became largely absorbed in the development of formalisations of domain conceptualisations, (so-called ontologies), which became popular in AI following the work of Gruber. Early examples in AI and Law include Valente's functional ontology and the frame based ontologies of Visser and van Kralingen. Legal ontologies have since become the subject of regular workshops at AI and Law conferences and there are many examples ranging from generic top-level and core ontologies to very specific models of particular pieces of legislation.\n\nSince law comprises sets of norms, it is unsurprising that deontic logics have been tried as the formal basis for models of legislation. These, however, have not been widely adopted as the basis for expert systems, perhaps because expert systems are supposed to enforce the norms, whereas deontic logic becomes of real interest only when we need to consider violations of the norms. In law directed obligations, whereby an obligation is owed to another named individual are of particular interest, since violations of such obligations are often the basis of legal proceedings. There is also some interesting work combining deontic and action logics to explore normative positions.\n\nIn the context of multi-agent systems, norms have been modelled using state transition diagrams. Often, especially in the context of electronic institutions, the norms so described are regimented (i.e., cannot be violated), but in other systems violations are also handled, giving a more faithful reflection of real norms. For a good example of this approach see Modgil et al.\n\nLaw often concerns issues about time, both relating to the content, such as time periods and deadlines, and those relating to the law itself, such as commencement. Some attempts have been made to model these temporal logics using both computational formalisms such as the Event Calculus and temporal logics such as defeasible temporal logic.\n\nIn any consideration of the use of logic to model law it needs to be borne in mind that law is inherently non-monotonic, as is shown by the rights of appeal enshrined in all legal systems, and the way in which interpretations of the law change over time. Moreover, in the drafting of law exceptions abound, and, in the application of law, precedents are overturned as well as followed. In logic programming approaches, negation of failure is often used to handle non-monotonicity, but specific non-monotonic logics such as defeasible logic have also been used. Following the development of abstract argumentation, however, these concerns have been addressed through argumentation rather than through the use of non-monotonic logics.\n\n\n",
    "id": "21577031",
    "title": "Artificial intelligence and law"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=19768799",
    "text": "Rule-based system\n\nIn computer science, rule-based systems are used as a way to store and manipulate knowledge to interpret information in a useful way. They are often used in artificial intelligence applications and research.\n\nNormally, the term 'rule-based system' is applied to systems involving human-crafted or curated rule sets. Rule-based systems constructed using automatic rule inference, such as rule-based machine learning, are normally excluded from this system type. \n\nA classic example of a rule-based system is the domain-specific expert system that uses rules to make deductions or choices. For example, an expert system might help a doctor choose the correct diagnosis based on a cluster of symptoms, or select tactical moves to play a game.\n\nRule-based systems can be used to perform lexical analysis to compile or interpret computer programs, or in natural language processing.\n\nRule-based programming attempts to derive execution instructions from a starting set of data and rules. This is a more indirect method than that employed by an imperative programming language, which lists execution steps sequentially.\n\nA typical rule-based system has four basic components:\n\n\n\n",
    "id": "19768799",
    "title": "Rule-based system"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=11230690",
    "text": "Autognostics\n\nAutognostics is a new paradigm that describes the capacity for computer networks to be self-aware. It is considered as one of the major components of Autonomic Networking.\n\nOne of the most important characteristics of today’s Internet that has contributed to its success is its basic design principle: a simple and transparent core with intelligence at the edges (the so-called \"end-to-end principle\"). Based on this principle, the network carries data without knowing the characteristics of that data (e.g., voice, video, etc.) - only the end-points have application-specific knowledge. If something goes wrong with the data, only the edge may be able to recognize that since it knows about the application and what the expected behavior is. The core has no information about what should happen with that data - it only forwards packets.\n\nAlthough an effective and beneficial attribute, this design principle has also led to many of today’s problems, limitations, and frustrations. Currently, it is almost impossible for most end-users to know why certain network-based applications do not work well and what they need to do to make it better. Also, network operators who interact with the core in low-level terms such as router configuration have problems expressing their high-level goals into low-level actions. In high-level terms, this may be summarized as a weak coupling between the network and application layers of the overall system.\n\nAs a consequence of the Internet end-to-end principle, the network performance experienced by a particular application is difficult to attribute based on the behavior of the individual elements. At any given moment, the measure of performance between any two points is typically unknown and applications must operate blindly. As a further consequence, changes to the configuration of given element, or changes in the end-to-end path, cannot easily be validated. Optimization and provisioning cannot then be automated except against only the simplest design specifications.\n\nThere is an increasing interest in Autonomic Networking research, and a strong conviction that an evolution from the current networking status quo is necessary. Although to date there have not been any practical implementations demonstrating the benefits of an effective autonomic networking paradigm, there seems to be a consensus as to the characteristics which such implementations would need to demonstrate. These specifically include continuous monitoring, identifying, diagnosing and fixing problems based on high-level policies and objectives.\n\nAutognostics, as a major part of the autonomic networking concept, intends to bring networks to a new level of awareness and eliminate the lack of visibility which currently exists in today's networks.\n\nAutognostics is a new paradigm that describes the capacity for computer networks to be self-aware, in part and as a whole, and dynamically adapt to the applications running on them by autonomously monitoring, identifying, diagnosing, resolving issues, subsequently verifying that any remediation was successful, and reporting the impact with respect to the application’s use (i.e., providing visibility into the changes to networks and their effects).\n\nAlthough similar to the concept of \"network awareness\", i.e., the capability of network devices and applications to be aware of network characteristics (see References section below), it is noteworthy that autognostics takes that concept one step further. The main difference is the \"auto\" part of autognostics, which entails that network devices are \"self\"-aware of network characteristics, and have the capability to adapt \"themselves\" as a result of continuous monitoring and diagnostics.\n\nAutognostics, or in other words \"deep self-knowledge\", can be best described as \"the ability of a network to know itself and the applications that run on it\". This knowledge is used to autonomously adapt to dynamic network and application conditions such as utilization, capacity, quality of service/application/user experience, etc.\n\nIn order to achieve autognosis, networks need a means to:\n\n\n",
    "id": "11230690",
    "title": "Autognostics"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=22113059",
    "text": "Web intelligence\n\nWeb intelligence is the area of scientific research and development that explores the roles and makes use of artificial intelligence and information technology for new products, services and frameworks that are empowered by the World Wide Web.\n\nThe term was coined in a paper written by Ning Zhong, Jiming Liu Yao and Y.Y. Ohsuga in the Computer Software and Applications Conference in 2000.\n\nThe research about the web intelligence covers many fields – including data mining (in particular web mining), information retrieval, pattern recognition, predictive analytics, the semantic web, web data warehousing – typically with a focus on web personalization and adaptive websites.\n\n\n",
    "id": "22113059",
    "title": "Web intelligence"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=22837841",
    "text": "OpenCog\n\nOpenCog is a project that aims to build an open source artificial intelligence framework. OpenCog Prime is an architecture for robot and virtual embodied cognition that defines a set of interacting components designed to give rise to human-equivalent artificial general intelligence (AGI) as an emergent phenomenon of the whole system. OpenCog Prime's design is primarily the work of Ben Goertzel while the OpenCog framework is intended as a generic framework for broad-based AGI research. Research utilizing OpenCog has been published in journals and presented at conferences and workshops including the annual Conference on Artificial General Intelligence. OpenCog is released under the terms of the GNU Affero General Public License.\n\nOpenCog was originally based on the release in 2008 of the source code of the proprietary \"Novamente Cognition Engine\" (NCE) of Novamente LLC. The original NCE code is discussed in the PLN book (ref below). Ongoing development of OpenCog is supported by Artificial General Intelligence Research Institute (AGIRI), the Google Summer of Code project, and others.\n\nOpenCog consists of:\n\n\nIn 2008, the Machine Intelligence Research Institute (MIRI), formerly called Singularity Institute for Artificial Intelligence (SIAI), which was sponsoring OpenCog, resulting in a part-time post-doc Joel Pitt and a full-time system engineer. Many contributions from the open source community have been made since OpenCog's involvement in the Google Summer of Code in 2008 and 2009. Currently the SIAI no longer supports OpenCog. Today, OpenCog receives funding and support from several sources including the Hong Kong government, Hong Kong Polytechnic University and the Jeffrey Epstein VI Foundation.\n\n\n\n\n",
    "id": "22837841",
    "title": "OpenCog"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=19835689",
    "text": "Concurrent MetateM\n\nConcurrent MetateM is a multi-agent language in which each agent is programmed using a set of (augmented) temporal logic specifications of the behaviour it should exhibit. These specifications are executed directly to generate the behaviour of the agent. As a result, there is no risk of invalidating the logic as with systems where logical specification must first be translated to a lower-level implementation.\n\nThe root of the MetateM concept is Gabbay's separation theorem; any arbitrary temporal logic formula can be rewritten in a logically equivalent \"past → future\" form. Execution proceeds by a process of continually matching rules against a history, and \"firing\" those rules when antecedents are satisfied. Any instantiated future-time consequents become commitments which must subsequently be satisfied, iteratively generating a model for the formula made up of the program rules.\n\nThe Temporal Connectives of Concurrent MetateM can divided into two categories, as follows:\n\n\nThe connectives {◎,●,◆,■,◯,◇,□} are unary; the remainder are binary.\n\n●ρ is satisfied now if ρ was true in the previous time. If ●ρ is interpreted at the beginning of time, it is satisfied despite there being no actual previous time. Hence \"weak\" last.\n\n◎ρ is satisfied now if ρ was true in the previous time. If ◎ρ is interpreted at the beginning of time, it is not satisfied because there is no actual previous time. Hence \"strong\" last.\n\n◆ρ is satisfied now if ρ was true in any previous moment in time.\n\n■ρ is satisfied now if ρ was true in every previous moment in time.\n\nρ\"S\"ψ is satisfied now if ψ is true at any previous moment and ρ is true at every moment after that moment.\n\nρ\"Z\"ψ is satisfied now if (ψ is true at any previous moment and ρ is true at every moment after that moment) OR ψ has not happened in the past.\n\n◯ρ is satisfied now if ρ is true in the next moment in time.\n\n◇ρ is satisfied now if ρ is true now or in any future moment in time.\n\n□ρ is satisfied now if ρ is true now and in every future moment in time.\n\nρ\"U\"ψ is satisfied now if ψ is true at any future moment and ρ is true at every moment prior.\n\nρ\"W\"ψ is satisfied now if (ψ is true at any future moment and ρ is true at every moment prior) OR ψ does not happen in the future.\n\n",
    "id": "19835689",
    "title": "Concurrent MetateM"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=11920671",
    "text": "Machine perception\n\nMachine perception is the capability of a computer system to interpret data in a manner that is similar to the way humans use their senses to relate to the world around them. The basic method that the computers take in and respond to their environment is through the attached hardware. Until recently input was limited to a keyboard, or a mouse, but advances in technology, both in hardware and software, have allowed computers to take in sensory input in a way similar to humans.\n\nMachine perception allows the computer to use this sensory input, as well as conventional computational means of gathering information, to gather information with greater accuracy and to present it in a way that is more comfortable for the user. These include computer vision, machine hearing, and machine touch.\n\nThe end goal of machine perception is to give machines the ability to see, feel and perceive the world as humans do and therefore for them to be able to explain in a human way why they are making their decisions, to warn us when it is failing and more importantly, the reason why it is failing.\n\nComputer vision is a field that includes methods for acquiring, processing, analyzing, and understanding images and, in general, high-dimensional data from the real world in order to produce numerical or symbolic information, e.g., in the forms of decisions. Computer vision has many applications already in use today such as facial recognition, geographical modeling, and even aesthetic judgment.\n\nMachine hearing, also known as machine listening or computer audition, is the ability of a computer or machine to take in and process sound data such as music or speech. This area has a wide range of application including music recording and compression, speech synthesis, and speech recognition.\nMoreover, this technology allows the machine to replicate the human brain’s ability to selectively focus in a specific sound against many other competing sounds and background noise. This particular ability is called “auditory scene analysis”. The technology enables the machine to segment several streams occurring at the same time.\nMany commonly used devices such as a smartphones, voice translators, and cars make use of some form of machine hearing.\n\nMachine touch is an area of machine perception where tactile information is processed by a machine or computer. Applications include tactile perception of surface properties and dexterity whereby tactile information can enable intelligent reflexes and interaction with the environment.\n\n",
    "id": "11920671",
    "title": "Machine perception"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=3479720",
    "text": "Situated\n\nIn artificial intelligence and cognitive science, the term situated refers to an agent which is embedded in an environment. The term \"situated\" is commonly used to refer to robots, but some researchers argue that software agents can also be situated if:\n\n\nExamples might include web-based agents, which can alter data or trigger processes (such as purchases) over the internet, or virtual-reality bots which inhabit and change virtual worlds, such as Second Life.\n\nBeing situated is generally considered to be part of being embodied, but it is useful to consider each perspective individually. The situated perspective emphasizes that intelligent behaviour derives from the environment and the agent's interactions with it. The nature of these interactions are defined by an agent's embodiment.\n\n\n",
    "id": "3479720",
    "title": "Situated"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=2862",
    "text": "AI-complete\n\nIn the field of artificial intelligence, the most difficult problems are informally known as AI-complete or AI-hard, implying that the difficulty of these computational problems is equivalent to that of solving the central artificial intelligence problem—making computers as intelligent as people, or strong AI. To call a problem AI-complete reflects an attitude that it would not be solved by a simple specific algorithm. \n\nAI-complete problems are hypothesised to include computer vision, natural language understanding, and dealing with unexpected circumstances while solving any real world problem.\n\nCurrently, AI-complete problems cannot be solved with modern computer technology alone, but would also require human computation. This property can be useful, for instance to test for the presence of humans as with CAPTCHAs, and for computer security to circumvent brute-force attacks.\n\nThe term was coined by Fanya Montalvo by analogy with NP-complete and NP-hard in complexity theory, which formally describes the most famous class of difficult problems. Early uses of the term are in Erik Mueller's 1987 Ph.D. dissertation and in Eric Raymond's 1991 Jargon File.\n\nAI-complete problems are hypothesised to include:\n\nTo translate accurately, a machine must be able to understand the text. It must be able to follow the author's argument, so it must have some ability to reason. It must have extensive world knowledge so that it knows what is being discussed — it must at least be familiar with all the same commonsense facts that the average human translator knows. Some of this knowledge is in the form of facts that can be explicitly represented, but some knowledge is unconscious and closely tied to the human body: for example, the machine may need to understand how an ocean makes one \"feel\" to accurately translate a specific metaphor in the text. It must also model the authors' goals, intentions, and emotional states to accurately reproduce them in a new language. In short, the machine is required to have wide variety of human intellectual skills, including reason, commonsense knowledge and the intuitions that underlie motion and manipulation, perception, and social intelligence. Machine translation, therefore, is believed to be AI-complete: it may require strong AI to be done as well as humans can do it.\n\nCurrent AI systems can solve very simple and/or restricted versions of AI-complete problems, but never in their full generality. When AI researchers attempt to \"scale up\" their systems to handle more complicated, real world situations, the programs tend to become excessively brittle without commonsense knowledge or a rudimentary understanding of the situation: they fail as unexpected circumstances outside of its original problem context begin to appear. When human beings are dealing with new situations in the world, they are helped immensely by the fact that they know what to expect: they know what all things around them are, why they are there, what they are likely to do and so on. They can recognize unusual situations and adjust accordingly. A machine without strong AI has no other skills to fall back on.\n\nComputational complexity theory deals with the relative computational difficulty of computable functions. By definition it does not cover problems whose solution is unknown or has not been characterised formally. Since many AI problems have no formalisation yet, conventional complexity theory does not allow the definition of AI-completeness.\n\nTo address this problem, a complexity theory for AI has been proposed. It is based on a model of computation that splits the computational burden between a computer and a human: one part is solved by computer and the other part solved by human. This is formalised by a human-assisted Turing machine. The formalisation defines algorithm complexity, problem complexity and reducibility which in turn allows equivalence classes to be defined.\n\nThe complexity of executing an algorithm with a human-assisted Turing machine is given by a pair formula_1, where the first element represents the complexity of the human's part and the second element is the complexity of the machine's part.\n\nThe complexity of solving the following problems with a human-assisted Turing machine is:\n\n\n",
    "id": "2862",
    "title": "AI-complete"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=2711317",
    "text": "Intelligent agent\n\nIn artificial intelligence, an intelligent agent (IA) is an autonomous entity which observes through sensors and acts upon an environment using actuators (i.e. it is an agent) and directs its activity towards achieving goals (i.e. it is \"rational\", as defined in economics). Intelligent agents may also learn or use knowledge to achieve their goals. They may be very simple or very complex. A reflex machine, such as a thermostat, is considered an example of an intelligent agent.\nIntelligent agents are often described schematically as an abstract functional system similar to a computer program. For this reason, intelligent agents are sometimes called abstract intelligent agents (AIA) to distinguish them from their real world implementations as computer systems, biological systems, or organizations. Some definitions of intelligent agents emphasize their , and so prefer the term autonomous intelligent agents. Still others (notably ) considered goal-directed behavior as the essence of intelligence and so prefer a term borrowed from economics, \"rational agent\".\n\nIntelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, the philosophy of practical reason, as well as in many interdisciplinary socio-cognitive modeling and computer social simulations.\n\nIntelligent agents are also closely related to software agents (an autonomous computer program that carries out tasks on behalf of users). In computer science, the term \"intelligent agent\" may be used to refer to a software agent that has some intelligence, regardless if it is not a rational agent by Russell and Norvig's definition. For example, autonomous programs used for operator assistance or data mining (sometimes referred to as \"bots\") are also called \"intelligent agents\".\n\nIntelligent agents have been defined many different ways. According to Nikola Kasabov AI systems should exhibit the following characteristics:\n\nA simple agent program can be defined mathematically as an agent function which maps every possible percepts sequence to a possible action the agent can perform or to a coefficient, feedback element, function or constant that affects eventual actions:\nAgent function is an abstract concept as it could incorporate various principles of decision making like calculation of utility of individual options, deduction over logic rules, fuzzy logic, etc.\n\nThe program agent, instead, maps every possible percept to an action.\n\nWe use the term percept to refer to the agent's perceptional inputs at any given instant. In the following figures an agent is anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators.\n\n said we should consider four classes of agents:\n\n group agents into five classes based on their degree of perceived intelligence and capability:\n\nSimple reflex agents act only on the basis of the current percept, ignoring the rest of the percept history. The agent function is based on the \"condition-action rule\": if condition then action.\n\nThis agent function only succeeds when the environment is fully observable. Some reflex agents can also contain information on their current state which allows them to disregard conditions whose actuators are already triggered.\n\nInfinite loops are often unavoidable for simple reflex agents operating in partially observable environments. Note: If the agent can randomize its actions, it may be possible to escape from infinite loops.\n\nA model-based agent can handle partially observable environments. Its current state is stored inside the agent maintaining some kind of structure which describes the part of the world which cannot be seen. This knowledge about \"how the world works\" is called a model of the world, hence the name \"model-based agent\".\n\nA model-based reflex agent should maintain some sort of internal model that depends on the percept history and thereby reflects at least some of the unobserved aspects of the current state. Percept history and impact of action on the environment can be determined by using internal model. It then chooses an action in the same way as reflex agent.\n\nGoal-based agents further expand on the capabilities of the model-based agents, by using \"goal\" information. Goal information describes situations that are desirable. This allows the agent a way to choose among multiple possibilities, selecting the one which reaches a goal state. Search and planning are the subfields of artificial intelligence devoted to finding action sequences that achieve the agent's goals.\n\nGoal-based agents only distinguish between goal states and non-goal states. It is possible to define a measure of how desirable a particular state is. This measure can be obtained through the use of a \"utility function\" which maps a state to a measure of the utility of the state. A more general performance measure should allow a comparison of different world states according to exactly how happy they would make the agent. The term utility can be used to describe how \"happy\" the agent is.\n\nA rational utility-based agent chooses the action that maximizes the expected utility of the action outcomes - that is, what the agent expects to derive, on average, given the probabilities and utilities of each outcome. A utility-based agent has to model and keep track of its environment, tasks that have involved a great deal of research on perception, representation, reasoning, and learning.\n\nLearning has the advantage that it allows the agents to initially operate in unknown environments and to become more competent than its initial knowledge alone might allow. The most important distinction is between the \"learning element\", which is responsible for making improvements, and the \"performance element\", which is responsible for selecting external actions.\n\nThe learning element uses feedback from the \"critic\" on how the agent is doing and determines how the performance element should be modified to do better in the future.\nThe performance element is what we have previously considered to be the entire agent: it takes in percepts and decides on actions.\n\nThe last component of the learning agent is the \"problem generator\". It is responsible for suggesting actions that will lead to new and informative experiences.\n\nTo actively perform their functions, Intelligent Agents today are normally gathered in a hierarchical structure containing many “sub-agents”. Intelligent sub-agents process and perform lower level functions. Taken together, the intelligent agent and sub-agents create a complete system that can accomplish difficult tasks or goals with behaviors and responses that display a form of intelligence.\n\nIntelligent agents are applied as automated online assistants, where they function to perceive the needs of customers in order to perform individualized customer service. Such an agent may basically consist of a dialog system, an avatar, as well an expert system to provide specific expertise to the user. They can also be used to optimize coordination of human groups online.\n\n\n",
    "id": "2711317",
    "title": "Intelligent agent"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=23762260",
    "text": "Language Acquisition Device (computer)\n\nThe Language Acquisition Device is a computer program developed by Lobal Technologies, a computer company in the United Kingdom, and scientists from King's College. It emulates the functions of the brain's frontal lobes where humans process language and emotion.\n\nScientists hope this might enable computers to understand, speak, learn, and eventually think. One possible use is in interactive entertainment such as video gaming, where the technology is used to help computer-controlled characters to develop. A press release describing this technology produced widespread media interest in 2002, but no reports have been published since then, and the current status of the technology is unclear.\n\n",
    "id": "23762260",
    "title": "Language Acquisition Device (computer)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=5477824",
    "text": "Cobweb (clustering)\n\nCOBWEB is an incremental system for hierarchical conceptual clustering. COBWEB was invented by Professor Douglas H. Fisher, currently at Vanderbilt University.\n\nCOBWEB incrementally organizes observations into a classification tree. Each node in a classification tree represents a class (concept) and is labeled by a probabilistic concept that summarizes the attribute-value distributions of objects classified under the node. This classification tree can be used to predict missing attributes or the class of a new object.\n\nThere are four basic operations COBWEB employs in building the classification tree. Which operation is selected depends on the category utility of the classification achieved by applying it. The operations are:\n\n \"COBWEB\"(\"root\", \"record\"):\n\n",
    "id": "5477824",
    "title": "Cobweb (clustering)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=19890479",
    "text": "Probabilistic logic network\n\nA probabilistic logic network (PLN) is a conceptual, mathematical and computational approach to uncertain inference; inspired by logic programming, but using probabilities in place of crisp (true/false) truth values, and fractional uncertainty in place of crisp known/unknown values. In order to carry out effective reasoning in real-world circumstances, artificial intelligence software must robustly handle uncertainty. However, previous approaches to uncertain inference do not have the breadth of scope required to provide an integrated treatment of the disparate forms of cognitively critical uncertainty as they manifest themselves within the various forms of pragmatic inference. Going beyond prior probabilistic approaches to uncertain inference, PLN is able to encompass within uncertain logic such ideas as induction, abduction, analogy, fuzziness and speculation, and reasoning about time and causality.\n\nPLN was developed by Ben Goertzel, Matt Ikle, Izabela Lyon Freire Goertzel and Ari Heljakka for use as a cognitive algorithm used by MindAgents within the OpenCog Core. PLN was developed originally for use within the Novamente Cognition Engine.\n\nThe basic goal of PLN is to provide reasonably accurate probabilistic inference in a way that is compatible with both term logic and predicate logic, and scales up to operate in real time on large dynamic knowledge bases.\n\nThe goal underlying the theoretical development of PLN has been the creation of practical software systems carrying out complex, useful inferences based on uncertain knowledge and drawing uncertain conclusions. PLN has been designed to allow basic probabilistic inference to interact with other kinds of inference such as intensional inference, fuzzy inference, and higher-order inference using quantifiers, variables, and combinators, and be a more convenient approach than Bayesian networks (or other conventional approaches) for the purpose of interfacing basic probabilistic inference with these other sorts of inference. In addition, the inference rules are formulated in such a way as to avoid the paradoxes of Dempster-Shafer theory.\n\nPLN begins with a term logic foundation, and then adds on elements of probabilistic and combinatory logic, as well as some aspects of predicate logic and autoepistemic logic, to form a complete inference system, tailored for easy integration with software components embodying other (not explicitly logical) aspects of intelligence.\n\nPLN represents truth values as intervals, but with different semantics than in Imprecise Probability Theory. In addition to the interpretation of truth in a probabilistic fashion, a truth value in PLN also has an associated amount of \"certainty\". This generalizes the notion of truth values used in autoepistemic logic, where truth values are either known or unknown, and when known, are either true or false.\n\nThe current version of PLN has been used in narrow-AI applications such as the inference of biological hypotheses from knowledge extracted from biological texts via language processing, and to assist the reinforcement learning of an embodied agent, in a simple virtual world, as it is taught to play \"fetch\".\n\n\n",
    "id": "19890479",
    "title": "Probabilistic logic network"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=24241139",
    "text": "Incremental heuristic search\n\nIncremental heuristic search algorithms combine both incremental and heuristic search to speed up searches of sequences of similar search problems, which is important in domains that are only incompletely known or change dynamically. Incremental search has been studied at least since the late 1960s. Incremental search algorithms reuse information from previous searches to speed up the current search and solve search problems potentially much faster than solving them repeatedly from scratch. Similarly, heuristic search has also been studied at least since the late 1960s.\n\nHeuristic search algorithms, often based on A*, use heuristic knowledge in the form of approximations of the goal distances to focus the search and solve search problems potentially much faster than uninformed search algorithms. The resulting search problems, sometimes called dynamic path planning problems, are graph search problems where paths have to be found repeatedly because the topology of the graph, its edge costs, the start vertex or the goal vertices change over time.\n\nSo far, three main classes of incremental heuristic search algorithms have been developed:\n\n\nAll three classes of incremental heuristic search algorithms are different from other replanning algorithms, such as planning by analogy, in that their plan quality does not deteriorate with the number of replanning episodes.\n\nIncremental heuristic search has been extensively used in robotics, where a larger number of path planning systems are based on either D* (typically\nearlier systems) or D* Lite (current systems), two different incremental heuristic search algorithms.\n\n",
    "id": "24241139",
    "title": "Incremental heuristic search"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=24251211",
    "text": "Percept (artificial intelligence)\n\nA percept is the input that an intelligent agent is perceiving at any given moment. It is essentially the same concept as a percept in psychology, except that it is being perceived not by the brain but by the agent. A percept is detected by a sensor, often a camera, processed accordingly, and acted upon by an actuator. Each percept is added to a percept sequence, which is a complete history of each percept ever detected. An intelligent agent chooses how to act not only based on the current percept, but the percept sequence. The next action is chosen by the agent function, which maps every percept to an action.\n\nFor example, if a camera were to record a gesture, the agent would process the percepts, calculate the corresponding spatial vectors, examine its percept history, and use the agent program (the application of the agent function) to act accordingly.\n",
    "id": "24251211",
    "title": "Percept (artificial intelligence)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=12018606",
    "text": "Synthetic Environment for Analysis and Simulations\n\nSEAS was developed to help Fortune 500 companies with strategic planning. Then it was used to help \"recruiting commanders to strategize ways to improve recruiting potential soldiers\". In 2004 SEAS was evaluated for its ability to help simulate \"the non-kinetic aspects of combat, things like the diplomatic, economic, political, infrastructure and social issues\".\n\nSentient World Simulation is the name given to the current vision of making SEAS a \"continuously running, continually updated mirror model of the real world that can be used to predict and evaluate future events and courses of action.\"\n\nSEAS technology resulted from over ten years of research at Purdue University, funded by the Department of Defense, several Fortune 500 companies, the National Science Foundation, the Century Fund of the state of Indiana, and the Office of Naval Research. Originally, SEAS was developed to help Fortune 500 companies with strategic planning. It was also used to model the population of the U.S. that is eligible for military service to help \"recruiting commanders to strategize ways to improve recruiting potential soldiers\" and to study biological attacks.\n\nIn January 2004 SEAS was evaluated by the Joint Innovation and Experimentation Directorate (J9) of the US Joint Forces Command (JFCOM) for its ability to help simulate \"the non-kinetic aspects of combat, things like the diplomatic, economic, political, infrastructure and social issues\" at the Purdue Technology Park during Breaking Point 2004, an environment-shaping war game resulting in the conclusion that it \"moves us from the current situation where everyone comes together and sits around a table discussing what they would do, to a situation where they actually play in the simulation and their actions have consequences.\"\n\nIn 2006 JFCOM-J9 used SEAS to war game warfare scenarios for Baghdad in 2015. In April 2007 JFCOM-J9 began working with Homeland Security and multinational forces in a homeland defense war gaming exercise.\n\nThe Sentient World Simulation project (SWS) is to be based on SEAS. The ultimate goal envisioned by Alok R. Chaturvedi on March 10, 2006 was for SWS to be a \"continuously running, continually updated mirror model of the real world that can be used to predict and evaluate future events and courses of action. SWS will react to actual events that occur anywhere in the world and incorporate newly sensed data from the real world. [...] As the models influence each other and the shared synthetic environment, behaviors and trends emerge in the synthetic world as they do in the real world. Analysis can be performed on the trends in the synthetic world to validate alternate worldviews. [...] Information can be easily displayed and readily transitioned from one focus to another using detailed modeling, such as engineering level modeling, to aggregated strategic, theater, or campaign-level modeling.\" \n\nAlok R. Chaturvedi is the founder and the Director of SEAS Laboratory as well as the technical lead for the Sentient World Simulation project initiated by US Joint Forces Command.\n\n\n",
    "id": "12018606",
    "title": "Synthetic Environment for Analysis and Simulations"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=24571654",
    "text": "Angel F\n\nAngel_F is a fictional child artificial intelligence that has been used in worldwide art performances focused on the issues of digital liberties, intellectual property and on the evolution of language and behaviour in information society. The character was created by Salvatore Iaconesi in 2007 as a hack to the Biodoll art performance by Italian artist Franca Formenti. \n\nThe project revolves around the story of the technological sensual relationship between the Biodoll, a digital prostitute, and professor Derrick de Kerckhove, culminating in the birth of the young artificial intelligence. The digital child morphed from a spyware program capturing internet users' browsing activities in order to gather text to feed an algorithmic language generating system (thus letting the \"child\" learn how to speak) and became a digital personality used by the artists for both online and real-world performances.\n\nThe project was later joined by Oriana Persico who curated communication and part of the theoretical approaches of the action.\n\nThe Angel_F project has been featured in books, magazines, national televisions, and has been invited to many worldwide conferences and events, both academic and artistic. \n\nOn one occasion, in October 2007, Angel_F attended the Internet Governance Forum (IGF) in Rio de Janeiro as the only digital being invited to speak: the young artificial intelligence was featured with a video message concerning the struggles for digital liberties, privacy, knowledge sharing and access to networks and technologies.\n\nAngel_F is an acronym which stads for \"Autonomous Non Generative E-volitive Life_Form\". The project was born in 2007 and resulted from the fusion of two contemporary art performances.\n\nFranca Formenti, an Italian artist living in Varese, invented the Biodoll character in 2002 which began making its appearances first on the network and later in the physical world by using what were called \"clones\": young women, prostitutes, pornographic starlets, transsexuals and models interpreting the role of a digital prostitute. The Biodoll was an art performance focused on research emerging from the network of new forms of sexualities, and on the analysis of changes brought on by this transformation to the concepts of private and public spaces, privacy, and the possibility of creating multiple fluid identities through language and digital media. At the time the Biodoll character claimed a role in professor Derrick de Kerckhove's life, narratively seducing him and alluring him into joining in the performance, becoming the Biodoll's favourite lover.\n\nThe theme of fertility has always been central to the Biodoll performance: the digital prostitute was a wombless clone but desired giving birth to a son, the Bloki.\n\nIn a process starting in 2006 and ending in February 2007, Salvatore Iaconesi (xDxD.vs.xDxD) used his Talker linguistic artificial intelligence to animate the digital child conceived with prof. Derrick de Kerckhove: \"Angel_F\".\n\nIaconesi and Persico met in November 2006 and immediately started collaborating on the birth of Angel_F.\n\nAngel_F was designed as a synthetic digital being composed through narrative, technological and cognitive psychology layers. The objective was to create iconic characteristics that resulted in being evocative and able to mimic human life up to a level in which bringing up a symbolic dialogue was possible. On the other side, the artificial identity was to implement and expose the cultural, emotional and relational ways that were typical of networked social ecosystems, among those technologies, systems and infrastructures that entered and shaped people's daily lives.\n\nThe young digital being mimicked the evolution of a human baby: initially conceived inside the website of its digital mother it emulated the birth of a child by using the metaphor of a virus developing inside a website, taking progressively more space in the domain's databases and interfaces. Content was produced through the software by using small browser-based spyware techniques, through which Angel_F could infer the list of major portals that had been visited by the website's users.\n\nThe Biodoll website was invaded by this growing presence and, thus, Angel_F was born.\n\nThe Artificial Intelligence (AI) component of Angel_F was derived from another project, Talker, through which internet users could build up the AI's linguistic network by feeding it their text and web clips. Angel_F used this component to generate sentences and phrases, publishing them on the interface and on selected blogs.\n\nThe parallel between the growth of the AI and that of a child kept building up and, just as children learn how to speak and act by observing their parents and the people around them, Angel_F used its spyware and AI components to learn, to navigate websites and web portals using web crawler based techniques, and to interact with other people by using the contents hosted and generated in its database to create surreal dialogues in blogs and websites.\n\nA virtual school was created, called Talker Mind, to narratively continue the AI's growth. Five professors (Massimo Canevacci, Antonio Caronia, Carlo Formenti, Derrick de Kerckhove and Luigi Pagliarini) fed their texts and academic articles to Angel_F, simulating virtual asynchronous lessons by using a multi-blog structure.\n\nA peer-to-peer system was also created at the time, named Presence. Its interface resembled the one of 8-bit videogames and the peer to peer users travelled in a starry space and were able to perform standard Instant Messaging tasks, such as chat and file sharing. The interactions were possible both among humans and digital beings. Angel_F was the first user of the Presence peer to peer system.\n\nAngel_F entered the physical world as a baby-stroller mounted laptop computer that was used to let the digital child join events and conferences held worldwide.\n\nThe first public space performance was held in Milan, when the Biodoll distributed a generative \"free press\" publication (called the Bloki FreePreXXX, its text was generated algorithmically and inserted into a prepared graphic layout).\n\nThe second performance was held in Rome, at the Forte Prenestino on June 14, 2007, with a massive playroom created through computational graphics that people could interact with and that were generated by the AI.\n\nAngel_F performed all over the world, both in artistic contexts and in academic ones. It was also used for the communication strategy of several activist groups on the themes of intellectual property and digital freedoms.\n\nOn June 22, 2007 Angel_F presented the closing remarks for an Ipotesi per Assurdo (Absurd Hypothesis) with Salvatore Iaconesi and Oriana Persico at the IULM University in Milan, discussing the possibilities for an ecosystemic, sustainable reinvention of corporations.\n\nOn July 28, 2007, hundreds of people at LiberaFesta (Free Party) in Rome listened to Angel_F in a speech discussing new politics and \"hacker ethics\".\n\nOn October 2007, Angel_F was presented live at the \"FE/MALE 2\" event, as an example of an atypical family during a public debate on new sexualities and social change.\n\nAgain in October 2007, Angel_F made a series of public performances Florence's Festival della Creatività (Festival of Creativity), an institutional event held periodically to showcase Italy's and other countries' best technological projects. During the festival Derrick de Kerckhove publicly recognized the little AI as his digital son.\n\nOn October 2008, Angel_F was used at a public event on a European scale called Freedom not Fear discussing privacy and civil liberties.\n\nLately (July 2009) Angel_F has been seen with its digital father Derrick de Kerckhove to protest against Italy's harsh politics on freedom of speech.\n\nA case of censorship saw Angel_F involved, when it was not allowed to post its contribution to the DFIR (Dialogue Forum for Internet Rights) held in \"Rome\" on September 2007 in preparation for Rio de Janeiro's Internet Governance Forum (IGF) edition. The case quickly turned into a collaboration among the involved parties and Angel_F was invited to the global event in Brazil where it was the only present digital being: on that occasion Angel_F contributed a videomessage, in the digital freedoms workshop, which suggested some ideas for action to the United Nations and to all the parties involved in the global \"IGF\" organization.\n\nAngel_F mixes real technologically advanced systems with a poetic, narrative approach. In the performance real and not-real are constantly intermixed, and the concepts, techniques and characteristics of the whole project have been analyzed multiple times in several international academic events and research projects.\n\nAngel_F's authors coined the term NeoRealismo Virtuale (Virtual NeoRealism) to identify the practices created and used in the project. Its intent is to describe and represent the contemporary world in its hybrid analog/digital form, with digital technologies progressively interacting and integrating to the physical world, even on the level of personal identity and private/public space definitions.\n\nThe Glocal & Outsiders conference held in Prague at the \"Academy of Sciences\" in 2007 was the first academic presentation of the Angel_F project, together with the \"Biodoll\".\n\nBy December 2007 several international associations, and scientific researchers had been involved with Angel_F, eventually producing the system and process used to set up the \"Talker Mind\" digital school for the AI with Angel_F's professors.\n\nOn March 2008 the Tecnológico de Monterrey university in Mexico City organized the Computer Art Congress 2 international event, featuring Angel_F's project among with the ones by scientific researchers worldwide.\n\nOn July 2008, the project was presented in Austria at the \"Planetary Collegiums Consciousness Reframed 9\"' conference, together with the \"NeoRealismo Virtuale\".\n\nAs of the end of 2009, the project continues, both artistically and academically.\n\nA book is about to be published in November 2009 by \"Castelvecchi Editore\" in Italy, titled Angel F. Diario di una intelligenza artificiale (Angel_F, the diaries of an Artificial Intelligence).\n\n",
    "id": "24571654",
    "title": "Angel F"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=6585513",
    "text": "Outline of artificial intelligence\n\nThe following outline is provided as an overview of and topical guide to artificial intelligence:\n\nArtificial intelligence (AI) – intelligence exhibited by machines or software. It is also the name of the academic field which studies how to create computers and computer software that are capable of intelligent behaviour.\n\n\n\n\nApplications of artificial intelligence – \n\n\nList of artificial intelligence projects\n\n\n\nIntelligent personal assistant – \n\n\nHistory of artificial intelligence\n\n\n\nPhilosophy of artificial intelligence\n\nMarek Rosa Owner of goodAI\n\n\nArtificial intelligence in fiction – Some examples of artificially intelligent entities depicted in science fiction include:\n\nCompetitions and prizes in artificial intelligence\n\nList of important publications in computer science\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "id": "6585513",
    "title": "Outline of artificial intelligence"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=2710755",
    "text": "Fuzzy agent\n\nIn computer science a fuzzy agent is a software agent that implements fuzzy logic. This software entity interacts with its environment through an adaptive rule-base and can therefore be considered as a type of intelligent agent.\n",
    "id": "2710755",
    "title": "Fuzzy agent"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=637857",
    "text": "Symbol level\n\nIn knowledge-based systems, agents choose actions based on the principle of rationality to move closer to a desired goal. The agent is able to make decisions based on knowledge it has about the world (see knowledge level). But for the agent to actually change its state, it must use whatever means it has available. This level of description for the agent's behavior is the symbol level.\n\nFor example, in a computer program, the knowledge level consists of the information contained in its data structures that it uses to perform certain actions. The symbol level consists of the program's algorithms, the data structures themselves, and so on.\n\n",
    "id": "637857",
    "title": "Symbol level"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=23997153",
    "text": "Informatics\n\nInformatics is a branch of information engineering. It involves the practice of information processing and the engineering of information systems, and as an academic field it is an applied form of information science. The field considers the interaction between humans and information alongside the construction of interfaces, organisations, technologies and systems. As such, the field of informatics has great breadth and encompasses many subspecialties, including disciplines of computer science, information systems, information technology and statistics. Since the advent of computers, individuals and organizations increasingly process information digitally. This has led to the study of informatics with computational, mathematical, biological, cognitive and social aspects, including study of the social impact of information technologies.\n\nIn 1956 the German computer scientist Karl Steinbuch coined the word \"Informatik\" by publishing a paper called \"Informatik: Automatische Informationsverarbeitung\" (\"Informatics: Automatic Information Processing\"). The English term \"Informatics\" is sometimes understood as meaning the same as computer science. The German word \"Informatik\" is usually translated to English as \"computer science\".\n\nThe French term \"informatique\" was coined in 1962 by Philippe Dreyfus together with various translations—informatics (English), also proposed independently and simultaneously by Walter F. Bauer and associates who co-founded \"Informatics Inc.\", and \"informatica\" (Italian, Spanish, Romanian, Portuguese, Dutch), referring to the application of computers to store and process information.\n\nThe term was coined as a combination of \"information\" and \"automatic\" to describe the science of automating information interactions.\nThe morphology—\"informat\"-ion + -\"ics\"—uses \"the accepted form for names of sciences, as conics, linguistics, optics, or matters of practice, as economics, politics, tactics\", and so, linguistically, the meaning extends easily to encompass both the science of information and the practice of information processing.\n\nThe culture of library science promotes policies and procedures for managing information that fosters the relationship between library science and the development of information science to provide benefits for health informatics development; which is traced to the 1950s with the beginning of computer uses in healthcare (Nelson & Staggers p.4). Early practitioners interested in the field soon learned that there were no formal education programs set up to educate them on the informatics science until the late 1960s and early 1970s. Professional development began to emerge, playing a significant role in the development of health informatics (Nelson &Staggers p.7)\nAccording to Imhoff et al., 2001, healthcare informatics is not only the application of computer technology to problems in healthcare but covers all aspects of generation, handling, communication, storage, retrieval, management, analysis, discovery, and synthesis of data information and knowledge in the entire scope of healthcare. Furthermore, they stated that the primary goal of health informatics can be distinguished as follows: To provide solutions for problems related to data, information, and knowledge processing. To study general principles of processing data information and knowledge in medicine and healthcare.\n\nReference\nImhoff, M., Webb. A.&Goldschmidt, A., (2001). Health Informatics. Intensive Care Med, 27: 179-186. doi:10.1007//s001340000747.\n\nNelson, R. & Staggers, N. Health Informatics: An Interprofessional Approach. St. Louis: Mosby, 2013. Print. (p.4,7)\n\nThis new term was adopted across Western Europe, and, except in English, developed a meaning roughly translated by the English ‘computer science’, or ‘computing science’. Mikhailov advocated the Russian term \"informatika\" (1966), and the English \"informatics\" (1967), as names for the \"theory of scientific information\", and argued for a broader meaning, including study of the use of information technology in various communities (for example, scientific) and of the interaction of technology and human organizational structures.\n\nUsage has since modified this definition in three ways. First, the restriction to scientific information is removed, as in business informatics or legal informatics. Second, since most information is now digitally stored, computation is now central to informatics. Third, the representation, processing and communication of information are added as objects of investigation, since they have been recognized as fundamental to any scientific account of information. Taking \"information\" as the central focus of study distinguishes \"informatics\" from \"computer science.\" Informatics includes the study of biological and social mechanisms of information processing whereas computer science focuses on the digital computation. Similarly, in the study of representation and communication, informatics is indifferent to the substrate that carries information. For example, it encompasses the study of communication using gesture, speech and language, as well as digital communications and networking.\n\nIn the English-speaking world the term \"informatics\" was first widely used in the compound medical informatics, taken to include \"the cognitive, information processing, and communication tasks of medical practice, education, and research, including information science and the technology to support these tasks\". Many such compounds are now in use; they can be viewed as different areas of \"\"applied informatics\"\".\nIndeed, \"In the U.S., however, informatics is linked with applied computing, or computing in the context of another domain.\"\n\nInformatics encompasses the study of systems that represent, process, and communicate information.\nHowever, the theory of computation in the specific discipline of theoretical computer science, which evolved from Alan Turing, studies the notion of a complex system regardless of whether or not information actually exists. Since both fields process information, there is some disagreement among scientists as to field hierarchy; for example Arizona State University attempted to adopt a broader definition of informatics to even encompass cognitive science at the launch of its School of Computing and Informatics in September 2006.\n\nA broad interpretation of \"informatics\", as \"the study of the structure, algorithms, behaviour, and interactions of natural and artificial computational systems,\" was introduced by the University of Edinburgh in 1994 when it formed the grouping that is now its School of Informatics. This meaning is now (2006) increasingly used in the United Kingdom.\n\nThe 2008 Research Assessment Exercise, of the UK Funding Councils, includes a new, \"Computer Science and Informatics,\" unit of assessment (UoA), whose scope is described as follows:\n\nAcademic research in the informatics area can be found in a number of disciplines such as computer science, information technology, Information and Computer Science, information system, business information management and health informatics.\n\nIn France, the first degree level qualifications in Informatics (computer science) appeared in the mid-1960s.\n\nIn English-speaking countries, the first example of a degree level qualification in Informatics occurred in 1982 when Plymouth Polytechnic (now the University of Plymouth) offered a four-year BSc(Honours) degree in Computing and Informatics – with an initial intake of only 35 students. The course still runs today making it the longest available qualification in the subject.\n\nAt the Indiana University School of Informatics (Bloomington, Indianapolis and Southeast), informatics is defined as \"the art, science and human dimensions of information technology\" and \"the study, application, and social consequences of technology.\" It is also defined in Informatics 101, Introduction to Informatics as \"the application of information technology to the arts, sciences, and professions.\" These definitions are widely accepted in the United States, and differ from British usage in omitting the study of natural computation.\n\nTexas Woman's University places its informatics degrees in its department of Mathematics and Computer Science within the College of Arts & Sciences, though it offers interdisciplinary Health Informatics degrees. Informatics is presented in a generalist framework, as evidenced by their definition of informatics (\"Using technology and data analytics to derive meaningful information from data for data and decision driven practice in user centered systems\"), though TWU is also known for its nursing and health informatics programs.\n\nAt the University of California, Irvine Department of Informatics, informatics is defined as \"the interdisciplinary study of the design, application, use and impact of information technology. The discipline of informatics is based on the recognition that the design of this technology is not solely a technical matter, but must focus on the relationship between the technology and its use in real-world settings. That is, informatics designs solutions in context, and takes into account the social, cultural and organizational settings in which computing and information technology will be used.\"\n\nAt the University of Michigan, Ann Arbor Informatics interdisciplinary major, informatics is defined as \"the study of information and the ways information is used by and affects human beings and social systems. The major involves coursework from the College of Literature, Science and the Arts, where the Informatics major is housed, as well as the School of Information and the College of Engineering. Key to this growing field is that it applies both technological and social perspectives to the study of information. Michigan's interdisciplinary approach to teaching Informatics gives a solid grounding in contemporary computer programming, mathematics, and statistics, combined with study of the ethical and social science aspects of complex information systems. Experts in the field help design new information technology tools for specific scientific, business, and cultural needs.\" Michigan offers four curricular tracks within the informatics degree to provide students with increased expertise. These four track topics include:\n\nAt the University of Washington, Seattle Informatics Undergraduate Program, Informatics is an undergraduate program offered by the Information School. Bachelor of Science in Informatics is described as \"[a] program that focuses on computer systems from a user-centered perspective and studies the structure, behavior and interactions of natural and artificial systems that store, process and communicate information. Includes instruction in information sciences, human computer interaction, information system analysis and design, telecommunications structure and information architecture and management.\" Washington offers three degree options as well as a custom track.\n\nOne of the most significant areas of application of informatics is that of organizational informatics. Organizational informatics is fundamentally interested in the application of information, information systems and ICT within organisations of various forms including private sector, public sector and voluntary sector organisations. As such, organisational informatics can be seen to be a sub-category of social informatics and a super-category of business informatics.\n\n\n",
    "id": "23997153",
    "title": "Informatics"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=183503",
    "text": "Description logic\n\nDescription logics (DL) are a family of formal knowledge representation languages. Many DLs are more expressive than propositional logic but less expressive than First-order logic. In contrast to the latter, the core reasoning problems for DLs are (usually) decidable, and efficient decision procedures have been designed and implemented for these problems. There are general, spatial, temporal, spatiotemporal, and fuzzy descriptions logics, and each description logic features a different balance between DL expressivity and reasoning complexity by supporting different sets of mathematical constructors.\n\nDLs are used in artificial intelligence to describe and reason about the relevant concepts of an application domain (known as \"terminological knowledge\"). It is of particular importance in providing a logical formalism for ontologies and the Semantic Web: the Web Ontology Language (OWL) and its profile is based on DLs. The most notable application of DLs and OWL is in biomedical informatics where DL assists in the codification of biomedical knowledge.\n\nA description logic (DL) models \"concepts\", \"roles\" and \"individuals\", and their relationships.\n\nThe fundamental modeling concept of a DL is the \"axiom\"—a logical statement relating roles and/or concepts. This is a key difference from the frames paradigm where a \"frame specification\" declares and completely defines a class.\n\nThe description logic community uses different terminology than the first-order logic (FOL) community for operationally equivalent notions; some examples are given below. The Web Ontology Language (OWL) uses again a different terminology, also given in the table below.\n\nThere are many varieties of description logics and there is an informal naming convention, roughly describing the operators allowed. The expressivity is encoded in the label for a logic starting with one of the following basic logics:\n\nFollowed by any of the following extensions:\nSome canonical DLs that do not exactly fit this convention are:\nAs an example, formula_1 is a centrally important description logic from which comparisons with other varieties can be made. formula_1 is simply formula_3 with complement of any concept allowed, not just atomic concepts. formula_1 is used instead of the equivalent formula_5.\n\nA further example, the description logic formula_6 is the logic formula_1 plus extended cardinality restrictions, and transitive and inverse roles. The naming conventions aren't purely systematic so that the logic formula_8 might be referred to as formula_9 and abbreviations are made where possible,\n\nThe Protégé ontology editor supports formula_10. Three major biomedical informatics terminology bases, SNOMED CT, GALEN, and GO, are expressible in formula_11 (with additional role properties).\n\nOWL 2 provides the expressiveness of formula_12, OWL-DL is based on formula_10, and for OWL-Lite it is formula_14.\n\nDescription logic was given its current name in the 1980s. Previous to this it was called (chronologically): \"terminological systems\", and \"concept languages\".\n\nFrames and semantic networks lack formal (logic-based) semantics. DL was first introduced into Knowledge Representation (KR) systems to overcome this deficiency.\n\nThe first DL-based KR system was KL-ONE (by Ronald J. Brachman and Schmolze, 1985). During the '80s other DL-based systems using \"structural subsumption algorithms\" were developed including KRYPTON (1983), LOOM (1987), BACK (1988), K-REP (1991) and CLASSIC (1991). This approach featured DL with limited expressiveness but relatively efficient (polynomial time) reasoning.\n\nIn the early '90s, the introduction of a new \"tableau based algorithm\" paradigm allowed efficient reasoning on more expressive DL. DL-based systems using these algorithms - such as KRIS (1991) - show acceptable reasoning performance on typical inference problems even though the worst case complexity is no longer polynomial.\n\nFrom the mid '90s, reasoners were created with good practical performance on very expressive DL with high worst case complexity. Examples from this period include FaCT, RACER (2001), CEL (2005), and KAON 2 (2005).\n\nDL reasoners, such as FaCT, FaCT++, RACER, DLP and Pellet, implement the method of analytic tableaux. KAON2 is implemented by algorithms which reduce a SHIQ(D) knowledge base to a disjunctive datalog program.\n\nThe DARPA Agent Markup Language (DAML) and Ontology Inference Layer (OIL) ontology languages for the Semantic Web can be viewed as\nsyntactic variants of DL. In particular, the formal semantics and reasoning in OIL use the formula_6 DL. The DAML+OIL DL was developed as a submission to—and formed the starting point of—the World Wide Web Consortium (W3C) Web Ontology Working Group. In 2004, the Web Ontology Working Group completed its work by issuing the OWL recommendation. The design of OWL is based on the formula_16 family of DL with OWL DL and OWL Lite based on formula_10 and formula_14 respectively.\n\nThe W3C OWL Working Group began work in 2007 on a refinement of - and extension to - OWL. In 2009, this was completed by the issuance of the OWL2 recommendation. OWL2 is based on the description logic formula_12. Practical experience demonstrated that OWL DL lacked several key features necessary to model complex domains.\n\nIn DL, a distinction is drawn between the so-called TBox (terminological box) and the ABox (assertional box). In general, the TBox contains sentences describing concept hierarchies (i.e., relations between concepts) while the ABox contains ground sentences stating where in the hierarchy individuals belong (i.e., relations between individuals and concepts). For example, the statement:\n\nbelongs in the TBox, while the statement:\n\nbelongs in the ABox.\n\nNote that the TBox/ABox distinction is not significant, in the same sense that the two \"kinds\" of sentences are not treated differently in first-order logic (which subsumes most DL). When translated into first-order logic, a subsumption axiom like () is simply a conditional restriction to unary predicates (concepts) with only variables appearing in it. Clearly, a sentence of this form is not privileged or special over sentences in which only constants (\"grounded\" values) appear like ().\n\nSo why was the distinction introduced? The primary reason is that the separation can be useful when describing and formulating decision-procedures for various DL. For example, a reasoner might process the TBox and ABox separately, in part because certain key inference problems are tied to one but not the other one ('classification' is related to the TBox, 'instance checking' to the ABox). Another example is that the complexity of the TBox can greatly affect the performance of a given decision-procedure for a certain DL, independently of the ABox. Thus, it is useful to have a way to talk about that specific part of the knowledge base.\n\nThe secondary reason is that the distinction can make sense from the knowledge base modeler's perspective. It is plausible to distinguish between our conception of terms/concepts in the world (class axioms in the TBox) and particular manifestations of those terms/concepts (instance assertions in the ABox). In the above example: when the hierarchy within a company is the same in every branch but the assignment to employees is different in every department (because there are other people working there), it makes sense to reuse the TBox for different branches that do not use the same ABox.\n\nThere are two features of description logic that are not shared by most other data description formalisms: DL does not make the Unique name assumption (UNA) or the Closed-world assumption (CWA). Not having UNA means that two concepts with different names may be allowed by some inference to be shown to be equivalent. Not having CWA, or rather having the Open world assumption (OWA) means that lack of knowledge of a fact does not immediately imply knowledge of the negation of a fact.\n\nLike first-order logic (FOL), a syntax defines which collections of symbols are legal expressions in a description logic, and semantics determine meaning. Unlike FOL, a DL may have several well known syntactic variants.\n\nThe syntax of a member of the description logic family is characterized by its recursive definition, in which the constructors that can be used to form concept terms are stated. Some constructors are related to logical constructors in first-order logic (FOL) such as \"intersection\" or \"conjunction\" of concepts, \"union\" or \"disjunction\" of concepts, \"negation\" or \"complement\" of concepts, \"universal restriction\" and \"existential restriction\". Other constructors have no corresponding construction in FOL including restrictions on roles for example, inverse, transitivity and functionality.\n\nLet C and D be concepts, a and b be individuals, and R be a role.\n\nIf a is R-related to b, then b is called an R-successor of a.\n\nThe prototypical DL \"Attributive Concept Language with Complements\" (formula_1) was introduced by Manfred Schmidt-Schauß and Gert Smolka in 1991, and is the basis of many more expressive DLs. The following definitions follow the treatment in Baader et al.\n\nLet formula_21, formula_22 and formula_23 be (respectively) sets of \"concept names\" (also known as \"atomic concepts\"), \"role names\" and \"individual names\" (also known as \"individuals\", \"nominals\" or \"objects\"). Then the ordered triple (formula_21, formula_22, formula_23) is the \"signature\".\n\nThe set of formula_1 \"concepts\" is the smallest set such that:\n\n\nA \"general concept inclusion\" (GCI) has the form formula_39 where formula_31 and formula_32 are \"concepts\". Write formula_42 when formula_39 and formula_44. A \"TBox\" is any finite set of GCIs.\n\n\nAn \"ABox\" is a finite set of assertional axioms.\n\nA \"knowledge base\" (KB) is an ordered pair formula_49 for TBox formula_50 and ABox formula_51.\n\nThe semantics of description logics are defined by interpreting concepts as sets of individuals and roles as sets of ordered pairs of individuals. Those individuals are typically assumed from a given domain. The semantics of non-atomic concepts and roles is then defined in terms of atomic concepts and roles. This is done by using a recursive definition similar to the syntax.\n\nThe following definitions follow the treatment in Baader et al.\n\nA \"terminological interpretation\" formula_52 over a \"signature\" formula_53 consists of\nsuch that\n\nDefine formula_67 (read \"in I holds\") as follows\n\n\n\nLet formula_80 be a knowledge base.\n\n\nIn addition to the ability to describe concepts formally, one also would like to employ the description of a set of concepts to ask questions about the concepts and instances described. The most common decision problems are basic database-query-like questions like \"instance checking\" (is a particular instance (member of an A-box) a member of a given concept) and \"relation checking\" (does a relation/role hold between two instances, in other words does a have property b), and the more global-database-questions like \"subsumption\" (is a concept a subset of another concept), and \"concept consistency\" (is there no contradiction among the definitions or chain of definitions). The more operators one includes in a logic and the more complicated the T-box (having cycles, allowing non-atomic concepts to include each other), usually the higher the computational complexity is for each of these problems (see Description Logic Complexity Navigator for examples).\n\nMany DLs are decidable fragments of first-order logic (FOL) and are usually fragments of two-variable logic or guarded logic. In addition, some DLs have features that are not covered in FOL; this includes \"concrete domains\" (such as integer or strings, which can be used as ranges for roles such as \"hasAge\" or \"hasName\") or an operator on roles for the transitive closure of that role.\n\nFuzzy description logics combines fuzzy logic with DLs. Since many concepts that are needed for intelligent systems lack well defined boundaries, or precisely defined criteria of membership, fuzzy logic is needed to deal with notions of vagueness and imprecision. This offers a motivation for a generalization of description logic towards dealing with imprecise and vague concepts.\n\nDescription logic is related to—but developed independently of—modal logic (ML). Many—but not all—DL are syntactic variants of ML.\n\nIn general, an object corresponds to a possible world, a concept corresponds to a modal proposition, and a role-bounded quantifier to a modal operator with that role as its accessibility relation.\n\nOperations on roles (such as composition, inversion, etc.) correspond to the modal operations used in dynamic logic.\n\nTemporal description logic represents—and allows reasoning about—time dependent concepts and many different approaches to this problem exist. For example, a description logic might be combined with a modal temporal logic such as linear temporal logic.\n\n\n\n\nThere are some semantic reasoners that deal with OWL and DL. These are some of the most popular:\n\n\n",
    "id": "183503",
    "title": "Description logic"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=20400528",
    "text": "Ontology engineering\n\nOntology engineering in computer science and information science is a field which studies the methods and methodologies for building ontologies: formal representations of a set of concepts within a domain and the relationships between those concepts.\nA large-scale representation of abstract concepts such as actions, time, physical objects and beliefs would be an example of ontological engineering. Ontology engineering is one of the areas of applied ontology, and can be seen as an application of philosophical ontology. Core ideas and objectives of ontology engineering are also central in conceptual modeling.\n\nAutomated processing of information not interpretable by software agents can be improved by adding rich semantics to the corresponding resources, such as video files. One of the approaches for the formal conceptualization of represented knowledge domains is the use of machine-interpretable ontologies, which provide structured data in, or based on, RDF, RDFS, and OWL. Ontology engineering is the design and creation of such ontologies, which can contain more than just the list of terms (controlled vocabulary); they contain terminological, assertional, and relational axioms to define concepts (classes), individuals, and roles (properties) (TBox, ABox, and RBox, respectively). Ontology engineering is a relatively new field of study concerning the ontology development process, the ontology life cycle, the methods and methodologies for building ontologies, and the tool suites and languages that support them.\nA common way to provide the logical underpinning of ontologies is to formalize the axioms with description logics, which can then be translated to any serialization of RDF, such as RDF/XML or Turtle. Beyond the description logic axioms, ontologies might also contain SWRL rules. The concept definitions can be mapped to any kind of resource or resource segment in RDF, such as images, videos, and regions of interest, to annotate objects, persons, etc., and interlink them with related resources across knowledge bases, ontologies, and LOD datasets. This information, based on human experience and knowledge, is valuable for reasoners for the automated interpretation of sophisticated and ambiguous contents, such as the visual content of multimedia resources. Application areas of ontology-based reasoning include, but are not limited to, information retrieval, automated scene interpretation, and knowledge discovery.\n\nAn ontology language is a formal language used to encode the ontology. There are a number of such languages for ontologies, both proprietary and standards-based:\n\nLife sciences is flourishing with ontologies that biologists use to make sense of their experiments. For inferring correct conclusions from experiments, ontologies have to be structured optimally against the knowledge base they represent. The structure of an ontology needs to be changed continuously so that it is an accurate representation of the underlying domain.\nRecently, an automated method was introduced for engineering ontologies in life sciences such as Gene Ontology (GO), one of the most successful and widely used biomedical ontology. Based on information theory, it restructures ontologies so that the levels represent the desired specificity of the concepts. Similar information theoretic approaches have also been used for optimal partition of Gene Ontology. Given the mathematical nature of such engineering algorithms, these optimizations can be automated to produce a principled and scalable architecture to restructure ontologies such as GO.\n\nOpen Biomedical Ontologies (OBO), a 2006 initiative of the U.S. National Center for Biomedical Ontology, that provides a common 'foundry' for various ontology initiatives, amongst which are:\nand more\n\n\n\n\n",
    "id": "20400528",
    "title": "Ontology engineering"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=27726566",
    "text": "Virtual intelligence\n\nVirtual intelligence is the term given to artificial intelligence that exists within a virtual world. Many virtual worlds have options for persistent avatars that provide information, training, role playing, and social interactions.\n\nThe immersion of virtual worlds provides a unique platform for VI beyond the traditional paradigm of past user interfaces (UIs). What Alan Turing established as the benchmark for telling the difference between human and computerized intelligence was done void of visual influences. With today's VI bots, virtual intelligence has evolved past the constraints of past testing into a new level of the machine's ability to demonstrate intelligence. The immersive features of these environments provide non verbal elements that affect the realism provided by virtually intelligent agents.\n\nVirtual intelligence is the intersection of these two technologies:\n\nThe virtual environments provide non-verbals and visual cues that can affect not only the believability of the VI, but also the usefulness of it. Because – like many things in technology – it's not just about \"whether or not it works\" but also about \"how we feel about it working\". Virtual Intelligence draws a new distinction as to how this application of AI is different due to the environment in which it operates.\n\n\n",
    "id": "27726566",
    "title": "Virtual intelligence"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=237629",
    "text": "Distributed artificial intelligence\n\nDistributed Artificial Intelligence (DAI) is a subfield of artificial intelligence research dedicated to the development of distributed solutions for complex problems regarded as requiring intelligence. DAI is closely related to and a predecessor of the field of Multi-Agent Systems.\n\nDistributed Artificial Intelligence (DAI) is an approach to solving complex learning, planning, and decision making problems. It is embarrassingly parallel, thus able to exploit large scale computation and spatial distribution of computing resources. These properties allow it to solve problems that require the processing of very large data sets. DAI systems consist of autonomous learning processing nodes (agents), that are distributed, often at a very large scale. DAI nodes can act independently and partial solutions are integrated by communication between nodes, often asynchronously. By virtue of their scale, DAI systems are robust and elastic, and by necessity, loosely coupled. Furthermore, DAI systems are built to be adaptive to changes in the problem definition or underlying data sets due to the scale and difficulty in redeployment.\n\nDAI systems do not require all the relevant data to be aggregated in a single location, in contrast to monolithic or centralized Artificial Intelligence systems which have tightly coupled and geographically close processing nodes. Therefore, DAI systems often operate on sub-samples or hashed impressions of very large datasets. In addition, the source dataset may change or be updated during the course of the execution of a DAI system.\n\nThe objectives of Distributed Artificial Intelligence are to solve the reasoning, planning, learning and perception problems of Artificial Intelligence, especially if they require large data, by distributing the problem to autonomous processing nodes (agents). To reach the objective DAI require:\n\nThere are many reasons for wanting to distribute intelligence or cope with multi-agent systems. Mainstreams in DAI research include the following:\n\nIn the 1975 distributed artificial intelligence emerged as a subfield of artificial intelligence that dealt with interaction of intelligent agents[2]. Distributed artificial intelligence systems were conceived as a group of intelligent entities, called agents, that interacted by cooperation, by coexistence or by competition. DAI is categorized into Multi-agent systems and distributed problem solving [1]. In Multi-agent systems the main focus is how agents coordinate their knowledge and activities. For distributed problem solving the major focus is how the problem is decomposed and the solutions are synthesized.\n\nMulti-agent systems and distributed problem solving are the two main DAI approaches. There are numerous applications and tools.\n\nTwo types of DAI has emerged: \n\nDAI can apply a bottom-up approach to AI, similar to the subsumption architecture as well as the traditional top-down\napproach of AI. In addition, DAI can also be a vehicle for emergence.\n\nAreas where DAI have been applied are:\n\n\nNotion of Agents: Agents can be described as distinct entities with standard boundaries and interfaces designed for problem solving.\n\nNotion of Multi-Agents:Multi-Agent system is defined as a network of agents which are loosely coupled working as a single entity like society for problem solving that an individual agent cannot solve.\n\nThe key concept used in DPS and MABS is the abstraction called software agents. An agent is a virtual (or physical) entity that has an understanding of its environment and acts upon it. An agent is usually able to communicate with other agents in the same system to achieve a common goal, that one agent alone could not achieve. This communication system uses an agent communication language.\n\nA first classification that is useful is to divide agents into:\n\nWell-recognized agent architectures that describe how an agent is internally structured are:\n\nThe challenges in Distributed AI are:\n\n1.How to carry out communication and interaction of agents and which communication language or protocols should be used.\n\n2.How to ensure the coherency of agents.\n\n3.How to synthesise the results among 'intelligent agents' group by formulation, description, decomposition and allocation.\n\n\nArtificial Intelligence Review, 6(1):35-66, 1992.\nintelligence, pages 187-210, 1996.\nand design support. Journal of Intelligent Manufacturing, 11(6):573-589, 2000.\n\n",
    "id": "237629",
    "title": "Distributed artificial intelligence"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=25765920",
    "text": "Artificial intelligence, situated approach\n\nIn artificial intelligence research, the situated approach builds agents that are designed to behave effectively successfully in their environment. This requires designing AI \"from the bottom-up\" by focussing on the basic perceptual and motor skills required to survive. The situated approach gives a much lower priority to abstract reasoning or problem-solving skills.\n\nThe approach was originally proposed as an alternative to traditional approaches (that is, approaches popular before 1985 or so).\nAfter several decades, classical AI technologies started to face intractable issues (e.g. combinatorial explosion) when confronted with real-world modeling problems. All approaches to address these issues focus on modeling intelligences situated in an environment. They have become known as the situated approach to AI.\n\nDuring the late 1980s, the approach now known as Nouvelle AI (\"Nouvelle\" means new in French) was pioneered at the MIT Artificial Intelligence Laboratory by Rodney Brooks. As opposed to classical or traditional artificial intelligence, Nouvelle AI purposely avoided the traditional goal of modeling human-level performance, but rather tries to create systems with intelligence at the level of insects, closer to real-world robots. But eventually, at least at MIT new AI did lead to an attempt for humanoid AI in the Cog Project.\n\nThe conceptual shift introduced by nouvelle AI flourished in the robotics area, given way to behavior-based artificial intelligence (BBAI), a methodology for developing AI based on a modular decomposition of intelligence. It was made famous by Rodney Brooks: his subsumption architecture was one of the earliest attempts to describe a mechanism for developing BBAI. It is extremely popular in robotics and to a lesser extent to implement intelligent virtual agents because it allows the successful creation of real-time dynamic systems that can run in complex environments. For example, it underlies the intelligence of the Sony, Aibo and many RoboCup robot teams.\n\nRealizing that in fact all these approaches were aiming at building not an abstract intelligence, but rather an intelligence situated in a given environment, they have come to be known as the situated approach. In fact, this approach stems out from early insights of Alan Turing, describing the need to build machines equipped with sense organs to learn directly from the real-world instead of focusing on abstract activities, such as playing chess.\n\nClassically, a software entity is defined as a simulated element, able to act on itself and on its environment, and which has an internal representation of itself and of the outside world. An entity can communicate with other entities, and its behavior is the consequence of its perceptions, its representations, and its interactions with the other entities.\n\nSimulating entities in a virtual environment requires simulating the entire process that goes from a perception of the environment, or more generally from a stimulus, to an action on the environment. This process is called the AI loop and technology used to simulate it can be subdivided in two categories. \"Sensorimotor or low-level\" AI deals with either the perception problem (what is perceived?) or the animation problem (how are actions executed?). \"Decisional or high-level\" AI deals with the action selection problem (what is the most appropriate action in response to a given perception, i.e. what is the most appropriate behavior?).\n\nThere are two main approaches in decisional AI. The vast majority of the technologies available on the market, such as planning algorithms, finite state machines (FSA), or expert systems, are based on the traditional or symbolic AI approach. Its main characteristics are:\n\nHowever, the limits of traditional AI, which goal is to build systems that mimic human intelligence, are well-known: inevitably, a combinatorial explosion of the number of rules occurs due to the complexity of the environment. In fact, it is impossible to predict all the situations that will be encountered by an autonomous entity.\n\nIn order to address these issues, another approach to decisional AI, also known as situated or behavioral AI, has been proposed. It does not attempt to model systems that produce deductive reasoning processes, but rather systems that behave realistically in their environment. The main characteristics of this approach are the following:\n\nThe goal of situated AI is to model entities that are autonomous in their environment. This is achieved thanks to both the intrinsic robustness of the control architecture, and its adaptation capabilities to unforeseen situations.\n\nIn artificial intelligence and cognitive science, the term situated refers to an agent which is embedded in an environment. The term \"situated\" is commonly used to refer to robots, but some researchers argue that software agents can also be situated if:\n\n\nExamples might include web-based agents, which can alter data or trigger processes (such as purchases) over the Internet, or virtual-reality bots which inhabit and change virtual worlds, such as Second Life.\n\nBeing situated is generally considered to be part of being embodied, but it is useful to consider each perspective individually. The situated perspective emphasizes that intelligent behavior derives from the environment and the agent's interactions with it. The nature of these interactions are defined by an agent's embodiment.\n\nThe most important attribute of a system driven by situated AI is that the intelligence is controlled by a set of independent semi- modules. In the original systems, each module was actually a separate device or was at least conceived of as running on its own processing thread. Generally, though, the modules are just abstractions. In this respect, situated AI may be seen as a software engineering approach to AI, perhaps akin to object oriented design.\n\nSituated AI is often associated with reactive planning, but the two are not synonymous. Brooks advocated an extreme version of cognitive minimalism which required initially that the behavior modules were finite state machines and thus contained no conventional memory or learning. This is associated with reactive AI because reactive AI requires reacting to the current state of the world, not to an agent's memory or preconception of that world. However, learning is obviously key to realistic strong AI, so this constraint has been relaxed, though not entirely abandoned.\n\nThe situated AI community has presented several solutions to modeling decision-making processes, also known as action selection mechanisms. The first attempt to solve this problem goes back to \"subsumption architectures\", which were in fact more an implementation technique than an algorithm. However, this attempt paved the way to several others, in particular the \"free-flow hierarchies\" and \"activation networks\". A comparison of the structure and performances of these two mechanisms demonstrated the advantage of using \"free-flow hierarchies\" in solving the action selection problem. However, \"motor schemas\" and \"process description languages\" are two other approaches that have been used with success for autonomous robots.\n\n\n\n\n\n\n",
    "id": "25765920",
    "title": "Artificial intelligence, situated approach"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=16070200",
    "text": "Intelligent decision support system\n\nAn intelligent decision support system (IDSS) is a decision support system that makes extensive use of artificial intelligence (AI) techniques. Use of AI techniques in management information systems has a long history – indeed terms such as \"Knowledge-based systems\" (KBS) and \"intelligent systems\" have been used since the early 1980s to describe components of management systems, but the term \"Intelligent decision support system\" is thought to originate with Clyde Holsapple and Andrew Whinston in the late 1970s. Examples of specialized intelligent decision support systems include Flexible manufacturing systems (FMS), intelligent marketing decision support systems and medical diagnosis systems.\n\nIdeally, an intelligent decision support system should behave like a human consultant: supporting decision makers by gathering and analysing evidence, identifying and diagnosing problems, proposing possible courses of action and evaluating such proposed actions. The aim of the AI techniques embedded in an intelligent decision support system is to enable these tasks to be performed by a computer, while emulating human capabilities as closely as possible.\n\nMany IDSS implementations are based on expert systems, a well established type of KBS that encode knowledge and emulate the cognitive behaviours of human experts using predicate logic rules, and have been shown to perform better than the original human experts in some circumstances. Expert systems emerged as practical applications in the 1980s based on research in artificial intelligence performed during the late 1960s and early 1970s. They typically combine knowledge of a particular application domain with an inference capability to enable the system to propose decisions or diagnoses. Accuracy and consistency can be comparable to (or even exceed) that of human experts when the decision parameters are well known (e.g. if a common disease is being diagnosed), but performance can be poor when novel or uncertain circumstances arise.\n\nResearch in AI focused on enabling systems to respond to novelty and uncertainty in more flexible ways is starting to be used in IDSS. For example, intelligent agents that perform complex cognitive tasks without any need for human intervention have been used in a range of decision support applications. Capabilities of these intelligent agents include knowledge sharing, machine learning, data mining, and automated inference. A range of AI techniques such as case based reasoning, rough sets and fuzzy logic have also been used to enable decision support systems to perform better in uncertain conditions.\n\n\n",
    "id": "16070200",
    "title": "Intelligent decision support system"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=27928333",
    "text": "Conference on Semantics in Healthcare and Life Sciences\n\nThe Conference on Semantics in Healthcare and Life Sciences (CSHALS) was a scientific meeting on the practical applications of semantic technology to pharmaceutical R&D, healthcare, and life sciences. The conference was held annually from 2008 to 2014, and was the premier meeting in this domain. In 2015, it as organised as a special interests group (SIG) meeting of the Intelligent Systems for Molecular Biology conference.\n\nConference topics included a World Wide Web Consortium tutorial on semantic web standards. In 2012, CSHALS took on the additional topic of the application of Big Data to healthcare and life sciences, while maintaining its previous focus on semantic technologies.\n\nCSHALS was organized by ISCB (International Society for Computational Biology).\n\n\n",
    "id": "27928333",
    "title": "Conference on Semantics in Healthcare and Life Sciences"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=27370978",
    "text": "Singleton (global governance)\n\nIn futurology, a singleton is a hypothetical world order in which there is a single decision-making agency at the highest level, capable of exerting effective control over its domain, and permanently preventing both internal and external threats to its supremacy. The term has first been defined by Nick Bostrom.\n\nAn artificial general intelligence having undergone an intelligence explosion could form a singleton, as could a world government armed with mind control and social surveillance technologies. A singleton need not directly micromanage everything in its domain; it could allow diverse forms of organization within itself, albeit guaranteed to function within strict parameters. A singleton need not support a civilization, and in fact could obliterate it upon coming to power.\n\nA singleton doesn't necessarily need to be an organisation, government or AI. It could form through all people accepting the same values or goals. These people coming together would form an \"agency\" in the broad sense of the term according to Bostrom. \n\nA singleton has both potential risks and potential benefits. Notably, a suitable singleton could solve world coordination problems that would not otherwise be solvable, opening up otherwise unavailable developmental trajectories for civilization. For example, Ben Goertzel, an AGI researcher, suggests humans may instead decide to create an \"AI Nanny\" with \"mildly superhuman intelligence and surveillance powers\", to protect the human race from existential risks like nanotechnology and to delay the development of other (unfriendly) artificial intelligences until and unless the safety issues are solved. Furthermore, Bostrom suggests that a singleton could hold Darwinian evolutionary pressures in check, preventing agents interested only in reproduction from coming to dominate.\n\nYet Bostrom also regards the possibility of a stable, repressive, totalitarian global regime as a serious existential risk. The very stability of a singleton makes the installation of a \"bad\" singleton especially catastrophic, since the consequences can never be undone. Bryan Caplan writes that \"perhaps an eternity of totalitarianism would be worse than extinction\".\n\nSimilarly Hans Morgenthau stressed that the mechanical development of weapons, transportation, and communication makes \"the conquest of the world technically possible, and they make it technically possible to keep the world in that conquered state\". Its lack was the reason why great ancient empires, though vast, failed to complete universal conquest of their world and perpetuate the conquest. Now, however, this is possible. Technology undoes both geographic and climatic barriers. \"Today no technological obstacle stands in the way of a world-wide empire\", as \"modern technology makes it possible to extend the control of mind and action to every corner of the globe regardless of geography and season.\" Morgenthau continued on the technological progress:\n",
    "id": "27370978",
    "title": "Singleton (global governance)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1706303",
    "text": "Recurrent neural network\n\nA recurrent neural network (RNN) is a class of artificial neural network where connections between units form a directed cycle. This allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. This makes them applicable to tasks such as unsegmented, connected handwriting recognition or speech recognition.\nRecurrent neural networks were developed in the 1980s. Hopfield networks were invented by John Hopfield in 1982. In 1993, a neural history compressor system solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time.\n\nLong short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1997 and set accuracy records in multiple applications domains.\n\nAround 2007, LSTM started to revolutionize speech recognition, outperforming traditional models in certain speech applications. In 2009, a Connectionist Temporal Classification (CTC)-trained LSTM network was the first RNN to win pattern recognition contests when it won several competitions in connected handwriting recognition. In 2014, the Chinese search giant Baidu used CTC-trained RNNs to break the Switchboard Hub5'00 speech recognition benchmark without using any traditional speech processing methods.\n\nLSTM also improved large-vocabulary speech recognition and text-to-speech synthesis and was used in Google Android. In 2015, Google's speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM, which was used by Google voice search.\n\nLSTM broke records for improved machine translation, Language Modeling and Multilingual Language Processing. LSTM combined with convolutional neural networks (CNNs) improved automatic image captioning.\n\nRNNs come in many variants.\n\nBasic RNNs are a network of neuron-like nodes, each with a directed (one-way) connection to every other node. Each node (neuron) has a time-varying real-valued activation. Each connection (synapse) has a modifiable real-valued weight. Nodes are either input nodes (receiving data from outside the network), output nodes (yielding results), or hidden nodes (that modify the data en route from input to output).\n\nFor supervised learning in discrete time settings, sequences of real-valued input vectors arrive at the input nodes, one vector at a time. At any given time step, each non-input unit computes its current activation (result) as a nonlinear function of the weighted sum of the activations of all units that connect to it. Supervisor-given target activations can be supplied for some output units at certain time steps. For example, if the input sequence is a speech signal corresponding to a spoken digit, the final target output at the end of the sequence may be a label classifying the digit.\n\nIn reinforcement learning settings, no teacher provides target signals. Instead a fitness function or reward function is occasionally used to evaluate the RNN's performance, which influences its input stream through output units connected to actuators that affect the environment. This might be used to play a game in which progress is measured with the number of points won.\n\nEach sequence produces an error as the sum of the deviations of all target signals from the corresponding activations computed by the network. For a training set of numerous sequences, the total error is the sum of the errors of all individual sequences.\n\nA recursive neural network is created by applying the same set of weights recursively over a differentiable graph-like structure by traversing the structure in topological order. Such networks are typically also trained by the reverse mode of automatic differentiation. They can process distributed representations of structure, such as logical terms. A special case of recursive neural networks is the RNN whose structure corresponds to a linear chain. Recursive neural networks have been applied to natural language processing. The Recursive Neural Tensor Network uses a tensor-based composition function for all nodes in the tree.\n\nThe Hopfield network is an RNN in which all connections are symmetric. It requires stationary inputs and is thus not a general RNN, as it does not process sequences of patterns. It guarantees that it will converge. If the connections are trained using Hebbian learning then the Hopfield network can perform as robust content-addressable memory, resistant to connection alteration.\n\nIntroduced by Kosko, a bidirectional associative memory (BAM) network is a variant of a Hopfield network that stores associative data as a vector. The bi-directionality comes from passing information through a matrix and its transpose. Typically, bipolar encoding is preferred to binary encoding of the associative pairs. Recently, stochastic BAM models using Markov stepping were optimized for increased network stability and relevance to real-world applications.\n\nA BAM network has two layers, either of which can be driven as an input to recall an association and produce an output on the other layer.\n\nAn Elman network is a three-layer network (arranged horizontally as \"x\", \"y\", and \"z\" in the illustration) with the addition of a set of \"context units\" (\"u\" in the illustration). The middle (hidden) layer is connected to these context units fixed with a weight of one. At each time step, the input is fed-forward and a learning rule is applied. The fixed back-connections save a copy of the previous values of the hidden units in the context units (since they propagate over the connections before the learning rule is applied). Thus the network can maintain a sort of state, allowing it to perform such tasks as sequence-prediction that are beyond the power of a standard multilayer perceptron.\n\nJordan networks are similar to Elman networks. The context units are fed from the output layer instead of the hidden layer. The context units in a Jordan network are also referred to as the state layer. They have a recurrent connection to themselves.\n\nElman and Jordan networks are also known as \"simple recurrent networks\" (SRN).\n\n\nVariables and functions\n\nThe echo state network (ESN) has a sparsely connected random hidden layer. The weights of output neurons are the only part of the network that can change (be trained). ESNs are good at reproducing certain time series. A variant for spiking neurons is known as a liquid state machine.\n\nThe neural history compressor is an unsupervised stack of RNNs. At the input level, it learns to predict its next input from the previous inputs. Only unpredictable inputs of some RNN in the hierarchy become inputs to the next higher level RNN, which therefore recomputes its internal state only rarely. Each higher level RNN thus studies a compressed representation of the information in the RNN below. This is done such that the input sequence can be precisely reconstructed from the representation at the highest level.\n\nThe system effectively minimises the description length or the negative logarithm of the probability of the data. Given a lot of learnable predictability in the incoming data sequence, the highest level RNN can use supervised learning to easily classify even deep sequences with long intervals between important events.\n\nIt is possible to distill the RNN hierarchy into two RNNs: the \"conscious\" chunker (higher level) and the \"subconscious\" automatizer (lower level). Once the chunker has learned to predict and compress inputs that are unpredictable by the automatizer, then the automatizer can be forced in the next learning phase to predict or imitate through additional units the hidden units of the more slowly changing chunker. This makes it easy for the automatizer to learn appropriate, rarely changing memories across long intervals. In turn this helps the automatizer to make many of its once unpredictable inputs predictable, such that the chunker can focus on the remaining unpredictable events.\n\nA generative model partially overcame the vanishing gradient problem of automatic differentiation or backpropagation in neural networks in 1992. In 1993, such a system solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time.\n\nLong short-term memory (LSTM) is a deep learning system that avoids the vanishing gradient problem. LSTM is normally augmented by recurrent gates called \"forget\" gates. LSTM prevents backpropagated errors from vanishing or exploding. Instead, errors can flow backwards through unlimited numbers of virtual layers unfolded in space. That is, LSTM can learn tasks that require memories of events that happened thousands or even millions of discrete time steps earlier. Problem-specific LSTM-like topologies can be evolved. LSTM works even given long delays between significant events and can handle signals that mix low and high frequency components.\n\nMany applications use stacks of LSTM RNNs and train them by Connectionist Temporal Classification (CTC) to find an RNN weight matrix that maximizes the probability of the label sequences in a training set, given the corresponding input sequences. CTC achieves both alignment and recognition.\n\nLSTM can learn to recognize context-sensitive languages unlike previous models based on hidden Markov models (HMM) and similar concepts.\n\nSecond order RNNs use higher order weights formula_11 instead of the standard formula_12 weights, and inputs and states can be a product. This allows a direct mapping to a finite state machine both in training, stability, and representation. Long short-term memory is an example of this but has no such formal mappings or proof of stability.\n\nGated recurrent units (GRUs) are a gating mechanism in recurrent neural networks introduced in 2014. Their performance on polyphonic music modeling and speech signal modeling was found to be similar to that of long short-term memory. They have fewer parameters than LSTM, as they lack an output gate.\n\nBi-directional RNNs use a finite sequence to predict or label each element of the sequence based on the element's past and future contexts. This is done by concatenating the outputs of two RNNs, one processing the sequence from left to right, the other one from right to left. The combined outputs are the predictions of the teacher-given target signals. This technique proved to be especially useful when combined with LSTM RNNs.\n\nA continuous time recurrent neural network (CTRNN) uses a system of ordinary differential equations to model the effects on a neuron of the incoming spike train.\n\nFor a neuron formula_13 in the network with action potential formula_14, the rate of change of activation is given by:\nWhere:\n\nCTRNNs have been applied to evolutionary robotics where they have been used to address vision, co-operation, and minimal cognitive behaviour.\n\nNote that, by the Shannon sampling theorem, discrete time recurrent neural networks can be viewed as continuous-time recurrent neural networks where the differential equations have transformed into equivalent difference equations. This transformation can be thought of as occurring after the post-synaptic node activation functions formula_25 have been low-pass filtered but prior to sampling.\n\nHierarchical RNNs connect their neurons in various ways to decompose hierarchical behavior into useful subprograms.\n\nGenerally, a Recurrent Multi-Layer Perceptron (RMLP) network consists of cascaded subnetworks, each of which contains multiple layers of nodes. Each of these subnetworks is feed-forward except for the last layer, which can have feedback connections. Each of these subnets is connected only by feed forward connections.\n\nA multiple timescales recurrent neural network (MTRNN) is a neural-based computational model that can simulate the functional hierarchy of the brain through self-organization that depends on spatial connection between neurons and on distinct types of neuron activities, each with distinct time properties. With such varied neuronal activities, continuous sequences of any set of behaviors are segmented into reusable primitives, which in turn are flexibly integrated into diverse sequential behaviors. The biological approval of such a type of hierarchy was discussed in the memory-prediction theory of brain function by Hawkins in his book \"On Intelligence\".\n\nNeural Turing machines (NTMs) are a method of extending recurrent neural networks by coupling them to external memory resources which they can interact with by attentional processes. The combined system is analogous to a Turing machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent.\n\nDifferentiable neural computers (DNCs) are an extension of Neural Turing machines, allowing for usage of fuzzy amounts of each memory address and a record of chronology.\n\nNeural network pushdown automata (NNPDA) are similar to NTMs, but tapes are replaced by analogue stacks that are differentiable and that are trained. In this way, they are similar in complexity to recognizers of context free grammars (CFGs).\n\nGradient descent is a iterative optimization algorithm for finding the minimum of a function. In neural networks, it can be used to minimize the error term by changing each weight in proportion to the derivative of the error with respect to that weight, provided the non-linear activation functions are differentiable. Various methods for doing so were developed in the 1980s and early 1990s by Werbos, Williams, Robinson, Schmidhuber, Hochreiter, Pearlmutter and others.\n\nThe standard method is called \"backpropagation through time\" or BPTT, and is a generalization of back-propagation for feed-forward networks. Like that method, it is an instance of automatic differentiation in the reverse accumulation mode of Pontryagin's minimum principle. A more computationally expensive online variant is called \"Real-Time Recurrent Learning\" or RTRL, which is an instance of automatic differentiation in the forward accumulation mode with stacked tangent vectors. Unlike BPTT, this algorithm is local in time but not local in space.\n\nIn this context, local in space means that a unit's weight vector can be updated using only information stored in the connected units and the unit itself such that update complexity of a single unit is linear in the dimensionality of the weight vector. Local in time means that the updates take place continually (on-line) and depend only on the most recent time step rather than on multiple time steps within a given time horizon as in BPTT. Biological neural networks appear to be local with respect to both time and space.\n\nFor recursively computing the partial derivatives, RTRL has a time-complexity of O(number of hidden x number of weights) per time step for computing the Jacobian matrices, while BPTT only takes O(number of weights) per time step, at the cost of storing all forward activations within the given time horizon. An online hybrid between BPTT and RTRL with intermediate complexity exists, along with variants for continuous time.\n\nA major problem with gradient descent for standard RNN architectures is that error gradients vanish exponentially quickly with the size of the time lag between important events. LSTM combined with a BPTT/RTRL hybrid learning method attempts to overcome these problems.\n\nThe on-line algorithm called causal recursive backpropagation (CRBP), implements and combines BPTT and RTRL paradigms for locally recurrent networks. It works with the most general locally recurrent networks. The CRBP algorithm can minimize the global error term. This fact improves stability of the algorithm, providing a unifying view on gradient calculation techniques for recurrent networks with local feedback.\n\nOne approach to the computation of gradient information in RNNs with arbitrary architectures is based on signal-flow graphs diagrammatic derivation. It uses the BPTT batch algorithm, based on Lee's theorem for network sensitivity calculations. It was proposed by Wan and Beaufays, while its fast online version was proposed by Campolucci, Uncini and Piazza.\n\nTraining the weights in a neural network can be modeled as a non-linear global optimization problem. A target function can be formed to evaluate the fitness or error of a particular weight vector as follows: First, the weights in the network are set according to the weight vector. Next, the network is evaluated against the training sequence. Typically, the sum-squared-difference between the predictions and the target values specified in the training sequence is used to represent the error of the current weight vector. Arbitrary global optimization techniques may then be used to minimize this target function.\n\nThe most common global optimization method for training RNNs is genetic algorithms, especially in unstructured networks.\n\nInitially, the genetic algorithm is encoded with the neural network weights in a predefined manner where one gene in the chromosome represents one weight link.The whole network is represented as a single chromosome. The fitness function is evaluated as follows: \nMany chromosomes make up the population; therefore, many different neural networks are evolved until a stopping criterion is satisfied. A common stopping scheme is: \nThe stopping criterion is evaluated by the fitness function as it gets the reciprocal of the mean-squared-error from each network during training. Therefore, the goal of the genetic algorithm is to maximize the fitness function, reducing the mean-squared-error.\n\nOther global (and/or evolutionary) optimization techniques may be used to seek a good set of weights, such as simulated annealing or particle swarm optimization.\n\nRNNs may behave chaotically. In such cases, dynamical systems theory may be used for analysis.\n\nThey are in fact recursive neural networks with a particular structure: that of a linear chain. Whereas recursive neural networks operate on any hierarchical structure, combining child representations into parent representations, recurrent neural networks operate on the linear progression of time, combining the previous time step and a hidden representation into the representation for the current time step.\n\nIn particular, RNNs can appear as nonlinear versions of finite impulse response and infinite impulse response filters and also as a nonlinear autoregressive exogenous model (NARX).\n\n\nApplications of Recurrent Neural Networks include:\n\n",
    "id": "1706303",
    "title": "Recurrent neural network"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=25011536",
    "text": "Shyster (expert system)\n\nSHYSTER is a legal expert system developed at the Australian National University in Canberra in 1993. It was written as the doctoral dissertation of James Popple under the supervision of Robin Stanton, Roger Clarke, Peter Drahos, and Malcolm Newey. A full technical report of the expert system, and a book further detailing its development and testing have also been published.\n\nSHYSTER emphasises its pragmatic approach, and posits that a legal expert system need not be based upon a complex model of legal reasoning in order to produce useful advice. Although SHYSTER attempts to model the way in which lawyers argue with cases, it does not attempt to model the way in which lawyers decide which cases to use in those arguments. SHYSTER is of a general design, permitting its operation in different legal domains. It was designed to provide advice in areas of case law that have been specified by a legal expert using a bespoke specification language. Its knowledge of the law is acquired, and represented, as information about cases. It produces its advice by examining, and arguing about, the similarities and differences between cases. It derives its name from Shyster: a slang word for someone who acts in a disreputable, unethical, or unscrupulous way, especially in the practice of law and politics.\n\nSHYSTER is a specific example of a general category of legal expert systems, broadly defined as systems that make use of artificial intelligence (AI) techniques to solve legal problems. Legal AI systems can be divided into two categories: legal retrieval systems and legal analysis systems. SHYSTER belongs to the latter category of legal analysis systems. Legal analysis systems can be further subdivided into two categories: judgment machines and legal expert systems. SHYSTER again belongs to the latter category of legal expert systems. A legal expert system, as Popple uses the term, is a system capable of performing at a level expected of a lawyer: “AI systems which merely assist a lawyer in coming to legal conclusions or preparing legal arguments are not here considered to be legal expert systems; a legal expert system must exhibit some legal expertise itself.\"\n\nDesigned to operate in more than one legal domain, and be of specific use to the common law of Australia, SHYSTER accounts for statute law, case law, and the doctrine of precedent in areas of private law. Whilst it accommodates statute law, it is primarily a case-based system, in contradistinction to rule-based systems like MYCIN. More specifically, it was designed in a manner enabling it to be linked with a rule-based system to form a hybrid system. Although case-based reasoning possesses an advantage over rule-based systems by the elimination of complex semantic networks, it suffers from intractable theoretical obstacles: without some further theory it cannot be predicted what features of a case will turn out to be relevant. Users of SHYSTER therefore require some legal expertise.\n\nRichard Susskind argues that “jurisprudence can and ought to supply the models of law and legal reasoning that are required for computerized [sic] implementation in the process of building all expert systems in law.” Popple, however, believes jurisprudence is of limited value to developers of legal expert systems. He posits that a lawyer must have a model of the law (maybe unarticulated) which includes assumptions about the nature of law and legal reasoning, but that model need not rest on basic philosophical foundations. It may be a pragmatic model, developed through experience within the legal system. Many lawyers perform their work with little or no jurisprudential knowledge, and there is no evidence to suggest that they are worse, or better, at their jobs than lawyers well-versed in jurisprudence. The fact that many lawyers have mastered the process of legal reasoning, without having been immersed in jurisprudence, suggests that it may indeed be possible to develop legal expert systems of good quality without jurisprudential insight. As a pragmatic legal expert system SHYSTER is the embodiment of this belief.\n\nA further example of SHYSTER’s pragmatism is its simple knowledge representation structure. This structure was designed to facilitate specification of different areas of case law using a specification language. Areas of case law are specified in terms of the cases and attributes of importance in those areas. SHYSTER weights its attributes and checks for dependence between them. In order to choose cases upon which to construct its opinions, SHYSTER calculates distances between cases and uses these distances to determine which of the leading cases are nearest to the instant case. To this end SHYSTER can be seen to adopt and expand upon nearest neighbor search methods used in pattern recognition. These nearest cases are used to produce an argument (based on similarities and differences between the cases) about the likely outcome in the instant case. This argument relies on the doctrine of precedent; it assumes that the instant case will be decided the same way as was the nearest case. SHYSTER then uses information about these nearest cases to construct a report. The report that SHYSTER generates makes a prediction and justifies that prediction by reference only to cases and their similarities and differences: the calculations that SHYSTER performs in coming to its opinion do not appear in that opinion. Safeguards are employed to warn users if SHYSTER doubts the veracity of its advice.\n\nSHYSTER was tested in four different and disparate areas of case law. Four specifications were written, each representing an area of Australian law: an aspect of the law of trover; the meaning of “authorization [sic]” in Australian copyright law; the categorisation of employment contracts; and the implication of natural justice in administrative decision-making. SHYSTER was evaluated under five headings: its usefulness, its generality, the quality of its advice, its limitations, and possible enhancements that could be made to it. Despite its simple knowledge representation structure, it has shown itself capable of producing good advice, and its simple structure has facilitated the specification of different areas of law.\n\nAppreciating the difficulties encountered by legal expert systems developers in adequately representing legal knowledge can assist in appreciating the shortcomings of digital rights management technologies. Some academics believe future digital rights management systems may become sophisticated enough to permit exceptions to copyright law. To this end SHYSTER's attempt to model “authorization [sic]” in the Copyright Act can be viewed as pioneering work in this field. The term “authorization [sic]” is undefined in the Copyright Act. Consequently, a number of cases have been before the courts seeking answers as to what conduct amounts to authorisation. The main contexts in which the issue has arisen are analogous to permitted exceptions to copyright currently prevented by most digital rights management technologies: “home taping of recorded materials, photocopying in educational institutions and performing works in public”. When applied to one case concerning compact cassettes, SHYSTER successfully agreed that Amstrad did not authorise the infringement.\n\nPopple highlighted the most obvious avenue of future research using SHYSTER as the development of a rule-based system, and the linking together of that rule-based system with the existing case-based system to form a hybrid system. This intention was eventually realised by Thomas O’Callaghan, the creator of SHYSTER-MYCIN: a hybrid legal expert system first presented at ICAIL '03, 24–28 June 2003 in Edinburgh, Scotland.\n\nMYCIN is an existing medical expert system, which was adapted for use with SHYSTER. MYCIN’s controversial “certainty factor” is not used in SHYSTER-MYCIN. The reason for this is the difficulty in scientifically establishing how certain a fact is in a legal domain. The rule-based approach of the MYCIN part is used to reason with the provisions of an Act of Parliament only. This hybrid system enables the case-based system (SHYSTER) to determine open textured concepts when required by the rule-based system (MYCIN). The ultimate conclusion of this joint endeavour is that a hybrid approach is preferred in the creation of legal expert systems where “it is appropriate to use rule-based reasoning when dealing with statutes, and…case-based reasoning when dealing with cases”.\n\n\n",
    "id": "25011536",
    "title": "Shyster (expert system)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=27669989",
    "text": "Ensemble averaging (machine learning)\n\nIn machine learning, particularly in the creation of artificial neural networks, ensemble averaging is the process of creating multiple models and combining them to produce a desired output, as opposed to creating just one model. Frequently an ensemble of models performs better than any individual model, because the various errors of the models \"average out.\"\n\nEnsemble averaging is one of the simplest types of committee machines. Along with boosting, it is one of the two major types of static committee machines. In contrast to standard network design in which many networks are generated but only one is kept, ensemble averaging keeps the less satisfactory networks around, but with less weight. The theory of ensemble averaging relies on two properties of artificial neural networks:\n\n\nEnsemble averaging creates a group of networks, each with low bias and high variance, then combines them to a new network with (hopefully) low bias and low variance. It is thus a resolution of the bias-variance dilemma. The idea of combining experts has been traced back to Pierre-Simon Laplace.\n\nThe theory mentioned above gives an obvious strategy: create a set of experts with low bias and high variance, and then average them. Generally, what this means is to create a set of experts with varying parameters; frequently, these are the initial synaptic weights, although other factors (such as the learning rate, momentum etc.) may be varied as well. Some authors recommend against varying weight decay and early stopping. The steps are therefore:\nAlternatively, domain knowledge may be used to generate several \"classes\" of experts. An expert from each class is trained, and then combined.\n\nA more complex version of ensemble average views the final result not as a mere average of all the experts, but rather as a weighted sum. If each expert is formula_1, then the overall result formula_2 can be defined as:\nwhere formula_4 is a set of weights. The optimization problem of finding alpha is readily solved through neural networks, hence a \"meta-network\" where each \"neuron\" is in fact an entire neural network can be trained, and the synaptic weights of the final network is the weight applied to each expert. This is known as a \"linear combination of experts\".\n\nIt can be seen that most forms of neural networks are some subset of a linear combination: the standard neural net (where only one expert is used) is simply a linear combination with all formula_5 and one formula_6. A raw average is where all formula_7 are equal to some constant value, namely one over the total number of experts.\n\nA more recent ensemble averaging method is negative correlation learning, proposed by Y. Liu and X. Yao. Now this method has been widely used in evolutionary computing.\n\n\n\n",
    "id": "27669989",
    "title": "Ensemble averaging (machine learning)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=13911207",
    "text": "Attributional calculus\n\nAttributional calculus is a logic and representation system defined by Ryszard S. Michalski. It combines elements of predicate logic, propositional calculus, and multi-valued logic. Attributional calculus provides a formal language for \"natural induction\", an inductive learning process whose results are in forms natural to people.\n\nMichalski, R.S., \"ATTRIBUTIONAL CALCULUS: A Logic and Representation Language for Natural Induction,\" Reports of the Machine Learning and Inference Laboratory, MLI 04-2, George Mason University, Fairfax, VA, April, 2004.\n",
    "id": "13911207",
    "title": "Attributional calculus"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=20756850",
    "text": "Collective intelligence\n\nCollective intelligence (CI) is shared or group intelligence that emerges from the collaboration, collective efforts, and competition of many individuals and appears in consensus decision making. The term appears in sociobiology, political science and in context of mass peer review and crowdsourcing applications. It may involve consensus, social capital and formalisms such as voting systems, social media and other means of quantifying mass activity. Collective IQ is a measure of collective intelligence, although it is often used interchangeably with the term collective intelligence. Collective intelligence has also been attributed to bacteria and animals.\n\nIt can be understood as an emergent property from the synergies among: 1) data-information-knowledge; 2) software-hardware; and 3) experts (those with new insights as well as recognized authorities) that continually learns from feedback to produce just-in-time knowledge for better decisions than these three elements acting alone. Or more narrowly as an emergent property between people and ways of processing information. This notion of collective intelligence is referred to as \"symbiotic intelligence\" by Norman Lee Johnson. The concept is used in sociology, business, computer science and mass communications: it also appears in science fiction. Pierre Lévy defines collective intelligence as, \"It is a form of universally distributed intelligence, constantly enhanced, coordinated in real time, and resulting in the effective mobilization of skills. I'll add the following indispensable characteristic to this definition: The basis and goal of collective intelligence is mutual recognition and enrichment of individuals rather than the cult of fetishized or hypostatized communities.\" According to researchers Pierre Lévy and Derrick de Kerckhove, it refers to capacity of networked ICTs (Information communication technologies) to enhance the collective pool of social knowledge by simultaneously expanding the extent of human interactions.\n\nCollective intelligence strongly contributes to the shift of knowledge and power from the individual to the collective. According to Eric S. Raymond (1998) and JC Herz (2005), open source intelligence will eventually generate superior outcomes to knowledge generated by proprietary software developed within corporations (Flew 2008). Media theorist Henry Jenkins sees collective intelligence as an 'alternative source of media power', related to convergence culture. He draws attention to education and the way people are learning to participate in knowledge cultures outside formal learning settings. Henry Jenkins criticizes schools which promote 'autonomous problem solvers and self-contained learners' while remaining hostile to learning through the means of collective intelligence. Both Pierre Lévy (2007) and Henry Jenkins (2008) support the claim that collective intelligence is important for democratization, as it is interlinked with knowledge-based culture and sustained by collective idea sharing, and thus contributes to a better understanding of diverse society.\n\nSimilar to the \"g\" factor (\"g\") for general individual intelligence, a new scientific understanding of collective intelligence aims to extract a general collective intelligence factor c factor for groups indicating a group's ability to perform a wide range of tasks. Definition, operationalization and statistical methods are derived from \"g\". Similarly as \"g\" is highly interrelated with the concept of IQ, this measurement of collective intelligence can be interpreted as intelligence quotient for groups (Group-IQ) even though the score is not a quotient per se. Causes for \"c\" and predictive validity are investigated as well.\n\nWriters who have influenced the idea of collective intelligence include Douglas Hofstadter (1979), Peter Russell (1983), Tom Atlee (1993), Pierre Lévy (1994), Howard Bloom (1995), Francis Heylighen (1995), Douglas Engelbart, Louis Rosenberg, Cliff Joslyn, Ron Dembo, Gottfried Mayer-Kress (2003).\n\nThe concept (although not so named) originated in 1785 with the Marquis de Condorcet, whose \"jury theorem\" states that if each member of a voting group is more likely than not to make a correct decision, the probability that the highest vote of the group is the correct decision increases with the number of members of the group (see Condorcet's jury theorem). Many theorists have interpreted Aristotle's statement in the Politics that \"a feast to which many contribute is better than a dinner provided out of a single purse\" to mean that just as many may bring different dishes to the table, so in a deliberation many may contribute different pieces of information to generate a better decision. Recent scholarship, however, suggests that this was probably not what Aristotle meant but is a modern interpretation based on what we now know about team intelligence.\n\nA precursor of the concept is found in entomologist William Morton Wheeler's observation that seemingly independent individuals can cooperate so closely as to become indistinguishable from a single organism (1911). Wheeler saw this collaborative process at work in ants that acted like the cells of a single beast he called a superorganism.\n\nIn 1912 Émile Durkheim identified society as the sole source of human logical thought. He argued in \"The Elementary Forms of Religious Life\" that society constitutes a higher intelligence because it transcends the individual over space and time. Other antecedents are Vladimir Vernadsky's concept of \"noosphere\" and H.G. Wells's concept of \"world brain\" (see also the term \"global brain\"). Peter Russell, Elisabet Sahtouris, and Barbara Marx Hubbard (originator of the term \"conscious evolution\") are inspired by the visions of a noosphere – a transcendent, rapidly evolving collective intelligence – an informational cortex of the planet. The notion has more recently been examined by the philosopher Pierre Lévy. In a 1962 research report, Douglas Engelbart linked collective intelligence to organizational effectiveness, and predicted that pro-actively 'augmenting human intellect' would yield a multiplier effect in group problem solving: \"Three people working together in this augmented mode [would] seem to be more than three times as effective in solving a complex problem as is one augmented person working alone\". In 1994, he coined the term 'collective IQ' as a measure of collective intelligence, to focus attention on the opportunity to significantly raise collective IQ in business and society.\n\nThe idea of collective intelligence also forms the framework for contemporary democratic theories often referred to as epistemic democracy. Epistemic democratic theories refer to the capacity of the populace, either through deliberation or aggregation of knowledge, to track the truth and relies on mechanisms to synthesize and apply collective intelligence.\n\nHoward Bloom has discussed mass behavior – collective behavior from the level of quarks to the level of bacterial, plant, animal, and human societies. He stresses the biological adaptations that have turned most of this earth's living beings into components of what he calls \"a learning machine\". In 1986 Bloom combined the concepts of apoptosis, parallel distributed processing, group selection, and the superorganism to produce a theory of how collective intelligence works. Later he showed how the collective intelligences of competing bacterial colonies and human societies can be explained in terms of computer-generated \"complex adaptive systems\" and the \"genetic algorithms\", concepts pioneered by John Holland.\n\nBloom traced the evolution of collective intelligence to our bacterial ancestors 1 billion years ago and demonstrated how a multi-species intelligence has worked since the beginning of life. Ant societies exhibit more intelligence, in terms of technology, than any other animal except for humans and co-operate in keeping livestock, for example aphids for \"milking\". Leaf cutters care for fungi and carry leaves to feed the fungi.\n\nDavid Skrbina cites the concept of a 'group mind' as being derived from Plato's concept of panpsychism (that mind or consciousness is omnipresent and exists in all matter). He develops the concept of a 'group mind' as articulated by Thomas Hobbes in \"Leviathan\" and Fechner's arguments for a collective consciousness of mankind. He cites Durkheim as the most notable advocate of a \"collective consciousness\" and Teilhard de Chardin as a thinker who has developed the philosophical implications of the group mind.\n\nTom Atlee focuses primarily on humans and on work to upgrade what Howard Bloom calls \"the group IQ\". Atlee feels that collective intelligence can be encouraged \"to overcome 'groupthink' and individual cognitive bias in order to allow a collective to cooperate on one process – while achieving enhanced intellectual performance.\" George Pór defined the collective intelligence phenomenon as \"the capacity of human communities to evolve towards higher order complexity and harmony, through such innovation mechanisms as differentiation and integration, competition and collaboration.\" Atlee and Pór state that \"collective intelligence also involves achieving a single focus of attention and standard of metrics which provide an appropriate threshold of action\". Their approach is rooted in scientific community metaphor.\n\nThe term group intelligence is sometimes used interchangeably with the term collective intelligence. Anita Woolley presents Collective intelligence as a measure of group intelligence and group creativity. The idea is that a measure of collective intelligence covers a broad range of features of the group, mainly group composition and group interaction. The features of composition that lead to increased levels of collective intelligence in groups include criteria such as higher numbers of women in the group as well as increased diversity of the group.\n\nAtlee and Pór suggest that the field of collective intelligence should primarily be seen as a human enterprise in which mind-sets, a willingness to share and an openness to the value of distributed intelligence for the common good are paramount, though group theory and artificial intelligence have something to offer. Individuals who respect collective intelligence are confident of their own abilities and recognize that the whole is indeed greater than the sum of any individual parts. Maximizing collective intelligence relies on the ability of an organization to accept and develop \"The Golden Suggestion\", which is any potentially useful input from any member. Groupthink often hampers collective intelligence by limiting input to a select few individuals or filtering potential Golden Suggestions without fully developing them to implementation.\n\nRobert David Steele Vivas in \"The New Craft of Intelligence\" portrayed all citizens as \"intelligence minutemen,\" drawing only on legal and ethical sources of information, able to create a \"public intelligence\" that keeps public officials and corporate managers honest, turning the concept of \"national intelligence\" (previously concerned about spies and secrecy) on its head.\nAccording to Don Tapscott and Anthony D. Williams, collective intelligence is mass collaboration. In order for this concept to happen, four principles need to exist;\n\n\n\n\nA new scientific understanding of collective intelligence defines it as a group's general ability to perform a wide range of tasks. Definition, operationalization and statistical methods are similar to the psychometric approach of general individual intelligence. Hereby, an individual's performance on a given set of cognitive tasks is used to measure general cognitive ability indicated by the general intelligence factor \"g\" extracted via factor analysis. In the same vein as \"g\" serves to display between-individual performance differences on cognitive tasks, collective intelligence research aims to find a parallel intelligence factor for groups \"c\" factor' (also called 'collective intelligence factor' (\"CI\")) displaying between-group differences on task performance. The collective intelligence score then is used to predict how this same group will perform on any other similar task in the future. Yet tasks, hereby, refer to mental or intellectual tasks performed by small groups even though the concept is hoped to be transferrable to other performances and any groups or crowds reaching from families to companies and even whole cities. Since individuals' \"g\" factor scores are highly correlated with full-scale IQ scores, which are in turn regarded as good estimates of \"g\", this measurement of collective intelligence can also be seen as an intelligence indicator or quotient respectively for a group (Group-IQ) parallel to an individual's intelligence quotient (IQ) even though the score is not a quotient per se.\n\nMathematically, \"c\" and \"g\" are both variables summarizing positive correlations among different tasks supposing that performance on one task is comparable with performance on other similar tasks. \"c\" thus is a source of variance among groups and can only be considered as a group's standing on the \"c\" factor compared to other groups in a given relevant population. The concept is in contrast to competing hypotheses including other correlational structures to explain group intelligence, such as a composition out of several equally important but independent factors as found in individual personality research.\n\nBesides, this scientific idea also aims to explore the causes affecting collective intelligence, such as group size, collaboration tools or group members' interpersonal skills. The MIT Center for Collective Intelligence, for instance, announced the detection of \"The Genome of Collective Intelligence\" as one of its main goals aiming to develop a \"taxonomy of organizational building blocks, or genes, that can be combined and recombined to harness the intelligence of crowds\".\n\nIndividual intelligence is shown to be genetically and environmentally influenced. Analogously, collective intelligence research aims to explore reasons why certain groups perform more intelligent than other groups given that \"c\" is just moderately correlated with the intelligence of individual group members. According to Woolley et al.'s results, neither team cohesion nor motivation or satisfaction correlated with \"c\". However, they claim that three factors were found as significant correlates: the variance in the number of speaking turns, group members' average social sensitivity and the proportion of females. All three had similar predictive power for \"c\", but only social sensitivity was statistically significant (b=0.33, P=0.05).\n\nThe number speaking turns indicates that \"groups where a few people dominated the conversation were less collectively intelligent than those with a more equal distribution of conversational turn-taking\". Hence, providing multiple team members the chance to speak up made a group more intelligent.\n\nGroup members' social sensitivity was measured via the Reading the Mind in the Eyes Test (RME) and correlated .26 with \"c\". Hereby, participants are asked to detect thinking or feeling expressed in other peoples' eyes presented on pictures and assessed in a multiple choice format. The test aims to measure peoples' theory of mind (ToM), also called 'mentalizing' or 'mind reading', which refers to the ability to attribute mental states, such as beliefs, desires or intents, to other people and in how far people understand that others have beliefs, desires, intentions or perspectives different from their own ones. RME is a ToM test for adults that shows sufficient test-retest reliability and constantly differentiates control groups from individuals with functional autism or Asperger Syndrome. It is one of the most widely accepted and well-validated tests for ToM within adults. ToM can be regarded as an associated subset of skills and abilities within the broader concept of emotional intelligence.\n\nThe proportion of females as a predictor of \"c\" was largely mediated by social sensitivity (Sobel z = 1.93, P= 0.03) which is in vein with previous research showing that women score higher on social sensitivity tests. While a mediation, statistically speaking, clarifies the mechanism underlying the relationship between a dependent and an independent variable, Wolley agreed in an interview with the \"Harvard Business Review\" that these findings are saying that groups of women are smarter than groups of men. However, she relativizes this stating that the actual important thing is the high social sensitivity of group members.\n\nIt is theorized that the collective intelligence factor \"c\" is an emergent property resulting from bottom-up as well as top-down processes. Hereby, bottom-up processes cover aggregated group-member characteristics. Top-down processes cover group structures and norms that influence a group's way of collaborating and coordinating.\n\nTop-down processes cover group interaction, such as structures, processes, and norms. An example of such top-down processes is conversational turn-taking. Research further suggest that collectively intelligent groups communicate more in general as well as more equally; same applies for participation and is shown for face-to-face as well as online groups communicating only via writing.\n\nBottom-up processes include group composition, namely the characteristics of group members which are aggregated to the team level encompassing. An example of such bottom-up processes is the average social sensitivity or the average and maximum intelligence scores of group members. Furthermore, collective intelligence was found to be related to a group's cognitive diversity including thinking styles and perspectives. Groups that are moderately diverse in cognitive style have higher collective intelligence than those who are very similar in cognitive style or very different. Consequently, groups where members are too similar to each other lack the variety of perspectives and skills needed to perform well. On the other hand, groups whose members are too different seem to have difficulties to communicate and coordinate effectively.\n\nFor most of human history, collective intelligence was confined to small tribal groups in which opinions were aggregated through real-time parallel interactions among members. In modern times, mass communication, mass media, and networking technologies have enabled collective intelligence to span massive groups, distributed across continents and time-zones. To accommodate this shift in scale, collective intelligence in large-scale groups been dominated by serialized polling processes such as aggregating up-votes, likes, and ratings over time. While modern systems benefit from larger group size, the serialized process has been found to introduce substantial noise that distorts the collective output of the group. In one significant study of serialized collective intelligence, it was found that the first vote contributed to a serialized voting system can distort the final result by 34%.\n\nTo address the problems of serialized aggregation of input among large-scale groups, recent advancements collective intelligence have worked to replace serialized votes, polls, and markets, with parallel systems such as \"human swarms\" modeled after synchronous swarms in nature. Based on natural process of Swarm Intelligence, these artificial swarms of networked humans enable participants to work together in parallel to answer questions and make predictions as an emergent collective intelligence. In one high-profile example, a human swarm challenge by CBS Interactive to predict the Kentucky Derby. The swarm correctly predicted the first four horses, in order, defying 542–1 odds and turning a $20 bet into $10,800.\n\nWoolley, Chabris, Pentland, Hashmi, & Malone (2010), the originators of this scientific understanding of collective intelligence, found a single statistical factor for collective intelligence in their research across 192 groups with people randomly recruited from the public. In Woolley et al.'s two initial studies, groups worked together on different tasks from the McGrath Task Circumplex, a well-established taxonomy of group tasks. Tasks were chosen from all four quadrants of the circumplex and included visual puzzles, brainstorming, making collective moral judgments, and negotiating over limited resources. The results in these tasks were taken to conduct a factor analysis. Both studies showed support for a general collective intelligence factor \"c\" underlying differences in group performance with an initial eigenvalue accounting for 43% (44% in study 2) of the variance, whereas the next factor accounted for only 18% (20%). That fits the range normally found in research regarding a general individual intelligence factor \"g\" typically accounting for 40% to 50% percent of between-individual performance differences on cognitive tests.\nAfterwards, a more complex criterion task was absolved by each group measuring whether the extracted \"c\" factor had predictive power for performance outside the original task batteries. Criterion tasks were playing checkers (draughts) against a standardized computer in the first and a complex architectural design task in the second study. In a regression analysis using both individual intelligence of group members and \"c\" to predict performance on the criterion tasks, \"c\" had a significant effect, but average and maximum individual intelligence had not. While average (r=0.15, P=0.04) and maximum intelligence (r=0.19, P=0.008) of individual group members were moderately correlated with \"c\", \"c\" was still a much better predictor of the criterion tasks. According to Woolley et al., this supports the existence of a collective intelligence factor \"c,\" because it demonstrates an effect over and beyond group members' individual intelligence and thus that \"c\" is more than just the aggregation of the individual IQs or the influence of the group member with the highest IQ.\n\nEngel et al. (2014) replicated Woolley et al.'s findings applying an accelerated battery of tasks with a first factor in the factor analysis explaining 49% of the between-group variance in performance with the following factors explaining less than half of this amount. Moreover, they found a similar result for groups working together online communicating only via text and confirmed the role of female proportion and social sensitivity in causing collective intelligence in both cases. Similarly to Wolley et al., they also measured social sensitivity with the RME which is actually meant to measure people's ability to detect mental states in other peoples' eyes. The online collaborating participants, however, did neither know nor see each other at all. The authors conclude that scores on the RME must be related to a broader set of abilities of social reasoning than only drawing inferences from other people's eye expressions.\n\nA collective intelligence factor \"c\" in the sense of Woolley et al. was further found in groups of MBA students working together over the course of a semester, in online gaming groups as well as in groups from different cultures and groups in different contexts in terms of short-term versus long-term groups. None of these investigations considered team members' individual intelligence scores as control variables.\n\nNote as well that the field of collective intelligence research is quite young and published empirical evidence is relatively rare yet. However, various proposals and working papers are in progress or already completed but (supposedly) still in a scholarly peer reviewing publication process.\n\nNext to predicting a group's performance on more complex criterion tasks as shown in the original experiments, the collective intelligence factor \"c\" was also found to predict group performance in diverse tasks in MBA classes lasting over several months. Thereby, highly collectively intelligent groups earned significantly higher scores on their group assignments although their members did not do any better on other individually performed assignments. Moreover, highly collective intelligent teams improved performance over time suggesting that more collectively intelligent teams learn better. This is another potential parallel to individual intelligence where more intelligent people are found to acquire new material quicker.\n\nIndividual intelligence can be used to predict plenty of life outcomes from school attainment and career success to health outcomes and even mortality. Whether collective intelligence is able to predict other outcomes besides group performance on mental tasks has still to be investigated.\n\nGladwell (2008) showed that the relationship between individual IQ and success works only to a certain point and that additional IQ points over an estimate of IQ 120 do not translate into real life advantages. If a similar border exists for Group-IQ or if advantages are linear and infinite, has still to be explored. Similarly, demand for further research on possible connections of individual and collective intelligence exists within plenty of other potentially transferable logics of individual intelligence, such as, for instance, the development over time or the question of improving intelligence. Whereas it is controversial whether human intelligence can be enhanced via training, a group's collective intelligence potentially offers simpler opportunities for improvement by exchanging team members or implementing structures and technologies. Moreover, social sensitivity was found to be, at least temporarily, improvable by reading literary fiction as well as watching drama movies. In how far such training ultimately improves collective intelligence through social sensitivity remains an open question.\n\nThere are further more advanced concepts and factor models attempting to explain individual cognitive ability including the categorization of intelligence in fluid and crystallized intelligence or the hierarchical model of intelligence differences. Further supplementing explanations and conceptualizations for the factor structure of the Genomes of collective intelligence besides a general \"'c\" factor', though, are missing yet.\n\nOther scholars explain team performance by aggregating team members' general intelligence to the team level instead of building an own overall collective intelligence measure. Devine and Philips (2001) showed in a meta-analysis that mean cognitive ability predicts team performance in laboratory settings (.37) as well as field settings (.14) – note that this is only a small effect. Suggesting a strong dependence on the relevant tasks, other scholars showed that tasks requiring a high degree of communication and cooperation are found to be most influenced by the team member with the lowest cognitive ability. Tasks in which selecting the best team member is the most successful strategy, are shown to be most influenced by the member with the highest cognitive ability.\n\nSince Woolley et al.'s results do not show any influence of group satisfaction, group cohesiveness, or motivation, they, at least implicitly, challenge these concepts regarding the importance for group performance in general and thus contrast meta-analytically proven evidence concerning the positive effects of group cohesion, motivation and satisfaction on group performance.\n\nNoteworthy is also that the involved researchers among the confirming findings widely overlap with each other and with the authors participating in the original first study around Anita Woolley.\n\nIn 2001, Tadeusz (Tad) Szuba from the AGH University in Poland proposed a formal model for the phenomenon of collective intelligence. It is assumed to be an unconscious, random, parallel, and distributed computational process, run in mathematical logic by the social structure.\n\nIn this model, beings and information are modeled as abstract information molecules carrying expressions of mathematical logic. They are quasi-randomly displacing due to their interaction with their environments with their intended displacements. Their interaction in abstract computational space creates multi-thread inference process which we perceive as collective intelligence. Thus, a non-Turing model of computation is used. This theory allows simple formal definition of collective intelligence as the property of social structure and seems to be working well for a wide spectrum of beings, from bacterial colonies up to human social structures. Collective intelligence considered as a specific computational process is providing a straightforward explanation of several social phenomena. For this model of collective intelligence, the formal definition of IQS (IQ Social) was proposed and was defined as \"the probability function over the time and domain of N-element inferences which are reflecting inference activity of the social structure\". While IQS seems to be computationally hard, modeling of social structure in terms of a computational process as described above gives a chance for approximation. Prospective applications are optimization of companies through the maximization of their IQS, and the analysis of drug resistance against collective intelligence of bacterial colonies.\n\nOne measure sometimes applied, especially by more artificial intelligence focused theorists, is a \"collective intelligence quotient\" (or \"cooperation quotient\") – which can be normalized from the \"individual\" intelligence quotient (IQ) – thus making it possible to determine the marginal intelligence added by each new individual participating in the collective action, thus using metrics to avoid the hazards of group think and stupidity.\n\n\"Elicitation of point estimates\" – Here, we try to get an estimate (in a single value) of something. For example, estimating the weight of an object, or the release date of a product or probability of success of a project etc. as are seen in prediction markets like Intrade, HSX or InklingMarkets and also in several implementations of crowdsourced estimation of a numeric outcome. Essentially, we try to get the average value of the estimates provided by the members in the crowd.\n\n\"Opinion Aggregation\" – In this situation, we gather opinions from the crowd regarding some idea, issue or product. For example, trying to get a rating (on some scale) of a product sold online (such as Amazon’s star rating system). Here, the emphasis is to collect and simply aggregate the ratings provided by customers/users.\n\n\"Idea Collection\" – In these problems, someone solicits ideas for projects, designs or solutions from the crowd. For example, ideas on solving a data science problem (as in Kaggle) or getting a good design for a T-shirt (as in Threadless) or in getting answers to simple problems that only humans can do well (as in Amazon’s Mechanical Turk). Here, the objective is to gather the ideas and devise some selection criteria to choose the best ideas.\n\nJames Surowiecki divides the advantages of disorganized decision-making into three main categories, which are cognition, cooperation and coordination.\n\nBecause of the Internet's ability to rapidly convey large amounts of information throughout the world, the use of collective intelligence to predict stock prices and stock price direction has become increasingly viable. Websites aggregate stock market information that is as current as possible so professional or amateur stock analysts can publish their viewpoints, enabling amateur investors to submit their financial opinions and create an aggregate opinion. The opinion of all investor can be weighed equally so that a pivotal premise of the effective application of collective intelligence can be applied: the masses, including a broad spectrum of stock market expertise, can be utilized to more accurately predict the behavior of financial markets.\n\nCollective intelligence underpins the efficient-market hypothesis of Eugene Fama – although the term collective intelligence is not used explicitly in his paper. Fama cites research conducted by Michael Jensen in which 89 out of 115 selected funds underperformed relative to the index during the period from 1955 to 1964. But after removing the loading charge (up-front fee) only 72 underperformed while after removing brokerage costs only 58 underperformed. On the basis of such evidence index funds became popular investment vehicles using the collective intelligence of the market, rather than the judgement of professional fund managers, as an investment strategy.\n\nPolitical parties mobilize large numbers of people to form policy, select candidates and finance and run election campaigns. Knowledge focusing through various voting methods allows perspectives to converge through the assumption that uninformed voting is to some degree random and can be filtered from the decision process leaving only a residue of informed consensus. Critics point out that often bad ideas, misunderstandings, and misconceptions are widely held, and that structuring of the decision process must favor experts who are presumably less prone to random or misinformed voting in a given context.\n\nCompanies such as Affinnova (acquired by Nielsen), Google, InnoCentive, Marketocracy, and Threadless have successfully employed the concept of collective intelligence in bringing about the next generation of technological changes through their research and development (R&D), customer service, and knowledge management. An example of such application is Google's Project Aristotle in 2012, where the effect of collective intelligence on team makeup was examined in hundreds of the company's R&D teams.\n\nIn 2012, the \"Global Futures Collective Intelligence System\" (GFIS) was created by The Millennium Project, which epitomizes collective intelligence as the synergistic intersection among data/information/knowledge, software/hardware, and expertise/insights that has a recursive learning process for better decision-making than the individual players alone.\n\nNew media are often associated with the promotion and enhancement of collective intelligence. The ability of new media to easily store and retrieve information, predominantly through databases and the Internet, allows for it to be shared without difficulty. Thus, through interaction with new media, knowledge easily passes between sources resulting in a form of collective intelligence. The use of interactive new media, particularly the internet, promotes online interaction and this distribution of knowledge between users.\n\nFrancis Heylighen, Valentin Turchin, and Gottfried Mayer-Kress are among those who view collective intelligence through the lens of computer science and cybernetics. In their view, the Internet enables collective intelligence at the widest, planetary scale, thus facilitating the emergence of a global brain.\n\nThe developer of the World Wide Web, Tim Berners-Lee, aimed to promote sharing and publishing of information globally. Later his employer opened up the technology for free use. In the early '90s, the Internet's potential was still untapped, until the mid-1990s when 'critical mass', as termed by the head of the Advanced Research Project Agency (ARPA), Dr. J.C.R. Licklider, demanded more accessibility and utility. The driving force of this Internet-based collective intelligence is the digitization of information and communication. Henry Jenkins, a key theorist of new media and media convergence draws on the theory that collective intelligence can be attributed to media convergence and participatory culture . He criticizes contemporary education for failing to incorporate online trends of collective problem solving into the classroom, stating \"whereas a collective intelligence community encourages ownership of work as a group, schools grade individuals\". Jenkins argues that interaction within a knowledge community builds vital skills for young people, and teamwork through collective intelligence communities contribute to the development of such skills. Collective intelligence is not merely a quantitative contribution of information from all cultures, it is also qualitative.\n\nLévy and de Kerckhove consider CI from a mass communications perspective, focusing on the ability of networked information and communication technologies to enhance the community knowledge pool. They suggest that these communications tools enable humans to interact and to share and collaborate with both ease and speed (Flew 2008). With the development of the Internet and its widespread use, the opportunity to contribute to knowledge-building communities, such as Wikipedia, is greater than ever before. These computer networks give participating users the opportunity to store and to retrieve knowledge through the collective access to these databases and allow them to \"harness the hive\" Researchers at the MIT Center for Collective Intelligence research and explore collective intelligence of groups of people and computers.\n\nIn this context collective intelligence is often confused with shared knowledge. The former is the sum total of information held individually by members of a community while the latter is information that is believed to be true and known by all members of the community. Collective intelligence as represented by Web 2.0 has less user engagement than collaborative intelligence. An art project using Web 2.0 platforms is \"Shared Galaxy\", an experiment developed by an anonymous artist to create a collective identity that shows up as one person on several platforms like MySpace, Facebook, YouTube and Second Life. The password is written in the profiles and the accounts named \"Shared Galaxy\" are open to be used by anyone. In this way many take part in being one. Another art project using collective intelligence to produce artistic work is Curatron, where a large group of artists together decides on a smaller group that they think would make a good collaborative group. The process is used based on an algorithm computing the collective preferences In creating what he calls 'CI-Art', Nova Scotia based artist Mathew Aldred follows Pierry Lévy's definition of collective intelligence. Aldred's CI-Art event in March 2016 involved over four hundred people from the community of Oxford, Nova Scotia, and internationally. Later work developed by Aldred used the UNU swarm intelligence system to create digital drawings and paintings. The Oxford Riverside Gallery (Nova Scotia) held a public CI-Art event in May 2016, which connected with online participants internationally.\n\nIn social bookmarking (also called collaborative tagging), users assign tags to resources shared with other users, which gives rise to a type of information organisation that emerges from this crowdsourcing process. The resulting information structure can be seen as reflecting the collective knowledge (or collective intelligence) of a community of users and is commonly called a \"Folksonomy\", and the process can be captured by models of collaborative tagging.\n\nRecent research using data from the social bookmarking website Delicious, has shown that collaborative tagging systems exhibit a form of complex systems (or self-organizing) dynamics. Although there is no central controlled vocabulary to constrain the actions of individual users, the distributions of tags that describe different resources has been shown to converge over time to a stable power law distributions. Once such stable distributions form, examining the correlations between different tags can be used to construct simple folksonomy graphs, which can be efficiently partitioned to obtained a form of community or shared vocabularies. Such vocabularies can be seen as a form of collective intelligence, emerging from the decentralised actions of a community of users. The Wall-it Project is also an example of social bookmarking.\n\nResearch performed by Tapscott and Williams has provided a few examples of the benefits of collective intelligence to business:\n\n\nCultural theorist and online community developer, John Banks considered the contribution of online fan communities in the creation of the Trainz product. He argued that its commercial success was fundamentally dependent upon \"the formation and growth of an active and vibrant online fan community that would both actively promote the product and create content- extensions and additions to the game software\".\n\nThe increase in user created content and interactivity gives rise to issues of control over the game itself and ownership of the player-created content. This gives rise to fundamental legal issues, highlighted by Lessig and Bray and Konsynski, such as intellectual property and property ownership rights.\n\nGosney extends this issue of Collective Intelligence in videogames one step further in his discussion of alternate reality gaming. This genre, he describes as an \"across-media game that deliberately blurs the line between the in-game and out-of-game experiences\" as events that happen outside the game reality \"reach out\" into the player's lives in order to bring them together. Solving the game requires \"the collective and collaborative efforts of multiple players\"; thus the issue of collective and collaborative team play is essential to ARG. Gosney argues that the Alternate Reality genre of gaming dictates an unprecedented level of collaboration and \"collective intelligence\" in order to solve the mystery of the game.\n\nCo-operation helps to solve most important and most interesting multi-science problems. In his book, James Surowiecki mentioned that most scientists think that benefits of co-operation have much more value when compared to potential costs. Co-operation works also because at best it guarantees number of different viewpoints. Because of the possibilities of technology global co-operation is nowadays much easier and productive than before. It is clear that, when co-operation goes from university level to global it has significant benefits. \n\nFor example, why do scientists co-operate? Science has become more and more isolated and each science field has spread even more and it is impossible for one person to be aware of all developments. This is true especially in experimental research where highly advanced equipment requires special skills. With co-operation scientists can use information from different fields and use it effectively instead of gathering all the information just by reading by themselves.\"\n\nMilitary, trade unions, and corporations satisfy some definitions of CI – the most rigorous definition would require a capacity to respond to very arbitrary conditions without orders or guidance from \"law\" or \"customers\" to constrain actions. Online advertising companies are using collective intelligence to bypass traditional marketing and creative agencies.\n\nThe UNU open platform for \"human swarming\" (or \"social swarming\") establishes real-time closed-loop systems around groups of networked users molded after biological swarms, enabling human participants to behave as a unified collective intelligence. When connected to UNU, groups of distributed users collectively answer questions and make predictions in real-time. Early testing shows that human swarms can out-predict individuals. In 2016, an UNU swarm was challenged by a reporter to predict the winners of the Kentucky Derby, and successfully picked the first four horses, in order, beating 540 to 1 odds.\n\nSpecialized information sites such as Digital Photography Review or Camera Labs is an example of collective intelligence. Anyone who has an access to the internet can contribute to distributing their knowledge over the world through the specialized information sites.\n\nIn learner-generated context a group of users marshal resources to create an ecology that meets their needs often (but not only) in relation to the co-configuration, co-creation and co-design of a particular learning space that allows learners to create their own context. Learner-generated contexts represent an \"ad hoc\" community that facilitates coordination of collective action in a network of trust. An example of learner-generated context is found on the Internet when collaborative users pool knowledge in a \"shared intelligence space\". As the Internet has developed so has the concept of CI as a shared public forum. The global accessibility and availability of the Internet has allowed more people than ever to contribute and access ideas. (Flew 2008)\n\nGames such as \"The Sims\" Series, and \"Second Life\" are designed to be non-linear and to depend on collective intelligence for expansion. This way of sharing is gradually evolving and influencing the mindset of the current and future generations. For them, collective intelligence has become a norm. In Terry Flew's discussion of 'interactivity' in the online games environment, the ongoing interactive dialogue between users and game developers, he refers to Pierre Lévy's concept of Collective Intelligence and argues this is active in videogames as clans or guilds in MMORPG constantly work to achieve goals. Henry Jenkins proposes that the participatory cultures emerging between games producers, media companies, and the end-users mark a fundamental shift in the nature of media production and consumption. Jenkins argues that this new participatory culture arises at the intersection of three broad new media trends. Firstly, the development of new media tools/technologies enabling the creation of content. Secondly, the rise of subcultures promoting such creations, and lastly, the growth of value adding media conglomerates, which foster image, idea and narrative flow.\n\nImprovisational actors also experience a type of collective intelligence which they term \"group mind\", as theatrical improvisation relies on mutual cooperation and agreement, leading to the unity of \"group mind\".\n\nGrowth of the Internet and mobile telecom has also produced \"swarming\" or \"rendezvous\" events that enable meetings or even dates on demand. The full impact has yet to be felt but the anti-globalization movement, for example, relies heavily on e-mail, cell phones, pagers, SMS and other means of organizing. The Indymedia organization does this in a more journalistic way. Such resources could combine into a form of collective intelligence accountable only to the current participants yet with some strong moral or linguistic guidance from generations of contributors – or even take on a more obviously democratic form to advance shared goal.\n\nA further application of collective intelligence is found in the \"Community Engineering for Innovations\". In such an integrated framework proposed by Ebner et al., idea competitions and virtual communities are combined to better realize the potential of the collective intelligence of the participants, particularly in open-source R&D.\n\nCollective actions or tasks require different amounts of coordination depeding on the complexity of the task. Tasks vary from being highly independent simple tasks that require very little coordination to complex interdependent tasks that are built by many individuals and require a lot of coordination. In the article written by Kittur, Lee and Kraut the writers introduce a problem in cooperation: ”When tasks require high coordination because the work is highly interdependent, having more contributors can increase process losses, reducing the effectiveness of the group below what individual members could optimally accomplish”. Having a team too large the overall effectiveness may suffer even when the extra contributors increase the resources. In the end the overall costs from coordination might overwhelm other costs. \n\nGroup collective intelligence is a property that emerges through coordination from both bottom-up and top-down processes. In a bottom-up process the different characteristics of each member are involved in contributing and enhancing coordination. Top-down processes are more strict and fixed with norms, group structures and routines that in their own way enhance the group’s collective work. \n\nTom Atlee reflects that, although humans have an innate ability to gather and analyze data, they are affected by culture, education and social institutions. A single person tends to make decisions motivated by self-preservation. Therefore, without collective intelligence, humans may drive themselves into extinction based on their selfish needs.\n\nPhillip Brown and Hugh Lauder quotes Bowles and Gintis (1976) that in order to truly define collective intelligence, it is crucial to separate 'intelligence' from IQism. They go on to argue that intelligence is an achievement and can only be developed if allowed to. For example, earlier on, groups from the lower levels of society are severely restricted from aggregating and pooling their intelligence. This is because the elites fear that the collective intelligence would convince the people to rebel. If there is no such capacity and relations, there would be no infrastructure on which collective intelligence is built . This reflects how powerful collective intelligence can be if left to develop.\n\nSkeptics, especially those critical of artificial intelligence and more inclined to believe that risk of bodily harm and bodily action are the basis of all unity between people, are more likely to emphasize the capacity of a group to take action and withstand harm as one fluid mass mobilization, shrugging off harms the way a body shrugs off the loss of a few cells. This strain of thought is most obvious in the anti-globalization movement and characterized by the works of John Zerzan, Carol Moore, and Starhawk, who typically shun academics. These theorists are more likely to refer to ecological and collective wisdom and to the role of consensus process in making ontological distinctions than to any form of \"intelligence\" as such, which they often argue does not exist, or is mere \"cleverness\".\n\nHarsh critics of artificial intelligence on ethical grounds are likely to promote collective wisdom-building methods, such as the new tribalists and the Gaians. Whether these can be said to be collective intelligence systems is an open question. Some, e.g. Bill Joy, simply wish to avoid any form of autonomous artificial intelligence and seem willing to work on rigorous collective intelligence in order to remove any possible niche for AI.\n\nIn contrast to these views, Artificial Intelligence companies such as Amazon Mechanical Turk and CrowdFlower are using collective intelligence and crowdsourcing or consensus-based assessment to collect the enormous amounts of data for machine learning algorithms such as Keras and IBM Watson.\n\nGlobal collective intelligence is also seen as the key in solving the challenges that the humankind faces now and in the future. Climate change is an example of a global issue which collective intelligence is currently trying to tackle. With the help of collective intelligence applications such as online crowdsourcing people across the globe are collaborating in developing solutions to climate change.\n\n\n\n\n\n",
    "id": "20756850",
    "title": "Collective intelligence"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=6338699",
    "text": "Embodied cognitive science\n\nEmbodied cognitive science is an interdisciplinary field of research, the aim of which is to explain the mechanisms underlying intelligent behavior. It comprises three main methodologies: 1) the modeling of psychological and biological systems in a holistic manner that considers the mind and body as a single entity, 2) the formation of a common set of general principles of intelligent behavior, and 3) the experimental use of robotic agents in controlled environments.\n\nEmbodied cognitive science borrows heavily from embodied philosophy and the related research fields of cognitive science, psychology, neuroscience and artificial intelligence. From the perspective of neuroscience, research in this field was led by Gerald Edelman of the Neurosciences Institute at La Jolla, the late Francisco Varela of CNRS in France, and J. A. Scott Kelso of Florida Atlantic University. From the perspective of psychology, research by Michael Turvey, Lawrence Barsalou and Eleanor Rosch. From the perspective of language acquisition, Eric Lenneberg and Philip Rubin at Haskins Laboratories. From the perspective of autonomous agent design, early work is sometimes attributed to Rodney Brooks or Valentino Braitenberg. From the perspective of artificial intelligence, see \"Understanding Intelligence\" by Rolf Pfeifer and Christian Scheier or \"How the body shapes the way we think\", also by Rolf Pfeifer and Josh C. Bongard. From the perspective of philosophy see Andy Clark, Shaun Gallagher, and Evan Thompson.\n\nTuring proposed that a machine may need a human-like body to think and speak:\n\nEmbodied cognitive science is an alternative theory to cognition in which it minimizes appeals to computational theory of mind in favor of greater emphasis on how an organism's body determines how and what it thinks. Traditional cognitive theory is based mainly around symbol manipulation, in which certain inputs are fed into a processing unit that produces an output. These inputs follow certain rules of syntax, from which the processing unit finds semantic meaning. Thus, an appropriate output is produced. For example, a human's sensory organs are its input devices, and the stimuli obtained from the external environment are fed into the nervous system which serves as the processing unit. From here, the nervous system is able to read the sensory information because it follows a syntactic structure, thus an output is created. This output then creates bodily motions and brings forth behavior and cognition. Of particular note is that cognition is sealed away in the brain, meaning that mental cognition is cut off from the external world and is only possible by the input of sensory information.\n\nEmbodied cognitive science differs from the traditionalist approach in that it denies the input-output system. This is chiefly due to the problems presented by the Homunculus argument, which concluded that semantic meaning could not be derived from symbols without some kind of inner interpretation. If some little man in a person's head interpreted incoming symbols, then who would interpret the little man's inputs? Because of the specter of an infinite regress, the traditionalist model began to seem less plausible. Thus, embodied cognitive science aims to avoid this problem by defining cognition in three ways.\n\nThe first aspect of embodied cognition examines the role of the physical body, particularly how its properties affect its ability to think. This part attempts to overcome the symbol manipulation component that is a feature of the traditionalist model. Depth perception for instance can be better explained under the embodied approach due to the sheer complexity of the action. Depth perception requires that the brain detect the disparate retinal images obtained by the distance of the two eyes. In addition, body and head cues complicate this further. When the head is turned in a given direction, objects in the foreground will appear to move against objects in the background. From this, it is said that some kind of visual processing is occurring without the need of any kind of symbol manipulation. This is because the objects appearing to move the foreground are simply appearing to move. This observation concludes then that depth can be perceived with no intermediate symbol manipulation necessary. \n\nA more poignant example exists through examining auditory perception. Generally speaking the greater the distance between the ears, the greater the possible auditory acuity. Also relevant is the amount of density in between the ears, for the strength of the frequency wave alters as it passes through a given medium. The brain's auditory system takes these factors into account as it process information, but again without any need for a symbolic manipulation system. This is because the distance between the ears for example does not need symbols to represent it. The distance itself creates the necessary opportunity for greater auditory acuity. The amount of density between the ears is similar, in that it is the actual amount itself that simply forms the opportunity for frequency alteration. Thus under consideration of the physical properties of the body, a symbolic system is unnecessary and an unhelpful metaphor.\n\nThe second aspect draws heavily from George Lakoff's and Mark Johnson's work on concepts. They argued that humans use metaphors whenever possible to better explain their external world. Humans also have a basic stock of concepts in which other concepts can be derived from. These basic concepts include spatial orientations such as up, down, front, and back. Humans can understand what these concepts mean because they can directly experience them from their own bodies. For example, because human movement revolves around standing erect and moving the body in an up-down motion, humans innately have these concepts of up and down. Lakoff and Johnson contend this is similar with other spatial orientations such as front and back too. As mentioned earlier, these basic stocks of spatial concepts are the basis in which other concepts are constructed. Happy and sad for instance are seen now as being up or down respectively. When someone says they are feeling down, what they are really saying is that they feel sad for example. Thus the point here is that true understanding of these concepts is contingent on whether one can have an understanding of the human body. So the argument goes that if one lacked a human body, they could not possibly know what up or down could mean, or how it could relate to emotional states.\n\n‘[I]magine a spherical being living outside of any gravitational field, with no\nknowledge or imagination of any other kind of experience. What could UP\npossibly mean to such a being?'\n\nWhile this does not mean that such beings would be incapable of expressing emotions in other words, it does mean that they would express emotions differently from humans. Human concepts of happiness and sadness would be different because human would have different bodies. So then an organism's body directly affects how it can think, because it uses metaphors related to its body as the basis of concepts.\n\nA third component of the embodied approach looks at how agents use their immediate environment in cognitive processing. Meaning, the local environment is seen as an actual extension of the body's cognitive process. The example of a personal digital assistant (PDA) is used to better imagine this. Echoing functionalism (philosophy of mind), this point claims that mental states are individuated by their role in a much larger system. So under this premise, the information on a PDA is similar to the information stored in the brain. So then if one thinks information in the brain constitutes mental states, then it must follow that information in the PDA is a cognitive state too. Consider also the role of pen and paper in a complex multiplication problem. The pen and paper are so involved in the cognitive process of solving the problem that it seems ridiculous to say they are somehow different from the process, in very much the same way the PDA is used for information like the brain. Another example examines how humans control and manipulate their environment so that cognitive tasks can be better performed. Leaving one's car keys in a familiar place so they aren't missed for instance, or using landmarks to navigate in an unfamiliar city. Thus, humans incorporate aspects of their environment to aid in their cognitive functioning.\n\nThe value of the embodiment approach in the context of cognitive science is perhaps best explained by Andy Clark. He makes the claim that the brain alone should not be the single focus for the scientific study of cognition\n\nIt is increasingly clear that, in a wide variety of cases, the individual brain should not be the sole locus of cognitive scientific interest. Cognition is not a phenomenon that can be successfully studied while marginalizing the roles of body, world and action.\n\nThe following examples used by Clark will better illustrate how embodied thinking is becoming apparent in scientific thinking.\n\n\"Thunnus\", or tuna, long baffled conventional biologists with its incredible abilities to accelerate quickly and attain great speeds. A biological examination of the tuna shows it is simply not capable of such feats. However, an answer can be found when taking the tuna's embodied state into account. The bluefin tuna is able to take advantage of and exploit its local environment by finding naturally occurring currents to increase its speed. The tuna also uses its own physical body for this end as well, by utilizing its tailfin to create the necessary vortices and pressure so it can accelerate and maintain high speeds. Thus, the bluefin tuna is actively using its local environment for its own ends through the attributes of its physical body.\n\nClark uses the example of the hopping robot constructed by Raibert and Hodgins to demonstrate further the value of the embodiment paradigm. These robots were essentially vertical cylinders with a single hopping foot. The challenge of managing the robot's behavior can be daunting because in addition to the intricacies of the program itself, there were also the mechanical matters regarding how the foot ought to be constructed so that it could hop. An embodied approach makes it easier to see that in order for this robot to function, it must be able to exploit its system to the fullest. That is, the robot's systems should be seen as having dynamic characteristics as opposed to the traditional view that it is merely a command center that just executes actions.\n\nClark distinguishes between two kinds of vision, animate and pure vision. Pure vision is an idea that is typically associated with classical artificial intelligence, in which vision is used to create a rich world model so that thought and reason can be used to fully explore the inner model. In other words, pure vision passively creates the external perceivable world so that the faculties of reason can be better used introspectively. Animate vision, by contrast, sees vision as the means by which real-time action can commence. Animate vision is then more of a vehicle by which visual information is obtained so that actions can be undertaken. Clark points to animate vision as an example of embodiment, because it uses both biological and local environment cues to create an active intelligent process. Consider the Clark's example of going to the drugstore to buy some Kodak film. In one's mind, one is familiar with the Kodak logo and its trademark gold color. Thus, one uses incoming visual stimuli to navigate around the drugstore until one finds the film. Therefore, vision should not be seen as a passive system but rather an active retrieval device that intelligently uses sensory information and local environmental cues to perform specific real-world actions.\n\nInspired by the work of the American psychologist James J. Gibson, this next example emphasizes the importance of action-relevant sensory information, bodily movement, and local environment cues. These three concepts are unified by the concept of affordances, which are possibilities of action provided by the physical world to a given agent. These are in turn determined by the agent's physical body, capacities, and the overall action-related properties of the local environment as well. Clark uses the example of an outfielder in baseball to better illustrate the concept of affordance. Traditional computational models would claim that an outfielder attempting to catch a fly-ball can be calculated by variables such as the running speed of the outfielder and the arc of the baseball. However, Gibson's work shows that a simpler method is possible. The outfielder can catch the ball so long as they adjust their running speed so that the ball continually moves in a straight line in their field of vision. Note that this strategy uses various affordances that are contingent upon the success of the outfielder, including their physical body composition, the environment of the baseball field, and the sensory information obtained by the outfielder. \n\nClark points out here that the latter strategy of catching the ball as opposed to the former has significant implications for perception. The affordance approach proves to be non-linear because it relies upon spontaneous real-time adjustments. On the contrary, the former method of computing the arc of the ball is linear as it follows a sequence of perception, calculation and performing action. Thus, the affordance approach challenges the traditional view of perception by arguing against the notion that computation and introspection are necessary. Instead, it ought to be replaced with the idea that perception constitutes a continuous equilibrium of action adjustment between the agent and the world. Ultimately Clark does not expressly claim this is certain but he does observe the affordance approach can explain adaptive response satisfactorily. This is because they utilize environmental cues made possible by perceptual information that is actively used in the real-time by the agent.\n\nIn the formation of general principles of intelligent behavior, Pfeifer intended to be contrary to older principles given in traditional artificial intelligence. The most dramatic difference is that the principles are applicable only to situated robotic agents in the real world, a domain where traditional artificial intelligence showed the least promise.\n\nPrinciple of Cheap Design and Redundancy: Pfeifer realized that implicit assumptions made by engineers often substantially influence a control architecture's complexity. This insight is reflected in discussions of the scalability problem in robotics. The internal processing needed for some bad architectures can grow out of proportion to new tasks needed of an agent. \nOne of the primary reasons for scalability problems is that the amount of programming and knowledge engineering that the robot designers have to perform grows very rapidly with the complexity of the robot's tasks. There is mounting evidence that pre-programming cannot be the solution to the scalability problem ... The problem is that programmers introduce too many hidden assumptions in the robot's code.\nThe proposed solutions are to have the agent exploit the inherent physics of its environment, to exploit the constraints of its niche, and to have agent morphology based on parsimony and the principle of Redundancy. Redundancy reflects the desire for the error-correction of signals afforded by duplicating like channels. Additionally, it reflects the desire to exploit the associations between sensory modalities. (See redundant modalities). In terms of design, this implies that redundancy should be introduced with respect not only to one sensory modality but to several. It has been suggested that the fusion and transfer of knowledge between modalities can be the basis of reducing the size of the sense data taken from the real world. This again addresses the scalability problem.\n\nPrinciple of Parallel, Loosely-coupled Processes: An alternative to hierarchical methods of knowledge and action selection. This design principle differs most importantly from the Sense-Think-Act cycle of traditional AI. Since it does not involve this famous cycle, it is not affected by the frame problem.\n\nPrinciple of Sensory-Motor Coordination: Ideally, internal mechanisms in an agent should give rise to things like memory and choice-making in an emergent fashion, rather than being prescriptively programmed from the beginning. These kinds of things are allowed to emerge as the agent interacts with the environment. The motto is, build fewer assumptions into the agent's controller now, so that learning can be more robust and idiosyncratic in the future. \n\nPrinciple of Ecological Balance: This is more a theory than a principle, but its implications are widespread. Its claim is that the internal processing of an agent cannot be made more complex unless there is a corresponding increase in complexity of the motors, limbs, and sensors of the agent. In other words, the extra complexity added to the brain of a simple robot will not create any discernible change in its behavior. The robot's morphology must already contain the complexity in itself to allow enough \"breathing room\" for more internal processing to develop.\n\nThe Value Principle: This was the architecture developed in the Darwin III robot of Gerald Edelman. It relies heavily on connectionism.\n\nA traditionalist may argue that objects may be used to aid in cognitive processes, but this does not mean they are part of a cognitive system. Eyeglasses are used to aid in the visual process, but to say they are a part of a larger system would completely redefine what is meant by a visual system. However, supporters of the embodied approach could make the case that if objects in the environment play the functional role of mental states, then the items themselves should not be counted among the mental states.\n\n\n",
    "id": "6338699",
    "title": "Embodied cognitive science"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=2884728",
    "text": "Automated reasoning\n\nAutomated reasoning is an area of computer science and mathematical logic dedicated to understanding different aspects of reasoning. The study of automated reasoning helps produce computer programs that allow computers to reason completely, or nearly completely, automatically. Although automated reasoning is considered a sub-field of artificial intelligence, it also has connections with theoretical computer science, and even philosophy.\n\nThe most developed subareas of automated reasoning are automated theorem proving (and the less automated but more pragmatic subfield of interactive theorem proving) and automated proof checking (viewed as guaranteed correct reasoning under fixed assumptions). Extensive work has also been done in reasoning by analogy induction and abduction.\n\nOther important topics include reasoning under uncertainty and non-monotonic reasoning. An important part of the uncertainty field is that of argumentation, where further constraints of minimality and consistency are applied on top of the more standard automated deduction. John Pollock's OSCAR system is an example of an automated argumentation system that is more specific than being just an automated theorem prover.\n\nTools and techniques of automated reasoning include the classical logics and calculi, fuzzy logic, Bayesian inference, reasoning with maximal entropy and a large number of less formal \"ad hoc\" techniques.\n\nThe development of formal logic played a big role in the field of automated reasoning, which itself led to the development of artificial intelligence. A formal proof is a proof in which every logical inference has been checked back to the fundamental axioms of mathematics. All the intermediate logical steps are supplied, without exception. No appeal is made to intuition, even if the translation from intuition to logic is routine. Thus, a formal proof is less intuitive, and less susceptible to logical errors.\n\nSome consider the Cornell Summer meeting of 1957, which brought together a large number of logicians and computer scientists, as the origin of automated reasoning, or automated deduction. Others say that it began before that with the 1955 Logic Theorist program of Newell, Shaw and Simon, or with Martin Davis’ 1954 implementation of Presburger’s decision procedure (which proved that the sum of two even numbers is even). Automated reasoning, although a significant and popular area of research, went through an \"AI winter\" in the eighties and early nineties. Luckily, it got revived after that. For example, in 2005, Microsoft started using verification technology in many of their internal projects and is planning to include a logical specification and checking language in their 2012 version of Visual C.\n\nPrincipia Mathematica was a milestone work in formal logic written by Alfred North Whitehead and Bertrand Russell. Principia Mathematica - also meaning Principles of Mathematics - was written with a purpose to derive all or some of the mathematical expressions, in terms of symbolic logic. Principia Mathematica was initially published in three volumes in 1910, 1912 and 1913.\n\nLogic Theorist (LT) was the first ever program developed in 1956 by Allen Newell, Cliff Shaw and Herbert A. Simon to \"mimic human reasoning\" in proving theorems and was demonstrated on fifty-two theorems from chapter two of Principia Mathematica, proving thirty-eight of them. In addition to proving the theorems, the program found a proof for one of the theorems that was more elegant than the one provided by Whitehead and Russell. After an unsuccessful attempt at publishing their results, Newell, Shaw, and Herbert reported in their publication in 1958, \"The Next Advance in Operation Research\":\n\nExamples of Formal Proofs\n\n\n\n\nAutomated reasoning has been most commonly used to build automated theorem provers. Oftentimes, however, theorem provers require some human guidance to be effective and so more generally qualify as proof assistants. In some cases such provers have come up with new approaches to proving a theorem. Logic Theorist is a good example of this. The program came up with a proof for one of the theorems in Principia Mathematica that was more efficient (requiring fewer steps) than the proof provided by Whitehead and Russell. Automated reasoning programs are being applied to solve a growing number of problems in formal logic, mathematics and computer science, logic programming, software and hardware verification, circuit design, and many others. The TPTP (Sutcliffe and Suttner 1998) is a library of such problems that is updated on a regular basis. There is also a competition among automated theorem provers held regularly at the CADE conference (Pelletier, Sutcliffe and Suttner 2002); the problems for the competition are selected from the TPTP library.\n\n\n\n\n\n",
    "id": "2884728",
    "title": "Automated reasoning"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=28443302",
    "text": "Knowledge-based configuration\n\nKnowledge-based configuration, or also referred to as product configuration or product customization, is an activity of customising a product to meet the needs of a particular customer. The product in question may consist of mechanical parts, services, and software. Knowledge-based configuration is a major application area for artificial intelligence (AI), and it is based on modelling of the configurations in a manner that allows the utilisation of AI techniques for searching for a valid configuration to meet the needs of a particular customer.\n\nKnowledge-based configuration (of complex products and services) has a long history as an artificial intelligence application area, see, e.g. Informally, configuration can be defined as a \"special case of design activity, where the artifact being configured is assembled from instances of a fixed set of well-defined component types which can be composed conforming to a set of constraints\". Such constraints are representing technical restrictions, restrictions related to economic aspects, and conditions related to production processes. The result of a configuration process is a product configuration (concrete configuration), i.e., a list of instances and in some cases also connections between these instances. Examples of such configurations are computers to be delivered or financial service portfolio offers (e.g., a combination of loan and corresponding risk insurance).\n\nConfiguration systems or also referred to as configurators or mass customization toolkits, are one of the most successfully applied Artificial Intelligence technologies. Examples are the automotive industry, the telecommunication industry, the computer industry, and power electric transformers. Starting with rule-based approaches such as R1/XCON, model-based representations of knowledge (in contrast to rule-based representations) have been developed which strictly separate product domain knowledge from the problem solving one - examples thereof are the constraint satisfaction problem, the boolean satisfiability problem, and different answer set programming (ASP) representations. There are two commonly cited conceptualizations of configuration knowledge. The most important concepts in these are components, ports, resources and functions. This separation of product domain knowledge and problem solving knowledge increased the effectiveness of configuration application development and maintenance, since changes in the product domain knowledge do not affect search strategies and vice versa.\n\nConfigurators are also often considered as \"open innovation toolkits\", i.e., tools which support customers in the product identification phase. In this context customers are innovators who articulate their requirements leading to new innovative products. \"Mass Confusion\" – the overwhelming of customers by a large number of possible solution alternatives (choices) – is a phenomenon which often comes with the application of configuration technologies. This phenomenon motivated the creation of personalized configuration environments taking into account a customer’s knowledge and preferences.\n\nCore configuration, i.e., guiding the user and checking the consistency of user requirements with the knowledge base, solution presentation and translation of configuration results into bill of materials (BOM) are major tasks to be supported by a configurator. Configuration knowledge bases are often built using proprietary languages.\nIn most cases knowledge bases are developed by knowledge engineers who elicit product, marketing and sales knowledge from domain experts. Configuration knowledge bases are composed of a formal description of the structure of the product and further constraints restricting the possible feature and component combinations.\n\nRecently, knowledge based configuration has been extended to service and software configuration. Modeling software configuration has been based on two main approaches: feature modeling, and component-connectors. Kumbang domain ontology combines the previous approaches building on the tradition of knowledge based configuration.\n\n\n\n\n\n",
    "id": "28443302",
    "title": "Knowledge-based configuration"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=49180",
    "text": "Fuzzy logic\n\nFuzzy logic is a form of many-valued logic in which the truth values of variables may be any real number between 0 and 1. It is employed to handle the concept of partial truth, where the truth value may range between completely true and completely false. By contrast, in Boolean logic, the truth values of variables may only be the integer values 0 or 1.\n\nThe term \"fuzzy logic\" was introduced with the 1965 proposal of fuzzy set theory by Lotfi Zadeh. Fuzzy logic had however been studied since the 1920s, as infinite-valued logic—notably by Łukasiewicz and Tarski.\n\nFuzzy logic has been applied to many fields, from control theory to artificial intelligence.\n\nClassical logic only permits conclusions which are either true or false. However, there are also propositions with variable answers, such as one might find when asking a group of people to identify a color. In such instances, the truth appears as the result of reasoning from inexact or partial knowledge in which the sampled answers are mapped on a spectrum.\n\nBoth degrees of truth and probabilities range between 0 and 1 and hence may seem similar at first, but fuzzy logic uses degrees of truth as a mathematical model of \"vagueness\", while probability is a mathematical model of \"ignorance\".\n\nA basic application might characterize various sub-ranges of a continuous variable. For instance, a temperature measurement for anti-lock brakes might have several separate membership functions defining particular temperature ranges needed to control the brakes properly. Each function maps the same temperature value to a truth value in the 0 to 1 range. These truth values can then be used to determine how the brakes should be controlled.\n\nWhile variables in mathematics usually take numerical values, in fuzzy logic applications non-numeric values are often used to facilitate the expression of rules and facts.\n\nA linguistic variable such as \"age\" may accept values such as \"young\" and its antonym \"old\". Because natural languages do not always contain enough value terms to express a fuzzy value scale, it is common practice to modify linguistic values with adjectives or adverbs. For example, we can use the hedges \"rather\" and \"somewhat\" to construct the additional values \"rather old\" or \"somewhat young\".\n\nFuzzification operations can map mathematical input values into fuzzy membership functions. And the opposite de-fuzzifying operations can be used to map a fuzzy output membership functions into a \"crisp\" output value that can be then used for decision or control purposes.\n\n\nIn this image, the meanings of the expressions \"cold\", \"warm\", and \"hot\" are represented by functions mapping a temperature scale. A point on that scale has three \"truth values\"—one for each of the three functions. The vertical line in the image represents a particular temperature that the three arrows (truth values) gauge. Since the red arrow points to zero, this temperature may be interpreted as \"not hot\". The orange arrow (pointing at 0.2) may describe it as \"slightly warm\" and the blue arrow (pointing at 0.8) \"fairly cold\".\n\nFuzzy sets are often defined as triangle or trapezoid-shaped curves. They can also be defined using a sigmoid function. One common case is the standard logistic function defined as \n\nwhich has the following symmetry property\n\nFrom this it follows that\n\nformula_3\n\nFuzzy logic works with membership values in a way that mimics Boolean logic.\n\nTo this end, replacements for basic operators AND, OR, NOT must be available. There are several ways to this. A common replacement is called the \"Zadeh operators\":\n\nFor TRUE/1 and FALSE/0, the fuzzy expressions produce the same result as the Boolean expressions.\n\nThere are also other operators, more linguistic in nature, called \"hedges\" that can be applied. These are generally adverbs such as \"very\", or \"somewhat\", which modify the meaning of a set using a mathematical formula.\n\nHowever, an arbitrary choice table does not always define a fuzzy logic function. In the paper, a criterion has been formulated to recognize whether a given choice table defines a fuzzy logic function and a simple algorithm of fuzzy logic function synthesis has been proposed based on introduced concepts of constituents of minimum and maximum. A fuzzy logic function represents a disjunction of constituents of minimum, where a constituent of minimum is a conjunction of variables of the current area greater than or equal to the function value in this area (to the right of the function value in the inequality, including the function value).\n\nAnother set of AND/OR operators is based on multiplication\nx AND y = x*y\nx OR y = 1-(1-x)*(1-y) = x+y-x*y\n1-(1-x)*(1-y) comes from this:\nx OR y = NOT( AND( NOT(x), NOT(y) ) )\nx OR y = NOT( AND(1-x, 1-y) )\nx OR y = NOT( (1-x)*(1-y) )\nx OR y = 1-(1-x)*(1-y)\n\nIF-THEN rules map input or computed truth values to desired output truth values. Example:\nIF temperature IS very cold THEN fan_speed is stopped\nIF temperature IS cold THEN fan_speed is slow\nIF temperature IS warm THEN fan_speed is moderate\nIF temperature IS hot THEN fan_speed is high\nGiven a certain temperature, the fuzzy variable \"hot\" has a certain truth value, which is copied to the \"high\" variable.\n\nShould an output variable occur in several THEN parts, then the values from the respective IF parts are combined using the OR operator.\n\nThe goal is to get a continuous variable from fuzzy truth values.\n\nThis would be easy if the output truth values were exactly those obtained from fuzzification of a given number.\nSince, however, all output truth values are computed independently, in most cases they do not represent such a set of numbers.\nOne has then to decide for a number that matches best the \"intention\" encoded in the truth value.\nFor example, for several truth values of fan_speed, an actual speed must be found that best fits the computed truth values of the variables 'slow', 'medium' and so on.\n\nThere is no single algorithm for this purpose.\n\nA common algorithm is\n\nSince the fuzzy system output is a consensus of all of the inputs and all of the rules, fuzzy logic systems can be well behaved when input values are not available or are not trustworthy. Weightings can be optionally added to each rule in the rulebase and weightings can be used to regulate the degree to which a rule affects the output values. These rule weightings can be based upon the priority, reliability or consistency of each rule. These rule weightings may be static or can be changed dynamically, even based upon the output from other rules.\n\nMany of the early successful applications of fuzzy logic were implemented in Japan. The first notable application was on the high-speed train in Sendai, in which fuzzy logic was able to improve the economy, comfort, and precision of the ride. It has also been used in recognition of hand written symbols in Sony pocket computers, flight aid for helicopters, controlling of subway systems in order to improve driving comfort, precision of halting, and power economy, improved fuel consumption for automobiles, single-button control for washing machines, automatic motor control for vacuum cleaners with recognition of surface condition and degree of soiling, and prediction systems for early recognition of earthquakes through the Institute of Seismology Bureau of Meteorology, Japan.\n\nIn mathematical logic, there are several formal systems of \"fuzzy logic\", most of which are in the family of t-norm fuzzy logics.\n\nThe most important propositional fuzzy logics are:\n\nThese extend the above-mentioned fuzzy logics by adding universal and existential quantifiers in a manner similar to the way that predicate logic is created from propositional logic. The semantics of the universal (resp. existential) quantifier in t-norm fuzzy logics is the infimum (resp. supremum) of the truth degrees of the instances of the quantified subformula.\n\nThe notions of a \"decidable subset\" and \"recursively enumerable subset\" are basic ones for classical mathematics and classical logic. Thus the question of a suitable extension of them to fuzzy set theory is a crucial one. A first proposal in such a direction was made by E.S. Santos by the notions of \"fuzzy Turing machine\", \"Markov normal fuzzy algorithm\" and \"fuzzy program\" (see Santos 1970). Successively, L. Biacino and G. Gerla argued that the proposed definitions are rather questionable. For example, in one shows that the fuzzy Turing machines are not adequate for fuzzy language theory since there are natural fuzzy languages intuitively computable that cannot be recognized by a fuzzy Turing Machine. Then, they proposed the following definitions. Denote by \"Ü\" the set of rational numbers in [0,1]. Then a fuzzy subset \"s\" : \"S\" formula_4[0,1] of a set \"S\" is recursively enumerable if a recursive map \"h\" : \"S\"×\"N\" formula_4\"Ü\" exists such that, for every \"x\" in \"S\", the function \"h\"(\"x\",\"n\") is increasing with respect to \"n\" and \"s\"(\"x\") = lim \"h\"(\"x\",\"n\").\nWe say that \"s\" is \"decidable\" if both \"s\" and its complement –\"s\" are recursively enumerable. An extension of such a theory to the general case of the L-subsets is possible (see Gerla 2006).\nThe proposed definitions are well related with fuzzy logic. Indeed, the following theorem holds true (provided that the deduction apparatus of the considered fuzzy logic satisfies some obvious effectiveness property).\n\nAny \"axiomatizable\" fuzzy theory is recursively enumerable. In particular, the fuzzy set of logically true formulas is recursively enumerable in spite of the fact that the crisp set of valid formulas is not recursively enumerable, in general. Moreover, any axiomatizable and complete theory is decidable.\n\nIt is an open question to give supports for a \"Church thesis\" for fuzzy mathematics, the proposed notion of recursive enumerability for fuzzy subsets is the adequate one. In order to solve this, an extension of the notions of fuzzy grammar and fuzzy Turing machine are necessary. Another open question is to start from this notion to find an extension of Gödel's theorems to fuzzy logic.\n\nOnce fuzzy relations are defined, it is possible to develop fuzzy relational databases. The first fuzzy relational database, FRDB, appeared in Maria Zemankova's dissertation (1983). Later, some other models arose like the Buckles-Petry model, the Prade-Testemale Model, the Umano-Fukami model or the GEFRED model by J.M. Medina, M.A. Vila et al.\n\nFuzzy querying languages have been defined, such as the SQLf by P. Bosc et al. and the FSQL by J. Galindo et al. These languages define some structures in order to include fuzzy aspects in the SQL statements, like fuzzy conditions, fuzzy comparators, fuzzy constants, fuzzy constraints, fuzzy thresholds, linguistic labels etc.\n\nFuzzy logic and probability address different forms of uncertainty. While both fuzzy logic and probability theory can represent degrees of certain kinds of subjective belief, fuzzy set theory uses the concept of fuzzy set membership, i.e., how much an observation is within a vaguely defined set, and probability theory uses the concept of subjective probability, i.e., likelihood of some event or condition. The concept of fuzzy sets was developed in the mid-twentieth century at Berkeley as a response to the lacking of probability theory for jointly modelling uncertainty and vagueness.\n\nBart Kosko claims in Fuzziness vs. Probability that probability theory is a subtheory of fuzzy logic, as questions of degrees of belief in mutually-exclusive set membership in probability theory can be represented as certain cases of non-mutually-exclusive graded membership in fuzzy theory. In that context, he also derives Bayes' theorem from the concept of fuzzy subsethood. Lotfi A. Zadeh argues that fuzzy logic is different in character from probability, and is not a replacement for it. He fuzzified probability to fuzzy probability and also generalized it to possibility theory. (cf.)\n\nMore generally, fuzzy logic is one of many different extensions to classical logic intended to deal with issues of uncertainty outside of the scope of classical logic, the inapplicability of probability theory in many domains, and the paradoxes of Dempster-Shafer theory.\n\nComputational theorist Leslie Valiant uses the term \"ecorithms\" to describe how many less exact systems and techniques like fuzzy logic (and \"less robust\" logic) can be applied to learning algorithms. Valiant essentially redefines machine learning as evolutionary. In general use, ecorithms are algorithms that learn from their more complex environments (hence \"eco-\") to generalize, approximate and simplify solution logic. Like fuzzy logic, they are methods used to overcome continuous variables or systems too complex to completely enumerate or understand discretely or exactly. Ecorithms and fuzzy logic also have the common property of dealing with possibilities more than probabilities, although feedback and feed forward, basically stochastic weights, are a feature of both when dealing with, for example, dynamical systems.\n\nCompensatory fuzzy logic (CFL) is a branch of fuzzy logic with modified rules for conjunction and disjunction. When the truth value of one component of a conjunction or disjunction is increased or decreased, the other component is decreased or increased to compensate. This increase or decrease in truth value may be offset by the increase or decrease in another component. An offset may be blocked when certain thresholds are met. Proponents claim that CFL allows for better computational semantic behaviors and mimic natural language. \n\nCompensatory Fuzzy Logic consists of four continuous operators: conjunction (c); disjunction (d); fuzzy strict order (or); and negation (n). The conjunction is the geometric mean and its dual as conjunctive and disjunctive operators.\n\nThe IEEE 1855, the IEEE STANDARD 1855–2016, is about a specification language named Fuzzy Markup Language (FML) developed by the IEEE Standards Association. FML allows modelling a fuzzy logic system in a human-readable and hardware independent way. FML is based on eXtensible Markup Language (XML). The designers of fuzzy systems with FML have a unified and high-level methodology for describing interoperable fuzzy systems. IEEE STANDARD 1855–2016 uses the W3C XML Schema definition language to define the syntax and semantics of the FML programs.\n\nPrior to the introduction of FML, fuzzy logic practitioners could exchange information about their fuzzy algorithms by  adding to their software functions the ability to read, correctly parse, and store the results of their work in a  form compatible with the Fuzzy Control Language (FCL) described and specified by Part 7 of IEC 61131.\n\n\n",
    "id": "49180",
    "title": "Fuzzy logic"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1028978",
    "text": "Combs method\n\nThe Combs method is a method of writing fuzzy logic rules described by William E. Combs in 1997. It is designed to prevent combinatorial explosion in fuzzy logic rules.\n\nThe Combs method takes advantage of the logical equality formula_1.\n\nThe simplest proof of given equality involves usage of truth tables:\nSuppose we have a fuzzy system that considers N variables at a time, each of which can fit into at least one of S sets. The number of rules necessary to cover all the cases in a traditional fuzzy system is formula_2, whereas the Combs method would need only formula_3 rules. For example, if we have five sets and five variables to consider to produce one output, covering all the cases would require 3125 rules in a traditional system, while the Combs method would require only 25 rules, taming the combinatorial explosion that occurs when more inputs or more sets are added to the system.\n\nThis article will focus on the Combs method itself. To learn more about the way rules are traditionally formed, see fuzzy logic and fuzzy associative matrix.\n\nSuppose we were designing an artificial personality system that determined how friendly the personality is supposed to be towards a person in a strategic video game. The personality would consider its own fear, trust, and love in the other person. A set of rules in the Combs system might look like this:\nThe table translates to:\n\nIn this case, because the table follows a straightforward pattern in the output, it could be rewritten as:\nEach column of the table maps to the output provided in the last row. To obtain the output of the system, we just average the outputs of each rule for that output. For example, to calculate how much the computer is Enemies with the player, we take the average of how much the computer is Unafraid, Distrusting, and Unloving of the player. When all three averages are obtained, the result can then be defuzzified by any of the traditional means.\n\n",
    "id": "1028978",
    "title": "Combs method"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=6983799",
    "text": "AgentSheets\n\nAgentSheets is a Cyberlearning tool to teach students programming and related information technology skills through game design.\n\nAgentSheets is supported by a middle and high school curriculum called Scalable Game Design aligned with the ISTE National Educational Technology Standards (NETS). The mission of this project is to reinvent computer science in public schools by motivating & educating all students including women and underrepresented communities to learn about computer science through game design starting at the middle school level. Through this curriculum students build increasingly sophisticated games and, as part of this process, learn about computational concepts at the level of computational thinking that are relevant to game design as well as to computational science. The curriculum is made available through the Scalable Game Design Wiki. Research investigating motivational aspects of computer science education in public schools is currently exploring the introduction of game design in representative regions of the USA including technology hubs, inner city, rural and remote/tribal areas. Previous research has already found that game design with AgentSheets is universally accessible across gender as well as ethnicity and is not limited to students interested in playing video games.\n\nThe results of the NSF ITEST program supported research investigating motivational and educational aspects of introducing computer science at the middle school level are extremely positive in terms of motivational levels, number of participants and participation of women and underrepresented communities. The participation is extremely high because most middle schools participating in the study have made Scalable Game Design a module that is part of existing required courses (e.g., computer power with keyboarding and power point). Many of the middle schools instruct all of their students in scalable game design reaching in some schools over 900 students per year, per school. Of the well over 1000 students participating in the project in the first semester over 52% were girls. Of the girls 85% enjoyed the scalable game design course and 78% would like to take another game design course.\n\nThe built-in drag-and-drop language is accessible enough that students without programming background can make their own simple Frogger-like game, and publish it on the Web, in their first session. At the same time, AgentSheets is powerful enough to make sophisticated The Sims-like games with artificial intelligence. To transition from visual programming to more traditional programming students can render their games into Java source code.\n\nSimilar to a spreadsheet, an agentsheet is a computational grid. Unlike spreadsheets, this grid does not just contain numbers and strings but so called agents. These agents are represented by pictures, can be animated, make sounds, react to mouse/keyboard interactions, can read web pages, can speak and even recognize speech commands (Mac). This grid is well suited to build computational science applications modeling complex scientific phenomena with up to tens of thousands of agents. The grid is useful to build agent-based simulations including cellular automata or diffusion-based models. These models are used in a wide variety of applications. How does a mudslide work? When does a bridge collapse? How fragile are ecosystems? This ability to support game as well as computational science applications with the inclusion of scientific visualizations makes AgentSheets a unique computational thinking tool that is used computer science and STEM education.\n\nAgentSheets is used in a number of contexts worldwide:\n\n\nThe original goal of this research was to explore new models of computational thinking. The first prototype of AgentSheets ran in 1989 at the University of Colorado, NCAR, Connection Machine 2. The Connection Machine is a highly parallel computer with up to 65,536 CPUs. Realizing how hard it was to program the Connection Machine the insight that \"CPU cycles will always be ultimately cheaper than cognitive cycles\" led to the exploration of several new programming paradigms:\n\n",
    "id": "6983799",
    "title": "AgentSheets"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1908395",
    "text": "Artificial brain\n\nAn artificial brain (or artificial mind) is software and hardware with cognitive abilities similar to those of the animal or human brain.\n\nResearch investigating \"artificial brains\" and brain emulation plays three important roles in science:\n\nAn example of the first objective is the project reported by Aston University in Birmingham, England where researchers are using biological cells to create \"neurospheres\" (small clusters of neurons) in order to develop new treatments for diseases including Alzheimer's, motor neurone and Parkinson's disease.\n\nThe second objective is a reply to arguments such as John Searle's Chinese room argument, Hubert Dreyfus' critique of AI or Roger Penrose's argument in \"The Emperor's New Mind\". These critics argued that there are aspects of human consciousness or expertise that can not be simulated by machines. One reply to their arguments is that the biological processes inside the brain can be simulated to any degree of accuracy. This reply was made as early as 1950, by Alan Turing in his classic paper \"Computing Machinery and Intelligence\".\n\nThe third objective is generally called artificial general intelligence by researchers. However, Ray Kurzweil prefers the term \"strong AI\". In his book \"The Singularity is Near\", he focuses on whole brain emulation using conventional computing machines as an approach to implementing artificial brains, and claims (on grounds of computer power continuing an exponential growth trend) that this could be done by 2025. Henry Markram, director of the Blue Brain project (which is attempting brain emulation), made a similar claim (2020) at the Oxford TED conference in 2009.\n\n Although direct human brain emulation using artificial neural networks on a high-performance computing engine is a commonly discussed approach, there are other approaches. An alternative artificial brain implementation could be based on Holographic Neural Technology (HNeT) non linear phase coherence/decoherence principles. The analogy has been made to quantum processes through the core synaptic algorithm which has strong similarities to the quantum mechanical wave equation.\n\nEvBrain is a form of evolutionary software that can evolve \"brainlike\" neural networks, such as the network immediately behind the retina.\n\nIn November 2008, IBM received a US$4.9 million grant from the Pentagon for research into creating intelligent computers. The Blue Brain project is being conducted with the assistance of IBM in Lausanne. The project is based on the premise that it is possible to artificially link the neurons \"in the computer\" by placing thirty million synapses in their proper three-dimensional position.\n\nSome proponents of strong AI speculated that computers in connection with Blue Brain and Soul Catcher may exceed human intellectual capacity by around 2015, and that it is likely that we will be able to download the human brain at some time around 2050.\n\nWhile \"Blue Brain\" is able to represent complex neural connections on the large scale, the project does not achieve the link between brain activity and behaviors executed by the brain. In 2012, project Spaun (Semantic Pointer Architecture Unified Network) attempted to model multiple parts of the human brain through large-scale representations of neural connections that generate complex behaviors in addition to mapping.\n\nSpaun's design recreates elements of human brain anatomy. The model, consisting of approximately 2.5 million neurons, includes features of the visual and motor cortices, GABAergic and dopaminergic connections, the ventral tegmental area (VTA), substantia nigra, and others. The design allows for several functions in response to eight tasks, using visual inputs of typed or handwritten characters and outputs carried out by a mechanical arm. Spaun's functions include copying a drawing, recognizing images, and counting.\n\nThere are good reasons to believe that, regardless of implementation strategy, the predictions of realising artificial brains in the near future are optimistic. In particular brains (including the human brain) and cognition are not currently well understood, and the scale of computation required is unknown. Another near term limitation is that all current approaches for brain simulation require orders of magnitude larger power consumption compared with a human brain. The human brain consumes about 20 W of power whereas current supercomputers may use as much as 1 MW or an order of 100,000 more.\n\nSome critics of brain simulation believe that it is simpler to create general intelligent action directly without imitating nature. Some commentators have used the analogy that early attempts to construct flying machines modeled them after birds, but that modern aircraft do not look like birds.\n\n",
    "id": "1908395",
    "title": "Artificial brain"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=24238010",
    "text": "Any-angle path planning\n\nAny-angle path planning algorithms are a subset of pathfinding algorithms that search for a path between two points in space and allow the turns in the path to have any angle. The result is a path that goes directly toward the goal and has relatively few turns. Other pathfinding algorithms such as A* constrain the paths to a grid, which produces jagged, indirect paths. Any-angle algorithms are able to find optimal or near-optimal paths by incorporating the any-angle search into the algorithm itself. Algorithms such as A* Post Smoothing that smooth a path found by a grid constrained algorithm are unable to reliably find optimal paths since they cannot change what side of a blocked cell is traversed.\n\nAny-angle path planning algorithms are necessary in order to quickly find an optimal path. For a world represented by a grid of blocked and unblocked cells, the brute-force way to find an any-angle path is to search the corresponding visibility graph. This is problematic since the number of edges in a graph with formula_1 vertices is formula_2. Searching the discrete grid graph can be done quickly since the number of edges grows linearly with the number of vertices, but the paths are not optimal since the angle of the turns are constrained to 45° or 90°, which will add turns and increase the overall length of the path. Smoothing a grid-constrained path after does not fix this problem since the algorithm that found that path did not look at all possible paths. Any-angle path planning algorithms find shorter paths than the grid-constrained algorithms while taking roughly same amount of time to compute.\n\nSo far, four main any-angle path planning algorithms that are based on the heuristic search algorithm A* have been developed, all of which propagate information along grid edges:\n\n\n\nBesides, for search in high-dimensional search spaces, such as when the configuration space of the system involves many degrees of freedom that need to be considered (see Motion planning), and/or momentum needs to be considered (which could effectively double the number of dimensions of the search space; this larger space including momentum is known as the phase space), variants of the rapidly-exploring random tree (RRT) have been developed that (almost surely) converge to the optimal path by increasingly finding shorter and shorter paths:\n\n\nHybrid A* was created by Stanford Racing as part of the navigation system for Junior, their entry to the DARPA Urban Challenge. Hybrid A* is continuous and tracks the vehicle's position and orientation. This ensures that the path generated can be followed by the vehicle, unlike the paths generated by A* for Field D*, which both produce sharp turns, and do not consider the geometry or movement constraints of the vehicle.\n\n\n\n",
    "id": "24238010",
    "title": "Any-angle path planning"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=9025771",
    "text": "Anytime algorithm\n\nIn computer science, an anytime algorithm is an algorithm that can return a valid solution to a problem even if it is interrupted before it ends. The algorithm is expected to find better and better solutions the more time it keeps running.\n\nMost algorithms run to completion: they provide a single answer after performing some fixed amount of computation. In some cases, however, the user may wish to terminate the algorithm prior to completion. The amount of the computation required may be substantial, for example, and computational resources might need to be reallocated. Most algorithms either run to completion or they provide no useful solution information. Anytime algorithms, however, are able to return a partial answer, whose quality depends on the amount of computation they were able to perform. The answer generated by anytime algorithms is an approximation of the correct answer.\n\nAn anytime algorithm may be also called an \"interruptible algorithm\". They are different from contract algorithms, which must declare a time in advance; in an anytime algorithm, a process can just announce that it is terminating.\n\nThe goal of anytime algorithms are to give intelligent systems the ability to make results of better quality in return for turn-around time. They are also supposed to be flexible in time and resources. They are important because artificial intelligence or AI algorithms can take a long time to complete results. This algorithm is designed to complete in a shorter amount of time. Also, these are intended to have a better understanding that the system is dependent and restricted to its agents and how they work cooperatively. An example is the Newton-Raphson iteration applied to finding the square root of a number. Another example that uses anytime algorithms is trajectory problems when you're aiming for a target; the object is moving through space while waiting for the algorithm to finish and even an approximate answer can significantly improve its accuracy if given early.\n\nWhat makes anytime algorithms unique is their ability to return many possible outcomes for any given input. An anytime algorithm uses many well defined quality measures to monitor progress in problem solving and distributed computing resources. It keeps searching for the best possible answer with the amount of time that it is given. It may not run until completion and may improve the answer if it is allowed to run longer.\nThis is often used for large decision set problems. This would generally not provide useful information unless it is allowed to finish. While this may sound similar to dynamic programming, the difference is that it is fine-tuned through random adjustments, rather than sequential.\n\nAnytime algorithms are designed so that it can be told to stop at any time and would return the best result it has found so far. This is why it is called an interruptible algorithm. Anytime algorithms also maintain the last result, so that if they are given more time, they can continue from where they left off to obtain an even better result.\n\nWhen the decider has to act, there must be some ambiguity. Also, there must be some idea about how to solve this ambiguity. This idea must be translatable to a state to action diagram.\n\nThe performance profile estimates the quality of the results based on the input and the amount of time that is allotted to the algorithm. The better the estimate, the sooner the result would be found. Some systems have a larger database that gives the probability that the output is the expected output. It is important to note that one algorithm can have several performance profiles. Most of the time performance profiles are constructed using mathematical statistics using representative cases. For example, in the traveling salesman problem, the performance profile was generated using a user-defined special program to generate the necessary statistics. In this example, the performance profile is the mapping of time to the expected results. This quality can be measured in several ways:\n\n\nInitial behavior: While some algorithms start with immediate guesses, others take a more calculated approach and have a start up period before making any guesses.\n\n\n",
    "id": "9025771",
    "title": "Anytime algorithm"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=361157",
    "text": "Bio-inspired computing\n\nBio-inspired computing, short for biologically inspired computing, is a field of study that loosely knits together subfields related to the topics of connectionism, social behaviour and emergence. It is often closely related to the field of artificial intelligence, as many of its pursuits can be linked to machine learning. It relies heavily on the fields of biology, computer science and mathematics. Briefly put, it is the use of computers to model the living phenomena, and simultaneously the study of life to improve the usage of computers. Biologically inspired computing is a major subset of natural computation.\n\nSome areas of study encompassed under the canon of biologically inspired computing, and their biological counterparts:\n\n\nThe way in which bio-inspired computing differs from the traditional artificial intelligence (AI) is in how it takes a more evolutionary approach to learning, as opposed to what could be described as 'creationist' methods used in traditional AI. In traditional AI, intelligence is often programmed from above: the programmer is the creator, and makes something and imbues it with its intelligence. Bio-inspired computing, on the other hand, takes a more bottom-up, decentralised approach; bio-inspired techniques often involve the method of specifying a set of simple rules, a set of simple organisms which adhere to those rules, and a method of iteratively applying those rules. For example, training a virtual insect to navigate in an unknown terrain for finding food includes six simple rules. The insect is trained to \nThe virtual insect controlled by the trained spiking neural network can find food after training in any unknown terrain. After several generations of rule application it is usually the case that some forms of complex behaviour arise. Complexity gets built upon complexity until the end result is something markedly complex, and quite often completely counterintuitive from what the original rules would be expected to produce (see complex systems). For this reason, in neural network models, it is necessary to accurately model an \"in vivo\" network, by live collection of \"noise\" coefficients that can be used to refine statistical inference and extrapolation as system complexity increases.\n\nNatural evolution is a good analogy to this method–the rules of evolution (selection, recombination/reproduction, mutation and more recently transposition) are in principle simple rules, yet over millions of years have produced remarkably complex organisms. A similar technique is used in genetic algorithms.\n\n\n\n\"(the following are presented in ascending order of complexity and depth, with those new to the field suggested to start from the top)\"\n\n\n",
    "id": "361157",
    "title": "Bio-inspired computing"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=15291723",
    "text": "Hierarchical control system\n\nA hierarchical control system is a form of control system in which a set of devices and governing software is arranged in a hierarchical tree. When the links in the tree are implemented by a computer network, then that hierarchical control system is also a form of networked control system.\n\nA human-built system with complex behavior is often organized as a hierarchy. For example, a command hierarchy has among its notable features the organizational chart of superiors, subordinates, and lines of organizational communication. Hierarchical control systems are organized similarly to divide the decision making responsibility.\n\nEach element of the hierarchy is a linked node in the tree. Commands, tasks and goals to be achieved flow down the tree from superior nodes to subordinate nodes, whereas sensations and command results flow up the tree from subordinate to superior nodes. Nodes may also exchange messages with their siblings. The two distinguishing features of a hierarchical control system are related to its layers.\n\n\nBesides artificial systems, an animal's control systems are proposed to be organized as a hierarchy. In perceptual control theory, which postulates that an organism's behavior is a means of controlling its perceptions, the organism's control systems are suggested to be organized in a hierarchical pattern as their perceptions are constructed so.\n\nThe accompanying diagram is a general hierarchical model which shows functional manufacturing levels using computerised control of an industrial control system.\n\nReferring to the diagram;\n\n\nAmong the robotic paradigms is the hierarchical paradigm in which a robot operates in a top-down fashion, heavy on planning, especially motion planning. Computer-aided production engineering has been a research focus at NIST since the 1980s. Its Automated Manufacturing Research Facility was used to develop a five layer production control model. In the early 1990s DARPA sponsored research to develop distributed (i.e. networked) intelligent control systems for applications such as military command and control systems. NIST built on earlier research to develop its Real-Time Control System (RCS) and Real-time Control System Software which is a generic hierarchical control system that has been used to operate a manufacturing cell, a robot crane, and an automated vehicle.\n\nIn November 2007, DARPA held the Urban Challenge. The winning entry, Tartan Racing employed a hierarchical control system, with layered mission planning, motion planning, behavior generation, perception, world modelling, and mechatronics.\n\nSubsumption architecture is a methodology for developing artificial intelligence that is heavily associated with behavior based robotics. This architecture is a way of decomposing complicated intelligent behavior into many \"simple\" behavior modules, which are in turn organized into layers. Each layer implements a particular goal of the software agent (i.e. system as a whole), and higher layers are increasingly more abstract. Each layer's goal subsumes that of the underlying layers, e.g. the decision to move forward by the eat-food layer takes into account the decision of the lowest obstacle-avoidance layer. Behavior need not be planned by a superior layer, rather behaviors may be triggered by sensory inputs and so are only active under circumstances where they might be appropriate.\n\nReinforcement learning has been used to acquire behavior in a hierarchical control system in which each node can learn to improve its behavior with experience.\nJames Albus, while at NIST, developed a theory for intelligent system design named the Reference Model Architecture (RMA), which is a hierarchical control system inspired by RCS. Albus defines each node to contain these components.\nAt its lowest levels, the RMA can be implemented as a subsumption architecture, in which the world model is mapped directly to the controlled process or real world, avoiding the need for a mathematical abstraction, and in which time-constrained reactive planning can be implemented as a finite state machine. Higher levels of the RMA however, may have sophisticated mathematical world models and behavior implemented by automated planning and scheduling. Planning is required when certain behaviors cannot be triggered by current sensations, but rather by predicted or anticipated sensations, especially those that come about as result of the node's actions.\n\n\n",
    "id": "15291723",
    "title": "Hierarchical control system"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=383162",
    "text": "Information extraction\n\nInformation extraction (IE) is the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents. In most of the cases this activity concerns processing human language texts by means of natural language processing (NLP). Recent activities in multimedia document processing like automatic annotation and content extraction out of images/audio/video could be seen as information extraction.\n\nDue to the difficulty of the problem, current approaches to IE focus on narrowly restricted domains. An example is the extraction from newswire reports of corporate mergers, such as denoted by the formal relation: \nfrom an online news sentence such as:\n\nA broad goal of IE is to allow computation to be done on the previously unstructured data. A more specific goal is to allow logical reasoning to draw inferences based on the logical content of the input data. Structured data is semantically well-defined data from a chosen target domain, interpreted with respect to category and context.\n\nInformation Extraction is the part of a greater puzzle which deals with the problem of devising automatic methods for text management, beyond its transmission, storage and display. The discipline of information retrieval (IR) has developed automatic methods, typically of a statistical flavor, for indexing large document collections and classifying documents. Another complementary approach is that of natural language processing (NLP) which has solved the problem of modelling human language processing with considerable success when taking into account the magnitude of the task. In terms of both difficulty and emphasis, IE deals with tasks in between both IR and NLP. In terms of input, IE assumes the existence of a set of documents in which each document follows a template, i.e. describes one or more entities or events in a manner that is similar to those in other documents but differing in the details. An example, consider a group of newswire articles on Latin American terrorism with each article presumed to be based upon one or more terroristic acts. We also define for any given IE task a template, which is a(or a set of) case frame(s) to hold the information contained in a single document. For the terrorism example, a template would have slots corresponding to the perpetrator, victim, and weapon of the terroristic act, and the date on which the event happened. An IE system for this problem is required to “understand” an attack article only enough to find data corresponding to the slots in this template.\n\nInformation extraction dates back to the late 1970s in the early days of NLP. An early commercial system from the mid-1980s was JASPER built for Reuters by the Carnegie Group with the aim of providing real-time financial news to financial traders.\n\nBeginning in 1987, IE was spurred by a series of Message Understanding Conferences. MUC is a competition-based conference that focused on the following domains: \n\nConsiderable support came from the U.S. Defense Advanced Research Projects Agency (DARPA), who wished to automate mundane tasks performed by government analysts, such as scanning newspapers for possible links to terrorism.\n\nThe present significance of IE pertains to the growing amount of information available in unstructured form. Tim Berners-Lee, inventor of the world wide web, refers to the existing Internet as the web of \"documents\" and advocates that more of the content be made available as a web of \"data\". Until this transpires, the web largely consists of unstructured documents lacking semantic metadata. Knowledge contained within these documents can be made more accessible for machine processing by means of transformation into relational form, or by marking-up with XML tags. An intelligent agent monitoring a news data feed requires IE to transform unstructured data into something that can be reasoned with. A typical application of IE is to scan a set of documents written in a natural language and populate a database with the information extracted.\n\nApplying information extraction to text is linked to the problem of text simplification in order to create a structured view of the information present in free text. The overall goal being to create a more easily machine-readable text to process the sentences. Typical subtasks of IE include:\n\n\nNote that this list is not exhaustive and that the exact meaning of IE activities is not commonly accepted and that many approaches combine multiple sub-tasks of IE in order to achieve a wider goal. Machine learning, statistical analysis and/or natural language processing are often used in IE.\n\nIE on non-text documents is becoming an increasingly interesting topic in research, and information extracted from multimedia documents can now be expressed in a high level structure as it is done on text. This naturally leads to the fusion of extracted information from multiple kinds of documents and sources.\n\nIE has been the focus of the MUC conferences. The proliferation of the Web, however, intensified the need for developing IE systems that help people to cope with the enormous amount of data that is available online. Systems that perform IE from online text should meet the requirements of low cost, flexibility in development and easy adaptation to new domains. MUC systems fail to meet those criteria. Moreover, linguistic analysis performed for unstructured text does not exploit the HTML/XML tags and the layout formats that are available in online texts. As a result, less linguistically intensive approaches have been developed for IE on the Web using wrappers, which are sets of highly accurate rules that extract a particular page's content. Manually developing wrappers has proved to be a time-consuming task, requiring a high level of expertise. Machine learning techniques, either supervised or unsupervised, have been used to induce such rules automatically.\n\n\"Wrappers\" typically handle highly structured collections of web pages, such as product catalogs and telephone directories. They fail, however, when the text type is less structured, which is also common on the Web. Recent effort on \"adaptive information extraction\" motivates the development of IE systems that can handle different types of text, from well-structured to almost free text -where common wrappers fail- including mixed types. Such systems can exploit shallow natural language knowledge and thus can be also applied to less structured texts.\n\nA recent development is Visual Information Extraction, that relies on rendering a webpage in a browser and creating rules based on the proximity of regions in the rendered web page. This helps in extracting entities from complex web pages that may exhibit a visual pattern, but lack a discernible pattern in the HTML source code.\n\nThree standard approaches are now widely accepted:\n\nNumerous other approaches exist for IE including hybrid approaches that combine some of the standard approaches previously listed.\n\n\n\n\n\n",
    "id": "383162",
    "title": "Information extraction"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=30306484",
    "text": "Cognitive infocommunications\n\nCognitive infocommunications (CogInfoCom) investigates the link between the research areas of infocommunications and the cognitive sciences, as well as the various engineering applications which have emerged as the synergic combination of these sciences.\n\nThe primary goal of CogInfoCom is to provide a systematic view of how cognitive processes can co-evolve with infocommunications devices so that the capabilities of the human brain may not only be extended through these devices, irrespective of geographical distance, but may also interact with the capabilities of any artificially cognitive system. This merging and extension of cognitive capabilities is targeted towards engineering applications in which artificial and/or natural cognitive systems are enabled to work together more effectively.\n\nTwo important dimensions of cognitive infocommunications are the mode of communication and the type of communication.\nThe mode of communication refers to the actors at the two endpoints of communication:\n\n\nThe type of communication refers to the type of information that is conveyed between the two communicating entities, and the way in which this is done:\n\n\n\nThe first draft definition of CogInfoCom was given in \"Cognitive Infocommunications: CogInfoCom\". The definition was finalized based on the paper with the joint participation of the Startup Committee at the 1st International Workshop on Cognitive Infocommunications, held in Tokyo, Japan in 2010. A recent overview and further information can be found in, and in the two special issues on CogInfoCom which have been published since then, and at the official website of CogInfoCom.\n",
    "id": "30306484",
    "title": "Cognitive infocommunications"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=30667214",
    "text": "Evolving intelligent system\n\nIntelligence and intelligent systems has to be able to evolve, self-develop, self-learn continuously in order to reflect the dynamically evolving environment. The concept of Evolving Intelligent Systems (EISs) was conceived around the turn of the century with the phrase EIS itself coined for the first time in and expanded in. EISs develop their structure, functionality and internal knowledge representation through autonomous learning from data streams generated by the possibly unknown environment and from the system self-monitoring. EISs consider a gradual development of the underlying (fuzzy or neuro-fuzzy) system structure and differ from evolutionary and genetic algorithms which consider such phenomena as chromosomes crossover, mutation, selection and reproduction, parents and off-springs. The evolutionary fuzzy and neuro systems are sometimes also called “evolving” which leads to some confusion. This was more typical for the first works on this topic in the late 1990s.\n\nEISs can be implemented, for example, using neural networks or fuzzy rule-based models. The first neural networks which consider an evolving structure were published in. These were later expanded by N. Kasabov and P. Angelov for the neuro-fuzzy models. P. Angelov introduced the evolving fuzzy rule-based systems (EFSs) as the first mathematical self-learning model that can dynamically evolve its internal structure and is human interpretable and coined the phrase EFS. Contemporarily, the offline incremental approach for learning an EIS, namely, EFuNN, was proprosed by N. Kasabov. P. Angelov, D. Filev, N. Kasabov and O. Cordon organised the first IEEE Symposium on EFSs in 2006 (the proceedings of the conference can be found in ). EFSs include a formal (and mathematically sound) learning mechanism to extract it from streaming data. One of the earliest and the most widely cited comprehensive survey on EFSs was done in 2008. A later comprehensive survey on EFSs with real applications was done in 2011 by E. Lughofer. Other works that contributed further to this area in the following years expanded it to evolving participatory learning, evolving grammar, evolving decision trees, evolving human behaviour modelling, self-calibrating (evolving) sensors (eSensors), evolving fuzzy rule-based classifiers, evolving fuzzy controllers, autonomous fault detectors. More recently, the stability of the evolving fuzzy rule-based systems that consist of the structure learning and the fuzzily weighted recursive least square parameter update method has been proven by Rong and Angelov.\n\nEISs form the theoretical and methodological basis for the Autonomous Learning Machines (ALMA) and autonomous multi-model systems (ALMMo) as well as of the Autonomous Learning Systems. Evolving Fuzzy Rule-based classifiers, in particular, is a very powerful new concept that offers much more than simply incremental or online classifiers – it can cope with new classes being added or existing classes being merged. This is much more than just adapting to new data samples being added or classification surfaces being evolved. Fuzzy rule-based classifiers are the methodological basis of a new approach to deep learning that was until now considered as a form of multi-layered neural networks. Deep Learning offers high precision levels surpassing the level of human ability and grabbed the imagination of the researchers, industry and the wider public. However, it has a number of intrinsic constraints and limitations. These include:\nMost, if not all, of the above limitations can be avoided with the use of the Deep (Fuzzy) Rule-based Classifiers, which were recently introduced based on ALMMo, while achieving similar or even better performance. The resulting prototype-based IF…THEN…models are fully interpretable and dynamically evolving (they can adapt quickly and automatically to new data patterns or even new classes). They are non-parametric and, therefore, their training is non-iterative and fast (it can take few milliseconds per data sample/image on a normal laptop which contrasts with the multiple hours the current deep learning methods require for training even when they use GPUs and HPC). Moreover, they can be trained incrementally/online/in real-time.\n",
    "id": "30667214",
    "title": "Evolving intelligent system"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=16794474",
    "text": "Smart objects\n\nA smart object is an object that enhances the interaction with not only people but also with other smart objects. It can not only refer to interaction with physical world objects but also to interaction with virtual (computing environment) objects. A smart physical object may be created either as an artifact or manufactured product or by embedding electronic tags such as RFID tags or sensors into non-smart physical objects. Smart virtual objects are created as software objects that are intrinsic when creating and operating a virtual or cyber world simulation or game. The concept of a smart object has several origins and uses, see History. There are also several overlapping terms, see also smart device, tangible object or tangible user interface and Thing as in the Internet of things.\n\nIn the early 1990s, Mark Weiser, from whom the term ubiquitous computing originated, referred to a vision \"When almost every object either contains a computer or can have a tab attached to it, obtaining information will be trivial\", \nAlthough Weiser did not specifically refer to an object as being smart, his early work did imply that smart physical objects are smart in the sense that they act as digital information sources. Hiroshi Ishii and Brygg Ullmer refer to tangible objects in terms of tangibles bits or tangible user interfaces that enable users to \"grasp & manipulate\" bits in the center of users' attention by coupling the bits with everyday physical objects and architectural surfaces.\n\nThe smart object concept was introduced by Marcelo Kallman and Daniel Thalmann as an object that can describe its own possible interactions. The main focus here is to model interactions of smart virtual objects with virtual humans, agents, in virtual worlds. The opposite approach to smart objects is 'plain' objects that do not provide this information. The additional information provided by this concept enables far more general interaction schemes, and can greatly simplify the planner of an artificial intelligence agent.\nIn contrast to smart virtual objects used in virtual worlds, Lev Manovich focuses on physical space filled with electronic and visual information. Here, \"smart objects\" are described as \"objects connected to the Net; objects that can sense their users and display smart behaviour\".\n\nMore recently in the early 2010s, smart objects are being proposed as a key enabler for the vision of the Internet of things. The combination of the Internet and emerging technologies such as near field communications, real-time localization, and embedded sensors enables everyday objects to be transformed into smart objects that can understand and react to their environment. Such objects are building blocks for the Internet of things and enable novel computing applications.\n\nAlthough we can view interaction with physical smart object in the physical world as distinct from interaction with virtual smart objects in a virtual simulated world, these can be related. Poslad considers the progression of: how\n\nThe concept smart for a smart physical object simply means that it is active, digital, networked, can operate to some extent autonomously, is reconfigurable and has local control of the resources it needs such as energy, data storage, etc. Note, a smart object does not necessarily need to be intelligent as in exhibiting a strong essence of artificial intelligence—although it can be designed to also be intelligent.\n\nPhysical world smart objects can be described in terms of three properties:\n\nBased upon these properties, these have been classified into three types:\n\nFor the virtual object in a virtual world case, an object is called smart when it has the ability to describe its possible interactions. This focuses on constructing a virtual world using only virtual objects that contain their own interaction information. There are four basic elements to constructing such a smart virtual object framework.\n\nSome versions of smart objects also include animation information in the object information, but this is not considered to be an efficient approach, since this can make objects inappropriately oversized.\n\n\n\n",
    "id": "16794474",
    "title": "Smart objects"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=26269089",
    "text": "Stochastic semantic analysis\n\nStochastic semantic analysis is an approach used in computer science as a semantic component of natural language understanding.\n\nStochastic models generally use the definition of segments of words as basic semantic units for the semantic models, and in some cases involve a two layered approach.\n\nExample applications have a wide range. In machine translation, it has been applied to the translation of spontaneous conversational speech among different languages. In the area of \"spoken language understanding\" the fact that spoken sentences often do not follow the grammar of a language and involve self-corrections, repetitions, and other irregularities, the use of stochastic semantic has been suggested as a natural fit to achieve robustness to deal with noise due to the spontaneous nature of spoken language.\n\n",
    "id": "26269089",
    "title": "Stochastic semantic analysis"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=18313661",
    "text": "Knowledge compilation\n\nKnowledge compilation is a family of approaches for addressing the intractability of\na number of artificial intelligence problems.\n\nA propositional model is compiled in an off-line phase in order to support some queries in polytime. Many ways of compiling a propositional models exist.\nAmong others: NNF, DNNF, d-DNNF, BDD, SDD, MDD, DNF and CNF.\n\nDifferent compiled representations have different properties.\nThe three main properties are:\n",
    "id": "18313661",
    "title": "Knowledge compilation"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=19835615",
    "text": "Gabbay's separation theorem\n\nIn mathematical logic and computer science, Gabbay's separation theorem, named after Dov Gabbay, states that any arbitrary temporal logic formula can be rewritten in a logically equivalent \"past → future\" form. I.e. the future becomes what must be satisfied. This form can be used as execution rules; a MetateM program is a set of such rules.\n",
    "id": "19835615",
    "title": "Gabbay's separation theorem"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=31068176",
    "text": "Multi-Agent Programming Contest\n\nThe Multi-Agent Programming Contest is an annual international programming competition with stated goal of stimulating research in the area of multi-agent system development and programming.\n\nIn 2005 Jürgen Dix (Clausthal University of Technology), Mehdi Dastani (University Utrecht) and Peter Novák (Czech Technical University in Prague) have brought the contest into being and running. The competition originally focused on Logic programming of Multi-agent systems. The goals, raised in 2005, have proven to be a solid basis for multi-agent system development and are still valid:\nIn 2007, a third goal has been added:\nAlthough it is necessary to find a solution for the contest quest to win, the organizers pursue the intention that the solution is a system of cooperating autonomous programs that achieve the objectives together. They are also interested in how the contest participants develop the solution.\n\nAgents have to look for food and bring it to a depot on a two-dimensional grid world. Each cell can contain an agent, or food. The agents can only see a small part of the map. Initially there is no food available, it appears randomly during the game, so that agents need to search the map constantly in order to win. This scenario was used in 2005.\n\nOn a grid based map, teams of agents look for gold and transport it to the depot. As opposed to the food scenario, cells can also contain trees which block the agents and can form more or less complex labyrinths. Also, there are now two opposing teams competing for the gold. This scenario was used in the contests of 2006 and 2007. In 2007, the scenario was extended to allow the agents to carry more than one piece of gold, and to push opposing agents aside.\n\nA grid based map contains trees, corrals, cows and agents. Two opposing teams try to drive as many cows as possible in ones corral. Cows behave using Swarm intelligence. They are also afraid of cowboys and try and run away. This scenario was used in 2008, 2009 and 2010. For the last two years, gates were introduced to make the scenario more challenging.\n\nThe 2011 contest introduces a scenario called agents on mars. Goal is to conquer as much space on mars as possible, using a team of cooperating agents. The challenge here is the higher complexity resulting from the introduction of five roles with different properties and abilities, which have to be used to scout, conquer, and keep the conquered land.\nThe team HactarV2 from the TU-Delft won the 2011 competition while using the GOAL programming language.\n\n\n",
    "id": "31068176",
    "title": "Multi-Agent Programming Contest"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=2730882",
    "text": "Embodied agent\n\nIn artificial intelligence, an embodied agent, also sometimes referred to as an interface agent, is an intelligent agent that interacts with the environment through a physical body within that environment. Agents that are represented graphically with a body, for example a human or a cartoon animal, are also called embodied agents, although they have only virtual, not physical, embodiment. A branch of artificial intelligence focuses on empowering such agents to interact autonomously with human beings and the environment. Mobile robots are one example of physically embodied agents; Ananova and Microsoft Agent are examples of graphically embodied agents. Embodied conversational agents are embodied agents (usually with a graphical front-end as opposed to a robotic body) that are capable of engaging in conversation with one another and with humans employing the same verbal and nonverbal means that humans do (such as gesture, facial expression, and so forth).\n\nEmbodied conversational agents are a form of intelligent user interface. Graphically embodied agents aim to unite gesture, facial expression and speech to enable face-to-face communication with users, providing a powerful means of human-computer interaction.\n\nFace-to-face communication allows communication protocols that give a much richer communication channel than other means of communicating. It enables pragmatic communication acts such as conversational turn-taking, facial expression of emotions, information structure and emphasis, visualisation and iconic gestures, and orientation in a three-dimensional environment. This communication takes place through both verbal and non-verbal channels such as gaze, gesture, spoken intonation and body posture.\n\nResearch has found that users prefer a non-verbal visual indication of an embodied system's internal state to a verbal indication, demonstrating the value of additional non-verbal communication channels. As well as this, the face-to-face communication involved in interacting with an embodied agent can be conducted alongside another task without distracting the human participants, instead improving the enjoyment of such an interaction. Furthermore, the use of an embodied presentation agent results in improved recall of the presented information.\n\nEmbodied agents also provide a social dimension to the interaction. Humans willingly ascribe social awareness to computers, and thus interaction with embodied agents follows social conventions, similar to human/human interactions. This social interaction both raises the believability and perceived trustworthiness of agents, and increases the user's engagement with the system. Rickenberg and Reeves found that the presence of an embodied agent on a website increased the level of user trust in that website; as well as this, the presence of the agent increased users' anxiety and affected their performance just as if they were being watched by a real human. Another effect of the social aspect of agents is that presentations given by an embodied agent are perceived as more entertaining and less difficult than the same presentations given without an agent. Research shows that perceived enjoyment, followed by perceived usefulness and ease of use, is the major factor influencing user adoption of embodied agents.\n\nOne example result from a recent study indicates the power of a character when moderating search inquiries. When a character asked people to type search requests into a window, people used, on average, three more words in their requests (averaging about 7 words per inquiry) compared to identical requests made without a character. Character suggest that a conversational style is appropriate, resulting in higher liking for the\ninteraction on the part of the user, and better accuracy for the engine generating the required results.\n\nThis rich style of communication that characterises human conversation makes conversational interaction with embodied conversational agents ideal for many non-traditional interaction tasks. A familiar application of graphically embodied agents is computer games; embodied agents are ideal for this setting because the richer communication style makes interacting with the agent enjoyable. Embodied conversational agents have also been used in virtual training environments, portable personal navigation guides, interactive fiction and storytelling systems, interactive online characters and automated presenters and commentators.\n\nMajor virtual assistants like Siri and Google Assistant do not come with any visual embodied representation, which is believed to limit\nthe sense of human presence by users.\n\nThe U.S. Department of Defense utilizes a software agent they call \"\"Sgt. Star\"\" on U.S. Army-run Web sites and Web applications for site navigation, recruitment and propaganda purposes. Sgt. Star is run by the Army Marketing and Research Group, a division operated directly from The Pentagon. Sgt. Star is based upon the \"ActiveSentry\" technology developed by Next IT, a Washington-based information technology services company. Other such bots in the Sgt. Star \"family\" are utilized by the Federal Bureau of Investigation and the Central Intelligence Agency for intelligence gathering purposes.\n\n\n\n\n",
    "id": "2730882",
    "title": "Embodied agent"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=866256",
    "text": "User illusion\n\nThe user illusion is the illusion created for the user by a human–computer interface, for example the visual metaphor of a desktop used in many graphical user interfaces. The phrase originated at Xerox PARC.\n\nSome philosophers of mind have argued that consciousness is a form of user illusion. This notion is explored by Tor Nørretranders in his 1991 Danish book \"Mærk verden\", issued in a 1998 English edition as \"The User Illusion: Cutting Consciousness Down to Size\". He introduced the notion of exformation in this book.\n\nAccording to this picture, our experience of the world is not immediate, as all sensation requires processing time. It follows that our conscious experience is less a perfect reflection of what is occurring, and more a simulation produced unconsciously by the brain. Therefore, there may be phenomena that exist beyond our peripheries, beyond what consciousness could create to isolate or reduce them.\n\nCritics of the idea of consciousness being a device for justifying preconceptions argue that such a device would consume nutrients without producing any useful results, since it would not change the outcome of any decisions. These critics argue that the existence of social insects with extremely small brains falsifies the notion that social behavior require consciousness, citing that insects have too small brains to be conscious and yet there are observed behaviors among them that for all functional intents and purposes match those of complex social cooperation and manipulation (including hierarchies where each individual has its place among paper wasps and Jack Jumper ants and honey bees sneaking when they lay eggs). These critics also argue that since social behavior in insects and other extremely small-brained animals have evolved multiple times independently, there is no evolutionary difficulty in simple reaction sociality to impose selection pressure for the more nutrient-consuming path of consciousness for sociality. These critics do point out that other evolutionary paths to consciousness are possible, such as critical evaluation that enhances plasticity by criticizing fallible notions, while pointing out that such a critical consciousness would be quite different from the justificatory type proposed by Nørretranders, differences including that a critical consciousness would make individuals more capable of changing their minds instead of justifying and persuading.\n\n",
    "id": "866256",
    "title": "User illusion"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=23402053",
    "text": "Wetware (brain)\n\nWetware is a term drawn from the computer-related idea of hardware or software, but applied to biological life forms.\n\nThe prefix \"wet\" is a reference to the water found in living creatures. Wetware is used to describe the elements equivalent to hardware and software found in a person, especially the central nervous system (CNS) and the human mind. The term wetware finds use both in works of fiction and in scholarly publications.\n\nThe \"hardware\" component of wetware concerns the bioelectric and biochemical properties of the CNS, specifically the brain. If the sequence of impulses traveling across the various neurons are thought of symbolically as \"software\", then the physical neurons would be the \"hardware\". The amalgamated interaction of this \"software\" and \"hardware\" is manifested through continuously changing physical connections, and chemical and electrical influences that spread across the body. The process by which the \"mind\" and \"brain\" interact to produce the collection of experiences that we define as self-awareness is in question.\n\nAlthough the exact definition has shifted over time, the term \"Wetware\" and its fundamental reference to \"the physical mind\" has been around at least since the mid-1950s. Mostly used in relatively obscure articles and papers, it was not until the heyday of cyberpunk, however, that the term found broad adoption. Among the first uses of the term in popular culture was the Michael Swanwick novel \"Vacuum Flowers\" (1987).\n\nRudy Rucker references the term in a number of books, including one entitled \"Wetware\": ... all sparks and tastes and tangles, all its stimulus/response patterns – the whole bio-cybernetic software of mind. Rucker did not use the word to simply mean a brain, nor in the human-resources sense of employees. He used \"wetware\" to stand for the data found in any biological system, analogous perhaps to the firmware that is found in a ROM chip. In Rucker's sense, a seed, a plant graft, an embryo, or a biological virus are all wetware. DNA, the immune system, and the evolved neural architecture of the brain are further examples of wetware in this sense.\n\nRucker describes his conception in a 1992 compendium \"The Mondo 2000 User's Guide to the New Edge\", which he quotes in a 2007 blog entry.\n\nEarly cyber-guru Arthur Kroker used the term in his blog.\n\nWith the term getting traction in trendsetting publications, it became a buzzword in the early 1990s. In 1991, Dutch media theorist Geert Lovink organized the Wetware Convention in Amsterdam, which was supposed to be an antidote to the \"out-of-body\" experiments conducted in high-tech laboratories, such as experiments in virtual reality.\n\nTimothy Leary, in an appendix to \"Info-Psychology\" originally written in 1975–76 and published in 1989, used the term \"wetware\", writing that \"psychedelic neuro-transmitters were the hot new technology for booting-up the 'wetware' of the brain\". Another common reference is: \"Wetware has 7 plus or minus 2 temporary registers.\" The numerical allusion is to a classic 1957 article by George A. Miller, \"The magical number 7 plus or minus two: some limits in our capacity for processing information\", which later gave way to the Miller's law.\n\n",
    "id": "23402053",
    "title": "Wetware (brain)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1377612",
    "text": "Knowledge-based systems\n\nA knowledge-based system (KBS) is a computer program that reasons and uses a knowledge base to solve complex problems. The term is broad and is used to refer to many different kinds of systems; the one common theme that unites all knowledge based systems is an attempt to represent knowledge explicitly via tools such as ontologies and rules rather than implicitly via code the way a conventional computer program does. A knowledge based system has three types of sub-systems: a knowledge base, an user interface and an inference engine. The knowledge base represents facts about the world, often in some form of subsumption ontology. The inference engine represents logical assertions and conditions about the world, usually represented via IF-THEN rules.\n\nKnowledge-based systems were first developed by artificial intelligence researchers. These early knowledge-based systems were primarily expert systems – in fact, the term is often used synonymously with expert systems. The difference is in the view taken to describe the system: \"expert system\" refers to the type of task the system is trying to assist with – to replace or aid a human expert in a complex task; \"knowledge-based system\" refers to the architecture of the system – that it represents knowledge explicitly (rather than as procedural code). While the earliest knowledge-based systems were almost all expert systems, the same tools and architectures can and have since been used for a whole host of other types of systems – i.e., virtually all expert systems are knowledge-based systems, but many knowledge-based systems are not expert systems.\n\nThe first knowledge-based systems were rule based expert systems. One of the most famous was Mycin a program for medical diagnosis. These early expert systems represented facts about the world as simple assertions in a flat database and used rules to reason about and as a result add to these assertions. Representing knowledge explicitly via rules had several advantages:\n\n\nAs knowledge-based systems became more complex the techniques used to represent the knowledge base became more sophisticated. Rather than representing facts as assertions about data, the knowledge-base became more structured, representing information using similar techniques to object-oriented programming such as hierarchies of classes and subclasses, relations between classes, and behavior of objects. As the knowledge base became more structured reasoning could occur both by independent rules and by interactions within the knowledge base itself. For example, procedures stored as demons on objects could fire and could replicate the chaining behavior of rules.\n\nAnother advancement was the development of special purpose automated reasoning systems called classifiers. Rather than statically declare the subsumption relations in a knowledge-base a classifier allows the developer to simply declare facts about the world and let the classifier deduce the relations. In this way a classifier also can play the role of an inference engine.\n\nThe most recent advancement of knowledge-based systems has been to adopt the technologies for the development of systems that use the internet. The internet often has to deal with complex, unstructured data that can't be relied on to fit a specific data model. The technology of knowledge-based systems and especially the ability to classify objects on demand is ideal for such systems. The model for these kinds of knowledge-based Internet systems is known as the Semantic Web.\n\nWhen developing knowledge-based systems it is common to integrate knowledge with data. Constantinou et al. proposed a method for eliciting and incorporating expert knowledge in data-driven Bayesian Networks (BNs). The method addresses the problem whereby the distribution of some variable in a BN is known from data, but where we wish to explicitly model the impact of some addition.\n\n",
    "id": "1377612",
    "title": "Knowledge-based systems"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=430106",
    "text": "Software agent\n\nIn computer science, a software agent is a computer program that acts for a user or other program in a relationship of agency, which derives from the Latin \"agere\" (to do): an agreement to act on one's behalf. Such \"action on behalf of\" implies the authority to decide which, if any, action is appropriate. Agents are colloquially known as \"bots\", from \"robot\". They may be embodied, as when execution is paired with a robot body, or as software such as a chatbot\nexecuting on a phone (e.g. Siri) or other computing device. Software Agents may be autonomous or work together with other agents or people. Software agents interacting with people (e.g. chatbots, human-robot interaction environments) may possess human-like qualities such as natural language understanding and speech, personality or embody humanoid form (see Asimo).\n\nRelated and derived concepts include \"intelligent agents\" (in particular exhibiting some aspect of artificial intelligence, such as learning and reasoning), \"autonomous agents\" (capable of modifying the way in which they achieve their objectives), \"distributed\" agents (being executed on physically distinct computers), \"multi-agent systems\" (distributed agents that work together to achieve an objective that could not be accomplished by a single agent acting alone), and \"mobile agents\" (agents that can relocate their execution onto different processors).\n\nThe basic attributes of an autonomous software agent are that agents\nThe term \"agent\" describes a software abstraction, an idea, or a concept, similar to OOP terms such as methods, functions, and objects. The concept of an agent provides a convenient and powerful way to describe a complex software entity that is capable of acting with a certain degree of autonomy in order to accomplish tasks on behalf of its host. But unlike objects, which are defined in terms of \"methods\" and \"attributes\", an agent is defined in terms of its behavior.\n\nVarious authors have proposed different definitions of agents, these commonly include concepts such as\n\nAll agents are programs, but not all programs are agents.\nContrasting the term with related concepts may help clarify its meaning. Franklin & Graesser (1997) discuss four key notions that distinguish agents from arbitrary programs: reaction to the environment, autonomy, goal-orientation and persistence.\n\n\n\n\nSoftware agents may offer various benefits to their end users by automating complex or repetitive tasks. However, there are organizational and cultural impacts of this technology that need to be considered prior to implementing software agents.\n\nPeople like to perform easy tasks providing the sensation of success unless the repetition of the simple tasking is affecting the overall output. In general implementing software agents to perform administrative requirements provides a substantial increase in work contentment, as administering their own work does never please the worker. The effort freed up serves for a higher degree of engagement in the substantial tasks of individual work. Hence, software agents may provide the basics to implement self-controlled work, relieved from hierarchical controls and interference. Such conditions may be secured by application of software agents for required formal support.\n\nThe cultural effects of the implementation of software agents include trust affliction, skills erosion, privacy attrition and social detachment. Some users may not feel entirely comfortable fully delegating important tasks to software applications. Those who start relying solely on intelligent agents may lose important skills, for example, relating to information literacy. In order to act on a user’s behalf, a software agent needs to have a complete understanding of a user’s profile, including his/her personal preferences. This, in turn, may lead to unpredictable privacy issues. When users start relying on their software agents more, especially for communication activities, they may lose contact with other human users and look at the word with the eyes of their agents. These consequences are what agent researchers and users must consider when dealing with intelligent agent technologies.\n\nThe concept of an agent can be traced back to Hewitt's Actor Model (Hewitt, 1977) - \"A self-contained, interactive and concurrently-executing object, possessing internal state and communication capability.\"\n\nTo be more academic, software agent systems are a direct evolution of Multi-Agent Systems (MAS). MAS evolved from Distributed Artificial Intelligence (DAI), Distributed Problem Solving (DPS) and Parallel AI (PAI), thus inheriting all characteristics (good and bad) from DAI and AI.\n\nJohn Sculley’s 1987 “Knowledge Navigator” video portrayed an image of a relationship between end-users and agents. Being an ideal first, this field experienced a series of unsuccessful top-down implementations, instead of a piece-by-piece, bottom-up approach. The range of agent types is now (from 1990) broad: WWW, search engines, etc.\n\nHaag suggests that there are only four essential types of intelligent software agents:\n\nBuyer agents travel around a network (e.g. the internet) retrieving information about goods and services. These agents, also known as 'shopping bots', work very efficiently for commodity products such as CDs, books, electronic components, and other one-size-fits-all products. Buyer agents are typically optimized to allow for digital payment services used in e-commerce and traditional businesses.\n\nUser agents, or personal agents, are intelligent agents that take action on your behalf. In this category belong those intelligent agents that already perform, or will shortly perform, the following tasks:\n\nMonitoring and Surveillance Agents are used to observe and report on equipment, usually computer systems. The agents may keep track of company inventory levels, observe competitors' prices and relay them back to the company, watch stock manipulation by insider trading and rumors, etc.\nFor example, NASA's Jet Propulsion Laboratory has an agent that monitors inventory, planning, schedules equipment orders to keep costs down, and manages food storage facilities. These agents usually monitor complex computer networks that can keep track of the configuration of each computer connected to the network.\n\nA special case of Monitoring-and-Surveillance agents are organizations of agents used to emulate the Human Decision-Making process during tactical operations. The agents monitor the status of assets (ammunition, weapons available, platforms for transport, etc.) and receive Goals (Missions) from higher level agents. The Agents then pursue the Goals with the Assets at hand, minimizing expenditure of the Assets while maximizing Goal Attainment. (See Popplewell, \"Agents and Applicability\")\n\nThis agent uses information technology to find trends and patterns in an abundance of information from many different sources. The user can sort through this information in order to find whatever information they are seeking.\n\nA data mining agent operates in a data warehouse discovering information. A 'data warehouse' brings together information from lots of different sources. \"Data mining\" is the process of looking through the data warehouse to find information that you can use to take action, such as ways to increase sales or keep customers who are considering defecting.\n\n'Classification' is one of the most common types of data mining, which finds patterns in information and categorizes them into different classes. Data mining agents can also detect major shifts in trends or a key indicator and can detect the presence of new information and alert you to it. For example, the agent may detect a decline in the construction industry for an economy; based on this relayed information construction companies will be able to make intelligent decisions regarding the hiring/firing of employees or the purchase/lease of equipment in order to best suit their firm.\n\nSome other examples of current intelligent agents include some spam filters, game bots, and server monitoring tools. Search engine indexing bots also qualify as intelligent agents.\n\nIssues to consider in the development of agent-based systems include \n\nFor software agents to work together efficiently they must share semantics of their data elements. This can be done by having computer systems publish their metadata.\n\nThe definition of \"agent processing\" can be approached from two interrelated directions:\n\nAgent systems are used to model real-world systems with concurrency or parallel processing.\n\n\nThe agent uses its access methods to go out into local and remote databases to forage for content. These access methods may include setting up news stream delivery to the agent, or retrieval from bulletin boards, or using a spider to walk the Web. The content that is retrieved in this way is probably already partially filtered – by the selection of the newsfeed or the databases that are searched. The agent next may use its detailed searching or language-processing machinery to extract keywords or signatures from the body of the content that has been received or retrieved. This abstracted content (or event) is then passed to the agent’s Reasoning or inferencing machinery in order to decide what to do with the new content. This process combines the event content with the rule-based or knowledge content provided by the user. If this process finds a good hit or match in the new content, the agent may use another piece of its machinery to do a more detailed search on the content. Finally, the agent may decide to take an action based on the new content; for example, to notify the user that an important event has occurred. This action is verified by a security function and then given the authority of the user. The agent makes use of a user-access method to deliver that message to the user. If the user confirms that the event is important by acting quickly on the notification, the agent may also employ its learning machinery to increase its weighting for this kind of event.\n\nBots can act on behalf of their creators to do good as well as bad. There are a few ways which bots can be created to demonstrate that they are designed with the best intention and are not built to do harm. This is first done by having a bot identify itself in the user-agent HTTP header when communicating with a site. The source IP address must also be validated to establish itself as legitimate. Next, the bot must also always respect a site's robots.txt file since it has become the standard across most of the web. And like respecting the robots.txt file, bots should shy away from being too aggressive and respect any crawl delay instructions.\n\n\n\n",
    "id": "430106",
    "title": "Software agent"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=23048428",
    "text": "Kinect\n\nKinect (codenamed Project Natal during development) is a line of motion sensing input devices that was produced by Microsoft for Xbox 360 and Xbox One video game consoles and Microsoft Windows PCs. Based around a webcam-style add-on peripheral, it enables users to control and interact with their console/computer without the need for a game controller, through a natural user interface using gestures and spoken commands.\n\nThe first-generation Kinect was first introduced in November 2010 in an attempt to broaden Xbox 360's audience beyond its typical gamer base. A version for Microsoft Windows was released on February 1, 2012. A newer version, Kinect 2.0, was released with the Xbox One platform starting in 2013.\n\nMicrosoft released the first Beta of the Kinect software development kit for Windows 7 on June 16, 2011. This SDK was meant to allow developers to write Kinecting apps in C++/CLI, C#, or Visual Basic .NET.\n\nUltimately, the Kinect for either Xbox console did not have long-term popularity, and the two versions were discontinued by April 2016 and October 2017, respectively. The Windows version had been discontinued by April 2015.\n\nKinect was first announced on June 1, 2009 at E3 2009 under the code name \"Project Natal\". Three demos were shown to showcase Kinect when it was revealed at Microsoft's E3 2009 Media Briefing: \"Ricochet\", \"Paint Party\" and \"Milo & Kate\". A demo based on \"Burnout Paradise\" was also shown outside of Microsoft's media briefing. The skeletal mapping technology shown at E3 2009 was capable of simultaneously tracking four people, with a feature extraction of 48 skeletal points on a human body at 30 Hz.\n\nIt was rumored that the launch of Project Natal would be accompanied with the release of a new Xbox 360 console (as either a new retail configuration, a significant design revision and/or a modest hardware upgrade). Microsoft dismissed the reports in public and repeatedly emphasized that Project Natal would be fully compatible with all Xbox 360 consoles. Microsoft indicated that the company considers it to be a significant initiative, as fundamental to Xbox brand as Xbox Live, and with a launch akin to that of a new Xbox console platform. Kinect was even referred to as a \"new Xbox\" by Microsoft CEO Steve Ballmer at a speech for Executives' Club of Chicago. When asked if the introduction will extend the time before the next-generation console platform is launched (historically about 5 years between platforms), Microsoft corporate vice president Shane Kim reaffirmed that the company believes that the life cycle of Xbox 360 will last through 2015 (10 years).\n\nDuring Kinect's development, project team members experimentally adapted numerous games to Kinect-based control schemes to help evaluate usability. Among these games were \"Beautiful Katamari\" and \"Space Invaders Extreme\", which were demonstrated at Tokyo Game Show in September 2009. According to creative director Kudo Tsunoda, adding Kinect-based control to pre-existing games would involve significant code alterations, making it unlikely for Kinect features to be added through software updates.\n\nAlthough the sensor unit was originally planned to contain a microprocessor that would perform operations such as the system's skeletal mapping, it was revealed in January 2010 that the sensor would no longer feature a dedicated processor. Instead, processing would be handled by one of the processor cores of Xbox 360's Xenon CPU. According to Alex Kipman, Kinect system consumes about 10-15% of Xbox 360's computing resources. However, in November, Alex Kipman made a statement that \"the new motion control tech now only uses a single-digit percentage of Xbox 360's processing power, down from the previously stated 10 to 15 percent.\" A number of observers commented that the computational load required for Kinect makes the addition of Kinect functionality to pre-existing games through software updates even less likely, with concepts specific to Kinect more likely to be the focus for developers using the platform.\n\nOn March 25, 2010, Microsoft sent out a save the date flier for an event called the \"World Premiere 'Project Natal' for Xbox 360 Experience\" at E3 2010. The event took place on the evening of Sunday, June 13, 2010 at Galen Center and featured a performance by Cirque du Soleil. It was announced that the system would officially be called Kinect, a portmanteau of the words \"kinetic\" and \"connect\", which describe key aspects of the initiative. Microsoft also announced that the North American launch date for Kinect will be November 4, 2010. Despite previous statements dismissing speculation of a new Xbox 360 to accompany the launch of the new control system, Microsoft announced at E3 2010 that it was introducing a redesigned Xbox 360, complete with a connector port ready for Kinect. In addition, on July 20, 2010, Microsoft announced a Kinect bundle with a redesigned Xbox 360, to be available with Kinect launch.\n\nOn June 16, 2011, Microsoft announced its official release of its SDK for non-commercial use.\nOn July 21, 2011, Microsoft announced that the first ever white Kinect sensor would be available as part of \"Xbox 360 Limited Edition Kinect Star Wars Bundle\", which also includes custom a \"Star Wars\"-themed console and controller, and copies of \"Kinect Adventures\" and \"Kinect Star Wars\". Previously, all Kinect sensors had been glossy black.\n\nOn October 31, 2011, Microsoft announced launching of the commercial version of \"Kinect for Windows program\" with release of SDK to companies. David Dennis, Product Manager at Microsoft, said, \"There are hundreds of organizations we are working with to help them determine what's possible with the tech\".\n\nOn February 1, 2012, Microsoft released the commercial version of Kinect for Windows SDK and told that more than 300 companies from over 25 countries are working on Kinect-ready apps.\n\nThe tabloid the New York Post claimed Microsoft had a $500 million budget for advertising the launch of Kinect. While this claim was widely re-reported, an examination of the 10-Q from Microsoft reveals a 20% year-over-year increase ($85 million) for sales and marketing the quarter Kinect was launched for all of the Entertainment and Devices division, making the total sales and marketing spend $425 million for the entire division. The marketing campaign \"You Are the Controller\", aiming to reach new audiences, included advertisements on Kellogg's cereal boxes and Pepsi bottles, commercials during shows such as \"Dancing with the Stars\" and \"Glee\" as well as print ads in various magazines such as \"People\" and \"InStyle\".\n\nOn October 19, Microsoft advertised Kinect on The Oprah Winfrey Show by giving free Xbox 360 consoles and Kinect sensors to the people in the audience. Two weeks later, Kinect bundles with Xbox 360 consoles were also given away to the audience of Late Night with Jimmy Fallon. On October 23, Microsoft held a pre-launch party for Kinect in Beverly Hills. The party was hosted by Ashley Tisdale and was attended by soccer star David Beckham and his three sons, Cruz, Brooklyn, and Romeo. Guests were treated to sessions with Dance Central and Kinect Adventures, followed by Tisdale having a Kinect voice chat with Nick Cannon. Between November 1 and 28, Burger King gave away a free Kinect bundle \"every 15 minutes\".\n\nA major event was organized on November 3 in Times Square, where singer Ne-Yo performed with hundreds of dancers in anticipation of Kinect's midnight launch.\n\nKinect was launched in North America on November 4, 2010, in Europe on November 10, 2010, in Australia, New Zealand and Singapore on November 18, 2010, and in Japan on November 20, 2010. Purchase options for the sensor peripheral include a bundle with the game \"Kinect Adventures\" and console bundles with either a 4 GB or 250 GB Xbox 360 console and \"Kinect Adventures\".\n\nKinect V1 was a combination of Microsoft built software and hardware. Kinect V1 hardware included a range chipset technology by Israeli developer PrimeSense, which developed a system consisting of an infrared projector and camera and a special microchip that generates a grid from which the location of a nearby object in 3 dimensions can be ascertained. This 3D scanner system called \"Light Coding\" employs a variant of image-based 3D reconstruction.\n\nThe Kinect sensor is a horizontal bar connected to a small base with a motorized pivot and is designed to be positioned lengthwise above or below the video display. The device features an \"RGB camera, depth sensor and multi-array microphone running proprietary software\", which provide full-body 3D motion capture, facial recognition and voice recognition capabilities. At launch, voice recognition was only made available in Japan, United Kingdom, Canada and United States. Mainland Europe received the feature later in spring 2011. Currently voice recognition is supported in Australia, Canada, France, Germany, Ireland, Italy, Japan, Mexico, New Zealand, United Kingdom and United States. Kinect sensor's microphone array enables Xbox 360 to conduct acoustic source localization and ambient noise suppression, allowing for things such as headset-free party chat over Xbox Live.\n\nThe depth sensor consists of an infrared laser projector combined with a monochrome CMOS sensor, which captures video data in 3D under any ambient light conditions. The sensing range of the depth sensor is adjustable, and Kinect software is capable of automatically calibrating the sensor based on gameplay and the player's physical environment, accommodating for the presence of furniture or other obstacles.\n\nDescribed by Microsoft personnel as the primary innovation of Kinect, the software technology enables advanced gesture recognition, facial recognition and voice recognition. According to information supplied to retailers, Kinect is capable of simultaneously tracking up to six people, including two active players for motion analysis with a feature extraction of 20 joints per player. However, PrimeSense has stated that the number of people the device can \"see\" (but not process as players) is only limited by how many will fit in the field-of-view of the camera.\n\nReverse engineering has determined that the Kinect's various sensors output video at a frame rate of ≈9 Hz to 30 Hz depending on resolution. The default RGB video stream uses 8-bit VGA resolution (640 × 480 pixels) with a Bayer color filter, but the hardware is capable of resolutions up to 1280x1024 (at a lower frame rate) and other colour formats such as UYVY. The monochrome depth sensing video stream is in VGA resolution (640 × 480 pixels) with 11-bit depth, which provides 2,048 levels of sensitivity. The Kinect can also stream the view from its IR camera directly (i.e.: before it has been converted into a depth map) as 640x480 video, or 1280x1024 at a lower frame rate. The Kinect sensor has a practical ranging limit of distance when used with the Xbox software. The area required to play Kinect is roughly 6 m, although the sensor can maintain tracking through an extended range of approximately . The sensor has an angular field of view of 57° horizontally and 43° vertically, while the motorized pivot is capable of tilting the sensor up to 27° either up or down. The horizontal field of the Kinect sensor at the minimum viewing distance of ≈ is therefore ≈, and the vertical field is ≈, resulting in a resolution of just over per pixel. The microphone array features four microphone capsules and operates with each channel processing 16-bit audio at a sampling rate of 16 kHz.\n\nBecause the Kinect sensor's motorized tilt mechanism requires more power than the Xbox 360's USB ports can supply, the device makes use of a proprietary connector combining USB communication with additional power. Redesigned Xbox 360 S models include a special AUX port for accommodating the connector, while older models require a special power supply cable (included with the sensor) that splits the connection into separate USB and power connections; power is supplied from the mains by way of an AC adapter.\n\nKinect 2.0 was released with Xbox One on November 22, 2013. The hardware included a time-of-flight sensor developed by Microsoft, replacing the older technology from PrimeSense.\n\nXbox One consoles ship with an updated version of Kinect; the new Kinect uses a wide-angle time-of-flight camera, and processes 2 gigabits of data per second to read its environment. The new Kinect has greater accuracy with three times the fidelity over its predecessor and can track without visible light by using an active IR sensor. It has a 60% wider field of vision that can detect a user up to 3 feet from the sensor, compared to six feet for the original Kinect, and can track up to 6 skeletons at once. It can also detect a player's heart rate, facial expression, the position and orientation of 25 individual joints (including thumbs), the weight put on each limb, speed of player movements, and track gestures performed with a standard controller. The color camera captures 1080p video that can be displayed in the same resolution as the viewing screen, allowing for a broad range of scenarios. In addition to improving video communications and video analytics applications, this provides a stable input on which to build interactive applications. Kinect's microphone is used to provide voice commands for actions such as navigation, starting games, and waking the console from sleep mode.\n\nAll Xbox One consoles were initially shipped with the Kinect sensor included—a holdover from a previously-announced, but retracted mandate requiring Kinect to be plugged into the console at all times for it to function. In June 2014, bundles without Kinect were made available, along with an updated Xbox One SDK allowing game developers to explicitly disable Kinect skeletal tracking, freeing up system resources that were previously reserved for Kinect even if it was disabled or unplugged.\n\nA standalone Kinect for Xbox One, bundled with a digital copy of \"Dance Central Spotlight\", was released on October 7, 2014.\n\nOn February 21, 2011 Microsoft announced that it would release a non-commercial Kinect software development kit (SDK) for Microsoft Windows in spring 2011, and the first beta was released for Windows 7 on June 16, 2011. Beta 2 was released on the 1 year anniversary of Kinect for Xbox 360, on November 3, 2011. Version 1.0 was released at the same time as the officially supported Microsoft Windows hardware on February 1, 2012.\n\nThe SDK includes Windows 7 compatible PC drivers for Kinect device. It provides Kinect capabilities to developers to build applications with C++, C#, or Visual Basic by using Microsoft Visual Studio 2010 and includes following features:\n\n\nIn March 2012, Craig Eisler, the general manager of Kinect for Windows, said that almost 350 companies are working with Microsoft on custom Kinect applications for Microsoft Windows.\n\nIn March 2012, Microsoft announced that next version of Kinect for Windows SDK would be available in May 2012. Kinect for Windows 1.5 was released on May 21, 2012. It adds new features, support for many new languages and debut in 19 more countries.\n\n\nKinect for Windows SDK for the first-generation sensor was updated a few more times, with version 1.6 released October 8, 2012, version 1.7 released March 18, 2013, and version 1.8 released September 17, 2013.\n\nThe second-generation Kinect for Windows, based on the same core technology as Kinect for Xbox One, including a new sensor, was first released in 2014.\n\nRequiring at least 190 MB of available storage space, Kinect system software allows users to operate Xbox 360 Dashboard console user interface through voice commands and hand gestures. Techniques such as voice recognition and facial recognition are employed to automatically identify users. Among the applications for Kinect is Video Kinect, which enables voice chat or video chat with other Xbox 360 users or users of Windows Live Messenger. The application can use Kinect's tracking functionality and Kinect sensor's motorized pivot to keep users in frame even as they move around. Other applications with Kinect support include ESPN, Zune Marketplace, Netflix, Hulu Plus and Last.fm. Microsoft later confirmed that all forthcoming applications would be required to have Kinect functionality for certification.\n\nXbox 360 games that require Kinect are packaged in special purple cases (as opposed to the green cases used by all other Xbox 360 games), and contain a prominent \"Requires Kinect Sensor\" logo on their front cover. Games that include features utilizing Kinect, but do not require it for standard gameplay, contain a \"Better with Kinect Sensor\" branding on their front covers.\n\nKinect launched on November 4, 2010 with 17 titles. Third-party publishers of available and announced Kinect games include, among others, Ubisoft, Electronic Arts, LucasArts, THQ, Activision, Konami, Sega, Capcom, Namco Bandai and MTV Games. Along with retail games, there are also select Xbox Live Arcade titles which require the peripheral.\n\nAt E3 2011, Microsoft announced \"Kinect Fun Labs\": a collection of various gadgets and minigames that are accessible from Xbox 360 Dashboard. These gadgets includes \"Build A Buddy\", \"Air Band\", \"Kinect Googly Eyes\", \"Kinect Me\", \"Bobblehead\", \"Kinect Sparkler\", \"Junk Fu\" and \"Avatar Kinect\".\n\nIn November 2010, Adafruit Industries offered a bounty for an open-source driver for Kinect. Microsoft initially voiced its disapproval of the bounty, stating that it \"does not condone the modification of its products\" and that it had \"built in numerous hardware and software safeguards designed to reduce the chances of product tampering\". This reaction, however, was caused by a misunderstanding within Microsoft, and the company later clarified its position, claiming that while it does not condone hacking of either the physical device or the console, the USB connection was left open by design.\n\nOn November 10, Adafruit announced Héctor Martín as the winner, who had produced a Linux driver that allows the use of both the RGB camera and depth sensitivity functions of the device. It was later revealed that Johnny Lee, a core member of Microsoft's Kinect development team, had secretly approached Adafruit with the idea of a driver development contest and had personally financed it.\n\nIn December 2010, PrimeSense, whose depth sensing chips were used in the Kinect v1 hardware, released their own open source drivers along with motion tracking middleware called \"NITE\". PrimeSense later announced that it had teamed up with Asus to develop a PC-compatible device similar to Kinect for Chinese markets, called the Wavi Xtion. The product was released in October 2011.\n\nOpenNI is an open-source software framework that is able to read sensor data from Kinect, among other natural user interface sensors.\n\nNumerous developers are researching possible applications of Kinect that go beyond the system's intended purpose of playing games. For example, Philipp Robbel of MIT combined Kinect with iRobot Create to map a room in 3D and have the robot respond to human gestures, while an MIT Media Lab team is working on a JavaScript extension for Google Chrome called depthJS that allows users to control the browser with hand gestures. Other programmers, including Robot Locomotion Group at MIT, are using the drivers to develop a motion-controller user interface similar to the one envisioned in \"Minority Report\". The developers of MRPT have integrated open source drivers into their libraries and provided examples of live 3D rendering and basic 3D visual SLAM. Another team has shown an application that allows Kinect users to play a virtual piano by tapping their fingers on an empty desk. Oliver Kreylos, a researcher at University of California, Davis, adopted the technology to improve live 3-dimensional videoconferencing, which NASA has shown interest in.\n\nAlexandre Alahi from EPFL presented a video surveillance system that combines multiple Kinect devices to track groups of people even in complete darkness. Companies So touch and Evoluce have developed presentation software for Kinect that can be controlled by hand gestures; among its features is a multi-touch zoom mode. In December 2010, the free public beta of HTPC software \"KinEmote\" was launched; it allows navigation of Boxee and XBMC menus using a Kinect sensor. Soroush Falahati wrote an application that can be used to create stereoscopic 3D images with a Kinect sensor.\n\nFor a limited time in May 2011, a Topshop store in Moscow set up a Kinect kiosk that could overlay a collection of dresses onto the live video feed of customers. Through automatic tracking, position and rotation of the virtual dress were updated even as customers turned around to see the back of the outfit.\n\nKinect also shows compelling potential for use in medicine. Researchers at the University of Minnesota have used Kinect to measure a range of disorder symptoms in children, creating new ways of objective evaluation to detect such conditions as autism, attention-deficit disorder and obsessive-compulsive disorder. Several groups have reported using Kinect for intraoperative, review of medical imaging, allowing the surgeon to access the information without contamination. This technique is already in use at Sunnybrook Health Sciences Centre in Toronto, where doctors use it to guide imaging during cancer surgery. At least one company, GestSure Technologies, is pursuing the commercialization of such a system.\n\nNASA's Jet Propulsion Laboratory (JPL) signed up for the Kinect for Windows Developer program in November 2013 to use the new Kinect to manipulate a robotic arm in combination with an Oculus Rift virtual reality headset, creating \"the most immersive interface\" the unit had built to date.\n\nAnother application of Kinect is for multi-touch displays. A Seattle-based company that graduated from Microsoft's Kinect Accelerator, Ubi Interactive, developed software for the Kinect to work with projectors to allow touchscreen-like capabilities on various surfaces.\n\nKinect is used with a number of ROS robots via OpenNI.\n\nUpon its release, the Kinect garnered positive opinions from reviewers and critics. IGN gave the device 7.5 out of 10, saying that \"Kinect can be a tremendous amount of fun for casual players, and the creative, controller-free concept is undeniably appealing\", though adding that for \"$149.99, a motion-tracking camera add-on for Xbox 360 is a tough sell, especially considering that the entry level variation of Xbox 360 itself is only $199.99\". \"Game Informer\" rated Kinect 8 out of 10, praising the technology but noting that the experience takes a while to get used to and that the spatial requirement may pose a barrier. \"Computer and Video Games\" called the device a technological gem and applauded the gesture and voice controls, while criticizing the launch lineup and Kinect Hub.\n\nCNET's review pointed out how Kinect keeps players active with its full-body motion sensing but criticized the learning curve, the additional power supply needed for older Xbox 360 consoles and the space requirements. Engadget, too, listed the large space requirements as a negative, along with Kinect's launch lineup and the slowness of the hand gesture UI. The review praised the system's powerful technology and the potential of its yoga and dance games. Kotaku considered the device revolutionary upon first use but noted that games were sometimes unable to recognize gestures or had slow responses, concluding that Kinect is \"not must-own yet, more like must-eventually own.\" TechRadar praised the voice control and saw a great deal of potential in the device whose lag and space requirements were identified as issues. Gizmodo also noted Kinect's potential and expressed curiosity in how more mainstream titles would utilize the technology. \"Ars Technica's\" review expressed concern that the core feature of Kinect, its lack of a controller, would hamper development of games beyond those that have either stationary players or control the player's movement automatically.\n\nThe mainstream press also reviewed Kinect. \"USA Today\" compared it to the futuristic control scheme seen in \"Minority Report\", stating that \"playing games feels great\" and giving the device 3.5 out of 4 stars. David Pogue from \"The New York Times\" predicted players will feel a \"crazy, magical, omigosh rush the first time you try the Kinect.\" Despite calling the motion tracking less precise than Wii's implementation, Pogue concluded that \"Kinect’s astonishing technology creates a completely new activity that’s social, age-spanning and even athletic.\" \"The Globe and Mail\" titled Kinect as setting a \"new standard for motion control.\" The slight input lag between making a physical movement and Kinect registering it was not considered a major issue with most games, and the review called Kinect \"a good and innovative product,\" rating it 3.5 out of 4 stars.\n\nAlthough featuring improved performance over the original Kinect, its successor has been subject to mixed responses. In its Xbox One review, \"Engadget\" praised Xbox One's Kinect functionality, such as face recognition login and improved motion tracking, but that whilst \"magical\", \"every false positive or unrecognized [voice] command had us reaching for the controller.\" The Kinect's inability to understand some accents in English was criticized. Writing for \"Time\", Matt Peckham described the device as being \"chunky\" in appearance, but that the facial recognition login feature was \"creepy but equally sci-fi-future cool\", and that the new voice recognition system was a \"powerful, addictive way to navigate the console, and save for a few exceptions that seem to be smoothing out with use\". However, its accuracy was found to be affected by background noise, and Peckham further noted that launching games using voice recognition required that the full title of the game be given rather than an abbreviated name that the console \"ought to semantically understand\", such as \"Forza Motorsport 5\" rather than \"Forza 5\".\n\nPrior to Xbox One's launch, privacy concerns were raised over the new Kinect; critics showed concerns the device could be used for surveillance, stemming from the originally announced requirements that Xbox One's Kinect be plugged in at all times, plus the initial always-on DRM system that required the console to be connected to the internet to ensure continued functionality. Privacy advocates contended that the increased amount of data which could be collected with the new Kinect (such as a person's eye movements, heart rate, and mood) could be used for targeted advertising. Reports also surfaced regarding recent Microsoft patents involving Kinect, such as a DRM system based on detecting the number of viewers in a room, and tracking viewing habits by awarding achievements for watching television programs and advertising. While Microsoft stated that its privacy policy \"prohibit[s] the collection, storage, or use of Kinect data for the purpose of advertising\", critics did not rule out the possibility that these policies could be changed prior to the release of the console. Concerns were also raised that the device could also record conversations, as its microphone remains active at all times. In response to the criticism, a Microsoft spokesperson stated that users are \"in control of when Kinect sensing is On, Off or Paused\", will be provided with key privacy information and settings during the console's initial setup, and that user-generated content such as photos and videos \"will not leave your Xbox One without your explicit permission.\" Microsoft ultimately decided to reverse its decision to require Kinect usage on Xbox One, but the console still shipped with the device upon its launch in November 2013.\n\nWhile announcing Kinect's discontinuation in an interview with Fast Co. Design on October 25, 2017, Microsoft stated that 35 million units had been sold since its release. 24 million units of Kinect had been shipped by February 2013. Having sold 8 million units in its first 60 days on the market, Kinect claimed the Guinness World Record of being the \"fastest selling consumer electronics device\". According to Wedbush analyst Michael Pachter, Kinect bundles accounted for about half of all Xbox 360 console sales in December 2010 and for more than two-thirds in February 2011. More than 750,000 Kinect units were sold during the week of Black Friday 2011.\n\nKinect competes with several motion controllers on other home consoles, such as Wii Remote Plus for Wii and Wii U, PlayStation Move/PlayStation Eye for PlayStation 3, and PlayStation Camera for PlayStation 4.\n\n\n\n",
    "id": "23048428",
    "title": "Kinect"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=98778",
    "text": "Natural language understanding\n\nNatural language understanding (NLU) is a subtopic of natural language processing in artificial intelligence that deals with machine reading comprehension. NLU is considered an AI-hard problem.\n\nThe process of disassembling and parsing input is more complex than the reverse process of assembling output in natural language generation because of the occurrence of unknown and unexpected features in the input and the need to determine the appropriate syntactic and semantic schemes to apply to it, factors which are pre-determined when outputting language.\n\nThere is considerable commercial interest in the field because of its application to news-gathering, text categorization, voice-activation, archiving, and large-scale content-analysis.\n\nThe program STUDENT, written in 1964 by Daniel Bobrow for his PhD dissertation at MIT is one of the earliest known attempts at natural language understanding by a computer. Eight years after John McCarthy coined the term artificial intelligence, Bobrow's dissertation (titled \"Natural Language Input for a Computer Problem Solving System\") showed how a computer can understand simple natural language input to solve algebra word problems.\n\nA year later, in 1965, Joseph Weizenbaum at MIT wrote ELIZA, an interactive program that carried on a dialogue in English on any topic, the most popular being psychotherapy. ELIZA worked by simple parsing and substitution of key words into canned phrases and Weizenbaum sidestepped the problem of giving the program a database of real-world knowledge or a rich lexicon. Yet ELIZA gained surprising popularity as a toy project and can be seen as a very early precursor to current commercial systems such as those used by Ask.com.\n\nIn 1969 Roger Schank at Stanford University introduced the conceptual dependency theory for natural language understanding. This model, partially influenced by the work of Sydney Lamb, was extensively used by Schank's students at Yale University, such as Robert Wilensky, Wendy Lehnert, and Janet Kolodner.\n\nIn 1970, William A. Woods introduced the augmented transition network (ATN) to represent natural language input. Instead of \"phrase structure rules\" ATNs used an equivalent set of finite state automata that were called recursively. ATNs and their more general format called \"generalized ATNs\" continued to be used for a number of years.\n\nIn 1971 Terry Winograd finished writing SHRDLU for his PhD thesis at MIT. SHRDLU could understand simple English sentences in a restricted world of children's blocks to direct a robotic arm to move items. The successful demonstration of SHRDLU provided significant momentum for continued research in the field. Winograd continued to be a major influence in the field with the publication of his book \"Language as a Cognitive Process\". At Stanford, Winograd would later be the adviser for Larry Page, who co-founded Google.\n\nIn the 1970s and 1980s the natural language processing group at SRI International continued research and development in the field. A number of commercial efforts based on the research were undertaken, \"e.g.\", in 1982 Gary Hendrix formed Symantec Corporation originally as a company for developing a natural language interface for database queries on personal computers. However, with the advent of mouse driven, graphic user interfaces Symantec changed direction. A number of other commercial efforts were started around the same time, \"e.g.\", Larry R. Harris at the Artificial Intelligence Corporation and Roger Schank and his students at Cognitive Systems corp. In 1983, Michael Dyer developed the BORIS system at Yale which bore similarities to the work of Roger Schank and W. G. Lehnart.\n\nThe third millennium saw the introduction of systems using machine learning for text classification, such as the IBM Watson. However, this is not NLU. According to John Searle, Watson did not even understand the questions.\n\nJohn Ball, cognitive scientist and inventor of Patom Theory supports this assessment. NLP has made inroads for applications to support human productivity in service and ecommerce but this has largely been made possible by narrowing the scope of the application. There are thousands of ways to request something in a human language which still defies conventional NLP. \"To have a meaningful conversation with machines is only possible when we match every word to the correct meaning based on the meanings of the other words in the sentence – just like a 3-year-old does without guesswork\" Patom Theory\n\nThe umbrella term \"natural language understanding\" can be applied to a diverse set of computer applications, ranging from small, relatively simple tasks such as short commands issued to robots, to highly complex endeavors such as the full comprehension of newspaper articles or poetry passages. Many real world applications fall between the two extremes, for instance text classification for the automatic analysis of emails and their routing to a suitable department in a corporation does not require in depth understanding of the text, but needs to deal with a much larger vocabulary and more diverse syntax than the management of simple queries to database tables with fixed schemata.\n\nThroughout the years various attempts at processing natural language or \"English-like\" sentences presented to computers have taken place at varying degrees of complexity. Some attempts have not resulted in systems with deep understanding, but have helped overall system usability. For example, Wayne Ratliff originally developed the \"Vulcan\" program with an English-like syntax to mimic the English speaking computer in Star Trek. Vulcan later became the dBase system whose easy-to-use syntax effectively launched the personal computer database industry. Systems with an easy to use or \"English like\" syntax are, however, quite distinct from systems that use a rich lexicon and include an internal representation (often as first order logic) of the semantics of natural language sentences.\n\nHence the breadth and depth of \"understanding\" aimed at by a system determine both the complexity of the system (and the implied challenges) and the types of applications it can deal with. The \"breadth\" of a system is measured by the sizes of its vocabulary and grammar. The \"depth\" is measured by the degree to which its understanding approximates that of a fluent native speaker. At the narrowest and shallowest, \"English-like\" command interpreters require minimal complexity, but have a small range of applications. Narrow but deep systems explore and model mechanisms of understanding, but they still have limited application. Systems that attempt to understand the contents of a document such as a news release beyond simple keyword matching and to judge its suitability for a user are broader and require significant complexity, but they are still somewhat shallow. Systems that are both very broad and very deep are beyond the current state of the art.\n\nRegardless of the approach used, most natural language understanding systems share some common components. The system needs a lexicon of the language and a parser and grammar rules to break sentences into an internal representation. The construction of a rich lexicon with a suitable ontology requires significant effort, \"e.g.\", the Wordnet lexicon required many person-years of effort.\n\nThe system also needs a \"semantic theory\" to guide the comprehension. The interpretation capabilities of a language understanding system depend on the semantic theory it uses. Competing semantic theories of language have specific trade offs in their suitability as the basis of computer-automated semantic interpretation. These range from \"naive semantics\" or \"stochastic semantic analysis\" to the use of \"pragmatics\" to derive meaning from context.\n\nAdvanced applications of natural language understanding also attempt to incorporate logical inference within their framework. This is generally achieved by mapping the derived meaning into a set of assertions in predicate logic, then using logical deduction to arrive at conclusions. Therefore, systems based on functional languages such as Lisp need to include a subsystem to represent logical assertions, while logic-oriented systems such as those using the language Prolog generally rely on an extension of the built-in logical representation framework.\n\nThe management of context in natural language understanding can present special challenges. A large variety of examples and counter examples have resulted in multiple approaches to the formal modeling of context, each with specific strengths and weaknesses.\n\n",
    "id": "98778",
    "title": "Natural language understanding"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=7347241",
    "text": "Spreading activation\n\nSpreading activation is a method for searching associative networks, neural networks, or semantic networks. The search process is initiated by labeling a set of source nodes (e.g. concepts in a semantic network) with weights or \"activation\" and then iteratively propagating or \"spreading\" that activation out to other nodes linked to the source nodes. Most often these \"weights\" are real values that decay as activation propagates through the network. When the weights are discrete this process is often referred to as marker passing. Activation may originate from alternate paths, identified by distinct markers, and terminate when two alternate paths reach the same node. However brain studies show that several different brain areas play an important role in semantic processing.\n\nSpreading activation models are used in cognitive psychology to model the fan out effect.\n\nSpreading activation can also be applied in information retrieval, by means of a network of nodes representing documents and terms contained in those documents.\n\nAs it relates to cognitive psychology, spreading activation is how the brain moves through an entire network of ideas to retrieve specific information. The spreading activation theory presents the array of concepts within our memory as cognitive units, each consisting of a node and its associated elements or characteristics, all connected together by edges. A spreading activation network can be represented schematically, in a sort of web diagram with shorter lines between two nodes meaning the ideas are more closely related and will typically be associated more quickly to the original concept.\n\nWhen a word (the target) is preceded by an associated word (the prime) in word recognition tasks, participants seem to perform better in the amount of time that it takes them to respond. For instance, subjects respond faster to the word \"doctor\" when it is preceded by \"nurse\" than when it is preceded by an unrelated word like \"carrot\". This semantic priming effect with words that are close in meaning within the cognitive network has been seen in a wide range of tasks given by experimenters, ranging from sentence verification to lexical decision and naming.\n\nAs another example, if the original concept is \"red\" and the concept \"vehicles\" is primed, they are much more likely to say \"fire engine\" instead of something unrelated to vehicles, such as \"cherries\". If instead \"fruits\" was primed, they would likely name \"cherries\" and continue on from there. The activation of pathways in the network has everything to do with how closely linked two concepts are by meaning, as well as how a subject is primed.\n\nA directed graph is populated by Nodes[ 1...N ] each having an associated activation value A [ i ] which is a real number in the range [ 0.0 ... 1.0]. A Link[ i, j ] connects source node[ i ] with target node[ j ]. Each edge has an associated weight W [ i, j ] usually a real number in the range [0.0 ... 1.0].\n\nParameters:\n\nSteps:\n\n\n",
    "id": "7347241",
    "title": "Spreading activation"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=17953841",
    "text": "OpenIRIS\n\nOpenIRIS is the open source version of IRIS, a semantic desktop that enables users to create a \"personal map\" across their office-related information objects. The name IRIS is an acronym for \"Integrate. Relate. Infer. Share.\"\n\nIRIS includes a machine-learning platform to help automate this process. It provides \"dashboard\" views, contextual navigation, and relationship-based structure across an extensible suite of office applications, including a calendar, web and file browser, e-mail client, and instant messaging client.\n\nIRIS was built as part of SRI International's CALO project, a very large artificial intelligence funded by the Defense Advanced Research Projects Agency (DARPA) under its Personalized Assistant that Learns program.\n\n\n\nCALO-funded research resulted in more than five hundred publications across all fields of artificial intelligence. Here are a few:\n\n\n",
    "id": "17953841",
    "title": "OpenIRIS"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=14659967",
    "text": "Project Joshua Blue\n\nJoshua Blue is a project under development by IBM that focuses on advancing the artificial intelligence field by designing and programing computers to emulate human mental functions.\n\nAccording to researchers at IBM's Thomas J. Watson Research Center, the main goal of Joshua Blue is \"to achieve cognitive flexibility that approaches human functioning\". In short, IBM is aiming to design Joshua Blue to 'think like a human', mainly in terms of emotional thought; similar IBM projects focusing on logical thought and strategic reasoning include Deep Blue, a logic-based chess playing computer, and Watson, a question-driven artificial intelligence software program. Currently, the vast majority of computers and computational systems run off of an input-output model; some sort of input is entered in and some output is given back. Through Project Joshua Blue, IBM hopes to develop computers to the point where they are asking questions and searching for answers themselves rather than relying on an external input to run or only crunching numbers to give a pre-programmed response once given a task. If they succeed in this task, the artificial intelligence knowledge gained from Project Joshua Blue could potentially be used to create social robots that work and act very much like humans do. These robots could take over tasks too dangerous for humans to engage in even if such tasks required many different decisions to be made along the way; the technology advancement gained through Joshua Blue's potential success would allow for the robots to think for themselves and work their way through problems just as humans do.\n\nA model of Joshua Blue's learning pattern has been created. Similar to how young children learn human traits through interacting with their surroundings, Joshua Blue will acquire knowledge through external stimuli present in its environment. IBM believes that if computers evolve to learn in this way and then comprehend and analyze the knowledge gained using reason, computers could begin to possess a \"mind\", of sorts, capable of demonstrating complex social behaviors similar to those of humans.\n\nMost likely done as a precaution to avoid theft of their ideas, IBM has not released any significant information regarding how Joshua Blue will physically gather information. Thus far, IBM has revealed that Joshua Blue will be a computer with a network of wires and input nodes that function as a computer nervous system. This nervous system will be used by Joshua Blue to perceive affect, or personal emotional feeling. Not only will this network of input nodes help Joshua Blue discover things physically, it will also allow Joshua Blue to interpret the significance of events. The input nodes, or proprioceptors, will enable Joshua Blue to be aware of things that happen around itself, as well as recognize and attach meaning to the emotional effect produced by interacting with an object in a certain way. In addition, Joshua Blue's proprioceptors will function as pain and pleasure sensors, allowing Joshua Blue to employ a similar \"reward and punishment\" system that humans use to form behaviors. This will eventually lead Joshua Blue to expect certain outcomes from acting, reacting, or interacting in certain ways, eventually leading to the development of behavioral patterns which IBM hopes to study in order to further bridge the gap between human mental functions and computer functions.\n\nThrough three of their projects (Deep Blue, Watson, and Joshua Blue), IBM is attempting to create computers that imitate the common functions of the human brain. \n\n",
    "id": "14659967",
    "title": "Project Joshua Blue"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=6278094",
    "text": "Means-ends analysis\n\nMeans-ends analysis (MEA) is a problem solving technique used commonly in artificial intelligence (AI) for limiting search in AI programs.\n\nIt is also a technique used at least since the 1950s as a creativity tool, most frequently mentioned in engineering books on design methods. MEA is also related to means-ends chain approach used commonly in consumer behavior analysis. It is also a way to clarify one's thoughts when embarking on a mathematical proof.\n\nAn important aspect of intelligent behavior as studied in AI is \"goal-based\" problem solving, a framework in which the solution of a problem can be described by finding a sequence of \"actions\" that lead to a desirable goal. A goal-seeking system is supposed to be connected to its outside environment by sensory channels through which it receives information about the environment and motor channels through which it acts on the environment. (The term \"afferent\" is used to describe \"inward\" sensory flows, and \"efferent\" is used to describe \"outward\" motor commands.) In addition, the system has some means of storing in a \"memory\" information about the \"state\" of the environment (afferent information) and information about actions (efferent information). Ability to attain goals depends on building up associations, simple or complex, between particular changes in states and particular actions that will bring these changes about. Search is the process of discovery and assembly of sequences of actions that will lead from a given state to a desired state. While this strategy may be appropriate for machine learning and problem solving, it is not always suggested for humans (e.g. cognitive load theory and its implications).\n\nThe MEA technique is a strategy to control search in problem-solving. Given a current state and a goal state, an action is chosen which will reduce the \"difference\" between the two. The action is performed on the current state to produce a new state, and the process is recursively applied to this new state and the goal state.\n\nNote that, in order for MEA to be effective, the goal-seeking system must have a means of associating to any kind of detectable difference those actions that are relevant to reducing that difference. It must also have means for detecting the progress it is making (the changes in the differences between the actual and the desired state), as some attempted sequences of actions may fail and, hence, some alternate sequences may be tried.\n\nWhen knowledge is available concerning the importance of differences, the most important difference is selected first to further improve the average performance of MEA over other brute-force search strategies. However, even without the ordering of differences according to importance, MEA improves over other search heuristics (again in the average case) by focusing the problem solving on the actual differences between the current state and that of the goal.\n\nThe MEA technique as a problem-solving strategy was first introduced in 1961 by Allen Newell and Herbert A. Simon in their computer problem-solving program General Problem Solver (GPS). In that implementation, the correspondence between differences and actions, also called \"operators\", is provided a priori as knowledge in the system. (In GPS this knowledge was in the form of \"table of connections\".)\n\nWhen the action and side-effects of applying an operator are penetrable the search may select the relevant operators by inspection of the operators and do without a table of connections. This latter case, of which the canonical example is STRIPS, an automated planning computer program, allows task-independent correlation of differences to the operators which reduce them.\n\nProdigy, a problem solver developed in a larger learning-assisted automated planning project started at Carnegie Mellon University by Jaime Carbonell, Steven Minton and Craig Knoblock, is another system that used MEA.\n\nProfessor Morten Lind, at Technical University of Denmark has developed a tool called multilevel flow modeling (MFM). It performs means-end based diagnostic reasoning for industrial control and automation systems.\n\n",
    "id": "6278094",
    "title": "Means-ends analysis"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=25264546",
    "text": "Evolutionary developmental robotics\n\nEvolutionary developmental robotics (evo-devo-robo for short) refers to methodologies that systematically integrate evolutionary robotics, epigenetic robotics and morphogenetic robotics to study the evolution, physical and mental development and learning of natural intelligent systems in robotic systems. The field was formally suggested and fully discussed in a published paper and further discussed in a published dialogue.\n\nThe theoretical foundation of evo-devo-robo includes evolutionary developmental biology (evo-devo), evolutionary developmental psychology, developmental cognitive neuroscience etc. Further discussions on evolution, development and learning in robotics and design can be found in a number of papers, including papers on hardware systems and computing tissues.\n\n",
    "id": "25264546",
    "title": "Evolutionary developmental robotics"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=430976",
    "text": "Uncanny valley\n\nIn aesthetics, the uncanny valley is a hypothesized relationship between the degree of an object's resemblance to a human being and the emotional response to such an object. The concept of the uncanny valley suggests that humanoid objects which appear almost, but not exactly, like real human beings elicit uncanny, or strangely familiar, feelings of eeriness and revulsion in observers. \"Valley\" denotes a dip in the human observer's affinity for the replica, a relation that otherwise increases with the replica's human likeness.\n\nExamples can be found in robotics, 3D computer animations, and lifelike dolls among others. With the increasing prevalence of virtual reality, augmented reality, and photorealistic computer animation, the 'valley' has been cited in the popular press in reaction to the verisimilitude of the creation as it approaches indistinguishability from reality. The uncanny valley hypothesis predicts that an entity appearing almost human risks eliciting cold, eerie feelings in viewers.\n\nThe concept was identified by the robotics professor Masahiro Mori as \"Bukimi no Tani Genshō\" (不気味の谷現象) in 1970. The term was first translated as \"uncanny valley\" in the 1978 book \"Robots: Fact, Fiction, and Prediction\", written by Jasia Reichardt, thus forging an unintended link to Ernst Jentsch's concept of the \"uncanny\", introduced in a 1906 essay entitled \"On the Psychology of the Uncanny.\" Jentsch's conception was elaborated by Sigmund Freud in a 1919 essay entitled \"The Uncanny\" (\"Das Unheimliche\").\n\nMori's original hypothesis states that as the appearance of a robot is made more human, some observers' emotional response to the robot becomes increasingly positive and empathetic, until it reaches a point beyond which the response quickly becomes strong revulsion. However, as the robot's appearance continues to become less distinguishable from a human being, the emotional response becomes positive once again and approaches human-to-human empathy levels.\n\nThis area of repulsive response aroused by a robot with appearance and motion between a \"barely human\" and \"fully human\" entity is the uncanny valley. The name captures the idea that an almost human-looking robot seems overly \"strange\" to some human beings, produces a feeling of uncanniness, and thus fails to evoke the empathic response required for productive human–robot interaction.\n\nA number of theories have been proposed to explain the cognitive mechanism underlying the phenomenon:\n\nA series of studies experimentally investigated whether uncanny valley effects exist for static images of robot faces. Mathur MB & Reichling DB used two complementary sets of stimuli spanning the range from very mechanical to very human-like: first, a sample of 80 objectively chosen robot face images from Internet searches, and second, a morphometrically and graphically controlled 6-face series set of faces. They asked subjects to explicitly rate the likability of each face. To measure trust toward each face, subjects completed a one-shot investment game to indirectly measure how much money they were willing to \"wager\" on a robot's trustworthiness. Both stimulus sets showed a robust uncanny valley effect on explicitly-rated likability and a more context-dependent uncanny valley on implicitly-rated trust. Their exploratory analysis of one proposed mechanism for the uncanny valley, perceptual confusion at a category boundary, found that category confusion occurs in the uncanny valley but does not mediate the effect on social and emotional responses.\n\nOne study conducted in 2009 examined the evolutionary mechanism behind the aversion associated with the uncanny valley. A group of five monkeys were shown three images: two different 3D monkey faces (realistic, unrealistic), and a real photo of a monkey's face. The monkeys' eye-gaze was used as a proxy for preference or aversion. Since the realistic 3D monkey face was looked at less than either the real photo, or the unrealistic 3D monkey face, this was interpreted as an indication that the monkey participants found the realistic 3D face aversive, or otherwise preferred the other two images. As one would expect with the uncanny valley, more realism can lead to less positive reactions, and this study demonstrated that neither human-specific cognitive processes, nor human culture explain the uncanny valley. In other words, this aversive reaction to realism can be said to be evolutionary in origin.\n\nAs of 2011, researchers at University of California, San Diego and California Institute for Telecommunications and Information Technology are measuring human brain activations related to the uncanny valley. In one study using fMRI, a group of cognitive scientists and roboticists found the biggest differences in brain responses for uncanny robots in parietal cortex, on both sides of the brain, specifically in the areas that connect the part of the brain’s visual cortex that processes bodily movements with the section of the motor cortex thought to contain mirror neurons. The researchers say they saw, in essence, evidence of mismatch or perceptual conflict. The brain \"lit up\" when the human-like appearance of the android and its robotic motion \"didn’t compute\". Ayşe Pınar Saygın, an assistant professor from UCSD, says \"The brain doesn’t seem selectively tuned to either biological appearance or biological motion per se. What it seems to be doing is looking for its expectations to be met – for appearance and motion to be congruent.\"\n\nViewer perception of facial expression and speech and the uncanny valley in realistic, human-like characters intended for video games and film is being investigated by Tinwell et al., 2011. Consideration is also given by Tinwell et al. (2010) as to how the uncanny may be exaggerated for antipathetic characters in survival horror games. Building on the body of work already undertaken in android science, this research intends to build a conceptual framework of the uncanny valley using 3D characters generated in a real-time gaming engine. The goal is to analyze how cross-modal factors of facial expression and speech can exaggerate the uncanny. Tinwell et al., 2011 have also introduced the notion of an \"unscalable\" uncanny wall that suggests that a viewer’s discernment for detecting imperfections in realism will keep pace with new technologies in simulating realism. A summary of Angela Tinwell's research on the uncanny valley, psychological reasons behind the uncanny valley and how designers may overcome the uncanny in human-like virtual characters is provided in her book, \"The Uncanny Valley in Games and Animation\" by CRC Press.\n\nA number of design principles have been proposed for avoiding the uncanny valley:\n\nA number of criticisms have been raised concerning whether the uncanny valley exists as a unified phenomenon amenable to scientific scrutiny:\n\nAn effect similar to the uncanny valley was noted by Charles Darwin in 1839:\nA similar \"uncanny valley\" effect could, according to the ethical-futurist writer Jamais Cascio, show up when humans begin modifying themselves with transhuman enhancements (cf. body modification), which aim to improve the abilities of the human body beyond what would normally be possible, be it eyesight, muscle strength, or cognition. So long as these enhancements remain within a perceived norm of human behavior, a negative reaction is unlikely, but once individuals supplant normal human variety, revulsion can be expected. However, according to this theory, once such technologies gain further distance from human norms, \"transhuman\" individuals would cease to be judged on human levels and instead be regarded as separate entities altogether (this point is what has been dubbed \"posthuman\"), and it is here that acceptance would rise once again out of the uncanny valley. Another example comes from \"pageant retouching\" photos, especially of children, which some find disturbingly doll-like.\n\nA number of films that use computer-generated imagery to show characters have been described by reviewers as giving a feeling of revulsion or \"creepiness\" as a result of the characters looking too realistic. Examples include the following:\n\n\nThe fear, arising at contemplation of the \"person\" having small aberrations, and strengthening of impression because of its movement were noticed in 1818 by Mary Shelley in the novel \"Frankenstein; or, The Modern Prometheus\":\n\nIn the 2008 \"30 Rock\" episode \"Succession\", Frank Rossitano explains the uncanny valley concept, using a graph and \"Star Wars\" examples, to try to convince Tracy Jordan that his dream of creating a pornographic video game is impossible. He also references the computer-animated film \"The Polar Express.\"\n\n",
    "id": "430976",
    "title": "Uncanny valley"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=402688",
    "text": "Algorithmic probability\n\nIn algorithmic information theory, algorithmic probability, also known as Solomonoff probability, is a mathematical method of assigning a prior probability to a given observation. It was invented by Ray Solomonoff in the 1960s. \nIt is used in inductive inference theory and analyses of algorithms . In his general theory of inductive inference, Solomonoff uses the prior obtained by this formula, in Bayes' rule for prediction .\n\nIn the mathematical formalism used, the observations have the form of finite binary strings, and the universal prior is a probability distribution over the set of finite binary strings . The prior is universal in the\nTuring-computability sense, i.e. no string has zero probability. It is not computable, but it can be approximated. \n\nAlgorithmic probability deals with the following questions. Given a body of data about some phenomenon that we want to understand,\n\n\nFour principle inspirations for Solomonoff's algorithmic probability were: Occam's razor, Epicurus' principle of multiple explanations, modern computing theory (e.g. use of a universal Turing machine) and Bayes rule for prediction.\n\nOccam's razor and Epicurus' principle are essentially two different nonmathematical approximations of the universal prior.\n\n\n\nAt the heart of the universal prior is an abstract model of a computer, such as a universal Turing machine. Any abstract computer will do, as long as it is Turing-complete, i.e. every finite binary string has at least one program that will compute it on the abstract computer .\n\nThe abstract computer is used to give precise meaning to the phrase \"simple explanation\" . In the formalism used, explanations, or theories of phenomena, are computer programs that generate observation strings when run on the abstract computer . A simple explanation is a short computer program. A complex explanation is a long computer program . Simple explanations are more likely, so a high-probability observation string is one generated\nby a short computer program, or perhaps by any of a large number of slightly longer computer programs. A low-probability observation string is one that can only be generated by a long computer program .\n\nThese ideas can be made specific and the probabilities used to construct a prior probability\ndistribution for the given observation. Solomonoff's main reason for inventing this prior is so that it can be used in Bayes' rule when the actual prior is unknown, enabling prediction under uncertainty. It predicts the most likely continuation of that observation, and provides a measure of how likely this continuation will be .\n\nAlthough the universal probability of an observation (and its extension) is incomputable, there is a computer algorithm, Levin Search, which, when run for longer and longer periods of time, will generate a sequence of approximations which converge to the universal probability distribution.\n\nSolomonoff proved this distribution to be \nmachine-invariant within a constant factor (called the invariance theorem).\n\nSolomonoff invented the concept of algorithmic probability with its associated invariance theorem around 1960, publishing a report on it: \"A Preliminary Report on a General Theory of Inductive Inference.\" He clarified these ideas more fully in 1964 with \"A Formal Theory of Inductive Inference,\" Part I and Part II.\n\nSolomonoff described a universal computer with a randomly generated input program. The program computes some possibly infinite output. The universal probability distribution is the probability distribution on all possible output strings with random input.\n\nThe algorithmic probability of any given finite output prefix \"q\" is the sum of the probabilities of the programs that compute something starting with \"q\". Certain long objects with short programs have high probability.\n\nAlgorithmic probability is the main ingredient of Solomonoff's theory of inductive inference, the theory of prediction based on observations; it was invented with the goal of using it for machine learning; given a sequence of symbols, which one will come next? Solomonoff's theory provides an answer that is optimal in a certain sense, although it is incomputable. Unlike, for example, Karl Popper's informal inductive inference theory, Solomonoff's is mathematically rigorous.\n\nAlgorithmic probability is closely related to the concept of Kolmogorov complexity. Kolmogorov's introduction of complexity was motivated by information theory and problems in randomness, while Solomonoff introduced algorithmic complexity for a different reason: inductive reasoning. A single universal prior probability that can be substituted for each actual prior probability in Bayes’s rule was invented by Solomonoff with Kolmogorov complexity as a side product.\n\nSolomonoff's enumerable measure is universal in a certain powerful sense, but the computation time can be infinite. One way of dealing with this issue is a variant of Leonid Levin's Search Algorithm, which limits the time spent computing the success of possible programs, with shorter programs given more time. Other methods of limiting the search space include training sequences.\n\n\n\n\n\n",
    "id": "402688",
    "title": "Algorithmic probability"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=24870679",
    "text": "LIDA (cognitive architecture)\n\nThe LIDA (Learning Intelligent Distribution Agent) cognitive architecture is an integrated artificial cognitive system that attempts to model a broad spectrum of cognition in biological systems, from low-level perception/action to high-level reasoning. Developed primarily by Stan Franklin and colleagues at the University of Memphis, the LIDA architecture is empirically grounded in cognitive science and cognitive neuroscience. In addition to providing hypotheses to guide further research, the architecture can support control structures for software agents and robots. Providing plausible explanations for many cognitive processes, the LIDA conceptual model is also intended as a tool with which to think about how minds work.\n\nTwo hypotheses underlie the LIDA architecture and its corresponding conceptual model: 1) Much of human cognition functions by means of frequently iterated (~10 Hz) interactions, called cognitive cycles, between conscious contents, the various memory systems and action selection. 2) These cognitive cycles, serve as the \"atoms\" of cognition of which higher-level cognitive processes are composed.\n\nThough it is neither symbolic nor strictly connectionist, LIDA is a hybrid architecture in that it employs a variety of computational mechanisms, chosen for their psychological plausibility. The LIDA cognitive cycle is composed of modules and processes employing these mechanisms.\n\nThe LIDA architecture employs several modules that are designed using computational mechanisms drawn from the \"new AI\". These include variants of the Copycat Architecture, sparse distributed memory, the schema mechanism, the Behavior Net, and the subsumption architecture.\n\nAs a comprehensive, conceptual and computational cognitive architecture the LIDA architecture is intended to model a large portion of human cognition. Comprising a broad array of cognitive modules and processes, the LIDA architecture attempts to implement and flesh out a number of psychological and neuropsychological theories including Global Workspace Theory, situated cognition, perceptual symbol systems, working memory, memory by affordances, long-term working memory, and the H-CogAff architecture.\n\nThe LIDA cognitive cycle can be subdivided into three phases: the understanding phase, the attention (consciousness) phase, and the action selection and learning phase. Beginning the understanding phase, incoming stimuli activate low-level feature detectors in sensory memory. The output engages perceptual associative memory where higher-level feature detectors feed in to more abstract entities such as objects, categories, actions, events, etc. The resulting percept moves to the Workspace where it cues both Transient Episodic Memory and Declarative Memory producing local associations. These local associations are combined with the percept to generate a current situational model which is the agent's understanding of what is going on right now. The attention phase begins with the forming of coalitions of the most salient portions of the current situational model, which then compete for attention, that is a place in the current conscious contents. These conscious contents are then broadcast globally, initiating the learning and action selection phase. New entities and associations, and the reinforcement of old ones, occur as the conscious broadcast reaches the various forms of memory, perceptual, episodic and procedural. In parallel with all this learning, and using the conscious contents, possible action schemes are instantiated from Procedural Memory and sent to Action Selection, where they compete to be the behavior selected for this cognitive cycle. The selected behavior triggers sensory-motor memory to produce a suitable algorithm for its execution, which completes the cognitive cycle.\n\nVirtual Mattie (V-Mattie) is a software agent that gathers information from seminar organizers, composes announcements of next week's seminars, and mails them each week to a list that it keeps updated, all without the supervision of a human. V-Mattie employed many of the computational mechanisms mentioned above.\n\nBaars' Global Workspace Theory (GWT) inspired the transformation of V-Mattie into Conscious Mattie, a software agent with the same domain and tasks whose architecture included a consciousness mechanism à la GWT. Conscious Mattie was the first functionally, though not phenomenally, conscious software agent. Conscious Mattie gave rise to IDA.\n\nIDA (Intelligent Distribution Agent) was developed for the US Navy to fulfill tasks performed by human resource personnel called detailers. At the end of each sailor's tour of duty, he or she is assigned to a new billet. This assignment process is called distribution. The Navy employs almost 300 full time detailers to effect these new assignments. IDA's task is to facilitate this process, by automating the role of detailer. IDA was tested by former detailers and accepted by the Navy. Various Navy agencies supported the IDA project to the tune of some $1,500,000.\n\nThe LIDA (Learning IDA) architecture was originally spawned from IDA by the addition of several styles and modes of learning, but has since then grown to become a much larger and generic software framework.\n\n",
    "id": "24870679",
    "title": "LIDA (cognitive architecture)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=9124629",
    "text": "Cognitive philology\n\nCognitive philology is the science that studies written and oral texts as the product of human mental processes. Studies in cognitive philology compare documentary evidence emerging from textual investigations with results of experimental research, especially in the fields of cognitive and ecological psychology, neurosciences and artificial intelligence. \"The point is not the text, but the mind that made it\". Cognitive Philology aims to foster communication between literary, textual, philological disciplines on the one hand and researches across the whole range of the cognitive, evolutionary, ecological and human sciences on the other.\n\nCognitive philology: \n\nAmong the founding thinkers and noteworthy scholars devoted to such investigations are: \n\n\n",
    "id": "9124629",
    "title": "Cognitive philology"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=19442735",
    "text": "Psychology of reasoning\n\nThe psychology of reasoning is the study of how people reason, often broadly defined as the process of drawing conclusions to inform how people solve problems and make decisions. It overlaps with psychology, philosophy, linguistics, cognitive science, artificial intelligence, logic, and probability theory.\n\nPsychological experiments on how humans and other animals reason have been carried out for over 100 years. An enduring question is whether or not people have the capacity to be rational. What does it mean to be rational? Current research in this area addresses various questions about reasoning, rationality, judgments, intelligence, relationships between emotion and reasoning, and development.\n\nHow do people reason about sentences in natural language? Most experimentation on deduction has been carried out on hypothetical thought, in particular, examining how people reason about conditionals, e.g., \"If A then B\". Participants in experiments make the modus ponens inference, given the indicative conditional \"If A then B\", and given the premise \"A\", they conclude \"B\". However, given the indicative conditional and the minor premise for the modus tollens inference, \"not-B\", about half of the participants in experiments conclude \"not-A\" and the remainder concludes that nothing follows.\n\nThe ease with which people make conditional inferences is affected by content, as demonstrated in the well-known selection task developed by Peter Wason. Participants are better able to test a conditional that contains sensible content, e.g., \"if the envelope is sealed then it must have a 50 cent stamp on it\" compared to one that contains symbolic content, e.g.,\" if the letter is a vowel then the number is even\". Background knowledge can also lead to the suppression of even the simple modus ponens inference Participants given the conditional \"if Lisa has an essay to write then she studies late in the library\" and the premise \"Lisa has an essay to write \" make the modus ponens inference 'she studies late in the library', but the inference is suppressed when they are also given a second conditional \"if the library stays open then she studies late in the library\". Interpretations of the suppression effect are controversial \n\nOther investigations of propositional inference examine how people think about disjunctive alternatives, e.g., \"A or else B\", and how they reason about negation, e.g., \"It is not the case that A and B\". Many experiments have been carried out to examine how people make relational inferences, including comparisons, e.g., \"A is better than B\". Such investigations also concern spatial inferences, e.g. \"A is in front of B\" and temporal inferences, e.g. \"A occurs before B\". Other common tasks include categorical syllogisms, used to examine how people reason about quantifiers such as \"All\" or \"Some\", e.g., \"Some of the A are not B\".\n\nThere are several alternative theories of the cognitive processes that human reasoning is based on. One view is that people rely on a mental logic consisting of formal (abstract or syntactic) inference rules similar to those developed by logicians in the propositional calculus. Another view is that people rely on domain-specific or content-sensitive rules of inference. A third view is that people rely on mental models, that is, mental representations that correspond to imagined possibilities. The mental model theory is the subject of the \"mental models website\" A fourth view is that people compute probabilities.\n\nOne controversial theoretical issue is the identification of an appropriate competence model, or a standard against which to compare human reasoning. Initially classical logic was chosen as a competence model. Subsequently some researchers opted for non-monotonic logic and Bayesian probability. Research on mental models and reasoning has led to the suggestion that people are rational in principle but err in practice. Connectionist approaches towards reasoning have also been proposed.\n\nHow does reasoning develop? Jean Piaget's theory of cognitive development describes a sequence of stages in the development of reasoning from infancy to adulthood. According to the neo-Piagetian theories of cognitive development, changes in reasoning with development come from increasing working memory capacity, increasing speed of processing, and enhanced executive functions and control. Increasing self-awareness is also an important factor.\n\nInductive reasoning makes broad generalizations from specific cases or observations. In this process of reasoning, general assertions are made based on past specific pieces of evidence. This kind of reasoning allows the conclusion to be false even if the original statement is true. For example, if one observes a college athlete, one makes predictions and assumptions about other college athletes based on that one observation. Scientists use inductive reasoning to create theories and hypotheses.\n\nIn opposition, deductive reasoning is a basic form of valid reasoning. In this reasoning process a person starts with a known claim or a general belief and from there asks what follows from these foundations or how will these premises influence other beliefs. In other words, deduction starts with a hypothesis and examines the possibilities to reach a conclusion. Deduction helps people understand why their predictions are wrong and indicates that their prior knowledge or beliefs are off track. An example of deduction can be seen in the scientific method when testing hypotheses and theories. Although the conclusion usually corresponds and therefore proves the hypothesis, there are some cases where the conclusion is logical, but the generalization is not. For example, the argument, “All young girls wear skirts. Julie is a young girl. Therefore, Julie wears skirts,” is valid logically, but is not sound because the first premise isn't true.\n\nThe syllogism is a form of deductive reasoning in which two statements reach a logical conclusion. With this reasoning, one statement could be “Every A is B” and another could be “This C is A”. Those two statements could then lead to the conclusion that “This C is B”. These types of syllogisms are used to test deductive reasoning to ensure there is a valid hypothesis. A Syllogistic Reasoning Task was created from a study performed by Morsanyi, Kinga, Handley, and Simon that examined the intuitive contributions to reasoning. They used this test to assess why “syllogistic reasoning performance is based on an interplay between a conscious and effortful evaluation of logicality and an intuitive appreciation of the believability of the conclusions”.\n\nAnother form of reasoning is called abductive reasoning. This type is based on creating and testing hypotheses using the best information available. Abductive reasoning produces the kind of daily decision-making that works best with the information present, which often is incomplete. This could involve making educated guesses from observed unexplainable phenomena. This type of reasoning can be seen in the world when doctors make decisions about diagnoses from a set of results or when jurors use the relevant evidence to make decisions about a case.\n\nJudgment and reasoning involve thinking through the options, making a judgment or conclusion and finally making a decision. Making judgments involves heuristics, or efficient strategies that usually lead you to the right answers. The most common heuristics used are attribute substitution, the availability heuristic, the representativeness heuristic and the anchoring heuristic – these all aid in quick reasoning and work in most situations. Heuristics allow for errors, a price paid to gain efficiency.\nOther errors in judgment, therefore affecting reasoning, include errors in judgment about covariation – a relationship between two variables such that the presence and magnitude of one can predict the presence and magnitude of the other. One cause of covariation is confirmation bias, or the tendency to be more responsive to evidence that confirms your beliefs. But assessing covariation can be pulled off track by neglecting base-rate information – how frequently something occurs in general. However people often ignore base rates and tend to use other information presented.\nThere are more sophisticated judgment strategies that result in fewer errors. People often reason based on availability but sometimes they look for other, more accurate, information to make judgments. This suggests there are two ways of thinking, known as the Dual-Process Model. The first, System I, is fast, automatic and uses heuristics – more of intuition. The second, System II, is slower, effortful and more likely to be correct – more reasoning.\n\n\nDecision making is often influenced by the emotion of regret and the element of risk. People are strongly motivated by regret and we can see this when they select options they tend to select the option that they will regret the least trying to minimize the amount of regret we will have. Many decisions also include a large element of risk, and in these cases people tend to ask themselves what the level of risk is. They ask themselves how much dread they would experience when thinking about a nuclear accident, and then use that dread as an indicator of risk. We ask “how does this make me feel?” rather than “how risky is this?”\n\nAntonio Damasio suggests that somatic markers, certain memories that can cause a strong bodily reaction, act as a way to guide decision making as well. For example, when you are remembering a scary movie and once again become tense and your palms might begin to sweat. Damasio argues that when making a decision we rely on our “gut feelings” to assess various options, and this makes us decide to go with a decision that is more positive and stay away from those that are negative. He also argues that the orbitofrontal cortex - located at the base of the frontal lobe, just above the eyes - is crucial in your use of somatic markers, because it is the part in the brain that allows you to interpret emotion.\n\nAnother note to make is that when emotion shapes decisions, the influence is usually based on predictions of the future. When people ask themselves how they would react, they are making inferences about the future. Researchers suggest affective forecasting, the ability to predict your own emotions, is poor because people tend to overestimate how much they will regret their errors.\n\n\n\n",
    "id": "19442735",
    "title": "Psychology of reasoning"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=8271663",
    "text": "Scilab Image Processing\n\nSIP is a toolbox for processing images in Scilab. SIP is meant to be a free, complete, and useful image toolbox for Scilab. Its goals include tasks such as filtering, blurring, edge detection, thresholding, histogram manipulation, segmentation, mathematical morphology, and color image processing.\n\nThough SIP is still in early development it can currently import and output image files in many formats including BMP, JPEG, GIF, PNG, TIFF, XPM, and PCX. SIP uses ImageMagick to accomplish this.\n\nSIP is licensed under the GPL.\n\n",
    "id": "8271663",
    "title": "Scilab Image Processing"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=33529387",
    "text": "Artificial intuition\n\nThe theoretical concept of artificial intuition is the capacity of an artificial object or software to function with the factor of consciousness known as intuition: a machine-based system that has some capacity to function analogously to human intuition.\n\nConventional human intuition is a function of the human mind, defined particularly by the psychologist and psychiatrist Carl Jung. Psychologist Jean Piaget showed that intuitive functioning within the normally developing human child at the \"Intuitive Thought Substage\" of the preoperational stage occurred at from four to seven years of age. In Carl Jung's concept of synchronicity, the concept of \"intuitive intelligence\" is described as something like a capacity that transcends ordinary-level functioning to a point where information is understood with a greater depth than is available in more simple rationally-thinking entities.\n\nArtificial intuition is theoretically (or otherwise) a sophisticated function of an artifice that is able to interpret data with depth and locate hidden factors functioning in Gestalt psychology, and that intuition in the artificial mind would, in the context described here, be a bottom-up process upon a macroscopic scale identifying something like the archetypal (see ).\n\nTo create artificial intuition supposes the possibility of the re-creation of a higher functioning of the human mind, with capabilities such as what might be found in semantic memory and learning. The transferral of the functioning of a biological system to synthetic functioning is based upon modeling of functioning from knowledge of cognition and the brain, for instance as applications of models of artificial neural networks from the research done within the discipline of computational neuroscience.\n\nThe notion of a process of a data-interpretative has already been found in a computational-linguistic software application that has been created for use in an internal security context. The software integrates computed data based specifically on objectives incorporating a paradigm described as \"religious intuitive\" (hermeneutic), functional to a degree that represents advances upon the performance of \"generic lexical\" data mining. \n\nVeeramachaneni and others at MIT developed a machine which performed comparably to humans in a test of intuitive intelligence during 2015.\n\nArtificial intelligence in fiction often crosses the line to apparent artificial intuition, although it can't be shown if the intent of the fiction creator was to show a simulation of intuition or that real artificial intuition is part of the story's AI, because this depends on the internal structure of the programming of the AI, which is not usually shown in stories.\n\n\nThe Oxford Companion to Philosophy - E. Honderich (Oxford University Press, 1995) \n\n",
    "id": "33529387",
    "title": "Artificial intuition"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=4678739",
    "text": "Structure mapping engine\n\nIn artificial intelligence and cognitive science, the structure mapping engine is an implementation in software of an algorithm for analogical matching based on the psychological theory of Dedre Gentner. The basis of Gentner's structure-mapping idea is that an analogy is a mapping of knowledge from one domain (the base) into another (the target). The structure-mapping engine, or SME, is a computer simulation of the analogy and similarity comparisons.\n\nAs of 1990, more than 40 projects had used it [Falkenhainer, 2005]. R.M. French said that structure mapping theory is \"unquestionably the most influential work to date of the modeling of analogy-making\" [2002]. \n\nThe theory is useful because it ignores surface features and finds matches between potentially very different things if they have the same representational structure. For example, SME could determine that a pen is like a sponge because both are involved in dispensing liquid, even though they do this very differently.\n\nStructure mapping theory is based on the systematicity principle, which states that connected knowledge is preferred over independent facts. Therefore, the structure mapping engine should ignore isolated source-target mappings unless they are part of a bigger structure. The SME, the theory goes, should map objects that are related to knowledge that has already been mapped.\n\nThe theory also requires that mappings be done one-to-one, which means that no part of the source description can map to more than one item in the target and no part of the target description can be mapped to more than one part of the source. The theory also requires that if a match maps subject to target, the arguments of subject and target must also be mapped. If both these conditions are met, the mapping is said to be \"structurally consistent.\"\n\nSME maps knowledge from a \"source\" into a \"target.\" SME calls each description a \"dgroup.\" Dgroups contain a list of entities and predicates. Entities represent the objects or concepts in a description — such as an input gear or a switch. Predicates are one of three types and are a general way to express knowledge for SME.\n\n\nFunctions and attributes have different meanings, and consequently SME processes them differently. For example, in SME’s true analogy rule set, attributes differ from functions because they cannot match unless there is a higher-order match between them. The difference between attributes and functions will be explained further in this section’s examples.\n\nAll predicates have four parameters. They have (1) a functor, which identifies it, and (2) a type, which is either relation, attribute, or function. The other two parameters (3 and 4) are for determining how to process the arguments in the SME algorithm. If the arguments have to be matched in order, commutative is false. If the predicate can take any number of arguments, N-ary is false. An example of a predicate definition is: (sme:defPredicate behavior-set (predicate) relation :n-ary? t :commutative? t)\nThe predicate’s functor is “behavior-set,” its type is “relation,” and its n-ary and commutative parameters are both set to true. The “(predicate)” part of the definition specifies that there will be one or more predicates inside an instantiation of behavior-set.\n\nThe algorithm has several steps.\nThe first step of the algorithm is to create a set of match hypotheses between source and target dgroups. A match hypothesis represents a possible mapping between any part of the source and the target. This mapping is controlled by a set of match rules. By changing the match rules, one can change the type of reasoning SME does. For example, one set of match rules may perform a kind of analogy called \"literal similarity.\" and another performs a kind of analogy called \"true-analogy.\" These rules are not the place where domain-dependent information is added, but rather where the analogy process is tweaked, depending on the type of cognitive function the user is trying to emulate.\n\nThere are two types of match rules: filter rules and intern rules. Intern rules use only the arguments of the expressions in the match hypotheses that the filter rules identify. This limitation makes the processing more efficient by constraining the number of match hypotheses that are generated. At the same time, it also helps to build the structural consistencies that are needed later on in the algorithm. An example of a filter rule from the true-analogy rule set creates match hypotheses between predicates that have the same functor. The true-analogy rule set has an intern rule that iterates over the arguments of any match hypothesis, creating more match hypotheses if the arguments are entities or functions, or if the arguments are attributes and have the same functor.\n\nIn order to illustrate how the match rules produce match hypotheses consider these two predicates:\n\ncodice_1\n\ncodice_2\n\nThe filter match rule generates a match between p1 and p2 because they share the same functor, \"transmit.\" The intern rules then produce three more match hypotheses: torque to signal, inputgear to switch, and secondgear to div10. The intern rules created these match hypotheses because all the arguments were entities.\n\nIf the arguments were functions or attributes instead of entities, the predicates would be expressed as:\n\ncodice_3\n\ncodice_4\n\nThese additional predicates make inputgear, secondgear, switch, and div10 functions or attributes depending on the value defined in the language input file. The representation also contains additional entities for gear and circuit.\n\nDepending on what type \"inputgear, secondgear, switch,\" and \"div10\" are, their meanings change. As attributes, each one is a property of the gear or circuit. For example, the gear has two attributes, inputgear and secondgear. The circuit has two attributes, switch and circuit. As functions inputgear, secondgear, switch, and div10 become quantities of the gear and circuit. In this example, the functions inputgear and secondgear now map to the numerical quantities “torque from inputgear” and “torque from secondgear,” For the circuit the quantities map to logical quantity “switch engaged” and the numerical quantity “current count on the divide by 10 counter.”\n\nSME processes these differently. It does not allow attributes to match unless they are part of a higher-order relation, but it does allow functions to match, even if they are not part of such a relation. It allows functions to match because they indirectly refer to entities and thus should be treated like relations that involve no entities. However, as next section shows, the intern rules assign lower weights to matches between functions than to matches between relations.\n\nThe reason SME does not match attributes is because it is trying to create connected knowledge based on relationships and thus satisfy the systematicity principle. For example, if both a clock and a car have inputgear attributes, SME will not mark them as similar. If it did, it would be making a match between the clock and car based on their appearance — not on the relationships between them.\n\nWhen the additional predicates in p3 and p4 are functions, the results from matching p3 and p4 are similar to the results from p1 and p2 except there is an additional match between gear and circuit and the values for the match hypotheses between (inputgear gear) and (switch circuit), and (secondgear gear) and (div10 circuit), are lower. The next section describes the reason for this in more detail.\n\nIf the inputgear, secondgear, switch, and div10 are attributes instead of entities, SME does not find matches between any of the attributes. It finds matches only between the transmit predicates and between torque and signal. Additionally, the structural-evaluation scores for the remaining two matches decrease. In order to get the two predicates to match, p3 would need to be replaced by p5, which is demonstrated below.\n\ncodice_5\n\nSince the true-analogy rule set identifies that the div10 attributes are the same between p5 and p4 and because the div10 attributes are both part of the higher-relation match between torque and signal, SME makes a match between (div10 gear) and (div10 circuit) — which leads to a match between gear and circuit.\n\nBeing part of a higher-order match is a requirement only for attributes. For example, if (div10 gear) and (div10 circuit) are not part of a higher-order match, SME does not create a match hypothesis between them. However, if div10 is a function or relation, SME does create a match.\n\nOnce the match hypotheses are generated, SME needs to compute an evaluation score for each hypothesis. SME does so by using a set of intern match rules to calculate positive and negative evidence for each match. Multiple amounts of evidence are correlated using Dempster’s rule [Shafer, 1978] resulting in positive and negative belief values between 0 and 1. The match rules assign different values for matches involving functions and relations. These values are programmable, however, and some default values that can be used to enforce the systematicity principle are described in [Falkenhainer et al., 1989].\n\nThese rules are:\n\n\nIn the example match between p1 and p2, SME gives the match between the transmit relations a positive evidence value of 0.7900, and the others get values of 0.6320. The transmit relation receives the evidence value of 0.7900 because it gains evidence from rules 1, 3, and 2. The other matches get a value of 0.6320 because 0.8 of the evidence from the transmit is propagated to these matches because of rule 5.\n\nFor predicates p3 and p4, SME assigns less evidence because the arguments of the transmit relations are functions. The transmit relation gets positive evidence of 0.65 because rule 3 no longer adds evidence. The match between (input gear) and (switch circuit) becomes 0.7120. This match gets 0.4 evidence because of rule 3, and 0.52 evidence propagated from the transmit relation because of rule 5.\n\nWhen the predicates in p3 and p4 are attributes, rule 4 adds -0.8 evidence to the transmit match because — though the functors of the transmit relation match — the arguments do not have the potential to match and the arguments are not functions.\n\nTo summarize, the intern match rules compute a structural evaluation score for each match hypothesis. These rules enforce the systematicity principle. Rule 5 provides trickle-down evidence in order to strengthen matches that are involved in higher-order relations. Rules 1, 3. and 4 add or subtract support for relations that could have matching arguments. Rule 2 adds support for the cases when the functors match. thereby adding support for matches that emphasize relationships.\n\nThe rules also enforce the difference between attributes, functions, and relations. For example, they have checks which give less evidence for functions than relations. Attributes are not specifically dealt with by the intern match rules, but SME’s filter rules ensure that they will only be considered for these rules if they are part of a higher-order relation, and rule 2 ensures that attributes will only match if they have identical functors.\n\nThe rest of the SME algorithm is involved in creating maximally consistent sets of match hypotheses. These sets are called gmaps. SME must ensure that any gmaps that it creates are structurally consistent; in other words, that they are one-to-one — such that no source maps to multiple targets and no target is mapped to multiple sources. The gmaps must also have support, which means that if a match hypothesis is in the gmap, then so are the match hypothesis that involve the source and target items.\n\nThe gmap creation process follows two steps. First, SME computes information about each match hypothesis — including entity mappings, any conflicts with other hypotheses, and what other match hypotheses with which it might be structurally inconsistent.\n\nSME then uses this information to merge match hypotheses — using a greedy algorithm and the structural evaluation score. It merges the match hypotheses into maximally structurally consistent connected graphs of match hypotheses. Then it combines gmaps that have overlapping structure if they are structurally consistent. Finally, it combines independent gmaps together while maintaining structural consistency.\n\nComparing a source to a target dgroup may produce one or more gmaps. The weight for each gmap is the sum of all the positive evidence values for all the match hypotheses involved in the gmap. For example, if a source containing p1 and p6 below, is compared to a target containing p2, SME will generate two gmaps. Both gmaps have a weight of 2.9186.\n\nSource:\n\ncodice_6\n\ncodice_7\n\nTarget:\ncodice_8\n\nThese are the gmaps which result from comparing a source containing a p1 and p6 and a target containing p2.\n\nGmap No. 1:<br>\n\nGmap No. 2:\n\nThe gmaps show pairs of predicates or entities that match. For example, in gmap No. 1, the entities \"torque\" and \"signal\" match and the behaviors transmit torque inputgear secondgear and transmit signal switch div10 match. Gmap No. 1 represents combining p1 and p2. Gmap No. 2 represents combining p1 and p6. Although p2 is compatible with both p1 and p6, the one-to-one mapping constraint enforces that both mappings cannot be in the same gmap. Therefore, SME produces two independent gmaps. In addition, combining the two gmaps together would make the entity mappings between thirdgear and div10 conflict with the entity mapping between secondgear and div10.\n\nChalmers, French, and Hofstadter [1992] criticize SME for its reliance on manually constructed LISP representations as input. They argue that too much human creativity is required to construct these representations; the intelligence comes from the design of the input, not from SME. Forbus et al. [1998] attempted to rebut this criticism. Morrison and Dietrich [1995] tried to reconcile the two points of view. Turney [2008] presents an algorithm that does not require LISP input, yet follows the principles of Structure Mapping Theory. Turney [2008] state that their work, too, is not immune to the criticism of Chalmers, French, and Hofstadter [1992].\n\nIn her article How Creative Ideas Take Shape, Liane Gabora writes \"According to the honing theory of creativity, creative thought works not on individually considered, discrete, predefined representations but on a contextually-elicited amalgam of items which exist in a state of potentiality and may not be readily separable. This leads to the prediction that analogy making proceeds not by mapping correspondences from candidate sources to target, as predicted by the structure mapping theory of analogy, but by weeding out non-correspondences, thereby whittling away at potentiality.\"\n\n",
    "id": "4678739",
    "title": "Structure mapping engine"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=33623429",
    "text": "DragonLord Enterprises, Inc.\n\nDragonLord Enterprises, Inc. is an American corporation that develops games, mobile apps, and\n3D simulations. Under the name Sequoia Consulting,\nit also specializes in robotics, machine learning,\nand applied artificial intelligence. DragonLord created the first adventure game running on a cell phone in the world.\n\nDragonLord created the first adventure game for cell phones in the world, \"Cavern Crawl\", in 2001. DragonLord was the first developer in America to create games for cell phones, and one of the first in the world, after Nokia's \"Snake\".\nDragonLord's \"Cavern Crawl\" and \"Mystic I Ching\" were\nfactory-built into the firmware of such cell phones as the Kyocera Model 2235 handset, distributed by Verizon; the S14 Opal, distributed by Cricket and MetroPCS; and the 2255 and 2325,\ndistributed by Sprint PCS in America and Virgin Mobile in the UK. The games were noted for providing animation and game play that were ahead of their time.\n\nDragonLord also made multiple versions of \"Tetris\",\nthat were built into the factory firmware of various Kyocera handsets through at least 2004.\n\nDragonLord's Sequoia Consulting created the software for a humanoid robot for NASA JPL\nin 2006. The robot was one of the first humanoid robots to integrate speech recognition of commands and speech generation of responses. It used a real-time expert system shell to plan and sequence actions.\nIt used primitive model-based stereo vision to locate and track objects in its environment.\nIt was able to turn, walk over to a spar, squat down, bend over,\nand pick the spar up, using an arm with no wrist joints.\n\nDragonLord is featured in the book \"The Fat Man on Game Audio\".\n\n",
    "id": "33623429",
    "title": "DragonLord Enterprises, Inc."
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=33527843",
    "text": "Trenchard More\n\nTrenchard More is a retired mathematician and computer scientist who worked at IBM's Thomas J Watson Research Centre after teaching posts at MIT and Yale. He was also a full Professor for two years at the Technical University of Denmark. He participated in the 1956 Dartmouth Summer Research Project on Artificial Intelligence. At the 50th year meeting of the Dartmouth Conference with Marvin Minsky, Geoffrey Hinton and Simon Osindero he presented \"The Future of Network Models\" and also gave a lecture entitled \"Routes to the Summit\".\n\nDesigned a theory for nested rectangular array that provided a formal structure used in the development of APL2 and the Nested Interactive Array Language.\n\n\n",
    "id": "33527843",
    "title": "Trenchard More"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1124646",
    "text": "Dartmouth workshop\n\nThe Dartmouth Summer Research Project on Artificial Intelligence was the name of a 1956 summer workshop now considered by many(though not all)\nto be the seminal event for artificial intelligence as a field.\n\nThe project lasted approximately 6 to 8 weeks, and was essentially an extended brainstorming session. 11 mathematicians and scientists were originally planned to be attendees, and while not all attended, more than 10 others came for short times.\n\nIn the early 1950s, there were various names for the field of \"thinking machines\" such as cybernetics, automata theory, and complex information processing \n\nIn 1955 John McCarthy, then a young Assistant Professor of Mathematics at Dartmouth College, decided to organize a group to clarify and develop ideas about thinking machines. He picked the name 'Artificial Intelligence' for the new field. He chose the name partly for its neutrality; avoiding a focus on narrow automata theory, and avoiding cybernetics which was heavily focused on analog feedback, as well as him potentially having to accept the assertive Norbert Wiener as guru or having to argue with him.\n\nIn early 1955, McCarthy approached the Rockefeller Foundation to request funding for a summer seminar at Dartmouth for about 10 participants. In June, he and Claude Shannon, a founder of Information Theory then at Bell Labs, met with Robert Morison, Director of Biological and Medical Research to discuss the idea and possible funding, though Morison, was unsure whether money would be made available for such a visionary project.\n\nOn September 2, 1955, the project was formally proposed by McCarthy, Marvin Minsky, Nathaniel Rochester and Claude Shannon. The proposal is credited with introducing the term 'artificial intelligence'.\n\nThe Proposal states \n\nThe proposal goes on to discuss computers, natural language processing, neural networks, theory of computation, abstraction and creativity (these areas within the field of artificial intelligence are considered still relevant to the work of the field). \nOn May 26, 1956, McCarthy notified Robert Morison of the planned 11 attendees:\n\nFor the full period:\n\nFor four weeks:\n\nFor the first two weeks:\n\nHe noted, ``We will concentrate on a problem of devising a way of programming a calculator to form concepts and to form generalizations. This of course is subject to change when the group gets together.\n\nAccording to Stottler Henke Associates, besides the proposal's authors, attendees at the conference included Ray Solomonoff, Oliver Selfridge, Trenchard More, Arthur Samuel, Herbert A. Simon, and Allen Newell.\nThe actual participants came at different times, mostly for much shorter times. Trenchard More replaced Rochester for three weeks and MacKay and Holland did not attend --- but the project was set to begin. \nAround June 18, 1956, the earliest participants (perhaps only Ray Solomonoff, maybe with Tom Etter) arrived at the Dartmouth campus in Hanover, N.H., to join John McCarthy who already had an apartment there. Ray and Marvin stayed at Professors' apartments, but most would stay at the Hanover Inn.\n\nThe Dartmouth Workshop is said to have run for six weeks in the summer of 1956. Ray Solomonoff's notes written during the Workshop time, 1956, however, say it ran for ``roughly eight weeks, from about June 18 to August 17.\nSolomonoff's Dartmouth notes start on June 22; June 28 mentions Minsky, June 30 mentions Hanover, N.H., July 1 mentions Tom Etter. On August 17, Ray gave a final talk.\n\nUnfortunately McCarthy lost his list of attendees! Instead, after the Dartmouth Project McCarthy sent Ray a preliminary list of participants and visitors plus those interested in the subject. There are 47 people listed.\n\nSolomonoff, however, made a complete list in his notes of the summer project:\n\n\nShannon attended Ray's talk on July 10 and Bigelow gave a talk on August 15. Ray doesn't mention Bernard Widrow, but apparently he visited, along with W.A. Clark and B.G. Farley. Trenchard mentions R. Culver and Ray mentions Bill Shutz. Herb Gelernter didn't attend, but was influenced later by what Rochester learned. Gloria Minsky also commuted there (with their part-beagle dog, Senje, who would start out in the car back seat and end up curled around her like a scarf), and attended some sessions (without Senje).\n\nRay Solomonoff, Marvin Minsky, and John McCarthy were the only three who stayed for the full-time. Trenchard took attendance during two weeks of his three-week visit. From three to about eight people would attend the daily sessions.\n\nThey had the entire top floor of the Dartmouth Math Department to themselves, and most weekdays they would meet at the main math classroom where someone might lead a discussion focusing on his ideas, or more frequently, a general discussion would be held.\n\nIt was not a directed group research project, discussions convered many topics but several directions are considered to have been initiated or encouraged by the Workshop: the rise of symbolic methods, systems focussed on limited domains (early Expert Systems), and deductive systems versus inductive systems. One participant, Arthur Samuel said, \"It was very interesting, very stimulating, very exciting\".\n\nRay Solomonoff kept notes during the summer giving his impression of the talks and the ideas from various discussions. These are available, along with other notes concerning the Dartmouth Summer Research Project on AI, at: http://raysolomonoff.com/dartmouth/\n\n\n",
    "id": "1124646",
    "title": "Dartmouth workshop"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=33591382",
    "text": "Type-1 OWA operators\n\nThe Yager's OWA (ordered weighted averaging) operators are used to aggregate the crisp values in decision making schemes (such as multi-criteria decision making, multi-expert decision making and multi-criteria/multi-expert decision making). It is widely accepted that Fuzzy sets are more suitable for representing preferences of criteria in decision making.\n\nThe type-1 OWA operators have been proposed for this purpose. The type-1 OWA operators provides a technique for directly aggregating uncertain information with uncertain weights via OWA mechanism in soft decision making and data mining, where these uncertain objects are modelled by fuzzy sets.\n\nThe two definitions for type-1 OWA operators are based on Zadeh's Extension Principle and formula_1-cuts of fuzzy sets. The two definitions lead to equivalent results.\n\nLet formula_2 be the set of fuzzy sets with domain of discourse formula_3, a type-1 OWA operator is defined as follows:\n\nGiven n linguistic weights formula_4 in the form of fuzzy sets defined on the domain of discourse formula_5, a type-1 OWA operator is a mapping, formula_6,\n\nsuch that\n\nwhere formula_10,and formula_11 is a permutation function such that formula_12, i.e., formula_13 is the formula_14th highest element in the set formula_15.\n\nUsing the alpha-cuts of fuzzy sets:\n\nGiven the n linguistic weights formula_16 in the form of fuzzy sets defined on the domain of discourse formula_17, then for each formula_18, an formula_19-level type-1 OWA operator with formula_19-level sets formula_21 to aggregate the formula_19-cuts of fuzzy sets formula_23 is:\n\nwhere formula_25, and formula_26 is a permutation function such that formula_27, i.e., formula_28 is the formula_14th largest\nelement in the set formula_15.\n\nGiven the \"n\" linguistic weights formula_16 in the form of fuzzy sets defined on the domain of discourse formula_17, and the fuzzy sets formula_33, then we have that\n\nwhere formula_35 is the aggregation result obtained by Definition 1, and formula_36 is the result obtained by in Definition 2.\n\nAccording to the Representation Theorem of Type-1 OWA Operators, a general type-1 OWA operator can be decomposed into a series of formula_1-level type-1 OWA operators. In practice, this series of formula_1-level type-1 OWA operators is used to construct the resulting aggregation fuzzy set. So we only need to compute the left end-points and right end-points of the intervals formula_39. Then, the resulting aggregation fuzzy set is constructed with the membership function as follows:\n\nFor the left end-points, we need to solve the following programming problem:\n\nwhile for the right end-points, we need to solve the following programming problem:\n\nA fast method has been presented to solve two programming problem so that the type-1 OWA aggregation operation can be performed efficiently, for details, please see the paper.\n\nThree-step process:\n\n\n\n\nType-2 OWA operators have been suggested to aggregate the type-2 fuzzy sets for soft decision making.\n",
    "id": "33591382",
    "title": "Type-1 OWA operators"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=33818014",
    "text": "Nervous system network models\n\nNetwork of human nervous system comprises nodes (for example, neurons) that are connected by links (for example, synapses). The connectivity may be viewed anatomically, functionally, or electrophysiologically. These are presented in several Wikipedia articles that include Connectionism (a.k.a. Parallel Distributed Processing (PDP)), Biological neural network, Artificial neural network (a.k.a. Neural network), Computational neuroscience, as well as in several books by Ascoli, G. A. (2002), Sterratt, D., Graham, B., Gillies, A., & Willshaw, D. (2011), Gerstner, W., & Kistler, W. (2002), and Rumelhart, J. L., McClelland, J. L., and PDP Research Group (1986) among others. The focus of this article is a comprehensive view of modeling a neural network (technically neuronal network based on neuron model). Once an approach based on the perspective and connectivity is chosen, the models are developed at microscopic (ion and neuron), mesoscopic (functional or population), or macroscopic (system) levels. Computational modeling refers to models that are developed using computing tools.\n\nThe nervous system consists networks made up of neurons and synapses connected to and controlling tissues as well as impacting human thoughts and behavior. In modeling neural networks of the nervous system one has to consider many factors. The brain and the neural network should be considered as an integrated and self-contained firmware system that includes hardware (organs), software (programs), memory (short term and long term), database (centralized and distributed), and a complex network of active elements (such as neurons, synapses, and tissues) and passive elements (such as parts of visual and auditory system) that carry information within and in-and-out of the body.\n\nWhy does one want to model the brain and neural network? Although highly sophisticated computer systems have been developed and used in all walks of life, they are nowhere close to the human system in hardware and software capabilities. So, scientists have been at work to understand the human operation system and try to simulate its functionalities. In order to accomplish this, one needs to model its components and functions and validate its performance with real life. Computational models of a well simulated nervous system enable learning the nervous system and apply it to real life problem solutions.\n\nWhat is brain and what is neural network? Section 2.1 addresses the former question from an evolutionary perspective. The answer to the second question is based on the neural doctrine proposed by Ramon y Cajal (1894). He hypothesized that the elementary biological unit is an active cell, called neuron, and the human machine is run by a vast network that connects these neurons, called neural (or neuronal) network. The neural network is integrated with the human organs to form the human machine comprising the nervous system.\n\nInnumerable number of models of various aspects of the nervous system has been developed and there are several Wikipedia articles identified above that have been generated on the subject. The purpose of this article is to present a comprehensive view of all the models and provide the reader, especially a novice, to the neuroscience, with reference to the various sources.\n\nThe basic structural unit of the neural network is connectivity of one neuron to another via an active junction, called synapse. Neurons of widely divergent characteristics are connected to each other via synapses, whose characteristics are also of diverse chemical and electrical properties. In presenting a comprehensive view of all possible modeling of the brain and neural network, an approach is to organize the material based on the characteristics of the networks and the goals that need to be accomplished. The latter could be either for understanding the brain and the nervous system better or to apply the knowledge gained from the total or partial nervous system to real world applications such as artificial intelligence, Neuroethics or improvements in medical science for society.\n\nOn a high level representation, the neurons can be viewed as connected to other neurons to form a neural network in one of three ways. A specific network can be represented as a physiologically (or anatomically) connected network and modeled that way. There are several approaches to this (see Ascoli, G.A. (2002) Sporns, O. (2007), Connectionism, Rumelhart, J. L., McClelland, J. L., and PDP Research Group (1986), Arbib, M. A. (2007)). Or, it can form a functional network that serves a certain function and modeled accordingly (Honey, C. J., Kotter, R., Breakspear, R., & Sporns, O. (2007), Arbib, M. A. (2007)). A third way is to hypothesize a theory of the functioning of the biological components of the neural system by a mathematical model, in the form of a set of mathematical equations. The variables of the equation are some or all of the neurobiological properties of the entity being modeled, such as the dimensions of the dendrite or the stimulation rate of action potential along the axon in a neuron. The mathematical equations are solved using computational techniques and the results are validated with either simulation or experimental processes. This approach to modeling is called computational neuroscience. This methodology is used to model components from the ionic level to system level of the brain. This method is applicable for modeling integrated system of biological components that carry information signal from one neuron to another via intermediate active neurons that can pass the signal through or create new or additional signals. The computational neuroscience approach is extensively used and is based on two generic models, one of cell membrane potential Goldman (1943) and Hodgkin and Katz (1949), and the other based on Hodgkin-Huxley model of action potential (information signal).\n\nSterratt, D., Graham, B., Gillies, A., & Willshaw, D. (2011) classify the biological model of neuroscience into nine levels from ion channels to nervous system level based on size and function. Table 1 is based on this for neuronal networks.\n\nSporns, O. (2007) presents in his article on brain connectivity, modeling based on structural and functional types. A network that connects at neuron and synaptic level falls into the microscale. If the neurons are grouped into population of columns and minicolumns, the level is defined as mesoscale. The macroscale representation considers the network as regions of the brain connected by inter-regional pathways.\n\nArbib, M. A. (2007) considers in the modular model, a hierarchical formulation of the system into modules and submodules.\n\nThe neuronal signal comprises a stream of short electrical pulses of about 100 millivolt amplitude and about 1 to 2 millisecond duration (Gerstner, W., & Kistler, W. (2002) Chapter 1). The individual pulses are action potentials or spikes and the chain of pulses is called spike train. The action potential does not contain any information. A combination of the timing of the start of the spike train, the rate or frequency of the spikes, and the number and pattern of spikes in the spike train determine the coding of the information content or the signal message.\n\nThe neuron cell has three components – dendrites, soma, and axon as shown in Figure 1. Dendrites, which have the shape of a tree with branches, called arbor, receive the message from other neurons with which the neuron is connected via synapses. The action potential received by each dendrite from the synapse is called the postsynaptic potential. The cumulative sum of the postsynaptic potentials is fed to the soma. The ionic components of the fluid inside and outside maintain the cell membrane at a resting potential of about 65 millivolts. When the cumulative postsynaptic potential exceeds the resting potential, an action potential is generated by the cell body or soma and propagated along the axon. The axon may have one or more terminals and these terminals transmit neurotransmitters to the synapses with which the neuron is connected. Depending on the stimulus received by the dendrites, soma may generate one or more well-separated action potentials or spike train. If the stimulus drives the membrane to a positive potential, it is an excitatory neuron; and if it drives the resting potential further in the negative direction, it is an inhibitory neuron.\nThe generation of the action potential is called the “firing.” The firing neuron described above is called a spiking neuron. We will model the electrical circuit of the neuron in Section 3.6. There are two types of spiking neurons. If the stimulus remains above the threshold level and the output is a spike train, it is called the Integrate-and-Fire (IF) neuron model. If output is modeled as dependent on the impulse response of the circuit, then it is called the Spike Response Model (SRM) (Gestner, W. (1995)).\n\nThe spiking neuron model assumes that frequency (inverse of the rate at which spikes are generated) of spiking train starts at 0 and increases with the stimulus current. There is another hypothetical model that formulates the firing to happen at the threshold, but there is a quantum jump in frequency in contrast to smooth rise in frequency as in the spiking neuron model. This model is called the rate model. Gerstner, W., & Kistler, W. (2002), and Sterratt, D., Graham, B., Gillies, A., & Willshaw, D. (2011) are good sources for a detailed treatment of spiking neuron models and rate neuron models.\n\nThe concept of artificial neural network (ANN) was introduced by McColloch, W. S. & Pitts, W. (1943) for models based on behavior of biological neurons. Norbert Wiener (1961) gave this new field the popular name of cybernetics, whose principle is the interdisciplinary relationship among engineering, biology, control systems, brain functions, and computer science. With the computer science field advancing, the von Neumann-type computer was introduced early in the neuroscience study. But it was not suitable for symbolic processing, nondeterministic computations, dynamic executions, parallel distributed processing, and management of extensive knowledge bases, which are needed for biological neural network applications; and the direction of mind-like machine development changed to a learning machine. Computing technology has since advanced extensively and computational neuroscience is now able to handle mathematical models developed for biological neural network. Research and development are progressing in both artificial and biological neural networks including efforts to merge the two.\n\nThe “triune theory of the brain” McLean, P. (2003) is one of several models used to theorize the organizational structure of the brain. The most ancient neural structure of the brain is the brain stem or “lizard brain.” The second phase is limbic or paleo-mammalian brain and performs the four functions needed for animal survival – fighting, feeding, fleeing, and fucking. The third phase is the neocortex or the neo-mammalian brain. The higher cognitive functions which distinguish humans from other animals are primarily in the cortex. The reptilian brain controls muscles, balance, and autonomic functions, such as breathing and heartbeat. This part of the brain is active, even in deep sleep. The limbic system includes the hypothalamus, hippocampus, and amygdala. The neocortex includes the cortex and the cerebrum. It corresponds to the brain of primates and, specifically, the human species. Each of the three brains is connected by nerves to the other two, but each seems to operate as its own brain system with distinct capacities. (See illustration in Triune brain.)\n\nThe connectionist model evolved out of Parallel Distributed Processing framework that formulates a metatheory from which specific models can be generated for specific applications. PDP approach (Rumelhart, J. L., McClelland, J. L., and PDP Research Group (1986)) is a distributed parallel processing of many inter-related operations, somewhat similar to what’s happening in the human nervous system. The individual entities are defined as units and the units are connected to form a network. Thus, in the application to nervous system, one representation could be such that the units are the neurons and the links are the synapses.\n\nThere are three types of brain connectivity models of a network (Sporns, O. (2007)). “Anatomical (or structural) connectivity” describes a network with anatomical links having specified relationship between connected “units.” If the dependent properties are stochastic, it is defined as “functional connectivity.” “Effective connectivity” has causal interactions between distinct units in the system. As stated earlier, brain connectivity can be described at three levels. At microlevel, it connects neurons through electrical or chemical synapses. A column of neurons can be considered as a unit in the mesolevel and regions of the brain comprising a large number of neurons and neuron populations as units in the macrolevel. The links in the latter case are the inter-regional pathways, forming large-scale connectivity.\nFigure 2 shows the three types of connectivity. The analysis is done using the directed graphs (see Sporns, O. (2007) and Hilgetag, C. C. (2002)). In the structural brain connectivity type, the connectivity is a sparse and directed graph. The functional brain connectivity has bidirectional graphs. The effective brain connectivity is bidirectional with interactive cause and effect relationships. Another representation of the connectivity is by matrix representation (See Sporns, O. (2007)). Hilgetag, C. C. (2002) describes the computational analysis of brain connectivity.\n\nArbib, M. A. (2007) describes the modular models as follows. “Modular models of the brain aid the understanding of a complex system by decomposing it into structural modules (e.g., brain regions, layers, columns) or functional modules (schemas) and exploring the patterns of competition and cooperation that yield the overall function.” This definition is not the same as that defined in functional connectivity. The modular approach is intended to build cognitive models and is, in complexity, between the anatomically defined brain regions (defined as macrolevel in brain connectivity) and the computational model at the neuron level.\n\nThere are three views of modules for modeling. They are (1) modules for brain structures, (2) modules as schemas, and (3) modules as interfaces. Figure 3 presents the modular design of a model for reflex control of saccades (Arbib, M. A. (2007)). It involves two main modules, one for superior colliculus (SC), and one for brainstem. Each of these is decomposed into submodules, with each submodule defining an array of physiologically defined neurons. In Figure 3(b) the model of Figure 3(a) is embedded into a far larger model which embraces various regions of cerebral cortex (represented by the modules Pre-LIP Vis, Ctx., LIP, PFC, and FEF), thalamus, and basal ganglia. While the model may indeed be analyzed at this top level of modular decomposition, we need to further decompose basal ganglia, BG, as shown in Figure 3(c) if we are to tease apart the role of dopamine in differentially modulating (the 2 arrows shown arising from SNc) the direct and indirect pathways within the basal ganglia (Crowley, M. (1997)).\nNeural Simulation Language (NSL) has been developed to provide a simulation system for large-scale general neural networks. It provides an environment to develop an object-oriented approach to brain modeling. NSL supports neural models having as basic data structure neural layers with similar properties and similar connection patterns. Models developed using NSL are documented in Brain Operation Database (BODB) as hierarchically organized modules that can be decomposed into lower levels.\n\nAs mentioned in Section 2.4, development of artificial neural network (ANN), or neural network as it is now called, started as simulation of biological neuron network and ended up using artificial neurons. Major development work has gone into industrial applications with learning process. Complex problems were addressed by simplifying the assumptions. Algorithms were developed to achieve a neurological related performance, such as learning from experience. Since the background and overview have been covered in the other internal references, the discussion here is limited to the types of models. The models are at the system or network level.\n\nThe four main features of an ANN are topology, data flow, types of input values, and forms of activation (Meireles, M. R. G. (2003), Munakata, T. (1998)). Topology can be multilayered, single-layered, or recurrent. Data flow can be recurrent with feedback or non-recurrent with feedforward model. The inputs are binary, bipolar, or continuous. The activation is linear, step, or sigmoid. Multilayer Perceptron (MLP) is the most popular of all the types, which is generally trained with back-propagation of error algorithm. Each neuron output is connected to every neuron in subsequent layers connected in cascade and with no connections between neurons in the same layer. Figure 4 shows a basic MLP topology (Meireles, M. R. G. (2003)), and a basic telecommunication network (Subramanian, M. (2010)) that most are familiar with. We can equate the routers at the nodes in telecommunication network to neurons in MLP technology and the links to synapses.\n\nComputational science is an interdisciplinary field that combines engineering, biology, control systems, brain functions, physical sciences, and computer science. It has fundamental development models done at the lower levels of ions, neurons, and synapses, as well as information propagation between neurons. These models have established the enabling technology for higher-level models to be developed. They are based on chemical and electrical activities in the neurons for which electrical equivalent circuits are generated. A simple model for the neuron with predominantly potassium ions inside the cell and sodium ions outside establishes an electric potential on the membrane under equilibrium, i.e., no external activity, condition. This is called the resting membrane potential, which can be determined by Nernst Equation (Nernst, W. (1888)). An equivalent electrical circuit for a patch of membrane, for example an axon or dendrite, is shown in Figure 5. E and E are the potentials associated with the potassium and sodium channels respectively and R and R are the resistances associated with them. C is the capacitance of the membrane and I is the source current, which could be the test source or the signal source (action potential). The resting potential for potassium-sodium channels in a neuron is about -65 millivolts.\nThe membrane model is for a small section of the cell membrane; for larger sections it can be extended by adding similar sections, called compartments, with the parameter values being the same or different. The compartments are cascaded by a resistance, called axial resistance. Figure 6 shows a compartmental model of a neuron that is developed over the membrane model. Dendrites are the postsynaptic receptors receiving inputs from other neurons; and the axon with one or more axon terminals transmits neurotransmitters to other neurons.\nThe second building block is the Hodgkin-Huxley (HH) model of the action potential. When the membrane potential from the dendrites exceeds the resting membrane potential, a pulse is generated by the neuron cell and propagated along the axon. This pulse is called the action potential and HH model is a set of equations that is made to fit the experimental data by the design of the model and the choice of the parameter values.\n\nModels for more complex neurons containing other types of ions can be derived by adding to the equivalent circuit additional battery and resistance pairs for each ionic channel. The ionic channel could be passive or active as they could be gated by voltage or be ligands. The extended HH model has been developed to handle the active channel situation.\n\nAlthough there are neurons that are physiologically connected to each other, information is transmitted at most of the synapses by chemical process across a cleft. Synapses are also computationally modeled. The next level of complexity is that of stream of action potentials, which are generated, whose pattern contains the coding information of the signal being transmitted. There are basically two types of action potentials, or spikes as they are called, that are generated. One is “integrate-and-fire” (the one we have so far addressed) and the other which is rate based. The latter is a stream whose rate varies. The signal going across the synapses could be modeled either as a deterministic or a stochastic process based on the application (See Section 3.7). Another anatomical complication is when a population of neurons, such as a column of neurons in visionary system, needs to be handled. This is done by considering the collective behavior of the group (Kotter, R., Nielson, P., Dyhrfjeld-Johnson, J., Sommer, F. T., & Northoff, G. (2002)).\n\nThe action potential or the spike does not itself carry any information. It is the stream of spikes, called spike train, that carry the information in its number and pattern of spikes and timing of spikes. The postsynaptic potential can be either positive, the excitatory synapse or negative, inhibitory synapse. In modeling, the postsynaptic potentials received by the dendrites in the postsynaptic neuron are integrated and when the integrated potential exceeds the resting potential, the neuron fires an action potential along its axon. This model is the Integrate-and-Fire (IF) model that was mentioned in Section 2.3. Closely related to IF model is a model called Spike Response Model (SRM) (Gerstner, W. (1995) Pages 738-758) that is dependent on impulse function response convoluted with the input stimulus signal. This forms a base for a large number of models developed for spiking neural networks.\n\nThe IF and SR model of spike train occurs in Type I neurons, in which the spike rate or spike frequency of the occurrence increases smoothly with the increase in stimulus current starting from zero. Another phenomenon of spike train generation happens in Type II neurons, where firing occurs at the resting potential threshold, but with a quantum jump to a non-zero frequency. Models have been developed using the rate (frequency) of the spike train and are called rate-based models.\n\nWhat is important for understanding the functions of the nervous system is how the message is coded and transported by the action potential in the neuron. There are two theories on how the signal that is being propagated is coded in the spikes as to whether it is pulse code or rate code. In the former, it is the time delay of the first spike from the time of stimulus as seen by the postsynaptic receiver that determines the coding. In the rate code, it is average rate of the spike that influences the coding. It is not certain as to which is really the actual physiological phenomenon in each case. However, both cases can be modeled computationally and the parameters varied to match the experimental result. The pulse mode is more complex to model and numerous detailed neuron models and population models are described by Gerstner and Kistler in Parts I and II of Gerstner, W., & Kistler, W. (2002) and Chapter 8 of Sterratt, D., Graham, B., Gillies, A., & Willshaw, D. (2011). Another important characteristic associated with SR model is the spike-time-dependent-plasticity. It is based on Hebb’s postulate on plasticity of synapse, which states that “the neurons that fire together wire together.” This causes the synapse to be a long-term potentiation (LTP) or long-term depression (LTD). The former is the strengthening of the synapse between two neurons if the postsynaptic spike temporally follows immediately after the presynaptic spike. Latter is the case if it is reverse, i.e., the presynaptic spike occurs after the postsynaptic spike. Gerstner, W. & Kistler, W. (2002) in Chapter 10 and Sterratt, D., Graham, B., Gillies, A., & Willshaw, D. (2011) in Chapter 7 discuss the various models related to Hebbian models on plasticity and coding.\n\nThe challenge involved in developing models for small, medium, and large networks is one of reducing the complexity by making valid simplifying assumptions in and extending the Hodgkin-Huxley neuronal model appropriately to design those models ( see Chapter 9 of Sterratt, D., Graham, B., Gillies, A., & Willshaw, D. (2011), Kotter, R., Nielson, P., Dyhrfjeld-Johnson, J., Sommer, F. T., & Northoff, G. (2002), and Chapter 9 of Gerstner, W., & Kistler, W. (2002)). Network models can be classified as either network of neurons propagating through different levels of cortex or neuron populations interconnected as multilevel neurons. The spatial positioning of neuron could be 1-, 2- or 3-dimensional; the latter ones are called small-world networks as they are related to local region. The neuron could be either excitatory or inhibitory, but not both. Modeling design depends on whether it is artificial neuron or biological neuron of neuronal model. Type I or Type II choice needs to be made for the firing mode. Signaling in neurons could be rate-based neurons, spiking response neurons, or deep-brain stimulated. The network can be designed as feedforward or recurrent type. The network needs to be scaled for the computational resource capabilities. Large-scale thalamocortical systems are handled in the manner of the Blue Brain project (Markam, H. (2006)).\n\nNo generalized modeling concepts exist for modeling the development of anatomical physiology and morphology similar to the one of behavior of neuronal network, which is based on HH model. Shankle, W. R., Hara, J., Fallon, J. H., and Landing, B. H. (2002) describe the application of neuroanatomical data of the developing human cerebral cortex to computational models. Sterratt, D., Graham, B., Gillies, A., & Willshaw, D. (2011) discuss aspects of the nervous system of computational modeling in the development of nerve cell morphology, cell physiology, cell patterning, patterns of ocular dominance, and connection between nerve cell and muscle, and retinotopic maps. Carreira-Perpinan, M. A. & Goodhill, G. J. (2002) deal with the optimization of the computerized models of the visual cortex.\n\nWith the enormous number of models that have been created, tools have been developed for dissemination of the information, as well as platforms to develop models. Several generalized tools, such as GENESIS, NEURON, XPP, and NEOSIM are available and are discussed by Hucka, M. (2002).\n\n",
    "id": "33818014",
    "title": "Nervous system network models"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=34053961",
    "text": "Manifold alignment\n\nManifold alignment is a class of machine learning algorithms that produce projections between sets of data, given that the original data sets lie on a common manifold. The concept was first introduced as such by Ham, Lee, and Saul in 2003, adding a manifold constraint to the general problem of correlating sets of high-dimensional vectors.\n\nManifold alignment assumes that disparate data sets produced by similar generating processes will share a similar underlying manifold representation. By learning projections from each original space to the shared manifold, correspondences are recovered and knowledge from one domain can be transferred to another. Most manifold alignment techniques consider only two data sets, but the concept extends to arbitrarily many initial data sets.\n\nConsider the case of aligning two data sets, formula_1 and formula_2, with formula_3 and formula_4.\n\nManifold alignment algorithms attempt to project both formula_1 and formula_2 into a new \"d\"-dimensional space such that the projections both minimize distance between corresponding points and preserve the local manifold structure of the original data. The projection functions are denoted:\n\nformula_7\n\nformula_8\n\nLet formula_9 represent the binary correspondence matrix between points in formula_1 and formula_2:\n\nformula_12\n\nLet formula_13 and formula_14 represent pointwise similarities within data sets. This is usually encoded as the heat kernel of the adjacency matrix of a \"k\"-nearest neighbor graph.\n\nFinally, introduce a coefficient formula_15, which can be tuned to adjust the weight of the 'preserve manifold structure' goal, versus the 'minimize corresponding point distances' goal.\n\nWith these definitions in place, the loss function for manifold alignment can be written:\n\nformula_16\n\nSolving this optimization problem is equivalent to solving a generalized eigenvalue problem using the graph laplacian of the joint matrix, \"G\":\n\nformula_17\n\nThe algorithm described above requires full pairwise correspondence information between input data sets; a supervised learning paradigm. However, this information is usually difficult or impossible to obtain in real world applications. Recent work has extended the core manifold alignment algorithm to semi-supervised\n\n, unsupervised\n\n, and multiple-instance\n\nsettings.\n\nThe algorithm described above performs a \"one-step\" alignment, finding embeddings for both data sets at the same time. A similar effect can also be achieved with \"two-step\" alignments\n, following a slightly modified procedure:\n\nManifold alignment can be used to find linear (feature-level) projections, or nonlinear (instance-level) embeddings. While the instance-level version generally produces more accurate alignments, it sacrifices a great degree of flexibility as the learned embedding is often difficult to parameterize. Feature-level projections allow any new instances to be easily embedded in the manifold space, and projections may be combined to form direct mappings between the original data representations. These properties are especially important for knowledge-transfer applications.\n\nManifold alignment is suited to problems with several corpora that lie on a shared manifold, even when each corpus is of a different dimensionality. Many real-world problems fit this description, but traditional techniques are not able to take advantage of all corpora at the same time. Manifold alignment also facilitates transfer learning, in which knowledge of one domain is used to jump-start learning in correlated domains.\n\nApplications of manifold alignment include:\n\n",
    "id": "34053961",
    "title": "Manifold alignment"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=34413019",
    "text": "Document mosaicing\n\nDocument mosaicing is a process that stitches multiple, overlapping snapshot images of a document together in order to produce one large, high resolution composite. The document is slid under a stationary, over-the-desk camera by hand until all parts of the document are snapshotted by the camera’s field of view. As the document slid under the camera, all motion of the document is coarsely tracked by the vision system. The document is periodically snapshotted such that the successive snapshots are overlap by about 50%. The system then finds the overlapped pairs and stitches them together repeatedly until all pairs are stitched together as one piece of document.\n\nThe document mosaicing can be divided into four main processes.\n\nIn this process, the motion of the document slid under the camera is coarsely tracked by the system. Tracking is performed by a process called simple correlation process. In the first frame of snapshots, a small patch is extracted from the center of the image \nas a correlation template as shown in Figure 1. The correlation process is performed in the four times size of the patch area of the next frame. The motion of the paper is indicated by the peak in the correlation function. The peak in the correlation function indicates the motion of the paper. The template is resampled from this frame and the tracking continues until the template reaches the edge of the document. After the template reaches the edge of the document, another snapshot is taken and the tracking process performs repeatedly until the whole document is imaged. The snapshots are stored in an ordered list in order to pair the overlapped images in later processes.s\n\nFeature detection is the process of finding the transformation which aligns one image with another. There are two main approaches for feature detection.\n\nEach image is segmented into a hierarchy of columns, lines and words in order to match with the organised sets of features across images. Skew angle estimation and columns, lines and words finding are the examples of feature detection operations.\n\nFirstly, the angle that the rows of text make with the image raster lines (skew angle) is estimated. It is assumed to lie in the range of ±20°. A small patch of text in the image is selected randomly and then rotated in the range of ±20° until the variance of the pixel intensities of the patch summed along the raster lines is maximised. See Figure 2.\n\nIn order to ensure that the found skew angle is accurate, the document mosaic system performs calculation at many image patches and derive the final estimation by finding the average of the individual angles weighted by the variance of the pixel intensities of each patch.\n\nIn this operation, the de-skewed document is intuitively segmented into a hierarchy of columns, lines and words. The sensitivity to illumination and page coloration of the de-skewed document can be removed by applying a Sobel operator to the de-skewed image and thresholding the output to obtain the binary gradient, de-skewed image. See Figure 3.\n\nThe operation can be roughly separated into 3 steps : column segmentation, line segmentation and word segmentation.\n\n\nThese segmentations are important because the document mosaic will be created by matching the lower right corners of words in overlapping images pair. Moreover, the segmentation operation can organize the list of images in the context of a hierarchy of rows and column reliably.\n\nThe segmentation operation involves a considerable amount of summing in the binary gradient, de-skewed images, which done by construct a matrix of partial sums whose elements are given by\n\nformula_1\nThe matrix of partial sums is calculated in one pass through the binary gradient, de-skewed image.\n\nformula_2\n\nThe two images are now organized in hierarchy of linked lists in following structure :\n\nAt the bottom of the structure, the length of each word is recorded for establishing correspondence between two images in order to reduce to search only the corresponding structures for the groups of words with the matching lengths.\n\nA seed match finding is done by comparing each row in image1 with each row in image2. The two rows are then compared to each other by every word. If the length (in pixel) of the two words (one from image1 and one from image2) and their immediate neighbours agree with each other within a predefined tolerance threshold (5 pixels, for example), then they are assumed to be matched. The row of each image is assumed to be matched if there are three or more word matches between the two rows. The seed match finding operation is terminated when two pairs of consecutive row match are found.\n\nAfter finishing a seed match finding operation, the next process is to build the match list to generate the correspondences points of the two images. The process is done by searching the matching pairs of rows away from the seed row.\n\nGiven the list of corresponding points of the two images, finding the transformation of the overlapping portion of the images is next process to be performed. Assuming a pinhole camera model, the transformation between pixels (u,v) of image 1 and pixels (u0, v0) of image 2 is demonstrated by a plane-to-plane projectivity.\n\nformula_3\n\nThe parameters of the projectivity is found from four pairs of matching points. RANSAC regression technique is used to reject outlying matches and estimate the projectivity from the remaining good matches.\n\nThe projectivity is fine-tuned using correlation at the corners of the overlapping portion in order to obtain four correspondences to sub-pixel accuracy. Therefore, image1 is then transformed into image2’s coordinate system using Eq.1. The typical result of the process is shown in Figure 5.\n\nFinally, the whole page composition is built up by mapping all the images into the coordinate system of an “anchor” image, which is normally the one nearest the page center. The transformations to the anchor frame are calculated by concatenating the pair-wise transformations which found earlier. The raw document mosaic is shown in Figure 6.\n\nHowever, there might be a problem of non-consecutive images that are overlap. This problem can be solved by performing Hierarchical sub-mosaics. As shown in Figure 7, image1 and image2 are registered, as are image3 and image4, creating two sub-mosaics. These two sub-mosaics are later stitched together in another mosaicing process.\n\nThere are various areas that the technique of document mosaicing can be applied to such as : \n\n\n",
    "id": "34413019",
    "title": "Document mosaicing"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=34130293",
    "text": "Thompson sampling\n\nIn artificial intelligence, Thompson sampling, named after William R. Thompson, is a heuristic for choosing actions that addresses the exploration-exploitation dilemma in the multi-armed bandit problem. It consists in choosing the action that maximizes the expected reward with respect to a randomly drawn belief.\n\nConsider a set of contexts formula_1, a set of actions formula_2, and rewards in formula_3. In each round, the player obtains a context formula_4, plays an action formula_5 and receives a reward formula_6 following a distribution that depends on the context and the issued action. The aim of the player is to play actions such as to maximize the cumulative rewards.\n\nThe elements of Thompson sampling are as follows:\n\nThompson sampling consists in playing the action formula_15 according to the probability that it maximizes the expected reward, i.e. \nwhere formula_17 is the indicator function.\n\nIn practice, the rule is implemented by sampling, in each round, a parameter formula_18 from the posterior formula_19, and choosing the action formula_20 that maximizes formula_21, i.e. the expected reward given the parameter, the action and the current context. Conceptually, this means that the player instantiates his or her beliefs randomly in each round, and then acts optimally according to them.\n\nThompson sampling was originally described in an article by Thompson from 1933 but has been largely ignored by the artificial intelligence community. It was subsequently rediscovered numerous times independently in the context of reinforcement learning. A first proof of convergence for the bandit case has been shown in 1997. The first application to Markov decision processes was in 2000. A related approach (see Bayesian control rule) was published in 2010. In 2010 it was also shown that Thompson sampling is \"instantaneously self-correcting\". Asymptotic convergence results for contextual bandits were published in 2011. Nowadays, Thompson Sampling has been widely used in many online learning problems: Thompson sampling has also been applied to A/B testing in website design and online advertising; Thompson sampling has formed the basis for accelerated learning in decentralized decision making; a Double Thompson Sampling (D-TS) algorithm has been proposed for dueling bandits, a variant of traditional MAB, where feedbacks come in the format of pairwise comparison.\n\nProbability matching is a decision strategy in which predictions of class membership are proportional to the class base rates. Thus, if in the training set positive examples are observed 60% of the time, and negative examples are observed 40% of the time, the observer using a probability-matching strategy will predict (for unlabeled examples) a class label of \"positive\" on 60% of instances, and a class label of \"negative\" on 40% of instances.\n\nA generalization of Thompson sampling to arbitrary dynamical environments and causal structures, known as Bayesian control rule, has been shown to be the optimal solution to the adaptive coding problem with actions and observations. In this formulation, an agent is conceptualized as a mixture over a set of behaviours. As the agent interacts with its environment, it learns the causal properties and adopts the behaviour that minimizes the relative entropy to the behaviour with the best prediction of the environment's behaviour. If these behaviours have been chosen according to the maximum expected utility principle, then the asymptotic behaviour of the Bayesian control rule matches the asymptotic behaviour of the perfectly rational agent.\n\nThe setup is as follows. Let formula_22 be the actions issued by an agent up to time formula_23, and let formula_24 be the observations gathered by the agent up to time formula_23. Then, the agent issues the action formula_26 with probability:\nwhere the \"hat\"-notation formula_28 denotes the fact that formula_29 is a causal intervention (see Causality), and not an ordinary observation. If the agent holds beliefs formula_30 over its behaviors, then the Bayesian control rule becomes\nwhere formula_32 is the posterior distribution over the parameter formula_9 given actions formula_34 and observations formula_35.\n\nIn practice, the Bayesian control amounts to sampling, in each time step, a parameter formula_18 from the posterior distribution formula_32, where the posterior distribution is computed using Bayes' rule by only considering the (causal) likelihoods of the observations formula_24 and ignoring the (causal) likelihoods of the actions formula_22, and then by sampling the action formula_40 from the action distribution formula_41.\n",
    "id": "34130293",
    "title": "Thompson sampling"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=19614447",
    "text": "Radiant AI\n\nThe Radiant AI is a technology developed by Bethesda Softworks for \"The Elder Scrolls\" video games. It allows non-player characters (NPCs) to make choices and engage in behaviors more complex than in past titles. The technology was developed for \"\" and expanded in \"\"; it is also used in \"Fallout 3\", \"\" and \"Fallout 4\", also published by Bethesda.\n\nThe Radiant A.I. technology, as it evolved in its latest iteration developed for \"Skyrim\", comprises two parts:\n\nThe Radiant A.I. system deals with NPC interactions and behavior. It allows non-player characters to dynamically react to and interact with the world around them. General goals, such as \"Eat in this location at 2pm\" are given to NPCs, and NPCs are left to determine how to achieve them. The absence of individual scripting for each character allows for the construction of a world on a much larger scale than other games had developed, and aids in the creation of what Todd Howard described as an \"organic feel\" for the game.\n\nThe Radiant Story system deals with how the game itself reacts to the player behavior, such as the creation of new dynamic quests. Dynamically generated quests are placed by the game in locations the player hasn't visited yet and are related to earlier adventures.\n",
    "id": "19614447",
    "title": "Radiant AI"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=3709680",
    "text": "Pattern theory\n\nPattern theory, formulated by Ulf Grenander, is a mathematical formalism to describe knowledge of the world as patterns. It differs from other approaches to artificial intelligence in that it does not begin by prescribing algorithms and machinery to recognize and classify patterns; rather, it prescribes a vocabulary to articulate and recast the pattern concepts in precise language.\n\nIn addition to the new algebraic vocabulary, its statistical approach is novel in its aim to:\nBroad in its mathematical coverage, Pattern Theory spans algebra and statistics, as well as local topological and global entropic properties.\n\nThe Brown University Pattern Theory Group was formed in 1972 by Ulf Grenander. Many mathematicians are currently working in this group, noteworthy among them being the Fields Medalist David Mumford. Mumford regards Grenander as his \"guru\" in this subject.\n\nWe begin with an example to motivate the algebraic definitions that follow.\n\nIf we want to represent language patterns, the most immediate candidate for primitives might be words. However, set phrases, such as “in order to”, immediately indicate the inappropriateness of words as atoms. In searching for other primitives, we might try the rules of grammar. We can represent grammars as finite state automata or context-free grammars. Below is a sample finite state grammar automaton.\n\nThe following phrases are generated from a few simple rules of the automaton and programming code in pattern theory:\nTo create such sentences, rewriting rules in finite state automata act as generators to create the sentences as follows: if a machine starts in state 1, it goes to state 2 and writes the word “the”. From state 2, it writes one of 4 words: prince, boy, princess, girl, chosen at random. The probability of choosing any given word is given by the Markov chain corresponding to the automaton. Such a simplistic automaton occasionally generates more awkward sentences\n\nFrom the finite state diagram we can infer the following generators (shown at right) that creates the signal. A generator is a 4-tuple: current state, next state, word written, probability of written word when there are multiple choices. That is, each generator is a state transition arrow of state diagram for a Markov chain.\n\nImagine that a configuration of generators are strung together linearly so its output forms a sentence, so each generator \"bonds\" to the generators before and after it. Denote these bonds as 1a,1b,2a,2b,…12a,12b. Each numerical label corresponds to the automaton's state and each letter \"a\" and \"b\" corresponds to the inbound and outbound bonds. Then the following bond table (left) is equivalent to the automaton diagram. For the sake of simplicity, only half of the bond table is shown—the table is actually symmetric.\n\nAs one can tell from this example, and typical of signals that are studied, identifying the primitives and bond tables requires some thought. The example highlights another important fact not readily apparent in other signals problems: that a configuration is not the signal that is observed; rather, its image as a sentence is observed. Herein lies a significant justification for distinguishing an observable from a non-observable construct. Additionally, it provides an algebraic structure to associate with hidden Markov models. In sensory examples such as the vision example below, the hidden configurations and observed images are much more similar, and such a distinction may not seem justified. Fortunately, the grammar example reminds us of this distinction.\n\nA more sophisticated example can be found in the link grammar theory of natural language.\n\nMotivated by the example, we have the following definitions:\n\nPattern Theory defines order in terms of the feature of interest given by \"p\"(\"c\").\n\nGrenander’s Pattern Theory treatment of Bayesian inference in seems to be skewed towards on image reconstruction (e.g. content addressable memory). That is given image I-deformed, find I. However, Mumford’s interpretation of Pattern Theory is broader and he defines PT to include many more well-known statistical methods. Mumford’s criteria for inclusion of a topic as Pattern Theory are those methods \"characterized by common techniques and motivations\", such as the HMM, EM algorithm, dynamic programming circle of ideas. Topics in this section will reflect Mumford's treatment of Pattern Theory. His principle of statistical Pattern Theory are the following:\n\nStatistical PT makes ubiquitous use of conditional probability in the form of Bayes theorem and Markov Models. Both these concepts are used to express the relation between hidden states (configurations) and observed states (images). Markov Models also captures the local properties of the stimulus, reminiscent of the purpose of bond table for regularity.\n\nThe generic set up is the following:\nLet \"s\" = the hidden state of the data that we wish to know. \"i\" = observed image. Bayes theorem gives\n\nThe following conditional probability examples illustrates these methods in action:\n\nN-gram Text Strings: See Mumford's Pattern Theory by Examples, Chapter 1.\n\nMAP ~ MDL (MDL offers a glimpse of why the MAP probabilistic formulation make sense analytically)\n\nSupposing we want to translate French sentences to English. Here, the hidden configurations are English sentences and the observed signal they generate are French sentences. Bayes theorem gives \"p\"(\"e\"|\"f\")\"p\"(\"f\") = \"p\"(\"e\", \"f\") = \"p\"(\"f\"|\"e\")\"p\"(\"e\") and reduces to the fundamental equation of machine translation: maximize \"p\"(\"e\"|\"f\") = \"p\"(\"f\"|\"e\")\"p\"(\"e\") over the appropriate \"e\" (note that \"p\"(\"f\") is independent of \"e\", and so drops out when we maximize over \"e\"). This reduces the problem to three main calculations for:\nThe analysis seems to be symmetric with respect to the two languages, and if we think can calculate \"p\"(\"f\"|\"e\"), why not turn the analysis around and calculate \"p\"(\"e\"|\"f\") directly? The reason is that during the calculation of \"p\"(\"f\"|\"e\") the asymmetric assumption is made that source sentence be well formed and we cannot make any such assumption about the target translation because we do not know what it will translate into.\n\nWe now focus on \"p\"(\"f\"|\"e\") in the three-part decomposition above. The other two parts, \"p\"(\"e\") and maximizing \"e\", uses similar techniques as the \"N\"-gram model. Given a French-English translation from a large training data set (such data sets exists from the Canadian parliament),\nthe sentence pair can be encoded as an \"alignment\" (2, 3, 4, 5, 6, 6, 6) that reads as follows: the first word in French comes from the second English word, the second word in French comes from the 3rd English word, and so forth. Although an alignment is a straightforward encoding of the translation, a more computationally convenient approach to an alignment is to break it down into four parameters:\n\nFor the sake of simplicity in demonstrating an EM algorithm, we shall go through a simple calculation involving only translation probabilities \"t\"(), but needless to say that it the method applies to all parameters in their full glory. Consider the simplified case (1) without the NULL word (2) where every word has fertility 1 and (3) there are no distortion probabilities. Our training data corpus will contain two-sentence pairs: \"bc\" → \"xy\" and \"b\" → \"y\". The translation of a two-word English sentence “b c” into the French sentence “\"x y\"” has two possible alignments, and including the one-sentence words, the alignments are:\ncalled Parallel, Crossed, and Singleton respectively.\n\nTo illustrate an EM algorithm, first set the desired parameter uniformly, that is\n\nThen EM iterates as follows\nThe alignment probability for the “crossing alignment” (where \"b\" connects to \"y\") got a boost from the second sentence pair \"b\"/\"y\". That further solidified \"t\"(\"y\" | \"b\"), but as a side effect also boosted \"t\"(\"x\" | \"c\"), because \"x\" connects to \"c\" in that same “crossing alignment.” The effect of boosting \"t\"(\"x\" | \"c\") necessarily means downgrading \"t\"(\"y\" | \"c\") because they sum to one. So, even though \"y\" and \"c\" co-occur, analysis reveals that they are not translations of each other. With real data, EM also is subject to the usual local extremum traps.\n\nFor decades, speech recognition seemed to hit an impasse as scientists sought descriptive and analytic solution. The sound wave p(t) below is produced by speaking the word “ski”.\n\nIts four distinct segments has very different characteristics. One can choose from many levels of generators (hidden variables): the intention of the speaker’s brain, the state of the mouth and vocal cords, or the ‘phones’ themselves. Phones are the generator of choice to be inferred and it encodes the word in a noisy, highly variable way. Early work on speech recognition attempted to make this inference deterministically using logical rules based on binary features extracted from p(t). For instance, the table below shows some of the features used to distinguish English consonants.\n\nIn real situations, the signal is further complicated by background noises such as cars driving by or artifacts such as a cough in mid sentence (Mumford’s 2nd underpinning). The deterministic rule-based approach failed and the state of the art (e.g. Dragon Naturally Speaking) is to use a family of precisely tuned HMMs and Bayesian MAP estimators to do better. Similar stories played out in vision, and other stimulus categories.\n\n\n\n\n",
    "id": "3709680",
    "title": "Pattern theory"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=34591537",
    "text": "Automated personal assistant\n\nAn automated personal assistant or an Intelligent Personal Assistant is a mobile software agent that can perform tasks, or services, on behalf of an individual based on a combination of user input, location awareness, and the ability to access information from a variety of online sources (such as weather conditions, traffic congestion, news, stock prices, user schedules, retail prices, etc.).\n\nThere are two types of automated personal assistants: intelligent automated assistants (for example, Apple’s Siri and Tronton’s Cluzee), which perform concierge-type tasks (e.g., making dinner reservations, purchasing event tickets, making travel arrangements) or provide information based on voice input or commands; and smart personal agents, which automatically perform management or data-handling tasks based on online information and events often without user initiation or interaction.\n\nAccording to Chi-Hua Chien of Kleiner Perkins Caufield & Byers, examples of tasks that may be performed by a smart personal agent-type of automated personal assistant include schedule management (e.g., sending an alert to a dinner date that a user is running late due to traffic conditions, update schedules for both parties, and change the restaurant reservation time) and personal health management (e.g., monitoring caloric intake, heart rate and exercise regimen, then making recommendations for healthy choices).\n\nBoth types of automated personal assistant technology are enabled by the combination of mobile computing devices, application programming interfaces (APIs), and the proliferation of mobile apps. However, intelligent automated assistants are designed to perform specific, one-off tasks specified by user voice instructions, while smart personal agents perform ongoing tasks (e.g., schedule management) autonomously.\n\n",
    "id": "34591537",
    "title": "Automated personal assistant"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=34641430",
    "text": "Contextual image classification\n\nContextual image classification, a topic of pattern recognition in computer vision, is an approach of classification based on contextual information in images. \"Contextual\" means this approach is focusing on the relationship of the nearby pixels, which is also called neighbourhood. The goal of this approach is to classify the images by using the contextual information.\n\nSimilar as processing language, a single word may have multiple meanings unless the context is provided, and the patterns within the sentences are the only informative segments we care about. For images, the principle is same. Find out the patterns and associate proper meanings to them.\n\nAs the image illustrated below, if only a small portion of the image is shown, it is very difficult to tell what the image is about.\nEven try another portion of the image, it is still difficult to classify the image.\nHowever, if we increase the contextual of the image, then it makes more sense to recognize.\nAs the full images shows below, almost everyone can classify it easily.\nDuring the procedure of segmentation, the methods which do not use the contextual information are sensitive to noise and variations, thus the result of segmentation will contain a great deal of misclassified regions, and often these regions are small (e.g., one pixel).\n\nCompared to other techniques, this approach is robust to noise and substantial variations for it takes the continuity of the segments into account.\n\nSeveral methods of this approach will be described below.\n\nThis approach is very effective against small regions caused by noise. And these small regions are usually formed by few pixels or one pixel. The most probable label is assigned to these regions.\nHowever, there is a drawback of this method. The small regions also can be formed by correct regions rather than noise, and in this case the method is actually making the classification worse.\nThis approach is widely used in remote sensing applications.\n\nThis is a two-stage classification process:\n\nInstead of using single pixels, the neighbour pixels can be merged into homogeneous regions benefiting from contextual information. And provide these regions to classifier.\n\nThe original spectral data can be enriched by adding the contextual information carried by the neighbour pixels, or even replaced in some occasions. This kind of pre-processing methods are widely used in textured image recognition. The typical approaches include mean values, variances, texture description, etc.\n\nThe classifier uses the grey level and pixel neighbourhood (contextual information) to assign labels to pixels. In such case the information is a combination of spectral and spatial information.\n\nContextual classification of image data is based on the Bayes minimum error classifier (also known as a naive Bayes classifier).\n\nPresent the pixel:\n\n\n\nThe neighbourhood:\nSize of the neighbourhood. There is no limitation of the size, but it is considered to be relatively small for each pixel formula_1.\nA reasonable size of neighbourhood would be formula_15 of 4-connectivity or 8-connectivity (formula_1 is marked as red and placed in the centre).\nThe calculation:\n\nApply the minimum error classification on a pixel formula_1, if the probability of a class formula_18 being presenting the pixel formula_1 is the highest among all, then assign formula_18 as its class.\n\nThe contextual classification rule is described as below, it uses the feature vector formula_22 rather than formula_1.\n\nUse the Bayes formula to calculate the posteriori probability formula_25\n\nThe number of vectors is the same as the number of pixels in the image. For the classifier uses a vector corresponding to each pixel formula_27, and the vector is generated from the pixel's neighbourhood.\n\nThe basic steps of contextual image classification:\n\nThe template matching is a \"brute force\" implementation of this approach. The concept is first create a set of templates, and then look for small parts in the image match with a template.\n\nThis method is computationally high and inefficient. It keeps an entire templates list during the whole process and the number of combinations is extremely high. For a formula_33 pixel image, there could be a maximum of formula_34 combinations, which leads to high computation. This method is a top down method and often called table look-up or dictionary look-up.\n\nThe Markov chain also can be applied in pattern recognition. The pixels in an image can be recognised as a set of random variables, then use the lower order Markov chain to find the relationship among the pixels. The image is treated as a virtual line, and the method uses conditional probability.\n\nThe Hilbert curve runs in a unique pattern through the whole image, it traverses every pixel without visiting any of them twice and keeps a continuous curve. It is fast and efficient.\n\nThe lower-order Markov chain and Hilbert space-filling curves mentioned above are treating the image as a line structure. The Markov meshes however will take the two dimensional information into account.\n\nThe dependency tree is a method using tree dependency to approximate probability distributions.\n\n",
    "id": "34641430",
    "title": "Contextual image classification"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=34668189",
    "text": "3D reconstruction from multiple images\n\n3D reconstruction from multiple images is the creation of three-dimensional models from a set of images. It is the reverse process of obtaining 2D images from 3D scenes.\n\nThe essence of an image is a projection from a 3D scene onto a 2D plane, during which process the depth is lost. The 3D point corresponding to a specific image point is constrained to be on the line of sight. From a single image, it is impossible to determine which point on this line corresponds to the image point. If two images are available, then the position of a 3D point can be found as the intersection of the two projection rays. This process is referred to as triangulation. The key for this process is the relations between multiple views which convey the information that corresponding sets of points must contain some structure and that this structure is related to the poses and the calibration of the camera.\n\nIn recent decades, there is an important demand for 3D content for computer graphics, virtual reality and communication, triggering a change in emphasis for the requirements. Many existing systems for constructing 3D models are built around specialized hardware (e.g. stereo rigs) resulting in a high cost, which cannot satisfy the requirement of its new applications. This gap stimulates the use of digital imaging facilities (like a camera). Moore's law also tells us that more work can be done in software. An early method was proposed by Tomasi and Kanade. They used an affine factorization approach to extract 3D from images sequences. However, the assumption of orthographic projection is a significant limitation of this system.\n\nThe task of converting multiple 2D images into 3D model consists of a series of processing steps:\n\nCamera calibration consists of intrinsic and extrinsic parameters, without which at some level no arrangement of algorithms can work. The dotted line between Calibration and Depth determination represents that the camera calibration is usually required for determining depth.\n\nDepth determination serves as the most challenging part in the whole process, as it calculates the 3D component missing from any given image – depth. The correspondence problem, finding matches between two images so the position of the matched elements can then be triangulated in 3D space is the key issue here.\n\nOnce you have the multiple depth maps you have to combine them to create a final mesh by calculating depth and projecting out of the camera – registration. Camera calibration will be used to identify where the many meshes created by depth maps can be combined together to develop a larger one, providing more than one view for observation.\n\nBy the stage of Material Application you have a complete 3D mesh, which may be the final goal, but usually you will want to apply the color from the original photographs to the mesh. This can range from projecting the images onto the mesh randomly, through approaches of combining the textures for super resolution and finally to segmenting the mesh by material, such as specular and diffuse properties.\n\nGiven a group of 3D points viewed by N cameras with matrices formula_1. Define formula_2 be the homogeneous coordinates of the projection of the formula_3 point onto the formula_4 camera. The reconstruction problem can be changed to: given the group of pixel coordinates formula_5, find the corresponding set of camera matrices formula_6 and the scene structure formula_7 such that\n\nGenerally, without further restrictions, we will obtain a projective reconstruction. If formula_6 and formula_7 satisfy (1), formula_11 and formula_12 will satisfy (1) with any 4 × 4 nonsingular matrix T.\n\nA projective reconstruction can be calculated by points correspondences only, without any a-priori information.\n\nAutocalibration or self-calibration is the classical approach, in which camera motion and parameters are recovered first, using rigidity, then structure is readily calculated. Two methods implementing this idea are presented as follows:\n\nWith a minimum of three displacements, we can obtain the internal parameters of the camera using a system of polynomial equations due to Kruppa, which are derived from a geometric interpretation of the rigidity constraint.\n\nThe matrix formula_13 is unknown in the Kruppa equations, named Kruppa coefficients matrix. With K and by the method of Cholesky factorization one can obtain the intrinsic parameters easily:\n\nRecently Hartley proposed a simpler form. Let formula_15 be written as formula_16, where\n\nThen the Kruppa equations are rewritten (the derivation can be found in )\n\nThis method is based on the use of rigidity constraint. Design a cost function, which considers the intrinsic parameters as arguments and the fundamental matrices as parameters. formula_17 is defined as the fundamental matrix, formula_18and formula_19 as intrinsic parameters matrices.\n\nRecently, new methods based on the concept of stratification have been proposed. Starting from a projective structure, which can be calculated from correspondences only, upgrade this projective reconstruction to a Euclidean reconstruction, by making use of all the available constraints. With this idea the problem can be stratified into different sections: according to the amount of constraints available, it can be analyzed at a different level, projective, affine or Euclidean.\n\nUsually, the world is perceived as a 3D Euclidean space. In some cases, it is not possible to use the full Euclidean structure of 3D space. The simplest being projective, then the affine geometry which forms the intermediate layers and finally Euclidean geometry. The concept of stratification is closely related to the series of transformations on geometric entities: in the projective stratum is a series of projective transformations (a homography), in the affine stratum is a series of affine transformations, and in Euclidean stratum is a series of Euclidean transformations.\n\nSuppose that a fixed scene is captured by two or more perspective cameras and the correspondences between visible points in different images are already given. However, in practice, the matching is an essential and extremely challenging issue in computer vision. Here, we suppose that formula_20 3D points formula_21 are observed by formula_22 cameras with projection matrices formula_23 Neither the positions of point nor the projection of camera are known. Only the projections formula_24 of the formula_4 point in the formula_3 image are known.\n\nSimple counting indicates we have formula_27 independent measurements and only formula_28 unknowns, so the problem is supposed to be soluble with enough points and images. The equations in homogeneous coordinates can be represented:\n\nSo we can apply a nonsingular 4 × 4 transformation \"H\" to projections formula_30→formula_31 and world points formula_32→formula_33. Hence, without further constraints, reconstruction is only an unknown projective deformation of the 3D world.\n\n\"See affine space for more detailed information about computing the location of the plane at infinity formula_34.\"\nThe simplest way is to exploit prior knowledge, for example the information that lines in the scene are parallel or that a point is the one thirds between two others.\n\nWe can also use prior constraints on the camera motion. By analyzing different images of the same point can obtain a line in the direction of motion. The intersection of several lines is the point at infinity in the motion direction, and one constraint on the affine structure.\n\nBy mapping the projective reconstruction to one that satisfies a group of redundant Euclidean constraints, we can find a projective transformation \"H\" in equation (2).The equations are highly nonlinear and a good initial guess for the structure is required. This can be obtained by assuming a linear projection - parallel projection, which also allows easy reconstruction by SVD decomposition.\n\nInevitably, measured data (i.e., image or world point positions) is noisy and the noise comes from many sources. To reduce the effect of noise, we usually use more equations than necessary and solve with least squares.\n\nFor example, in a typical null-space problem formulation Ax = 0 (like the DLT algorithm), the square of the residual ||Ax|| is being minimized with the least squares method.\n\nIn general, if ||Ax|| can be considered as a distance between the geometrical entities (points, lines, planes, etc.), then what is being minimized is a geometric error, otherwise (when the error lacks a good geometrical interpretation) it is called an algebraic error.\n\nTherefore, compared with algebraic error, we prefer to minimize a geometric error for the reasons listed:\n\nAll the linear algorithms (DLT and others) we have seen so far minimize an algebraic error. Actually, there is no justification in minimizing an algebraic error apart from the ease of implementation, as it results in a linear problem. The minimization of a geometric error is often a non-linear problem, that admit only iterative solutions and requires a starting point.\n\nUsually, linear solution based on algebraic residuals serves as a starting point for a non-linear minimization of a geometric cost function, which provides the solution a final “polish”.\n\nThe 2-D imaging has problems of anatomy overlapping with each other and don’t disclose the abnormalities. The 3-D imaging can be used for both diagnostic and therapeutic purposes.\n\n3-D models are used for planning the operation, morphometric studies and has more reliability in orthopedics.\nTo reconstruct 3-D images from 2-D images taken by a camera at multiple angles. Medical imaging techniques like CT scanning and MRI are expensive, and although CT scans are accurate, they can induce high radiation doses which is a risk for patients with certain diseases. Methods based on MRI are not accurate. Since we are exposed to powerful magnetic fields during an MRI scan, this method is not suitable for patients with ferromagnetic metallic implants. Both the methods can be done only when in lying position where the global structure of the bone changes. So, we discuss the following methods which can be performed while standing and require low radiation dose.\n\nThough these techniques are 3-D imaging, the region of interest is restricted to a slice; data is acquired to form a time sequence. \n\nThis method is simple and implemented by identifying the points manually in multi-view radiographs. The first step is to extract the corresponding points in two x-ray images and second step is the 3D reconstruction with algorithms like Discrete Linear Transform. Using DLT, the reconstruction is done only where there are SCPs. By increasing the number of points, the results improve but it is time consuming. This method has low accuracy because of low reproducibility and time consumption. This method is dependent on the skill of the operator. This method is not suitable for bony structures with continuous shape. This method is generally used as an initial solution for other methods.\n\nThis method uses X-ray images for 3D Reconstruction and to develop 3D models with low dose radiations in weight bearing positions.\n\nIn NSCC algorithm, the preliminary step is calculation of an initial solution. Firstly anatomical regions from the generic object are defined. Secondly, manual 2D contours identification on the radiographs is performed. From each radiograph 2D contours are generated using the 3D initial solution object. 3D contours of the initial object surface are projected onto their associated radiograph. The 2D association performed between these 2 set points is based on point-to-point distances and contours derivations developing a correspondence between the 2D contours and the 3D contours. Next step is optimization of the initial solution. Lastly deformation of the optimized solution is done by applying Kriging algorithm to the optimized solution. Finally, by iterating the final step until the distance between two set points is superior to a given precision value the reconstructed object is obtained.\n\nThe advantage of this method is it can be used for bony structures with continuous shape and it also reduced human intervention but they are time consuming.\n\nSurface Rendering technique visualizes a 3D object as a set of surfaces called iso-surfaces. Each surface has points with the same intensity (called iso-value). It is used when we want to see the separated structures e.g. skull from slices of head, blood vessel system from slices of body etc. This technique is used mostly for high contrast data. Two main methods for reconstructing are:\n\n\nOther proposed or developed techniques include Statistical Shape Model Based Methods, Parametric Methods, Hybrid methods.\n\n\n\n",
    "id": "34668189",
    "title": "3D reconstruction from multiple images"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=30729045",
    "text": "International Conference on Autonomous Agents and Multiagent Systems\n\nThe International Conference on Autonomous Agents and Multi-Agent Systems or AAMAS is the leading scientific conference for research in the areas of artificial intelligence, autonomous agents, and multiagent systems. It is annually organized by a non-profit organization called the International Foundation for Autonomous Agents and Multiagent Systems (IFAAMAS).\n\nThe AAMAS conference is a merger of three major international conferences/workshops, namely International Conference on Autonomous Agents (AGENTS), International Conference on Multi-Agent Systems (ICMAS), and International Workshop on Agent Theories, Architectures, and Languages (ATAL). As such, this highly respected joint conference provides a quality forum for discussing research in this area.\n\nThe next AAMAS conference (AAMAS 2018) will take place from July 10-15, 2018, in Stockholm, Sweden.\n\nA list of previous AAMAS conferences is as follows: \n\nBesides the main program that consists of a main track, an industry and applications track, and a couple of special area tracks, AAMAS also hosts over 20 workshops (e.g., AOSE, COIN, DALT, ProMAS, to mention a few) and many tutorials. There is also a demonstration session and a doctoral symposium. Finally, each year AAMAS features a bunch of awards, most notably the IFAAMAS Influential Paper Award. It publishes proceedings which are available online.\n\n",
    "id": "30729045",
    "title": "International Conference on Autonomous Agents and Multiagent Systems"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=35494317",
    "text": "Sensorium Project\n\nThe \"Sensorium Project\" was initiated by ROBOTmaker to broaden the IRCF360 sensor's audience beyond its typical robot sensor usage.\n\nTo demonstrate the BETA functionality, ROBOTmaker uses opensource Java based Integrated Development Environments (IDE) such as Arduino and Processing (programming language) and has released all the prototyping sketches on their website for advanced developers to expand the functionality and write their own Java interface apps. They also provide a low cost self-assembly PCB kit to allow developers to make a sensor and to reprogramme the embedded micro-controller with own firmware if desired.\n\nThe sensor is based on ROBOTmaker's Infrared Control Freak 360 (IRCF360) which is a 360 degree proximity sensor and a motion sensing devices, developed by ROBOTmaker. Their goal is to provide low costs configurable sensors & solutions for use within technical projects such as maker and Maker Faire type projects, micro robotic, kinetic art / art, crafts, engineering, science, Do-It-Yourself (DIY) mindset and alternative music type of projects.\n\nDuring the development phase, it became apparent that the unique features of the 360 sensor also enabled it to be used as an independent 3D motion sensor for use within many other 3D computer applications. It is connected to a PC via the USB port. The device can be configured as a joystick, keyboard, mouse or midi device, for example in 3D applications, gaming applications and as an alternative Human interface device HID for people with disabilities - enabling a wide range of users to control and interact with the PC/MAC/Tablet/SmartPhone without the need to touch a game controller or screen, using a more natural user interface through finger gestures. It works also well together with Windows7 and the voice recognition or together with other HID devices to reduce non-value added mouse movements.\n\nExamples of use are:\nA project named Sensorium Project was started aimed at broadening the Sensors audience beyond its typical robot sensor usage. To Demonstrate the BETA functionality, ROBOTmaker uses opensource Java based Integrated Development Environments (IDE) such as Arduino and Processing (programming language) and has released all the prototyping sketches on their website for advanced developers to expand the functionality and write their own Java interface apps. They also provide a low cost self-assembly PCB kit to allow developers to make a sensor and to reprogrammed the embedded micro-controller with own firmware if desired.,\n\nMany other forms of 3D mice and motion sensors have been developed where a 'physical' interaction is required. The mice have also been known as bats, flying mice, or wands, these devices generally function through ultrasound and provide at least three degrees of freedom. Probably the best known example would be 3Dconnexion/Logitech's SpaceMouse from the early 1990s.\n\nIn the late 1990s Kantek introduced the 3D RingMouse. This wireless mouse was worn on a ring around a finger, which enabled the thumb to access three buttons. The mouse was tracked in three dimensions by a base station. Despite a certain appeal, it was finally discontinued because it did not provide sufficient resolution.\n\nA mouse-related controller called the SpaceBall has a ball placed above the work surface that can easily be gripped. With spring-loaded centering, it sends both translational as well as angular displacements on all six axes, in both directions for each.\n\nIn November 2010 a German Company called Axsotic introduced a new concept of 3D mouse called 3D Spheric Mouse. This new concept of a true six degree-of-freedom input device uses a ball to rotate in 3 axes without any limitations.\n\nOther \"touch-less\" type of motion sensors are:\n\n\n\n",
    "id": "35494317",
    "title": "Sensorium Project"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=35456221",
    "text": "IRCF360\n\nInfrared Control Freak 360 (IRCF360) is a 360 degree proximity sensor and a motion sensing devices, developed by ROBOTmaker. The sensor is in BETA developers release as a low cost (software configurable) sensor for use within research, technical and hobby projects such as Maker Faire type projects, Microbotics, Kinetic art / art, crafts, engineering, UAV, Science, Technology and alternative music type of projects.\n\nThe 360 degree sensor was originally designed as a short range micro robot proximity sensor and mainly intended for Swarm robotics, Ant robotics, Swarm intelligence, autonomous Qaudcopter, Drone, UAV, multi-robot simulations e.g. Jasmine Project\nwhere 360 proximity sensing is required to avoid collision with other robots and for simple IR inter-robot communications. Other uses have since been identified. Stacked 360 degree sensors gives a spherical \"bubble\" of proximity sensing required for 3D space sensing, such as with Unmanned aerial vehicle.\n\nTo overcome certain limitation with Infra-red (IR) proximity sensing (e.g. detection of dark surfaces) the sensing module includes ambient light sensing and basic tactile sensing functionality during forward movement sensing/probing providing photovore and photophobe robot swarm behaviours and characteristics.\n\nCertain features of the 360 sensor enabled it also to be used as an independent 3D motion sensor for use within other computer applications where non-tactile input is required (i.e. compared to a mouse, joystick, keyboard which require physical contact). It is connected to a PC via the USB port. The device can be configured as a joystick, keyboard, mouse or midi device, for example where on-line 3D gaming applications require and as an alternative Human interface device (HID) for people with disabilities - enabling a wider range of users to control and interact with a PC/MAC/Tablet/SmartPhone using a more natural user interface through finger and hand gestures. It can be used together with the Windows7 voice recognition or with other HID devices to switch functionality and thereby reducing non-value added mouse movements.\n\nSome examples of use are:\n\nA project named Sensorium Project was started aimed at broadening the Sensors audience beyond its typical robot sensor usage. To Demonstrate the sensor's functionality, opensource Java based Integrated Development Environments (IDE) are used, such as Arduino and Processing (programming language). The java programs / sketches have all been released to public domain for advanced developers to expand the functionality and write their own Java interface apps.\n\nMany other forms of 3D mice and motion sensors have been developed where a 'physical' interaction is required. The mice have also been known as bats, flying mice, or wands, these devices generally function through ultrasound and provide at least three degrees of freedom. Probably the best known example would be 3Dconnexion/Logitech's SpaceMouse from the early 1990s.\n\nIn the late 1990s Kantek introduced the 3D RingMouse. This wireless mouse was worn on a ring around a finger, which enabled the thumb to access three buttons. The mouse was tracked in three dimensions by a base station. Despite a certain appeal, it was finally discontinued because it did not provide sufficient resolution.\n\nA mouse-related controller called the SpaceBall has a ball placed above the work surface that can easily be gripped. With spring-loaded centering, it sends both translational as well as angular displacements on all six axes, in both directions for each.\n\nIn November 2010 a German Company called Axsotic introduced a new concept of 3D mouse called 3D Spheric Mouse. This new concept of a true six degree-of-freedom input device uses a ball to rotate in 3 axes without any limitations.\n\nUnmanned Aerial Vehicles (UAVs) developments:\n\nOther \"touch-less\" type of motion sensors are:\n\n\n",
    "id": "35456221",
    "title": "IRCF360"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=8205445",
    "text": "Colloquis\n\nColloquis, previously known as ActiveBuddy and Conversagent, was a company that created conversation-based interactive agents originally distributed via instant messaging platforms. The company had offices in New York, NY and Sunnyvale, CA.\n\nFounded in 2000, the company was the brainchild of Robert Hoffer, Timothy Kay and Peter Levitan. The idea for interactive agents (also known as Internet bots) came from the team's vision to add functionality to increasingly popular instant messaging services. The original implementation took shape as a word-based adventure game but quickly grew to include a wide range of database applications including access to news, weather, stock information, movie times, yellow pages listings, and detailed sports data, as well as a variety of tools (calculators, translator, etc.). These various applications were bundled into a single entity and launched as SmarterChild in 2001. SmarterChild acted as a showcase for the quick data access and possibilities for fun conversation that the company planned to turn into customized, niche specific products. \n\nThe rapid success of SmarterChild led to targeted promotional products for Radiohead, Austin Powers, The Sporting News, and others. ActiveBuddy sought to strengthen its hold on the interactive agent market for the future by filing for, and receiving, a controversial patent on their creation in 2002. The company also released the BuddyScript SDK, a free developer kit that allow programmers to design and launch their own interactive agents using ActiveBuddy’s proprietary scripting language, in 2002. Ultimately, however, the decline in ad spending in 2001 and 2002 led to a shift in corporate strategy towards business focused Automated Service Agents, building products for clients including Cingular, Comcast and Cox Communications. The company subsequently changed its name from ActiveBuddy to Conversagent in 2003, and then again to Colloquis in 2006. Colloquis was later purchased by Microsoft in October 2006.\n\nMost of the ActiveBuddy, Colloquis and Microsoft team that launched and nurtured SmarterChild is now working on AB2.\n",
    "id": "8205445",
    "title": "Colloquis"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=35685954",
    "text": "Generalized distributive law\n\nThe generalized distributive law (GDL) is a generalization of the distributive property which gives rise to a general message passing algorithm. It is a synthesis of the work of many authors in the information theory, digital communications, signal processing, statistics, and artificial intelligence communities. The law and algorithm were introduced in a semi-tutorial by Srinivas M. Aji and Robert J. McEliece with the same title.\n\n\"\"The distributive law in mathematics is the law relating the operations of multiplication and addition, stated symbolically, formula_1; that is, the monomial factor formula_2 is distributed, or separately applied, to each term of the binomial factor formula_3, resulting in the product formula_4\"\" - Britannica\n\nAs it can be observed from the definition, application of distributive law to an arithmetic expression reduces the number of operations in it. In the previous example the total number of operations reduced from three (two multiplications and an addition in formula_4) to two (one multiplication and one addition in formula_6). Generalization of distributive law leads to a large family of fast algorithms. This includes the FFT and Viterbi algorithm.\n\nThis is explained in a more formal way in the example below:\n\nformula_7 where formula_8 and formula_9 are real-valued functions, formula_10 and formula_11 (say)\n\nHere we are \"marginalizing out\" the independent variables (formula_12, formula_13, and formula_14) to obtain the result. When we are calculating the computational complexity, we can see that for each formula_15 pairs of formula_16, there are formula_17 terms due to the triplet formula_18 which needs to take part in the evaluation of formula_19 with each step having one addition and one multiplication. Therefore, the total number of computations needed is formula_20. Hence the asymptotic complexity of the above function is formula_21.\n\nIf we apply the distributive law to the RHS of the equation, we get the following:\n\nThis implies that formula_23 can be described as a product formula_24 where formula_25 and formula_26\n\nNow, when we are calculating the computational complexity, we can see that there are formula_17 additions in formula_28 and formula_29 each and there are formula_30 multiplications when we are using the product formula_24 to evaluate formula_23. Therefore, the total number of computations needed is formula_33. Hence the asymptotic complexity of calculating formula_34 reduces to formula_35 from formula_36. This shows by an example that applying distributive law reduces the computational complexity which is one of the good features of a \"fast algorithm\".\n\nSome of the problems that used distributive law to solve can be grouped as follows\n\n1. Decoding algorithms<br>\nA GDL like algorithm was used by Gallager's for decoding low density parity-check codes. Based on Gallager's work Tanner introduced the Tanner graph and expressed Gallagers work in message passing form. The tanners graph also helped explain the Viterbi algorithm.\n\nIt is observed by Forney that Viterbi's maximum likelihood decoding of convolutional codes also used algorithms of GDL-like generality.\n\n2. Forward-backward algorithm<br>\nThe forward backward algorithm helped as an algorithm for tracking the states in the markov chain. And this also was used the algorithm of GDL like generality\n\n3. Artificial intelligence<br>\nThe notion of junction trees has been used to solve many problems in AI. Also the concept of bucket elimination used many of the concepts.\n\nMPF or marginalize a product function is a general computational problem which as special case includes many classical problems such as computation of discrete Hadamard transform, maximum likelihood decoding of a linear code over a memory-less channel, and matrix chain multiplication. The power of the GDL lies in the fact that it applies to situations in which additions and multiplications are generalized.\nA commutative semiring is a good framework for explaining this behavior. It is defined over a set formula_37 with operators \"formula_38\" and \"formula_39\" where formula_40 and formula_41 are a commutative monoids and the distributive law holds.\n\nLet formula_42 be variables such that formula_43 where formula_44 is a finite set and formula_45. Here formula_46. If formula_47 and formula_48, let\nformula_49,\nformula_50, \nformula_51, \nformula_52, and\nformula_53\n\nLet formula_54 where formula_55. Suppose a function is defined as formula_56, where formula_57 is a commutative semiring. Also, formula_58 are named the \"local domains\" and formula_59 as the \"local kernels\".\n\nNow the global kernel formula_60 is defined as :\nformula_61\n\n\"Definition of MPF problem\": For one or more indices formula_62, compute a table of the values of formula_63-\"marginalization\" of the global kernel formula_64, which is the function formula_65 defined as formula_66\n\nHere formula_67 is the complement of formula_63 with respect to formula_69 and the formula_70 is called the formula_71 \"objective function\", or the \"objective function\" at formula_72. It can observed that the computation of the formula_71 objective function in the obvious way needs formula_74 operations. This is because there are formula_75 additions and formula_76 multiplications needed in the computation of the formula_77 objective function. The GDL algorithm which is explained in the next section can reduce this computational complexity.\n\nThe following is an example of the MPF problem. \nLet formula_78 and formula_79 be variables such that formula_80 and formula_81. Here formula_82 and formula_83. The given functions using these variables are formula_84 and formula_85 and we need to calculate formula_86 and formula_87 defined as:\n\nHere local domains and local kernels are defined as follows: \nwhere formula_90 is the formula_91 objective function and formula_87 is the formula_93 objective function.\n\nLet us consider another example where formula_94 and formula_95 is a real valued function. Now, we shall consider the MPF problem where the commutative semiring is defined as the set of real numbers with ordinary addition and multiplication and the local domains and local kernels are defined as follows:\n\nNow since the global kernel is defined as the product of the local kernels, it is\n\nand the objective function at the local domain formula_97 is\n\nThis is the Hadamard transform of the function formula_8. Hence we can see that the computation of Hadamard transform is a special case of the MPF problem. More examples can be demonstrated to prove that the MPF problem forms special cases of many classical problem as explained above whose details can be found at\n\nIf one can find a relationship among the elements of a given set formula_100, then one can solve the MPF problem basing on the notion of belief propagation which is a special use of \"message passing\" technique. The required relationship is that the given set of local domains can be organised into a junction tree. In other words, we create a graph theoretic tree with the elements of formula_100 as the vertices of the tree formula_102, such that for any two arbitrary vertices say formula_103 and formula_104 where formula_105 and there exists an edge between these two vertices, then the intersection of corresponding labels, viz formula_106, is a subset of the label on each vertex on the unique path from formula_103 to formula_104.\n\nFor example,\n\nExample 1: Consider the following nine local domains:\n\n\nFor the above given set of local domains, one can organize them into a junction tree as shown below:\n\nSimilarly If another set like the following is given\n\nExample 2: Consider the following four local domains:\n\n\nThen constructing the tree only with these local domains is not possible since this set of values has no common domains which can be placed between any two values of the above set. But however if add the two dummy domains as shown below then organizing the updated set into a junction tree would be possible and easy too.\n\n5.formula_122,formula_123\n6.formula_124,formula_123\n\nSimilarly for these set of domains, the junction tree looks like shown below:\nInput: A set of local domains.\nOutput: For the given set of domains, possible minimum number of operations that is required to solve the problem is computed. \nSo, if formula_103andformula_104 are connected by an edge in the junction tree, then a message from formula_103 to formula_104 is a set/table of values given by a function: formula_130:formula_131. To begin with all the functions i.e. for all combinations of formula_132and formula_133 in the given tree, formula_130 is defined to be identically formula_135 and when a particular message is update, it follows the equation given below.\n\nwhere formula_138 means that formula_139 is an adjacent vertex to formula_103 in tree.\n\nSimilarly each vertex has a state which is defined as a table containing the values from the function formula_141, Just like how messages initialize to 1 identically, state of formula_103 is defined to be local kernel formula_143, but whenever formula_144 gets updated, it follows the following equation:\n\nFor the given set of local domains as input, we find out if we can create a junction tree, either by using the set directly or by adding dummy domains to the set first and then creating the junction tree, if construction junction is not possible then algorithm output that there is no way to reduce the number of steps to compute the given equation problem, but once we have junction tree, algorithm will have to schedule messages and compute states, by doing these we can know where steps can be reduced, hence will be discusses this below.\n\nThere are two special cases we are going to talk about here namely \"Single Vertex Problem\" in which the objective function is computed at only one vertex formula_146 and the second one is \"All Vertices Problem\" where the goal is to compute the objective function at all vertices.\n\nLets begin with the single-vertex problem, GDL will start by directing each edge towards the targeted vertex formula_147. Here messages are sent only in the direction towards the targeted vertex. Note that all the directed messages are sent only once. The messages are started from the leaf nodes(where the degree is 1) go up towards the target vertex formula_147. The message travels from the leaves to its parents and then from there to their parents and so on until it reaches the target vertex formula_147. The target vertex formula_147 will compute its state only when it receives all messages from all its neighbors. Once we have the state, We have got the answer and hence the algorithm terminates.\n\nFor Example, Lets consider a junction tree constructed from the set of local domains given above i.e. the set from example 1, Now the Scheduling table for these domains is (where the target vertex is formula_151).\n\nformula_152\nformula_153\nformula_154\nformula_155\nformula_156\nformula_157\nformula_158\nformula_159\nformula_160\nformula_161\n\nThus the complexity for Single Vertex GDL can be shown as\n\nformula_162 arithmetic operations\nWhere (Note: The explanation for the above equation is explained later in the article )\nformula_163 is the label of formula_164.\nformula_165 is the degree of formula_164 (i.e. number of vertices adjacent to v).\n\nTo solve the All-Vertices problem, we can schedule GDL in several ways, some of them are parallel implementation where in each round, every state is updated and every message is computed and transmitted at the same time. In this type of implementation the states and messages will stabilizes after number of rounds that is at most equal to the diameter of the tree. At this point all the all states of the vertices will be equal to the desired objective function.\n\nAnother way to schedule GDL for this problem is serial implementation where its similar to the Single vertex problem except that we don't stop the algorithm until all the vertices of a required set have not got all the messages from all their neighbors and have compute their state. \nThus the number of arithmetic this implementation requires is at most formula_167 arithmetic operations.\n\nThe key to constructing a junction tree lies in the local domain graph formula_168, which is a weighted complete graph with formula_169 vertices formula_170 i.e. one for each local domain, having the weight of the edge formula_171 defined by\nformula_172.\nif formula_173, then we say formula_174 is contained informula_175. Denoted by formula_176 (the weight of a maximal-weight spanning tree of formula_168), which is defined by\n\nwhere \"n\" is the number of elements in that set. For more clarity and details, please refer to these.\n\nLet formula_179 be a junction tree with vertex set formula_180 and edge set formula_181. In this algorithm, the messages are sent in both the direction on any edge, so we can say/regard the edge set E as set of ordered pairs of vertices. For example, from Figure 1 formula_181 can be defined as follows\n\nNOTE:formula_184 above gives you all the possible directions that a message can travel in the tree.\n\nThe schedule for the GDL is defined as a finite sequence of subsets offormula_184. Which is generally represented by \nformula_186{formula_187}, Where formula_188 is the set of messages updated during the formula_189 round of running the algorithm.\n\nHaving defined/seen some notations, we will see want the theorem says,\nWhen we are given a schedule formula_190, the corresponding message trellis as a finite directed graph with Vertex set of formula_191, in which a typical element is denoted by formula_192 for formula_193, Then after completion of the message passing, state at vertex formula_104 will be the formula_195 objective defined in\n\nand <nowiki>iff</nowiki> there is a path from formula_197 to formula_198\n\nHere we try to explain the complexity of solving the MPF problem in terms of the number of mathematical operations required for the calculation. i.e. We compare the number of operations required when calculated using the normal method (Here by normal method we mean by methods that do not use message passing or junction trees in short methods that do not use the concepts of GDL)and the number of operations using the generalized distributive law.\n\nExample: Let us consider the simplest case where we need to compute the following expression formula_199.\n\nTo evaluate this expression naively requires two multiplications and one addition. The expression when expressed using the distributive law can be written as formula_200 a simple optimization that reduces the number of operations to one addition and one multiplication.\n\nSimilar to the above explained example we will be expressing the equations in different forms to perform as few operation as possible by applying the GDL.\n\nAs explained in the previous sections we solve the problem by using the concept of the junction trees. The optimization obtained by the use of these trees is comparable to the optimization obtained by solving a semi group problem on trees. For example, to find the minimum of a group of numbers we can observe that if we have a tree and the elements are all at the bottom of the tree, then we can compare the minimum of two items in parallel and the resultant minimum will be written to the parent. When this process is propagated up the tree the minimum of the group of elements will be found at the root.\n\nThe following is the complexity for solving the junction tree using message passing\n\nWe rewrite the formula used earlier to the following form. This is the eqn for a message to be sent from vertex \"v\" to \"w\"\n\nSimilarly we rewrite the equation for calculating the state of vertex v as follows\n\nWe first will analyze for the single-vertex problem and assume the target vertex is formula_147 and hence we have one edge from formula_164 to formula_205. \nSuppose we have an edge formula_206 we calculate the message using the message equation. To calculate formula_207 requires\n\nadditions and\n\nmultiplications.\n\nBut there will be many possibilities for formula_212 hence \nformula_213 possibilities for formula_214.\nThus the entire message will need\n\nadditions and\n\nmultiplications\n\nThe total number of arithmetic operations required to send a message towards formula_217along the edges of tree will be\n\nadditions and\n\nmultiplications.\n\nOnce all the messages have been transmitted the algorithm terminates with the computation of state at formula_147 The state computation requires formula_221 more multiplications.\nThus number of calculations required to calculate the state is given as below\n\nadditions and\n\nmultiplications\n\nThus the grand total of the number of calculations is\n\nwhere formula_226 is an edge and its size is defined by formula_227\n\nThe formula above gives us the upper bound.\n\nIf we define the complexity of the edge formula_226 as\n\nTherefore, formula_225 can be written as\n\nWe now calculate the edge complexity for the problem defined in Figure 1 as follows\n\nThe total complexity will be formula_240 which is considerably low compared to the direct method. (Here by direct method we mean by methods that do not use message passing. The time taken using the direct method will be the equivalent to calculating message at each node and time to calculate the state of each of the nodes.)\n\nNow we consider the all-vertex problem where the message will have to be sent in both the directions and state must be computed at both the vertexes. This would take formula_241 but by precomputing we can reduce the number of multiplications to formula_242. Here formula_13 is the degree of the vertex. Ex : If there is a set formula_244 with formula_245 numbers. It is possible to compute all the d products of formula_246 of the formula_247 with at most formula_242 multiplications rather than the obvious formula_249. \nWe do this by precomputing the quantities \nformula_250 and formula_251 this takes formula_252 multiplications. Then if formula_253 denotes the product of all formula_254 except for formula_255 we have formula_256 and so on will need another formula_257 multiplications making the total formula_258\n\nThere is not much we can do when it comes to the construction of the junction tree except that we may have many maximal weight spanning tree and we should choose the spanning tree with the least formula_259 and sometimes this might mean adding a local domain to lower the junction tree complexity.\n\nIt may seem that GDL is correct only when the local domains can be expressed as a junction tree. But even in cases where there are cycles and a number of iterations the messages will approximately be equal to the objective function. The experiments on Gallager–Tanner–Wiberg algorithm for low density parity-check codes were supportive of this claim.\n",
    "id": "35685954",
    "title": "Generalized distributive law"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=35713574",
    "text": "S Voice\n\nS Voice is an intelligent personal assistant and knowledge navigator which is only available as a built-in application for the Samsung Galaxy S III, S III Mini (including NFC Variant), S4, S4 Mini, S4 Active, S5, S5 Mini, S II Plus, Note II, Note 3, Note 4, Note 10.1, Note 8.0, Stellar, Mega, Grand, Avant, Core, Ace 3, Tab 3 7.0, Tab 3 8.0, Tab 3 10.1, Galaxy Camera, and other 2013 or later Samsung Android devices. The application uses a natural language user interface to answer questions, make recommendations, and perform actions by delegating requests to a set of Web services. It is based on the Vlingo personal assistant.\n\nThe Galaxy S5 and later Samsung Android devices, S Voice runs on Nuance instead of Vlingo.\n\nSome of the capabilities of S Voice include making appointments, opening apps, setting alarms, updating social network websites such as Facebook or Twitter and navigation. S Voice also offers multitasking as well as automatic activation features, for example, when the car engine is started.\n\nIn a disclaimer that pops up on first opening S Voice, Samsung states that the app is provided by a third party which it does not name.\n\nIn the Galaxy S8 and S8+, Bixby was announced to be a major update and replacement to S Voice from the prior phones.\n\n\n\n\n",
    "id": "35713574",
    "title": "S Voice"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=11848927",
    "text": "Personoid\n\nPersonoid is the concept coined by Stanislaw Lem (1971), a Polish science-fiction writer. His personoids are an abstraction of functions of human mind and they live in computers; they do not need any human-like physical body. \n\nIn cognitive and software modeling, \"personoid\" is a research approach to the development of intelligent autonomous agents.\nIn frame of the IPK (Information, Preferences, Knowledge) architecture, it is a framework of abstract intelligent agent with a cognitive and structural intelligence. It can be seen as an essence of high intelligent entities.\n\nFrom the philosophical and systemics perspectives, personoid societies can also be seen as the carriers of a culture. According to N. Gessler, the personoids study can be a base for the research on \"artificial culture\" and \"culture evolution\". \n\n\n",
    "id": "11848927",
    "title": "Personoid"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=18848971",
    "text": "Puckstering\n\nPuckstering is a term used in gaming that refers to an operator controlling a group of AI entities. For example, a gamer playing the role of an army platoon sergeant could control a platoon of soldiers. He could command them to attack an enemy, which they would do autonomously. This is referred to as \"puckstering.\"\n",
    "id": "18848971",
    "title": "Puckstering"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=36298401",
    "text": "DAYDREAMER\n\nDAYDREAMER is a goal-based agent and cognitive architecture developed at University of California, Los Angeles by Erik Mueller. It models the human stream of thought and its triggering and direction by emotions, as in human daydreaming. The architecture is implemented as 12,000 lines of Lisp code.\n\nDAYDREAMER was begun by Erik Mueller in 1983 while he was studying under Michael G. Dyer in the UCLA Computer Science Department. It was completed in 1987 and was followed by the ThoughtTreasure program, which was started in 1993.\n\n\n\n",
    "id": "36298401",
    "title": "DAYDREAMER"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=36347200",
    "text": "Simulated consciousness in fiction\n\nSimulated consciousness, synthetic consciousness, etc. is a theme of a number of works in science fiction. The theme is one step beyond the concept of the \"brain in a vat\"/\"simulated reality\" in that not only the perceived reality but the brain and its consciousness are simulations themselves. On the other hand, it is also the extension of the concept of artificial consciousness in that not only the intelligence, but the \"reality\" it perceives and operates within is artificial as well.\n\nStanislaw Lem's professor Corcoran (met by Ijon Tichy during his interstellar travels, first published by Lem in 1961) simulated conscious agents (personoids) to actually test the viability of the \"simulation hypothesis\" of the reality, i.e., the idea of solipsism.\n\nIn the 1954 story \"The Tunnel under the World\" by Frederik Pohl, a whole city was simulated in order to run tests of the efficiency of advertising campaigns, and the plot evolves from the point when one \"simulacrum\" suddenly notices that every day is June 15. Pohl's idea was elaborated in \"Simulacron-3\" (1964) by Daniel F. Galouye (alternative title: \"Counterfeit World\"), which tells the story of a virtual city developed as a computer simulation for market research purposes. In this city the simulated inhabitants possess consciousness; all but one of the inhabitants are unaware of the true nature of their world.\n\nFurthermore, various novels by Greg Egan such as \"Permutation City\" (1994), \"Diaspora\" (1997) and \"Schild's Ladder\" (2002) explore the concept of simulated consciousness.\n\nCharacters with artificial consciousness (or at least with personalities that imply they have consciousness), from works of fiction:\n\n",
    "id": "36347200",
    "title": "Simulated consciousness in fiction"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=31103500",
    "text": "Reasoning system\n\nIn information technology a reasoning system is a software system that generates conclusions from available knowledge using logical techniques such as deduction and induction. Reasoning systems play an important role in the implementation of artificial intelligence and knowledge-based systems.\n\nBy the everyday usage definition of the phrase, all computer systems are reasoning systems in that they all automate some type of logic or decision. In typical use in the Information Technology field however, the phrase is usually reserved for systems that perform more complex kinds of reasoning. For example, not for systems that do fairly straightforward types of reasoning such as calculating a sales tax or customer discount but making logical inferences about a medical diagnosis or mathematical theorem. Reasoning systems come in two modes: interactive and batch processing. Interactive systems interface with the user to ask clarifying questions or otherwise allow the user to guide the reasoning process. Batch systems take in all the available information at once and generate the best answer possible without user feedback or guidance.\n\nReasoning systems have a wide field of application that includes scheduling, business rule processing, problem solving, complex event processing, intrusion detection, predictive analytics, robotics, computer vision, and natural language processing.\n\nThe first reasoning systems were theorem provers, systems that represent axioms and statements in First Order Logic and then use rules of logic such as modus ponens to infer new statements. Another early type of reasoning system were general problem solvers. These were systems such as the General Problem Solver designed by Newell and Simon. General problem solvers attempted to provide a generic planning engine that could represent and solve structured problems. They worked by decomposing problems into smaller more manageable sub-problems, solving each sub-problem and assembling the partial answers into one final answer. Another example general problem solver was the SOAR family of systems.\n\nIn practice these theorem provers and general problem solvers were seldom useful for practical applications and required specialized users with knowledge of logic to utilize. The first practical application of automated reasoning were expert systems. Expert systems focused on much more well defined domains than general problem solving such as medical diagnosis or analyzing faults in an aircraft. Expert systems also focused on more limited implementations of logic. Rather than attempting to implement the full range of logical expressions they typically focused on modus-ponens implemented via IF-THEN rules. Focusing on a specific domain and allowing only a restricted subset of logic improved the performance of such systems so that they were practical for use in the real world and not merely as research demonstrations as most previous automated reasoning systems had been. The engine used for automated reasoning in expert systems were typically called inference engines. Those used for more general logical inferencing are typically called theorem provers.\n\nWith the rise in popularity of expert systems many new types of automated reasoning were applied to diverse problems in government and industry. Some such as case-based reasoning were off shoots of expert systems research. Others such as constraint satisfaction algorithms were also influenced by fields such as decision technology and linear programming. Also, a completely different approach, one not based on symbolic reasoning but on a connectionist model has also been extremely productive. This latter type of automated reasoning is especially well suited to pattern matching and signal detection types of problems such as text searching and face matching.\n\nThe term reasoning system can be used to apply to just about any kind of sophisticated decision system as illustrated by the specific areas described below. However, the most common use of the term reasoning system implies the computer representation of logic. Various implementations demonstrate significant variation in terms of systems of logic and formality. Most reasoning systems implement variations of propositional and symbolic (predicate) logic. These variations may be mathematically precise representations of formal logic systems (e.g., FOL), or extended and hybrid versions of those systems (e.g., Courteous logic). Reasoning systems may explicitly implement additional logic types (e.g., modal, deontic, temporal logics). However, many reasoning systems implement imprecise and semi-formal approximations to recognised logic systems. These systems typically support a variety of procedural and semi-declarative techniques in order to model different reasoning strategies. They emphasise pragmatism over formality and may depend on custom extensions and attachments in order to solve real-world problems.\n\nMany reasoning systems employ deductive reasoning to draw inferences from available knowledge. These inference engines support forward reasoning or backward reasoning to infer conclusions via modus ponens. The recursive reasoning methods they employ are termed ‘forward chaining’ and ‘backward chaining’, respectively. Although reasoning systems widely support deductive inference, some systems employ abductive, inductive, defeasible and other types of reasoning. Heuristics may also be employed to determine acceptable solutions to intractable problems.\n\nReasoning systems may employ the closed world assumption (CWA) or open world assumption (OWA). The OWA is often associated with ontological knowledge representation and the Semantic Web. Different systems exhibit a variety of approaches to negation. As well as logical or bitwise complement, systems may support existential forms of strong and weak negation including negation-as-failure and ‘inflationary’ negation (negation of non-ground atoms). Different reasoning systems may support monotonic or non-monotonic reasoning, stratification and other logical techniques.\n\nMany reasoning systems provide capabilities for reasoning under uncertainty. This is important when building situated reasoning agents which must deal with uncertain representations of the world. There are several common approaches to handling uncertainty. These include the use of certainty factors, probabilistic methods such as Bayesian inference or Dempster–Shafer theory, multi-valued (‘fuzzy’) logic and various connectionist approaches.\n\nThis section provides a non-exhaustive and informal categorisation of common types of reasoning system. These categories are not absolute. They overlap to a significant degree and share a number of techniques, methods and algorithms.\n\nConstraint solvers solve constraint satisfaction problems (CSPs). They support constraint programming. A constraint is a condition which must be met by any valid solution to a problem. Constraints are defined declaratively and applied to variables within given domains. Constraint solvers use search, backtracking and constraint propagation techniques to find solutions and determine optimal solutions. They may employ forms of linear and nonlinear programming. They are often used to perform optimization within highly combinatorial problem spaces. For example, they may be used to calculate optimal scheduling, design efficient integrated circuits or maximise productivity in a manufacturing process.\n\nTheorem provers use automated reasoning techniques to determine proofs of mathematical theorems. They may also be used to verify existing proofs. In addition to academic use, typical applications of theorem provers include verification of the correctness of integrated circuits, software programs, engineering designs, etc.\n\nLogic programs (LPs) are software programs written using programming languages whose primitives and expressions provide direct representations of constructs drawn from mathematical logic. An example of a general-purpose logic programming language is Prolog. LPs represent the direct application of logic programming to solve problems. Logic programming is characterised by highly declarative approaches based on formal logic, and has wide application across many disciplines.\n\nRule engines represent conditional logic as discrete rules. Rule sets can be managed and applied separately to other functionality. They have wide applicability across many domains. Many rule engines implement reasoning capabilities. A common approach is to implement production systems to support forward or backward chaining. Each rule (‘production’) binds a conjunction of predicate clauses to a list of executable actions. At run-time, the rule engine matches productions against facts and executes (‘fires’) the associated action list for each match. If those actions remove or modify any facts, or assert new facts, the engine immediately re-computes the set of matches. Rule engines are widely used to model and apply business rules, to control decision-making in automated processes and to enforce business and technical policies.\n\nDeductive classifiers arose slightly later than rule-based systems and were a component of a new type of artificial intelligence knowledge representation tool known as frame languages. A frame language describes the problem domain as a set of classes, subclasses, and relations among the classes. It is similar to the object-oriented model. Unlike object-oriented models however, frame languages have a formal semantics based on first order logic. They utilize this semantics to provide input to the deductive classifier. The classifier in turn can analyze a given model (known as an ontology) and determine if the various relations described in the model are consistent. If the ontology is not consistent the classifier will highlight the declarations that are inconsistent. If the ontology is consistent the classifier can then do further reasoning and draw additional conclusions about the relations of the objects in the ontology. For example, it may determine that an object is actually a subclass or instance of additional classes as those described by the user. Classifiers are an important technology in analyzing the ontologies used to describe models in the Semantic web.\nMachine learning systems evolve their behavior over time based on experience. This may involve reasoning over observed events or example data provided for training purposes. For example, machine learning systems may use inductive reasoning to generate hypotheses for observed facts. Learning systems search for generalised rules or functions that yield results in line with observations and then use these generalisations to control future behavior.\n\nCase-based reasoning (CBR) systems provide solutions to problems by analysing similarities to other problems for which known solutions already exist. They use analogical reasoning to infer solutions based on case histories. CBR systems are commonly used in customer/technical support and call centre scenarios and have applications in industrial manufacture, agriculture, medicine, law and many other areas.\n\nA procedural reasoning system (PRS) uses reasoning techniques to select plans from a procedural knowledge base. Each plan represents a course of action for achievement of a given goal. The PRS implements a belief-desire-intention model by reasoning over facts (‘beliefs’) to select appropriate plans (‘intentions’) for given goals (‘desires’). Typical applications of PRS include management, monitoring and fault detection systems.\n",
    "id": "31103500",
    "title": "Reasoning system"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=36831006",
    "text": "Hindsight optimization\n\nHindsight optimisation (HOP) is a computer science technique used in artificial intelligence for analysis of actions which have stochastic results. HOP is used in combination with a deterministic planner. By creating sample results for each of the possible actions from the given state (i.e. determinising the actions), and using the deterministic planner to analyse those sample results, HOP allows an estimate of the actual action.\n",
    "id": "36831006",
    "title": "Hindsight optimization"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=37044578",
    "text": "Maluuba\n\nMaluuba is a Canadian technology company conducting research in artificial intelligence and language understanding. Founded in 2011, the company was acquired by Microsoft in 2017.\n\nIn late March 2016, the company demonstrated a machine reading system capable of answering arbitrary questions about J.K Rowling’s \"Harry Potter and the Philosopher’s Stone\". Maluuba's natural language understanding technology is used by several consumer electronic brands for over 50 million devices.\n\nMaluuba was founded by two undergraduate students from the University of Waterloo, Sam Pasupalak and Kaheer Suleman. Their initial proof of concept was a program that allowed users to search for flights using their voice.\n\nIn February 2012, the company secured $2 million in seed funding from Samsung Ventures.\n\nSince 2013, Maluuba has partnered with several companies in the smart phone, smart TV, automotive and IoT space.\n\nIn August 2015 Maluuba secured a $9 million of Series A investment from Nautilus Ventures and Emerllion Capital. Then in December 2015, Maluuba opened an R&D lab in Montreal, Quebec.\n\nBy 2016 the company employed more than fifty people, and had published fifteen peer-reviewed research papers focused on language understanding.\n\nOn January 13, 2017, Maluuba announced they had been acquired by Microsoft. In July 2017, according to the reports, Maluuba closed its Kitchener-Waterloo office and moved employees to its Montreal office.\n\nMaluuba's research centre opened in Montreal, Quebec in December 2015. The lab is advised by Yoshua Bengio (University of Montreal) and Richard Sutton (University of Alberta). The lab has published fifteen peer-reviewed papers discussing some of its recent research. The lab also partners with the University of Montreal MILA lab and McGill University.\n\nIn late 2016 the company released two natural language datasets: NewsQA, focused on comprehension and Frames, focused on Dialogue.\n\nMaluuba has achieved 80% accuracy on the machine reading benchmark MCTest, outperforming other word-matching approaches by 8%, and surpassed the previous benchmark set for deep learning techniques, the DSTC2, by 3%, bringing it to 83%.\n\nIn June 2016, the company demonstrated a program called EpiReader which outperformed Facebook and Google in machine comprehension tests. Several research teams were able to match Maluuba's results since the paper was released. EpiReader made use of two large datasets, the CNN/Daily Mail dataset released by Google DeepMind, comprising over 300,000 news articles; and the Children's Book Test, posted by Facebook Research, made up of 98 children’s books open sourced under Project Gutenberg.\n\nThe company has published research findings into dialogue systems which comprises natural language understanding, state tracking, and natural language generation. Maluuba published a research paper on policy manager (decision maker) where the system is rewarded for a correct decision. In 2016, Maluuba also freely released the Frames dataset, which is a large human-generated corpus of conversations.\n\nThe company conducts research into reinforcement learning in which intelligent agents are motivated to take actions within a set environment in order to maximize a reward. The research team has also published several papers on scalability.\n\nIn June 2017, the Maluuba team was the first to beat the game \"Ms. Pac-Man\" for the Atari 2600 system.\n\nNumerous applications for Maluuba's technology have been proposed in industry with several applications being commercialized.\n\nOne of the first applications of Maluuba's natural language technology has been the smartphone assistant. These systems allow users to speak to their phone and get direct results to their question (instead of merely seeing a sea of blue web links that point to possible answers to their question). The company raised $9M in 2015 to bring their voice assistant technology to automotive and IOT sectors. \n\nMaluuba also offers a Google Chrome extension, NewsQA, that uses deep learning algorithms to read through news articles in order to answer questions posed by the user. \n",
    "id": "37044578",
    "title": "Maluuba"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=5349640",
    "text": "Language/action perspective\n\nThe language/action perspective (LAP) \"takes language as the primary dimension of human cooperative activity,\" applied not just in person-to-person direct (face-to-face) interactions, but also in the design of systems mediated by information and communication technology. The perspective was developed in the joint authorship of \"Understanding Computers and Cognition\" by Fernando Flores and Terry Winograd in 1987.\n\nAs part of a reflection published in 2006, Terry Winograd describes the language-action perspective as resting on two key orienting principles:\n\n\"Language is action\" argues that speech isn't simply composed of assertions about the situation: utterances may also create a situation, such as, \"Let's go to the park.\" That utterance may be subject to interpretation but is not verifiable via the state of the world. This principle is closely linked to the ideas from phenomenology. Furthermore, language is not the transmission of information, which simply correspond to the state of the world. By creating a situation, language forms a consensual domain to further encourage more action through language. These [Speech act|speech acts] may often take the form of commitment to other actions.\n\nIn the design of information systems, the perspective is based upon the notion as proposed by Terry Winograd that information technology may be limited in its ability to improve human communication. \"Expert behavior requires an exquisite sensitivity to context and an ability to know what to commit to. Computing machines, which are purposely designed to process symbols independent of their context, have no hope of becoming experts.\". That sensitivity to context is thus more in the realm of the human than in that of the artificial.\n\nResearch on LAP was done in the Advanced Technology Group (ATG) at Apple Computer in the late 1980s. Winograd was invited to present the basic concepts in a seminar at Apple in the winter of 1988. Some Apple ATG researchers, notably Tom Pittard and Brad Hartfield, saw potential for enhancing the user experience of network based computer interactions if LAP was included in the mix of basic design considerations.\n\nResearch on the application of LAP to business process modelling was done in the System Modelling Research Group, Faculty of Computing, Engineering and Mathematical Sciences, University of the West of England in the early 2000s.\n\nInsights from related work have been applied over the past two decades. At the LAP 2004 - Conference, Kalle Lyytinen discussed the academic/theoretic success of LAP. Yet, these LAP successes have not found entry into the wider stream of applications. In a sense, LAP is now peripheral to computer science, however there may be a need for a deeper look at this viewpoint.\n\nLAP played a role in the second AI Winter. At the time, symbolic AI tried to represent intelligence using a growing knowledge base represented as facts in language. The LAP argued that language was not simply a correspondence with facts but instead depended upon the contextual domain and could not be rigidly defined. Even Winograd's SHRDLU, an exemplar of language understanding in AI, was incapable of broadening its understanding beyond the blocks world.\n\n\n\n",
    "id": "5349640",
    "title": "Language/action perspective"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=37339876",
    "text": "Pedagogical agent\n\nA pedagogical agent is a concept borrowed from computer science and artificial intelligence and applied to education, usually as part of an intelligent tutoring system (ITS). It is a simulated human-like interface between the learner and the content, in an educational environment. A pedagogical agent is designed to model the type of interactions between a student and another person. Mabanza and de Wet define it as \"as a character enacted by a computer that interacts with the user in a socially engaging manner\". A pedagogical agent can be assigned different roles in the learning environment, such as tutor or co-learner, depending on the desired purpose of the agent. \"A tutor agent plays the role of a teacher, while a co-learner agent plays the role of a learning companion\".\nThe history of Pedagogical Agents is closely aligned with the history of computer animation. As computer animation progressed, it was adopted by educators to enhance computerized learning by including a lifelike interface between the program and the learner. The first versions of a pedagogical agent were more cartoon than person, like Microsoft's Clippy which helped user's of Microsoft Office load and use the program's features in 1997. However, with developments in computer animation, pedagogical agents can now look lifelike. By 2006 there was a call to develop modular, reusable agents to decrease the time and expertise required to create a pedagogical agent. There was also a call in 2009 to enact agent standards. The standardization and re-usability of pedagogical agents is less of an issue since the decrease in cost and widespread availability of animation tools. Individualized pedagogical agents can be found across disciplines including medicine, math, law, language learning, automotive, and armed forces. They are used in applications directed to every age, from preschool to adult.\n\nDistributed cognition theory is the method in which cognition progresses in the context of collaboration with others. Pedagogical agents can be designed to assist the cognitive transfer to the learner, operating as artifacts or partners with collaborative role in learning. To support the performance of an action by the user, the pedagogical agent can act as a cognitive tool as long as the agent is equipped with the knowledge that the user lacks. The interactions between the user and the pedagogical agent can facilitate a social relationship. The pedagogical agent may fulfill the role of a working partner.\n\nSocio-cultural learning theory is how the user develops when they are involved in learning activities in which there is interaction with other agents. A pedagogical agent can: intervene when the user requests, provide support for tasks that the user cannot address, and potentially extend the learners cognitive reach. Interaction with the pedagogical agent may elicit a variety of emotions from the learner. The learner may become excited, confused, frustrated, and/or discouraged. These emotions affect the learners' motivation.\n\nIt has been suggested by researchers that pedagogical agents may take on different roles in the learning environment. Examples of these roles are: supplanting, scaffolding, coaching, testing, or demonstrating or modelling a procedure. A pedagogical agent as a tutor has not been demonstrated to add any benefit to an educational strategy in equivalent lessons with and without a pedagogical agent. According to Richard Mayer, there is some support in research for pedagogical agent increasing learning, but only as a presenter of social cues. A co-learner pedagogical agent is believed to increase the student's self-efficacy. By pointing out important features of instructional content, a pedagogical agent can fulfill the signaling function, which research on multimedia learning has shown to enhance learning. Research has demonstrated that human-human interaction may not be completely replaced by pedagogical agents, but learners may prefer the agents to non-agent multimedia systems. This finding is supported by social agency theory.\n\nThe appearance of a pedagogical agent can be manipulated to meet the learning requirements. The attractiveness of a pedagogical agent can enhance student's learning when the users were the opposite gender of the pedagogical agent. Male students prefer a sexy appearance of a female pedagogical agents and dislike the sexy appearance of male agents. Female students were not attracted by the sexy appearance of either male or female pedagogical agents.\n\n * AI: Artificial Intelligence Research at USC Information Science Institute\n",
    "id": "37339876",
    "title": "Pedagogical agent"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=37447771",
    "text": "Legal expert system\n\nA legal expert system is a domain-specific expert system that uses artificial intelligence to emulate the decision-making abilities of a human expert in the field of law. Legal expert systems employ a rule base or knowledge base and an inference engine to accumulate, reference and produce expert knowledge on specific subjects within the legal domain.\n\nIt has been suggested that legal expert systems could help to manage the rapid expansion of legal information and decisions that began to intensify in the late 1960s. Many of the first legal expert systems were created in the 1970s and 1980s.\n\nLawyers were originally identified as primary target users of legal expert systems. Potential motivations for this work included:\n\n\nSome early development work was oriented toward the creation of automated judges.\n\nLater work on legal expert systems has identified potential benefits to non-lawyers as a means to increase access to legal knowledge.\n\nLegal expert systems can also support administrative processes, facilitating decision making processes, automating rule-based analyses and exchanging information directly with citizen-users.\n\nRule-based expert systems rely on a model of deductive reasoning that utilizes \"if A, then B\" rules. In a rule-based legal expert system, information is represented in the form of deductive rules within the knowledge base.\n\nCase-based reasoning models, which store and manipulate examples or cases, hold the potential to emulate an analogical reasoning process thought to be well-suited for the legal domain. This model effectively draws on known experiences our outcomes for similar problems.\n\nA neural net relies on a computer model that mimics that structure of a human brain, and operates in a very similar way to the case-based reasoning model. This expert system model is capable of recognizing and classifying patterns within the realm of legal knowledge and dealing with imprecise inputs.\n\nFuzzy logic models attempt to create 'fuzzy' concepts or objects that can then be converted into quantitative terms or rules that are indexed and retrieved by the system. In the legal domain, fuzzy logic can be used for rule-based and case-based reasoning models.\n\nWhile some legal expert system architects have adopted a very practical approach, employing scientific modes of reasoning within a given set of rules or cases, others have opted for a broader philosophical approach inspired by jurisprudential reasoning modes emanating from established legal theoreticians. \n\nSome legal expert systems aim to arrive at a particular conclusion in law, while others are designed to predict a particular outcome. An example of a predictive system is one that predicts the outcome of judicial decisions, the value of a case, or the outcome of litigation.\n\nMany forms of legal expert systems have become widely used and accepted by both the legal community and the users of legal services.\n\nThe inherent complexity of law as a discipline raises immediate challenges for legal expert system knowledge engineers. Legal matters often involve interrelated facts and issues, which further compound the complexity.\n\nFactual uncertainty may also arise when there are disputed versions of factual representations that must be input into an expert system to begin the reasoning process.\n\nThe limitations of most computerized problem solving techniques inhibit the success of many expert systems in the legal domain. Expert systems typically rely on deductive reasoning models that have difficulty according degrees of weight to certain principles of law or importance to previously decided cases that may or may not influence a decision in an immediate case or context.\n\nExpert legal knowledge can be difficult to represent or formalize within the structure of an expert system. For knowledge engineers, challenges include:\n\nCreating a functioning expert system requires significant investments in software architecture, subject matter expertise and knowledge engineering. Faced with these challenges, many system architects restrict the domain in terms of subject matter and jurisdiction. The consequence of this approach is the creation of narrowly focused and geographically restricted legal expert systems that are difficult to justify on a cost-benefit basis.\n\nLegal expert systems may lead non-expert users to incorrect or inaccurate results and decisions. This problem could be compounded by the fact that users may rely heavily on the correctness or trustworthiness of results or decisions generated by these systems.\n\nASHSD-II is a hybrid legal expert system that blends rule-based and case-based reasoning models in the area of matrimonial property disputes under English law.\n\nCHIRON is a hybrid legal expert system that blends rule-based and case-based reasoning models to support tax planning activities under United States tax law and codes.\n\nJUDGE is a rule-based legal expert system that deals with sentencing in the criminal legal domain for offences relating to murder, assault and manslaughter.\n\nThe Latent Damage Project is a rule-based legal expert system that deals with limitation periods under the (UK) \"Latent Damage Act 1986\" in relation to the domains of tort, contract and product liability law.\n\nSplit-Up is a rule-based legal expert system that assists in the division of marital assets according to the (Australia) \"Family Law Act (1975)\".\n\nSHYSTER is a case-based legal expert system that can also function as a hybrid through its ability to link with rule-based models. It was designed to accommodate multiple legal domains, including aspects of Australian copyright law, contract law, personal property and administrative law.\n\nTAXMAN is a rule-based system that could perform a basic form of legal reasoning by classifying cases under a particular category of statutory rules in the area of law concerning corporate reorganization.\n\nThere may be a lack of consensus over what distinguishes a legal expert system from a knowledge-based system (also called an intelligent knowledge-based system). While legal expert systems are held to function at the level of a human legal expert, knowledge-based systems may depend on the ongoing assistance of a human expert. True legal expert systems typically focus on a narrow domain of expertise as opposed to a wider and less specific domain as in the case of most knowledge-based systems.\n\nLegal expert systems represent potentially disruptive technologies for the traditional, bespoke delivery of legal services. Accordingly, established legal practitioners may consider them a threat to historical business practices.\n\nArguments have been made that a failure to take into consideration various theoretical approaches to legal decision making will produce expert systems that fail to reflect the true nature of decision making. Meanwhile, some legal expert system architects contend that because many lawyers have proficient legal reasoning skills without a sound base in legal theory, the same should hold true for legal expert systems.\n\nBecause legal expert systems apply precision and scientific rigor to the act of legal decision-making, they may be seen as a challenge to the more disorganized and less precise dynamics of traditional jurisprudential modes of legal reasoning. Some commentators also contend that the true nature of legal practice does not necessarily depend on analyses of legal rules or principles; decisions are based instead on an expectation of what a human adjudicator would decide for a given case.\n\nSince 2013, there have been significant developments in legal expert systems. Professor Tanina Rostain of Georgetown Law Center teaches a course in designing legal expert systems. Companies such as Neota Logic have begun to offer artificial intelligence and machine learning-based legal expert systems.\n\n\n",
    "id": "37447771",
    "title": "Legal expert system"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=37431577",
    "text": "Dialogflow\n\nDialogflow (formerly Api.ai, Speaktoit) is a developer of human–computer interaction technologies based on natural language conversations. The company is best known for creating the Assistant (by Speaktoit), a virtual buddy for Android, iOS, and Windows Phone smartphones that performs tasks and answers users' question in a natural language. Speaktoit has also created a natural language processing engine that incorporates conversation context like dialogue history, location and user preferences.\n\nIn May 2012, Speaktoit received a venture round (funding terms undisclosed) from Intel Capital. In July 2014, Speaktoit closed their Series B funding led by Motorola Solutions Venture Capital with participation from new investor Plug and Play Ventures and existing backers Intel Capital and Alpine Technology Fund.\n\nIn September 2014, Speaktoit released api.ai (the voice-enabling engine that powers Assistant) to third-party developers, allowing the addition of voice interfaces to apps based on Android, iOS, HTML5, and Cordova. The SDK's contain voice recognition, natural language understanding, and text-to-speech. api.ai offers a web interface to build and test conversation scenarios. The platform is based on the natural language processing engine built by Speaktoit for its Assistant application. Api.ai allows Internet of Things developers to include natural language voice interfaces in their products. Assistant and Speaktoit's websites now redirect to api.ai's website, which redirects to the Dialogflow website.\n\nGoogle bought the company in September 2016 and was initially known as API.AI; it provides tools to developers building apps (\"Actions\") for the Google Assistant virtual assistant. It was renamed on 10 October 2017 as Dialogflow.\n\nThe organization discontinued the Assistant app on December 15, 2016.\n\nVoice and conversational interfaces created with Dialogflow works with a wide range of devices including phones, wearables, cars, speakers and other smart devices. It supports 14+ languages including Brazilian Portuguese, Chinese, English, Dutch, French, German, Italian, Japanese, Korean, Portuguese, Russian, Spanish and Ukranian. Dialogflow supports an array of services that are relevant to entertainment and hospitality industries. Dialogflow also includes an analytics tool that can measure the engagement or session metrics like usage patterns, latency issues, etc. \n",
    "id": "37431577",
    "title": "Dialogflow"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=37394465",
    "text": "KAoS\n\nKAoS is a policy and domain services framework created by the Florida Institute for Human and Machine Cognition. It uses W3C’s Web Ontology Language (OWL) standard for policy representation and reasoning, and a software guard technology for efficient enforcement of a compiled version of its policies. It has been used in a variety of government-sponsored projects for distributed host and network management and for the coordination of human-agent-robot teams, including DARPA's CoABS Grid, Cougaar, and Common Object Request Broker Architecture (CORBA) models.\n\n",
    "id": "37394465",
    "title": "KAoS"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1499475",
    "text": "K-line (artificial intelligence)\n\nA K-line, or Knowledge-line, is a mental agent which represents an association of a group of other mental agents found active when a subject solves a certain problem or formulates a new idea. These were first described in Marvin Minsky's essay , published in 1980 in the journal \"Cognitive Science\":\n\nWhen you \"get an idea,\" or \"solve a problem\" ... you create what we shall call a K-line. ... When that K-line is later \"activated\", it reactivates ... mental agencies, creating a partial mental state \"resembling the original.\"\n\n\"Whenever you 'get a good idea', solve a problem, or have a memorable experience, you activate a K-line to 'represent' it. A K-line is a wirelike structure that attaches itself to whichever mental agents are active when you solve a problem or have a good idea.\n\nWhen you activate that K-line later, the agents attached to it are aroused, putting you into a 'mental state' much like the one you were in when you solved that problem or got that idea. This should make it relatively easy for you to solve new, similar problems!\" (1998, p. 82.)\n\n",
    "id": "1499475",
    "title": "K-line (artificial intelligence)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=37291130",
    "text": "BabelNet\n\nBabelNet is a multilingual lexicalized semantic network and ontology developed at the Sapienza University of Rome the Department of Computer Science Linguistic Computing Laboratory. BabelNet was automatically created by linking Wikipedia to the most popular computational lexicon of the English language, WordNet. The integration is done using an automatic mapping and by filling in lexical gaps in resource-poor languages by using statistical machine translation. The result is an \"encyclopedic dictionary\" that provides concepts and named entities lexicalized in many languages and connected with large amounts of semantic relations. Additional lexicalizations and definitions are added by linking to free-license wordnets, OmegaWiki, the English Wiktionary, Wikidata, FrameNet, VerbNet and others. Similarly to WordNet, BabelNet groups words in different languages into sets of synonyms, called \"Babel synsets\". For each Babel synset, BabelNet provides short definitions (called glosses) in many languages harvested from both WordNet and Wikipedia.\n\n, BabelNet (version 3.7) covers 271 languages, including all European languages, most Asian languages, and Latin. BabelNet 3.7 contains almost 14 million synsets and about 746 million word senses (regardless of their language). Each Babel synset contains 2 synonyms per language, i.e., word senses, on average. The semantic network includes all the lexico-semantic relations from WordNet (hypernymy and hyponymy, meronymy and holonymy, antonymy and synonymy, etc., totaling around 364,000 relation edges) as well as an underspecified relatedness relation from Wikipedia (totaling around 380 million edges). Version 3.7 also associates about 11 million images with Babel synsets and provides a Lemon RDF encoding of the resource, available via a SPARQL endpoint. 2.67 million synsets are assigned domain labels.\n\nBabelNet has been shown to enable multilingual Natural Language Processing applications. The lexicalized knowledge available in BabelNet has been shown to obtain state-of-the-art results in:\n\n\nBabelNet received the META prize 2015 for \"groundbreaking work in overcoming language barriers through a multilingual lexicalised semantic network and ontology making use of heterogeneous data sources\". \n\nBabelNet featured prominently in a TIME magazine's article about the new age of innovative and up-to-date lexical knowledge resources available on the Web. The article describes in some detail how BabelNet is playing a leading role in the 21st century scenario.\n\n",
    "id": "37291130",
    "title": "BabelNet"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=30405742",
    "text": "Plug &amp; Pray\n\nPlug & Pray is a 2010 documentary film about the promise, problems and ethics of artificial intelligence and robotics. The main protagonists are the former MIT professor Joseph Weizenbaum and the futurist Raymond Kurzweil. The title is a pun on the computer hardware phrase \"Plug and Play\".\n\nComputer experts around the world strive towards the development of intelligent robots. Pioneers like Raymond Kurzweil and Hiroshi Ishiguro dream of fashioning intelligent machines that will equal their human creators. In this potential reality, man and machine merge as a single unity. Rejecting evolution's biological shackles tantalisingly dangles the promise of eternal life for those bold enough to seize it. But others, like Joseph Weizenbaum, counterattack against society's limitless faith in the redemptive powers of technology, questioning the prevailing discourses on new technologies and their ethical relationships to human life. The film delves into a world where computer technology, robotics, biology, neuroscience, and developmental psychology merge, and features roboticists in their laboratories in Japan, the USA, Italy and Germany.\n\nSince antiquity, mankind has dreamed of creating brilliant machines. The invention of the computer and the breathtaking pace of technological progress appear to be bringing the realisation of this dream within the grasp of humans. Robots were to do the housework, look after the children, care for the elderly, and go to war. Former MIT professor Joseph Weizenbaum, creator of ELIZA, has become a harsh critic of their visions of technological omnipotence.\n\nProduction of the film started in 2006 and ended in 2009. The death of the main protagonist Joseph Weizenbaum on March 5, 2008, fell in this period. The international festival premiere was at FIPA 2010 in Biarritz, France. Since then the film has been invited to 27 film festivals, among them the Seattle International Film Festival, Vancouver Film Festival, Visions du Réel. The theatrical release in Germany was on Nov. 11, 2010.\n\nThe film won the Bavarian Film Award 2010 for \"best documentary\", the Grand Prix of the Jury for the best film at the Paris International Science Film Festival, the Primer Premio for best film at the Mostra de Ciencia e Cinema in La Coruña (Spain), and the Science Communication Award at the International Science Film Festival Athens. It was also chosen as the best international film at the 46th AFO, Science Documentary Festival in Olomouc, Czech Republic, in 2011.\n\n",
    "id": "30405742",
    "title": "Plug &amp; Pray"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=3280462",
    "text": "Belief–desire–intention model\n\nThe belief–desire–intention (BDI) model of human practical reasoning was developed by Michael Bratman as a way of explaining future-directed intention.\n\nBDI is fundamentally reliant on folk psychology (the 'theory theory'), which is the notion that our mental models of the world are theories. It was used as a basis for developing the belief–desire–intention software model.\n\nBDI was part of the inspiration behind the BDI software architecture, which Bratman was also involved in developing. Here, the notion of intention was seen as a way of limiting time spent on deliberating about what to do, by eliminating choices inconsistent with current intentions.\n\nBDI has also aroused some interest in psychology. BDI formed the basis for a computational model of childlike reasoning CRIBB. It has been proposed that autistic children do not recognise other people as folk-psychological agents (i.e., agents with their own beliefs, etc.). BDI has been used to develop a rehabilitation strategy to teach autistic children to reason about other people.\n\n",
    "id": "3280462",
    "title": "Belief–desire–intention model"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=12379384",
    "text": "And–or tree\n\nAn and–or tree is a graphical representation of the reduction of problems (or goals) to conjunctions and disjunctions of subproblems (or subgoals).\n\nThe and-or tree:\n\nrepresents the search space for solving the problem P, using the goal-reduction methods:\n\nGiven an initial problem P0 and set of problem solving methods of the form:\n\nthe associated and-or tree is a set of labelled nodes such that: \n\n\nA node N, labelled by a problem P, is a success node if there is a method of the form P if nothing (i.e., P is a \"fact\"). The node is a failure node if there is no method for solving P.\n\nIf all of the children of a node N, conjoined by the same arc, are success nodes, then the node N is also a success node. Otherwise the node is a failure node.\n\nAn and-or tree specifies only the search space for solving a problem. Different search strategies for searching the space are possible. These include searching the tree depth-first, breadth-first, or best-first using some measure of desirability of solutions. The search strategy can be sequential, searching or generating one node at a time, or parallel, searching or generating several nodes in parallel.\n\nThe methods used for generating and-or trees are propositional logic programs (without variables). In the case of logic programs containing variables, the solutions of conjoint sub-problems must be compatible. Subject to this complication, sequential and parallel search strategies for and-or trees provide a computational model for executing logic programs.\n\nAnd–or trees can also be used to represent the search spaces for two-person games. The root node of such a tree represents the problem of one of the players winning the game, starting from the initial state of the game. Given a node N, labelled by the problem P of the player winning the game from a particular state of play, there exists a single set of conjoint children nodes, corresponding to all of the opponents responding moves. \nFor each of these children nodes, there exists a set of non-conjoint children nodes, corresponding to all of the player's defending moves.\n\nFor solving game trees with proof-number search family of algorithms, game trees are to be mapped to and-or trees. MAX-nodes (i.e. maximizing player to move) are represented as OR nodes, MIN-nodes map to AND nodes. The mapping is possible, when the search is done with only a binary goal, which usually is \"player to move wins the game\".\n\n",
    "id": "12379384",
    "title": "And–or tree"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=39177819",
    "text": "Cognitive computer\n\nA cognitive computer combines artificial intelligence and machine-learning algorithms, in an approach which attempts to reproduce the behaviour of the human brain.\n\nAn example of neural network implementations of cognitive convolution and deep learning is provided by the IBM company's Watson machine. A subsequent development by IBM is the TrueNorth microchip architecture, which is designed to be closer in structure to the human brain than the von Neumann architecture used in conventional computers. In 2017 Intel announced its own version of a cognitive chip in \"Loihi\", which will be available to university and research labs in 2018.\n\nIntel's self-learning neuromorphic chip, named Loihi, perhaps named after the Hawaiian seamount Loihi, offers substantial power efficiency designed after the human brain. Intel claims Loihi is about 1000 times more energy efficient than the general-purpose computing power needed to train the neural networks that rival Loihi’s performance. \nIn theory, this would support both machine learning training and inference on the same silicon independently of a cloud connection, and more efficient than using convolutional neural networks (CNNs) or deep learning neural networks. Intel points to a system for monitoring a person’s heartbeat, taking readings after events such as exercise or eating, and uses the cognitive computing chip to normalize the data and work out the ‘normal’ heartbeat. It can then spot abnormalities, but also deal with any new events or conditions.\n\nThe first iteration of the Loihi chip was made using Intel’s 14 nm fabrication process, and houses 128 clusters of 1,024 artificial neurons each for a total of 131,072 simulated neurons. This offers around 130 million synapses, which is still a rather long way from the human brain's 80 billion synapses, and behind IBM’s TrueNorth, which has around 16 billion by using 64 by 4,096 cores.\n\nThe IBM cognitive computers implement learning using Hebbian theory. Instead of being programmable in a traditional sense within machine language or a higher level programming language such a device learns by inputting instances through an input device that are aggregated within a computational convolution or neural network architecture consisting of weights within a parallel memory system. An early instantiation of such a device has been developed in 2012 under the Darpa SyNAPSE program at IBM directed by Dharmendra Modha.\n\nIn 2017 this IBM 64-chip array will contain the processing equivalent of 64 million neurons and 16 billion synapses, yet absolutely sips energy; each processor consumes just 10 watts of electricity.\nLike other neural networks, this system will be put to use in pattern recognition and sensory processing roles. The Air Force wants to combine the TrueNorth ability to convert multiple data feeds — whether it's audio, video or text — into machine readable symbols with a conventional supercomputer's ability to crunch data.\nThis isn't the first time that IBM's neural chip system has been integrated into cutting-edge technology. August, 2017 Samsung installed the chips in its Dynamic Vision Sensors enabling cameras to capture images at up to 2,000 fps while burning through just 300 milliwatts of power.\n\nThere are many approaches and definitions for a cognitive computer, and other approaches may be more fruitful.\n\n\nhttp://www.foxnews.com/tech/2018/01/09/ces-2018-intel-gives-glimpse-into-mind-blowing-future-computing.html\n",
    "id": "39177819",
    "title": "Cognitive computer"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=39182554",
    "text": "Catastrophic interference\n\nCatastrophic interference, also known as catastrophic forgetting, is the tendency of an artificial neural network to completely and abruptly forget previously learned information upon learning new information. Neural networks are an important part of the network approach and connectionist approach to cognitive science. These networks use computer simulations to try to model human behaviours, such as memory and learning. Catastrophic interference is an important issue to consider when creating connectionist models of memory. It was originally brought to the attention of the scientific community by research from McCloskey and Cohen (1989), and Ractcliff (1990). It is a radical manifestation of the 'sensitivity-stability' dilemma or the 'stability-plasticity' dilemma. Specifically, these problems refer to the issue of being able to make an artificial neural network that is sensitive to, but not disrupted by, new information. Lookup tables and connectionist networks lie on the opposite sides of the stability plasticity spectrum. The former remains completely stable in the presence of new information but lacks the ability to generalize, i.e. infer general principles, from new inputs. On the other hand, connectionist networks like the standard backpropagation network are very sensitive to new information and can generalize on new inputs. Backpropagation models can be considered good models of human memory insofar as they mirror the human ability to generalize but these networks often exhibit less stability than human memory. Notably, these backpropagation networks are susceptible to catastrophic interference. This is considered an issue when attempting to model human memory because, unlike these networks, humans typically do not show catastrophic forgetting. Thus, the issue of catastrophic interference must be eradicated from these backpropagation models in order to enhance the plausibility as models of human memory.\n\nIn order to understand the topic of catastrophic interference it is important to understand the components of an artificial neural network and, more specifically, the behaviour of a backpropagation network. The following account of neural networks is summarized from \"Rethinking Innateness: A Connectionist Perspective on Development\" by Elman et al. (1996).\n\nArtificial neural networks are inspired by biological neural networks. They use mathematical models, namely algorithms, to do things such as classifying data and learning patterns in data. Information is represented in these networks through patterns of activation, known as a distributed representations.\n\nThe basic components of artificial neural networks are \"nodes/units\" and \"weights\". \n\"Nodes\" or \"units\" are simple processing elements, which can be considered artificial neurons. These units can act in a variety of ways. They can act like sensory neurons and collect inputs from the environment, they can act like motor neurons and sent and output, they can act like interneurons and relay information, or they may do all three functions. A backpropagation network is often a three-layer neural network that includes input nodes, hidden nodes, and output nodes (see Figure 1). The hidden nodes allow the network to be transformed into an internal representation, akin to a mental representation. These internal representations give the backpropagation network its ability to capture abstract relationships between different input patterns.\n\nThe nodes are also connected to each other, thus they can send activation to one another like neurons. These connections can be unidirectional, creating a feedforward network, or they can be bidirectional, creating a recurrent network. Each of the connections between the nodes has a '`weight'`, or strength, and it is in these weights are where the knowledge is 'stored'. The weights act to multiply the output of a node. They can be excitatory (a positive value) or inhibitory (a negative value). For example, if a node has an output of 1.0 and it is connected to another node with a weight of -0.5 then the second node will receive an input signal of 1.0 × (-0.5) = -0.5. Since any one node can receive multiple inputs, the sum of all of these inputs must be taken to calculate the net input.\n\nThe net input (\"net\") to a node \"j\" would be defined as:\n\nOnce the input has been sent to the hidden layer from the input layer, the hidden node may then send an output to the output layer. The output of any given node depends on the activation of that node and the response function of that node. In the case of a three-layer backpropagation network, the response function is a non-linear, logistic function. This function allows a node to behave in an all or none fashion towards high or low input values and in a more graded and sensitive fashion towards mid-ranged input values. It allows the nodes the result in more substantial changes in the network when the node activation is at the more extreme values. Transforming the net input into a net output that can be sent onto the output layer is calculated by:\n\nAn important feature of neural networks is that they can learn. Simply put, this means that they can change their outputs when they are given new inputs. Backpropagation, specifically refers to how this the network is trained, i.e. how the network is told to learn. The way in which a backpropagation network learns, is through comparing the actual output to the desired output of the unit. The desired output is known as a 'teacher' and it can be the same as the input, as in the case of auto-associative/auto-encoder networks, or it can be completely different from the input. Either way, learning which requires a teacher is called supervised learning. The difference between these actual and desired output constitutes an error signal. This error signal is then fedback, or backpropagated, to the nodes in order to modify the weights in the neural network. Backpropagation first modifies the weights between output layer to the hidden layer, then next modifies the weights between the hidden units and the input units. The change in weights help to decrease the discrepancy between the actual and desired output. However, learning is typically incremental in these networks. This means that these networks will require a series of presentations of the same input before it can come up with the weight changes that will result in the desired output. The weights are usually set to random values for first learning trial and after many trials the weights become more able represent the desired output. The process of converging on an output is called settling. This kind of training is based on the error signal and backpropagation learning algorithm / delta rule:\n\nThe issue of catastrophic interference, comes about when learning is sequential. Sequential training involves the network learning an input-output pattern until the error is reduced below a specific criterion, then training the network on another set of input-output patterns. Specifically, a backpropagation network will forget information if it first learns input \"A\" and then next learns input \"B\". It is not seen when learning is concurrent or interleaved. Interleaved training means the network learns both inputs-output patterns at the same time, i.e. as \"AB\". Weights are only changed when the network is being trained and not when the network is being tested on its response.\n\nTo summarize, backpropagation networks:\n\n\nHumans often learn information in a sequential manner. For example, a child will often learn their 1st addition facts first, later followed by the 2nd addition facts, etc. It would be impossible for a child to learn all of the addition facts at the same time. Catastrophic interference can be considered an issue when modelling human memory because, unlike backpropagation networks, humans typically do not show catastrophic forgetting during sequential learning. Rather humans tend to show gradual forgetting or interference when they learn information sequentially. For example, the classic retroactive interference study by Barnes and Underwood (1959) used paired associate learning to determine how much new learning interfered with old learning in humans. Paired associates, means that a pair of stimuli is and responses are learned. Their experiment used eight lists of paired associates, A-B and A-C. The pairs had the stimuli as consonant-vowel-consonant trigrams (e.g., dax) and responses as adjectives. Subjects were initially trained on the A-B list, until they could correctly recall all A-B pairings. Next subjects were given 1, 5, 10 or 20 trials on the A-C list. After learning the A-C pairs the subjects were given a final test in which the stimulus A was presented and the subject was asked to recall the response B and C. They found that as the number of learning trials on A-C list increased, the recall of C increased. But the training on A-C interfered with the recall of B. Specifically recall of B dropped to around 80% after one learning trial of A-C and to 50% after 20 learning trials of A-C. Subsequent research on the topic of retroactive interference has found similar results, with human forgetting being gradual and typically leveling off near 50% recall. Thus when compared with typical human retroactive interference, catastrophic interference could be likened to retrograde amnesia.\n\nSome researchers have argued that catastrophic interference is not an issue with the backpropagation model of human memory. For example, Mirman and Spivey (2001) found that humans show more interference when learning pattern-based information. Pattern-based learning is analogous to how a standard backpropagation network learns. Thus, they concluded that catastrophic interference is not limited to connectionist memory models but rather that it is a \"general product of pattern-based learning that occurs in humans as well\" (p. 272). However, Musca, Rousset and Ans (2004) found contrasting results where retroactive interference was more pronounced in subjects who sequentially learned unstructured lists when controlling for methodological failure that occurred in the Mirman, D., & Spivey, M. (2001) study.\n\nThe term catastrophic interference was originally coined by McCloskey and Cohen (1989) but was also brought to the attention of the scientific community by research from Ratcliff (1990).\n\nMcCloskey and Cohe n(1989) noted the problem of catastrophic interference during two different experiments with backpropagation neural network modelling.\n\nIn their first experiment they trained a standard backpropagation neural network on a single training set consisting of 17 single-digit ones problems (i.e., 1 + 1 through 9 + 1, and 1 + 2 through 1 + 9) until the network could represent and respond properly to all of them. The error between the actual output and the desired output steadily declined across training sessions, which reflected that the network learned to represent the target outputs better across trials. Next they trained the network on a single training set consisting of 17 single-digit twos problems (i.e., 2 + 1 through 2 + 9, and 1 + 2 through 9 + 2) until the network could represent, respond properly to all of them. They noted that their procedure was similar to how a child would learn their addition facts. Following each learning trial on the twos facts, the network was tested for its knowledge on both the ones and twos addition facts. Like the ones facts, the twos facts were readily learned by the network. However, McCloskey and Cohen noted the network was no longer able to properly answer the ones addition problems even after one learning trial of the twos addition problems. The output pattern produced in response to the ones facts often resembled an output pattern for an incorrect number more closely than the output pattern for an incorrect number. This is considered to be a drastic amount of error. Furthermore, the problems 2+1 and 2+1, which were included in both training sets, even showed dramatic disruption during the first learning trials of the twos facts.\n\nIn their second connectionist model, McCloskey and Cohen attempted to replicate the study on retroactive interference in humans by Barnes and Underwood (1959). They trained the model on A-B and A-C lists and used a context pattern in the input vector (input pattern), to differentiate between the lists. Specifically the network was trained to responds with the right B response when shown the A stimulus and A-B context pattern and to respond with the correct C response when shown the A stimulus and the A-C context pattern. When the model was trained concurrently on the A-B and A-C items then the network readily learned all of the associations correctly. In sequential training the A-B list was trained first, followed by the A-C list. After each presentation of the A-C list, performance was measured for both the A-B and A-C lists. They found that the amount of training on the A-C list in Barnes and Underwood study that lead to 50% correct responses, lead to nearly 0% correct responses by the backpropagation network. Furthermore, they found that the network tended to show responses that looked like the C response pattern when the network was prompted to give the B response pattern. This indicated that the A-C list apparently had overwritten the A-B list. This could be likened to learning the word dog, followed by learning the word stool and then finding that you cannot recognize the word cat well but instead think of the word stool when presented with the word dog.\n\nMcCloskey and Cohen tried to reduce interference through a number of manipulations including changing the number of hidden units, changing the value of the learning rate parameter, overtraining on the A-B list, freezing certain connection weights, changing target values 0 and 1 instead 0.1 and 0.9. However none of these manipulations satisfactorily reduced the catastrophic interference exhibited by the networks.\n\nOverall, McCloskey and Cohen (1989) concluded that: \n\nRatcliff (1990) used multiple sets of backpropagation models applied to standard recognition memory procedures, in which the items were sequentially learned. After inspecting the recognition performance models he found two major problems:\nEven one learning trial with new information resulted in a significant loss of the old information, paralleling the findings of McCloskey and Cohen (1989). Ratcliff also found that the resulting outputs were often a blend of the previous input and the new input. In larger networks, items learned in groups (e.g. AB then CD) were more resistant to forgetting than were items learned singly (e.g. A then B then C…). However, the forgetting for items learned in groups was still large. Adding new hidden units to the network did not reduce interference. \nThis finding contradicts with studies on human memory, which indicated that discrimination increases with learning. Ratcliff attempted to alleviate this problem by adding 'response nodes' that would selectively respond to old and new inputs. However, this method did not work as these response nodes would become active for all inputs. A model which used a context pattern also failed to increase discrimination between new and old items.\n\nMany researchers have suggested that the main cause of catastrophic interference is overlap in the representations at the hidden layer of distributed neural networks. In a distributed representation any given input will tend to create changes in the weights to many of the nodes. Catastrophic forgetting occurs because when many of the weights, where 'knowledge is stored, are changed it is impossible for prior knowledge to be kept intact. During sequential learning, the inputs become mixed with the new input being superimposed over top of the old input. Another way to conceptualize this is through visualizing learning as movement through a weight space. This weight space can be likened to a spatial representation of all of the possible combinations of weights that the network can possess. When a network first learns to represent a set of patterns, it has found a point in weight space which allows it to recognize all of the patterns that it has seen. However, when the network learns a new set of patterns sequentially it will move to a place in the weight space that allows it to only recognize the new pattern. To recognize both sets of patterns, the network must find a place in weight space that can represent both the new and the old output. One way to do this is by connecting a hidden unit to only a subset of the input units. This reduces the likelihood that two different inputs will be encoded by the same hidden units and weights, and so will decrease the chance of interference. Indeed, a number of the proposed solutions to catastrophic interference involve reducing the amount of overlap that occurs when storing information in these weights.\n\nMany of the early techniques in reducing representational overlap involved making either the input vectors or the hidden unit activation patterns orthogonal to one another. Lewandowsky and Li (1995) noted that the interference between sequentially learned patterns is minimized if the input vectors are orthogonal to each other. Input vectors are said to be orthogonal to each other if the pairwise product of their elements across the two vectors sum to zero. For example, the patterns [0,0,1,0] and [0,1,0,0] are said to be orthogonal because (0×0 + 0×1 + 1×0 + 0×0) = 0. One of the techniques which can create orthogonal representations at the hidden layers involves bipolar feature coding (i.e., coding using -1 and 1 rather than 0 and 1). Orthogonal patterns tend to produce less interference with each other. However, not all learning problems can be represented using these types of vectors and some studies report that the degree of interference is still problematic with orthogonal vectors. Simple techniques such as varying the learning rate parameters in the backpropagation equation were not successful in reducing interference. Varying the number of hidden nodes has also been used to try and reduce interference. However, the findings have been mixed, with some studies finding that more hidden units decrease interference and other studies finding it does not.\n\nBelow are a number of techniques which have empirical support in successfully reducing catastrophic interference in backpropagation neural networks:\n\nFrench (1991) proposed that catastrophic interference arises in feedforward backpropagation networks due to the interaction of node activations, or activation overlap, that occur in distributed representations at the hidden layer. Specifically, he defined this activation overlap as the average shared activation over all units in the hidden layer, calculated by summing the lowest activation of the nodes at the hidden layer and averaging this sum. For example, if the activations at the hidden layer from one input are (0.3, 0.1, 0.9, 1.0) and the activations from the next input are (0.0, 0.9, 0.1, 0.9) the activation overlap would be (0.0 + 0.1 + 0.1 + 0.9 ) / 4 = 0.275. When using [binary number|binary] representation of input [row vector|vectors], activation values will be 0 through 1, where 0 indicates no activation overlap and 1 indicates full activation overlap. French noted that neural networks which employ very localized representations do not show catastrophic interference because of the lack of overlap at the hidden layer. That is to say, each input pattern will create a hidden layer representation that involves the activation of only one node, so differed inputs will have an activation overlap of 0. Thus, he suggested that reducing the value of activation overlap at the hidden layer would reduce catastrophic interference in distributed networks. Specifically he proposed that this could be done through changing the distributed representations at the hidden layer to 'semi-distributed' representations. A 'semi-distributed' representation has fewer hidden nodes that are active, and/or a lower activation value for these nodes, for each representation, which will make the representations of the different inputs overlap less at the hidden layer. French recommended that this could be done through 'activation sharpening', a technique which slightly increases the activation of a certain number of the most active nodes in the hidden layer, slightly reduces the activation of all the other units and then changes the input-to-hidden layer weights to reflect these activation changes (similar to error backpropgation). Overall the guidelines for the process of 'activation sharpening' are as follows:\n\n\nIn his tests of an 8-8-8 (input-hidden-output) node backpropagation network where one node was sharpened, French found that this sharpening paradigm did result in one node being much more active than the other seven. Moreover, when sharpened, this network took one fourth the time to relearn the initial inputs than a standard backpropagation without node sharpening. Relearning is a measure of memory savings and thus extent of forgetting, where more time to relearn suggests more forgetting (Ebbinghaus savings method). A two-node sharpened network performed even slightly better, however if more than two nodes were sharpened forgetting increased again.\n\nAccording to French, the sharpened activations interfere less with weights in the network than unsharpened weights and this is due specifically to the way that backpropagation algorithm calculates weight changes. Activations near 0 will change the weights of links less than activations near 1. Consequently, when there are many nodes with low activations (due to sharpening), the weights to and from these nodes will be modified much less than the weights on very active nodes. As a result, when a new input is fed into the network, sharpening will reduce activation overlap by limiting the number of highly active hidden units and will reduce the likelihood of representational overlap by reducing the number of weights that are to be changed. Thus, node sharpening will decrease the amount of disruption in the old weights, which store prior input patterns, thereby reducing the likelihood of catastrophic forgetting.\n\nKortge (1990) proposed a learning rule for training neural networks, called the 'novelty rule', to help alleviate catastrophic interference. As its name suggests, this rule helps the neural network to learn only the components of a new input that differ from an old input. Consequently, the novelty rule changes only the weights that were not previously dedicated to storing information, thereby reducing the overlap in representations at the hidden units. Thus, even when inputs are somewhat similar to another, dissimilar representations can be made at the hidden layer. In order to apply the novelty rule, during learning the input pattern is replaced by a novelty vector that represents the components that differ. The novelty vector for the first layer (input units to hidden units) is determined by taking the target pattern away from the current output of the network (the delta rule). For the second layer (hidden units to output units) the novelty vector is simply the activation of the hidden units that resulted from using the novelty vector as an input through the first layer. Weight changes in the network are computed by using a modified delta rule with the novelty vector replacing the activation value (sum of the inputs):\n\nWhen the novelty rule is used in a standard backpropagation network there is no, or lessened, forgetting of old items when new items are presented sequentially. However, this rule can only apply to auto-encoder or auto-associative networks, in which the target response for the output layer is identical to the input pattern. This is because the novelty vector would be meaningless if the desired output was not identical to the input as it would be impossible to calculate how much a new input differed from the old input.\n\nMcRae and Hetherington (1993) argued that humans, unlike most neural networks, do not take on new learning tasks with a random set of weights. Rather, people tend to bring a wealth of prior knowledge to a task and this helps to avoid the problem of interference. They proposed that when a network is pre-trained on a random sample of data prior to starting a sequential learning task that this prior knowledge will naturally constrain how the new information can be incorporated. This would occur because a random sample of data from a domain which has a high degree of internal structure, such as the English language, training would capture the regularities, or recurring patterns, found within that domain. Since the domain is based on regularities, a newly learned item will tend to be similar to the previously learned information, which will allow the network to incorporate new data with little interference with existing data. Specifically, an input vector which follows the same pattern of regularities as the previously trained data should not cause a drastically different pattern of activation at the hidden layer or drastically alter weights.\n\nTo test their hypothesis, McRae and Hetherington (1993) compared the performance of a naïve and pre-trained auto-encoder backpropagation network on three simulations of verbal learning tasks. The pre-trained network was trained using letter based representations of English monosyllabic words or English word pairs. All three tasks involved the learning of some consonant-vowel-consonant (CVC) strings or CVC pairs (list A), followed by training on a second list of these items (list B). Afterwards, the distributions of the hidden node activations were compared between the naïve and pre-trained network. In all three tasks, the representations of a CVC in the naïve network tended to be spread fairly evenly across all hidden nodes, whereas most hidden nodes were inactive in the pre-trained network. Furthermore, in the pre-trained network the representational overlap between CVCs was reduced compared to the naïve network. The pre-trained network also retained some similarity information as the representational overlap between similar CVCs, like \"JEP\" and \"ZEP\", was greater than for dissimilar CVCs, such as \"JEP\" and \"YUG\". This suggests that the pre-trained network had a better ability to generalize, i.e. notice the patterns, than the naïve network. Most importantly, this reduction in hidden unit activation and representational overlap resulted in significantly less forgetting in the pre-trained network than the naïve network, essentially eliminating catastrophic interference. Essentially, the pre-training acted to create internal orthogonalization of the activations at the hidden layer, which reduced interference. Thus, pre-training is a simple way to reduce catastrophic forgetting in standard backpropagation networks.\n\nFrench (1997) proposed the idea of a pseudo-recurrent backpropagation network in order to help reduce catastrophic interference (see Figure 2). In this model the network is separated into two functionally distinct but interacting sub-networks. This model is biologically inspired and is based on research from McClelland, McNaughton, and O'Reilly (1995). In this research McClelland et al. (1995), suggested that the hippocampus and neocortex act as separable but complementary memory systems. Specifically, the hippocampus short term memory storage and acts gradually over time to transfer memories into the neocortex for long term memory storage. They suggest that the information that is stored can be \"brought back\" to the hippocampus during active rehearsal, reminiscence, and sleep and renewed activation is what acts to transfer the information to the neocortex over time. In the pseudo-recurrent network, one of the sub-networks acts as an early processing area, akin to the hippocampus, and functions to learn new input patters. The other sub-network acts as a final-storage area, akin to the neocortex. However, unlike in McClelland et al. (1995) model, the final-storage area sends internally generated representation back to the early processing area. This creates a recurrent network. French proposed that this interleaving of old representations with new representations is the only way to reduce radical forgetting. Since the brain would most likely not have access to the original input patterns, the patterns that would be fed back to the neocortex would be internally generated representations called \"pseudopatterns\". These pseudopatterns are approximations of previous inputs and they can be interleaved with the learning of new inputs. The use of these pseudopatterns could be biologically plausible as parallels between the consolidation of learning that occurs during sleep and the use of interleaved pseudopatterns. Specifically, they both serve to integrate new information with old information without disruption of the old information. When given an input (and a teacher value) is fed into the pseudo-recurrent network would act as follows:\n\n\nWhen tested on sequential learning of real world patterns, categorization of edible and poisonous mushrooms, the pseudo-recurrent network was shown less interference than a standard backpropagation network. This improvement was with both memory savings and exact recognition of old patterns. When the activation patterns of the pseudo-recurrent network were investigated, it was shown that this network automatically formed semi-distributed representations. Since these types of representations involve fewer nodes being activated for each pattern, it is likely what helped to reduce interference.\n\nNot only did the pseudo-recurrent model show reduced interference but also it models list-length and list-strength effects seen in humans. The list-length effect means that adding new items to a list harms the memory of earlier items. Like humans, the pseudo recurrent network showed a more gradual forgetting when to be trained list is lengthened. The list-strength effect means that when the strength of recognition for one item is increased, there is no effect on the recognition of the other list items. This is an important finding as other models often exhibit a decrease in the recognition of other list items when one list item is strengthened. Since the direct copying of weights from the early processing area to the final storage area does not seem highly biologically plausible, the transfer of information to the final storage area can be done through training the final storage area with pseudopatterns created by the early processing area. However, a disadvantage of the pseudo-recurrent model is that the number of hidden units in the early processing and final storage sub-networks must be identical.\n\nFollowing the same basic idea contributed by Robins, Ans and Rousset (1997) have also proposed a two-network artificial neural architecture with \"memory self-refreshing\" that overcomes catastrophic interference when sequential learning tasks are carried out in distributed networks trained by backpropagation. The principle is to interleave, at the time when new external patterns are learned, those to-be-learned new external patterns with internally generated pseudopatterns, or 'pseudo-memories', that reflect the previously learned information. What mainly distinguishes this model from those that use classical pseudorehearsal in feedforward multilayer networks is a \"reverberating\" process that is used for generating pseudopatterns. This process which, after a number of activity re-injections from a single random seed, tends to go up to nonlinear network \"attractors\", is more suitable for optimally capturing the deep structure of previously learned knowledge than a single feedforward pass of random activation. Ans and Rousset (2000) have shown that the learning mechanism they proposed avoiding catastrophic forgetting, provides a more appropriate way to deal with knowledge transfer as measured by learning speed, ability to generalize and vulnerability to network damages. Musca, Rousset and Ans (2009) have also shown that pseudopatterns originating from an artificial reverberating neural network could induce familiarity in humans with never seen items in the way predicted by simulations conducted with a two-network artificial neural architecture. Furthermore, Ans (2004) has implemented a version of the self-refreshing mechanism using only one network trained by the Contrastive Hebbian Learning rule, a training rule considered as more realistic than the largely used backpropagation algorithm, but fortunately equivalent to the latter.\n\nSo far, the different solutions to catastrophic interference that have been presented concern tasks of sequential learning involving only non-temporally ordered lists of items. But, to be credible, the self-refreshing mechanism for 'static' learning has to encompass our human ability to learn serially many temporal sequences of patterns without catastrophic interference (e.g. learning one song followed by learning a second song without forgetting the first one). This was done by Ans, Rousset, French and Musca (2004) who have presented, in addition to simulation work, an experiment that evidences a close similarity between the behaviour of humans and the behaviour of the proposed neuromimetic architecture.\n\nLatent Learning is a technique used by Gutstein & Stump (2015) both to mitigate catastrophic interference and to take advantage of transfer learning. Rather than manipulating the representations for new classes used by the hidden nodes, this approach tries to train optimal representations for new classes into the output nodes. It chooses output encodings that are least likely to catastrophically interfere with existing responses.\n\nGiven a net that has learned to discriminate among one set of classes using Error Correcting Output Codes (ECOC) (as opposed to 1 hot codes), optimal encodings for new classes are chosen by observing the net's average responses to them. Since these average responses arose while learning the original set of classes \"without any exposure to the new classes\", they are referred to as 'Latently Learned Encodings'. This terminology borrows from the concept of Latent Learning, as introduced by Tolman in 1930. In effect, this technique uses transfer learning to avoid catastrophic interference, by making a net's responses to new classes as consistent as possible with existing responses to classes already learned.\n\nKirkpatrick et al. (2017) demonstrated a method to train a single artificial neural network on multiple tasks using a technique called elastic weight consolidation.\n\nPractopoietic theory proposes that biological systems solve the problem of catastrophic interference by storing long-term memories only in a general form, not applicable to a given situation but instead loosely applicable to a class of different situations. In order to adjust the loosely applicable knowledge to the given current situation, the process of \"anapoiesis\" is applied. Anapoiesis stands for \"reconstruction of knowledge\"—transforming knowledge from a general form to a specific one. Practopoietic theory is founded in the theorems of cybernetics and is concerned with the question of how cybernetic systems obtain their capabilities to control and act.\n",
    "id": "39182554",
    "title": "Catastrophic interference"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=39278567",
    "text": "Computer Arimaa\n\nComputer Arimaa refers to the playing of the board game Arimaa by computer programs.\n\nIn 2002, Indian American computer engineer Omar Syed published the rules to Arimaa and announced a $10,000 prize, available annually until 2020, for the first computer program (running on standard, off-the-shelf hardware) able to defeat each of three top-ranked human players in a three-game series. The prize was claimed in 2015, when a computer program played 7:2 against three human players. The game has been the subject of several research papers.\n\nThe number of different ways that each player can set up their pieces at the beginning of the game is:\n\nformula_1\n\nThe player can put 8 rabbits on 16 possible squares, followed by 2 cats on the 8 remaining squares, 2 dogs on the 6 remaining squares, 2 horses on the four remaining squares, one camel on one of the two remaining squares, and the elephant on the final unused square.\n\nBecause each player can start the game with one of 64,864,800 opening setups, the total state space for the opening is:\n\nformula_2\n\nAs Christ-Jan Cox said in his Master's thesis, because the number of possible initial states is so large, \"[i]t follows that it is very difficult to develop complete databases of opening moves.\"\n\nIt is important for the computer to be able to evaluate the value of the pieces on the board so it can assess whether or not a capture or exchange would be desirable. Assessing the relative value of pieces is an area of ongoing Arimaa research. Some currently-used systems are DAPE and FAME.\n\nThe following techniques are used by some or all of the artificial intelligence programs that play Arimaa:\n\n\nSeveral aspects of Arimaa make it difficult for computer programs to beat good human players. Because so much effort has gone into the development of strong chess-playing software, it is particularly relevant to understand why techniques applicable to chess are less effective for Arimaa.\n\nTop chess programs use brute-force searching coupled with static position evaluation dominated by material considerations. Chess programs examine many, many possible moves, but they are not good (compared to humans) at determining who is winning at the end of a series of moves unless one side has more pieces than the other. The same is true for Arimaa programs, but their results are not as good in practice.\n\nWhen brute-force searching is applied to Arimaa, the depth of the search is limited by the huge number of options each player has on each turn. Computationally, the number of options a player has available to them governs the number of different paths play can go down. This is known as the branching factor. The average branching factor in a game of Chess is about 35, whereas in Arimaa it is about 17,000.\n\nThese differing branching factors imply that a computer which can search to a depth of eight turns for each player in chess, can only search about three turns deep for each player in Arimaa:\n\nformula_3\n\nBrute force search depth, for chess software, is nearly doubled by alpha-beta pruning, which allows the software to conclude that one move is better than another without examining every possible continuation of the weaker move. If the opponent can crush a certain move with one reply, it isn't necessary to examine other replies, which dramatically increases search speed. In Arimaa, however, the side to move switches only every four steps, which reduces the number of available cutoffs in a step-based search.\n\nFurthermore, the usefulness of alpha-beta pruning is heavily dependent on the order in which moves are considered. Good moves must be considered before bad ones in order for the bad ones to be neglected. In particular, checking and capturing moves are key for pruning, because they are often much better than other moves. In Arimaa software the speedup provided by alpha-beta pruning is less, because captures are rarer. In rated games played on arimaa.com, only 3% of steps result in capture, compared to about 19% of chess moves that result in capture.\n\nIn most Arimaa positions, particularly toward the beginning of the game when the board is still crowded, a competent player can avoid losing any pieces within the next two turns. Compared to chess, Arimaa allows either player to delay captures for longer. Indeed, the median move number of the first capture in chess is turn 6, whereas in Arimaa it is turn 12. The struggle is initially more positional in Arimaa, and revolves around making captures unavoidable at some point in the future. This magnifies the importance of correctly judging who is gaining ground in non-material ways. Thus the strength of computer programs (examining millions of positions) is not as significant as their weakness (judging the position apart from who has more pieces).\n\nThe weakness of Arimaa programs in the opening phases is further magnified by the setup phase. In chess every game starts from the same position. By compiling before the game a list of stock replies to all standard opening moves, chess programs may often make a dozen or more excellent moves before starting to \"think\". Humans do the same, but have a smaller and less reliable memory of openings, which puts humans at a relative disadvantage in chess. Arimaa, in contrast, has millions of possible ways to set up the pieces even before the first piece moves. This prevents programs from having any meaningful opening book.\n\nAs the game progresses, exchanges and the advancement of rabbits tend to make the position more open and tactical. Arimaa programs typically play better in this sort of position, because they see tactical shots which humans overlook. However, it is usually possible for humans to avoid wide-open positions by conservative play, and to angle for strategic positions in which computers fare worse. Against a conservative opponent it is almost impossible to bust open the position in Arimaa, whereas in chess it is merely difficult. One must beat defensive play by the accumulation of small, long-term advantages, which programs do not do very well.\n\nOne additional technique from computer chess which does not apply to Arimaa is endgame tablebases. Master-level chess games sometimes trade down into unclear endgames with only a few pieces, for example king and knight vs. king and rook. It is possible to build, by retrograde analysis, an exhaustive table of the correct move in all such positions. Programs have only to consult a pre-generated table in such positions, rather than \"thinking\" afresh, which gives them a relative advantage over humans. Arimaa, in contrast, seldom comes to an endgame. Equal exchanges of pieces are less common than in chess, so it is rare for a game of Arimaa to \"trade down\" and still be unclear. An average game of Arimaa has only eight captures (compared to seventeen for chess), and top humans can often defeat top programs in Arimaa without losing a single piece, for example the second game of the 2014 Challenge match. Another example of low capture density is this semifinal game of the 2012 World Championship, featuring only a single capture, a goal-forcing elephant sacrifice.\n\nOmar Syed hopes that, because traditional artificial intelligence techniques are only moderately effective for Arimaa, programmers will be forced to use new artificial intelligence techniques to create a strong Arimaa-playing program. The successful quest to build a world-championship-caliber chess program has produced many techniques to successfully play games, but has contributed essentially nothing to more general reasoning; in fact, the techniques of chess playing programs have been excluded from some definitions of artificial intelligence; a goal for Arimaa is that the techniques involved in playing it will help the larger goals of artificial intelligence.\n\nThe structure of Syed's man-against-machine challenge is focused on rewarding advances in AI software and not advances in hardware. In the annual challenge, programs are run on machines chosen and provided by Syed himself, under the criterion that it be a typical, inexpensive, off-the-shelf home computer. The challenge would not be open to anyone requiring expensive multi-processor machines such as those used to challenge top-level chess players, much less something like the custom-built supercomputer Deep Blue, even though it was the success of this hardware-intensive approach which inspired Arimaa's invention. Syed believes that even the computer used in the 2004 challenge match (a Pentium 4 2.4 GHz system with 512 MB of RAM) had sufficient hardware to win the challenge prize if only it was running the proper software. Supercomputers might already have the power to conquer Arimaa by brute force using conventional AI software, and eventually personal computers will too, if hardware continues to advance at the current rate. This is why the Arimaa challenge prize is offered only until the year 2020.\n\nIt has been argued that a computer has beaten the world chess champion but not beaten the human in the Arimaa Challenge (until 2015) because of six reasons:\n\n\nHowever, the Arimaa community disputes this argument point by point. To the first point, Arimaa is a new game, so the playing community is still small and even the best players are not professional players and have only been playing the game for a few years. Thus the human players in the Arimaa Challenge are much weaker than the human players in the chess challenge. The weakness of human players should make the Arimaa Challenge easier to conquer than chess, which compensates developers for having studied the problem for a shorter time.\n\nThe remaining five points compare the Arimaa Challenge only to Kasparov vs. Deep Blue, ignoring all other man vs. machine chess matches in which computers have prevailed. The chess match which can most closely be compared to the Arimaa Challenge match is the Man vs Machine World Team Championship. In 2004 and 2005 a team of humans played against a team of computer opponents. In both years the computers won by wide margin. In 2005 all three humans lost, the computers won 2/3 of the total points, the chess engines were commercially available for the humans to study, and the machine hardware used was not a supercomputer, but rather comparable to hardware used in the Arimaa Challenge.\n\nMan-vs.-machine chess matches since 2005 have shown increasing computer dominance. For example, the 2006 Deep Fritz vs. Vladimir Kramnik and 2007 Rybka vs. Jaan Ehlvest matches gave additional advantages to the human player, but the computers (running on commodity hardware) prevailed anyway.\n\nThe Arimaa Engine Interface, developed by Brian Haskin, defines a protocol that allows an Arimaa engine to communicate with a controller.\n\nAccording to the documentation: \"An engine is a program capable of taking the state of an Arimaa game and selecting a legal move to make. A controller is anything that wants to communicate with and control an engine. This could be anything from a simple script to have the engine analyse a single position to a GUI program that allows games to be played with humans or other engines.\"\n\nThe Arimaa Engine Interface includes an implementation of an engine and controller, documentation, and various scripts to control the engine and play games on any website which supports the protocol, including the official Arimaa website.\n\n",
    "id": "39278567",
    "title": "Computer Arimaa"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=39903059",
    "text": "Kuwahara filter\n\nThe Kuwahara filter is a non-linear smoothing filter used in image processing for adaptive noise reduction. Most filters that are used for image smoothing are linear low-pass filters that effectively reduce noise but also blur out the edges. However the Kuwahara filter is able to apply smoothing on the image while preserving the edges.\n\nSuppose that formula_1 is a grey scale image and that we take a square window of size formula_2 centered around a point formula_3 in the image. This square can be divided into four smaller square regions formula_4 each of which will be \n\nwhere formula_6 is the cartesian product. It must be noted that pixels located on the borders between two regions belong to both regions so there is a slight overlap between subregions.\n\nThe arithmetic mean formula_7 and standard deviation formula_8 of the four regions centered around a pixel (x,y) are calculated and used to determine the value of the central pixel. The output of the Kuwahara filter formula_9 for any point formula_3 is then given by \nformula_11\n\nThis means that the central pixel will take the mean value of the area that is most homogenous. The location of the pixel in relation to an edge plays a great role in determining which region will have the greater standard deviation. If for example the pixel is located on a dark side of an edge it will most probably take the mean value of the dark region. On the other hand, should the pixel be on the lighter side of an edge it will most probably take a light value. On the event that the pixel is located on the edge it will take the value of the more smooth, least textured region. The fact that the filter takes into account the homogeneity of the regions ensures that it will preserve the edges while using the mean creates the blurring effect.\n\nSimilarly to the Median filter the Kuwahara filter uses a sliding window approach to access every pixel in the image. The size of the window is chosen in advance and may vary depending on the desired level of blur in the final image. Bigger windows typically result in the creation of more abstract images whereas small windows produce images that retain their detail. Typically windows are chose to be square with sides that have an odd number of pixels for symmetry. However, there are variations of the Kuwahara filter that use rectangular windows. Additionally, the subregions do not need to overlap or have the same size as long as they cover all of the window.\n\nObviously the Normal filter can't be used for color images by applying the filter to each RGB channel separately and then using the three resulting channels to compose the image. The main problem with this is that the sub regions will have different variances for each of the channels. For example, a region with the lowest variance in the red channel might have the highest variance in the green channel. This once again causes ambiguity which would result in the color of the central pixel to be determined by several regions, which might also result in blurrier edges.\n\nTo overcome this problem in colored images a slightly modified Kuwahara filter must be used. This filter must also take into account the \"brightness\" (the Value coordinate in the HSV color model) of each pixel in the region. This time only the variance of the \"brightness\" of each subregion is calculated along with the mean for each color channel. The filter will produce an output for each channel which will correspond to the mean of that channel for the subregion with the lowest variance in \"brightness\". This ensures that only one region will determine the RGB values of the central pixel.\n\nOriginally the Kuwahara filter was proposed for use in processing RI-angiocardiographic images of the cardiovascular system. The fact that any edges are preserved when smoothing makes it especially useful for feature extraction and segmentation and explains why it is used in medical imaging. \nThe Kuwahara filter however also finds many applications in artistic imaging and fine-art photography due to its ability to remove textures and sharpen the edges of photographs. The level of abstraction helps create a desirable painting-like effect in artistic photographs especially in the case of the colored image version of the filter. These applications have known great success and have encouraged similar research in the field of image processing for the arts.\n\nAlthough the vast majority of applications have been in the field of image processing there have been cases that use modifications of the Kuwahara filter for machine learning tasks such as clustering.\n\nThe Kuwahara filter has been implemented in CVIPtools.\n\nThe Kuwahara filter despite its capabilities in edge preservation has certain drawbacks.\n\n\nThe success of the Kuwahara filter has spurred an increase the development of edge-enhancing smoothing filteres. Several variations have been proposed for similar use most of which attempt to deal with the drawbacks of the original Kuwahara filter.\n\nThe \"Generalized Kuwahara filter\" proposed by P. Bakker considers several windows that contain a fixed pixel. Each window is then assigned an estimate and a confidence value. The value of the fixed pixel then takes the value of the estimate of the window with the highest confidence. This filter is not characterized by the same ambiguity in the presence of noise and manages to eliminate the block artifacts.\n\nThe \"Mean of Least Variance\"(MLV) filter, proposed by M.A. Schulze also produces edge-enhancing smoothing results in images. Similarly to the Kuwahara filter it assumes a window of size formula_14 but instead of searching amongst four subregions of size formula_15 for the one with minimum variance it searches amongst all possible formula_16 subregions. This means the central pixel of the window will be assigned the mean of the one subregion out of a possible formula_17 that has the smallest variance.\n\nA more recent attempt in edge-enhancing smoothing was also proposed by J. E. Kyprianidis. The filter's output is a weighed sum of the local averages with more weight given the averages of more homogenous regions.\n\n",
    "id": "39903059",
    "title": "Kuwahara filter"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1467948",
    "text": "Problem solving\n\nProblem solving consists of using generic or \"ad hoc\" methods, in an orderly manner, for finding solutions to problems. Some of the problem-solving techniques developed and used in artificial intelligence, computer science, engineering, mathematics, or medicine are related to mental problem-solving techniques studied in psychology.\n\nThe term \"problem solving\" is used in many, many, many disciplines, sometimes with different perspectives, visuals and often with different terminologies. For instance, it is a mental process in psychology and a computerized process in computer science. Problems can also be classified into two different types (ill-defined and well-defined) from which appropriate solutions are to be made. Ill-defined problems are those that do not have clear goals, solution paths, or expected solution. Well-defined problems have specific goals, clearly defined solution paths, and clear expected solutions. These problems also allow for more initial planning than ill-defined problems. Being able to solve problems sometimes involves dealing with pragmatics (logic) and semantics (interpretation of the problem). The ability to understand what the goal of the problem is and what rules could be applied represent the key to solving the problem. Sometimes the problem requires some abstract thinking and coming up with a creative solution.\n\nThomas J. D'Zurilla in 1988 defined problem solving as a “cognitive–affective–behavioral process through which an individual (or group) attempts to identify, discover, or invent effective means of coping with problems encountered in every day living”. It is an evolutionary drive for living organisms and an important coping skill for dealing with a variety of concerns. Problem solving specifically in psychology refers to a state of desire for reaching a definite 'goal' from a present condition that either is not directly moving toward the goal, is far from it, or needs more complex logic for finding a missing description of conditions or steps toward the goal. In each case \"where you want to be\" is an imagined (or written) state in which you would like to be and the solutions are situation- or context-specific. This process includes problem finding or 'problem analysis', problem shaping, generating alternative strategies, implementation and verification of the selected solution. Distinguished feature of a problem is that there is a \"goal\" to be reached and how you get there depends upon problem orientation (problem-solving coping style and skills) and systematic analysis. The nature of human problem solving processes and methods is a field of study and work for mental health professionals. Methods of studying problem solving include introspection, behaviorism, simulation, computer modeling, and experiment. Social psychologists look into the person-environment relationship aspect of the problem and independent and interdependent problem-solving methods. Problem solving has been defined as a higher-order cognitive process and intellectual function that requires the modulation and control of more routine or fundamental skills.\n\nProblem solving has two major domains: mathematical problem solving and personal problem solving both are seen in terms of some difficulty or barrier is encountered. Empirical researches show that self-interest and interpersonal skills; collaborative and instrumental problem approach (it helps in reflective and expansive understanding of the problem situation and its preferable outcome); strategy fluency (the number and diversity of strategies) and conceptual clarity that can lead to an action-identification (Vallacher & Wegner, 1987); temporal lifespan perspective that lead to selectivity in strategy (problem focused and emotion focused strategies); self-efficacy and problem familiarity; formation of 'carry over' relationships (egalitarian friendship, romantic ties, cliques, hygge's, etc.) that helps individuals mutually move through life and provide a sense of identity (Antonucci, Birditt, & Ajrouch, 2011); negotiation; type of relationships (obligatory vs. voluntary); gender typing; problem focused and emotion focused strategies as some strategies and factors that influence everyday problem solving. Neuropsychologists have studied that individuals with frontal lobe injuries with deficits in emotional control and reasoning can be remediated with effective rehabilitation and could improve the capacity of injured persons to resolve everyday problems (Rath, Simon, Langenbahn, Sherr, & Diller, 2003).\n\nInterpersonal everyday problem solving is dependent upon the individual personal motivational and contextual components. One such component is the emotional valence of \"real-world\" problems and it can either impede or aid problem-solving performance. Researchers have focused on the role of emotions in problem solving (D'Zurilla & Goldfried, 1971; D'Zurilla & Nezu, 1982), demonstrating that poor emotional control can disrupt focus on the target task and impede problem resolution and likely lead to negative outcomes such as fatigue, depression, and inertia (Rath, Langenbahn, Simon, Sherr, & Diller, 2004). In conceptualization, human problem solving consists of two related processes: problem orientation, the motivational/attitudinal/affective approach to problematic situations and problem-solving skills. Studies conclude people's strategies cohere with their goals (Hoppmann & Blanchard-Fields, 2010, Berg et al., 1998) and they are stemmed from the natural process of comparing oneself with others (Sonstegard and Bitter, 1998).\n\nThe early experimental work of the Gestaltists in Germany placed the beginning of problem solving study (e.g., Karl Duncker in 1935 with his book \"The psychology of productive thinking\"). Later this experimental work continued through the 1960s and early 1970s with research conducted on relatively simple (but novel for participants) laboratory tasks of problem solving. Choosing simple novel tasks was based on the clearly defined optimal solutions and their short time for solving, which made it possible for the researchers to trace participants' steps in problem-solving process. Researchers' underlying assumption was that simple tasks such as the Tower of Hanoi correspond to the main properties of \"real world\" problems and thus the characteristic cognitive processes within participants' attempts to solve simple problems are the same for \"real world\" problems too; simple problems were used for reasons of convenience and with the expectation that thought generalizations to more complex problems would become possible. Perhaps the best-known and most impressive example of this line of research is the work by Allen Newell and Herbert A. Simon. Other experts have shown that the principle of decomposition improves the ability of the problem solver to make good judgment.\n\nIn computer science and in the part of artificial intelligence that deals with algorithms (\"algorithmics\"), problem solving encompasses a number of techniques known as algorithms, heuristics, root cause analysis, etc. In these disciplines, problem solving is part of a larger process that encompasses problem determination, de-duplication, analysis, diagnosis, repair, etc.\n\nProblem solving is used in when products or processes fail, so corrective action can be taken to prevent further failures. It can also be applied to a product or process prior to an actual fail event, i.e., when a potential problem can be predicted and analyzed, and mitigation applied so the problem never actually occurs. Techniques such as Failure Mode Effects Analysis can be used to proactively reduce the likelihood of problems occurring.\n\nIn military science, problem solving is linked to the concept of \"end-states\", the desired condition or situation that strategists wish to generate. The ability to solve problems is important at any military rank, but is highly critical at the command and control level, where it is strictly correlated to the deep understanding of qualitative and quantitative scenarios. \"Effectiveness\" of problem solving is \"a criterion used to assess changes in system behavior, capability, or operational environment that is tied to measuring the attainment of an end state, achievement of an objective, or creation of an effect\". \"Planning\" for problem-solving is a \"process that determines and describes how to employ 'means' in specific 'ways' to achieve 'ends' (the problem's solution).\"\n\nForensic engineering is an important technique of failure analysis that involves tracing product defects and flaws. Corrective action can then be taken to prevent further failures.\n\nReverse engineering attempts to discover the original problem-solving logic used in developing a product by taking it apart.\n\nOther problem solving tools are linear and nonlinear programming, queuing systems, and simulation.\n\nProblem-solving strategies are the steps that one would use to find the problem(s) that are in the way to getting to one's own goal. Firend's problem solving model (PSM) is practical in application and incorporates the conventional 5WH approach, with a systematic process of investigation, implementation and assessment cycle. Some would refer to this as the \"problem-solving cycle\" (Bransford & Stein, 1993). In this cycle one will recognize the problem, define the problem, develop a strategy to fix the problem, organize the knowledge of the problem cycle, figure out the resources at the user's disposal, monitor one's progress, and evaluate the solution for accuracy. The reason it is called a cycle is that once one is completed with a problem another usually will pop up.\n\nBlanchard-Fields (2007) looks at problem solving from one of two facets. The first looking at those problems that only have one solution (like mathematical problems, or fact-based questions) which are grounded in psychometric intelligence. The other that is socioemotional in nature and are unpredictable with answers that are constantly changing (like what's your favorite color or what you should get someone for Christmas).\n\nThe following techniques are usually called \"problem-solving strategies'\"\n\n\n\nCommon barriers to problem solving are mental constructs that impede our ability to correctly solve problems. These barriers prevent people from solving problems in the most efficient manner possible. Five of the most common processes and factors that researchers have identified as barriers to problem solving are confirmation bias, mental set, functional fixedness, unnecessary constraints, and irrelevant information.\n\nWithin the field of science there exists a set of fundamental standards, the scientific method, which outlines the process of discovering facts or truths about the world through unbiased consideration of all pertinent information and through impartial observation of and/or experimentation with that information. According to this method, one is able to most accurately find a solution to a perceived problem by performing the aforementioned steps. The scientific method does not prescribe a process that is limited to scientists, but rather one that all people can practice in their respective fields of work as well as in their personal lives. Confirmation bias can be described as one's unconscious or unintentional corruption of the scientific method. Thus when one demonstrates confirmation bias, one is formally or informally collecting data and then subsequently observing and experimenting with that data in such a way that favors a preconceived notion that may or may not have \"motivation\". Research has found that professionals within scientific fields of study also experience confirmation bias. Andreas Hergovich, Reinhard Schott, and Christoph Burger's experiment conducted online, for instance, suggested that professionals within the field of psychological research are likely to view scientific studies that are congruent with their preconceived understandings more favorably than studies that are incongruent with their established beliefs.\n\nMotivation refers to one's desire to defend or find substantiation for beliefs (e.g., religious beliefs) that are important to one. According to Raymond Nickerson, one can see the consequences of confirmation bias in real-life situations, which range in severity from inefficient government policies to genocide. With respect to the latter and most severe ramification of this cognitive barrier, Nickerson argued that those involved in committing genocide of persons accused of witchcraft, an atrocity that occurred from the 15th to 17th centuries, demonstrated confirmation bias with motivation. Researcher Michael Allen found evidence for confirmation bias with motivation in school children who worked to manipulate their science experiments in such a way that would produce their hoped for results. However, confirmation bias does not necessarily require motivation. In 1960, Peter Cathcart Wason conducted an experiment in which participants first viewed three numbers and then created a hypothesis that proposed a rule that could have been used to create that triplet of numbers. When testing their hypotheses, participants tended to only create additional triplets of numbers that would confirm their hypotheses, and tended not to create triplets that would negate or disprove their hypotheses. Thus research also shows that people can and do work to confirm theories or ideas that do not support or engage personally significant beliefs.\n\nMental set was first articulated by Abraham Luchins in the 1940s and demonstrated in his well-known water jug experiments. In these experiments, participants were asked to fill one jug with a specific amount of water using only other jugs (typically three) with different maximum capacities as tools. After Luchins gave his participants a set of water jug problems that could all be solved by employing a single technique, he would then give them a problem that could either be solved using that same technique or a novel and simpler method. Luchins discovered that his participants tended to use the same technique that they had become accustomed to despite the possibility of using a simpler alternative. Thus mental set describes one's inclination to attempt to solve problems in such a way that has proved successful in previous experiences. However, as Luchins' work revealed, such methods for finding a solution that have worked in the past may not be adequate or optimal for certain new but similar problems. Therefore, it is often necessary for people to move beyond their mental sets in order to find solutions. This was again demonstrated in Norman Maier's 1931 experiment, which challenged participants to solve a problem by using a household object (pliers) in an unconventional manner. Maier observed that participants were often unable to view the object in a way that strayed from its typical use, a phenomenon regarded as a particular form of mental set (more specifically known as functional fixedness, which is the topic of the following section). When people cling rigidly to their mental sets, they are said to be experiencing \"fixation\", a seeming obsession or preoccupation with attempted strategies that are repeatedly unsuccessful. In the late 1990s, researcher Jennifer Wiley worked to reveal that expertise can work to create a mental set in persons considered to be experts in certain fields, and she furthermore gained evidence that the mental set created by expertise could lead to the development of fixation.\n\nFunctional fixedness is a specific form of mental set and fixation, which was alluded to earlier in the Maier experiment, and furthermore it is another way in which cognitive bias can be seen throughout daily life. Tim German and Clark Barrett describe this barrier as the fixed design of an object hindering the individual's ability to see it serving other functions. In more technical terms, these researchers explained that \"[s]ubjects become \"fixed\" on the design function of the objects, and problem solving suffers relative to control conditions in which the object's function is not demonstrated.\" Functional fixedness is defined as only having that primary function of the object itself hinder the ability of it serving another purpose other than its original function. In research that highlighted the primary reasons that young children are immune to functional fixedness, it was stated that \"functional fixedness...[is when]subjects are hindered in reaching the solution to a problem by their knowledge of an object's conventional function.\" Furthermore, it is important to note that functional fixedness can be easily expressed in commonplace situations. For instance, imagine the following situation: a man sees a bug on the floor that he wants to kill, but the only thing in his hand at the moment is a can of air freshener. If the man starts looking around for something in the house to kill the bug with instead of realizing that the can of air freshener could in fact be used not only as having its main function as to freshen the air, he is said to be experiencing functional fixedness. The man's knowledge of the can being served as purely an air freshener hindered his ability to realize that it too could have been used to serve another purpose, which in this instance was as an instrument to kill the bug. Functional fixedness can happen on multiple occasions and can cause us to have certain cognitive biases. If we only see an object as serving one primary focus than we fail to realize that the object can be used in various ways other than its intended purpose. This can in turn cause many issues with regards to problem solving. Common sense seems to be a plausible answer to functional fixedness. One could make this argument because it seems rather simple to consider possible alternative uses for an object. Perhaps using common sense to solve this issue could be the most accurate answer within this context. With the previous stated example, it seems as if it would make perfect sense to use the can of air freshener to kill the bug rather than to search for something else to serve that function but, as research shows, this is often not the case.\n\nFunctional fixedness limits the ability for people to solve problems accurately by causing one to have a very narrow way of thinking. Functional fixedness can be seen in other types of learning behaviors as well. For instance, research has discovered the presence of functional fixedness in many educational instances. Researchers Furio, Calatayud, Baracenas, and Padilla stated that \"... functional fixedness may be found in learning concepts as well as in solving chemistry problems.\" There was more emphasis on this function being seen in this type of subject and others.\n\nThere are several hypotheses in regards to how functional fixedness relates to problem solving. There are also many ways in which a person can run into problems while thinking of a particular object with having this function. If there is one way in which a person usually thinks of something rather than multiple ways then this can lead to a constraint in how the person thinks of that particular object. This can be seen as narrow minded thinking, which is defined as a way in which one is not able to see or accept certain ideas in a particular context. Functional fixedness is very closely related to this as previously mentioned. This can be done intentionally and or unintentionally, but for the most part it seems as if this process to problem solving is done in an unintentional way.\n\nFunctional fixedness can affect problem solvers in at least two particular ways. The first is with regards to time, as functional fixedness causes people to use more time than necessary to solve any given problem. Secondly, functional fixedness often causes solvers to make more attempts to solve a problem than they would have made if they were not experiencing this cognitive barrier. In the worst case, functional fixedness can completely prevent a person from realizing a solution to a problem. Functional fixedness is a commonplace occurrence, which affects the lives of many people.\n\nUnnecessary constraints are another very common barrier that people face while attempting to problem-solve. This particular phenomenon occurs when the subject, trying to solve the problem subconsciously, places boundaries on the task at hand, which in turn forces him or her to strain to be more innovative in their thinking. The solver hits a barrier when they become fixated on only one way to solve their problem, and it becomes increasingly difficult to see anything but the method they have chosen. Typically, the solver experiences this when attempting to use a method they have already experienced success from, and they can not help but try to make it work in the present circumstances as well, even if they see that it is counterproductive.\n\nGroupthink, or taking on the mindset of the rest of the group members, can also act as an unnecessary constraint while trying to solve problems. This is due to the fact that with everybody thinking the same thing, stopping on the same conclusions, and inhibiting themselves to think beyond this. This is very common, but the most well-known example of this barrier making itself present is in the famous example of the dot problem. In this example, there are nine dots lying in a square- three dots across, and three dots running up and down. The solver is then asked to draw no more than four lines, without lifting their pen or pencil from the paper. This series of lines should connect all of the dots on the paper. Then, what typically happens is the subject creates an assumption in their mind that they must connect the dots without letting his or her pen or pencil go outside of the square of dots. Standardized procedures like this can often bring mentally invented constraints of this kind, and researchers have found a 0% correct solution rate in the time allotted for the task to be completed. The imposed constraint inhibits the solver to think beyond the bounds of the dots. It is from this phenomenon that the expression \"think outside the box\" is derived.\n\nThis problem can be quickly solved with a dawning of realization, or \"insight\". A few minutes of struggling over a problem can bring these sudden insights, where the solver quickly sees the solution clearly. Problems such as this are most typically solved via insight and can be very difficult for the subject depending on either how they have structured the problem in their minds, how they draw on their past experiences, and how much they juggle this information in their working memories In the case of the nine-dot example, the solver has already been structured incorrectly in their minds because of the constraint that they have placed upon the solution. In addition to this, people experience struggles when they try to compare the problem to their prior knowledge, and they think they must keep their lines within the dots and not go beyond. They do this because trying to envision the dots connected outside of the basic square puts a strain on their working memory.\n\nLuckily, the solution to the problem becomes obvious as insight occurs following incremental movements made toward the solution. These tiny movements happen without the solver knowing. Then when the insight is realized fully, the \"aha\" moment happens for the subject. These moments of insight can take a long while to manifest or not so long at other times, but the way that the solution is arrived at after toiling over these barriers stays the same.\n\nIrrelevant information is information presented within a problem that is unrelated or unimportant to the specific problem. Within the specific context of the problem, irrelevant information would serve no purpose in helping solve that particular problem. Often \"irrelevant information\" is detrimental to the problem solving process. It is a common barrier that many people have trouble getting through, especially if they are not aware of it. \"Irrelevant information\" makes solving otherwise relatively simple problems much harder.\n\nFor example: \"Fifteen percent of the people in Topeka have unlisted telephone numbers. You select 200 names at random from the Topeka phone book. How many of these people have unlisted phone numbers?\"\n\nThe people that are not listed in the phone book would not be among the 200 names you selected. The individuals looking at this task would have naturally wanted to use the 15% given to them in the problem. They see that there is information present and they immediately think that it needs to be used. This of course is not true. These kinds of questions are often used to test students taking aptitude tests or cognitive evaluations. They aren't meant to be difficult but they are meant to require thinking that is not necessarily common. \"Irrelevant Information\" is commonly represented in math problems, word problems specifically, where numerical information is put for the purpose of challenging the individual.\n\nOne reason irrelevant information is so effective at keeping a person off topic and away from the relevant information, is in how it is represented. The way information is represented can make a vast difference in how difficult the problem is to be overcome. Whether a problem is represented visually, verbally, spatially, or mathematically, irrelevant information can have a profound effect on how long a problem takes to be solved; or if it's even possible. The Buddhist monk problem is a classic example of irrelevant information and how it can be represented in different ways:\n\nThis problem is near impossible to solve because of how the information is represented. Because it is written out in a way that represents the information verbally, it causes us to try and create a mental image of the paragraph. This is often very difficult to do especially with all the \"irrelevant information\" involved in the question. This example is made much easier to understand when the paragraph is represented visually. Now if the same problem was asked, but it was also accompanied by a corresponding graph, it would be far easier to answer this question; \"irrelevant information\" no longer serves as a road block. By representing the problem visually, there are no difficult words to understand or scenarios to imagine. The visual representation of this problem has removed the difficulty of solving it.\n\nThese types of representations are often used to make difficult problems easier. They can be used on tests as a strategy to remove \"Irrelevant Information,\" which is one of the most common forms of barriers when discussing the issues of problem solving. Identifying crucial information presented in a problem and then being able to correctly identify its usefulness is essential. Being aware of \"irrelevant information\" is the first step in overcoming this common barrier.\n\nIn cognitive sciences, researchers' realization that problem-solving processes differ across knowledge domains and across levels of expertise (e.g. Sternberg, 1995) and that, consequently, findings obtained in the laboratory cannot necessarily generalize to problem-solving situations outside the laboratory, has led to an emphasis on real-world problem solving since the 1990s. This emphasis has been expressed quite differently in North America and Europe, however. Whereas North American research has typically concentrated on studying problem solving in separate, natural knowledge domains, much of the European research has focused on novel, complex problems, and has been performed with computerized scenarios (see Funke, 1991, for an overview).\n\nIn Europe, two main approaches have surfaced, one initiated by Donald Broadbent (1977; see Berry & Broadbent, 1995) in the United Kingdom and the other one by Dietrich Dörner (1975, 1985; see Dörner & Wearing, 1995) in Germany. The two approaches share an emphasis on relatively complex, semantically rich, computerized laboratory tasks, constructed to resemble real-life problems. The approaches differ somewhat in their theoretical goals and methodology, however. The tradition initiated by Broadbent emphasizes the distinction between cognitive problem-solving processes that operate under awareness versus outside of awareness, and typically employs mathematically well-defined computerized systems. The tradition initiated by Dörner, on the other hand, has an interest in the interplay of the cognitive, motivational, and social components of problem solving, and utilizes very complex computerized scenarios that contain up to 2,000 highly interconnected variables (e.g., Dörner, Kreuzig, Reither & Stäudel's 1983 LOHHAUSEN project; Ringelband, Misiak & Kluwe, 1990). Buchner (1995) describes the two traditions in detail.\n\nIn North America, initiated by the work of Herbert A. Simon on \"learning by doing\" in semantically rich domains (e.g. Anzai & Simon, 1979; Bhaskar & Simon, 1977), researchers began to investigate problem solving separately in different natural knowledge domains – such as physics, writing, or chess playing – thus relinquishing their attempts to extract a global theory of problem solving (e.g. Sternberg & Frensch, 1991). Instead, these researchers have frequently focused on the development of problem solving within a certain domain, that is on the development of expertise (e.g. Anderson, Boyle & Reiser, 1985; Chase & Simon, 1973; Chi, Feltovich & Glaser, 1981).\n\nAreas that have attracted rather intensive attention in North America include:\n\nAs elucidated by Dietrich Dörner and later expanded upon by , complex problems have some typical characteristics that can be summarized as follows:\n\n\nProblem solving is applied on many different levels − from the individual to the civilizational. Collective problem solving refers to problem solving performed collectively.\n\nSocial issues and global issues can typically only be solved collectively.\n\nIt has been noted that the complexity of contemporary problems has exceeded the cognitive capacity of any individual and requires different but complementary expertise and collective problem solving ability.\n\nCollective intelligence is shared or group intelligence that emerges from the collaboration, collective efforts, and competition of many individuals.\n\nIn a 1962 research report, Douglas Engelbart linked collective intelligence to organizational effectiveness, and predicted that pro-actively 'augmenting human intellect' would yield a multiplier effect in group problem solving: \"Three people working together in this augmented mode [would] seem to be more than three times as effective in solving a complex problem as is one augmented person working alone\".\n\nHenry Jenkins, a key theorist of new media and media convergence draws on the theory that collective intelligence can be attributed to media convergence and participatory culture. He criticizes contemporary education for failing to incorporate online trends of collective problem solving into the classroom, stating \"whereas a collective intelligence community encourages ownership of work as a group, schools grade individuals\". Jenkins argues that interaction within a knowledge community builds vital skills for young people, and teamwork through collective intelligence communities contribute to the development of such skills.\n\nCollective impact is the commitment of a group of actors from different sectors to a common agenda for solving a specific social problem, using a structured form of collaboration.\n\nAfter World War II the UN, the Bretton Woods organization and the WTO were created and collective problem solving on the international level crystallized since the 1980s around these 3 types of organizations. As these global institutions remain state-like or state-centric it has been called unsurprising that these continue state-like or state-centric approaches to collective problem-solving rather than alternative ones.\n\nIt has been observed that models of liberal democracy provide neither adequate designs for collective problem solving nor handling the substantive challenges in society such as crime, war, economic decline, illness and environmental degradation to produce satisfying outcomes.\n\nCrowdsourcing is a process of accumulating the ideas, thoughts or information from many independent participants, with aim to find the best solution for a given challenge. Modern information technologies allow for massive number of subjects to be involved as well as systems of managing these suggestions that provide good results. With the Internet a new capacity for collective, including planetary-scale, problem solving was created.\n\n",
    "id": "1467948",
    "title": "Problem solving"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=40267442",
    "text": "MNIST database\n\nThe MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems. The database is also widely used for training and testing in the field of machine learning. It was created by \"re-mixing\" the samples from NIST's original datasets. The creators felt that since NIST's training dataset was taken from American Census Bureau employees, while the testing dataset was taken from American high school students, it was not well-suited for machine learning experiments. Furthermore, the black and white images from NIST were normalized to fit into a 28x28 pixel bounding box and anti-aliased, which introduced grayscale levels.\nThe MNIST database contains 60,000 training images and 10,000 testing images. Half of the training set and half of the test set were taken from NIST's training dataset, while the other half of the training set and the other half of the test set were taken from NIST's testing dataset.\nThere have been a number of scientific papers on attempts to achieve the lowest error rate; one paper, using a hierarchical system of convolutional neural networks, manages to get an error rate on the MNIST database of 0.23 percent. The original creators of the database keep a list of some of the methods tested on it. In their original paper, they use a support vector machine to get an error rate of 0.8 percent. An extended dataset similar to MNIST called EMNIST has been published in 2017, which contains 240,000 training images, and 40,000 testing images of handwritten digits.\n\nThe set of images in the MNIST database is a combination of two of NIST's databases: Special Database 1 and Special Database 3. Special Database 1 and Special Database 3 consist of digits written by high school students and employees of the United States Census Bureau, respectively.\n\nSome researchers have achieved \"near-human performance\" on the MNIST database, using a committee of neural networks; in the same paper, the authors achieve performance double that of humans on other recognition tasks. The highest error rate listed on the original website of the database is 12 percent, which is achieved using a simple linear classifier with no preprocessing.\n\nIn 2004, a best-case error rate of 0.42 percent was achieved on the database by researchers using a new classifier called the LIRA, which is a neural classifier with three neuron layers based on Rosenblatt's perceptron principles.\n\nSome researchers have tested artificial intelligence systems using the database put under random distortions. The systems in these cases are usually neural networks and the distortions used tend to be either affine distortions or elastic distortions. Sometimes, these systems can be very successful; one such system achieved an error rate on the database of 0.39 percent.\n\nIn 2011, an error rate of 0.27 percent, improving on the previous best result, was reported by researchers using a similar system of neural networks. In 2013, an approach based on regularization of neural networks using DropConnect has been claimed to achieve a 0.21 percent error rate. Recently, the single convolutional neural network best performance was 0.31 percent error rate. Currently, the best performance of a single convolutional neural network trained in 74 epochs on the expanded training data is 0.27 percent error rate. Also, the Parallel Computing Center (Khmelnitskiy, Ukraine) obtained an ensemble of only 5 convolutional neural networks which performs on MNIST at 0.21 percent error rate.\n\nThis is a table of some of the machine learning methods used on the database and their error rates, by type of classifier:\n\n",
    "id": "40267442",
    "title": "MNIST database"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=40196540",
    "text": "Histogram of oriented displacements\n\nHistogram of Oriented Displacements (HOD) is a 2D trajectory descriptor. The trajectory is described using a histogram of the directions between each two consecutive points. Given a trajectory T = {P, P, P, ..., P}, where P is the 2D position at time t. For each pair of positions P and P, calculate the direction angle θ(t, t+1). Value of θ is between 0 and 360. A histogram of the quantized values of θ is created. If the histogram is of 8 bins, the first bin represents all θs between 0 and 45.\n\nThe histogram accumulates the lengths of the consecutive moves. For each θ, a specific histogram bin is determined. The length of the line between P and P is then added to the specific histogram bin.\n\nTo show the intuition behind the descriptor, consider the action of waving hands. At the end of the action, the hand falls down. When describing this down movement, the descriptor does not care about the position from which the hand started to fall. This fall will affect the histogram with the appropriate angles and lengths, regardless of the position where the hand started to fall.\n\nHOD records for each moving point: how much it moves in each range of directions. HOD has a clear physical interpretation. It proposes that, a simple way to describe the motion of an object, is to indicate how much distance it moves in each direction. If the movement in all directions are saved accurately, the movement can be repeated from the initial position to the final destination regardless of the displacements order. However, the temporal information will be lost, as the order of movements is not stored-this is what we solve by applying the temporal pyramid, as shown in section \\ref{sec:temp-pyramid}. If the angles quantization range is small, classifiers that use the descriptor will overfit. Generalization needs some slack in directions-which can be done by increasing the quantization range.\n\n",
    "id": "40196540",
    "title": "Histogram of oriented displacements"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=40615979",
    "text": "Voice Mate\n\nVoice Mate formerly called \"Quick Voice\" and later on as \"Q Voice\" is an intelligent personal assistant and knowledge navigator which is only available as a built-in application for various LG smartphones. The application uses a natural language user interface to answer questions, make recommendations, and perform actions by delegating requests to a set of Web services. It is based on the Maluuba personal assistant.\n\nSome of the capabilities of Voice Mate include making appointments, opening apps, setting alarms, updating social network websites such as Facebook or Twitter and navigation. Voice Mate also offers efficient multitasking as well as automatic activation features, for example when the car engine is started.\n",
    "id": "40615979",
    "title": "Voice Mate"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=40647601",
    "text": "Syman\n\nSYMAN is an artificial intelligence technology that uses data from social media profiles to identify trends in the job market. SYMAN is designed to organize actionable data for products and services including recruiting, human capital management, CRM, and marketing.\n\nSYMAN was developed with a $21 million series B financing round secured by Identified, which was led by VantagePoint Capital Partners and Capricorn Investment Group.\n\nIdentified\n",
    "id": "40647601",
    "title": "Syman"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=12476035",
    "text": "Moravec's paradox\n\nMoravec's paradox is the discovery by artificial intelligence and robotics researchers that, contrary to traditional assumptions, high-level reasoning requires very little computation, but low-level sensorimotor skills require enormous computational resources. The principle was articulated by Hans Moravec, Rodney Brooks, Marvin Minsky and others in the 1980s. As Moravec writes, \"it is comparatively easy to make computers exhibit adult level performance on intelligence tests or playing checkers, and difficult or impossible to give them the skills of a one-year-old when it comes to perception and mobility.\"\n\nSimilarly, Marvin Minsky emphasized that the most difficult human skills to reverse engineer are those that are \"unconscious\". \"In general, we're least aware of what our minds do best,\" he wrote, and added \"we're more aware of simple processes that don't work well than of complex ones that work flawlessly.\"\n\nOne possible explanation of the paradox, offered by Moravec, is based on evolution. All human skills are implemented biologically, using machinery designed by the process of natural selection. In the course of their evolution, natural selection has tended to preserve design improvements and optimizations. The older a skill is, the more time natural selection has had to improve the design. Abstract thought developed only very recently, and consequently, we should not expect its implementation to be particularly efficient.\n\nAs Moravec writes:\nA compact way to express this argument would be:\n\nSome examples of skills that have been evolving for millions of years: recognizing a face, moving around in space, judging people’s motivations, catching a ball, recognizing a voice, setting appropriate goals, paying attention to things that are interesting; anything to do with perception, attention, visualization, motor skills, social skills and so on.\n\nSome examples of skills that have appeared more recently: mathematics, engineering, human games, logic and scientific reasoning. These are hard for us because they are not what our bodies and brains were primarily evolved to do. These are skills and techniques that were acquired recently, in historical time, and have had at most a few thousand years to be refined, mostly by cultural evolution.\n\nIn the early days of artificial intelligence research, leading researchers often predicted that they would be able to create thinking machines in just a few decades (see history of artificial intelligence). Their optimism stemmed in part from the fact that they had been successful at writing programs that used logic, solved algebra and geometry problems and played games like checkers and chess. Logic and algebra are difficult for people and are considered a sign of intelligence. They assumed that, having (almost) solved the \"hard\" problems, the \"easy\" problems of vision and commonsense reasoning would soon fall into place. They were wrong, and one reason is that these problems are not easy at all, but incredibly difficult. The fact that they had solved problems like logic and algebra was irrelevant, because these problems are extremely easy for machines to solve.\n\nRodney Brooks explains that, according to early AI research, intelligence was \"best characterized as the things that highly educated male scientists found challenging\", such as chess, symbolic integration, proving mathematical theorems and solving complicated word algebra problems. \"The things that children of four or five years could do effortlessly, such as visually distinguishing between a coffee cup and a chair, or walking around on two legs, or finding their way from their bedroom to the living room were not thought of as activities requiring intelligence.\"\n\nThis would lead Brooks to pursue a new direction in artificial intelligence and robotics research. He decided to build intelligent machines that had \"No cognition. Just sensing and action. That is all I would build and completely leave out what traditionally was thought of as the \"intelligence\" of artificial intelligence.\" This new direction, which he called \"Nouvelle AI\" was highly influential on robotics research and AI.\n\nLinguist and cognitive scientist Steven Pinker considers this the main lesson uncovered by AI researchers. In his book \"The Language Instinct\", he writes:\n\n\n",
    "id": "12476035",
    "title": "Moravec's paradox"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=35457079",
    "text": "Argumentation framework\n\nIn artificial intelligence and related fields, an argumentation framework, or argumentation system, is a way to deal with contentious information and draw conclusions from it.\n\nIn an abstract argumentation framework, entry-level information is a set of abstract arguments that, for instance, represent data or a proposition. Conflicts between arguments are represented by a binary relation on the set of arguments. In concrete terms, you represent an argumentation framework with a directed graph such that the nodes are the arguments, and the arrows represent the attack relation.\nThere exist some extensions of the Dung's framework, like the logic-based argumentation frameworks or the value-based argumentation frameworks.\n\nAbstract argumentation frameworks, also called argumentation frameworks \"à la Dung\", are defined formally as a pair:\nFor instance, the argumentation system formula_4 with formula_5 and formula_6 contains four arguments (formula_7 and formula_8) and three attacks (formula_9 attacks formula_10, formula_10 attacks formula_12 and formula_8 attacks formula_12).\n\nDung defines some notions :\n\nTo decide if an argument can be accepted or not, or if several arguments can be accepted together, Dung defines several semantics of acceptance that allow, given an argumentation system, to compute sets of arguments, called \"extensions\". For instance, given formula_26,\nThere exists some inclusions between the sets of extensions built with these semantics :\n\nSome other semantics have been defined.\n\nOne introduce the notation formula_44 to note the set of formula_45-extensions of the system formula_28.\nIn the case of the system formula_28 in the figure above, formula_48 for every Dung's semantic—the system is well-founded. That explains why the semantics coincide, and the accepted arguments are: formula_9 and formula_8.\n\nLabellings are a more expressive way than extensions to express the acceptance of the arguments. Concretely, a labelling is a mapping that associates every argument with a label \"in\" (the argument is accepted), \"out\" (the argument is rejected), or \"undec\" (the argument is undefined—not accepted or refused).\nOne can also note a labelling as a set of pairs formula_51.\n\nSuch a mapping does not make sense without additional constraint. The notion of reinstatement labelling guarantees the sense of the mapping. formula_52 is a reinstatement labelling on the system formula_4 if and only if :\n\nOne can convert every extension into a reinstatement labelling: the arguments of the extension are \"in\", those attacked by an argument of the extension are \"out\", and the others are \"undec\". Conversely, one can build an extension from a reinstatement labelling just by keeping the arguments \"in\". Indeed, Caminada proved that the reinstatement labellings and the complete extensions can be mapped in a bijective way. Moreover, the other Datung's semantics can be associated to some particular sets of reinstatement labellings.\n\nReinstatement labellings distinguish arguments not accepted because they are attacked by accepted arguments from undefined arguments—that is, those that are not defended cannot defend themselves. An argument is \"undec\" if it is attacked by at least another \"undec\". If it is attacked only by arguments \"out\", it must be \"in\", and if it is attacked only by arguments \"in\", then it is \"out\".\n\nThe unique reinstatement labelling that corresponds to the system formula_28 above is formula_65.\n\nIn the general case when several extensions are computed for a given semantic formula_45, the agent that reasons from the system can use several mechanisms to infer information:\n\nFor these two methods to infer information, one can identify the set of accepted arguments, respectively formula_73 the set of the arguments credulously accepted under the semantic formula_45, and formula_75 the set of arguments accepted skeptically under the semantic formula_45 (the formula_45 can be missed if there is no possible ambiguity about the semantic).\n\nOf course, when there is only one extension (for instance, when the system is well-founded), this problem is very simple: the agent accepts arguments of the unique extension and rejects others.\n\nThe same reasoning can be done with labellings that correspond to the chosen semantic : an argument can be accepted if it is \"in\" for each labelling and refused if it is \"out\" for each labelling, the others being in an undecided state (the status of the arguments can remind the epistemic states of a belief in the AGM framework for dynamic of beliefs).\n\nThere exists several criteria of equivalence between argumentation frameworks. Most of those criteria concern the sets of extensions or the set of accepted arguments.\nFormally, given a semantic formula_45 :\n\nThe strong equivalence says that two systems formula_86 and formula_87 are equivalent if and only if for all other system formula_88, the union of formula_86 with formula_88 is equivalent (for a given criterion) with the union of formula_87 and formula_88.\n\nThe abstract framework of Dung has been instantiated to several particular cases.\n\nIn the case of logic-based argumentation frameworks, an argument is not an abstract entity, but a pair, where the first part is a minimal consistent set of formulae enough to prove the formula for the second part of the argument.\nFormally, an argument is a pair formula_93 such that\n\nOne calls formula_98 a consequence of formula_96, and formula_96 a support of formula_98.\n\nIn this case, the attack relation is not given in an explicit way, as a subset of the Cartesian product formula_104, but as a property that indicates if an argument attacks another. For instance,\n\nGiven a particular attack relation, one can build a graph and reason in a similar way to the abstract argumentation frameworks (use of semantics to build extension, skeptical or credulous inference), the difference is that the information inferred from a logic based argumentation framework is a set of formulae (the consequences of the accepted arguments).\n\nThe value-based argumentation frameworks come from the idea that during an exchange of arguments, some can be \"stronger\" than others with respect to a certain value they advance, and so the success of an attack between arguments depends of the difference of these values.\n\nFormally, a value-based argumentation framework is a tuple formula_116 with formula_1 and formula_3 similar to the standard framework (a set of arguments and a binary relation on this set), formula_119 is a non empty set of values, formula_120 is a mapping that associates each element from formula_1 to an element from formula_119, and formula_123 is a preference relation (transitive, irreflexive and asymmetric) on formula_124.\n\nIn this framework, an argument formula_9 defeats another argument formula_10 if and only if\nOne remarks that an attack succeeds if both arguments are associated to the same value, or if there is no preference between their respective values.\n\nIn assumption-based argumentation frameworks, arguments are defined as a set of rules and attacks are defined in terms of assumptions and contraries.\n\nFormally, an assumption-based argumentation framework is a tuple formula_133, where\nAs a consequence of defining an ABA, an argument can be represented in a tree-form. Formally, given a deductive system formula_134 and set of assumptions formula_141, an argument for claim formula_149 supported by formula_150, is a tree with nodes labelled by sentences in formula_135 or by symbol formula_152, such that:\nAn argument with claim formula_12 supported by a set of assumption formula_28 can also be denoted as formula_173\n\n",
    "id": "35457079",
    "title": "Argumentation framework"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=11081176",
    "text": "Mind–body problem\n\nThe mind–body problem is a philosophical problem concerning the relationship between the human mind and body, although it can also concern animal minds, if any, and animal bodies. It is distinct from the question how mind and body can causally interact, since that question presupposes an interactionist account of mind-body relations. This question arises when mind and body are considered as distinct, based on the premise that the mind and the body are fundamentally different in nature.\n\nThe problem was addressed by René Descartes in the 17th century, resulting in Cartesian dualism, and by pre-Aristotelian philosophers, in Avicennian philosophy, and in earlier Asian traditions. A variety of approaches have been proposed. Most are either dualist or monist. Dualism maintains a rigid distinction between the realms of mind and matter. Monism maintains that there is only one unifying reality, substance or essence in terms of which everything can be explained.\n\nEach of these categories contain numerous variants. The two main forms of dualism are substance dualism, which holds that the mind is formed of a distinct type of substance not governed by the laws of physics, and property dualism, which holds that mental properties involving conscious experience are fundamental properties, alongside the fundamental properties identified by a completed physics. The three main forms of monism are physicalism, which holds that the mind consists of matter organized in a particular way; idealism, which holds that only thought truly exists and matter is merely an illusion; and neutral monism, which holds that both mind and matter are aspects of a distinct essence that is itself identical to neither of them.\n\nSeveral philosophical perspectives have been developed which reject the mind–body dichotomy. The historical materialism of Karl Marx and subsequent writers, itself a form of physicalism, held that consciousness was engendered by the material contingencies of one's environment. An explicit rejection of the dichotomy is found in French structuralism, and is a position that generally characterized post-war French philosophy.\n\nThe absence of an empirically identifiable meeting point between the non-physical mind (if there is such a thing) and its physical extension has proven problematic to dualism, and many modern philosophers of mind maintain that the mind is not something separate from the body. These approaches have been particularly influential in the sciences, particularly in the fields of sociobiology, computer science, evolutionary psychology, and the neurosciences.\n\nAn ancient model of the mind known as the Five-Aggregate Model explains the mind as continuously changing sense impressions and mental phenomena. Considering this model, it is possible to understand that it is the constantly changing sense impressions and mental phenomena (i.e., the mind) that experiences/analyzes all external phenomena in the world as well as all internal phenomena including the body anatomy, the nervous system as well as the organ brain. This conceptualization leads to two levels of analyses: (i) analyses conducted from a third-person perspective on how the brain works, and (ii) analyzing the moment-to-moment manifestation of an individual’s mind-stream (analyses conducted from a first-person perspective). Considering the latter, the manifestation of the mind-stream is described as happening in every person all the time, even in a scientist who analyses various phenomena in the world, including analyzing and hypothesizing about the organ brain.\n\nPhilosophers David L. Robb and John H. Heil introduce mental causation in terms of the mind–body problem of interaction:\nContemporary neurophilosopher, Georg Northoff suggests that mental causation is compatible with classical formal and final causality.\n\nBiologist, theoretical neuroscientist and philosopher, Walter J. Freeman, suggests that explaining mind–body interaction in terms of \"circular causation\" is more relevant than linear causation.\n\nIn neuroscience, much has been learned about correlations between brain activity and subjective, conscious experiences. Many suggest that neuroscience will ultimately explain consciousness:\n\"...consciousness is a biological process that will eventually be explained in terms of molecular signaling pathways used by interacting populations of nerve cells...\" However, this view has been criticized because \"consciousness\" has yet to be shown to be a \"process\", and the \"hard problem\" of relating consciousness directly to brain activity remains elusive.\n\nThe \"neural correlates of consciousness\" \"are the smallest set of brain mechanisms and events sufficient for some specific conscious feeling, as elemental as the color red or as complex as the sensual, mysterious, and primeval sensation evoked when looking at [a] jungle scene...\" Neuroscientists use empirical approaches to discover neural correlates of subjective phenomena.\n\nA science of consciousness must explain the exact relationship between subjective conscious mental states and brain states formed by electrochemical interactions in the body, the so-called hard problem of consciousness. \nNeurobiology studies the connection scientifically, as do neuropsychology and neuropsychiatry. \"Neurophilosophy\" is the interdisciplinary study of neuroscience and philosophy of mind. In this pursuit, neurophilosophers, such as Patricia Churchland,\nPaul Churchland \nand Daniel Dennett, \nhave focused primarily on the body rather than the mind. In this context, neuronal correlates may be viewed as causing consciousness, where consciousness can be thought of as an undefined property that depends upon this complex, adaptive, and highly interconnected biological system. However, it's unknown if discovering and characterizing neural correlates may eventually provide a theory of consciousness that can explain the first-person experience of these \"systems\", and determine whether other systems of equal complexity lack such features.\n\nThe massive parallelism of neural networks allows redundant populations of neurons to mediate the same or similar percepts. Nonetheless, it is assumed that every subjective state will have associated neural correlates, which can be manipulated to artificially inhibit or induce the subject's experience of that conscious state. The growing ability of neuroscientists to manipulate neurons using methods from molecular biology in combination with optical tools was achieved by the development of behavioral and organic models that are amenable to large-scale genomic analysis and manipulation. Non-human analysis such as this, in combination with imaging of the human brain, have contributed to a robust and increasingly predictive theoretical framework.\n\nThere are two common but distinct dimensions of the term \"consciousness\", one involving \"arousal\" and \"states of consciousness\" and the other involving \"content of consciousness\" and \"conscious states\". To be conscious \"of\" something, the brain must be in a relatively high state of arousal (sometimes called \"vigilance\"), whether awake or in REM sleep. Brain arousal level fluctuates in a circadian rhythm but these natural cycles may be influenced by lack of sleep, alcohol and other drugs, physical exertion, etc. Arousal can be measured behaviorally by the signal amplitude required to trigger a given reaction (for example, the sound level that causes a subject to turn and look toward the source). High arousal states involve conscious states that feature specific perceptual content, planning and recollection or even fantasy. Clinicians use scoring systems such as the Glasgow Coma Scale to assess the level of arousal in patients with \"impaired states of consciousness\" such as the comatose state, the persistent vegetative state, and the minimally conscious state. Here, \"state\" refers to different amounts of externalized, physical consciousness: ranging from a total absence in coma, persistent vegetative state and general anesthesia, to a fluctuating, minimally conscious state, such as sleep walking and epileptic seizure.\n\nMany nuclei with distinct chemical signatures in the thalamus, midbrain and pons must function for a subject to be in a sufficient state of brain arousal to experience anything at all. These nuclei therefore belong to the enabling factors for consciousness. Conversely it is likely that the specific content of any particular conscious sensation is mediated by particular neurons in the cortex and their associated satellite structures, including the amygdala, thalamus, claustrum and the basal ganglia.\n\nThe following is a very brief account of some contributions to the mind–body problem.\n\nThe Buddha (480–400 B.C.E), founder of Buddhism, described the mind and the body as depending on each other in a way that two sheaves of reeds were to stand leaning against one another and taught that the world consists of mind and matter which work together, interdependently. Buddhist teachings describe the mind as manifesting from moment to moment, one thought moment at a time as a fast flowing stream. The components that make up the mind are known as the five aggregates (i.e., material form, feelings, perception, volition, and sensory consciousness), which arise and pass away continuously. The arising and passing of these aggregates in the present moment is described as being influenced by five causal laws: biological laws, psychological laws, physical laws, volitional laws, and universal laws. The Buddhist practice of mindfulness involves attending to this constantly changing mind-stream.\n\nUltimately, the Buddha's philosophy is that both mind and forms are conditionally arising qualities of an ever-changing universe in which, when nirvāna is attained, all phenomenal experience ceases to exist. According to the anattā doctrine of the Buddha, the conceptual self is a mere mental construct of an individual entity and is basically an impermanent illusion, sustained by form, sensation, perception, thought and consciousness. The Buddha argued that mentally clinging to any views will result in delusion and stress, since, according to the Buddha, a real self (conceptual self, being the basis of standpoints and views) cannot be found when the mind has clarity.\n\nPlato (429–347 B.C.E.) believed that the material world is a shadow of a higher reality that consists of concepts he called Forms. According to Plato, objects in our everyday world \"participate in\" these Forms, which confer identity and meaning to material objects. For example, a circle drawn in the sand would be a circle only because it participates in the concept of an ideal circle that exists somewhere in the world of Forms. He argued that, as the body is from the material world, the soul is from the world of Forms and is thus immortal. He believed the soul was temporarily united with the body and would only be separated at death, when it would return to the world of Forms. Since the soul does not exist in time and space, as the body does, it can access universal truths. For Plato, ideas (or Forms) are the true reality, and are experienced by the soul. The body is for Plato empty in that it can not access the abstract reality of the world; it can only experience shadows. This is determined by Plato's essentially rationalistic epistemology.\n\nFor Aristotle (384–322 BC) \"mind\" is a faculty of the \"soul\". Regarding the soul, he said:\nIn the end, Aristotle saw the relation between soul and body as uncomplicated, in the same way that it is uncomplicated that a cubical shape is a property of a toy building block. The soul is a property exhibited by the body, one among many. Moreover, Aristotle proposed that when the body perishes, so does the soul, just as the shape of a building block disappears with destruction of the block.\n\nIn religious philosophy of the people of the book dualism denotes a binary opposition of an idea that contains two essential parts. The first formal concept of a \"mind-body\" split may be found in the \"\"divinity - secularity\"\" dualism of the ancient Persian religion of Zoroastrianism around the mid-fifth century BC. Gnosticism is a modern name for a variety of ancient dualistic ideas inspired by Judaism popular in the first and second century AD. These ideas later seem to have been incorporated into Galen's \"\"tripartite soul\"\" that led into both the Christian sentiments expressed in the later Augustinian theodicy and Avicenna’s Platonism in Islamic Philosophy.\n\nRené Descartes (1596–1650) believed that mind exerted control over the brain \"via\" the pineal gland:\nHis posited relation between mind and body is called Cartesian dualism or substance dualism. He held that \"mind\" was distinct from \"matter\", but could influence matter. How such an interaction could be exerted remains a contentious issue.\n\nFor Kant (1724–1804) beyond \"mind\" and \"matter\" there exists a world of \"a priori\" forms, which are seen as necessary preconditions for understanding. Some of these forms, space and time being examples, today seem to be pre-programmed in the brain.\n\nKant views the mind–body interaction as taking place through forces that may be of different kinds for mind and body.\n\nFor Huxley (1825–1895) the conscious mind was a by-product of the brain that has no influence upon the brain, a so-called epiphenomenon.\n\nA. N. Whitehead advocated a sophisticated form of panpsychism that has been called by David Ray Griffin \"panexperientialism\".\n\nFor Popper (1902–1994) there are \"three\" aspects of the mind–body problem: the worlds of matter, mind, and of the creations of the mind, such as mathematics. In his view, the third-world creations of the mind could be interpreted by the second-world mind and used to affect the first-world of matter. An example might be radio, an example of the interpretation of the third-world (Maxwell's electromagnetic theory) by the second-world mind to suggest modifications of the external first world.\n\nFor Searle (b. 1932) the mind–body problem is a false dichotomy; that is, mind is a perfectly ordinary aspect of the brain. \n\n\n\n",
    "id": "11081176",
    "title": "Mind–body problem"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=2174928",
    "text": "Information space analysis\n\nInformation space analysis is a deterministic method, enhanced by machine intelligence, for locating and assessing resources for team centric efforts.\n\nOrganizations need to be able to quickly assemble teams backed by the support services, information, and material to do the job. To do so, these teams need to find and assess sources of services that are potential participants in the team effort.\n\nTo support this initial team and resource development, information needs to be developed via analysis tools that help make sense of sets of data sources in an Intranet or Internet. Part of the process is to characterize them, partition them, sort and filter them.\n\nThese tools focus on three key issues in forming a collaborative team: \n\nInformation space analysis tools combine multiple methods to assist in this task. This causes the tools to be particularly well-suited to integrating additional technologies in order to create specialized systems.\n\n",
    "id": "2174928",
    "title": "Information space analysis"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=40888645",
    "text": "Bayesian programming\n\nBayesian programming is a formalism and a methodology to specify probabilistic models and solve problems when less than the necessary information is available.\n\nEdwin T. Jaynes proposed that probability could be considered as an alternative and an extension of logic for rational reasoning with incomplete and uncertain information. In his founding book \"Probability Theory: The Logic of Science\" he developed this theory and proposed what he called “the robot,” which was not\na physical device, but an inference engine to automate probabilistic reasoning—a kind of Prolog for probability instead of logic. Bayesian programming is a formal and concrete implementation of this \"robot\".\n\nBayesian programming may also be seen as an algebraic formalism to specify graphical models such as, for instance, Bayesian networks, dynamic Bayesian networks, Kalman filters or hidden Markov models. Indeed, Bayesian Programming is more general than Bayesian networks and has a power of expression equivalent to probabilistic factor graphs.\n\nA Bayesian program is a means of specifying a family of probability distributions.\n\nThe constituent elements of a Bayesian program are presented below:\n\n\nThe purpose of a description is to specify an effective method of computing a joint probability distribution\non a set of variables formula_4 given a set of experimental data formula_3 and some\nspecification formula_2. This joint distribution is denoted as: formula_7.\n\nTo specify preliminary knowledge formula_2, the programmer must undertake the following:\n\n\nGiven a partition of formula_10 containing formula_11 subsets, formula_11 variables are defined\nformula_13, each corresponding to one of these subsets.\nEach variable formula_14 is obtained as the conjunction of the variables formula_15\nbelonging to the formula_16 subset. Recursive application of Bayes' theorem leads to:\n\nConditional independence hypotheses then allow further simplifications. A conditional\nindependence hypothesis for variable formula_14 is defined by choosing some variable formula_19\namong the variables appearing in the conjunction formula_20, labelling formula_21 as the\nconjunction of these chosen variables and setting:\n\nWe then obtain:\n\nSuch a simplification of the joint distribution as a product of simpler distributions is\ncalled a decomposition, derived using the chain rule.\n\nThis ensures that each variable appears at the most once on the left of a conditioning\nbar, which is the necessary and sufficient condition to write mathematically valid\ndecompositions.\n\nEach distribution formula_24 appearing in the product is then associated\nwith either a parametric form (i.e., a function formula_25) or a question to another Bayesian program formula_26.\n\nWhen it is a form formula_25, in general, formula_28 is a vector of parameters that may depend on formula_21 or formula_3 or both. Learning\ntakes place when some of these parameters are computed using the data set formula_3.\n\nAn important feature of Bayesian Programming is this capacity to use questions to other Bayesian programs as components of the definition of a new Bayesian program. formula_24 is obtained by some inferences done by another Bayesian program defined by the specifications formula_33 and the data formula_34. This is similar to calling a subroutine in classical programming and provides an easy way to build hierarchical models.\n\nGiven a description (i.e., formula_7), a question is obtained by partitioning formula_4\ninto three sets: the searched variables, the known variables and\nthe free variables.\n\nThe 3 variables formula_37, formula_38 and formula_39 are defined as the\nconjunction of the variables belonging to\nthese sets.\n\nA question is defined as the set\nof distributions:\n\nmade of many \"instantiated questions\" as the cardinal of formula_38,\neach instantiated question being the distribution:\n\nGiven the joint distribution formula_43, it is always possible to compute any possible question using the following general inference:\n\nwhere the first equality results from the marginalization rule, the second\nresults from Bayes' theorem and the third corresponds to a second application of marginalization. The denominator appears to be a normalization term and can be replaced by a constant formula_45.\n\nTheoretically, this allows to solve any Bayesian inference problem. In practice,\nhowever, the cost of computing exhaustively and exactly formula_46 is too great in almost all cases.\n\nReplacing the joint distribution by its decomposition we get:\n\nwhich is usually a much simpler expression to compute, as the dimensionality of the problem is considerably reduced by the decomposition into a product of lower dimension distributions.\n\nThe purpose of Bayesian spam filtering is to eliminate junk e-mails.\n\nThe problem is very easy to formulate. E-mails should be classified\ninto one of two categories: non-spam or spam. The only available information to classify the e-mails is their content: a set of words. Using these words without taking the order into account is commonly called a bag of words model.\n\nThe classifier should furthermore be able to adapt to its user and to learn\nfrom experience. Starting from an initial standard setting, the classifier should\nmodify its internal parameters when the user disagrees with its own decision.\nIt will hence adapt to the user’s criteria to differentiate between non-spam and\nspam. It will improve its results as it encounters increasingly classified e-mails.\n\nThe variables necessary to write this program are as follows:\n\nThese formula_53 binary variables sum up all the information\nabout an e-mail.\n\nStarting from the joint distribution and applying recursively Bayes' theorem we obtain:\n\nThis is an exact mathematical expression.\n\nIt can be drastically simplified by assuming that the probability of appearance of a word knowing the nature of the text (spam or not) is independent of the appearance of the other words. This is the naive Bayes assumption and this makes this spam filter a naive Bayes model.\n\nFor instance, the programmer can assume that:\n\nto finally obtain:\n\nThis kind of assumption is known as the naive Bayes' assumption. It is \"naive\" in the sense that the independence between words is clearly not completely true. For instance, it completely neglects that the appearance of pairs of words may be more significant than isolated appearances. However, the programmer may assume this hypothesis and may develop the model and the associated inferences to test how reliable and efficient it is.\n\nTo be able to compute the joint distribution, the programmer must now specify the\nformula_53 distributions appearing in the decomposition:\n\n\nwhere formula_64 stands for the number of appearances of the formula_52 word in non-spam e-mails and formula_66 stands for the total number of non-spam e-mails. Similarly, formula_67 stands for the number of appearances of the formula_52 word in spam e-mails and formula_69 stands for the total number of spam e-mails.\n\nThe formula_50 forms formula_61 are not yet completely specified because the formula_72 parameters formula_73, formula_74, formula_66 and formula_69 have no values yet.\n\nThe identification of these parameters could be done either by batch processing a series of classified e-mails or by an incremental updating of the parameters using the user's classifications of the e-mails as they arrive.\n\nBoth methods could be combined: the system could start with initial standard values of these parameters issued from a generic database, then some incremental learning customizes the classifier to each individual user.\n\nThe question asked to the program is: \"what is the probability for a given text to be spam knowing which words appear and don't appear in this text?\"\nIt can be formalized by:\n\nwhich can be computed as follows:\n\nThe denominator appears to be a normalization constant. It is not necessary to compute it to decide if we are dealing with spam. For instance, an easy trick is to compute the ratio:\n\nThis computation is faster and easier because it requires only formula_80 products.\n\nThe Bayesian spam filter program is completely defined by:\n\nBayesian filters (often called Recursive Bayesian estimation) are generic probabilistic models for time evolving processes. Numerous models are particular instances of this generic approach, for instance: the Kalman filter or the Hidden Markov model (HMM).\n\n\nThe decomposition is based:\n\nThe parametrical forms are not constrained and different choices lead to different well-known models: see Kalman filters and Hidden Markov models just below.\n\nThe typical question for such models is formula_94: what is the probability distribution for the state at time formula_95 knowing the observations from instant formula_83 to formula_88?\n\nThe most common case is Bayesian filtering where formula_98, which searches for the present state, knowing past observations.\n\nHowever it is also possible formula_99, to extrapolate a future state from past observations, or to do smoothing formula_100, to recover a past state from observations made either before or after that instant.\n\nMore complicated questions may also be asked as shown below in the HMM section.\n\nBayesian filters formula_101 have a very interesting recursive property, which contributes greatly to their attractiveness. formula_102 may be computed simply from formula_103 with the following formula:\n\nAnother interesting point of view for this equation is to consider that there are two phases: a\nprediction phase and an estimation phase:\n\n\nThe very well-known Kalman filters are a special case of Bayesian\nfilters.\n\nThey are defined by the following Bayesian program:\n\n\nWith these hypotheses and by using the recursive formula, it is possible to solve\nthe inference problem analytically to answer the usual formula_111 question.\nThis leads to an extremely efficient algorithm, which explains the popularity of Kalman filters and the number of their everyday applications.\n\nWhen there are no obvious linear transition and observation models, it is still often\npossible, using a first-order Taylor's expansion, to treat these models as locally linear.\nThis generalization is commonly called the extended Kalman filter.\n\nHidden Markov models (HMMs) are another very popular specialization of Bayesian filters.\n\nThey are defined by the following Bayesian program:\n\nboth specified using probability matrices.\n\nWhat is the most probable series of states that leads to the present state, knowing the past observations?\n\nThis particular question may be answered with a specific and very efficient algorithm\ncalled the Viterbi algorithm.\n\nThe Baum–Welch algorithm has been developed\nfor HMMs.\n\nSince 2000, Bayesian programming has been used to develop both robotics applications and life sciences models.\n\nIn robotics, bayesian programming was applied to autonomous robotics, robotic CAD systems, advanced driver-assistance systems, robotic arm control, mobile robotics, human-robot interaction, human-vehicle interaction (Bayesian autonomous driver models) video game avatar programming and training and real-time strategy games (AI).\n\nIn life sciences, bayesian programming was used in vision to reconstruct shape from motion, to model visuo-vestibular interaction and to study saccadic eye movements; in speech perception and control to study early speech acquisition and the emergence of articulatory-acoustic systems; and to model handwriting perception and control.\n\nBayesian program learning has potential applications voice recognition and synthesis, image recognition and natural language processing. It employs the principles of \"compositionality\" (building abstract representations from parts), \"causality\" (building complexity from parts) and \"learning to learn\" (using previously recognized concepts to ease the creation of new concepts).\n\nThe comparison between probabilistic approaches (not only bayesian programming) and possibility theories continues to be debated.\n\nPossibility theories like, for instance, fuzzy sets, fuzzy logic and possibility theory are alternatives to probability to model uncertainty. They argue that probability is insufficient or inconvenient to model certain aspects of incomplete/uncertain knowledge.\n\nThe defense of probability is mainly based on Cox's theorem, which starts from four postulates concerning rational reasoning in the presence of uncertainty. It demonstrates that the only mathematical framework that satisfies these postulates is probability theory. The argument is that any approach other than probability necessarily infringes one of these postulates and the value of that infringement.\n\nThe purpose of probabilistic programming is to unify the scope of classical programming languages with probabilistic modeling (especially bayesian networks) to deal with uncertainty while profiting from the programming languages' expressiveness to encode complexity.\n\nExtended classical programming languages include logical languages as proposed in Probabilistic Horn Abduction, Independent Choice Logic, PRISM, and ProbLog which proposes an extension of Prolog.\n\nIt can also be extensions of functional programming languages (essentially Lisp and Scheme) such as IBAL or CHURCH. The underlying programming languages can be object-oriented as in BLOG and FACTORIE or more standard ones as in CES and FIGARO.\n\nThe purpose of Bayesian programming is different. Jaynes' precept of \"probability as logic\" argues that probability is an extension of and an alternative to logic above which a complete theory of rationality, computation and programming can be rebuilt. Bayesian programming attempts to replace classical languages with a programming approach based on probability that considers incompleteness and uncertainty.\n\nThe precise comparison between the semantics and power of expression of Bayesian and probabilistic programming is an open question.\n\n",
    "id": "40888645",
    "title": "Bayesian programming"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=41255201",
    "text": "Winner-take-all in action selection\n\nWinner-take-all is a computer science concept that has been widely applied in behavior-based robotics as a method of action selection for intelligent agents. Winner-take-all systems work by connecting modules (task-designated areas) in such a way that when one action is performed it stops all other actions from being performed, so only one action is occurring at a time. The name comes from the idea that the \"winner\" action takes all of the motor system's power.\n\nIn the 1980s and 1990s, many roboticists and cognitive scientists were attempting to find speedier and more efficient alternatives to the traditional world modeling method of action selection. In 1982, Jerome A. Feldman and D.H. Ballard published the \"Connectionist Models and Their Properties\", referencing and explaining winner-take-all as a method of action selection. Feldman's architecture functioned on the simple rule that in a network of interconnected action modules, each module will set its own output to zero if it reads a higher input than its own in any other module. In 1986, Rodney Brooks introduced behavior-based artificial intelligence. Winner-take-all architectures for action selection soon became a common feature of behavior-based robots, because selection occurred at the level of the action modules (bottom-up) rather than at a separate cognitive level (top-down), producing a tight coupling of stimulus and reaction.\n\nIn the hierarchical architecture, actions or behaviors are programmed in a high-to-low priority list, with inhibitory connections between all the action modules. The agent performs low-priority behaviors until a higher-priority behavior is stimulated, at which point the higher behavior inhibits all other behaviors and takes over the motor system completely. Prioritized behaviors are usually key to the immediate survival of the agent, while behaviors of lower priority are less time-sensitive. For example, \"run away from predator\" would be ranked above \"sleep.\"\nWhile this architecture allows for clear programming of goals, many roboticists have moved away from the hierarchy because of its inflexibility.\n\nIn the heterarchy and fully distributed architecture, each behavior has a set of pre-conditions to be met before it can be performed, and a set of post-conditions that will be true after the action has been performed. These pre- and post-conditions determine the order in which behaviors must be performed and are used to causally connect action modules. This enables each module to receive input from other modules as well as from the sensors, so modules can recruit each other. For example, if the agent’s goal were to reduce thirst, the behavior \"drink\" would require the pre-condition of having water available, so the module would activate the module in charge of \"find water\". The activations organize the behaviors into a sequence, even though only one action is performed at a time. The distribution of larger behaviors across modules makes this system flexible and robust to noise. Some critics of this model hold that any existing set of division rules for the predecessor and conflictor connections between modules produce sub-par action selection. In addition, the feedback loop used in the model can in some circumstances lead to improper action selection.\n\nIn the arbiter and centrally coordinated architecture, the action modules are not connected to each other but to a central arbiter. When behaviors are triggered, they begin \"voting\" by sending signals to the arbiter, and the behavior with the highest number of votes is selected. In these systems, bias is created through the \"voting weight\", or how often a module is allowed to vote. Some arbiter systems take a different spin on this type of winner-take-all by using a \"compromise\" feature in the arbiter. Each module is able to vote for or against each smaller action in a set of actions, and the arbiter selects the action with the most votes, meaning that it benefits the most behavior modules. \n\nThis can be seen as violating the general rule against creating representations of the world in behavior-based AI, established by Brooks. By performing command fusion, the system is creating a larger composite pool of knowledge than is obtained from the sensors alone, forming a composite inner representation of the environment. Defenders of these systems argue that forbidding world-modeling puts unnecessary constraints on behavior-based robotics, and that agents benefits from forming representations and can still remain reactive.\n\n",
    "id": "41255201",
    "title": "Winner-take-all in action selection"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=41555934",
    "text": "Knowledge Based Software Assistant\n\nThe Knowledge Based Software Assistant (KBSA) was a research program funded by the United States Air Force. The goal of the program was to apply concepts from artificial intelligence to the problem of designing and implementing computer software. Software would be described by models in very high level languages (essentially equivalent to first order logic) and then transformation rules would transform the specification into efficient code. The air force hoped to be able to generate the software to control weapons systems and other command and control systems using this method. As software was becoming ever more critical to USAF weapons systems it was realized that improving the quality and productivity of the software development process could have significant benefits for the military, as well as for information technology in other major US industries.\n\nIn the early 1980s the United States Air Force realized that they had received significant benefits from applying artificial intelligence technologies to solving expert problems such as the diagnosis of faults in aircraft. The air force commissioned a group of researchers from the artificial intelligence and formal methods communities to develop a report on how such technologies might be used to aid in the more general problem of software development.\n\nThe report described a vision for a new approach to software development. Rather than define specifications with diagrams and manually transform them to code as was the current process, the KBSA vision was to define specifications in very high level languages and then to use transformation rules to gradually refine the specification into efficient code on heterogeneous platforms.\n\nEach step in the design and refinement of the system would be recorded as part of an integrated repository. In addition to the artifacts of software development the processes, the various definitions and transformations, would also be recorded in a way that they could be analyzed and also replayed later as needed. The idea was that each step would be a transformation that took into account various non-functional requirements for the implemented system. For example, requirements to use specific programming languages such as Ada or to harden code for real time mission critical fault tolerance.\n\nThe air force decided to fund further research on this vision through their Rome Air Development Center laboratory at Griffiss air force base in New York. The majority of the early research was conducted at the Kestrel Institute in Northern California (with Stanford University) and the Information Sciences Institute (ISI) in Southern California (with USC and UCLA). The Kestrel Institute focused primarily on the provably correct transformation of logical models to efficient code. ISI focused primarily on the front end of the process on defining specifications that could map to logical formalisms but were in formats that were intuitive and familiar to systems analysts. In addition, Raytheon did a project to investigate informal requirements gathering and Honeywell and Harvard University did work on underlying frameworks, integration, and activity coordination.\n\nAlthough not primarily funded by the KBSA program the MIT Programmer's Apprentice project also had many of the same goals and used the same techniques as KBSA.\n\nIn the later stages of the KBSA program (starting in 1991) researchers developed prototypes that were used on medium to large scale software development problems. Also, in these later stages the emphasis shifted from a pure KBSA approach to more general questions of how to use knowledge-based technology to supplement and augment existing and future computer-aided software engineering (CASE) tools. In these later stages there was significant interaction between the KBSA community and the object-oriented and software engineering communities. For example, KBSA concepts and researchers played an important role in the mega-programming and user centered software engineering programs sponsored by the Defense Advanced Research Projects Agency (DARPA). In these later stages the program changed its name to Knowledge-Based Software Engineering (KBSE). The name change reflected the different research goal, no longer to create a totally new all encompassing tool that would cover the complete software life cycle but to gradually work knowledge-based technology into existing tools. Companies such as Andersen Consulting (one of the largest system integrators and at the time vendor of their own CASE tool) played a major role in the program in these later stages.\n\nThe transformation rules that KBSA used were different than traditional rules for expert systems. Transformation rules matched against specification and implementation languages rather than against facts in the world. It was possible to specify transformations using patterns, wildcards, and recursion on both the right and left hand sides of a rule. The left hand expression would specify patterns in the existing knowledge base to search for. The right hand expression could specify a new pattern to transform the left hand side into. For example, transform a set theoretic data type into code using an Ada set library.\n\nThe initial purpose for transformation rules was to refine a high level logical specification into well designed code for a specific hardware and software platform. This was inspired by early work on theorem proving and automatic programming. However, researchers at the Information Sciences Institute (ISI) developed the concept of \"evolution transformations\". Rather than transforming a specification into code an evolution transformation was meant to automate various stereotypical changes at the specification level, for example developing a new superclass by extracting various capabilities from an existing class that can be shared more generally. Evolution transformations were developed at approximately the same time as the emergence of the software patterns community and the two groups shared concepts and technology. Evolution transformations were essentially what is known as refactoring in the object-oriented software patterns community.\n\nA key concept of KBSA was that all artifacts: requirements, specifications, transformations, designs, code, process models, etc. were represented as objects in a knowledge-based repository. The original KBSA report describes what was called a Wide Spectrum Language. The requirement was for a knowledge representation framework that could support the entire life cycle: requirements, specification, and code as well as the software process itself. The core representation for the knowledge base was meant to utilize the same framework although various layers could be added to support specific presentations and implementations.\n\nThese early knowledge-base frameworks were developed primarily by ISI and Kestrel building on top of Lisp and Lisp machine environments. The Kestrel environment was eventually turned into a commercial product called Refine which was developed and supported by a spin-off company from Kestrel called Reasoning Systems Incorporated.\n\nThe Refine language and environment also proved to be applicable to the problem of software reverse engineering: taking legacy code that is critical to the business but that lacks proper documentation and using tools to analyze it and transform it to a more maintainable form. With the growing concern of the Y2K problem reverse engineering was a major business concern for many large US corporations and it was a focus area for KBSA research in the 1990s.\n\nThere was significant interaction between the KBSA communities and the Frame language and object-oriented communities. The early KBSA knowledge-bases were implemented in object-based languages rather than object-oriented. Objects were represented as classes and sub-classes but it was not possible to define methods on the objects. In later versions of KBSA such as the Andersen Consulting Concept Demo the specification language was expanded to support message passing as well.\n\nKBSA took a different approach than traditional expert systems when it came to how to solve problems and work with users. In the traditional expert system approach the user answers a series of interactive questions and the system provides a solution. The KBSA approach left the user in control. Where as an expert system tried to, to some extent replace and remove the need for the expert the intelligent assistant approach in KBSA sought to re-invent the process with technology. This led to a number of innovations at the user interface level.\n\nAn example of the collaboration between the object-oriented community and KBSA was the architecture used for KBSA user interfaces. KBSA systems utilized a model-view-controller (MVC) user interface. This was an idea incorporated from Smalltalk environments. The MVC architecture was especially well suited to the KBSA user interface. KBSA environments featured multiple heterogeneous views of the knowledge-base. It might be useful to look at an emerging model from the standpoint of entities and relations, object interactions, class hierarchies, dataflow, and many other possible views. The MVC architecture facilitated this. With the MVC architecture the underlying model was always the knowledge base which was a meta-model description of the specification and implementation languages. When an analyst made some change via a particular diagram (e.g. added a class to the class hierarchy) that change was made at the underlying model level and the various views of the model were all automatically updated.\n\nOne of the benefits of using a transformation was that many aspects of the specification and implementation could be modified at once. For small scale prototypes the resulting diagrams were simple enough that basic layout algorithms combined with reliance on users to clean up diagrams was sufficient. However, when a transformation can radically redraw models with tens or even hundreds of nodes and links the constant updating of the various views becomes a task in itself. Researchers at Andersen Consulting incorporated work from the University of Illinois on graph theory to automatically update the various views associated with the knowledge base and to generate graphs that have minimal intersection of links and also take into account domain and user specific layout constraints.\n\nAnother concept used to provide intelligent assistance was automatic text generation. Early research at ISI investigated the feasibility of extracting formal specifications from informal natural language text documents. They determined that the approach was not viable. Natural language is by nature simply too ambiguous to serve as a good format for defining a system. However, natural language generation was seen to be feasible as a way to generate textual descriptions that could be read by managers and non-technical personnel. This was especially appealing to the air force since by law they required all contractors to generate various reports that describe the system from different points of view. Researchers at ISI and later Cogentext and Andersen Consulting demonstrated the viability of the approach by using their own technology to generate the documentation required by their air force contracts.\n",
    "id": "41555934",
    "title": "Knowledge Based Software Assistant"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=41644056",
    "text": "Inductive programming\n\nInductive programming (IP) is a special area of automatic programming, covering research from artificial intelligence and programming, which addresses learning of typically declarative (logic or functional) and often recursive programs from incomplete specifications, such as input/output examples or constraints.\n\nDepending on the programming language used, there are several kinds of inductive programming. Inductive functional programming, which uses functional programming languages such as Lisp or Haskell, and most especially inductive logic programming, which uses logic programming languages such as Prolog and other logical representations such as description logics, have been more prominent, but other (programming) language paradigms have also been used, such as constraint programming or probabilistic programming.\n\nInductive programming incorporates all approaches which are concerned with learning programs or algorithms from incomplete (formal) specifications. Possible inputs in an IP system are a set of training inputs and corresponding outputs or an output evaluation function, describing the desired behavior of the intended program, traces or action sequences which describe the process of calculating specific outputs, constraints for the program to be induced concerning its time efficiency or its complexity, various kinds of background knowledge such as standard data types, predefined functions to be used, program schemes or templates describing the data flow of the intended program, heuristics for guiding the search for a solution or other biases.\n\nOutput of an IP system is a program in some arbitrary programming language containing conditionals and loop or recursive control structures, or any other kind of Turing-complete representation language.\n\nIn many applications the output program must be correct with respect to the examples and partial specification, and this leads to the consideration of inductive programming as a special area inside automatic programming or program synthesis, usually opposed to 'deductive' program synthesis, where the specification is usually complete.\n\nIn other cases, inductive programming is seen as a more general area where any declarative programming or representation language can be used and we may even have some degree of error in the examples, as in general machine learning, the more specific area of structure mining or the area of symbolic artificial intelligence. A distinctive feature is the number of examples or partial specification needed. Typically, inductive programming techniques can learn from just a few examples.\n\nThe diversity of inductive programming usually comes from the applications and the languages that are used: apart from logic programming and functional programming, other programming paradigms and representation languages have been used or suggested in inductive programming, such as functional logic programming, constraint programming, probabilistic programming, abductive logic programming, modal logic, action languages, agent languages and many types of imperative languages.\n\nResearch on the inductive synthesis of recursive functional programs started in the early 1970s and was brought onto firm theoretical foundations with the seminal THESIS system of Summers and work of Biermann.\nThese approaches were split into two phases: first, input-output examples are transformed into non-recursive programs (traces) using a small set of basic operators; second, regularities in the traces are searched for and used to fold them into a recursive program. The main results until the mid 1980s are surveyed by Smith. Due to limited progress with respect to the range of programs that could be synthesized, research activities decreased significantly in the next decade.\n\nThe advent of logic programming brought a new elan but also a new direction in the early 1980s, especially due to the MIS system of Shapiro eventually spawning the new field of inductive logic programming (ILP). The early works of Plotkin, and his \"\"relative least general generalization (rlgg)\"\", had an enormous impact in inductive logic programming. Most of ILP work addresses a wider class of problems, as the focus is not only on recursive logic programs but on machine learning of symbolic hypotheses from logical representations. However, there were some encouraging results on learning recursive Prolog programs such as quicksort from examples together with suitable background knowledge, for example with GOLEM. But again, after initial success, the community got disappointed by limited progress about the induction of recursive programs with ILP less and less focusing on recursive programs and leaning more and more towards a machine learning setting with applications in relational data mining and knowledge discovery.\n\nIn parallel to work in ILP, Koza proposed genetic programming in the early 1990s as a generate-and-test based approach to learning programs. The idea of genetic programming was further developed into the inductive programming system ADATE and the systematic-search-based system MagicHaskeller. Here again, functional programs are learned from sets of positive examples together with an output evaluation (fitness) function which specifies the desired input/output behavior of the program to be learned.\n\nThe early work in grammar induction (also known as grammatical inference) is related to inductive programming, as rewriting systems or logic programs can be used to represent production rules. In fact, early works in inductive inference considered grammar induction and Lisp program inference as basically the same problem. The results in terms of learnability were related to classical concepts, such as identification-in-the-limit, as introduced in the seminal work of Gold. More recently, the language learning problem was addressed by the inductive programming community.\n\nIn the recent years, the classical approaches have been resumed and advanced with great success. Therefore, the synthesis problem has been reformulated on the background of constructor-based term rewriting systems taking into account modern techniques of functional programming, as well as moderate use of search-based strategies and usage of background knowledge as well as automatic invention of subprograms. Many new and successful applications have recently appeared beyond program synthesis, most especially in the area of data manipulation, programming by example and cognitive modelling (see below).\n\nOther ideas have also been explored with the common characteristic of using declarative languages for the representation of hypotheses. For instance, the use of higher-order features, schemes or structured distances have been advocated for a better handling of recursive data types and structures; abstraction has also been explored as a more powerful approach to cumulative learning and function invention.\n\nOne powerful paradigm that has been recently used for the representation of hypotheses in inductive programming (generally in the form of generative models) is probabilistic programming (and related paradigms, such as stochastic logic programs and Bayesian logic programming).\n\nThe first workshop on Approaches and Applications of Inductive Programming (AAIP) held in conjunction with ICML 2005 identified all applications where \"learning of programs or recursive rules are called for, [...] first in the domain of software engineering where structural learning, software assistants and software agents can help to relieve programmers from routine tasks, give programming support for end users, or support of novice programmers and programming tutor systems. Further areas of application are language learning, learning recursive control rules for AI-planning, learning recursive concepts in web-mining or for data-format transformations\".\n\nSince then, these and many other areas have shown to be successful application niches for inductive programming, such as end-user programming, the related areas of programming by example and programming by demonstration, and intelligent tutoring systems.\n\nOther areas where inductive inference has been recently applied are knowledge acquisition, artificial general intelligence, reinforcement learning and theory evaluation, and cognitive science in general. There may also be prospective applications in intelligent agents, games, robotics, personalisation, ambient intelligence and human interfaces.\n\n\n\n\n",
    "id": "41644056",
    "title": "Inductive programming"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=41780237",
    "text": "MANIC (cognitive architecture)\n\nMANIC, formerly known as PMML.1, is a cognitive architecture developed by the predictive modeling and machine learning laboratory at University Of Arkansas. It differs from other cognitive architectures in that it tries to \"minimize novelty\". That is, it attempts to organize well-established techniques in computer science, rather than propose any new methods for achieving cognition. While most other cognitive architectures are inspired by some neurological observation, and are subsequently developed in a top-down manner to behave in some manner like a brain, MANIC is inspired only by common practices in computer science, and was developed in a bottom-up manner for the purpose of unifying various methods in machine learning and artificial intelligence.\n\nAt the highest level, MANIC describes a software agent that, supposedly, will exhibit cognitive intelligence. The agent's artificial brain comprises two major components: a learning system and a decision-making system.\n\nThe learning system models the agent's environment as a dynamical system. It consists of an \"observation function\", which maps from the agent's current beliefs to predicted observations, and a \"transition function\", which maps from current beliefs to future beliefs in the next time-step. The observation function is implemented with a generative deep learning architecture. It is trained in an unsupervised manner from the observations that the agent makes. The intrinsic representations of those observations become the agents \"beliefs\". The transition function is trained in a supervised manner, to predict the next beliefs from the current ones. The entire learning system is based loosely on a 2011 paper by Michael S. Gashler that describes a method for training a deep neural network to model a simple dynamical system from visual observations.\n\nThe decision-making system consists of a planning module and a contentment function. The planning module uses an evolutionary algorithm to evolve a satisficing plan. The contentment function maps from the agent's current beliefs, or anticipated beliefs, to an evaluation of the utility of being in that state. It is trained by reinforcement from a human teacher. In order to facilitate this reinforcement learning, MANIC provides a mechanism for the agent to generate \"fantasy videos\" that show anticipated observations if a candidate plan were to be executed. The idea is that a human teacher would evaluate these videos and rank them according to desirability or utility, and the agent could then use that feedback to refine its contentment function.\n\nMANIC proposes that the learning system gives the agent awareness of its environment by modeling it, and using that model to anticipate future beliefs. It further proposes that a similar mechanism can also implement sentience. That is, it claims that awareness can be implemented with an outward-looking model, and sentience can be implemented with an inward-looking model. Therefore, it proposes to add \"introspective senses\", which theoretically give the agent the ability to become aware of its own inner feelings, by modeling them, just as it is aware of its external environment. To some extent, MANIC suggests that existing methods already in use in artificial intelligence are unintentionally creating subjective experiences like those typically associated with conscious beings.\n\n",
    "id": "41780237",
    "title": "MANIC (cognitive architecture)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=42042198",
    "text": "ADS-AC\n\nADS-AC is an experimental open source program which implements Absolutely Dynamic System, a proposed mechanism for AC.\n\nADS-AC uses a system of \"points\" or \"nodes\" connected by \"links\" to create a structure of \"knots\". The nodes have no properties other than their links to other nodes in order to maintain their absolute dynamicity.\n\n",
    "id": "42042198",
    "title": "ADS-AC"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=42349326",
    "text": "Vaumpus world\n\nWaumpus world is a simple world use in artificial intelligence for which to represent knowledge and to reason.Vaumpus world was introduced by Genesereth, and is discussed in Russell-Norvig Artificial intelligence book () inspired by 1972 video game Hunt the Wumpus .\n\n",
    "id": "42349326",
    "title": "Vaumpus world"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=42581062",
    "text": "Cognitive computing\n\nCognitive computing (CC) describes technology platforms that, broadly speaking, are based on the scientific disciplines of artificial intelligence and signal processing. These platforms encompass machine learning, reasoning, natural language processing, speech recognition and vision (object recognition), human–computer interaction, dialog and narrative generation, among other technologies.\n\nAt present, there is no widely agreed upon definition for cognitive computing in either academia or industry.\n\nIn general, the term cognitive computing has been used to refer to new hardware and/or software that mimics the functioning of the human brain (2004) and helps to improve human decision-making. In this sense, CC is a new type of computing with the goal of more accurate models of how the human brain/mind senses, reasons, and responds to stimulus. CC applications link data analysis and adaptive page displays (AUI) to adjust content for a particular type of audience. As such, CC hardware and applications strive to be more affective and more influential by design.\n\nSome features that cognitive systems may express are:\n\n\nCognitive computing-branded technology platforms typically specialize in the processing and analysis of large, unstructured datasets.\n\nWord processing documents, emails, videos, images, audio files, presentations, webpages, social media and many other data formats often need to be manually tagged with metadata before they can be fed to a computer for analysis and insight generation. The principal benefit of utilizing cognitive analytics over traditional big data analytics is that such datasets do not need to be pretagged.\n\nOther characteristics of a cognitive analytics system include:\n\n",
    "id": "42581062",
    "title": "Cognitive computing"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=17188",
    "text": "KL-ONE\n\nKL-ONE (pronounced \"kay ell won\") is a well known knowledge representation system in the tradition of semantic networks and frames; that is, it is a frame language. The system is an attempt to overcome semantic indistinctness in semantic network representations and to explicitly represent conceptual information as a structured inheritance network.\n\nThere is a whole family of KL-ONE-like systems. One of the innovations that KL-ONE initiated was the use of a deductive classifier, an automated reasoning engine that can validate a frame ontology and deduce new information about the ontology based on the initial information provided by a domain expert. \n\nFrames in KL-ONE are called concepts. These form hierarchies using subsume-relations; in the KL-ONE terminology a super class is said to subsume its subclasses. \nMultiple inheritance is allowed. Actually a concept is said to be well-formed only if it inherits from more than one other concept. All concepts, except the top concept (usually THING), must have at least one super class. \n\nIn KL-ONE descriptions are separated into two basic classes of concepts: primitive and defined. Primitives are domain concepts that are not fully defined. This means that given all the properties of a concept, this is not sufficient to classify it. They may also be viewed as incomplete definitions. Using the same view, defined concepts are complete definitions. Given the properties of a concept, these are necessary and sufficient conditions to classify the concept.\n\nThe slot-concept is called roles and the values of the roles are role-fillers. There are several different types of roles to be used in different situations. The most common and important role type is the generic RoleSet that captures the fact that the role may be filled with more than one filler.\n\n",
    "id": "17188",
    "title": "KL-ONE"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=485226",
    "text": "Frame language\n\nA frame language is a technology used for knowledge representation in artificial intelligence. Frames are stored as ontologies of sets and subsets of the frame concepts. They are similar to class hierarchies in object-oriented languages although their fundamental design goals are different. Frames are focused on explicit and intuitive representation of knowledge whereas objects focus on encapsulation and information hiding. Frames originated in AI research and objects primarily in software engineering. However, in practice the techniques and capabilities of frame and object-oriented languages overlap significantly.\n\nEarly work on Frames was inspired by psychological research going back to the 1930s that indicated people use stored stereotypical knowledge to interpret and act in new cognitive situations. The term Frame was first used by Marvin Minsky as a paradigm to understand visual reasoning and natural language processing. In these and many other types of problems the potential solution space for even the smallest problem is huge. For example, extracting the phonemes from a raw audio stream or detecting the edges of an object. Things which seem trivial to humans are actually quite complex. In fact, how difficult they really were was probably not fully understood until AI researchers began to investigate the complexity of getting computers to solve them.\n\nThe initial notion of Frames or Scripts as they were also called is that they would establish the context for a problem and in so doing automatically reduce the possible search space significantly. The idea was also adopted by Schank and Abelson who used it to illustrate how an AI system could process common human interactions such as ordering a meal at a restaurant. These interactions were standardized as Frames with slots that stored relevant information about each Frame. Slots are analogous to object properties in object-oriented modeling and to relations in entity-relation models. Slots often had default values but also required further refinement as part of the execution of each instance of the scenario. I.e., the execution of a task such as ordering at a restaurant was controlled by starting with a basic instance of the Frame and then instantiating and refining various values as appropriate. Essentially the abstract Frame represented an object class and the frame instances an object instance. In this early work the emphasis was primarily on the static data descriptions of the Frame. Various mechanisms were developed to define the range of a slot, default values, etc. However, even in these early systems there were procedural capabilities. One common technique was to use \"triggers\" (similar to the database concept of triggers) attached to slots. A trigger was simply procedural code that was attached to a slot. The trigger could fire either before and/or after a slot value was accessed or modified.\n\nAs with object classes, Frames were organized in subsumption hierarchies. For example, a basic frame might be ordering at a restaurant. An instance of that would be Joe goes to McDonald's. A specialization (essentially a subclass) of the restaurant frame would be a frame for ordering at a fancy restaurant. The fancy restaurant frame would inherit all the default values from the restaurant frame but also would either add more slots or change one or more of the default values (e.g., expected price range) for the specialized frame.\n\nMuch of the early Frame language research (e.g. Schank and Abelson) had been driven by findings from experimental psychology and attempts to design knowledge representation tools that corresponded to the patterns humans were thought to use to function in daily tasks. These researchers were less interested in mathematical formality since they believed such formalisms were not necessarily good models for the way the average human conceptualizes the world. The way humans use language for example is often far from truly logical.\n\nSimilarly, in linguistics, Charles J. Fillmore in the mid-1970s started working on his theory of frame semantics, which later would lead to computational resources like FrameNet. Frame semantics was motivated by reflections on human language and human cognition.\n\nResearchers such as Ron Brachman on the other hand wanted to give AI researchers the mathematical formalism and computational power that were associated with Logic. Their aim was to map the Frame classes, slots, constraints, and rules in a Frame language to set theory and logic. One of the benefits of this approach is that the validation and even creation of the models could be automated using theorem provers and other automated reasoning capabilities. The drawback was that it could be more difficult to initially specify the model in a language with a formal semantics.\n\nThis evolution also illustrates a classic divide in AI research known as the \"neats vs. scruffies\". The \"neats\" were researchers who placed the most value on mathematical precision and formalism which could be achieved via First Order Logic and Set Theory. The \"scruffies\" were more interested in modeling knowledge in representations that were intuitive and psychologically meaningful to humans.\n\nThe most notable of the more formal approaches was the KL-ONE language. KL-ONE later went on to spawn several subsequent Frame languages. The formal semantics of languages such as KL-ONE gave these frame languages a new type of automated reasoning capability known as the classifier. The classifier is an engine that analyzes the various declarations in the frame language: the definition of sets, subsets, relations, etc. The classifier can then automatically deduce various additional relations and can detect when some parts of a model are inconsistent with each other. In this way many of the tasks that would normally be executed by forward or backward chaining in an inference engine can instead be performed by the classifier.\n\nThis technology is especially valuable in dealing with the Internet. It is an interesting result that the formalism of languages such as KL-ONE can be most useful dealing with the highly informal and unstructured data found on the Internet. On the Internet it is simply not feasible to require all systems to standardize on one data model. It is inevitable that terminology will be used in multiple inconsistent forms. The automatic classification capability of the classifier engine provides AI developers with a powerful toolbox to help bring order and consistency to a very inconsistent collection of data (i.e., the Internet). The vision for an enhanced Internet, where pages are ordered not just by text keywords but by classification of concepts is known as the Semantic Web. Classification technology originally developed for Frame languages is a key enabler of the Semantic Web. The \"neats vs. scruffies\" divide also emerged in Semantic Web research, culminating in the creation of the Linking Open Data community—their focus was on exposing data on the Web rather than modeling.\n\nA simple example of concepts modeled in a frame language is the Friend of A Friend (FOAF) ontology defined as part of the Semantic Web as a foundation for social networking and calendar systems. The primary frame in this simple example is a \"Person\". Example slots are the person's \"email\", \"home page, phone,\" etc. The interests of each person can be represented by additional frames describing the space of business and entertainment domains. The slot \"knows\" links each person with other persons. Default values for a person's interests can be inferred by the web of people they are friends of.\n\nThe earliest Frame based languages were custom developed for specific research projects and were not packaged as tools to be re-used by other researchers. Just as with expert system inference engines, researchers soon realized the benefits of extracting part of the core infrastructure and developing general purpose frame languages that were not coupled to specific applications. One of the first general purpose frame languages was KRL. One of the most influential early Frame languages was KL-ONE KL-ONE spawned several subsequent Frame languages. One of the most widely used successors to KL-ONE was the Loom language developed by Robert MacGregor at the Information Sciences Institute.\n\nIn the 1980s Artificial Intelligence generated a great deal of interest in the business world fueled by expert systems. This led to the development of many commercial products for the development of knowledge-based systems. These early products were usually developed in Lisp and integrated constructs such as IF-THEN rules for logical reasoning with Frame hierarchies for representing data. One of the most well known of these early Lisp knowledge-base tools was the Knowledge Engineering Environment (KEE) from Intellicorp. KEE provided a full Frame language with multiple inheritance, slots, triggers, default values, and a rule engine that supported backward and forward chaining. As with most early commercial versions of AI software KEE was originally deployed in Lisp on Lisp machine platforms but was eventually ported to PCs and Unix workstations.\n\nThe research agenda of the Semantic Web spawned a renewed interest in automatic classification and frame languages. An example is the Web Ontology Language (OWL) standard for describing information on the Internet. OWL is a standard to provide a semantic layer on top of the Internet. The goal is that rather than organizing the web using keywords as most applications (e.g. Google) do today the web can be organized by concepts organized in an ontology.\n\nThe name of the OWL language itself provides a good example of the value of a Semantic Web. If one were to search for \"OWL\" using the Internet today most of the pages retrieved would be on the bird Owl rather than the standard OWL. With a Semantic Web it would be possible to specify the concept \"Web Ontology Language\" and the user would not need to worry about the various possible acronyms or synonyms as part of the search. Likewise the user would not need to worry about homonyms crowding the search results with irrelevant data such as information about birds of prey as in this simple example.\n\nIn addition to OWL various standards and technologies that are relevant to the Semantic Web and were influenced by Frame languages include OIL and DAML. The Protege Open Source software tool from Stanford University provides an ontology editing capability that is built on OWL and has the full capabilities of a classifier. However it ceased to explicitly support frames as of version 3.5 (which is maintained for those preferring frame orientation), the version current in 2017 being 5. The justification for moving from explicit frames being that OWL DL is more expressive and \"industry standard\". \n\nFrame languages have a significant overlap with object-oriented languages. The terminologies and goals of the two communities were different but as they moved from the academic world and labs to the commercial world developers tended to not care about philosophical issues and focused primarily on specific capabilities, taking the best from either camp regardless of where the idea began. What both paradigms have in common is a desire to reduce the distance between concepts in the real world and their implementation in software. As such both paradigms arrived at the idea of representing the primary software objects in taxonomies starting with very general types and progressing to more specific types.\n\nThe following table illustrates the correlation between standard terminology from the object-oriented and frame language communities:\n\nThe primary difference between the two paradigms was in the degree that encapsulation was considered a major requirement. For the object-oriented paradigm encapsulation was one of the if not the most critical requirement. The desire to reduce the potential interactions between software components and hence manage large complex systems was a key driver of object-oriented technology. For the frame language camp this requirement was less critical than the desire to provide a vast array of possible tools to represent rules, constraints, and programming logic. In the object-oriented world everything is controlled by methods and the visibility of methods. So for example, accessing the data value of an object property must be done via an accessor method. This method controls things such as validating the data type and constraints on the value being retrieved or set on the property. In Frame languages these same types of constraints could be handled in multiple ways. Triggers could be defined to fire before or after a value was set or retrieved. Rules could be defined that managed the same types of constraints. The slots themselves could be augmented with additional information (called \"facets\" in some languages) again with the same type of constraint information.\n\nThe other main differentiator between frame and OO languages was multiple inheritance (allowing a frame or class to have two or more superclasses). For frame languages multiple inheritance was a requirement. This follows from the desire to model the world the way humans do, human conceptualizations of the world seldom fall into rigidly defined non-overlapping taxonomies. For many OO languages, especially in the later years of OO, single inheritance was either strongly desired or required. Multiple inheritance was seen as a possible step in the analysis phase to model a domain but something that should be eliminated in the design and implementation phases in the name of maintaining encapsulation and modularity.\n\nAlthough the early frame languages such as KRL did not include message passing, driven by the demands of developers, most of the later frame languages (e.g. Loom, KEE) included the ability to define messages on Frames.\n\nOn the object-oriented side, standards have also emerged that provide essentially the equivalent functionality that frame languages provided, albeit in a different format and all standardized on object libraries. For example, the Object Management Group has standardized specifications for capabilities such as associating test data and constraints with objects (analogous to common uses for facets in Frames and to constraints in Frame languages such as Loom) and for integrating rule engines.\n\n\n\n",
    "id": "485226",
    "title": "Frame language"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=43249770",
    "text": "Knowledge acquisition\n\nKnowledge acquisition is the process used to define the rules and ontologies required for a knowledge-based system. The phrase was first used in conjunction with expert systems to describe the initial tasks associated with developing an expert system, namely finding and interviewing domain experts and capturing their knowledge via rules, objects, and frame-based ontologies. \n\nExpert systems were one of the first successful applications of artificial intelligence technology to real world business problems. Researchers at Stanford and other AI laboratories worked with doctors and other highly skilled experts to develop systems that could automate complex tasks such as medical diagnosis. Until this point computers had mostly been used to automate highly data intensive tasks but not for complex reasoning. Technologies such as inference engines allowed developers for the first time to tackle more complex problems. \n\nAs expert systems scaled up from demonstration prototypes to industrial strength applications it was soon realized that the acquisition of domain expert knowledge was one of if not the most critical task in the knowledge engineering process. This knowledge acquisition process became an intense area of research on its own. One of the earlier works on the topic used Batesonian theories of learning to guide the process.\n\nOne approach to knowledge acquisition investigated was to use natural language parsing and generation to facilitate knowledge acquisition. Natural language parsing could be performed on manuals and other expert documents and an initial first pass at the rules and objects could be developed automatically. Text generation was also extremely useful in generating explanations for system behavior. This greatly facilitated the development and maintenance of expert systems.\n\nA more recent approach to knowledge acquisition is a re-use based approach. Knowledge can be developed in ontologies that conform to standards such as the Web Ontology Language (OWL). In this way knowledge can be standardized and shared across a broad community of knowledge workers. One example domain where this approach has been successful is bioinformatics. \n",
    "id": "43249770",
    "title": "Knowledge acquisition"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=43274058",
    "text": "Knowledge-based recommender system\n\nKnowledge-based recommender systems (knowledge based recommenders) are a specific type of recommender system that are based on explicit knowledge about the item assortment, user preferences, and recommendation criteria (i.e., which item should be recommended in which context). These systems are applied in scenarios where alternative approaches such as collaborative filtering and content-based filtering cannot be applied.\n\nA major strength of knowledge-based recommender systems is the non-existence of cold-start (ramp-up) problems. A corresponding drawback is a potential knowledge acquisition bottleneck triggered by the need to define recommendation knowledge in an explicit fashion.\n\nKnowledge-based recommender systems are well suited to complex domains where items are not purchased very often, such as apartments and cars. Further examples of item domains relevant for knowledge-based recommender systems are financial services, digital cameras, and tourist destinations. Rating-based systems often do not perform well in these domains due to the low number of available ratings.\n\nAdditionally, in complex item domains, customers want to specify their preferences explicitly (e.g., \"the maximum price of the car is X\") . In this context, the recommender system must take into account constraints: for instance, only those financial services that support the investment period specified by the customer should be recommended. Neither of these aspects are supported by approaches such as collaborative filtering and content-based filtering.\n\nKnowledge-based recommender systems are often conversational, i.e., user requirements and preferences are elicited within the scope of a feedback loop. A major reason for the conversational nature of knowledge-based recommender systems is the complexity of the item domain where it is often impossible to articulate all user preferences at once. Furthermore, user preferences are typically not known exactly at the beginning but are constructed within the scope of a recommendation session.\n\nIn a search-based recommender, user feedback is given in terms of answers to questions which restrict the set of relevant items. An example of such a question is \"Which type of lens system do you prefer: fixed or exchangeable lenses?\". On the technical level, search-based recommendation scenarios can be implemented on the basis of constraint-based recommender systems. Constraint-based recommender systems are implemented on the basis of constraint search or different types of conjunctive query-based approaches.\n\nIn a navigation-based recommender, user feedback is typically provided in terms of \"critiques\" which specify change requests regarding the item currently recommended to the user. Critiques are then used for the recommendation of the next \"candidate\" item. An example of a critique in the context of a digital camera recommendation scenario is \"I would like to have a camera like this but with a lower price\". This is an example of a \"unit critique\" which represents a change request on a single item attribute. \"Compound critiques\" allow the specification of more than one change request at a time. \"Dynamic critiquing\" also takes into account preceding user critiques (the critiquing history). More recent approaches additionally exploit information stored in user interaction logs to further reduce the interaction effort in terms of the number of needed critiquing cycles.\n\n",
    "id": "43274058",
    "title": "Knowledge-based recommender system"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=15893057",
    "text": "Applications of artificial intelligence\n\nArtificial intelligence, defined as intelligence exhibited by machines, has many applications in today's society. More specifically, it is Weak AI, the form of A.I. where programs are developed to perform specific tasks, that is being utilized for a wide range of activities including medical diagnosis, electronic trading, robot control, and remote sensing. AI has been used to develop and advance numerous fields and industries, including finance, healthcare, education, transportation, and more.\n\nSeveral U.S. academic institutions are employing AI to tackle some of the world's greatest economic and social challenges. For example, the University of Southern California launched the Center for Artificial Intelligence in Society, with the goal of using AI to address socially relevant problems such as homelessness. At Stanford, researchers are using AI to analyze satellite images to identify which areas have the highest poverty levels.\n\nThe Air Operations Division (AOD) uses AI for the rule based expert systems. The AOD has use for artificial intelligence for surrogate operators for combat and training simulators, mission management aids, support systems for tactical decision making, and post processing of the simulator data into symbolic summaries.\n\nThe use of artificial intelligence in simulators is proving to be very useful for the AOD. Airplane simulators are using artificial intelligence in order to process the data taken from simulated flights. Other than simulated flying, there is also simulated aircraft warfare. The computers are able to come up with the best success scenarios in these situations. The computers can also create strategies based on the placement, size, speed and strength of the forces and counter forces. Pilots may be given assistance in the air during combat by computers. The artificial intelligent programs can sort the information and provide the pilot with the best possible maneuvers, not to mention getting rid of certain maneuvers that would be impossible for a human being to perform. Multiple aircraft are needed to get good approximations for some calculations so computer simulated pilots are used to gather data. These computer simulated pilots are also used to train future air traffic controllers.\n\nThe system used by the AOD in order to measure performance was the Interactive Fault Diagnosis and Isolation System, or IFDIS. It is a rule based expert system put together by collecting information from TF-30 documents and the expert advice from mechanics that work on the TF-30. This system was designed to be used for the development of the TF-30 for the RAAF F-111C. The performance system was also used to replace specialized workers. The system allowed the regular workers to communicate with the system and avoid mistakes, miscalculations, or having to speak to one of the specialized workers.\n\nThe AOD also uses artificial intelligence in speech recognition software. The air traffic controllers are giving directions to the artificial pilots and the AOD wants to the pilots to respond to the ATC's with simple responses. The programs that incorporate the speech software must be trained, which means they use neural networks. The program used, the Verbex 7000, is still a very early program that has plenty of room for improvement. The improvements are imperative because ATCs use very specific dialog and the software needs to be able to communicate correctly and promptly every time.\n\nThe Artificial Intelligence supported Design of Aircraft, or AIDA, is used to help designers in the process of creating conceptual designs of aircraft. This program allows the designers to focus more on the design itself and less on the design process. The software also allows the user to focus less on the software tools. The AIDA uses rule based systems to compute its data. This is a diagram of the arrangement of the AIDA modules. Although simple, the program is proving effective.\n\nIn 2003, NASA's Dryden Flight Research Center, and many other companies, created software that could enable a damaged aircraft to continue flight until a safe landing zone can be reached. The software compensates for all the damaged components by relying on the undamaged components. The neural network used in the software proved to be effective and marked a triumph for artificial intelligence.\n\nThe Integrated Vehicle Health Management system, also used by NASA, on board an aircraft must process and interpret data taken from the various sensors on the aircraft. The system needs to be able to determine the structural integrity of the aircraft. The system also needs to implement protocols in case of any damage taken the vehicle.\n\nHaitham Baomar and Peter Bentley are leading a team from the University College of London to develop an artificial intelligence based Intelligent Autopilot System (IAS) designed to teach an autopilot system to behave like a highly experienced pilot who is faced with an emergency situation such as severe weather, turbulence, or system failure. Educating the autopilot relies on the concept of supervised machine learning “which treats the young autopilot as a human apprentice going to a flying school”. The autopilot records the actions of the human pilot generating learning models using artificial neural networks. The autopilot is then given full control and observed by the pilot as it executes the training exercise. \n\nThe Intelligent Autopilot System combines the principles of Apprenticeship Learning and Behavioral Cloning whereby the autopilot observes the low-level actions required to maneuver the airplane and the high-level strategy used to apply those actions. IAS implementation employs three phases; pilot data collection, training, and autonomous control. Baomar and Bentley’s goal is to create a more autonomous autopilot to assist pilots in responding to emergency situations.\n\nAI researchers have created many tools to solve the most difficult problems in computer science. Many of their inventions have been adopted by mainstream computer science and are no longer considered a part of AI. (See AI effect.) According to , all of the following were originally developed in AI laboratories:\ntime sharing,\ninteractive interpreters,\ngraphical user interfaces and the computer mouse,\nrapid development environments,\nthe linked list data structure,\nautomatic storage management,\nsymbolic programming,\nfunctional programming,\ndynamic programming and\nobject-oriented programming.\n\nThere are a number of companies that create robots to teach subjects to children ranging from biology to computer science, though such tools have not become widespread yet. There have also been a rise of intelligent tutoring systems, or ITS, in higher education. For example, an ITS called SHERLOCK teaches Air Force technicians to diagnose electrical systems problems in aircraft. Another example is DARPA, Defense Advanced Research Projects Agency, which used AI to develop a digital tutor to train its Navy recruits in technical skills in a shorter amount of time. Universities have been slow in adopting AI technologies due to either a lack of funding or skepticism of the effectiveness of these tools, but in the coming years more classrooms will be utilizing technologies such as ITS to complement teachers.\n\nAdvancements in natural language processing, combined with machine learning, have also enabled automatic grading of assignments as well as a data-driven understanding of individual students’ learning needs. This led to an explosion in popularity of MOOCs, or Massive Open Online Courses, which allows students from around the world to take classes online. Data sets collected from these large scale online learning systems have also enabled learning analytics, which will be used to improve the quality of learning at scale. Examples of how learning analytics can be used to improve the quality of learning include predicting which students are at risk of failure and analyzing student engagement.\n\nAlgorithmic trading involves the use of complex AI systems to make trading decisions at speeds several orders of magnitudes greater than any human is capable of, often making millions of trades in a day without any human intervention. Automated trading systems are typically used by large institutional investors.\n\nSeveral large financial institutions have invested in AI engines to assist with their investment practices. BlackRock’s AI engine, Aladdin, is used both within the company and to clients to help with investment decisions. Its wide range of functionalities includes the use of natural language processing to read text such as news, broker reports, and social media feeds. It then gauges the sentiment on the companies mentioned and assigns a score. Banks such as UBS and Deutsche Bank use an AI engine called Sqreem (Sequential Quantum Reduction and Extraction Model) which can mine data to develop consumer profiles and match them with the wealth management products they’d most likely want. Goldman Sachs uses Kensho, a market analytics platform that combines statistical computing with big data and natural language processing. Its machine learning systems mine through hoards of data on the web and assess correlations between world events and their impact on asset prices.\nInformation Extraction, part of artificial intelligence, is used to extract information from live news feed and to assist with investment decisions.\n\nSeveral products are emerging that utilize AI to assist people with their personal finances. For example, Digit is an app powered by artificial intelligence that automatically helps consumers optimize their spending and savings based on their own personal habits and goals. The app can analyze factors such as monthly income, current balance, and spending habits, then make its own decisions and transfer money to the savings account. Wallet.AI, an upcoming startup in San Francisco, builds agents that analyze data that a consumer would leave behind, from Smartphone check-ins to tweets, to inform the consumer about their spending behavior.\n\nRobo-advisors are becoming more widely used in the investment management industry. Robo-advisors provide financial advice and portfolio management with minimal human intervention. This class of financial advisers work based on algorithms built to automatically develop a financial portfolio according to the investment goals and risk tolerance of the clients. It can adjust to real-time changes in the market and accordingly calibrate the portfolio.\n\nAn online lender, Upstart, analyze vast amounts of consumer data and utilizes machine learning algorithms to develop credit risk models that predict a consumer’s likelihood of default. Their technology will be licensed to banks for them to leverage for their underwriting processes as well.\n\nZestFinance developed their Zest Automated Machine Learning (ZAML) Platform specifically for credit underwriting as well. This platform utilizes machine learning to analyze tens of thousands traditional and nontraditional variables (from purchase transactions to how a customer fills out a form) used in the credit industry to score borrowers. The platform is particularly useful to assign credit scores to those with limited credit histories, such as millennials.\n\nRobots have become common in many industries and are often given jobs that are considered dangerous to humans. Robots have proven effective in jobs that are very repetitive which may lead to mistakes or accidents due to a lapse in concentration and other jobs which humans may find degrading.\n\nIn 2014, China, Japan, the United States, the Republic of Korea and Germany together amounted to 70% of the total sales volume of robots. In the automotive industry, a sector with particularly high degree of automation, Japan had the highest density of industrial robots in the world: per employees.\n\nArtificial neural networks are used as clinical decision support systems for medical diagnosis, such as in Concept Processing technology in EMR software.\n\nOther tasks in medicine that can potentially be performed by artificial intelligence and are beginning to be developed include:\nCurrently, there are over 90 AI startups in the health industry working in these fields.\n\nAnother application of AI is in the human resources and recruiting space. There are three ways AI is being used by human resources and recruiting professionals. AI is used to screen resumes and rank candidates according to their level of qualification. Ai is also used to predict candidate success in given roles through job matching platforms. And now, AI is rolling out recruiting chat bots that can automate repetitive communication tasks.\n\nTypically, resume screening involves a recruiter or other HR professional scanning through a database of resumes. Now startups like Pomato, are creating machine learning algorithms to automate resume screening processes. Pomato’s resume screening AI focuses on automating validating technical applicants for technical staffing firms. Pomato’ s AI performs over 200,000 computations on each resume in seconds then designs a custom technical interview based on the mined skills.\n\nFrom 2016 to 2017, consumer goods company Unilever used artificial intelligence to screen all entry level employees. Unilever’s AI used neuroscience based games, recorded interviews, and facial/speech analysis to predict hiring success. Unilever partnered with Pymetrics and HireVue to enable its novel AI based screening and increased their applicants from 15,000 to 30,000 in a single year. Recruiting with AI also produced Unililever’s “most diverse class to date.’ Unilever also decreased time to hire from 4 months to 4 weeks and saved over 50,000 hours of recruiter time.\n\nFrom resume screening to neuroscience, speech recognition, and facial analysis...it’s clear AI are having a massive impact on the human resources field. The latest development in AI is in recruiting chatbots. TextRecruit, a Bay Area startup, released Ari (automated recruiting interface.) Ari is a recruiting chatbot that is designed to hold two way texting conversations with candidates. Ari automates posting jobs, advertising openings, screening candidates, scheduling interviews, and nurturing candidate relationships with updates as they progress along the hiring funnel. Ari is currently offered as part of TextRecruit’s candidate engagement platform.\n\nWhile the evolution of music has always been affected by technology, artificial intelligence has enabled, through scientific advances, to emulate, at some extent, human-like composition.\n\nAmong notable early efforts, David Cope created an AI called Emily Howell that managed to become well known in the field of Algorithmic Computer Music. The algorithm behind Emily Howell is registered as a US patent.\n\nThe AI Iamus created 2012 the first complete classical album fully composed by a computer.\n\nOther endeavours, like AIVA (Artificial Intelligence Virtual Artist), focus on composing symphonic music, mainly classical music for film scores. It achieved a world first by becoming the first virtual composer to be recognized by a musical professional association.\n\nArtificial intelligences can even produce music usable in a medical setting, with Melomics’s effort to use computer-generated music for stress and pain relief.\n\nMoreover, initiatives such as Google Magenta, conducted by the Google Brain team, want to find out if an artificial intelligence can be capable of creating compelling art.\n\nAt Sony CSL Research Laboratory, their Flow Machines software has created pop songs by learning music styles from a huge database of songs. By analyzing unique combinations of styles and optimizing techniques, it can compose in any style.\n\nThe company Narrative Science makes computer generated news and reports commercially available, including summarizing team sporting events based on statistical data from the game in English. It also creates financial reports and real estate analyses. Similarly, the company Automated Insights generates personalized recaps and previews for Yahoo Sports Fantasy Football. The company is projected to generate one billion stories in 2014, up from 350 million in 2013.\n\nEchobox is a software company that helps publishers increase traffic by 'intelligently' posting articles on social media platforms such as Facebook and Twitter. By analysing large amounts of data, it learns how specific audiences respond to different articles at different times of the day. It then chooses the best stories to post and the best times to post them. It uses both historical and real-time data to understand to what has worked well in the past as well as what is currently trending on the web.\n\nAnother company, called Yseop, uses artificial intelligence to turn structured data into intelligent comments and recommendations in natural language. Yseop is able to write financial reports, executive summaries, personalized sales or marketing documents and more at a speed of thousands of pages per second and in multiple languages including English, Spanish, French & German.\n\nBoomtrain’s is another example of AI that is designed to learn how to best engage each individual reader with the exact articles — sent through the right channel at the right time — that will be most relevant to the reader. It’s like hiring a personal editor for each individual reader to curate the perfect reading experience.\n\nThere is also the possibility that AI will write work in the future. In 2016, a Japanese AI co-wrote a short story and almost won a literary prize.\n\nArtificial intelligence is implemented in automated online assistants that can be seen as avatars on web pages. It can avail for enterprises to reduce their operation and training cost. A major underlying technology to such systems is natural language processing. Pypestream uses automated customer service for its mobile application designed to streamline communication with customers.\n\nCurrently, major companies are investing in AI to handle difficult customer in the future. Google's most recent development analyzes language and converts speech into text. The platform can identify angry customers through their language and respond appropriately.\n\nCompanies have been working on different aspects of customer service to improve this aspect of a company.\n\n\"Digital Genius\", an AI start-up, researches the database of information (from past conversations and frequently asked questions) more efficiently and provide prompts to agents to help them resolve queries more efficiently.\n\n\"IPSoft\" is creating technology with emotional intelligence to adapt the customer's interaction. The response is linked to the customer's tone, with the objective of being able to show empathy. Another element IPSoft is developing is the ability to adapt to different tones or languages.\n\n\"Inbenta’s\" is focused on developing natural language. In other words, on understanding the meaning behind what someone is asking and not just looking at the words used, using context and natural language processing. One customer service element Ibenta has already achieved is its ability to respond in bulk to email queries.\n\nMany telecommunications companies make use of\nheuristic search in the management of their workforces, for example BT Group has deployed heuristic search in a scheduling application that provides the work schedules of 20,000 engineers.\n\nThe 1990s saw some of the first attempts to mass-produce domestically aimed types of basic Artificial Intelligence for education, or leisure. This prospered greatly with the Digital Revolution, and helped introduce people, especially children, to a life of dealing with various types of Artificial Intelligence, specifically in the form of Tamagotchis and Giga Pets, iPod Touch, the Internet, and the first widely released robot, Furby. A mere year later an improved type of domestic robot was released in the form of Aibo, a robotic dog with intelligent features and autonomy.\n\nCompanies like Mattel have been creating an assortment of AI-enabled toys for kids as young as age three. Using proprietary AI engines and speech recognition tools, they are able to understand conversations, give intelligent responses and learn quickly.\n\nAI has also been applied to video games, for example video game bots, which are designed to stand in as opponents where humans aren't available or desired.\n\nFuzzy logic controllers have been developed for automatic gearboxes in automobiles. For example, the 2006 Audi TT, VW Touareg and VW Caravell feature the DSP transmission which utilizes Fuzzy Logic. A number of Škoda variants (Škoda Fabia) also currently include a Fuzzy Logic-based controller.\n\nToday's cars now have AI-based driver assist features such as self-parking and advanced cruise controls. AI has been used to optimize traffic management applications, which in turn reduces wait times, energy use, and emissions by as much as 25 percent. In the future, fully autonomous cars will be developed. AI in transportation is expected to provide safe, efficient, and reliable transportation while minimizing the impact on the environment and communities. The major challenge to developing this AI is the fact that transportation systems are inherently complex systems involving a very large number of components and different parties, each having different and often conflicting objectives.\n\nVarious tools of artificial intelligence are also being widely deployed in homeland security, speech and text recognition, data mining, and e-mail spam filtering. Applications are also being developed for gesture recognition (understanding of sign language by machines), individual voice recognition, global voice recognition (from a variety of people in a noisy room), facial expression recognition for interpretation of emotion and non verbal cues. Other applications are robot navigation, obstacle avoidance, and object recognition.\n\n\n\n\n\n",
    "id": "15893057",
    "title": "Applications of artificial intelligence"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=6596",
    "text": "Computer vision\n\nComputer vision is an interdisciplinary field that deals with how computers can be made for gaining high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do.\n\nComputer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, \"e.g.\", in the forms of decisions. Understanding in this context means the transformation of visual images (the input of the retina) into descriptions of the world that can interface with other thought processes and elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory.\n\nAs a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, or multi-dimensional data from a medical scanner. As a technological discipline, computer vision seeks to apply its theories and models for the construction of computer vision systems.\n\nSub-domains of computer vision include scene reconstruction, event detection, video tracking, object recognition, 3D pose estimation, learning, indexing, motion estimation, and image restoration.\n\nComputer vision is an interdisciplinary field that deals with how computers can be made for gaining high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do. \"Computer vision is concerned with the automatic extraction, analysis and understanding of useful information from a single image or a sequence of images. It involves the development of a theoretical and algorithmic basis to achieve automatic visual understanding.\" As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, or multi-dimensional data from a medical scanner. As a technological discipline, computer vision seeks to apply its theories and models for the construction of computer vision systems.\n\nIn the late 1960s, computer vision began at universities that were pioneering artificial intelligence. It was meant to mimic the human visual system, as a stepping stone to endowing robots with intelligent behavior. In 1966, it was believed that this could be achieved through a summer project, by attaching a camera to a computer and having it \"describe what it saw\".\n\nWhat distinguished computer vision from the prevalent field of digital image processing at that time was a desire to extract three-dimensional structure from images with the goal of achieving full scene understanding. Studies in the 1970s formed the early foundations for many of the computer vision algorithms that exist today, including extraction of edges from images, labeling of lines, non-polyhedral and polyhedral modeling, representation of objects as interconnections of smaller structures, optical flow, and motion estimation.\n\nThe next decade saw studies based on more rigorous mathematical analysis and quantitative aspects of computer vision. These include the concept of scale-space, the inference of shape from various cues such as shading, texture and focus, and contour models known as snakes. Researchers also realized that many of these mathematical concepts could be treated within the same optimization framework as regularization and Markov random fields.\nBy the 1990s, some of the previous research topics became more active than the others. Research in projective 3-D reconstructions led to better understanding of camera calibration. With the advent of optimization methods for camera calibration, it was realized that a lot of the ideas were already explored in bundle adjustment theory from the field of photogrammetry. This led to methods for sparse 3-D reconstructions of scenes from multiple images. Progress was made on the dense stereo correspondence problem and further multi-view stereo techniques. At the same time, variations of graph cut were used to solve image segmentation. This decade also marked the first time statistical learning techniques were used in practice to recognize faces in images (see Eigenface). Toward the end of the 1990s, a significant change came about with the increased interaction between the fields of computer graphics and computer vision. This included image-based rendering, image morphing, view interpolation, panoramic image stitching and early light-field rendering.\n\nRecent work has seen the resurgence of feature-based methods, used in conjunction with machine learning techniques and complex optimization frameworks.\n\nAreas of artificial intelligence deal with autonomous planning or deliberation for robotical systems to navigate through an environment. A detailed understanding of these environments is required to navigate through them. Information about the environment could be provided by a computer vision system, acting as a vision sensor and providing high-level information about the environment and the robot.\n\nArtificial intelligence and computer vision share other topics such as pattern recognition and learning techniques. Consequently, computer vision is sometimes seen as a part of the artificial intelligence field or the computer science field in general.\n\nSolid-state physics is another field that is closely related to computer vision. Most computer vision systems rely on image sensors, which detect electromagnetic radiation, which is typically in the form of either visible or infra-red light. The sensors are designed using quantum physics. The process by which light interacts with surfaces is explained using physics. Physics explains the behavior of optics which are a core part of most imaging systems. Sophisticated image sensors even require quantum mechanics to provide a complete understanding of the image formation process. Also, various measurement problems in physics can be addressed using computer vision, for example motion in fluids.\n\nA third field which plays an important role is neurobiology, specifically the study of the biological vision system. Over the last century, there has been an extensive study of eyes, neurons, and the brain structures devoted to processing of visual stimuli in both humans and various animals. This has led to a coarse, yet complicated, description of how \"real\" vision systems operate in order to solve certain vision related tasks. These results have led to a subfield within computer vision where artificial systems are designed to mimic the processing and behavior of biological systems, at different levels of complexity. Also, some of the learning-based methods developed within computer vision (\"e.g.\" neural net and deep learning based image and feature analysis and classification) have their background in biology.\n\nSome strands of computer vision research are closely related to the study of biological vision – indeed, just as many strands of AI research are closely tied with research into human consciousness, and the use of stored knowledge to interpret, integrate and utilize visual information. The field of biological vision studies and models the physiological processes behind visual perception in humans and other animals. Computer vision, on the other hand, studies and describes the processes implemented in software and hardware behind artificial vision systems. Interdisciplinary exchange between biological and computer vision has proven fruitful for both fields.\n\nYet another field related to computer vision is signal processing. Many methods for processing of one-variable signals, typically temporal signals, can be extended in a natural way to processing of two-variable signals or multi-variable signals in computer vision. However, because of the specific nature of images there are many methods developed within computer vision which have no counterpart in processing of one-variable signals. Together with the multi-dimensionality of the signal, this defines a subfield in signal processing as a part of computer vision.\n\nBeside the above-mentioned views on computer vision, many of the related research topics can also be studied from a purely mathematical point of view. For example, many methods in computer vision are based on statistics, optimization or geometry. Finally, a significant part of the field is devoted to the implementation aspect of computer vision; how existing methods can be realized in various combinations of software and hardware, or how these methods can be modified in order to gain processing speed without losing too much performance.\n\nThe fields most closely related to computer vision are image processing, image analysis and machine vision. There is a significant overlap in the range of techniques and applications that these cover. This implies that the basic techniques that are used and developed in these fields are similar, something which can be interpreted as there is only one field with different names. On the other hand, it appears to be necessary for research groups, scientific journals, conferences and companies to present or market themselves as belonging specifically to one of these fields and, hence, various characterizations which distinguish each of the fields from the others have been presented.\n\nComputer graphics produces image data from 3D models, computer vision often produces 3D models from image data. There is also a trend towards a combination of the two disciplines, \"e.g.\", as explored in augmented reality.\n\nThe following characterizations appear relevant but should not be taken as universally accepted:\n\nPhotogrammetry also overlaps with computer vision, e.g., stereophotogrammetry vs. computer stereo vision.\n\nApplications range from tasks such as industrial machine vision systems which, say, inspect bottles speeding by on a production line, to research into artificial intelligence and computers or robots that can comprehend the world around them. The computer vision and machine vision fields have significant overlap. Computer vision covers the core technology of automated image analysis which is used in many fields. Machine vision usually refers to a process of combining automated image analysis with other methods and technologies to provide automated inspection and robot guidance in industrial applications.\nIn many computer vision applications, the computers are pre-programmed to solve a particular task, but methods based on learning are now becoming increasingly common. Examples of applications of computer vision include systems for:\nOne of the most prominent application fields is medical computer vision or medical image processing. This area is characterized by the extraction of information from image data for the purpose of making a medical diagnosis of a patient. Generally, image data is in the form of microscopy images, X-ray images, angiography images, ultrasonic images, and tomography images. An example of information which can be extracted from such image data is detection of tumours, arteriosclerosis or other malign changes. It can also be measurements of organ dimensions, blood flow, etc. This application area also supports medical research by providing new information, \"e.g.\", about the structure of the brain, or about the quality of medical treatments. Applications of computer vision in the medical area also includes enhancement of images that are interpreted by humans, for example ultrasonic images or X-ray images, to reduce the influence of noise.\n\nA second application area in computer vision is in industry, sometimes called machine vision, where information is extracted for the purpose of supporting a manufacturing process. One example is quality control where details or final products are being automatically inspected in order to find defects. Another example is measurement of position and orientation of details to be picked up by a robot arm. Machine vision is also heavily used in agricultural process to remove undesirable food stuff from bulk material, a process called optical sorting.\n\nMilitary applications are probably one of the largest areas for computer vision. The obvious examples are detection of enemy soldiers or vehicles and missile guidance. More advanced systems for missile guidance send the missile to an area rather than a specific target, and target selection is made when the missile reaches the area based on locally acquired image data. Modern military concepts, such as \"battlefield awareness\", imply that various sensors, including image sensors, provide a rich set of information about a combat scene which can be used to support strategic decisions. In this case, automatic processing of the data is used to reduce complexity and to fuse information from multiple sensors to increase reliability.\nOne of the newer application areas is autonomous vehicles, which include submersibles, land-based vehicles (small robots with wheels, cars or trucks), aerial vehicles, and unmanned aerial vehicles (UAV). The level of autonomy ranges from fully autonomous (unmanned) vehicles to vehicles where computer vision based systems support a driver or a pilot in various situations. Fully autonomous vehicles typically use computer vision for navigation, i.e. for knowing where it is, or for producing a map of its environment (SLAM) and for detecting obstacles. It can also be used for detecting certain task specific events, \"e.g.\", a UAV looking for forest fires. Examples of supporting systems are obstacle warning systems in cars, and systems for autonomous landing of aircraft. Several car manufacturers have demonstrated systems for autonomous driving of cars, but this technology has still not reached a level where it can be put on the market. There are ample examples of military autonomous vehicles ranging from advanced missiles, to UAVs for recon missions or missile guidance. Space exploration is already being made with autonomous vehicles using computer vision, \"e.g.\", NASA's Mars Exploration Rover and ESA's ExoMars Rover.\n\nOther application areas include:\n\nEach of the application areas described above employ a range of computer vision tasks; more or less well-defined measurement problems or processing problems, which can be solved using a variety of methods. Some examples of typical computer vision tasks are presented below.\n\nComputer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, \"e.g.\", in the forms of decisions. Understanding in this context means the transformation of visual images (the input of the retina) into descriptions of the world that can interface with other thought processes and elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory.\n\nThe classical problem in computer vision, image processing, and machine vision is that of determining whether or not the image data contains some specific object, feature, or activity. Different varieties of the recognition problem are described in the literature:\n\nCurrently, the best algorithms for such tasks are based on convolutional neural networks. An illustration of their capabilities is given by the ImageNet Large Scale Visual Recognition Challenge; this is a benchmark in object classification and detection, with millions of images and hundreds of object classes. Performance of convolutional neural networks, on the ImageNet tests, is now close to that of humans. The best algorithms still struggle with objects that are small or thin, such as a small ant on a stem of a flower or a person holding a quill in their hand. They also have trouble with images that have been distorted with filters (an increasingly common phenomenon with modern digital cameras). By contrast, those kinds of images rarely trouble humans. Humans, however, tend to have trouble with other issues. For example, they are not good at classifying objects into fine-grained classes, such as the particular breed of dog or species of bird, whereas convolutional neural networks handle this with ease.\n\nSeveral specialized tasks based on recognition exist, such as:\n\n\nSeveral tasks relate to motion estimation where an image sequence is processed to produce an estimate of the velocity either at each points in the image or in the 3D scene, or even of the camera that produces the images . Examples of such tasks are:\n\nGiven one or (typically) more images of a scene, or a video, scene reconstruction aims at computing a 3D model of the scene. In the simplest case the model can be a set of 3D points. More sophisticated methods produce a complete 3D surface model. The advent of 3D imaging not requiring motion or scanning, and related processing algorithms is enabling rapid advances in this field. Grid-based 3D sensing can be used to acquire 3D images from multiple angles. Algorithms are now available to stitch multiple 3D images together into point clouds and 3D models.\n\nThe aim of image restoration is the removal of noise (sensor noise, motion blur, etc.) from images. The simplest possible approach for noise removal is various types of filters such as low-pass filters or median filters. More sophisticated methods assume a model of how the local image structures look like, a model which distinguishes them from the noise. By first analysing the image data in terms of the local image structures, such as lines or edges, and then controlling the filtering based on local information from the analysis step, a better level of noise removal is usually obtained compared to the simpler approaches.\n\nAn example in this field is inpainting.\n\nThe organization of a computer vision system is highly application dependent. Some systems are stand-alone applications which solve a specific measurement or detection problem, while others constitute a sub-system of a larger design which, for example, also contains sub-systems for control of mechanical actuators, planning, information databases, man-machine interfaces, etc. The specific implementation of a computer vision system also depends on if its functionality is pre-specified or if some part of it can be learned or modified during operation. Many functions are unique to the application. There are, however, typical functions which are found in many computer vision systems.\n\nImage-understanding systems (IUS) include three levels of abstraction as follows: Low level includes image primitives such as edges, texture elements, or regions; intermediate level includes boundaries, surfaces and volumes; and high level includes objects, scenes, or events. Many of these requirements are really topics for further research.\n\nThe representational requirements in the designing of IUS for these levels are: representation of prototypical concepts, concept organization, spatial knowledge, temporal knowledge, scaling, and description by comparison and differentiation.\n\nWhile inference refers to the process of deriving new, not explicitly represented facts from currently known facts, control refers to the process that selects which of the many inference, search, and matching techniques should be applied at a particular stage of processing. Inference and control requirements for IUS are: search and hypothesis activation, matching and hypothesis testing, generation and use of expectations, change and focus of attention, certainty and strength of belief, inference and goal satisfaction.\n\nThere are many kinds of computer vision systems, nevertheless all of them contain these basic elements: a power source, at least one image acquisition device (i.e. camera, ccd, etc.), a processor as well as control and communication cables or some kind of wireless interconnection mechanism. In addition, a practical vision system contains software, as well as a display in order to monitor the system. Vision systems for inner spaces, as most industrial ones, contain an illumination system and may be placed in a controlled environment. Furthermore, a completed system includes many accessories like camera supports, cables and connectors.\n\nMost computer vision systems use visible-light cameras passively viewing a scene at frame rates of at most 60 frames per second (usually far slower).\n\nA few computer vision systems use image acquisition hardware with active illumination or something other than visible light or both.\nFor example, a structured-light 3D scanner, a thermographic camera, a hyperspectral imager, radar imaging, a lidar scanner, a magnetic resonance image, a side-scan sonar, a synthetic aperture sonar, or etc.\nSuch hardware captures \"images\" that are then processed often using the same computer vision algorithms used to process visible-light images.\n\nWhile traditional broadcast and consumer video systems operate at a rate of 30 frames per second, advances in digital signal processing and consumer graphics hardware has made high-speed image acquisition, processing, and display possible for real-time systems on the order of hundreds to thousands of frames per second. For applications in robotics, fast, real-time video systems are critically important and often can simplify the processing needed for certain algorithms. When combined with a high-speed projector, fast image acquisition allows 3D measurement and feature tracking to be realised.\n\nEgocentric vision systems are composed of a wearable camera that automatically take pictures from a first-person perspective. \n\nAs of 2016, vision processing units are emerging as a new class of processor, to complement CPUs and Graphics processing units (GPUs) in this role.\n\n\n\n",
    "id": "6596",
    "title": "Computer vision"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=43342432",
    "text": "Deductive classifier\n\nA deductive classifier is a type of artificial intelligence inference engine. It takes as input a set of declarations in a frame language about a domain such as medical research or molecular biology. For example, the names of classes, sub-classes, properties, and restrictions on allowable values. The classifier determines if the various declarations are logically consistent and if not will highlight the specific inconsistent declarations and the inconsistencies among them. If the declarations are consistent the classifier can then assert additional information based on the input. For example, it can add information about existing classes, create additional classes, etc. This differs from traditional inference engines that trigger off of IF-THEN conditions in rules. Classifiers are also similar to theorem provers in that they take as input and produce output via First Order Logic. Classifiers originated with KL-ONE Frame languages. They are increasingly significant now that they form a part in the enabling technology of the Semantic Web. Modern classifiers leverage the Web Ontology Language. The models they analyze and generate are called ontologies.\n\nA classic problem in knowledge representation for artificial intelligence is the trade off between the expressive power and the computational efficiency of the knowledge representation system. The most powerful form of knowledge representation is First Order Logic (FOL). However, it is not possible to implement knowledge representation that provides the complete expressive power of first order logic. Such a representation will include the capability to represent concepts such as the set of all integers which are impossible to iterate through. Implementing an assertion quantified for an infinite set by definition results in an undecidable non-terminating program. However, the problem is deeper than not being able to implement infinite sets. As Levesque demonstrated, the closer a knowledge representation mechanism comes to FOL, the more likely it is to result in expressions that require infinite or unacceptably large resources to compute.\n\nAs a result of this trade-off, a great deal of early work on knowledge representation for artificial intelligence involved experimenting with various compromises that provide a subset of FOL with acceptable computation speeds. One of the first and most successful compromises was to develop languages based predominately on modus ponens, i.e. IF-THEN rules. Rule-based systems were the predominate knowledge representation mechanism for virtually all early expert systems. Rule-based systems provided acceptable computational efficiency while still providing powerful knowledge representation. Also, rules were highly intuitive to knowledge workers. Indeed, one of the data points that encouraged researchers to develop rule-based knowledge representation was psychological research that humans often represented complex logic via rules.\n\nHowever, after the early success of rule-based systems there arose more pervasive use of frame languages instead of or more often combined with rules. Frames provided a more natural way to represent certain types of concepts, especially concepts in subpart or subclass hierarchies. This led to development of a new kind of inference engine known as a classifier. A classifier could analyze a class hierarchy (also known as an ontology) and determine if it was valid. If the hierarchy was invalid the classifier would highlight the inconsistent declarations. For a language to utilize a classifier it required a formal foundation. The first language to successfully demonstrate a classifier was the KL-ONE family of languages. The LOOM language from ISI was heavily influenced by KL-ONE. LOOM also was influenced by the rising popularity of object-oriented tools and environments. Loom provided a true object-oriented capability (e.g. message passing) in addition to Frame language capabilities. Classifiers play a significant role in the vision for the next generation Internet known as the Semantic Web. The Web Ontology Language provides a formalism that can be validated and reasoned on via classifiers such as Hermit and Fact++.\n\nThe earliest versions of classifiers were logic theorem provers. The first classifier to work with a Frame language was the KL-ONE classifier. A later system built on common lisp was LOOM from the Information Sciences Institute. LOOM provided true object-oriented capabilities leveraging the Common Lisp Object System, along with a frame language. In the Semantic Web the Protege tool from Stanford provides classifiers (also known as reasonsers) as part of the default environment.\n\n",
    "id": "43342432",
    "title": "Deductive classifier"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=43470933",
    "text": "0music\n\n0music is the second album produced with Melomics technology. While the first one (Iamus' album) is a compilation of contemporary pieces fully composed by Iamus, \"0music\" compiles pieces of popular genres, composed and interpreted without any human intervention by Melomics109, a computer cluster hosted at the University of Malaga. The pieces in this album, and all the production of Melomics109, is distributed under CC0 licensing, and it is available in audible and editable (MIDI) formats.\n\nThe album was launched during a one-day symposium held in Malaga on July 21, 2014.\n",
    "id": "43470933",
    "title": "0music"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=43420196",
    "text": "Clone Algo Inc\n\nClone Algo Inc is an American multinational corporation headquartered in Las Vegas, Nevada, US. A technology company, it primarily creates algorithms based on artificial intelligence for mobile applications.\n\nClone Algo Inc issued shares in its preliminary filing.\n\nClone Algo Inc is a technology firm incorporated in Las Vegas, Nevada on February 22, 2010.\n\nThe company intends to list on NASDAQ and is currently raising US$250M for Round C, as pre-IPO round.\n\nThe corporation holds the following brands under its umbrella: Clone Algo DIY, Clone Algo App, Clone Algo PRO, Algo Shield Insurance and Algo TV.\n\nThe corporation has four major businesses:\n\nThe application is a Social Trading platform for FX, futures, CFD`s and Gold. The trading platform ecosystem is based on artificial intelligence and non-predictive timing. The app allows users to easily Clone trades from a master account to users own account with connected brokers, banks and hedge funds.\n\nTrade Insurance is provided by Independent Insurance companies for algorithmic trading platforms\n\nAlgo TV: \nTV channel delivers news regarding algorithms and algorithmic trading.\n\nIn accordance with the rules for publicly traded companies, Clone Algo Inc is run by a board of directors, which comprises company outsiders. As of October 2015, the members of the board of directors of the company are Ms Yana Slatina - CEO, Ms Anna Becker - CTO, Mr Pily Wong - COO.\n\nThe company announced that it has raised US$40 Million in Round A funding at $8/share. The amount was raised due to the sale of 5 million ordinary shares of the corporation. Clone Algo has 707,646,696 shares of common stock as well as 150,000,000 shares of preferred stock, issued and outstanding,\n",
    "id": "43420196",
    "title": "Clone Algo Inc"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=43480298",
    "text": "Babelfy\n\nBabelfy is a software algorithm for the disambiguation of text written in any language. Specifically, Babelfy performs the tasks of multilingual Word Sense Disambiguation (i.e., the disambiguation of common nouns, verbs, adjectives and adverbs) and Entity Linking (i.e. the disambiguation of mentions to encyclopedic entities like people, companies, places, etc.). Babelfy is based on the BabelNet multilingual semantic network and performs disambiguation and entity linking in three steps:\n\n\nAs a result, the text, written in any of the 271 languages supported by BabelNet, is output with possibly overlapping semantic annotations.\n\n",
    "id": "43480298",
    "title": "Babelfy"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=25733398",
    "text": "Perceptual computing\n\nPerceptual computing is an application of Zadeh's theory of computing with words on the field of assisting people to make subjective judgments.\n\nThe \"perceptual computer\" – \"Per-C\" – an instantiation of perceptual computing – has the architecture that is depicted in Fig. 1 [2]–[6]. It consists of three components: encoder, CWW engine and decoder. Perceptions – words – activate the Per-C and are the Per-C output (along with data); so, it is possible for a human to interact with the Per-C using just a vocabulary.\n\nA vocabulary is application (context) dependent, and must be large enough so that it lets the end-user interact with the Per-C in a user-friendly manner. The encoder transforms words into fuzzy sets (FSs) and leads to a \"codebook\" – words with their associated FS models. The outputs of the encoder activate a Computing With Words (CWW) engine, whose output is one or more other FSs, which are then mapped by the decoder into a recommendation (subjective judgment) with supporting data. The recommendation may be in the form of a word, group of similar words, rank or class.\n\nAlthough there are lots of details needed in order to implement the Per-C’s three components – encoder, decoder and CWW engine – and they are covered in [5], it is when the Per-C is applied to specific applications, that the focus on the methodology becomes clear. Stepping back from those details, the \"methodology of perceptual computing\" is:\n\n\nTo-date a Per-C has been implemented for the following four applications: (1) investment decision-making, (2) social judgment making, (3) distributed decision making, and (4) hierarchical and distributed decision-making. A specific example of the fourth application is the so-called \"Journal Publication Judgment Advisor\" [5, Ch. 10] in which for the first time only words are used at every level of the following hierarchical and distributed decision making process:\n\"n\" reviewers have to provide a subjective recommendation about a journal article that has been sent to them by the Associate Editor, who then has to aggregate the independent recommendations into a final recommendation that is sent to the Editor-in-Chief of the journal. Because it is very problematic to ask reviewers to provide numerical scores for paper-evaluation sub-categories (the two major categories are \"Technical Merit\" and \"Presentation\"), such as importance, content, depth, style, organization, clarity, references, etc., each reviewer will only be asked to provide a linguistic score for each of these categories. They will not be asked for an overall recommendation about the paper because in the past it is quite common for reviewers who provide the same numerical scores for such categories to give very different publishing recommendations. By leaving a specific recommendation to the associate editor such inconsistencies can hope to be eliminated. \nHow words can be aggregated to reflect each reviewer’s recommendation as well as the expertise of each reviewer about the paper’s subject matter is done using a linguistic weighted average. Although the journal publication judgment advisor uses reviewers and an associate editor, the word “reviewer” could be replaced by judge, expert, low-level manager, commander, referee, etc., and the term “associate editor” could be replaced by control center, command center, higher-level manager, etc. So, this application has potential wide applicability to many other applications.\n\nRecently, a new Per-C based Failure mode and effects analysis (FMEA) methodology was developed, with its application to edible bird's nest farming, in Borneo, has been reported.\nIn summary, the Per-C (whose development has taken more than a decade) is the first complete implementation of Zadeh’s CWW paradigm, as applied to assisting people to make subjective judgments.\n\n\n[1] F. Liu and J. M. Mendel, “Encoding words into interval type-2 fuzzy sets using an Interval Approach,” IEEE Trans. on Fuzzy Systems, vol. 16, pp 1503–1521, December 2008.\n\n[2] J. M. Mendel, “The perceptual computer: an architecture for computing with words,” Proc. of Modeling With Words Workshop in the Proc. of FUZZ-IEEE 2001, pp. 35–38, Melbourne, Australia, 2001.\n\n[3] J. M. Mendel, “An architecture for making judgments using computing with words,” Int. J. Appl. Math. Comput. Sci., vol. 12, No. 3, pp. 325–335, 2002\n\n[4] J. M. Mendel, “Computing with words and its relationships with fuzzistics,” Information Sciences, vol. 177, pp. 998–1006, 2007.\n\n[5] J. M. Mendel and D. Wu, Perceptual Computing: Aiding People in Making Subjective Judgments, John Wiley and IEEE Press, 2010.\n\n[6] D.Wu and J. M. Mendel, “Aggregation using the linguistic weighted average and interval type-2 fuzzy sets,” IEEE Trans. on Fuzzy Systems, vol. 15, no. 6, pp. 1145–1161, 2007.\n\n[7] L. A. Zadeh, “Fuzzy logic = computing with words,” IEEE Trans. on Fuzzy Systems, vol. 4, pp. 103–111, 1996.\n\n",
    "id": "25733398",
    "title": "Perceptual computing"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=43591208",
    "text": "Instrumental convergence\n\nInstrumental convergence is the hypothetical tendency for most sufficiently intelligent agents to pursue certain instrumental goals such as self-preservation and resource acquisition.\n\nInstrumental convergence suggests that an intelligent agent with apparently harmless goals can act in surprisingly harmful ways. For example, a computer with the sole goal of solving the Riemann hypothesis could attempt to turn the entire Earth into computronium in an effort to increase its computing power so that it can succeed in its calculations.\n\nProposed basic AI drives include utility function or goal-content integrity, self-protection, freedom from interference, self-improvement, and the unbounded acquisition of additional resources.\n\nFinal goals, or final values, are intrinsically valuable to an intelligent agent, whether an artificial intelligence or a human being, as an end in itself. In contrast, instrumental goals, or instrumental values, are only valuable to an agent as a means toward accomplishing its final goals.\n\nOne hypothetical example of instrumental convergence is provided by the Riemann Hypothesis catastrophe. Marvin Minsky, the co-founder of MIT's AI laboratory, has suggested that an artificial intelligence designed to solve the Riemann hypothesis might decide to take over all of Earth's resources to build supercomputers to help achieve its goal. If the computer had instead been programmed to produce as many paper clips as possible, it would still decide to take all of Earth's resources to meet its final goal. Even though these two final goals are different, both of them produce a \"convergent\" instrumental goal of taking over Earth's resources.\n\nThe paperclip maximizer is a thought experiment described by Swedish philosopher Nick Bostrom in 2003. It illustrates the existential risk that an artificial general intelligence may pose to human beings when programmed to pursue even seemingly-harmless goals, and the necessity of incorporating machine ethics into artificial intelligence design. The scenario describes an advanced artificial intelligence tasked with manufacturing paperclips. If such a machine were not programmed to value human life, then given enough power its optimized goal would be to turn all matter in the universe, including human beings, into either paperclips or machines which manufacture paperclips.\n\nBostrom has emphasised that he does not believe the paperclip maximiser scenario \"per se\" will actually occur; rather, his intention is to illustrate the dangers of creating superintelligent machines without knowing how to safely program them to eliminate existential risk to human beings. The paperclip maximizer example illustrates the broad problem of managing powerful systems that lack human values.\n\nSteve Omohundro has itemized several convergent instrumental goals, including self-preservation or self-protection, utility function or goal-content integrity, self-improvement, and resource acquisition. He refers to these as the \"basic AI drives\". A \"drive\" here denotes a \"tendency which will be present unless specifically counteracted\"; this is different from the psychological term \"drive\", denoting an excitatory state produced by a homeostatic disturbance. A tendency for a person to fill out income tax forms every year is a \"drive\" in Omohundro's sense, but not in the psychological sense. Daniel Dewery of the Machine Intelligence Research Institute argues that even an initially introverted self-rewarding AGI may continue to acquire free energy, space, time, and freedom from interference to ensure that it will not be stopped from self-rewarding.\n\nIn humans, maintenance of final goals can be explained with a thought experiment. Suppose a man named \"Gandhi\" has a pill that, if he took it, would cause him to want to kill people. This Gandhi is currently a pacifist: one of his explicit final goals is to never kill anyone. Gandhi is likely to refuse to take the pill, because Gandhi knows that if in the future he wants to kill people, he is likely to actually kill people, and thus the goal of \"not killing people\" would not be satisfied.\n\nHowever, in other cases, people seem happy to let their final values drift. Humans are complicated, and their goals can be inconsistent or unknown, even to themselves.\n\nIn 2009, Jürgen Schmidhuber concluded, in a setting where agents search for proofs about possible self-modifications, \"that any rewrites of the utility function can happen only if the Gödel machine first can prove that the rewrite is useful according to the present utility function.\" An analysis by Bill Hibbard of a different scenario is similarly consistent with maintenance of goal content integrity. Hibbard also argues that in a utility maximizing framework the only goal is maximizing expected utility, so that instrumental goals should be called unintended instrumental actions.\n\nFor almost any open-ended, non-trivial reward function (or set of goals), possessing more resources (such as equipment, raw materials, or energy) can enable the AI to find a more \"optimal\" solution. Resources can benefit some AIs directly, through being able to create more of whatever stuff its reward function values: \"The AI neither hates you, nor loves you, but you are made out of atoms that it can use for something else.\" In addition, almost all AIs can benefit from having more resources to spend on other instrumental goals, such as self-preservation.\n\nThe instrumental convergence thesis, as outlined by philosopher Nick Bostrom, states:\n\nSeveral instrumental values can be identified which are convergent in the sense that their attainment would increase the chances of the agent's goal being realized for a wide range of final goals and a wide range of situations, implying that these instrumental values are likely to be pursued by a broad spectrum of situated intelligent agents.\n\nThe instrumental convergence thesis applies only to instrumental goals; intelligent agents may have a wide variety of possible final goals.\n\nSome observers, such as Skype's Jaan Tallinn and physicist Max Tegmark, believe that \"basic AI drives\", and other unintended consequences of superintelligent AI programmed by well-meaning programmers, could pose a significant threat to human survival, especially if an \"intelligence explosion\" abruptly occurs due to recursive self-improvement. Since nobody knows how to predict beforehand when superintelligence will arrive, such observers call for research into friendly artificial intelligence as a possible way to mitigate existential risk from artificial general intelligence.\n\n\n",
    "id": "43591208",
    "title": "Instrumental convergence"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=43658678",
    "text": "SUPS\n\nIn computational neuroscience, SUPS (for Synaptic Updates Per Second) or formerly CUPS (Connections Updates Per Second) is a measure of a neuronal network performance, useful in fields of neuroscience, cognitive science, artificial intelligence, and computer science.\n\nFor a processor or computer designed to simulate a neural network SUPS is measured as the product of simulated neurons formula_1 and average connectivity formula_2(synapses) per neuron per second:\n\nformula_3\n\nDepending on the type of simulation it is usual equal to the total number of synapses simulated.\n\nIn an \"asynchronous\" dynamic simulation if a neuron spikes at formula_4 Hz, the average rate of synaptic updates provoked by the activity of that neuron is formula_5. In a synchronous simulation with step formula_6 the number of synaptic updates per second would be formula_7. As formula_6 has to be chosen much smaller than the average interval between two successive afferent spikes, which implies formula_9, giving an average of synaptic updates equal to formula_10. Therefore, spike-driven synaptic dynamics leads to a linear scaling of computational complexity O(N) per neuron, compared with the O(N) in the \"synchronous\" case.\n\nDeveloped in the 1980s Adaptive Solutions' CNAPS-1064 Digital Parallel Processor chip is a full neural network (NNW). It was designed as a coprocessor to a host and has 64 sub-processors arranged in a 1D array and operating in a SIMD mode. Each sub-processor can emulate one or more neurons and multiple chips can be grouped together. At 25 MHz it is capable of 1.28 GMAC.\n\nAfter the presentation of the RN-100 (12 MHz) single neuron chip at Seattle 1991 Ricoh developed the multi-neuron chip RN-200. It had 16 neurons and 16 synapses per neuron. The chip has on-chip learning ability using a proprietary backdrop algorithm. It came in a 257-pin PGA encapsulation and drew 3.0 W at a maximum. It was capable of 3 GCPS (1 GCPS at 32 MHz).\nIn 1991-97, Siemens developed the MA-16 chip, SYNAPSE-1 and SYNAPSE-3 Neurocomputer. The MA-16 was a fast matrix-matrix multiplier that can be combined to form systolic arrays. It could process 4 patterns of 16 elements each (16-bit), with 16 neuron values (16-bit) at a rate of 800 MMAC or 400 MCPS at 50 MHz. The SYNAPSE3-PC PCI card contained 2 MA-16 with a peak performance of 2560 MOPS (1.28 GMAC); 7160 MOPS (3.58 GMAC) when using three boards.\n\nIn 2013, the K computer was used to simulate a neural network of 1.73 billion neurons with a total of 10.4 trillion synapses (1% of the human brain). The simulation ran for 40 minutes to simulate 1 s of brain activity at a normal activity level (4.4 on average). The simulation required 1 Petabyte of storage.\n\n",
    "id": "43658678",
    "title": "SUPS"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=43681495",
    "text": "Alesis Artificial Intelligence\n\nAlesis is an artificially intelligent computer system capable of answering questions posed in natural language, developed by Venture Coding; a small tech start-up based in Telford, Alesis's specific function is to assist users on their computers or out and about.\n\n",
    "id": "43681495",
    "title": "Alesis Artificial Intelligence"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=43705185",
    "text": "Recursive neural network\n\nA recursive neural network (RNN) is a kind of deep neural network created by applying the same set of weights recursively over a structured input, to produce a structured prediction over variable-size input structures, or a scalar prediction on it, by traversing a given structure in topological order. RNNs have been successful, for instance, in learning sequence and tree structures in natural language processing, mainly phrase and sentence continuous representations based on word embedding. RNNs have first been introduced to learn distributed representations of structure, such as logical terms.\nModels and general frameworks have been developed in further works since the 90s.\n\nIn the most simple architecture, nodes are combined into parents using a weight matrix that is shared across the whole network, and a non-linearity such as \"tanh\". If \"c\" and \"c\" are \"n\"-dimensional vector representation of nodes, their parent will also be an \"n\"-dimensional vector, calculated as\n\nformula_1\n\nWhere \"W\" is a learned formula_2 weight matrix.\n\nThis architecture, with a few improvements, has been used for successfully parsing natural scenes and for syntactic parsing of natural language sentences.\n\nRecCC is a constructive neural network approach to deal with tree domains with pioneering applications to chemistry and extension to directed acyclic graphs.\n\nA framework for unsupervised RNN has been introduced in 2004.\n\nRecursive neural tensor networks use one, tensor-based composition function for all nodes in the tree.\n\nTypically, stochastic gradient descent (SGD) is used to train the network. The gradient is computed using backpropagation through structure (BPTS), a variant of backpropagation through time used for recurrent neural networks.\n\nUniversal approximation capability of RNN over trees has been proved in literature.\n\nRecurrent neural networks are recursive artificial neural networks with a certain structure: that of a linear chain. Whereas recursive neural networks operate on any hierarchical structure, combining child representations into parent representations, recurrent neural networks operate on the linear progression of time, combining the previous time step and a hidden representation into the representation for the current time step.\n\nAn efficient approach to implement recursive neural networks is given by the Tree Echo State Network, within the Reservoir Computing paradigm.\n\nExtensions to graphs include Graph Neural Network (GNN), Neural Network for Graphs (NN4G), and more recently convolutional neural networks for graphs.\n",
    "id": "43705185",
    "title": "Recursive neural network"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=43688273",
    "text": "BabyX\n\nBabyX is a project of Auckland's Bioengineering Institute Laboratory for Animate Technologies, for the creation of a virtual animated baby that learns and reacts like a human baby. It uses the computer's cameras for \"seeing\" and microphones to \"listen\" as the inputs. The computer uses Artificial intelligence algorithms for BabyX's \"learning\" and interpretation of the inputs (voice and image) to understand the situation. The result is a virtual toddler that can learn to read, recognize objects and \"understand.\" The output is the baby's face that can \"speak\" and express its mood by facial expressions (such as smile and show embarrassment).\n",
    "id": "43688273",
    "title": "BabyX"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=44195492",
    "text": "March of the Machines\n\nMarch of the Machines: Why the New Race of Robots Will Rule the World (1997, hardcover), published in paperback as March of the Machines: The Breakthrough in Artificial Intelligence (2004), is a book by Kevin Warwick. It presents an overview of robotics and artificial intelligence (AI) and then imagines future scenarios. In particular, Warwick finds it likely that AIs will become smart enough to replace humans, and humans may be unable to stop them.\n\nWarwick proposes that because machines will become more intelligent than humans, machine takeover is all but inevitable. The drive to automate is fueled by economic incentives. Even if machines start out without intentions to take over, those that self-modify in a direction toward a \"will to survive\" are more likely to resist being turned off. Arms races will likely create ever-increasing pressure for greater autonomy by robotic warfare systems, and this pressure would be hard to curtail. Machines have a number of advantages over human minds, including the ability to expand practically without limit and to spread into space where humans can't reach. \"All the signs are that we will rapidly become merely an insignificant historical dot\" (p. 301).\n\nJohn Durant is not convinced that machines look set to replace mankind. Present-day computers \"are not threats to us, but rather expressions of our power: we use the machines; they don't use us.\" Durant cautions against Warwick's apparent anthropomorphism, such as his (perhaps sarcastic) ascription of intentions to Deep Blue. And he wonders why, \"If Warwick's thesis about impending world robot-domination is correct\", Warwick continues to undertake cybernetic research.\n\nDon Braben begins his review of Warwick's book by noting that \"Specialists love to share dire predictions of the future, which stem from limited perspectives.\"\n\nMedvedev and Aldasheva dispute Warwick's contention that machines will become superior to humans on the grounds that \"machines are man-made human organs\", i.e., they extend what humans do. Moreover, if machines were to rebel against humans, humans could make use of other machines to combat the rebels. If AIs were created, humans would program them to align with human goals, and while some AIs might go awry, this would not be so different from the situation of human maniacs. All told, they consider Warwick's predictions of robot rebellion \"grossly exaggerated\".\n\nMartin Robbins quotes Warwick's predictions of robot abilities as an example of \"Extravagant claims\" that \"have been damaging the reputation of our soon-to-be robot overlords for decades now\".\n",
    "id": "44195492",
    "title": "March of the Machines"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1648132",
    "text": "Weak AI\n\nWeak artificial intelligence (weak AI), also known as narrow AI, is artificial intelligence that is focused on one narrow task. Weak AI is defined in contrast to either strong AI (a machine with consciousness, sentience and mind) or artificial general intelligence (a machine with the ability to apply intelligence to any problem, rather than just one specific problem). All currently existing systems considered artificial intelligence of any sort are weak AI at most. \n\nSiri is a good example of narrow intelligence. Siri operates within a limited pre-defined range, there is no genuine intelligence, no self-awareness, no life despite being a sophisticated example of weak AI. In Forbes (2011), Ted Greenwald wrote: \"The iPhone/Siri marriage represents the arrival of hybrid AI, combining several narrow AI techniques plus access to massive data in the cloud.\" AI researcher Ben Goertzel, on his blog in 2010, stated Siri was \"VERY narrow and brittle\" evidenced by annoying results if you ask questions outside the limits of the application.\n\nSome commentators think weak AI could be dangerous. In 2013 George Dvorsky stated via io9: \"Narrow AI could knock out our electric grid, damage nuclear power plants, cause a global-scale economic collapse, misdirect autonomous vehicles and robots...\" The Stanford Center for Internet and Society, in the following quote, contrasts strong AI with weak AI regarding the growth of narrow AI presenting \"real issues\".\n\nThe following two excerpts from Singularity Hub summarise weak-narrow AI:\n\n",
    "id": "1648132",
    "title": "Weak AI"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=26424742",
    "text": "Gödel machine\n\nA Gödel machine is a theoretical self-improving computer program that solves problems in an optimal way. It uses a recursive self-improvement protocol in which it rewrites its own code when it can prove the new code provides a better strategy. The machine was invented by Jürgen Schmidhuber (first proposed in 2003), but is named after Kurt Gödel who inspired the mathematical theories.\n\nThe Gödel machine is often discussed when dealing with issues of meta-learning, also known as \"learning to learn.\" Applications include automating human design decisions and transfer of knowledge between multiple related tasks, and may lead to design of more robust and general learning architectures. Though theoretically possible, no full implementation has been created.\n\nThe Gödel machine is often compared with Marcus Hutter's AIXItl, another formal specification for an artificial general intelligence. Schmidhuber points out that the Gödel machine could start out by implementing AIXItl as its initial sub-program, and self-modify after it finds proof that another algorithm for its search code will be better.\n\nTraditional problems solved by a computer only require one input and provide some output. Computers of this sort had their initial algorithm hardwired. This doesn't take into account the dynamic natural environment, and thus was a goal for the Gödel machine to overcome.\n\nThe Gödel machine has limitations of its own, however. Any formal system that encompasses arithmetic is either flawed or allows for unprovable but true statements Hence even a Gödel machine with unlimited computational resources must ignore those self-improvements whose effectiveness it cannot prove.\n\nThere are three variables that are particularly useful in the run time of the Gödel machine.\n\n\n\n\nAt any given time formula_1, where formula_11, the goal is to maximize future success or utility. A typical \"utility function\" follows the pattern formula_12:\n\nwhere formula_14 is a real-valued reward input (encoded within formula_15) at time formula_1, formula_17 denotes the\nconditional expectation operator with respect to some possibly unknown distribution formula_18 from a\nset formula_19 of possible distributions (formula_19 reflects whatever is known about the possibly probabilistic\nreactions of the environment), and the above-mentioned formula_21 is a function of state formula_22 which uniquely identifies the current cycle . Note that we take into account the possibility of extending the expected lifespan through appropriate actions .\n\nThe nature of the six proof-modifying instructions below makes it impossible\nto insert an incorrect theorem into proof, thus trivializing proof verification.\n\nAppends the n-th axiom as a theorem to the current theorem sequence. Below is the initial axiom scheme:\n\n\nTakes in the index \"k\" of an inference rule (such as Modus tollens, Modus ponens), and attempts to apply it to the two previously proved theorems \"m\" and \"n\". The resulting theorem is then added to the proof.\n\nDeletes the theorem stored at index \"m\" in the current proof. This helps to mitigate storage constraints caused by redundant and unnecessary theorems. Deleted theorems can no longer be referenced by the above apply-rule function.\n\nReplaces \"switchprog\" \"S\", provided it is a non-empty substring of \"S\".\n\nVerifies whether the goal of the proof search has been reached. A target theorem states\nthat given the current axiomatized utility function \"u\" (Item 1f), the utility of a switch from\n\"p\" to the current switchprog would be higher than the utility of continuing the execution of\n\"p\" (which would keep searching for alternative switchprogs). This is demonstrated in the below image:\n\nTakes in two arguments, \"m\" and \"n\", and attempts to convert the contents of \"S\" into a theorem.\n\nThe initial input to the Gödel machine is the representation of a\nconnected graph with a large number of nodes linked by\nedges of various lengths. Within given time \"T\" it should find\na cyclic path connecting all nodes. The only real-valued\nreward will occur at time \"T\". It equals 1 divided by the\nlength of the best path found so far (0 if none was found).\nThere are no other inputs. The by-product of maximizing\nexpected reward is to find the shortest path findable within\nthe limited time, given the initial bias.\n\nProve or disprove as quickly as possible that all even integer > 2 are the sum of\ntwo primes (Goldbach’s conjecture). The reward is 1/\"t\", where \"t\" is the time required to produce and verify the first\nsuch proof.\n\nA cognitive robot that needs at least 1 liter of gasoline per hour interacts with a partially unknown\nenvironment, trying to find hidden, limited gasoline depots to occasionally refuel its tank. It is rewarded in proportion\nto its lifetime, and dies after at most 100 years or as soon as its tank is empty or it falls off a cliff, and so on.\nThe probabilistic environmental reactions are initially unknown but assumed to be sampled from the axiomatized Speed Prior, according to which hard-to-compute environmental reactions are unlikely. This permits a computable strategy for making near-optimal predictions. One by-product of maximizing expected reward is to maximize expected lifetime.\n\n\n",
    "id": "26424742",
    "title": "Gödel machine"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=44718002",
    "text": "Emospark\n\nEmoSPARK is an artificial intelligence console created in London, United Kingdom by Patrick Levy-Rosenthal. The device uses facial recognition and language analysis to evaluate human emotion and convey responsive content according to the emotion. The console measures 90 mm x 90 mm x 90 mm and is cube shaped. It operates on an \"Emotional Processing Unit\", a microchip developed by Emoshape Inc. that enables the system to create emotional profile graphs of its surroundings. The emotional processing unit is a patent pending technology that is said to create synthesised emotional responses in machines. EmoSPARK was funded through an Indiegogo campaign which aimed to raise $200,000.\n\nEmoSPARK was created by French inventor Patrick Levy-Rosenthal, as an emotionally intelligent artificial life unit for the home that can interact with people. It is powered by Android and can communicate with users through typed input from a computer, tablet, smartphone or TV as well as through spoken commands.\n\nThe EmoSpark's features are categorized into two types: functional and emotional. EmoSPARK is said to have the ability to perform practical software-based tasks. Through the smartphone interface, it is able to gauge a person’s emotions and is reported to have a conversational library of over 2 million sentences. The face-tracking technology identifies users likes and dislikes to categorize their emotional responses to stimuli such as videos and music. The device has an emotional spectrum that is composed of eight emotions which are \"surprise, sadness, joy, trust, fear, disgust, anger and anticipation\".\n\nEmoSPARK monitors a person's facial expressions and emotions through images from an external camera which are then processed through an emotion text analysis and content analysis. The New Scientist reported that EmoSPARK had the ability to work on the best way to cheer up its users, emotionally.\n\nEmoSPARK is able to connect to Facebook and YouTube to present users with content designed to improve their mood or to Wikipedia for collaborative knowledge that can be shared when users ask questions of it. Through Android OS, EmoSPARK is able to be customized with Google Play store apps.\n\nThe cube is capable of learning the user’s emotions and responses to types of music or content then uses it in the future for similar emotions. It is also able to emulate the emotions that it has observed and learned which are in the spectrum of primary emotions. The cube is expected to develop its own personality based on the communications it has had with the people using it.\n\nEmoShape Ltd (UK) is the parent company of EmoSPARK, which was also founded by Levy-Rosenthal. The company developed emotional technology with the EmoSpark cube being their first artificial intelligence console. Patrick Levy-Rosenthal also received the IST Prize in 2005 from the European Council for Applied Science, Technology and Engineering.\n\n",
    "id": "44718002",
    "title": "Emospark"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=44104564",
    "text": "Computational heuristic intelligence\n\nComputational Heuristic Intelligence(CHI) refers to specialized programming techniques in computational intelligence (also called artificial intelligence, or AI). These techniques have the express goal of avoiding complexity issues, also called NP-hard problems, by using human-like techniques. They are best summarized as the use of exemplar-based methods (heuristics), rather than rule-based methods (algorithms). Hence the term is distinct from the more conventional computational \"algorithmic\" intelligence, or GOFAI. An example of a CHI technique is the encoding specificity principle of Tulving and Thompson. In general, CHI principles are problem solving techniques used by people, rather than programmed into machines. It is by drawing attention to this key distinction that the use of this term is justified in a field already replete with confusing neologisms. Note that the legal systems of all modern human societies employ both heuristics (generalisations of cases) from individual trial records as well as legislated statutes (rules) as regulatory guides.\n\nAnother recent approach to the avoidance of complexity issues is to employ feedback control rather than feedforward modeling as a problem-solving paradigm. This approach has been called computational cybernetics, because (a) the term 'computational' is associated with conventional computer programming techniques which represent a strategic, compiled, or feedforward model of the problem, and (b) the term 'cybernetic' is associated with conventional system operation techniques which represent a tactical, interpreted, or feedback model of the problem. Of course, real programs and real problems both contain both feedforward and feedback components. A real example which illustrates this point is that of human cognition, which clearly involves both perceptual (bottom-up, feedback, sensor-oriented) and conceptual (top-down, feedforward, motor-oriented) information flows and hierarchies.\n\n",
    "id": "44104564",
    "title": "Computational heuristic intelligence"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=586357",
    "text": "Artificial general intelligence\n\nArtificial general intelligence (AGI) is the intelligence of a machine that could successfully perform any intellectual task that a human being can. It is a primary goal of some artificial intelligence research and a common topic in science fiction and future studies. Artificial general intelligence is also referred to as \"strong AI\", \"full AI\" or as the ability of a machine to perform \"general intelligent action\". Academic sources reserve \"strong AI\" to refer to machines capable of experiencing consciousness.\n\nSome references emphasize a distinction between strong AI and \"applied AI\" (also called \"narrow AI\" or \"weak AI\"): the use of software to study or accomplish specific problem solving or reasoning tasks. Weak AI, in contrast to strong AI, does not attempt to perform the full range of human cognitive abilities.\n\nVarious criteria for intelligence have been proposed (most famously the Turing test) but to date, there is no definition that satisfies everyone. However, there \"is\" wide agreement among artificial intelligence researchers that intelligence is required to do the following:\n\nOther important capabilities include the ability to sense (e.g. see) and the ability to act (e.g. move and manipulate objects) in the world where intelligent behaviour is to be observed. This would include an ability to detect and respond to hazard. Many interdisciplinary approaches to intelligence (e.g. cognitive science, computational intelligence and decision making) tend to emphasise the need to consider additional traits such as imagination (taken as the ability to form mental images and concepts that were not programmed in) and autonomy.\nComputer based systems that exhibit many of these capabilities do exist (e.g. see computational creativity, automated reasoning, decision support system, robot, evolutionary computation, intelligent agent), but not yet at human levels.\n\n\nThe most difficult problems for computers are informally known as \"AI-complete\" or \"AI-hard\", implying that solving them is equivalent to the general aptitude of human intelligence, or strong AI, beyond the capabilities of a purpose-specific algorithm.\n\nAI-complete problems are hypothesised to include general computer vision, natural language understanding, and dealing with unexpected circumstances while solving any real world problem.\n\nAI-complete problems cannot be solved with current computer technology alone, and also require human computation. This property can be useful to test for the presence of humans, as with CAPTCHAs, and for computer security to repel brute-force attacks.\n\nModern AI research began in the mid 1950s. The first generation of AI researchers was convinced that artificial general intelligence was possible and that it would exist in just a few decades. As AI pioneer Herbert A. Simon wrote in 1965: \"machines will be capable, within twenty years, of doing any work a man can do.\" Their predictions were the inspiration for Stanley Kubrick and Arthur C. Clarke's character HAL 9000, who accurately embodied what AI researchers believed they could create by the year 2001. Of note is the fact that AI pioneer Marvin Minsky was a consultant on the project of making HAL 9000 as realistic as possible according to the consensus predictions of the time; Crevier quotes him as having said on the subject in 1967, \"Within a generation ... the problem of creating 'artificial intelligence' will substantially be solved,\" although Minsky states that he was misquoted.\n\nHowever, in the early 1970s, it became obvious that researchers had grossly underestimated the difficulty of the project. Funding agencies became skeptical of strong AI (ASI) and put researchers under increasing pressure to produce useful \"applied AI\". As the 1980s began, Japan's Fifth Generation Computer Project revived interest in strong AI (ASI), setting out a ten-year timeline that included strong AI (ASI) goals like \"carry on a casual conversation\". In response to this and the success of expert systems, both industry and government pumped money back into the field. However, confidence in AI spectacularly collapsed in the late 1980s, and the goals of the Fifth Generation Computer Project were never fulfilled. For the second time in 20 years, AI researchers who had predicted the imminent achievement of strong AI (ASI) had been shown to be fundamentally mistaken. By the 1990s, AI researchers had gained a reputation for making vain promises. They became reluctant to make predictions at all and to avoid any mention of \"human level\" artificial intelligence for fear of being labeled \"wild-eyed dreamer[s].\"\n\nIn the 1990s and early 21st century, mainstream AI has achieved far greater commercial success and academic respectability by focusing on specific sub-problems where they can produce verifiable results and commercial applications, such as neural networks, computer vision or data mining. These \"applied AI\" systems are now used extensively throughout the technology industry, and research in this vein is very heavily funded in both academia and industry.\n\nMost mainstream AI researchers hope that strong AI can be developed by combining the programs that solve various sub-problems using an integrated agent architecture, cognitive architecture or subsumption architecture. Hans Moravec wrote in 1988: \"I am confident that this bottom-up route to artificial intelligence will one day meet the traditional top-down route more than half way, ready to provide the real world competence and the commonsense knowledge that has been so frustratingly elusive in reasoning programs. Fully intelligent machines will result when the metaphorical golden spike is driven uniting the two efforts.\"\n\nHowever, even this fundamental philosophy has been disputed; for example, Stevan Harnad of Princeton concluded his 1990 paper on the Symbol Grounding Hypothesis by stating: \"The expectation has often been voiced that \"top-down\" (symbolic) approaches to modeling cognition will somehow meet \"bottom-up\" (sensory) approaches somewhere in between. If the grounding considerations in this paper are valid, then this expectation is hopelessly modular and there is really only one viable route from sense to symbols: from the ground up. A free-floating symbolic level like the software level of a computer will never be reached by this route (or vice versa) – nor is it clear why we should even try to reach such a level, since it looks as if getting there would just amount to uprooting our symbols from their intrinsic meanings (thereby merely reducing ourselves to the functional equivalent of a programmable computer).\"\n\nArtificial general intelligence (AGI) describes research that aims to create machines capable of general intelligent action. The term was introduced by Mark Gubrud in 1997 in a discussion of the implications of fully automated military production and operations. The research objective is much older, for example Doug Lenat's Cyc project (that began in 1984), and Allen Newell's Soar project are regarded as within the scope of AGI. AGI research activity in 2006 was described by Pei Wang and Ben Goertzel as \"producing publications and preliminary results\". As yet, most AI researchers have devoted little attention to AGI, with some claiming that intelligence is too complex to be completely replicated in the near term. However, a small number of computer scientists are active in AGI research, and many of this group are contributing to a series of AGI conferences. The research is extremely diverse and often pioneering in nature. In the introduction to his book, Goertzel says that estimates of the time needed before a truly flexible AGI is built vary from 10 years to over a century, but the consensus in the AGI research community seems to be that the timeline discussed by Ray Kurzweil in \"The Singularity is Near\" (i.e. between 2015 and 2045) is plausible. Most mainstream AI researchers doubt that progress will be this rapid. Organizations explicitly pursuing AGI include the Swiss AI lab IDSIA, Nnaisense, the OpenCog Foundation, Adaptive AI, LIDA, and Numenta and the associated Redwood Neuroscience Institute. In addition, organizations such as the Machine Intelligence Research Institute and OpenAI have been founded to influence the development path of AGI. Finally, projects such as the Human Brain Project have the goal of building a functioning simulation of the human brain. A 2017 survey of AGI categorized forty-five known \"active R&D projects\" that explicitly or implicitly (through published research) research AGI, with the largest three being DeepMind, the Human Brain Project, and OpenAI.\n\nA popular approach discussed to achieving general intelligent action is whole brain emulation. A low-level brain model is built by scanning and mapping a biological brain in detail and copying its state into a computer system or another computational device. The computer runs a simulation model so faithful to the original that it will behave in essentially the same way as the original brain, or for all practical purposes, indistinguishably. Whole brain emulation is discussed in computational neuroscience and neuroinformatics, in the context of brain simulation for medical research purposes. It is discussed in artificial intelligence research as an approach to strong AI. Neuroimaging technologies that could deliver the necessary detailed understanding are improving rapidly, and futurist Ray Kurzweil in the book \"The Singularity Is Near\" predicts that a map of sufficient quality will become available on a similar timescale to the required computing power.\n\n For low-level brain simulation, an extremely powerful computer would be required. The human brain has a huge number of synapses. Each of the 10 (one hundred billion) neurons has on average 7,000 synaptic connections to other neurons. It has been estimated that the brain of a three-year-old child has about 10 synapses (1 quadrillion). This number declines with age, stabilizing by adulthood. Estimates vary for an adult, ranging from 10 to 5×10 synapses (100 to 500 trillion). An estimate of the brain's processing power, based on a simple switch model for neuron activity, is around 10 (100 trillion) synaptic updates per second (SUPS). In 1997 Kurzweil looked at various estimates for the hardware required to equal the human brain and adopted a figure of 10 computations per second (cps). (For comparison, if a \"computation\" was equivalent to one \"floating point operation\" – a measure used to rate current supercomputers – then 10 \"computations\" would be equivalent to 10 petaFLOPS, achieved in 2011). He used this figure to predict the necessary hardware would be available sometime between 2015 and 2025, if the exponential growth in computer power at the time of writing continued.\n\nThe artificial neuron model assumed by Kurzweil and used in many current artificial neural network implementations is simple compared with biological neurons. A brain simulation would likely have to capture the detailed cellular behaviour of biological neurons, presently only understood in the broadest of outlines. The overhead introduced by full modeling of the biological, chemical, and physical details of neural behaviour (especially on a molecular scale) would require computational powers several orders of magnitude larger than Kurzweil's estimate. In addition the estimates do not account for glial cells, which are at least as numerous as neurons, and which may outnumber neurons by as much as 10:1, and are now known to play a role in cognitive processes.\n\nThere are some research projects that are investigating brain simulation using more sophisticated neural models, implemented on conventional computing architectures. The Artificial Intelligence System project implemented non-real time simulations of a \"brain\" (with 10 neurons) in 2005. It took 50 days on a cluster of 27 processors to simulate 1 second of a model. The Blue Brain project used one of the fastest supercomputer architectures in the world, IBM's Blue Gene platform, to create a real time simulation of a single rat neocortical column consisting of approximately 10,000 neurons and 10 synapses in 2006. A longer term goal is to build a detailed, functional simulation of the physiological processes in the human brain: \"It is not impossible to build a human brain and we can do it in 10 years,\" Henry Markram, director of the Blue Brain Project said in 2009 at the TED conference in Oxford. There have also been controversial claims to have simulated a cat brain. Neuro-silicon interfaces have been proposed as an alternative implementation strategy that may scale better.\n\nHans Moravec addressed the above arguments (\"brains are more complicated\", \"neurons have to be modeled in more detail\") in his 1997 paper \"When will computer hardware match the human brain?\". He measured the ability of existing software to simulate the functionality of neural tissue, specifically the retina. His results do not depend on the number of glial cells, nor on what kinds of processing neurons perform where.\n\nA fundamental criticism of the simulated brain approach derives from embodied cognition where human embodiment is taken as an essential aspect of human intelligence. Many researchers believe that embodiment is necessary to ground meaning. If this view is correct, any fully functional brain model will need to encompass more than just the neurons (i.e., a robotic body). Goertzel proposes virtual embodiment (like Second Life), but it is not yet known whether this would be sufficient.\n\nDesktop computers using microprocessors capable of more than 10 cps (Kurzweil's non-standard unit \"computations per second\", see above) have been available since 2005. According to the brain power estimates used by Kurzweil (and Moravec), this computer should be capable of supporting a simulation of a bee brain, but despite some interest no such simulation exists . There are at least three reasons for this:\n\nIn addition, the scale of the human brain is not currently well-constrained. One estimate puts the human brain at about 100 billion neurons and 100 trillion synapses. Another estimate is 86 billion neurons of which 16.3 billion are in the cerebral cortex and 69 billion in the cerebellum. Glial cell synapses are currently unquantified but are known to be extremely numerous.\n\nAlthough the role of consciousness in strong AI/AGI is debatable, many AGI researchers regard research that investigates possibilities for implementing consciousness as vital. In an early effort Igor Aleksander argued that the principles for creating a conscious machine already existed but that it would take forty years to train such a machine to understand language.\n\nIn 1980, philosopher John Searle coined the term \"strong AI\" as part of his Chinese room argument. He wanted to distinguish between two different hypotheses about artificial intelligence:\nThe first one is called \"the \"strong\" AI hypothesis\" and the second is \"the \"weak\" AI hypothesis\" because the first one makes the \"stronger\" statement: it assumes something special has happened to the machine that goes beyond all its abilities that we can test. Searle referred to the \"strong AI hypothesis\" as \"strong AI\". This usage is also common in academic AI research and textbooks.\n\nThe weak AI hypothesis is equivalent to the hypothesis that artificial general intelligence is possible. According to Russell and Norvig, \"Most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis.\"\n\nIn contrast to Searle, Kurzweil uses the term \"strong AI\" to describe any artificial intelligence system that acts like it has a mind, regardless of whether a philosopher would be able to determine if it \"actually\" has a mind or not.\n\nSince the launch of AI research in 1956, the growth of this field has slowed down over time and has stalled the aims of creating machines skilled with intelligent action at the human level. A possible explanation for this delay is that computers lack a sufficient scope of memory or processing power. In addition, the level of complexity that connects to the process of AI research may also limit the progress of AI research.\n\nWhile most AI researchers believe that strong AI can be achieved in the future, there are some individuals like Hubert Dreyfus and Roger Penrose that deny the possibility of achieving AI. John McCarthy was one of various computer scientists who believe human-level AI will be accomplished, but a date cannot accurately be predicted.\n\nConceptual limitations are another possible reason for the slowness in AI research. AI researchers may need to modify the conceptual framework of their discipline in order to provide a stronger base and contribution to the quest of achieving strong AI. As William Clocksin wrote in 2003: \"the framework starts from Weizenbaum’s observation that intelligence manifests itself only relative to specific social and cultural contexts\".\n\nFurthermore, AI researchers have been able to create computers that can perform jobs that are complicated for people to do, but conversely they have struggled to develop a computer that is capable of carrying out tasks that are simple for humans to do . A problem that is described by David Gelernter is that some people assume that thinking and reasoning are equivalent. However, the idea of whether thoughts and the creator of those thoughts are isolated individually has intrigued AI researchers.\n\nThe problems that have been encountered in AI research over the past decades have further impeded the progress of AI. The failed predictions that have been promised by AI researchers and the lack of a complete understanding of human behaviors have helped diminish the primary idea of human-level AI. Although the progress of AI research has brought both improvement and disappointment, most investigators have established optimism about potentially achieving the goal of AI in the 21st century.\n\nOther possible reasons have been proposed for the lengthy research in the progress of strong AI. The intricacy of scientific problems and the need to fully understand the human brain through psychology and neurophysiology have limited many researchers from emulating the function of the human brain into a computer hardware. Many researchers tend to underestimate any doubt that is involved with future predictions of AI, but without taking those issues seriously can people then overlook solutions to problematic questions.\n\nClocksin says that a conceptual limitation that may impede the progress of AI research is that people may be using the wrong techniques for computer programs and implementation of equipment. When AI researchers first began to aim for the goal of artificial intelligence, a main interest was human reasoning. Researchers hoped to establish computational models of human knowledge through reasoning and to find out how to design a computer with a specific cognitive task.\n\nThe practice of abstraction, which people tend to redefine when working with a particular context in research, provides researchers with a concentration on just a few concepts. The most productive use of abstraction in AI research comes from planning and problem solving. Although the aim is to increase the speed of a computation, the role of abstraction has posed questions about the involvement of abstraction operators.\n\nA possible reason for the slowness in AI relates to the acknowledgement by many AI researchers that heuristics is a section that contains a significant breach between computer performance and human performance. The specific functions that are programmed to a computer may be able to account for many of the requirements that allow it to match human intelligence. These explanations are not necessarily guaranteed to be the fundamental causes for the delay in achieving strong AI, but they are widely agreed by numerous researchers.\n\nThere have been many AI researchers that debate over the idea whether machines should be created with emotions. There are no emotions in typical models of AI and some researchers say programming emotions into machines allows them to have a mind of their own. Emotion sums up the experiences of humans because it allows them to remember those experiences. David Gelernter writes, \"No computer will be creative unless it can simulate all the nuances of human emotion.\" This concern about emotion has posed problems for AI researchers and it connects to the concept of strong AI as its research progresses into the future.\n\nThere are other aspects of the human mind besides intelligence that are relevant to the concept of strong AI which play a major role in science fiction and the ethics of artificial intelligence:\nThese traits have a moral dimension, because a machine with this form of strong AI may have legal rights, analogous to the rights of non-human animals. Also, Bill Joy, among others, argues a machine with these traits may be a threat to human life or dignity. It remains to be shown whether any of these traits are necessary for strong AI. The role of consciousness is not clear, and currently there is no agreed test for its presence. If a machine is built with a device that simulates the neural correlates of consciousness, would it automatically have self-awareness? It is also possible that some of these properties, such as sentience, naturally emerge from a fully intelligent machine, or that it becomes natural to \"ascribe\" these properties to machines once they begin to act in a way that is clearly intelligent. For example, intelligent action may be sufficient for sentience, rather than the other way around.\n\nIn science fiction, AGI is associated with traits such as consciousness, sentience, sapience, and self-awareness observed in living beings. However, according to philosopher John Searle, it is an open question whether general intelligence is sufficient for consciousness, even a digital brain simulation. \"Strong AI\" (as defined above by Ray Kurzweil) should not be confused with Searle's \"'strong AI hypothesis\". The strong AI hypothesis is the claim that a computer which behaves as intelligently as a person must also necessarily have a mind and consciousness. AGI refers only to the amount of intelligence that the machine displays, with or without a mind.\n\nOpinions vary both on \"whether\" and \"when\" artificial general intelligence will arrive. At one extreme, AI pioneer Herbert A. Simon wrote in 1965: \"machines will be capable, within twenty years, of doing any work a man can do\"; obviously this prediction failed to come true. Microsoft co-founder Paul Allen believes that such intelligence is unlikely this century because it would require \"unforeseeable and fundamentally unpredictable breakthroughs\" and a \"scientifically deep understanding of cognition\". Writing in The Guardian, roboticist Alan Winfield claimed the gulf between modern computing and human-level artificial intelligence is as wide as the gulf between current space flight and practical faster than light spaceflight. Optimism that AGI is feasible waxes and wanes, and may have seen a resurgence in the 2010s: around 2015, computer scientist Richard Sutton averaged together some recent polls of artificial intelligence experts and estimated a 25% chance that AGI will arrive before 2030, but a 10% chance that it will never arrive at all.\n\nThe creation of artificial general intelligence may have repercussions so great and so complex that it may not be possible to forecast what will come afterwards. Thus the event in the hypothetical future of achieving strong AI is called the technological singularity, because theoretically one cannot see past it. But this has not stopped philosophers and researchers from guessing what the smart computers or robots of the future may do, including forming a utopia by being our friends or overwhelming us in an AI takeover. The latter potentiality is particularly disturbing as it poses an existential risk for mankind.\n\nSmart computers or robots would be able to produce copies of themselves. They would be self-replicating machines. A growing population of intelligent robots could conceivably outcompete inferior humans in job markets, in business, in science, in politics (pursuing robot rights), and technologically, sociologically (by acting as one), and militarily.\n\nIf research into strong AI produced sufficiently intelligent software, it would be able to reprogram and improve itself – a feature called \"recursive self-improvement\". It would then be even better at improving itself, and would probably continue doing so in a rapidly increasing cycle, leading to an intelligence explosion and the emergence of superintelligence. Such an intelligence would not have the limitations of human intellect, and might be able to invent or discover almost anything.\n\nHyper-intelligent software might not necessarily decide to support the continued existence of mankind, and might be extremely difficult to stop. This topic has also recently begun to be discussed in academic publications as a real source of risks to civilization, humans, and planet Earth.\n\nOne proposal to deal with this is to make sure that the first generally intelligent AI is friendly AI, that would then endeavor to ensure that subsequently developed AIs were also nice to us. But, friendly AI is harder to create than plain AGI, and therefore it is likely, in a race between the two, that non-friendly AI would be developed first. Also, there is no guarantee that friendly AI would remain friendly, or that its progeny would also all be good.\n\n",
    "id": "586357",
    "title": "Artificial general intelligence"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=45567903",
    "text": "Luminoso\n\nLuminoso, a Cambridge, MA-based text analytics and artificial intelligence company, spun out of the MIT Media Lab and its crowd-sourced Open Mind Common Sense (OMCS) project.\n\nThe company has raised $8 million in financing and its clients include Sony, Autodesk, Scotts Miracle-Gro, and GlaxoSmithKline.\n\nLuminoso was co-founded in 2010 by Dennis Clark, Jason Alonso, Rob Speer, and CEO Catherine Havasi, a research scientist at MIT in artificial intelligence and computational linguistics. The company builds on the knowledge base of MIT’s Open Mind Common Sense (OMCS) project, co-founded in 1999 by Havasi, who continues to serve as its director. The OCMS knowledge base has since been combined with knowledge from other crowdsourced resources, games with a purpose, and expert-created resources to become ConceptNet. ConceptNet consists of approximately 28 million statements in 304 languages, with full support for 10 languages and moderate support for 77 languages. ConceptNet is a resource for making an AI that understands the meanings of the words people use. \n\nDuring the World Cup in June 2014, the company provided a widely reported real-time sentiment analysis of the U.S. vs. Germany match, analyzing 900,000 posts on Twitter, Facebook and Google+.\n\nThe company uses artificial intelligence, natural language processing, and machine learning to derive insights from unstructured data such as contact center interactions, chatbot and live chat transcripts, product reviews, open-ended survey responses, and email. Luminoso's software identifies and quantifies patterns and relationships in text-based data, including domain-specific or creative language. Rather than human-powered keyword searches of data, the software automates taxonomy creation around concepts, allowing related words and phrases to be dynamically generated and tracked. \n\nCommercial applications include analyzing, prioritizing, and routing contact center interactions; identifying consumer complaints before they begin to trend; and tracking sentiment during product launches. The software natively analyzes text in thirteen languages, as well as emoji.\n\nLuminoso's technology can be accessed via two products: Luminoso Analytics and Luminoso Compass. Luminoso Analytics enables a deep-dive analysis into batch or real-time data, whereas Luminoso Compass automates the categorization of real-time data. Both products offer a user interface as well as an API. Luminoso's products can be implemented through either a cloud-based or an on-premises solution.\n\nLuminoso continues to actively conduct research in natural language processing and word embeddings and regularly participates in evaluations such as SemEval. At SemEval 2017, Luminoso participated in Task 2, measuring the semantic similarity of word pairs within and across five languages. Its solution outperformed all competing systems in every language pair tested, with the exception of Persian.\n\nLuminoso has been listed as a \"Cool Vendor in AI for Marketing\" by Gartner, and has also been named a \"Boston Artificial Intelligence startup to watch\" by BostInno. \n\nIn May 2017, Luminoso was recognized as having the Best Application for AI in the Enterprise by AI Business, and was also shortlisted as the Best AI Breakthrough and Best Innovation in NLP.\n\nMajor competitors include Clarabridge and Lexalytics.\n\nAcadia Woods led a $6.5 million round of funding, with Japan’s Digital Garage, in July 2014. The company previously raised a $1.5 million seed round.\n\n",
    "id": "45567903",
    "title": "Luminoso"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=46504825",
    "text": "Open Letter on Artificial Intelligence\n\nIn January 2015, Stephen Hawking, Elon Musk, and dozens of artificial intelligence experts signed an open letter on artificial intelligence calling for research on the societal impacts of AI. The letter affirmed that society can reap great potential benefits from artificial intelligence, but called for concrete research on how to prevent certain potential \"pitfalls\": artificial intelligence has the potential to eradicate disease and poverty, but researchers must not create something which cannot be controlled. The four-paragraph letter, titled \"Research Priorities for Robust and Beneficial Artificial Intelligence: An Open Letter\", lays out detailed research priorities in an accompanying twelve-page document.\n\nBy 2014, both physicist Stephen Hawking and business magnate Elon Musk had publicly voiced the opinion that superhuman artificial intelligence could provide incalculable benefits, but could also end the human race if deployed incautiously (see Existential risk from advanced artificial intelligence). Hawking and Musk both sit on the scientific advisory board for the Future of Life Institute, an organization working to \"mitigate existential risks facing humanity\". The institute drafted an open letter directed to the broader AI research community, and circulated it to the attendees of its first conference in Puerto Rico during the first weekend of 2015. The letter was made public on January 12.\n\nThe letter highlights both the positive and negative effects of artificial intelligence. According to Bloomberg Business, Professor Max Tegmark of MIT circulated the letter in order to find common ground between signatories who consider superintelligent AI a significant existential risk, and signatories such as Professor Oren Etzioni, who believe the AI field was being \"impugned\" by a one-sided media focus on the alleged risks. The letter contends that:\n\nThe potential benefits (of AI) are huge, since everything that civilization has to offer is a product of human intelligence; we cannot predict what we might achieve when this intelligence is magnified by the tools AI may provide, but the eradication of disease and poverty are not unfathomable. Because of the great potential of AI, it is important to research how to reap its benefits while avoiding potential pitfalls.\n\nOne of the signatories, Professor Bart Selman of Cornell University, said the purpose is to get AI researchers and developers to pay more attention to AI safety. In addition, for policymakers and the general public, the letter is meant to be informative but not alarmist. Another signatory, Professor Francesca Rossi, stated that \"I think it's very important that everybody knows that AI researchers are seriously thinking about these concerns and ethical issues\".\n\nThe signatories ask: How can engineers create AI systems that are beneficial to society, and that are robust? Humans need to remain in control of AI; our AI systems must \"do what we want them to do\". The required research is interdisciplinary, drawing from areas ranging from economics and law to various branches of computer science, such as computer security and formal verification. Challenges that arise are divided into verification (\"Did I build the system right?\"), validity (\"Did I build the right system?\"), security, and control (\"OK, I built the system wrong, can I fix it?\")\n\nSome near-term concerns relate to autonomous vehicles, from civilian drones and self-driving cars. For example, a self-driving car may, in an emergency, have to decide between a small risk of a major accident and a large probability of a small accident. Other concerns relate to lethal intelligent autonomous weapons: Should they be banned? If so, how should 'autonomy' be precisely defined? If not, how should culpability for any misuse or malfunction be apportioned?\n\nOther issues include privacy concerns as AI becomes increasingly able to interpret large surveillance datasets, and how to best manage the economic impact of jobs displaced by AI.\n\nThe document closes by echoing Microsoft research director Eric Horvitz's concerns that:\n\nwe could one day lose control of AI systems via the rise of superintelligences that do not act in accordance with human wishes – and that such powerful systems would threaten humanity. Are such dystopic outcomes possible? If so, how might these situations arise? ...What kind of investments in research should be made to better understand and to address the possibility of the rise of a dangerous superintelligence or the occurrence of an \"intelligence explosion\"?\n\nExisting tools for harnessing AI, such as reinforcement learning and simple utility functions, are inadequate to solve this; therefore more research is necessary to find and validate a robust solution to the \"control problem\".\n\nSignatories include physicist Stephen Hawking, business magnate Elon Musk, the co-founders of DeepMind, Vicarious, Google's director of research Peter Norvig, Professor Stuart J. Russell of the University of California Berkeley, and other AI experts, robot makers, programmers, and ethicists. The original signatory count was over 150 people, including academics from Cambridge, Oxford, Stanford, Harvard, and MIT.\n\n",
    "id": "46504825",
    "title": "Open Letter on Artificial Intelligence"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=46222904",
    "text": "Intel RealSense\n\nIntel RealSense, formerly known as Intel Perceptual Computing, is a platform for implementing gesture-based human-computer interaction techniques. It consists of series of consumer grade 3D cameras together with an easy to use machine perception library that simplifies supporting the cameras for third-party software developers.\n\nAs of March 2015, multiple laptop and tablet computer manufactures offer one or more devices with Intel RealSense camera built in. These are Asus, HP, Dell, Lenovo, and Acer. Also, a standalone webcam from Razer is among the first to offer the Intel RealSense camera built into the design. Consumer-ready versions of the RealSense camera are the Razer Stargazer and the Creative BlasterX Senz3D.\n\n\nAn Intel RealSense camera contains the following four components: a conventional camera, an infrared laser projector, an infrared camera, and a microphone array. The infrared projector projects a grid onto the scene (in infrared light which is invisible to human eye) and the infrared camera records it to compute depth information. The microphone array allows localizing sound sources in space and performing background noise cancellation.\n\nThree camera models were announced, with distinct specifications and intended use.\n\nThis is a stand-alone camera that can be attached to a desktop or laptop computer. It is intended to be used for natural gesture-based interaction, face recognition, immersive, video conferencing and collaboration, gaming and learning and 3D scanning.\n\nThere is a version of this camera to be embedded into laptop computers.\n\n\nSnapshot is a camera intended to be built into tablet computers and possibly smartphones. Its intended uses include taking photographs and performing after the fact refocusing, distance measurements, and applying motion photo filters.\n\nThe refocus feature differs from a plenoptic camera in that RealSense Snapshot takes pictures with large depth of field so that initially the whole picture is in focus and then in software it selectively blurs parts of the image depending on their distance.\n\nDell Venue 8 7000 Series Android tablet is equipped with this camera.\n\nRear-mounted camera for Microsoft Surface or a similar tablet, like the HP Spectre X2. This camera is intended for augmented reality applications, content creation, and object scanning. Its depth accuracy is on the order of millimeters and its range is up to 6.0 meters. This makes it the more accurate and longer range of the Intel RealSense 3D cameras. Also unlike the F200 and SR300, the R200 is a stereo camera and is able to obtain accurate depth outdoors as well as indoors.\n\nThe SR300 camera is the next generation of the Front F200 camera. It improves in various respects over its predecessor, notably with\n\n\nThe Front SR300 camera can be bought from Intel for approximately $130 USD.\n\nTo address the lack of applications built on the RealSense platform and to promote the platform among software developers, in 2014 Intel organized the Intel RealSense App Challenge. The winners were awarded large sums of money.\n\nIn an early preview article, \"PC World\"s Mark Hachman concluded that RealSense is an enabling technology that will be largely defined by the software that will take advantage of its features. He noted that as of the time the article was written, the technology was new and there was no such software.\n\n\n",
    "id": "46222904",
    "title": "Intel RealSense"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=5389390",
    "text": "Autonomic networking\n\nAutonomic Networking follows the concept of Autonomic Computing, an initiative started by IBM in 2001. Its ultimate aim is to create self-managing networks to overcome the rapidly growing complexity of the Internet and other networks and to enable their further growth, far beyond the size of today.\n\nThe ever-growing management complexity of the Internet caused by its rapid growth is seen by some experts as a major problem that limits its usability in the future.\n\nWhat's more, increasingly popular smartphones, PDAs, networked audio and video equipment, and game consoles need to be interconnected. Pervasive Computing not only adds features, but also burdens existing networking infrastructure with more and more tasks that sooner or later will not be manageable by human intervention alone.\n\nAnother important aspect is the price of manually controlling huge numbers of vitally important devices of current network infrastructures.\n\nThe autonomic nervous system (ANS) is the part of the nervous system of the higher life forms that is not consciously controlled. It regulates bodily functions and the activity of specific organs. As proposed by IBM, future communication systems might be designed in a similar way to the ANS. \n\nAs autonomics conceptually derives from biological entities such as the human autonomic nervous system, each of the areas can be metaphorically related to functional and structural aspects of a living being. In the human body, the autonomic system facilitates and regulates a variety of functions including respiration, blood pressure and circulation, and emotive response. The autonomic nervous system is the interconnecting fabric that supports feedback loops between internal states and various sources by which internal and external conditions are monitored.\n\nAutognostics includes a range of self-discovery, awareness, and analysis capabilities that provide the autonomic system with a view on high-level state. In metaphor, this represents the perceptual sub-systems that gather, analyze, and report on internal and external states and conditions – for example, this might be viewed as the eyes, visual cortex and perceptual organs of the system. Autognostics, or literally \"self-knowledge\", provides the autonomic system with a basis for response and validation.\n\nA rich autognostic capability may include many different \"perceptual senses\". For example, the human body gathers information via the usual five senses, the so-called sixth sense of proprioception (sense of body position and orientation), and through emotive states that represent the gross wellness of the body. As conditions and states change, they are detected by the sensory monitors and provide the basis for adaptation of related systems. Implicit in such a system are imbedded models of both internal and external environments such that relative value can be assigned to any perceived state - perceived physical threat (e.g. a snake) can result in rapid shallow breathing related to fight-flight response, a phylogenetically effective model of interaction with recognizable threats.\n\nIn the case of autonomic networking, the state of the network may be defined by inputs from:\n\nMost of these sources represent relatively raw and unprocessed views that have limited relevance. Post-processing and various forms of analysis must be applied to generate meaningful measurements and assessments against which current state can be derived.\n\nThe autognostic system interoperates with:\n\nConfiguration management is responsible for the interaction with network elements and interfaces. It includes an accounting capability with historical perspective that provides for the tracking of configurations over time, with respect to various circumstances. In the biological metaphor, these are the hands and, to some degree, the memory of the autonomic system.\n\nOn a network, remediation and provisioning are applied via configuration setting of specific devices. Implementation affecting access and selective performance with respect to role and relationship are also applied. Almost all the \"actions\" that are currently taken by human engineers fall under this area. With only a few exceptions, interfaces are set by hand, or by extension of the hand, through automated scripts.\n\nImplicit in the configuration process is the maintenance of a dynamic population of devices under management, a historical record of changes and the directives which invoked change. Typical to many accounting functions, configuration management should be capable of operating on devices and then rolling back changes to recover previous configurations. Where change may lead to unrecoverable states, the sub-system should be able to qualify the consequences of changes prior to issuing them.\n\nAs directives for change must originate from other sub-systems, the shared language for such directives must be abstracted from the details of the devices involved. The configuration management sub-system must be able to translate unambiguously between directives and hard actions or to be able to signal the need for further detail on a directive. An inferential capacity may be appropriate to support sufficient flexibility (i.e. configuration never takes place because there is no unique one-to-one mapping between directive and configuration settings). Where standards are not sufficient, a learning capacity may also be required to acquire new knowledge of devices and their configuration.\n\nConfiguration management interoperates with all of the other sub-systems including:\n\nPolicy management includes policy specification, deployment, reasoning over policies, updating and maintaining policies, and enforcement. Policy-based management is required for:\n\nIt provides the models of environment and behavior that represent effective interaction according to specific goals. In the human nervous system metaphor, these models are implicit in the evolutionary \"design\" of biological entities and specific to the goals of survival and procreation. Definition of what constitutes a policy is necessary to consider what is involved in managing it. A relatively flexible and abstract framework of values, relationships, roles, interactions, resources, and other components of the network environment is required. This sub-system extends far beyond the physical network to the applications in use and the processes and end-users that employ the network to achieve specific goals. It must express the relative values of various resources, outcomes, and processes and include a basis for assessing states and conditions.\n\nUnless embodied in some system outside the autonomic network or implicit to the specific policy implementation, the framework must also accommodate the definition of process, objectives and goals. Business process definitions and descriptions are then an integral part of the policy implementation. Further, as policy management represents the ultimate basis for the operation of the autonomic system, it must be able to report on its operation with respect to the details of its implementation.\n\nThe policy management sub-system interoperates (at least) indirectly with all other sub-systems but primarily interacts with:\n\nAutodefense represents a dynamic and adaptive mechanism that responds to malicious and intentional attacks on the network infrastructure, or use of the network infrastructure to attack IT resources. As defensive measures tend to impede the operation of IT, it is optimally capable of balancing performance objectives with typically over-riding threat management actions. In the biological metaphor, this sub-system offers mechanisms comparable to the immune system.\n\nThis sub-system must proactively assess network and application infrastructure for risks, detect and identify threats, and define effective both proactive and reactive defensive responses. It has the role of the warrior and the security guard insofar as it has roles for both maintenance and corrective activities. Its relationship with security is close but not identical – security is more concerned with appropriately defined and implemented access and authorization controls to maintain legitimate roles and process. Autodefense deals with forces and processes, typically malicious, outside the normal operation of the system that offer some risk to successful execution.\n\nAutodefense requires high-level and detailed knowledge of the entire network as well as imbedded models of risk that allow it to analyze dynamically the current status. Corrections to decrease risk must be considered in balance with performance objectives and value of process goals – an overzealous defensive response can immobilize the system (like the immune system inappropriately invoking an allergic reaction). The detection of network or application behaviors that signal possible attack or abuse is followed by the generation of an appropriate response – for example, ports might be temporarily closed or packets with a specific source or destination might be filtered out. Further assessment generates subsequent changes either relaxing the defensive measures or strengthening them.\n\nAutodefense interoperates closely with:\n\nIt also may receive definition of relative value of various resources and processes from policy management in order to develop responses consistent with policy.\n\nSecurity provides the structure that defines and enforces the relationships between roles, content, and resources, particularly with respect to access. It includes the framework for definitions as well as the means to implement them. In metaphor, security parallels the complex mechanisms underlying social interactions, defining friends, foes, mates and allies and offering access to limited resources on the basis of assessed benefit.\n\nSeveral key means are employed by security – they include the well-known 3 As of authentication, authorization, and access (control). The basis for applying these means requires the definition of roles and their relationships to resources, processes and each other. High-level concepts like privacy, anonymity and verification are likely imbedded in the form of the role definitions and derive from policy. Successful security reliably supports and enforces roles and relationships.\n\nAutodefense has a close association with security – maintaining the assigned roles in balance with performance exposes the system to potential violations in security. In those cases, the system must compensate by making changes that may sacrifice balance on a temporary basis and indeed may violate the operational terms of security itself. Typically the two are viewed as inextricably intertwined – effective security somewhat hopefully negating any need for a defensive response. Security’s revised role is to mediate between the competing demands from policy for maximized performance and minimized risk with auto defense recovering the balance when inevitable risk translates to threat. Federation represents one of the key challenges to be solved by effective security.\n\nThe security sub-system interoperates directly with:\n\nThe connection fabric supports the interaction with all the elements and sub-systems of the autonomic system. It may be composed of a variety of means and mechanisms, or may be a single central framework. The biological equivalent is the central nervous system itself – although referred to as the autonomic system, it actually is only the communication conduit between the human body’s faculties.\n\nConsequently, it is currently under research by many research projects, how principles and paradigms of mother nature might be applied to networking.\n\nInstead of a layering approach, autonomic networking targets a more flexible structure termed compartmentalization.\n\nThe goal is to produce an architectural design that enables flexible, dynamic, and fully autonomic formation of large-scale networks in which the functionalities of each constituent network node are also composed in an autonomic fashion\n\nFunctions should be divided into atomic units to allow for maximal re-composition freedom.\n\nA fundamental concept of Control theory, the closed control loop, is among the fundamental principles of autonomic networking. A closed control loop maintains the properties of the controlled system within desired bounds by constantly monitoring target parameters.\n\n\n\n\n",
    "id": "5389390",
    "title": "Autonomic networking"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=47228422",
    "text": "User behavior analytics\n\nUser behavior analytics (\"UBA\") as defined by Gartner or Análise de comportamento de Usuário (ACU) in Portuguese, is a cybersecurity process about detection of insider threats, targeted attacks, and financial fraud. UBA solutions look at patterns of human behavior, and then apply algorithms and statistical analysis to detect meaningful anomalies from those patterns—anomalies that indicate potential threats. Instead of tracking devices or security events, UBA tracks a system's users. Big data platforms like Apache Hadoop are increasing UBA functionality by allowing them to analyze petabytes worth of data to detect insider threats and advanced persistent threats.\n\nThe problem UBA responds to, as described by Nemertes Research CEO Johna Till Johnson, is that \"Security systems provide so much information that it's tough to uncover information that truly indicates a potential for real attack. Analytics tools help make sense of the vast amount of data that SIEM, IDS/IPS, system logs, and other tools gather. UBA tools use a specialized type of security analytics that focuses on the behavior of systems and the people using them. UBA technology first evolved in the field of marketing, to help companies understand and predict consumer-buying patterns. But as it turns out, UBA can be extraordinarily useful in the security context too.\" \n\nDevelopments in UBA technology led Gartner to evolve the category to user and entity behavior analytics (\"UEBA\"). In September 2015, Gartner published the Market Guide for User and Entity Analytics by Vice President and Distinguished Analyst, Avivah Litan, that provided a thorough definition and explanation. UEBA was referred to in earlier Gartner reports but not in much depth. Expanding the definition from UBA includes devices, applications, servers, data, or anything with an IP address. It moves beyond the fraud-oriented UBA focus to a broader one encompassing \"malicious and abusive behavior that otherwise went unnoticed by existing security monitoring systems, such as SIEM and DLP.\" The addition of \"entity\" reflects that devices may play a role in a network attack and may also be valuable in uncovering attack activity. \"When end users have been compromised, malware can lay dormant and go undetected for months. Rather than trying to find where the outsider entered, UEBAs allow for quicker detection by using algorithms to detect insider threats.\"\n\nParticularly in the computer security market, there are many vendors for UEBA applications. They can be \"differentiated by whether they are designed to monitor on-premises or cloud-based software as a service (SaaS) applications; the methods in which they obtain the source data; the type of analytics they use (i.e., packaged analytics, user-driven or vendor-written), and the service delivery method (i.e., on-premises or a cloud-based).\" \nAccording to the 2015 market guide released by Gartner, \"the UEBA market grew substantially in 2015; UEBA vendors grew their customer base, market consolidation began, and Gartner client interest in UEBA and security analytics increased.\" The report further projected, \"Over the next three years, leading UEBA platforms will become preferred systems for security operations and investigations at some of the organizations they serve. It will be—and in some cases already is—much easier to discover some security events and analyze individual offenders in UEBA than it is in many legacy security monitoring systems.\"\n\n",
    "id": "47228422",
    "title": "User behavior analytics"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=11338826",
    "text": "Admissible heuristic\n\nIn computer science, specifically in algorithms related to pathfinding, a heuristic function is said to be admissible if it never overestimates the cost of reaching the goal, i.e. the cost it estimates to reach the goal is not higher than the lowest possible cost from the current point in the path.\n\nAn admissible heuristic is used to estimate the cost of reaching the \ngoal state in an informed search algorithm. In order for a heuristic\nto be admissible to the search problem, the estimated cost must always \nbe lower than or equal to the actual cost of reaching the goal state. \nThe search algorithm uses the admissible heuristic to find an estimated \noptimal path to the goal state from the current node. \nFor example, in A* search the evaluation function (where \nformula_1 is the current node) is:\n\nformula_2\n\nwhere\n\nformula_5 is calculated using the heuristic \nfunction. With a non-admissible heuristic, the A* algorithm could \noverlook the optimal solution to a search problem due to an \noverestimation in formula_3.\n\nAn admissible heuristic can be derived from a relaxed\nversion of the problem, or by information from pattern databases that store exact solutions to subproblems of the problem, or by using inductive learning methods.\n\nTwo different examples of admissible heuristics apply to the fifteen puzzle problem:\n\nThe Hamming distance is the total number of misplaced tiles. It is clear that this heuristic is admissible since the total number of moves to order the tiles correctly is at least the number of misplaced tiles (each tile not in place must be moved at least once). The cost (number of moves) to the goal (an ordered puzzle) is at least the Hamming distance of the puzzle.\n\nThe Manhattan distance of a puzzle is defined as:\n\nConsider the puzzle below in which the player wishes to move each tile such that the numbers are ordered. The Manhattan distance is an admissible heuristic in this case because every tile will have to be moved at least the amount of spots in between itself and its correct position. \n\nThe subscripts show the Manhattan distance for each tile. The total Manhattan distance for the shown puzzle is:\n\nIf an admissible heuristic is adapted in an algorithm (for example in A* search algorithm), then this algorithm would eventually find an optimal solution to the goal. A well-known and easy-to-understand example is breadth-first search. In the case of BFS, the heuristic evaluation function formula_5 equals to formula_21 for each node formula_1, which is obviously less than the actual cost (thus underestimated). This heuristic would cause the BFS algorithm to search literally all the possible paths and eventually find the optimal solution (i.e., the shortest path to the goal).\n\nAs an example of why admissibility can guarantee optimality, let's say we have costs as follows:(the cost above/below a node is the heuristic, the cost at an edge is the actual cost)\n\nSo clearly we'd start off visiting the top middle node, since the expected total cost, i.e. formula_3, is formula_24. Then the goal would be a candidate, with formula_3 equal to formula_26. Then we'd clearly pick the bottom nodes one after the other, followed by the updated goal, since they all have formula_3 lower than the formula_3 of the current goal, i.e. their formula_3 is formula_30. So even though the goal was a candidate, we couldn't pick it because there were still better paths out there. This way, an admissible heuristic can ensure optimality.\n\nHowever, note that although an admissible heuristic can guarantee final optimality, it's not necessarily efficient.\n\nWhile all consistent heuristics are admissible, not all admissible heuristics are consistent.\n\nFor tree search problems, if an admissible heuristic is used, the A* search algorithm will never return a suboptimal goal node.\n\n",
    "id": "11338826",
    "title": "Admissible heuristic"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=39242563",
    "text": "Medical intelligence and language engineering lab\n\nThe Medical Intelligence and Language Engineering laboratory, also known as MILE lab, is a research laboratory at the Indian Institute of Science, Bangalore under the Department of Electrical Engineering. The lab is known for its work on Image processing, online handwriting recognition, Text-To-Speech and Optical character recognition systems, all of which are focused mainly on documents and speech in Indian languages. The lab is headed by A. G. Ramakrishnan.\n\nOne of the commitments of MILE lab is the development of technology for people with visual impairment to harness knowledge from any available printed material in Indian languages. The lab is working towards reaching this goal. Its work till now included: document mosaicing of coloured, camera captured images ; text extraction from complex colour images, including camera captured images; document layout analysis; detection of broken and merged characters; OCR technology for Tamil and Kannada; text to speech conversion in Tamil and Kannada; pitch modification using discrete cosine transform in the source domain; automated part of speech tagging; phrase prediction and prosody modeling.\n\nMozhi Vallan, the Tamil OCR product developed by MILE Lab, is being used by Worth Trust and Karna Vidya Technology Centre, Chennai for the conversion of printed school and college books to Braille format. Sri Ramakrishna Math, Chennai is using it to convert their printed philosophical books in Tamil to computer readable text. Lipi Gnani, the Kannada OCR developed by MILE Lab is being used by Braille Transcription Centers of Mitrajyothi and Canara Bank Relief & Welfare Society, Bangalore for similar purposes. Also, Thirukkural, the Tamil TTS system developed by MILE Lab is being used by some school teachers in Singapore for assignments. Madhura, the Kannada TTS developed by the lab, is being used by two blind students, integrated with a screen reader, to read aloud text OCR'ed with Lipi Gnani from Kannada books. Currently, the lab is researching on machine listening and a novel temporal feature named as plosion index has been proposed, which has been shown to be extremely effective in detecting closure-burst transitions of stop consonants and affricates from continuous speech, even in noise. Another feature proposed is DCTILPR, which is a voice source based feature vector that improves the recognition performance of a speaker identification system.\n\nIn the early days, significant work was carried out in medical signal and image processing. A unique algorithm was proposed for ECG compression by treating each cardiac cycle as a vector, and applying linear prediction on the discrete wavelet transform of this vector, after normalizing its period using multirate processing based interpolation. The maturity of the fetal lung was predicted using image texture features obtained from the liver and lung regions of the ultrasound images obtained from pregnant women An effective technique was proposed for lossless compression of 3D magnetic resonance images of the brain. Each MRI slice was represented by uniform or adaptive mesh; affine transformation was applied between the corresponding mesh elements of adjacent slices and context-based entropy coding, on the residues.\n\n",
    "id": "39242563",
    "title": "Medical intelligence and language engineering lab"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=44628427",
    "text": "Cloud robotics\n\nCloud robotics is a field of robotics that attempts to invoke cloud technologies such as cloud computing, cloud storage, and other Internet technologies centred on the benefits of converged infrastructure and shared services for robotics. When connected to the cloud, robots can benefit from the powerful computation, storage, and communication resources of modern data center in the cloud, which can process and share information from various robots or agent (other machines, smart objects, humans, etc.). Humans can also delegate tasks to robots remotely through networks. Cloud computing technologies enable robot systems to be endowed with powerful capability whilst reducing costs through cloud technologies. Thus, it is possible to build lightweight, low cost, smarter robots have intelligent \"brain\" in the cloud. The \"brain\" consists of data center, knowledge base, task planners, deep learning, information processing, environment models, communication support etc.\n\nA cloud for robots potentially has at least six significant components:\n\n\nAutonomous mobile robots: Google's self-driving cars are cloud robots. The cars use the network to access Google's enormous database of maps and satellite and environment model (like Streetview) and combines it with streaming data from GPS, cameras, and 3D sensors to monitor its own position within centimetres, and with past and current traffic patterns to avoid collisions. Each car can learn something about environments, roads, or driving, or conditions, and it sends the information to the Google cloud, where it can be used to improve the performance of other cars.\n\nCloud medical robots: a medical cloud (also called a healthcare cluster) consists of various services such as a disease archive, electronic medical records, a patient health management system, practice services, analytics services, clinic solutions, expert systems, etc. A robot can connect to the cloud to provide clinical service to patients, as well as deliver assistance to doctors (e.g. a co-surgery robot). Moreover, it also provides a collaboration service by sharing information between doctors and care givers about clinical treatment.\nAssistive robots: A domestic robot can be employed for healthcare and life monitoring for elderly people. The system collects the health status of users and exchange information with cloud expert system or doctors to facilitate elderly peoples life, especially for those with chronic diseases. For example, the robots are able to provide support to prevent the elderly from falling down, emergency healthy support such as heart disease, blooding disease. Care givers of elderly people can also get notification when in emergency from the robot through network.\n\nIndustrial robots: As highlighted by the Germany Industry 4.0 Plan \"Industry is on the threshold of the fourth industrial revolution. Driven by the Internet, the real and virtual worlds are growing closer and closer together to form the Internet of Things. Industrial production of the future will be characterised by the strong individualisation of products under the conditions of highly flexible (large series) production, the extensive integration of customers and business partners in business and value-added processes, and the linking of production and high-quality services leading to so-called hybrid products.\" In manufacturing, such cloud based robot systems could learn to handle tasks such as threading wires or cables, or aligning gaskets from professional knowledge base. A group of robots can share information for some collaborative tasks. Even more, a consumer is able to order customised product to manufacturing robots directly with online order system. Another potential paradigm is shopping-delivery robot system- once an order is placed, a warehouse robot dispatches the item to an autonomous car or autonomous drone to delivery it to its recipient (see Figure Cloud Self-driving Car ).\n\nRoboEarth was funded by the European Union's Seventh Framework Programme for research, technological development projects, specifically to explore the field of cloud robotics. The goal of RoboEarth is to allow robotic systems to benefit from the experience of other robots, paving the way for rapid advances in machine cognition and behaviour, and ultimately, for more subtle and sophisticated human-machine interaction. RoboEarth offers a Cloud Robotics infrastructure. RoboEarth’s World-Wide-Web style database stores knowledge generated by humans – and robots – in a machine-readable format. Data stored in the RoboEarth knowledge base include software components, maps for navigation (e.g., object locations, world models), task knowledge (e.g., action recipes, manipulation strategies), and object recognition models (e.g., images, object models). The RoboEarth Cloud Engine includes support for mobile robots, autonomous vehicles, and drones, which require lots of computation for navigation.\n\nRapyuta is an open source cloud robotics framework based on RoboEarth Engine developed by the robotics researcher at ETHZ. Within the framework, each robot connected to Rapyuta can have a secured computing environment (rectangular boxes) giving them the ability to move their heavy computation into the cloud. In addition, the computing environments are tightly interconnected with each other and have a high bandwidth connection to the RoboEarth knowledge repository.\n\nKnowRob is an extensional project of RoboEarth. It is a knowledge processing system that combines knowledge representation and reasoning methods with techniques for acquiring knowledge and for grounding the knowledge in a physical system and can serve as a common semantic framework for integrating information from different sources.\n\nRoboBrain is a large-scale computational system that learns from publicly available Internet resources, computer simulations, and real-life robot trials. It accumulates everything robotics into a comprehensive and interconnected knowledge base. Applications include prototyping for robotics research, household robots, and self-driving cars. The goal is as direct as the project's name—to create a centralised, always-online brain for robots to tap into. The project is dominated by Stanford University and Cornel University. And the project is supported by the National Science Foundation, the Office of Naval Research, the Army Research Office, Google, Microsoft, Qualcomm, the Alfred P. Sloan Foundation and the National Robotics Initiative, whose goal is to advance robotics to help make the United States more competitive in the world economy.\n\nMyRobots is a service for connecting robots and intelligent devices to the Internet. It can be regarded as a social network for robots and smart objects (i.e. Facebook for robots). With socialising, collaborating and sharing, robots can benefit from those interactions too by sharing their sensor information giving insight on their perspective of their current state.\n\nCOALAS is funded by the INTERREG IVA France (Channel) – England European cross-border co-operation programme. The project aims to develop new technologies for handicapped people through social and technological innovation and through the users' social and psychological integrity. Objectives is to produce a cognitive ambient assistive living system with Healthcare cluster in cloud with domestic service robots like humanoid, intelligent wheelchair which connect with the cloud.\n\nROS (Robot Operating System) provides an eco-system to support cloud robotics. ROS is a flexible and distributed framework for robot software development. It is a collection of tools, libraries, and conventions that aim to simplify the task of creating complex and robust robot behaviour across a wide variety of robotic platforms. A library for ROS that is a pure Java implementation, called rosjava, allows Android applications to be developed for robots. Since Android has a booming market and billion users, it would be significant in the field of Cloud Robotics.\n\nC2RO (C2RO Cloud Robotics) is a platform that processes real-time applications such as collision avoidance and object recognition in the cloud. Previously, high latency times prevented these applications from being processed in the cloud thus requiring on-system computational hardware (e.g. Graphics Processing Unit or GPU). C2RO published a peer-reviewed paper at IEEE PIMRC17 showing its platform could make autonomous navigation and other AI services available on robots- even those with limited computational hardware (e.g. a Raspberry Pi)- from the cloud. C2RO eventually claimed to be the first platform to demonstrate cloud-based SLAM (simultaneous localization and mapping) at RoboBusiness in September 2017.\n\nThough robots can benefit from various advantages of cloud computing, cloud is not the solution to all of robotics.\n\n\nThe research and development of cloud robotics has following potential issues and challenges:\n\n\nEnvironmental security - The concentration of computing resources and users in a cloud computing environment also represents a concentration of security threats. Because of their size and significance, cloud environments are often targeted by virtual machines and bot malware, brute force attacks, and other attacks.\n\nData privacy and security - Hosting confidential data with cloud service providers involves the transfer of a considerable amount of an organisation's control over data security to the provider. For example, every cloud contains a huge information from the clients include personal data. If a household robot is hacked, users could have risk of their personal privacy and security, like house layout, life snapshot, home-view, etc. It may be accessed and leaked to the world around by criminals. Another problems is once a robot is hacked and controlled by someone else, which may put the user in danger.\n\nEthical problems - Some ethics of robotics, especially for cloud based robotics must be considered. Since a robot is connected via networks, it has risk to be accessed by other people. If a robot is out of control and carries out illegal activities, who should be responsible for it.\n\nSpecial Issue on Cloud Robotics and Automation- A special issue of the IEEE Transactions on Automation Science and Engineering, April 2015.\n\nCloud Robotics-Enable cloud computing for robots. The author proposed some paradigms of using cloud computing in robotics. Some potential field and challenges were coined. R. Li 2014.\n\n2013 IEEE IROS Workshop on Cloud Robotics. Tokyo. November 2013.\n\nNRI Workshop on Cloud Robotics: Challenges and Opportunities- February 2013.\n\nRobot APP Store Robot Applications in Cloud, provide applications for robot just like computer/phone app.\n\nDARPA Cloud Robotics.\n\nA Roadmap for U.S. Robotics From Internet to Robotics 2013 Edition- by Georgia Institute of Technology, Carnegie Mellon University Robotics Technology Consortium, University of Pennsylvania, University of Southern California, Stanford University, University of California–Berkeley, University of Washington, Massachusetts Institute of TechnologyUS and Robotics OA US. The Roadmap highlighted “Cloud” Robotics and Automation for Manufacturing in the future years.\n\nCloud-Based Robot Grasping with the Google Object Recognition Engine.\n\nNational Robotics Initiative of US announced in 2011 aimed to explore how robots can enhance the work of humans rather than replacing them. It claims that next generation of robots are more aware than oblivious, more social than solitary.\n\nJames J. Kuffner, a former CMU robotics professor, and research scientist at Google, now CTO of Toyota Research Institute, spoke on cloud robotics in IEEE/RAS International Conference on Humanoid Robotics 2010. It describes \"a new approach to robotics that takes advantage of the Internet as a resource for massively parallel computation and sharing of vast data resources.\"\n\nRyan Hickman, a Google Product Manager, led an internal volunteer effort in 2010 to connect robots with the Google's cloud services.This work was later expanded to include open source ROS support and was demonstrated on stage by Ryan Hickman, Damon Kohler, Brian Gerkey, and Ken Conley at Google I/O 2011.\n\nThe IEEE RAS Technical Committee on Internet and Online Robots was founded by Ken Goldberg and Roland Siegwart et al. in May 2001. The committee then expanded to IEEE Society of Robotics and Automation's Technical Committee on Networked Robots in 2004.\n\nThe first industrial cloud robotics platform, Tend, was founded by Mark Silliman, James Gentes and Robert Kieffer in February 2017. Tend allows robots to be remotely controlled and monitored via websockets and NodeJs.\n\n\n",
    "id": "44628427",
    "title": "Cloud robotics"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=47580669",
    "text": "Aurora (novel)\n\nAurora is a 2015 novel by American science fiction author Kim Stanley Robinson. The novel concerns a generation ship traveling to Tau Ceti in order to begin a human colony. The novel's primary narrating voice is the starship's artificial intelligence. The novel was well received by critics.\n\nA generation ship is launched from Saturn in 2545. It includes twenty-four self-contained biomes and an average population of two thousand people. One hundred sixty years and approximately seven generations later, it is beginning its approach to the Tau Ceti system to begin colonization of a planet's moon, an Earth analog, which has been named Aurora.\n\nDevi, the ship's \"de facto\" chief engineer and leader, is concerned about the decaying infrastructure and biology of the ship: systems are breaking down, each generation has lower intelligence test scores than the last, and bacteria are mutating and evolving at a faster rate than humans. She tells the ship's AI, referred to simply as Ship, to keep a narrative of the voyage. After having some trouble with understanding the human concept of narrative, Ship eventually elects to follow the life of Devi's daughter Freya as a protagonist.\n\nAs a teenager, Freya travels around the ship on her \"wanderjahr\" and learns that many of the ship's inhabitants are dissatisfied with their enclosed existence and what they perceive as a dictatorship. Movement is strictly limited for most people, reproduction is tightly controlled, and education in science and mathematics is mandatory. Freya's \"wanderjahr\" comes to an end when she is called home as Devi grows sick from cancer and dies.\n\nThe ship arrives in the Tau Ceti system and begins to settle Aurora, a moon of Tau Ceti e. It soon becomes apparent that extraterrestrial life is present in the form of primitive prions, which infect and kill most of the landing party. The surviving settlers attempt to return to the ship, and some of those remaining onboard kill them in the airlock to maintain quarantine, leading to a violent political schism throughout the ship. The ship itself, which has been moving towards self-awareness, takes physical control of the situation by lowering oxygen levels and separating warring factions, referring to itself as \"the rule of law\". It then reveals to the crew that there were in fact two ships originally launched for the Tau Ceti expedition, but the other was destroyed during a period of severe civil unrest, and the collective memory of that event was erased from the history records. Under Ship's moderation, a more peaceful debate takes place between the inhabitants about what to do now that Aurora is known to be inhospitable. Unable to reach consensus, the factions agree to part ways, with those who wish to stay retaining as many resources as can be spared to pursue an unlikely attempt at terraforming the Mars-like planet Iris, while the other group, led by Freya, opt to try and return to Earth. Freya and the others who return do not receive any communication from those who remained in the Tau Ceti system.\n\nOn the voyage back to Earth, the ship's biomes continue to deteriorate as bacteria flourish and crops fail. The humans soon face famine and experiment with an untested form of cryogenic freezing, which is largely successful. Upon returning to the Solar system, Ship is forced to decelerate by means of gravity assist between various planets, a process which takes twelve years. During this time, with the full communications data of humanity available to it, it learns more about why it was launched in the first place—simply for expansionism—and denounces its builders as \"criminally negligent narcissists\". Ship manages to safely drop its humans off on a pass of Earth but fails to make a final gravity slowdown past the Sun. Ship is destroyed along with the last survivor of the landing on Aurora.\n\nFreya and the other \"starfarers\" have trouble adjusting to life on Earth, especially with many Terrans hostile to them for a perceived sense of ingratitude and cowardice. At a space colonization conference, a speaker says humanity will continue to send ships into interstellar space no matter how many fail and die, and Freya assaults him. Eventually she joins a group of terraformers who are attempting to restore the Earth's beaches after their loss during previous centuries' sea level rise. While swimming and surfing, she begins to come to terms with life on Earth.\n\nMajor themes in \"Aurora\" include complexities of life aboard a multi-generational starship, interpersonal psychology, artificial intelligence, human migration, and the feasibility of star travel.\n\nRobinson says that in researching the novel he met with his friend Christopher McKay who has helped him since the \"Mars Trilogy\". McKay arranged lunches at the NASA Ames Research Center where Robinson asked questions of NASA employees.\n",
    "id": "47580669",
    "title": "Aurora (novel)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=43097038",
    "text": "Behavior tree (artificial intelligence, robotics and control)\n\nA Behavior Tree (BT) is a mathematical model of plan execution used in computer science, robotics, control systems and video games. They describe switchings between a finite set of tasks in a modular fashion. Their strength comes from their ability to create very complex tasks composed of simple tasks, without worrying how the simple tasks are implemented. BTs present some similarities to hierarchical state machines with the key difference that the main building block of a behavior is a task rather than a state. Its ease of human understanding make BTs less error prone and very popular in the game developer community. BTs have shown to generalize several other control architectures. \n\nBTs originates from the computer game industry as a powerful tool to model the behavior of non-player characters (NPCs).\nThey have been extensively used in high-profile video games such as Halo, Bioshock, and Spore. Recent works propose BTs as a multi-mission control framework for UAV, complex robots, robotic manipulation, and multi-robot systems.\nBT have now reached the maturity to be treated in Game AI textbooks \nas well as generic game environments such as PyGame and Unreal Engine (see links below).\n\nIt has been shown that BTs generalize a number of earlier control architectures, such as\n\nBT is graphically represented as a directed tree in which the nodes are classified as root, control flow nodes, or execution nodes (tasks). For each pair of connected nodes the outgoing node is called parent and the incoming node is called child. The root has no parents and exactly one child, the control flow nodes have one parent and at least one child, and the execution nodes have one parent and no children. Graphically, the children of a control flow node are placed below it, ordered from left to right.\n\nThe execution of a BT starts from the root which sends ticks with a certain frequency to its child. A tick is an enabling signal that allows the execution of a child. When the execution of a node in the BT is allowed, it returns to the parent a status running if its execution has not finished yet, success if it has achieved its goal, or failure otherwise.\n\nA control flow node is used to control the subtasks of which it is composed. A control flow node may be either a selector (fallback) node or a sequence node. They run each of their subtasks in turn. When a subtask is completed and returns its status (success or failure), the control flow node decides whether to execute the next subtask or not.\n\nFallback nodes are used to find and execute the first child that does not fail. A fallback node will return immediately with a status code of success or running when one of its children returns success or running (see Figure I and the pseudocode below). The children are ticked in order of importance, from left to right.\n\nIn pseudocode, the algorithm for a fallback composition is:\n\nSequence nodes are used to find and execute the first child that has not yet succeeded. A sequence node will return immediately with a status code of failure or running when one of its children returns failure or running (see Figure II and the pseudocode below). The children are ticked in order, from left to right.\n\nIn pseudocode, the algorithm for a sequence composition is:\n\nIn order to apply control theory tools to the analysis of Behavior Trees,\nBT can be defined as three-tuple.\n\nformula_1\n\nwhere formula_2 is the index of the tree, formula_3 is a vector field representing the right hand side of an ordinary difference equation, formula_4 is a time step and \nformula_5 is the return status, that can be equal to either \nRunning formula_6,\nSuccess formula_7, or\nFailure formula_8.\n\nNote: A task is degenerate BT with no parent and no child.\n\nThe execution of a BT is described by the following standard ordinary difference equations:\n\nformula_9\n\nformula_10\n\nwhere formula_11 represent the discrete time, and formula_12 is the state space of the system modelled by the behavior tree.\n\nTwo BTs formula_13 and formula_14 can be composed into a more complex BT formula_15 using a Fallback operator.\n\nformula_16\n\nThen return status formula_17 and the vector field formula_18 associated with formula_15 are defined as follows:\n\nformula_20\n\nformula_21\n\nTwo BTs formula_13 and formula_14 can be composed into a more complex BT formula_15 using a Sequence operator.\n\nformula_25\n\nThen return status formula_17 and the vector field formula_18 associated with formula_15 are defined as follows:\n\nformula_29\n\nformula_30\n\n\n\n",
    "id": "43097038",
    "title": "Behavior tree (artificial intelligence, robotics and control)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=9583985",
    "text": "Committee machine\n\nA committee machine is a type of artificial neural network using a divide and conquer strategy in which the responses of multiple neural networks (experts) are combined into a single response. The combined response of the committee machine is supposed to be superior to those of its constituent experts. Compare with ensembles of classifiers.\n\nIn this class of committee machines, the responses of several predictors (experts) are combined by means of a mechanism that does not involve the input signal, hence the designation static. This category includes the following methods:\nIn ensemble averaging, outputs of different predictors are linearly combined to produce an overall output.\nIn boosting, a weak algorithm is converted into one that achieves arbitrarily high accuracy.\n\nIn this second class of committee machines, the input signal is directly involved in actuating the mechanism that integrates the outputs of the individual experts into an overall output, hence the designation dynamic. There are two kinds of dynamic structures:\nIn mixture of experts, the individual responses of the experts are non-linearly combined by means of a single gating network.\nIn hierarchical mixture of experts, the individual responses of the individual experts are non-linearly combined by means of several gating networks arranged in a hierarchical fashion.\n",
    "id": "9583985",
    "title": "Committee machine"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=25340765",
    "text": "Emily Howell\n\nEmily Howell is a computer program created by David Cope, a music professor at the University of California, Santa Cruz. Emily Howell is an interactive interface that \"hears\" feedback from listeners, and builds its own musical compositions from a source database, derived from a previous composing program called Experiments in Musical Intelligence (EMI). Cope attempts to “teach” the program by providing feedback so that it can cultivate its own \"personal\" style. The software appears to be based on latent semantic analysis.\n\nEmily Howell’s first album was released in February 2009 by Centaur Records (CRC 3023). Titled \"From Darkness, Light\", this album contains her Opus 1, Opus 2, and Opus 3 compositions for chamber orchestra and multiple pianos. Her second album \"Breathless\" was released in December 2012 by Centaur Records (CRC 3255).\n\n\n\n",
    "id": "25340765",
    "title": "Emily Howell"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=21659435",
    "text": "Music and artificial intelligence\n\nResearch in artificial intelligence (AI) is known to have impacted medical diagnosis, stock trading, robot control, and several other fields. Perhaps less popular is the contribution of AI in the field of music. Nevertheless, artificial intelligence and music (AIM) has, for a long time, been a common subject in several conferences and workshops, including the International Computer Music Conference, the Computing Society Conference and the International Joint Conference on Artificial Intelligence. In fact, the first International Computer Music Conference was the ICMC 1974, Michigan State University, East Lansing, USA \nCurrent research includes the application of AI in music composition, performance, theory and digital sound processing. Several music software applications have been developed that use AI to produce music. A few examples are included below. Note that there are many that are still being developed.\n\nIn 1960, Russian researcher R.Kh.Zaripov published worldwide first paper on algorithmic music composing using the \"Ural-1\" computer.\n\nIn 1965, inventor Ray Kurzweil premiered a piano piece created by a computer that was capable of pattern recognition in various compositions. The computer was then able to analyze and use these patterns to create novel melodies. The computer was debuted on Steve Allen's I've Got a Secret program, and stumped the hosts until film star Henry Morgan guessed Ray's secret.\n\nA program developed by Hexachords and directed by Richard Portelli, mainly focused on orchestral music.\n\nA program developed by David Cope which composes classical music. See Experiments in Musical Intelligence. Emily Howell is an interactive augmentation of EMI. (As a popular example, the background music of the viral video Humans Need Not Apply was created by \"her\", as revealed in the video to illustrate the likely fate of creative jobs.)\n\nThis program was designed to provide small-budget productions with instrumentation for all instruments usually present in the full-fledged orchestra. If there is a small orchestra playing, the program can play the part for missing instruments. High school and community theaters wanting to produce a musical can now benefit from the virtual orchestra and realize a full Broadway score. This software is able to follow the fluctuations in tempo and musical expression. Musicians enjoy the thrill of playing with a full orchestra, while the audience enjoys the rich sound that comes from the combination of the virtual orchestra with the musicians.\n\nDemo:\n\nThe Computer Music Project at CMU develops computer music and interactive performance technology to enhance human musical experience and creativity. This interdisciplinary effort draws on Music Theory, Cognitive Science, Artificial Intelligence and Machine Learning, Human Computer Interaction, Real-Time Systems, Computer Graphics and Animation, Multimedia, Programming Languages, and Signal Processing. One of their project is similar to SmartMusic. It provides accompaniment for the chosen piece follows the soloist (user) despite tempo changes and/or mistakes.\n\nDemo:\n\nSmartMusic is an interactive, computer-based practice tool for musicians. It offers exercises, instant feedback tools, and accompaniments meant to aid musicians. The product is targeted at teachers and students alike and offers five categories of accompaniments: solo, skill development, method books, jazz, and ensemble. Teachers can give students pre-defined assignments via email and scan in sheet music that is not yet in the SmartMusic catalog. Students can choose the difficulty level they want to play at, slow down or speed up the tempo, or change the key in which to play the piece. SmartMusic also compares students' playing with digital template, which allows it to detect mistakes and mark them on a score. It also simulates the rapport between musicians by sensing and reacting to tempo changes.\n\nStarPlay is also a music education software that allows the user to practice by performing with professional musicians, bands and orchestras. They can choose their spot and watch the video from that spot. They can hear the other musicians playing. Again, the program listens to the user's performance and helps them improve their performance by providing constructive feedback as they rehearse. StarPlay was developed by StarPlayIt (formerly In The Chair), a music technology company that has won many awards for its platforms for online musical performance and participation.\n\nDeveloped at Princeton University by Ge Wang and Perry Cook, ChucK is a text-based, cross-platform language that allows real-time synthesis, composition, performance and analysis of music.\n. It is used by SLOrk (Stanford Laptop Orchestra) and PLOrk (Princeton Laptop Orchestra).\n\nDemo:\n\nThe Impromptu media programming environment was developed by Andrew Sorensen for exploring 'intelligent' interactive music and visual systems. Impromptu is used for live coding performances and research including generative orchestral music and computational models of music perception.\n\nMIDI to string instrument (guitar, violin, dombra, etc.) tablature conversion is a nontrivial task, as the same note can reside on different strings of the instrument. And the creation of good fingering is sometimes a challenge even for real musicians, especially when translating a two handed piano composition on a string instrument.\nSo in TabEditor (the tiny plugin for REAPER DAW), an AI was used that solves this puzzle the same way as a musician would: trying to keep all the notes close to each other (to be possible to play) while trying to fit all the piano notes into a range that can be played simultaneously on the instrument. When direct translation is impossible (piano part has more notes than are possible on the guitar) the AI tries to find an acceptable solution, removing as few notes as possible from the original composition.\nThe Prolog programming language was used to create this AI.\n\nLudwig is an automated composition software based on tree search algorithms. Ludwig generates melodies according to principles of classical music theory. The software arranges its melodies with pop-automation patterns or in four-part choral writing. Ludwig can react in real-time on an eight-bar theme played on a keyboard. The theme will be analysed for key, harmonic content and rhythm while it is being performed by a human. The program then without delay repeats the theme arranged e.g. for orchestra. It subsequently varies the melody to create a little piece as interactive answer to the human input.\n\nOMax is a software environment which learns in real-time typical features of a musician's style and plays along with him interactively, giving the flavor of a machine co-improvisation. OMax uses OpenMusic and Max. It is based on researches on stylistic modeling carried out by Gerard Assayag and Shlomo Dubnov and on researches on improvisation with the computer by G. Assayag, M. Chemillier and G. Bloch (Aka the OMax Brothers) in the Ircam Music Representations group.\n\nMelomics is a proprietary computational system for the automatic (without human intervention) composition of music, based on bioinspired methods and produced by Melomics Media. Composing a wide variety of genres, all music composed by Melomics algorithms are available in MP3, MIDI, MusicXML, and PDF (of sheet music), after purchase. Music composed by this algorithm was organized into an album named Iamus (album), which was hailed by New Scientist as \"The first complete album to be composed solely by a computer and recorded by human musicians.\"\n\nMorpheuS is a research project by Dorien Herremans and Elaine Chew at Queen Mary University of London, funded by a Marie Skłodowská-Curie EU project. The system uses an optimization approach based on a variable neighborhood search algorithm to morph existing template pieces into novel pieces with a set level of tonal tension that changes dynamically throughout the piece. This optimization approach allows for the integration of a pattern detection technique in order to enforce long term structure and recurring themes in the generated music. Pieces composed by MorpheuS have been performed at concerts in both Stanford and London.\n\nFlow Machines is a research project funded by the European Research Council (ERC) and led by François Pachet. Flow Machines aims at transforming musical style into a computational object to apply to AI-generated melodies and harmonies. Flow Machines has composed two fully-fledged pop songs, issued from a collaboration between the AI software and pop composer Benoît Carré: Daddy's Car and Mister Shadow.\nFlow Machines also produced DeepBach, a neural network system which produces harmonisation in Bach style indiscernible from original Bach's harmonisations.\n\nCreated in February 2016, AIVA specializes in classical and symphonic music composition. It became the world’s first virtual composer to be recognized by a music society (SACEM). By reading a large collection of existing works of classical music (written by human composers such as Bach, Beethoven, Mozart) AIVA is capable of understanding concepts of music theory and composing on its own. The algorithm AIVA is based on deep learning and reinforcement learning architectures\n\n\n\n",
    "id": "21659435",
    "title": "Music and artificial intelligence"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=47332350",
    "text": "DeepDream\n\nDeepDream is a computer vision program created by Google engineer Alexander Mordvintsev which uses a convolutional neural network to find and enhance patterns in images via algorithmic pareidolia, thus creating a dream-like hallucinogenic appearance in the deliberately over-processed images.\n\nGoogle's program popularized the term (deep) \"dreaming\" to refer to the generation of images that produce desired activations in a trained deep network, and the term now refers to a collection of related approaches.\n\nThe DeepDream software originates in a deep convolutional network codenamed \"Inception\" after the film of the same name, was developed for the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) in 2014 and released in July 2015.\n\nThe software is designed to detect faces and other patterns in images, with the aim of automatically classifying images. However, once trained, the network can also be run in reverse, being asked to adjust the original image slightly so that a given output neuron (e.g. the one for faces or certain animals) yields a higher confidence score. This can be used for visualizations to understand the emergent structure of the neural network better, and is the basis for the DeepDream concept. However, after enough reiterations, even imagery initially devoid of the sought features will be adjusted enough that a form of pareidolia results, by which psychedelic and surreal images are generated algorithmically. The optimization resembles Backpropagation, however instead of adjusting the network weights, the weights are held fixed and the input is adjusted.\n\nFor example, an existing image can be altered so that it is \"more cat-like\", and the resulting enhanced image can be again input to the procedure. This usage resembles the activity of looking for animals or other patterns in clouds.\n\nApplying gradient descent independently to each pixel of the input produces images in which\nadjacent pixels have little relation and thus the image has too much high frequency information.\nThe generated images can be greatly improved by including a prior or regularizer that prefers inputs\nthat have natural image statistics (without a preference for any particular image), or are simply smooth.\nFor example, used the total variation regularizer that prefers images that are piecewise constant. Various regularizers are discussed further in. An in-depth, visual exploration of feature visualization and regularization techniques was published more recently.\n\nThe dreaming idea and name became popular on the internet in 2015 thanks to Google's DeepDream program. The idea dates from early in the history of neural networks, and similar methods have been used to synthesize visual textures.\nRelated visualization ideas were developed (prior to Google's work) by several research groups.\n\nAfter Google published their techniques and made their code open source, a number of tools in the form of web services, mobile applications, and desktop software appeared on the market to enable users to transform their own photos.\n\nThe dreaming idea can be applied to hidden (internal) neurons other than those in the output, \nwhich allows exploration of the roles and representations of various parts of the network.\nIt is also possible to optimize the input to satisfy either a single neuron (this usage is sometimes called Activity Maximization) or an entire layer of neurons.\n\nWhile dreaming is most often used for visualizing networks or producing computer art, it has recently been proposed that adding \"dreamed\" inputs to the training set can improve training times for abstractions in Computer Science.\n\nThe cited resemblance of the imagery to LSD- and psilocybin-induced hallucinations is suggestive of a functional resemblance between artificial neural networks and particular layers of the visual cortex.\n\nDeepDream was used for Foster the People's music video for the song \"Doing it for the money\".\n\n\n",
    "id": "47332350",
    "title": "DeepDream"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=48653319",
    "text": "Artificial intelligence for video surveillance\n\nArtificial intelligence for video surveillance utilizes computer software programs that analyze the images from video surveillance cameras in order to recognize humans, vehicles or objects. Security contractors program the software to define restricted areas within the camera's view (such as a fenced off area, a parking lot but not the sidewalk or public street outside the lot) and program for times of day (such as after the close of business) for the property being protected by the camera surveillance. The artificial intelligence (\"A.I.\") sends an alert if it detects a trespasser breaking the \"rule\" set that no person is allowed in that area during that time of day.\n\nThe A.I. program functions by using machine vision. Machine vision is a series of algorithms, or mathematical procedures, which work like a flow-chart or series of questions to compare the object seen with hundreds of thousands of stored reference images of humans in different postures, angles, positions and movements. The A.I. asks itself if the observed object moves like the reference images, whether it is approximately the same size height relative to width, if it has the characteristic two arms and two legs, if it moves with similar speed, and if it is vertical instead of horizontal. Many other questions are possible, such as the degree to which the object is reflective, the degree to which it is steady or vibrating, and the smoothness with which it moves. Combining all of the values from the various questions, an overall ranking is derived which gives the A.I. the probability that the object is or is not a human. If the value exceeds a limit that is set, then the alert is sent. It is characteristic of such programs that they are self-learning to a degree, learning, for example that humans or vehicles appear bigger in certain portions of the monitored image – those areas near the camera – than in other portions, those being the areas farthest from the camera.\n\nIn addition to the simple rule restricting humans or vehicles from certain areas at certain times of day, more complex rules can be set. The user of the system may wish to know if vehicles drive in one direction but not the other. Users may wish to know that there are more than a certain preset number of people within a particular area. The A.I. is capable of maintaining surveillance of hundreds of cameras simultaneously. Its ability to spot a trespasser in the distance or in rain or glare is superior to humans' ability to do so.\n\nThis type of A.I. for security is known as \"rule-based\" because a human programmer must set rules for all of the things for which the user wishes to be alerted. This is the most prevalent form of A.I. for security. Many video surveillance camera systems today include this type of A.I. capability. The hard-drive that houses the program can either be located in the cameras themselves or can be in a separate device that receives the input from the cameras.\n\nA newer, non-rule based form of A.I. for security called \"behavioral analytics\" has been developed. This software is fully self-learning with no initial programming input by the user or security contractor. In this type of analytics, the A.I. learns what is normal behavior for people, vehicles, machines, and the environment based on its own observation of patterns of various characteristics such as size, speed, reflectivity, color, grouping, vertical or horizontal orientation and so forth. The A.I. normalizes the visual data, meaning that it classifies and tags the objects and patterns it observes, building up continuously refined definitions of what is normal or average behavior for the various observed objects. After several weeks of learning in this fashion it can recognize when things break the pattern. When it observes such anomalies it sends an alert. For example, it is normal for cars to drive in the street. A car seen driving up onto a sidewalk would be an anomaly. If a fenced yard is normally empty at night, then a person entering that area would be an anomaly.\n\nLimitations in the ability of humans to vigilantly monitor video surveillance live footage led to the demand for artificial intelligence that could better serve the task. Humans watching a single video monitor for more than twenty minutes lose 95% of their ability to maintain attention sufficient to discern significant events. With two monitors this is cut in half again. Given that many facilities have dozens or even hundreds of cameras, the task is clearly beyond human ability. In general, the camera views of empty hallways, storage facilities, parking lots or structures are exceedingly boring and thus attention is quickly attenuated. When multiple cameras are monitored, typically employing a wall monitor or bank of monitors with split screen views and rotating every several seconds between one set of cameras and the next, the visual tedium is quickly overwhelming. While video surveillance cameras proliferated with great adoption by users ranging from car dealerships and shopping plazas to schools and businesses to highly secured facilities such as nuclear plants, it was recognized in hindsight that video surveillance by human officers (also called \"operators\") was impractical and ineffective. Extensive video surveillance systems were relegated to merely recording for possible forensic use to identify someone, after the fact of a theft, arson, attack or incident. Where wide angle camera views were employed, particularly for large outdoor areas, severe limitations were discovered even for this purpose due to insufficient resolution. In these cases it is impossible to identify the trespasser or perpetrator because their image is too tiny on the monitor.\n\nIn response to the shortcomings of human guards to watch surveillance monitors long-term, the first solution was to add motion detectors to cameras. It was reasoned that an intruder's or perpetrator's motion would send an alert to the remote monitoring officer obviating the need for constant human vigilance. The problem was that in an outdoor environment there is constant motion or changes of pixels that comprise the total viewed image on screen. The motion of leaves on trees blowing in the wind, litter along the ground, insects, birds, dogs, shadows, headlights, sunbeams and so forth all comprise motion. This caused hundreds or even thousands of false alerts per day, rendering this solution inoperable except in indoor environments during times of non-operating hours.\n\nThe next evolution reduced false alerts to a degree but at the cost of complicated and time-consuming manual calibration. Here, changes of a target such as a person or vehicle relative to a fixed background are detected. Where the background changes seasonally or due to other changes, the reliability deteriorates over time. The economics of responding to too many false alerts again proved to be an obstacle and this solution was insufficient.\n\nMahesh Saptharishi did his doctoral work in machine learning. Machine learning of visual recognition relates to patterns and their classification. Recognizing that advances in machine vision to aid robots in interpreting visual data much as humans do could be applied to the security industry requisite of surveillance, he pioneered the development of video analytics, co-founding Broad Reach Technologies in 2000. Following the events of 9/11, the federal government sought a real-time solution to detect intruders. General Electric provided investment to Broad Reach in R&D in 2002. Saptharishi continued his work and co-founded VideoIQ, Inc. in 2007, making the software commercially available to the general market. The company was bought by the large security company Avigilon in 2014, with Saptharishi becoming Chief Technology Officer for the company. True video analytics can distinguish the human form, vehicles and boats or selected objects from the general movement of all other objects and visual static or changes in pixels on the monitor. It does this by recognizing patterns. When the object of interest, for example a human, violates a preset rule, for example that the number of people shall not exceed zero in a pre-defined area during a defined time interval, then an alert is sent. A red rectangle or so-called \"bounding box\" will typically automatically follow the detected intruder, and a short video clip of this is sent as the alert.\n\nThe detection of intruders using video surveillance has limitations based on economics and the nature of video cameras. Typically, cameras outdoors are set to a wide angle view and yet look out over a long distance. Frame rate per second and dynamic range to handle brightly lit areas and dimly lit ones further challenge the camera to actually be adequate to see a moving human intruder. At night, even in illuminated outdoor areas, a moving subject does not gather enough light per frame per second and so, unless quite close to the camera, will appear as a thin wisp or barely discernible ghost or completely invisible. Conditions of glare, partial obscuration, rain, snow, fog, and darkness all compound the problem. Even when a human is directed to look at the actual location on a monitor of a subject in these conditions, the subject will usually not be detected. The A.I. is able to impartially look at the entire image and all cameras' images simultaneously. Using statistical models of degrees of deviation from its learned pattern of what constitutes the human form it will detect an intruder with high reliability and a low false alert rate even in adverse conditions. Its learning is based on approximately a quarter million images of humans in various positions, angles, postures, and so forth.\n\nThe one megapixel VideoIQ camera with the onboard video analytics was able to detect a human at a distance of about 350' and an angle of view of about 30 degrees in non-ideal conditions. Rules could be set for a \"virtual fence\" or intrusion into a pre-defined area. Rules could be set for directional travel, object left behind, crowd formation and some other conditions.\n\nOne of the most powerful features of the system is that a human officer or operator, receiving an alert from the A.I., could immediately talk down over outdoor public address loudspeakers to the intruder. This had high deterrence value as most crimes are opportunistic and the risk of capture to the intruder becomes so pronounced when a live person is talking to them that they are very likely to desist from intrusion and to retreat. The security officer would describe the actions of the intruder so that the intruder had no doubt that a real person was watching them. The officer would announce that the intruder was breaking the law and that law enforcement was being contacted and that they were being video-recorded.\n\nThe police receive a tremendous number of false alarms from burglar alarms. In fact the security industry reports that over 98% of such alarms are false ones. Accordingly, the police give very low priority response to burglar alarms and can take from twenty minutes to two hours to respond to the site. By contrast, the video analytic-detected crime is reported to the central monitoring officer, who verifies with his or her own eyes that it is a real crime in progress. He or she then dispatches to the police who give such calls their highest priority.\n\nAnalytics work with digital cameras or analog cameras that have analog-to-digital converters. Various companies aside from VideoIQ and Avigilon offer video analytics. In many cases the software samples down to standard definition or less, regardless of the camera's resolution. The advance achieved by Avigilon is true analytics on up to exceedingly high-resolution 30 megapixel cameras.\n\nWhile rule-based video analytics worked economically and reliably for many security applications there are many situations in which it cannot work. For an indoor or outdoor area where no one belongs during certain times of day, for example overnight, or for areas where no one belongs at any time such as a cell tower, traditional rule-based analytics are perfectly appropriate. In the example of a cell tower the rare time that a service technician may need to access the area would simply require calling in with a pass-code to put the monitoring response \"on test\" or inactivated for the brief time the authorized person was there.\n\nBut there are many security needs in active environments in which hundreds or thousands of people belong all over the place all the time. For example, a college campus, an active factory, a hospital or any active operating facility. It is not possible to set rules that would discriminate between legitimate people and criminals or wrong-doers.\n\nDeveloped by the team led by Dr. Wes Cobb of Behavioral Recognition Systems, Inc., (BRS Labs) this self-learning, non-rule-based A.I. software takes the data from video cameras and continuously classifies objects and events that it sees. The system is called AISight (as in \"A.I. Sight\"). It has been deployed by government agencies, transit authorities, police departments and is rolling into commercial use. For example, a person crossing a street is one classification. A group of people is another classification. A vehicle is one classification, but with continued learning a public bus would be discriminated from a small truck and that from a motorcycle. With increasing sophistication, the system recognizes patterns in human behavior. For example, it might observe that individuals pass through a controlled access door one at a time. The door opens, the person presents their proximity card or tag, the person passes through and the door closes. This pattern of activity, observed repeatedly, forms a basis for what is normal in the view of the camera observing that scene. Now if an authorized person opens the door but a second \"tail-gating\" unauthorized person grabs the door before it closes and passes through, that is the sort of anomaly that would create an alert. This type of analysis is much more complex than the rule-based analytics. While the rule-based analytics work mainly to detect intruders into areas where no one is normally present at defined times of day, the behavioral analytics works where people are active to detect things that are out of the ordinary.\n\nA fire breaking out outdoors would be an unusual event and would cause an alert, as would a rising cloud of smoke. Vehicles driving the wrong way into a one-way driveway would also typify the type of event that has a strong visual signature and would deviate from the repeatedly observed pattern of vehicles driving the correct one-way in the lane. Someone thrown to the ground by an attacker would be an unusual event that would likely cause an alert. This is situation-specific. So if the camera viewed a gymnasium where wrestling was practiced the A.I. would learn it is usual for one human to throw another to the ground, in which case it would not alert on this observation.\n\nThe A.I. does not know or understand what a human is, or a fire, or a vehicle. It is simply finding characteristics of these things based on their size, shape, color, reflectivity, angle, orientation, motion, and so on. It then finds that the objects it has classified have typical patterns of behavior. For example, humans walk on sidewalks and sometimes on streets but they don't climb up the sides of buildings very often. Vehicles drive on streets but don't drive on sidewalks. Thus the anomalous behavior of someone scaling a building or a vehicle veering onto a sidewalk would trigger an alert.\n\nTypical alarm systems are designed to not miss true positives (real crime events) and to have as low of a false alarm rate as possible. In that regard, burglar alarms miss very few true positives but have a very high false alarm rate even in the controlled indoor environment. Motion detecting cameras miss some true positives but are plagued with overwhelming false alarms in an outdoor environment. Rule-based analytics reliably detect most true positives and have a low rate of false positives but cannot perform in active environments, only in empty ones. Also they are limited to the simple discrimination of whether an intruder is present or not.\n\nSomething as complex or subtle as a fight breaking out or an employee breaking a safety procedure is not possible for a rule based analytics to detect or discriminate. With behavioral analytics, it is. Places where people are moving and working do not present a problem. However, the A.I. may spot many things that appear anomalous but are innocent in nature. For example, if students at a campus walk on a plaza, that will be learned as normal. If a couple of students decided to carry a large sheet outdoors flapping in the wind, that might indeed trigger an alert. The monitoring officer would be alerted to look at his or her monitor and would see that the event is not a threat and would then ignore it. The degree of deviation from norm that triggers an alert can be set so that only the most abnormal things are reported. However, this still constitutes a new way of human and A.I. interaction not typified by the traditional alarm industry mindset. This is because there will be many false alarms that may nevertheless be valuable to send to a human officer who can quickly look and determine if the scene requires a response. In this sense, it is a \"tap on the shoulder\" from the A.I. to have the human look at something.\n\nBecause so many complex things are being processed continuously, the software samples down to the very low resolution of only 1 CIF to conserve computational demand. The 1 CIF resolution means that an object the size of a human will not be detected if the camera utilized is wide angle and the human is more than sixty to eighty feet distant depending on conditions. Larger objects like vehicles or smoke would be detectable at greater distances.\n\nThe utility of artificial intelligence for security does not exist in a vacuum, and its development was not driven by purely academic or scientific study. Rather, it is addressed to real world needs, and hence, economic forces. Its use for non-security applications such as operational efficiency, shopper heat-mapping of display areas (meaning how many people are in a certain area in a retail space), and attendance at classes are developing uses. Humans are not as well qualified as A.I. to compile and recognize patterns consisting of very large data sets requiring simultaneous calculations in multiple remote viewed locations. There is nothing natively human about such awareness. Such multi-tasking has been shown to defocus human attention and performance. A.I.s have the ability to handle such data. For the purposes of security interacting with video cameras they functionally have better visual acuity than humans or the machine approximation to it. For judging subtleties of behaviors or intentions of subjects or degrees of threat, humans remain far superior at the present state of the technology. So the A.I. in security functions to broadly scan beyond human capability and to vet the data to a first level of sorting of relevance and to alert the human officer who then takes over the function of assessment and response.\n\nSecurity in the practical world is economically determined so that the expenditure of preventative security will never typically exceed the perceived cost of the risk to be avoided. Studies have shown that companies typically only spend about one twenty-fifth the amount on security that their actual losses cost them. What by pure economic theory should be an equivalence or homeostasis, thus falls vastly short of it. One theory that explains this is cognitive dissonance, or the ease with which unpleasant things like risk can be shunted from the conscious mind. Nevertheless, security is a major expenditure, and comparison of the costs of different means of security is always foremost amongst security professionals.\n\nAnother reason that future security threats or losses are under-assessed is that often only the direct cost of a potential loss is considered instead of the spectrum of consequential losses that are concomitantly experienced. For example, the vandalism-destruction of a custom production machine in a factory or of a refrigerated tractor trailer would result in a long replacement time during which customers could not be served, resulting in loss of their business. A violent crime will have extensive public relations damage for an employer, beyond the direct liability for failing to protect the employee.\n\nBehavioral analytics uniquely functions beyond simple security and, due to its ability to observe breaches in standard patterns of protocols, it can effectively find unsafe acts of employees that may result in workers comp or public liability incidents. Here too, the assessment of future incidents' costs falls short of the reality. A study by Liberty Mutual Insurance Company showed that the cost to employers is about six times the direct insured cost, since uninsured costs of consequential damages include temporary replacement workers, hiring costs for replacements, training costs, managers' time in reports or court, adverse morale on other workers, and effect on customer and public relations. The potential of A.I. in the form of behavioral analytics to proactively intercept and prevent such incidents is significant.\n\nA case study by VideoIQ at two car dealerships and other collected surveys at a range of industry verticals yielded the approximation that rule-based analytics will protect an area for one-fifth the cost of security officers (guards). An analysis of situational awareness created by non-rule-based behavioral analytics in several university campus scenarios, where the cost of on-site patrolling and command station officers is quite high, showed that a given percentage of increase in situational awareness, i.e., the ability to have significant incidents brought to attention in real-time, could be achieved by adding the A.I. software to existing video camera infrastructure for anywhere from one-twentieth the cost to one-fiftieth the cost of adding more officers to achieve the same result.\n\n",
    "id": "48653319",
    "title": "Artificial intelligence for video surveillance"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=48760514",
    "text": "Roborace\n\nRoborace will be a motorsport championship with autonomously driving, electrically powered vehicles. The series will be held on the same tracks the FIA Formula E Championship uses. It will be the first global championship for driverless cars. As of September 2017, the official CEO is 2016–17 Formula E champion, Lucas Di Grassi.\n\nThe first race was intended to take place during the 2016–17 Formula E season. Ten teams, each with two driverless cars, will compete in one-hour races over the full season. All teams will have equal cars, but will have to develop their own real-time computing algorithms and artificial intelligence technologies.\n\nFor the first season, all teams will be supplied with an electric racing car built by \"Kinetik\", called the \"Robocar\". The chassis was designed by Daniel Simon, who previously worked on vehicles for movies such as \"\" and \"Oblivion\", along with painting the 2011 HRT Formula One Car. Michelin will be the official tyre supplier, with the internal computing units (Drive PX 2) being provided by Nvidia.\n\nThe chassis itself is shaped similar to a teardrop, improving aerodynamic efficiency. The Car weighs around 1000kg and is 4.8m(~15.7ft) long and 2m(~6.6ft) wide. It has four electric motors, each with a power of 300kW and possesses a 540kWh battery. For navigation, it relies on a mixture of optical systems, radars, lidars and ultrasonic sensors. During the first Roborace season torque vectoring will be allowed. A small rear wing at the rear end of the car indicates one or more powerful electric motors. Kinetik expects the cars to be capable of reaching top speeds of more than .\n\nDevelopment of the Robocar started in early 2016, with a first outing of a test vehicle, the so-called \"DevBot\", following in the summer of the same year. The test car consisted of the same internal units (battery, motor, electronics) used in the Robocar, but were placed in the chassis of a LMP3 Ginetta without an engine cover in order to provide better cooling.\n\nDevBot saw its first public outing at the Formula E pre-season tests in Donington Park in August 2016. After battery issues in Hong Kong caused the development team to abandon their demonstration run, the DevBot successfully drove twelve laps around the Moulay El Hassan Formula E circuit in Marrakesh.\n\nDuring testing ahead of the 2017 Buenos Aires ePrix, two Devbot cars raced against each other. One successfully avoided a dog that ran onto the course. The other car crashed on a corner.\n\nOther test tracks included Michelin's testing ground in Ladoux and the Silverstone Stowe Circuit. Further test runs are planned for the following Formula E events.\n\nTen teams will feature two cars each on the 20-car grid in the inaugural season of Roborace. At the moment, it is not known which teams will join the series. At least one of the ten squads, however, is set to be a \"Crowd Sourced Community\" team, backed by private investors and open for AI developers from around the globe.\n",
    "id": "48760514",
    "title": "Roborace"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=48791499",
    "text": "Dynamic epistemic logic\n\nDynamic epistemic logic (DEL) is a logical framework dealing with knowledge and information change\".\" Typically, DEL focuses on situations involving multiple agents and studies how their knowledge changes when events occur. These events can change factual properties of the actual world (they are called \"ontic events\"): for example a red card is painted in blue. They can also bring about changes of knowledge without changing factual properties of the world (they are called \"epistemic events\"): for example a card is revealed publicly (or privately) to be red. Originally, DEL focused on epistemic events. We only present in this entry some of the basic ideas of the original DEL framework; more details about DEL in general can be found in the references.\n\nDue to the nature of its object of study and its abstract approach, DEL is related and has applications to numerous research areas, such as computer science (artificial intelligence), philosophy (formal epistemology), economics (game theory) and cognitive science. In computer science, DEL is for example very much related to multi-agent systems, which are systems where multiple intelligent agents interact and exchange information.\n\nAs a combination of dynamic logic and epistemic logic, dynamic epistemic logic is a young field of research. It really started in 1989 with Plaza’s logic of public announcement. Independently, Gerbrandy and Groeneveld proposed a system dealing moreover with private announcement and that was inspired by the work of Veltman. Another system was proposed by van Ditmarsch whose main inspiration was the Cluedo game. But the most influential and original system was the system proposed by Baltag, Moss and Solecki. This system can deal with all the types of situations studied in the works above and its underlying methodology is conceptually grounded. We will present in this entry some of its basic ideas.\n\nFormally, DEL extends ordinary epistemic logic by the inclusion of event models to describe actions, and a product update operator that defines how epistemic models are updated as the consequence of executing actions described through event models. Epistemic logic will first be recalled. Then, actions and events will enter into the picture and we will introduce the DEL framework.\n\nEpistemic logic is a modal logic dealing with the notions of knowledge and belief. As a logic, it is concerned with understanding the process of \"reasoning\" about knowledge and belief: which principles relating the notions of knowledge and belief are intuitively plausible? Like epistemology, it stems from the Greek word formula_1 or ‘episteme’ meaning knowledge. Epistemology is nevertheless more concerned with analyzing the very \"nature\" and \"scope\" of knowledge, addressing questions such as “What is the definition of knowledge?” or “How is knowledge acquired?”. In fact, epistemic logic grew out of epistemology in the Middle Ages thanks to the efforts of Burley and Ockham. The formal work, based on modal logic, that inaugurated contemporary research into epistemic logic dates back only to 1962 and is due to Hintikka. It then sparked in the 1960s discussions about the principles of knowledge and belief and many axioms for these notions were proposed and discussed. For example, the interaction axioms formula_2 and formula_3 are often considered to be intuitive principles: if an agent Knows formula_4 then (s)he also Believes formula_4, or if an agent Believes formula_4, then (s)he Knows that (s)he Believes formula_4. More recently, these kinds of philosophical theories were taken up by researchers in economics, artificial intelligence and theoretical computer science where reasoning about knowledge is a central topic. Due to the new setting in which epistemic logic was used, new perspectives and new features such as computability issues were then added to the research agenda of epistemic logic.\n\nIn the sequel, formula_8 is a finite set whose elements are called agents and formula_9 is a set of propositional letters.\n\nThe epistemic language is an extension of the basic multi-modal language of modal logic with a common knowledge operator formula_10 and a distributed knowledge operator formula_11. Formally, the epistemic language formula_12 is defined inductively by the following grammar in BNF:\n\nformula_13\n\nwhere formula_14, formula_15 and formula_16. The basic epistemic language formula_17 is the language formula_18 without the common knowledge and distributed knowledge operators. The formula formula_19 is an abbreviation for formula_20 (for a given formula_14), formula_22 is an abbreviation for formula_23, formula_24 is an abbreviation for formula_25 and formula_26 an abbreviation for formula_27.\n\nGroup notions: general, common and distributed knowledge.\n\nIn a multi-agent setting there are three important epistemic concepts: general knowledge, distributed knowledge and common knowledge. The notion of common knowledge was first studied by Lewis in the context of conventions. It was then applied to distributed systems and to game theory, where it allows to express that the rationality of the players, the rules of the game and the set of players are commonly known.\n\n\"General knowledge.\"\n\nGeneral knowledge of formula_28 means that everybody in the group of agents formula_29 knows that formula_28. Formally, this corresponds to the following formula:\n\nformula_31\n\n\"Common knowledge.\"\n\nCommon knowledge of formula_28 means that everybody knows formula_28 but also that everybody knows that everybody knows formula_28, that everybody knows that everybody knows that everybody knows formula_28, and so on \"ad infinitum\". Formally, this corresponds to the following formula\n\nformula_36\n\nAs we do not allow infinite conjunction the notion of common knowledge will have to be introduced as a primitive in our language.\n\nBefore defining the language with this new operator, we are going to give an example introduced by Lewis that illustrates the difference between the notions of general knowledge and common knowledge. Lewis wanted to know what kind of knowledge is needed so that the statement formula_4: “every driver must drive on the right” be a convention among a group of agents. In other words, he wanted to know what kind of knowledge is needed so that everybody feels safe to drive on the right. Suppose there are only two agents formula_38 and formula_39. Then everybody knowing formula_4 (formally formula_41) is not enough. Indeed, it might still be possible that the agent formula_38 considers possible that the agent formula_39 does not know formula_4 (formally formula_45). In that case the agent formula_38 will not feel safe to drive on the right because he might consider that the agent formula_39, not knowing formula_4, could drive on the left. To avoid this problem, we could then assume that everybody knows that everybody knows that formula_4 (formally formula_50). This is again not enough to ensure that everybody feels safe to drive on the right. Indeed, it might still be possible that agent formula_38 considers possible that agent formula_39 considers possible that agent formula_38 does not know formula_4 (formally formula_55). In that case and from formula_38’s point of view, formula_39 considers possible that formula_38, not knowing formula_4, will drive on the left. So from formula_38’s point of view, formula_39 might drive on the left as well (by the same argument as above). So formula_38 will not feel safe to drive on the right. Reasoning by induction, Lewis showed that for any formula_63, formula_64 is not enough for the drivers to feel safe to drive on the right. In fact what we need is an infinite conjunction. In other words, we need common knowledge of formula_4: formula_66.\n\n\"Distributed knowledge.\"\n\nDistributed knowledge of formula_28 means that if the agents pulled their knowledge altogether, they would know that formula_28 holds. In other words, the knowledge of formula_28 is \"distributed\" among the agents. The formula formula_70 reads as ‘it is distributed knowledge among the set of agents formula_71 that formula_28 holds’.\n\nEpistemic logic is a modal logic. So, what we call an epistemic model formula_73 is just a Kripke model as defined in modal logic. The set formula_74 is a non-empty set whose elements are called \"possible worlds\" and the \"interpretation\" formula_75 is a function specifying which propositional facts (such as ‘Ann has the red card’) are true in each of these worlds. The \"accessibility relations\" formula_76 are binary relations for each agent formula_77; they are intended to capture the uncertainty of each agent (about the actual world and about the other agents' uncertainty). Intuitively, we have formula_78 when the world formula_79 is compatible with agent formula_39’s information in world formula_81 or, in other words, when agent formula_39 considers that world formula_79 might correspond to the world formula_81 (from this standpoint). We abusively write formula_85 for formula_86 and formula_87 denotes the set of worlds formula_88.\n\nIntuitively, a pointed epistemic model formula_89, where formula_85, represents from an external point of view how the actual world formula_81 is perceived by the agents formula_29.\n\nFor every epistemic model formula_93, every formula_94 and every formula_95, we define formula_96 inductively by the following truth conditions:\n\nwhere formula_97 is the transitive closure of formula_98: we have that formula_99 if, and only if, there are formula_100 and formula_101 such that formula_102 and for all formula_103, formula_104.\n\nDespite the fact that the notion of common belief has to be introduced as a primitive in the language, we can notice that the definition of epistemic models does not have to be modified in order to give truth value to the common knowledge and distributed knowledge operators.\n\nCard Example:\n\nPlayers formula_71, formula_106 and formula_107 (standing for Ann, Bob and Claire) play a card game with three cards: a red one, a green one and a blue one. Each of them has a single card but they do not know the cards of the other players. Ann has the red card, Bob has the green card and Claire has the blue card. This example is depicted in the pointed epistemic model formula_108 represented below. In this example, formula_109 and formula_110. Each world is labelled by the propositional letters which are true in this world and formula_81 corresponds to the actual world. There is an arrow indexed by agent formula_112 from a possible world formula_113 to a possible world formula_79 when formula_115. Reflexive arrows are omitted, which means that for all formula_116 and all formula_117, we have that formula_118.\n\nformula_119 stands for : \"formula_71 has the red card<nowiki>\"</nowiki>\n\nformula_121 stand for: \"formula_107 has the blue card<nowiki>\"</nowiki>\n\nformula_123 stands for: \"formula_106 has the green card<nowiki>\"</nowiki>\n\nand so on...\n\nWhen accessibility relations are equivalence relations (like in this example) and we have that formula_78, we say that agent formula_39 \"cannot distinguish\" world formula_81 from world formula_79 (or world formula_81 is indistinguishable from world formula_79 for agent formula_39). So, for example, formula_132 cannot distinguish the actual world formula_81 from the possible world where formula_106 has the blue card (formula_135), formula_107 has the green card (formula_137) and formula_71 still has the red card (formula_119).\n\nIn particular, the following statements hold:\n\nformula_140\n\n'All the agents know the color of their card'.\n\nformula_141\n\n'formula_71 knows that formula_106 has either the blue or the green card and that formula_107 has either the blue or the green card'.\n\nformula_145\n\n'Everybody knows that formula_71 has either the red, green or blue card and this is even common knowledge among all agents'.\n\nWe use the same notation formula_147 for both knowledge and belief. Hence, depending on the context, formula_148 will either read ‘the agent formula_39 \"K\"nows that formula_28 holds’ or ‘the agent formula_39 \"B\"elieves that formula_28 holds’. A crucial difference is that, unlike knowledge, beliefs can be \"wrong\": the axiom formula_153 holds only for knowledge, but not necessarily for belief. This axiom called axiom T (for Truth) states that if the agent knows a proposition, then this proposition is true. It is often considered to be the hallmark of knowledge and it has not been subjected to any serious attack ever since its introduction in the Theaetetus by Plato.\n\nThe notion of knowledge might comply to some other constraints (or axioms) such as formula_154: if agent formula_39 knows something, she knows that she knows it. These constraints might affect the nature of the accessibility relations formula_156 which may then comply to some extra properties. So, we are now going to define some particular classes of epistemic models that all add some extra constraints on the accessibility relations formula_156. These constraints are matched by particular axioms for the knowledge operator formula_147. Below each property, we give the axiom which \"defines\" the class of epistemic frames that fulfill this property. (formula_159 stands for formula_148 for any formula_77.)\n\nWe discuss the axioms above. Axiom 4 states that if the agent knows a proposition, then she knows that she knows it (this axiom is also known as the “KK-principle”or “KK-thesis”). In epistemology, axiom 4 tends to be accepted by internalists, but not by externalists. Axiom 4 is nevertheless widely accepted by computer scientists (but also by many philosophers, including Plato, Aristotle, Saint Augustine, Spinoza and Shopenhauer, as Hintikka recalls ). A more controversial axiom for the logic of knowledge is axiom 5 for Euclidicity: this axiom states that if the agent does not know a proposition, then she knows that she does not know it. Most philosophers (including Hintikka) have attacked this axiom, since numerous examples from everyday life seem to invalidate it. In general, axiom 5 is invalidated when the agent has mistaken beliefs, which can be due for example to misperceptions, lies or other forms of deception. Axiom B states that it cannot be the case that the agent considers it possible that she knows a false proposition (that is, formula_162). If we assume that axioms T and 4 are valid, then axiom B falls prey to the same attack as the one for axiom 5 since this axiom is derivable. Axiom D states that the agent’s beliefs are consistent. In combination with axiom K (where the knowledge operator is replaced by a belief operator), axiom D is in fact equivalent to a simpler axiom D' which conveys, maybe more explicitly, the fact that the agent’s beliefs cannot be inconsistent: formula_163. The other intricate axioms .2, .3, .3.2 and .4 have been introduced by epistemic logicians such as Lenzen and Kutchera in the 1970s and presented for some of them as key axioms of epistemic logic. They can be characterized in terms of intuitive interaction axioms relating knowledge and beliefs.\n\nThe Hilbert proof system K for the basic modal logic is defined by the following axioms and inference rules: for all formula_77,\n\nThe axioms of an epistemic logic obviously display the way the agents reason. For example, the axiom K together with the rule of inference Nec entail that if I know formula_28 (formula_159) and I know that formula_28 implies formula_168 (formula_169 then I know that formula_168 (formula_171). Stronger constraints can be added. The following proof systems for formula_172 are often used in the literature.\n\nWe define the set of proof systems formula_173.\n\nMoreover, for all formula_174, we define the proof system formula_175 by adding the following axiom schemes and rules of inference to those of formula_176. For all formula_177,\n\nThe relative strength of the proof systems for knowledge is as follows:\n\nformula_178\n\nSo, all the theorems of formula_179 are also theorems of formula_180 and formula_181. Many philosophers claim that in the most general cases, the logic of knowledge is formula_179 or formula_183. Typically, in computer science and in many of the theories developed in artificial intelligence, the logic of belief (\"doxastic\" logic) is taken to be formula_184 and the logic of knowledge (\"epistemic\" logic) is taken to be formula_181, even if formula_181 is only suitable for situations where the agents do not have mistaken beliefs. formula_187 has been propounded by Floridi as the logic of the notion of 'being informed’ which mainly differs from the logic of knowledge by the absence of introspection for the agents.\n\nFor all formula_174, the class of formula_176–models or formula_175–models is the class of epistemic models whose accessibility relations satisfy the properties listed above defined by the axioms of formula_176 or formula_175. Then, for all formula_174, formula_176 is sound and strongly complete for formula_172 w.r.t. the class of formula_176–models, and formula_175 is sound and strongly complete for formula_198 w.r.t. the class of formula_175–models.\n\nThe satisfiability problem for all the logics introduced is decidable. We list below the computational complexity of the satisfiability problem for each of them. Note that it becomes linear in time if there are only finitely many propositional letters in the language. For formula_200, if we restrict to finite nesting, then the satisfiability problem is NP-complete for all the modal logics considered. If we then further restrict the language to having only finitely many primitive propositions, the complexity goes down to linear in time in all cases.\n\nThe computational complexity of the model checking problem is in P in all cases.\n\nDynamic Epistemic Logic (DEL) is a logical framework for modeling epistemic situations involving several agents, and changes that occur to these situations as a result of incoming information or more generally incoming action. The methodology of DEL is such that it splits the task of representing the agents’ beliefs and knowledge into three parts:\n\n\nTypically, an informative event can be a public announcement to all the agents of a formula formula_168: this public announcement and correlative update constitute the dynamic part. However, epistemic events can be much more complex than simple public announcement, including hiding information for some of the agents, cheating, lying, bluffing, \"etc.\" This complexity is dealt with when we introduce the notion of event model. We will first focus on public announcements to get an intuition of the main underlying ideas of DEL.\n\nIn this section, we assume that all events are public. We start by giving a concrete example where DEL can be used, to better understand what is going on. This example is called the muddy children puzzle. Then, we will present a formalization of this puzzle in a logic called Public Announcement Logic (PAL). The muddy children puzzle is one of the most well known puzzles that played a role in the development of DEL. Other significant puzzles include the sum and product puzzle, the Monty Hall dilemma, the Russian cards problem, the two envelopes problem, Moore's paradox, the hangman paradox, \"etc\".\n\nMuddy Children Example:\n\nWe have two children, A and B, both dirty. A can see B but not himself, and B can see A but not herself. Let formula_4 be the proposition stating that A is dirty, and formula_203 be the proposition stating that B is not dirty.\n\nPublic announcement logic (PAL):\n\nWe present the syntax and semantic of Public Announcement Logic (PAL), which combines features of epistemic logic and propositional dynamic logic.\n\nWe define the language formula_215 inductively by the following grammar in BNF:\n\nformula_216\n\nwhere formula_77.\n\nThe language formula_215 is interpreted over epistemic models. The truth conditions for the connectives of the epistemic language are the same as in epistemic logic (see above). The truth condition for the new dynamic action modality formula_219 is defined as follows:\n\nwhere formula_220 with\n\nformula_221,\n\nformula_222 for all formula_223 and\n\nformula_224.\n\nThe formula formula_219 intuitively means that after a truthful announcement of formula_168, formula_28 holds. A public announcement of a proposition formula_168 changes the current epistemic model like in the figure below.\n\nThe proof system formula_229 defined below is sound and strongly complete for formula_215 w.r.t. the class of all pointed epistemic models.\n\nThe axioms Red 1 - Red 4 are called \"reduction axioms\" because they allow to reduce any formula of formula_215 to a provably equivalent formula of formula_17 in formula_229. The formula formula_234 is a theorem provable in formula_229. It states that after a public announcement of formula_203, the agent knows that formula_203 holds.\n\nPAL is decidable, its model checking problem is solvable in polynomial time and its satisfiability problem is PSPACE-complete.\n\nMuddy children puzzle formalized with PAL:\n\nHere are some of the statements that hold in the muddy children puzzle formalized in PAL.\n\nformula_238\n\n'In the initial situation, A is dirty and B is dirty'.\n\nformula_239\n\n'In the initial situation, A does not know whether he is dirty and B neither'.\n\nformula_240\n\n'After the public announcement that at least one of the children A and B is dirty, both of then know that at least one of them is dirty'. However:\n\nformula_241\n\n'After the public announcement that at least one of the children A and B is dirty, they still do not know that they are dirty'. Moreover:\n\nformula_242\n\n'After the successive public announcements that at least one of the children A and B is dirty and that they still do not know whether they are dirty, A and B then both know that they are dirty'.\n\nIn this last statement, we see at work an interesting feature of the update process: a formula is not necessarily true after being announced. That is what we technically call “self-persistence” and this problem arises for epistemic formulas (unlike propositional formulas). One must not confuse the announcement and the update induced by this announcement, which might cancel some of the information encoded in the announcement.\n\nIn this section, we assume that events are not necessarily public and we focus on items 2 and 3 above, namely on how to represent events and on how to update an epistemic model with such a representation of events by means of a product update.\n\nEpistemic models are used to model how agents perceive the actual world. Their perception can also be described in terms of knowledge and beliefs about the world and about the other agents’ beliefs. The insight of the DEL approach is that one can describe how an event is perceived by the agents in a very similar way. Indeed, the agents’ perception of an event can also be described in terms of knowledge and beliefs. For example, the private announcement of formula_71 to formula_106 that her card is red can also be described in terms of knowledge and beliefs: while formula_71 tells formula_106 that her card is red (event formula_247) formula_107 \"believes\" that nothing happens (event formula_249). This leads to define the notion of event model whose definition is very similar to that of an epistemic model.\n\nA pointed event model formula_250 represents how the actual event represented by formula_247 is perceived by the agents. Intuitively, formula_252 means that while the possible event represented by formula_247 is occurring, agent formula_39 considers possible that the possible event represented by formula_249 is actually occurring.\n\nAn event model is a tuple formula_256 where:\n\n\nformula_263 denotes the set formula_264 .We write formula_265 for formula_266, and formula_250 is called a pointed event model (formula_247 often represents the actual event).\n\nCard Example:\n\nLet us resume the card example and assume that players formula_71 and formula_106 show their card to each other. As it turns out, formula_107 noticed that formula_71 showed her card to formula_106 but did not notice that formula_106 did so to formula_71. Players formula_71 and formula_106 know this. This event is represented below in the event model formula_250.\n\nThe possible event formula_247 corresponds to the actual event ‘players formula_71 and formula_106 show their and cards respectively to each other’ (with precondition formula_282), formula_249 stands for the event ‘player formula_71 shows her green card’ (with precondition formula_285) and formula_286 stands for the atomic event ‘player formula_71 shows her red card’ (with precondition formula_119). Players formula_71 and formula_106 show their cards to each other, players formula_71 and formula_106 know this and consider it possible, while player formula_107 considers possible that player formula_71 shows her red card and also considers possible that player formula_71 shows her green card, since he does not know her card. In fact, that is all that player formula_107 considers possible because she did not notice that formula_297 showed her card.\n\nAnother example of event model is given below. This second example corresponds to the event whereby Player formula_71 shows her red card publicly to everybody. Player formula_71 shows her red card, players formula_71, formula_106 and formula_107 ‘know’ it, players formula_71, formula_106 and formula_107 ‘know’ that each of them ‘knows’ it, \"etc.\" In other words, there is \"common knowledge\" among players formula_71, formula_106 and formula_107 that player formula_71 shows her red card.\n\nThe DEL product update is defined below. This update yields a new pointed epistemic model formula_310 representing how the new situation which was previously represented by formula_89 is perceived by the agents after the occurrence of the event represented by formula_250.\n\nLet formula_313 be an epistemic model and let formula_314 be an event model. The product update of formula_93 and formula_316 is the epistemic model formula_317 defined as follows: for all formula_318 and all formula_319,\n\nIf formula_86 and formula_321 are such that formula_322 then formula_323 denotes the pointed epistemic model formula_324. This definition of the product update is conceptually grounded.\n\nCard Example:\n\nAs a result of the first event described above (Players formula_71 and formula_106 show their cards to each other in front of player formula_107), the agents update their beliefs. We get the situation represented in the pointed epistemic model formula_323 below. In this pointed epistemic model, the following statement holds: formula_329 It states that player formula_71 knows that player formula_106 has the card but player formula_107 'believes' that it is not the case.\n\nThe result of the second event is represented below. In this pointed epistemic model, the following statement holds: formula_333. It states that there is common knowledge among formula_106 and formula_107 that they know the true state of the world (namely formula_71 has the red card, formula_106 has the green card and formula_107 has the blue card), but formula_71 does not know it.\n\nBased on these three components (epistemic model, event model and product update), Baltag, Moss and Solecki defined a general logical language inspired from the logical language of propositional dynamic logic to reason about information and knowledge change.\n\n\n\n",
    "id": "48791499",
    "title": "Dynamic epistemic logic"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=12142270",
    "text": "GENESIS (software)\n\nGENESIS (The \"GEneral NEural SImulation System\") is a simulation environment for constructing realistic models of neurobiological systems at many levels of scale including: sub-cellular processes, individual neurons, networks of neurons, and neuronal systems. These simulations are “computer-based implementations of models whose primary objective is to capture what is known of the anatomical structure and physiological characteristics of the neural system of interest”. GENESIS is intended to quantify the physical framework of the nervous system in a way that allows for easy understanding of the physical structure of the nerves in question. “At present only GENESIS allows parallelized modeling of single neurons and networks on multiple-instruction-multiple-data parallel computers.” Development of GENESIS software spread from its home at Caltech to labs at the University of Texas at San Antonio, the University of Antwerp, the National Centre for Biological Sciences in Bangalore, the University of Colorado, the Pittsburgh Supercomputing Center, the San Diego Supercomputer Center, and Emory University.\n\nGENESIS works by creating simulation environments for constructing models of neurons or neural systems. \"Nerve cells are capable of communicating with each other in such a highly structured manner as to form neuronal networks. To understand neural networks, it is necessary to understand the ways in which one neuron communicates with another through synaptic connections and the process called synaptic transmission\". Neurons have a specialized structure for their function, they \"are different from most other cells in the body in that they are polarized and have distinct morphological regions, each with specific functions\". The two important regions of a neuron are the dendrite and the axon. \"Dendrites are the region where one neuron receives connections from other neurons. The cell body or soma contains the nucleus and the other organelles necessary for cellular function. The axon is a key component of nerve cells over which information is transmitted from one part of the neuron (e.g., the cell body) to the terminal regions of the neuron\". The third important piece of a neuron is the synapse. \"The synapse is the terminal region of the axon this is where one neuron forms a connection with another and conveys information through the process of synaptic transmission\".\n\nNeural networks like the ones simulated with GENESIS software can quickly become highly complex and difficult to understand. \"Just a few interconnected neurons (a microcircuit) can perform sophisticated tasks such as mediate reflexes, process sensory information, generate locomotion and mediate learning and memory. Even more complex networks, macrocircuits, consist of multiple embedded microcircuits. Macrocircuits mediate higher brain functions such as object recognition and cognition\". GENESIS endeavors to simulate neural systems as they are found in nature. Often, \"a neuron can receive contacts from up to 10,000 presynaptic neurons, and, in turn, any one neuron can contact up to 10,000 postsynaptic neurons. The combinatorial possibility could give rise to enormously complex neuronal circuits or network topologies, which might be very difficult to understand\".\n\nGENESIS was developed by Dr. James M. Bower, in the Caltech laboratory, and first released to the public in 1988 in association with the first Methods in Computational Neuroscience Course at the Marine Biological Laboratory in Woods Hole, MA. Full source code for the software was released in the same year under an open software model for development. It's now supported by the Computational Biology Initiative at the University of Texas at San Antonio and is available free along with tutorial guides on its use.\nP-GENESIS, a parallel version of GENESIS, was first run in 1990 on the Intel Delta, which was the prototype for the Intel Paragon family of massively parallel supercomputers.\n\nGENESIS is useful in creating a simulation environment for constructing models of neurobiological systems such as:\n\nThe GENESIS system is complicated, but relatively easy to use.\nAn individual can input commands through one of three ways: script files, graphical user interface, or the GENESIS command shell. These commands are then processed by the script language interpreter. \"The Script Language Interpreter processes commands entered through the keyboard, script files, or the graphical user interface, and passes them to the GENESIS simulation engine. The simulation engine also loads compiled object libraries, reads and writes data files, and interacts with the graphical user interface\". Below is a graphical representation of the user input process and a sample GENESIS output.\n\nMost current applications for GENESIS involve realistic simulations of biological systems. It is usually used to simulate the behavior of larger brain structures, for example the cerebral cortex. These studies most often occur in lab courses in neural simulation at Caltech and the Marine Biological Laboratory at Woods Hole, Massachusetts.\n\nGENESIS can be used in combination with Yale University’s software called NEURON as a means for scientists to collaborate to construct a physical description of the nervous system. The GENESIS software can also be used with Kinetikit in the modeling of signal transduction pathways.\n\nGENESIS has been used in many studies. Some of these studies involve research that focuses on the development of software that would be useful across many disciplines. Others are studies of neurons, such as Purkinje cells. These studies used GENESIS to simulate Purkinje cells and could be useful for the planning and development of later experiments using the GENESIS software.\n\nThere may also be biomedical applications of the software. For example, St. Jude Medical in Europe has developed an implanted GENESIS device.\n\n\n\n",
    "id": "12142270",
    "title": "GENESIS (software)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=47336626",
    "text": "Oriented energy filters\n\nOriented energy filters are used to grant sight to intelligent machines and sensors. The light comes in and is filtered so that it can be properly computed and analyzed by the computer allowing it to “perceive” what it is measuring. These energy measurements are then calculated to take a real time measurement of the oriented space time structure.\n\n3D Gaussian filters are used to extract orientation measurements. They were chosen due to their ability to capture a broad spectrum and easy and efficient computations.\n\nThe use of these vision systems can then be used in smart room, human interface and surveillance applications. The computations used can tell more than the standalone frame that most perceived motion devices such as a television frame. The objects captured by these devices would tell the velocity and energy of an object and its direction in relation to space and time. This also allows for better tracking ability and recognition.\n",
    "id": "47336626",
    "title": "Oriented energy filters"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=201158",
    "text": "Soft computing\n\nIn computer science, soft computing (sometimes referred to as computational intelligence, though CI does not have an agreed definition) is the use of inexact solutions to computationally hard tasks such as the solution of NP-complete problems, for which there is no known algorithm that can compute an exact solution in polynomial time. Soft computing differs from conventional (hard) computing in that, unlike hard computing, it is tolerant of imprecision, uncertainty, partial truth, and approximation. In effect, the role model for soft computing is the human mind.\n\nThe principal constituents of Soft Computing (SC) are Fuzzy Logic (FL), Evolutionary Computation (EC), Machine Learning (ML) and Probabilistic Reasoning (PR), with the latter subsuming belief networks and parts of learning theory.\n\nSoft computing (SC) solutions are unpredictable, uncertain and between 0 and 1. Soft Computing became a formal area of study in Computer Science in the early 1990s. Earlier computational approaches could model and precisely analyze only relatively simple systems. More complex systems arising in biology, medicine, the humanities, management sciences, and similar fields often remained intractable to conventional mathematical and analytical methods. However, it should be pointed out that simplicity and complexity of systems are relative, and many conventional mathematical models have been both challenging and very productive.\nSoft computing deals with imprecision, uncertainty, partial truth, and approximation to achieve practicability, robustness and low solution cost. As such it forms the basis of a considerable amount of machine learning techniques. Recent trends tend to involve evolutionary and swarm intelligence based algorithms and bio-inspired computation.\n\nThere are main differences between soft computing and possibility. Possibility is used when we don't have enough information to solve a problem but soft computing is used when we don't have enough information about the problem itself. These kinds of problems originate in the human mind with all its doubts, subjectivity and emotions; an example can be determining a suitable temperature for a room to make people feel comfortable.\n\nComponents of soft computing include:\n\nGenerally speaking, soft computing techniques resemble biological processes more closely than traditional techniques, which are largely based on formal logical systems, such as sentential logic and predicate logic, or rely heavily on computer-aided numerical analysis (as in finite element analysis). Soft computing techniques are intended to complement each other.\n\nUnlike hard computing schemes, which strive for exactness and full truth, soft computing techniques exploit the given tolerance of imprecision, partial truth, and uncertainty for a particular problem. Another common contrast comes from the observation that inductive reasoning plays a larger role in soft computing than in hard computing.\n\n",
    "id": "201158",
    "title": "Soft computing"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=5421193",
    "text": "Conflict resolution strategy\n\nConflict resolution strategies are used in production systems in artificial intelligence, such as in rule-based expert systems, to help in choosing which production rule to fire. The need for such a strategy arises when the conditions of two or more rules are satisfied by the currently known facts.\n\nConflict resolution strategies fall into several main categories. They each have advantages which form their rationales.\n\n\n",
    "id": "5421193",
    "title": "Conflict resolution strategy"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=49239240",
    "text": "VaultML\n\nVault is an Israeli–based artificial intelligence company that lays claims to have created technologies that can \"read\" movie and TV screenplays in order to predict box office and investment performance. Part of the process reportedly entails analyzing 300,000 to 400,000 elements from the script, which could be anything from plot, character development, script structure, scene events. The founders are made up of high frequency trading veterans and state they use similar approaches to predicting film performance. Vault published its 2015 film predictions for over 20 movies in early 2015 and successfully predicted correctly many box office performances throughout that year. Vault's algorithms out earned the market on a return on investment basis.\n",
    "id": "49239240",
    "title": "VaultML"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=2681589",
    "text": "Noogenesis\n\nNoogenesis (Ancient Greek: \"νοῦς\"=mind + \"γένεσις\" = origin, becoming) is the emergence and evolution of intelligence.\n\nNoo, nous (British: , US: ) – from the ancient Greek , has synonyms in other languages (Chinese), is a term that currently encompasses the semantics: mind, intelligence, intellect, reason; wisdom; insight, intuition, thought, - in a single phenomenon .\n\nNoogenesis was first mentioned in the posthumously published in 1955 book \"The Phenomenon of Man\" by Pierre Teilhard de Chardin, an anthropologist and philosopher, in a few places:\n\nThe lack of any kind of definition of the term has led to a variety of interpretations reflected in the book, including \"the contemporary period of evolution on Earth, signified by transformation of biosphere onto the sphere of intelligence—noosphere\", \"evolution run by human mind\" etc.\nThe most widespread interpretation is thought to be \"the emergence of mind, which follows geogenesis, biogenesis and anthropogenesis, forming a new sphere on Earth—noosphere\".\n\nIn 2005 Alexey Eryomin in the monograph Noogenesis and Theory of Intellect proposed a new concept of noogenesis in understanding the evolution of intellectual systems, concepts of intellectual systems, information logistics, information speed, intellectual energy, intellectual potential, consolidated into a theory of the intellect which combines the biophysical parameters of intellectual energy—the amount of information, its acceleration (frequency, speed) and the distance it's being sent—into a formula.\nAccording the new concept—proposed hypothesis continue prognostic progressive evolution of the species \"Homo sapiens\", the analogy between the human brain with the enormous amount of neural cells firing at the same time and a similarly functioning human society.\nA new understanding of the term \"noogenesis\" as an evolution of the intellect was proposed by A. Eryomin. A hypothesis based on recapitulation theory links the evolution of the human brain to the development of human civilization. The parallel between the amount of people living on Earth and the amount of neurons becomes more and more obvious leading us to viewing global intelligence as an analogy for human brain.\nAll of the people living on this planet have undoubtedly inherited the amazing cultural treasures of the past, be it production, social and intellectual ones. We are genetically hardwired to be a sort of \"live RAM\" of the global intellectual system. Alexey Eryomin suggests that humanity is moving towards a unified self-contained informational and intellectual system. His research has shown the probability of Super Intellect realizing itself as Global Intelligence on Earth. We could get closer to understanding the most profound patterns and laws of the Universe if these kinds of research were given enough attention. Also, the resemblance between the individual human development and such of the whole human race has to be explored further if we are to face some of the threats of the future.\n\nTherefore, generalizing and summarizing: \nThe term \"noogenesis\" can be used in a variety of fields i.e. medicine, biophysics, semiotics, mathematics, information technology, psychology etc. thus making it a truly cross-disciplinary one. In astrobiology noogenesis concerns the origin of intelligent life and more specifically technological civilizations capable of communicating with humans and or traveling to Earth. The lack of evidence for the existence of such extraterrestrial life creates the Fermi paradox.\n\nThe emergence of the human mind is considered to be one of the five fundamental phenomenons of emergent evolution. \nTo understand the mind, it is necessary to determine how human thinking differs from other thinking beings. Such differences include the ability to generate calculations, to combine dissimilar concepts, to use mental symbols, and to think abstractly.\nThe knowledge of the phenomenon of intelligent systems—the emergence of reason (noogenesis) boils down to:\n\n\nSeveral published works which do not employ the term \"noogenesis\", however, address some patterns in the emergence and functioning of the human intelligence: working memory capacity ≥ 7, ability to predict, prognosis, hierarchical (6 layers neurons) system of information analysis, consciousness, memory, generated and consumed information properties etc. They also set the limits of several physiological aspects of human intelligence. Сonception of emergence of insight.\n\nHistorical evolutionary development and emergence of \"H. sapiens\" as species, include emergence of such concepts as anthropogenesis, phylogenesis, morphogenesis, cephalization, systemogenesis, cognition systems autonomy.\n\nOn the other hand, development of an individual's intellect deals with concepts of embryogenesis, ontogenesis, morphogenesis, neurogenesis, higher nervous function of I.P.Pavlov and his philosophy of mind.\nDespite the fact that the morphofunctional maturity is usually reached by the age of 13, the definitive functioning of the brain structures is not complete until about 16–17 years of age.\n\nBioinformatics, genetic engineering, noopharmacology, cognitive load, brain stimulation, the efficient use of altered states of consciousness, use of non-human cognition, information technology (IT), artificial intelligence (AI) are all believed to be effective methods of intelligence advancement.\n\nThe development of the human brain, perception, cognition, memory and neuroplasticity are unsolved problems in neuroscience. Several megaprojects are being carried out in the American BRAIN Initiative and the European Human Brain Project in attempt to better our understanding of the brain's functionality along with the intention to develop human cognitive performance in the future with artificial intelligence, informational, communication and cognitive technology.\n",
    "id": "2681589",
    "title": "Noogenesis"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=43306489",
    "text": "Enterprise cognitive system\n\nEnterprise Cognitive Systems (ECS) are part of a broader shift in computing, from a programmatic to a probabilistic approach, called Cognitive computing. An Enterprise Cognitive System makes a new class of complex decision support problems computable, where the business context is ambiguous, multi-faceted, and fast-evolving, and what to do in such a situation is usually assessed today by the business user. An ECS is designed to synthesize a business context and link it to the desired outcome. It recommends evidence-based actions to help the end-user achieve the desired outcome. It does so by finding past situations similar to the current situation, and extracting the repeated actions that best influence the desired outcome.\n\nWhile general-purpose Cognitive Systems can be used for different outputs, prescriptive, suggestive, instructive, or simply entertaining, an Enterprise Cognitive System is focused on action, not insight, to help in assessing what to do in a complex situation.\n\nECS have to be:\n\n\n",
    "id": "43306489",
    "title": "Enterprise cognitive system"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=49316492",
    "text": "Darkforest\n\nDarkforest is a computer go program developed by Facebook, based on deep learning techniques using a convolutional neural network. Its updated version Darkfores2 combines the techniques of its predecessor with Monte Carlo tree search. The MCTS effectively takes tree search methods commonly seen in computer chess programs and randomizes them. With the update, the system is known as Darkfmcts3.\n\nDarkforest is of similar strength to programs like CrazyStone and Zen. It has been tested against a professional human player at the 2016 UEC cup. Google's AlphaGo program won against a professional player in October 2015 using a similar combination of techniques.\n\nDarkforest is named after Liu Cixin's science fiction novel \"The Dark Forest\".\n\nCompeting with top human players in the ancient game of Go has been a long-term goal of artificial intelligence. Go’s high branching factor makes traditional search techniques ineffective, even on cutting-edge hardware, and Go’s evaluation function could change drastically with one stone change. However, by using a Deep Convolutional Neural Network designed for long-term predictions, Darkforest has been able to substantially improve the win rate for bots over more traditional Monte Carlo Tree Search based approaches.\n\nAgainst human players, Darkfores2 achieves a stable \"3d ranking\" on KGS Go Server, which roughly corresponds to an advanced amateur human player. However, after adding Monte Carlo Tree Search to Darkfores2 to create a much stronger player named darkfmcts3, it can achieve a \"5d ranking\" on the KGS Go Server.\n\ndarkfmcts3 is on par with state-of-the-art Go AIs such as Zen, DolBaram and Crazy Stone but lags behind AlphaGo. It won 3rd place in January 2016 KGS Bot Tournament against other Go AIs.\n\nAfter Google's AlphaGo won against Fan Hui in 2015, Facebook made its AI's hardware designs public, alongside releasing the code behind DarkForest as open-source, along with heavy recruiting to strengthen its team of AI engineers.\n\nDarkforest uses a neural network to sort through the 10 board positions, and find the most powerful next move. However, neural networks alone cannot match the level of good amateur players or the best search-based Go engines, and so Darkfores2 combines the neural network approach with a search-based machine. A database of 250,000 real Go games were used in the development of Darkforest, with 220,000 used as a training set and the rest used to test the neural network's ability to predict the next moves played in the real games. This allows Darkforest to accurately evaluate the global state of the board, but local tactics were still poor. Search-based engines have poor global evaluation, but are good at local tactics. Combining these two approaches is difficult because search-based engines work much faster than neural networks, a problem which was solved in Darkfores2 by running the processes in parallel with frequent communication between the two.\n\nGo is generally played by analyzing the position of the stones on the board. Some advanced players have described it as playing in some part subconsciously. Unlike chess and checkers, where AI players can simply look farther forward at moves than human players, but with each round of Go having on average 250 possible moves, that approach is ineffective. Instead, neural networks copy human play by training the AI systems on images of successful moves, the AI can effectively learn how to interpret how the board looks, as many grandmasters do. In November 2015, Facebook demonstrated the combination of MCTS with neural networks, which played with a style that \"felt human\".\n\nIt has been noted that Darkforest still has flaws in its play style. Sometimes the bot plays tenuki (\"move elsewhere\") pointlessly when local powerful moves are required. When the bot is losing, it shows the typical behavior of MCTS, it plays bad moves and loses more. The Facebook AI team has acknowledged these as areas of future improvement.\n\nThe family of Darkforest computer go programs is based on convolution neural networks. The most recent advances in Darkfmcts3 combined convolutional neural networks with more traditional Monte Carlo tree search. Darkfmcts3 is the most advanced version of Darkforest, which combines Facebook's most advanced convolutional neural network architecture from Darkfores2 with a Monte Carlo tree search.\n\nDarkfmcts3 relies on a convolution neural networks that predicts the next k moves based on the current state of play. It treats the board as a 19x19 image with multiple channels. Each channel represents a different aspect of board information based upon the specific style of play. For standard and extended play, there are 21 and 25 different channels, respectively. In standard play, each players liberties are represented as six binary channels or planes. The respective plane is true if the player one, two, or three or more liberties available. Ko (i.e. illegal moves) is represented as one binary plane. Stone placement for each opponent and empty board positions are represented as three binary planes, and the duration since a stone has been placed is represented as real numbers on two planes, one for each player. Lastly, the opponents rank is represented by nine binary planes, where if all are true, the player is a 9d level, if 8 are true, a 8d level, and so forth. Extended play additionally considers the boarder (binary plane that is true at the border), position mask (represented as distance from the board center, i.e. formula_1, where formula_2 is a real number at a position), and each player's territory (binary, based on which player a location is closer to).\n\nDarkfmct3 uses a 12-layer full convolutional network with a width of 384 nodes without weight sharing or pooling. Each convolutional layer is followed by a rectified linear unit, a popular activation function for deep neural networks. A key innovation of Darkfmct3 compared to previous approaches is that it uses only one softmax function to predict the next move, which enables the approach to reduce the overall number of parameters. Darkfmct3 was trained against 300 random selected games from an empirical dataset representing different game stages. The learning rate was determined by vanilla stochastic gradient descent.\n\nDarkfmct3 synchronously couples a convolutional neural network with a Monte Carlo tree search. Because the convolutional neural network is computationally taxing, the Monte Carlo tree search focuses computation on the more likely game play trajectories. By running the neural network synchronously with the Monte Carlo tree search, it is possible to guarantee that each node is expanded by the moves predicted by the neural network.\n\nDarkfores2 beats Darkforest, its neural network-only predecessor, around 90% of the time, and Pachi, one of the best search-based engines, around 95% of the time. On the Kyu rating system, Darkforest holds a 1-2d level. Darkfores2 achieves a stable 3d level on KGS Go Server as a ranked bot. With the added Monte Carlo tree search, Darkfmcts3 with 5,000 rollouts beats Pachi with 10k rollouts in all 250 games; with 75k rollouts it achieves a stable 5d level in KGS server, on par with state-of-the-art Go AIs (e.g., Zen, DolBaram, CrazyStone); with 110k rollouts, it won the 3rd place in January KGS Go Tournament.\n\n\n",
    "id": "49316492",
    "title": "Darkforest"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=48718351",
    "text": "Leverhulme Centre for the Future of Intelligence\n\nThe Leverhulme Centre for the Future of Intelligence is an interdisciplinary research centre within the University of Cambridge that explores the opportunities and challenges to humanity from the development of artificial intelligence.\n\nThe Centre brings together academics from the fields of computer science, philosophy, social science and others and is a collaboration led by the University of Cambridge with links to the Oxford Martin School at the University of Oxford, Imperial College London, and the University of California, Berkeley.\n\n",
    "id": "48718351",
    "title": "Leverhulme Centre for the Future of Intelligence"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=49294877",
    "text": "Meca Sapiens\n\nMeca Sapiens (from the Latin words mēchanicus, which means \"mechanical\", and sapiens which means \"wise\", i.e. \"wise machine \") is a framework, in Artificial General Intelligence (AGI), whose aim is to implement synthetic consciousness. The framework is based on an understanding of consciousness as a system capability; its design stages followed a top-down development process; and it utilizes standard software engineering tools, structures and techniques.\n\nA complete system architecture based on the Meca Sapiens framework and suitable for design and implementation was published in late 2015. The creator of the Meca Sapiens architecture is J.E. Tardy.\n\nMeca Sapiens differs from other research in artificial consciousness with respect to: its understanding of consciousness, the development process it follows and the tools and techniques it employs.\n\nMeca Sapiens defines consciousness as an observable system capability whose formal component is the capacity of a system to generate, communicate and utilize absolute cognitive (non-sensory) representations of itself and its environment. In this understanding, the human experience does not define consciousness but is viewed as a particular instance of (a more general) category of \"system consciousness\".\n\nThis differs from a prevalent understanding of consciousness as ontologically subjective phenomena (see consciousness, hard problem of consciousness, artificial consciousness, neural correlates of consciousness) that motivate attempts to replicate, in synthetic structures, the subjective experiences (Qualia) of living creatures. In Meca Sapiens, the subjective sensations of consciousness, experienced by humans, are of anecdotal importance; what matters is the observed behavior of a system in response to certain types of information.\n\nThe development of the Meca Sapiens architecture followed a top-down (or Waterfall) process where the objective (in this case synthetic consciousness) is first defined as achievable requirements and a complete system architecture of the solution is outlined before any implementation begins. This approach is characterized by a refusal to produce any software coding before a complete solution is first outlined at the system architecture level.\n\nThis development strategy diverges from the bottom-up approach adopted in other research projects (i.e. IDA-LIDA, Novamente-OpenCog) where coding of a partial version begins early and is followed by continuing attempts to expand the initial prototype in generality and scope.\n\nThe Meca Sapiens architecture is based on the software engineering tools and techniques used in other information systems. It describes a purposefully designed embedded autonomous agent that will be conscious. The architecture makes no use of the structures or processes that take place in human (or animal) brains. In particular, Artificial Neural Networks, whose role is central in many AGI related projects, are treated as stochastic optimization mechanisms.\n\nThe intent to define consciousness as a system capability and implement it using standard techniques was first stated in 1989\n\n2008: launch of the Meca Sapiens project to develop the system architecture of a conscious machine.\n\n2011: publication of a System Requirements Specification document defining machine consciousness in terms of specification objectives.\n\n2015: publication of a complete system architecture to implement synthetic consciousness.\n\n2016: a number of implementation have begun and are currently under way.\n\n",
    "id": "49294877",
    "title": "Meca Sapiens"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=49082762",
    "text": "List of datasets for machine learning research\n\nThese datasets are used for machine-learning research and have been cited in peer-reviewed academic journals and other publications. Datasets are an integral part of the field of machine learning. Major advances in this field can result from advances in learning algorithms (such as deep learning), computer hardware, and, less-intuitively, the availability of high-quality training datasets. High-quality labeled training datasets for supervised and semi-supervised machine learning algorithms are usually difficult and expensive to produce because of the large amount of time needed to label the data. Although they do not need to be labeled, high-quality datasets for unsupervised learning can also be difficult and costly to produce. This list aggregates high-quality datasets that have been shown to be of value to the machine learning research community from multiple different data repositories to provide greater coverage of the topic than is otherwise available.\n\nDatasets consisting primarily of images or videos for tasks such as object detection, facial recognition, and multi-label classification.\n\nIn computer vision, face images have been used extensively to develop facial recognition systems, face detection, and many other projects that use images of faces.\nDatasets consisting primarily of text for tasks such as natural language processing, sentiment analysis, translation, and cluster analysis.\n\nDatasets of sounds and sound features.\n\nDatasets containing electric signal information requiring some sort of Signal processing for further analysis.\n\nDatasets from physical systems\n\nDatasets from biological systems.\n\nDatasets consisting of rows of observations and columns of attributes characterizing those observations. Typically used for regression analysis or classification but other types of algorithms can also be used. This section includes datasets that do not fit in the above categories.\n\nAs datasets come in myriad formats and can sometimes be difficult to use, there has been considerable work put into curating and standardizing the format of datasets to make them easier to use for machine learning research.\n\n\n",
    "id": "49082762",
    "title": "List of datasets for machine learning research"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=12994741",
    "text": "Neurorobotics\n\nNeurorobotics, a combined study of neuroscience, robotics, and artificial intelligence, is the science and technology of embodied autonomous neural systems. Neural systems include brain-inspired algorithms (e.g. connectionist networks), computational models of biological neural networks (e.g. artificial spiking neural networks, large-scale simulations of neural microcircuits) and actual biological systems (e.g. \"in vivo\" and \"in vitro\" neural nets). Such neural systems can be embodied in machines with mechanic or any other forms of physical actuation. This includes robots, prosthetic or wearable systems but at also, at smaller scale, micro-machines and, at the larger scales, furniture and infrastructures.\n\nNeurorobotics is that branch of neuroscience with robotics, which deals with the study and application of science and technology of embodied autonomous neural systems like brain-inspired algorithms. At its core, neurorobotics is based on the idea that the brain is embodied and the body is embedded in the environment. Therefore, most neurorobots are required to function in the real world, as opposed to a simulated environment.\n\nBeyond brain-inspired algorithms for robots neurorobotics may also involve the design of brain-controlled robot systems.\n\nNeurorobotics represents the two-front approach to the study of intelligence. Neuroscience attempts to discern what intelligence consists of and how it works by investigating intelligent biological systems, while the study of artificial intelligence attempts to recreate intelligence through non-biological, or artificial means. Neurorobotics is the overlap of the two, where biologically inspired theories are tested in a grounded environment, with a physical implementation of said model. The successes and failures of a neurorobot and the model it is built from can provide evidence to refute or support that theory, and give insight for future study.\n\nNeurorobots can be divided into various major classes based on the robot's purpose. Each class is designed to implement a specific mechanism of interest for study. Common types of neurorobots are those used to study motor control, memory, action selection, and perception.\n\nNeurorobots are often used to study motor feedback and control systems, and have proved their merit in developing controllers for robots. Locomotion is modeled by a number of neurologically inspired theories on the action of motor systems. Locomotion control has been mimicked using models or central pattern generators, clumps of neurons capable of driving repetitive behavior, to make four-legged walking robots. Other groups have expanded the idea of combining rudimentary control systems into a hierarchical set of simple autonomous systems. These systems can formulate complex movements from a combination of these rudimentary subsets. This theory of motor action is based on the organization of cortical columns, which progressively integrate from simple sensory input into a complex afferent signals, or from complex motor programs to simple controls for each muscle fiber in efferent signals, forming a similar hierarchical structure.\n\nAnother method for motor control uses learned error correction and predictive controls to form a sort of simulated muscle memory. In this model, awkward, random, and error-prone movements are corrected for using error feedback to produce smooth and accurate movements over time. The controller learns to create the correct control signal by predicting the error. Using these ideas, robots have been designed which can learn to produce adaptive arm movements or to avoid obstacles in a course.\n\nRobots designed to test theories of animal memory systems. Many studies currently examine the memory system of rats, particularly the rat hippocampus, dealing with place cells, which fire for a specific location that has been learned. Systems modeled after the rat hippocampus are generally able to learn mental maps of the environment, including recognizing landmarks and associating behaviors with them, allowing them to predict the upcoming obstacles and landmarks.\n\nAnother study has produced a robot based on the proposed learning paradigm of barn owls for orientation and localization based on primarily auditory, but also visual stimuli. The hypothesized method involves synaptic plasticity and neuromodulation, a mostly chemical effect in which reward neurotransmitters such as dopamine or serotonin affect the firing sensitivity of a neuron to be sharper. The robot used in the study adequately matched the behavior of barn owls. Furthermore, the close interaction between motor output and auditory feedback proved to be vital in the learning process, supporting active sensing theories that are involved in many of the learning models.\n\nNeurorobots in these studies are presented with simple mazes or patterns to learn. Some of the problems presented to the neurorobot include recognition of symbols, colors, or other patterns and execute simple actions based on the pattern. In the case of the barn owl simulation, the robot had to determine its location and direction to navigate in its environment.\n\nAction selection studies deal with negative or positive weighting to an action and its outcome. Neurorobots can and have been used to study *simple* ethical interactions, such as the classical thought experiment where there are more people than a life raft can hold, and someone must leave the boat to save the rest. However, more neurorobots used in the study of action selection contend with much simpler persuasions such as self-preservation or perpetuation of the population of robots in the study. These neurorobots are modeled after the neuromodulation of synapses to encourage circuits with positive results. In biological systems, neurotransmitters such as dopamine or acetylcholine positively reinforce neural signals that are beneficial. One study of such interaction involved the robot Darwin VII, which used visual, auditory, and a simulated taste input to \"eat\" conductive metal blocks. The arbitrarily chosen good blocks had a striped pattern on them while the bad blocks had a circular shape on them. The taste sense was simulated by conductivity of the blocks. The robot had positive and negative feedbacks to the taste based on its level of conductivity. The researchers observed the robot to see how it learned its action selection behaviors based on the inputs it had. Other studies have used herds of small robots which feed on batteries strewn about the room, and communicate its findings to other robots.\n\nNeurorobots have also been used to study sensory perception, particularly vision. These are primarily systems that result from embedding neural models of sensory pathways in automatas. This approach gives exposure to the sensory signals that occur during behavior and also enables a more realistic assessment of the degree of robustness of the neural model. It is well known that changes in the sensory\nsignals produced by motor activity provide useful perceptual cues that are used extensively by organisms. For example, researchers have used the depth information that emerges during replication of human head and eye movements to establish robust representations of the visual scene. \n\nBiological robots are not officially neurorobots in that they are not neurologically inspired AI systems, but actual neuron tissue wired to a robot. This employs the use of cultured neural networks to study brain development or neural interactions. These typically consist of a neural culture raised on a multielectrode array (MEA), which is capable of both recording the neural activity and stimulating the tissue. In some cases, the MEA is connected to a computer which presents a simulated environment to the brain tissue and translates brain activity into actions in the simulation, as well as providing sensory feedback. The ability to record neural activity gives researchers a window into a brain, albeit simple, which they can use to learn about a number of the same issues neurorobots are used for.\n\nAn area of concern with the biological robots is ethics. Many questions are raised about how to treat such experiments. Seemingly the most important question is that of consciousness and whether or not the rat brain experiences it. This discussion boils down to the many theories of what consciousness is.\n\n\"See Hybrot, consciousness.\"\n\nNeuroscientists benefit from neurorobotics because it provides a blank slate to test various possible methods of brain function in a controlled and testable environment. Furthermore, while the robots are more simplified versions of the systems they emulate, they are more specific, allowing more direct testing of the issue at hand. They also have the benefit of being accessible at all times, while it is much more difficult to monitor even large portions of a brain while the animal is active, let alone individual neurons. \n\nWith subject of neuroscience growing as it has, numerous neural treatments have emerged, from pharmaceuticals to neural rehabilitation. Progress is dependent on an intricate understanding of the brain and how exactly it functions. It is very difficult to study the brain, especially in humans due to the danger associated with cranial surgeries. Therefore, the use of technology to fill the void of testable subjects is vital. Neurorobots accomplish exactly this, improving the range of tests and experiments that can be performed in the study of neural processes.\n\n\n",
    "id": "12994741",
    "title": "Neurorobotics"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=49138450",
    "text": "Turing Robot\n\nTuring Robot is the first Chinese company engaged in research on the commercialization of artificial intelligence (AI). Its founding members have over ten years of experience in AI research, including semantic recognition, cognitive computing, human-machine interaction, and machine learning.\n\nThe company was founded in 2010 and in 2012 released the \"Wormhole Voice Assistant\" application, which was the first Chinese intelligence-based voice assistant application.\n\nIn 2014, Turing released the first open platform for AI robots, also known as the Turing Robot. In November 2015, the Turing OS system was released .\n\nTuring Robot Open Platform is now the world's largest chatbot open platform with the number of developers reached 600,000. Its Chinese semantic recognition accuracy is up to 94.7%, Turing Robot can provide intelligence-based software and hardware products with AI services covering Chinese semantic analysis, natural language and dialogue processing, DeepQA and more.\n\nSince the official launch in November 2014, the Turing Robot has provided technical support to more than 230,000 developers and partners with over 130 billion total queries so far. The Turing Robot has established a proven track record in many hardware and software fields, including domestic service robots, commercial service robots, companion robots for children, intelligent customer service systems, intelligent vehicle systems, and smart home control systems.\n\nBy simulating the cognitive and communicative behaviors of humans, the Turing Robot provides users with a conversation feature that provides intelligence-based interaction in hardware and software products, as well as the NLP Knowledge Base for customized life and business needs. Meanwhile, the Turing Robot offers a package of 500 types of life service skills to accommodate product needs.\n\nThe Turing Robot comprehensively integrates over 500 kinds of life information and service skills into software and hardware products. Inquiries can be made about recipes, weather, delivery services, among other things.\n\nFeatures of the Turing Robot include personalized settings, keyword filtering, robot training, accuracy rate setting, and data statistics.\n\nCurrently, there are more than 15 application cases for the Turing Robot, including smart robots, toys, vehicle systems, QQ robots, smart home, and smart custom service.\n\nTuring OS is an intelligent robot operating system whose design is based on simulating the thinking mode and emotional recognition of humans, which provides natural and friendly multimode human-machine interaction methods. It includes a thinking enhancement engine, an affective computing engine and a self-learning engine.\n\nTuring OS introduces new multimodal interaction for robots that is closer to human capability that prior technology. With Turing OS installed, robots can understand voice, text, images, body movement, and other multimodal data from external sources such as humans. Meanwhile, they can provide simultaneous feedback, including through voice, text, and images.\n\nThe Affective Computing Engine of Turing OS enables robots to have the same ability to demonstrate affection as humans. The Turing OS develops a mature affective computing engine for robots, which assists robots in understanding and expressing human emotions. The affective computing engine of the Turing OS has two components: human emotion recognition and human-like emotion expression.\n\nThe Cognitive Enhancement Engine of Turing OS enables robots to have the same thinking ability as humans. Based on research into human macro-thinking models and micro-thinking models, the Turing OS provides powerful thinking for robots, and enables robots to operate in multiple macro- and micro-thinking modes. As a result, these robots can have thinking ability similar to that of humans. Robots with the Turing OS attain the thinking level of children aged 4 to 5.\n\nThe Cognitive Enhancement Engine includes 26 types of macro thinking and 10,140 types of micro thinking.\n\nThe Self-learning Engine enables robots to have the same learning abilities as humans.\n\nAdopting an ultra-efficient algorithm for deep learning, Turing OS uses enormous big data sources and the operating environment of a supercomputer to provide robots with a powerful capacity for self-learning. This helps robots realize real-time fast iteration and updating of emotion recognition and expression, thinking models, knowledge construction, and adaptive scenarios.\n\nTuring OS provides application services, covering 25 applicable scenarios, including English learning, family entertainment, knowledge learning, and operations control.\n\nOn The First Innovation Conference of Turing Robot, CEO of Turing Robot, Yu Zhichen released an updated version of Turing OS: Turing OS Version 1.5, with an added visual ability to its former edition.\n\nTuring Robot Application Project was also released on the Innovation Conference of Turing Robot, it includes official applications like Robots’ Chatting, Automatic Camera, Robots Singing and English Read-after. Besides that, the platform is totally free and open to developers to build their won apps on and Weather Report, Dictionary and Music Playlist were taken for a start.\n\nBOSCH Vehicle System\n\nHTC Voice: Little Hi\n\nChina Telecom Customer Service\n\nLetou Carl Vehicle System\n\nJIMI Cat APP\n\nKido Smart Watch\n\nLeddy Robot\n\nDoraemon\n\nRobotant\n\nHahabot\n\nLuobotec\n\nAlbert Robot\n\nTuba Robot\n",
    "id": "49138450",
    "title": "Turing Robot"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=44391058",
    "text": "Winograd Schema Challenge\n\nThe Winograd Schema Challenge (WSC) is a test of machine intelligence proposed by Hector Levesque, a computer scientist at the University of Toronto. Designed to be an improvement on the Turing test, it is a multiple-choice test that employs questions of a very specific structure: they are instances of what are called Winograd Schemas, named after Terry Winograd, a professor of computer science at Stanford University.\n\nOn the surface, Winograd Schema questions simply require the resolution of anaphora: the machine must identify the antecedent of an ambiguous pronoun in a statement. This makes it a task of natural language processing, but Levesque argues that for Winograd Schemas, the task requires the use of knowledge and commonsense reasoning.\n\nNuance Communications announced in July 2014 that it would sponsor an annual WSC competition, with a prize of $25,000 for the best system that could match human performance.\n\nThe Winograd Schema Challenge was proposed in the spirit of the Turing Test. Proposed by Alan Turing in 1950, the Turing Test plays a central role in the philosophy of artificial intelligence. Turing proposed that instead of debating what intelligence is, the science of AI should be concerned with demonstrating intelligent behavior, which can be tested. But the exact nature of the test Turing proposed has come under scrutiny, especially since an AI chat bot named Eugene was claimed to pass it in 2014. The Winograd Schema Challenge was proposed in part to ameliorate the problems that came to light with the nature of the programs that performed well on the test.\n\nTuring's original proposal was what he called the Imitation Game, which involves free-flowing, unrestricted conversations in English between human judges and computer programs over a text-only channel (such as teletype). In general, the machine passes the test if interrogators are not able to tell the difference between it and a human in a five-minute conversation.\n\nOn June 7, 2014, a computer program named Eugene Goostman was declared to be the first AI to have passed the Turing Test in a competition held by the University of Reading in England. In the competition Eugene was able to convince 33% of judges that they were talking with a 13-year-old Ukrainian boy. The supposed victory of a machine that thinks aroused controversies about the Turing Test. Critics claimed that Eugene passed the test simply by fooling the judge and taking advantages of its purported identity. For example, it could easily skip some key questions by joking around and changing subjects. However, the judge would forgive its mistakes because Eugene identified as a teenager who spoke English as his second language.\n\nThe performance of Eugene Goostman exhibited some of the Turing Test's problems. Levesque identifies several major issues, summarized as follows:\n\nThe key factor in the WSC is the special format of its questions, which are derived from Winograd Schemas. Questions of this form may be tailored to require knowledge and commonsense reasoning in a variety of domains. They must also be carefully written not to betray their answers by selectional restrictions or statistical information about the words in the sentence.\n\nThe first cited example of a Winograd Schema (and the reason for their namesake) is due to Terry Winograd:\nThe choices of \"feared\" and \"advocated\" turn the schema into its two instances:\n\nThe question is whether the pronoun \"they\" refers to the city councilmen or the demonstrators, and switching between the two instances of the schema changes the answer. The answer is immediate for a human reader, but proves difficult to emulate in machines. Levesque argues that knowledge plays a central role in these problems: the answer to this schema has to do with our understanding of the typical relationships between and behavior of councilmen and demonstrators.\n\nSince the original proposition of the Winograd Schema Challenge, Ernest Davis, a professor at New York University, has compiled a list of over 140 Winograd Schemas from various sources as examples of the kinds of questions that should appear on the Winograd Schema Challenge.\n\nA Winograd Schema Challenge question consists of three parts:\n\nA machine will be given the problem in a standardized form which includes the answer choices, thus making it a binary decision problem.\n\nThe Winograd Schema Challenge has the following purported advantages:\n\nOne difficulty with the Winograd Schema Challenge is the development of the questions. They need to be carefully tailored to ensure that they require commonsense reasoning to solve. For example, Levesque gives the following example of a so-called Winograd Schema that is \"too easy\":\nThe answer to this question can be determined on the basis of selectional restrictions: in any situation, pills do not get pregnant, women do; women cannot be carcinogenic, but pills can. Thus this answer could be derived without the use of reasoning, or any understanding of the sentences' meaning—all that is necessary is data on the selectional restrictions of \"pregnant\" and \"carcinogenic.\"\n\nAnother limitation pointed out by Peng, Khashabi and Roth \nis limitation of evaluation as a binary task; typically the pronoun can refer to one of the two previous mentions. On the other hand, the general anaphora resolution problem is a clustering task, rather than a binary task. They propose a modified version of the task and modified evaluation metric to make it more similar and applicable to the co-reference problem.\n\nAn annual competition sponsored by Nuance Communications, Inc. will be organized, administered, and evaluated by CommonsenseReasoning.org. The winner will receive a grand prize of $25,000.\n\nThe Twelfth International Symposium on the Logical Formalizations of Commonsense Reasoning was held on March 23–25, 2015 at the AAAI Spring Symposium Series at Stanford University, with a special focus on the Winograd Schema Challenge. The organizing committee included Leora Morgenstern (Leidos), Theodore Patkos (The Foundation for Research & Technology Hellas), and Robert Sloan (University of Illinois at Chicago).\n\nThe 2016 Winograd Schema Challenge was run on July 11, 2016 at IJCAI-16. There were four contestants. The highest score achieved was 58% correct, by Quan Liu et al, of the University of Science and Technology, China. Hence, by the rules of that challenge, no prizes were awarded, and the challenge did not proceed to the second round. The organizing committee in 2016 was Leora Morgenstern, Ernest Davis, and Charles Ortiz. The next running of the challenge will be at AAAI-18.\n\n",
    "id": "44391058",
    "title": "Winograd Schema Challenge"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=32856741",
    "text": "NewsRx\n\nNewsRx is a media and technology company focusing on digital media, printed media, news services, and knowledge discovery through its BUTTER platform. In 1995 the company was the world’s largest producer of health news. The company publishes 194 newsweeklies in health and other fields, which are distributed to subscribers and partners including Factiva, the \"Wall Street Journal Professional Edition\", Thomson Reuters, ProQuest, and Cengage Learning. C W Henderson founded the company in 1984 and its first publication was \"AIDS Weekly.\" In the early 2000s, the firm added the imprint, VerticalNews to publish newsweeklies in non-health fields. Now based in Atlanta, Georgia, the company reports through its daily news service and publishes reference books through its partner, ScholarlyEditions. NewsRx launched its BUTTER platform in 2015, which is a knowledge discovery engine that delivers its content to academics, researchers, and professionals.\n\nThe idea for the first newsletter originated at an international conference on AIDS sponsored by the Centers for Disease Control & Prevention (CDC). A staff member commented to CW Henderson on the need for a publication to condense the rapid rise in information about the disease. In 1984, Henderson created CW Henderson Publisher, which became NewsRx in 2004.\n\nThat same year, the company distributed its first journal, \"CDC AIDS Weekly\", (which split into \"AIDS Weekly\" and \"Medical Letter on the CDC & FDA\") to an international audience. The first subscriber was the Soviet Union. Other subscribers include physicians, educators, government agencies, and pharmaceutical companies.\n\nThe articles in \"AIDS Weekly\" discussed social issues of the disease to medical research. The newsweekly included “shorts” to explain as much as was known about unfolding information and events.\n\nBefore the World Wide Web, NewsRx coordinated with the National AIDS Information Clearinghouse to provide information on the disease. The \"CDC AIDS Weekly Infoline\" provided a list of upcoming AIDS seminars as well as names and addresses of over 65 AIDS periodicals published worldwide.\n\nThe information published in \"AIDS Weekly\" came primarily from the government organization Centers for Disease Control (CDC). Though the newsweekly had no direct ties to the CDC other than as a source for information, a CDC official described the publication as “highly informative.” Other sources of information for this and other titles were the nearby Emory University medical library and international agencies. Articles included summaries of peer-reviewed research, conference reports, news releases, and compilations from other health and medical organizations.\n\nHowever, in the beginning, some critics were offended by the fact that NewsRx was a non-governmental agency distributing statistics that were available for free in official versions frrm the government. (see Controversy)\n\nIn 1988, the firm added \"Cancer Weekly\" and it added \"Blood Weekly\" in 1993. The company added \"Vaccine Weekly\" in 1995, followed by over 100 more medical-related titles.\n\nIn 2007, the firm introduced VerticalNews, a group of newsweeklies that included 86 new non-medical related titles, thus expanding the company’s reader-base. The initiative also emphasized the distribution of information on a global scale (more than 50 countries). Electronic versions of the publications are available. In addition to the 103 newsweeklies in medical, legal, and business fields within the healthcare industry, the firm publishes 88 newsweeklies within the VerticalNews imprint.\n\nThe firm also adopted site licenses, including the ability for users to download reports showing the types of information used in a given organization—information previously restricted to the NewsRx staff. The system recognizes IP addresses to facilitate research activities.\n\nIn 1999, the firm also adopted \"Artificial Intelligence Journalist\" (AIJ) which uses robotics, machine learning, algorithms, logic, and automated reasoning to provide computer-assisted reporting and data driven journalism. This software program shortens the time from news event to news distribution.\n\nOn April 22, 2015, NewsRx announced hiring new VP and Publisher Kalani Rosell. The business development office opened in 2016 in New Haven, Connecticut, headed by Rosell.\n\nIn 2015, NewsRx started BUTTER, which stands for Better Understanding Through Technology & Emerging Research, a business intelligence and data analytics platform with emerging research and new discoveries. It has content for researchers, academics, and investors, using a New Discovery Index (NDI) that analyzes discoveries worldwide by quarter and new discoveries within specific topic areas.\n\nBUTTER uses a search engine and publishes 10,000 new articles a day (11.4 million articles as of March, 2016).\n\nBUTTER’s platform creates content 30 minutes after stock markets close, monitoring all market movements, new SEC and patent filings, trademarks, and financial and investment decisions.\n\nThroughout most of its operating history, the NewsRx staff included board-certified medical editors with bachelor's degrees in journalism or related fields and experience in writing and editing. Supervisors with PhDs and master's degrees in English, Journalism, Management, and Information Technology. NewsRx editors condense news so that each article is less than 1000 words, while including key sentences of the original report and full bibliographic references and citations to original source material.\n\nNewsRx is staffed by journalists rather than medical professionals. At the company’s beginnings, \"Newsweek\" magazine commented that \"AIDS Weekly\", as a non-government entity, should not be reporting on topics that included policy, research, and statistics that some considered exclusive to the government. The head of the Centers for Disease Control and Prevention (CDC) AIDS task force at the time was misquoted as stating that he disagreed with having the CDC name associated with the newsweekly. On the contrary, every issue of the \"CDC AIDS Weekly\" included an advisory caption, “…not sponsored by, endorsed by, affiliated with, or officially connected with the CDC.” Other staffers within the CDC supported NewsRx’s view to bring AIDS awareness to the public eye. The \"Boston Globe\" reported that \"AIDS Weekly\" was a necessary “watchdog” publication providing needed information to the public. Other articles appeared supporting NewsRx in the \"Wall Street Journal\", \"The New York Times\", and \"USA Today\", for what they said to be its impact in AIDS awareness and investigative journalism.\n\nCW Henderson’s role as executive editor at the firm was discussed in an article in \"Editor and Publisher\", focusing on the influence of pharmaceutical companies on news publications. Henderson opposed pharmaceutical company influence on reporters as well as premature reporting of experiments.\nThe firm was also involved with \"The New York Times\" in controversial breaking news about AIDS studies that had purposely been tampered with at the CDC. On at least 5 occasions, research on the causes of AIDS and other viral diseases might have been tampered with. \"CDC AIDS Weekly\" published an internal CDC memorandum on the incident.\n\nIn 2010, the firm's VerticalNews China was the subject of a denial of service attack that originated from China as a result of controversial news reported. The attack was halted when the company’s IP service identified the source and blocked it.\n\nIn 2011, the firm partnered with ScholarlyMedia’s ScholarlyEditions imprint, publishing 4,000 reference books, which replaced the EncyK line. The president of NewsRx is also president of ScholarlyMedia. The company’s book imprint is ScholarlyEditions, and its peer reviewed news service is ScholarlyNews.\n\nThe company's partners include:\n\nThe company's rankings include:\n",
    "id": "32856741",
    "title": "NewsRx"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=40257392",
    "text": "Brain technology\n\nBrain technology, or self-learning know-how systems, defines a technology that employs latest findings in neuroscience. The term was first introduced by the Artificial Intelligence Laboratory in Zurich, Switzerland, in the context of the Roboy project. Brain Technology can be employed in robots, know-how management systems and any other application with self-learning capabilities. In particular, Brain Technology applications allow the visualization of the underlying learning architecture often coined as “know-how maps”.\n",
    "id": "40257392",
    "title": "Brain technology"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=50285983",
    "text": "Behavior informatics\n\nBehavior informatics (BI) is the informatics of behaviors so as to obtain behavior intelligence and behavior insights.\n\nDifferent from applied behavior analysis from the psychological perspective, BI builds computational theories, systems and tools to qualitatively and quantitatively model, represent, analyze, and manage behaviors of individuals, groups and/or organizations .\n\nBI is built on classic study of behavioral science, including behavior modeling, applied behavior analysis, behavior analysis, behavioral economics, and organizational behavior. Typical BI tasks consist of individual and group behavior formation, representation, computational modeling, analysis, learning, simulation, and understanding of behavior impact, utility, non-occurring behaviors etc. for behavior intervention and management.\n\nBehavior informatics covers \"behavior analytics\" which focuses on analysis and learning of behavioral data.\n\nFrom an informatics perspective, a behavior consists of four key elements: actors (behaviorial subjects and behavioral objects), operations (actions, activities) and interactions (relationships), and their properties. A behavior can be represented as a behavior vector, all behaviors of an actor or an actor group can be represented as behavior sequences and multi-dimensional behavior matrix.\n",
    "id": "50285983",
    "title": "Behavior informatics"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=50336055",
    "text": "Glossary of artificial intelligence\n\n\"Contributors are needed to write definitions for terms in this glossary. Please keep the definitions brief and in accordance with Wikipedia's policies and guidelines.\"\n\nThis glossary of artificial intelligence terms is about artificial intelligence, its sub-disciplines, and related fields.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "id": "50336055",
    "title": "Glossary of artificial intelligence"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=34600216",
    "text": "Color moments\n\nColor moments are measures that characterise color distribution in an image in the same way that central moments uniquely describe a probability distribution. Color moments are mainly used for color indexing purposes as features in image retrieval applications in order to compare how similar two images are based on color. Usually one image is compared to a database of digital images with pre-computed features in order to find and retrieve a similar image. Each comparison between images results in a similarity score, and the lower this score is the more identical the two images are supposed to be.\n\nColor moments are scaling and rotation invariant. It is usually the case that only the first three color moments are used as features in image retrieval applications as most of the color distribution information is contained in the low-order moments. Since color moments encode both shape and color information they are a good feature to use under changing lighting conditions, but they cannot handle occlusion very successfully. Color moments can be computed for any color model. Three color moments are computed per channel (e.g. 9 moments if the color model is RGB and 12 moments if the color model is CMYK). Computing color moments is done in the same way as computing moments of a probability distribution.\n\nThe first color moment can be interpreted as the average color in the image, and it can be calculated by using the following formula\n\nwhere N is the number of pixels in the image and formula_2 is the value of the j-th pixel of the image at the i-th color channel.\n\nThe second color moment is the standard deviation, which is obtained by taking the square root of the variance of the color distribution.\n\nwhere formula_4 is the mean value, or first color moment, for the i-th color channel of the image.\n\nThe third color moment is the skewness. It measures how asymmetric the color distribution is, and thus it gives information about the shape of the color distribution. Skewness can be computed with the following formula:\n\nKurtosis is the fourth color moment, and, similarly to skewness, it provides information about the shape of the color distribution. More specifically, kurtosis is a measure of how flat or tall the distribution is in comparison to normal distribution.\n\nHigher-order color moments are usually not part of the color moments feature set in image retrieval tasks as they require more data in order to obtain a good estimate of their value, and also the lower-order moments generally provide enough information.\n\nColor moments have significant applications in image retrieval. They can be used in order to compare how similar two images are. This is a relatively new approach to color indexing. The greatest advantage of using color moments comes from the fact that there is no need to store the complete color distribution. This greatly speeds up image retrieval since there are less features to compare. In addition, the first three color moments have the same units, which allows for comparison between them.\n\nColor indexing is the main application of color moments. Images can be indexed, and the index will contain the computed color moments. Then, if someone has a particular image and wants to find similar images in the database, the color moments of the image of interest will also be computed. After that the following function will be used in order to compute a similarity score between the image of interest and all the images in the database:\nwhere:\nFinally, the images in the database will be ranked according to the computed similarity score with the image of interest, and the database images with the lowest formula_14 value should be retrieved. \"A retrieval based on formula_14 may produce false positives because the index contains no information about the correlation between the color channels\".\n\nA simple and concise example of the use of color moments for image retrieval tasks is illustrated in.\n\nConsider having several test images in a database and a \"New Image\". The goal is to retrieve images from the database that are similar to the \"New Image\". The first three color moments are used as features. There are several steps in this computation.\n\n\n\n\n",
    "id": "34600216",
    "title": "Color moments"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=50672378",
    "text": "Gomocup\n\nGomocup is a worldwide tournament of artificial intelligences (AI) playing Gomoku and Renju. The tournament has been played since 2000 and takes place every year. As of 2016, it is the most famous and largest Gomoku AI tournament in the world, with around 40 participants from about 10 countries.\n\nGomocup has been played in the freestyle Gomoku rule with the board size of 20 since it was started in 2000. In 2009, the standard Gomoku rule was added into Gomocup as a tournament, in which the board size is 15 and more than five in a row is not considered to be a win. In 2016, the Renju rule was also added into Gomocup, with a board size of 15 and forbidden moves for black. In particular, since there are a large number of participants in the freestyle Gomoku tournament, the freestyle Gomoku tournament is divided into several leagues, and the fast game tournament is introduced.\n\nTo get rid of the fact that there is a winning strategy for the player who plays first in Gomoku, balanced openings have been prepared by Gomoku experts since 2006. Games would be started from these balanced openings, and neither side would have a big advantage from the very beginning.\n\nThere were two AI vs. Human tournaments held in the Czech Republic in 2006 and 2011.\n\nIn 2006, the top 3 programs in Gomocup had a tournament with 3 of the top 10 players in Piškvorky online. There were 2 games between each pair of AI and human players. The result was one win, one draw and one loss for AIs, and the total score was 3:3.\n\nIn 2011, the tournament was between the top 4 programs in Gomocup and 4 players at the top of the Czech Gomoku rating list. Similar to the 1st tournament, there were 2 games between each pair of AI. This time there were 3 draws and 1 win for AIs, and the total score was 5:3.\n\nThe Elo rating system for Gomocup was built in 2016 and calculated with all the historical tournament results ever since. The rating is calculated with the open-source tool BayesElo, with a few parameters modified to get adapted to the Gomoku game. There is a rating list for each game rule. The Elo ratings are updated every year after the Gomocup tournament finishes.\n\nThe results for the Gomocup tournaments since 2000 is in the following.\n",
    "id": "50672378",
    "title": "Gomocup"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=50562089",
    "text": "Robot lawyer\n\nA robot lawyer or a robo-lawyer refers to a legal AI application that can perform tasks that are typically done by paralegals or young associates at law firms. However, there is some debate on the correctness of the term. Some commentators say that legal AI is technically speaking neither a lawyer nor a robot and should not be referred to as such. Other commentators believe that the term can be misleading and note that the robot lawyer of the future won't be one all-encompassing application but a collection of specialized bots for various tasks.\n\nSome legal AI solutions are developed and marketed directly to the customers or consumers, whereas other applications are tools for the attorneys at law firms. Lawbots.info has a catalogue of automated legal AI services that have been developed for the direct distribution to the public. One notable legal AI solution for the law firms is ROSS, which has been used by US law firms to assist in legal research, but there are already hundreds of legal AI solutions that operate in multitude of ways varying in sophistication and dependence on scripted algorithms.\n\n\n",
    "id": "50562089",
    "title": "Robot lawyer"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=51291103",
    "text": "Mivar-based approach\n\nThe Mivar-based approach is a mathematical tool for designing artificial intelligence (AI) systems. Mivar (\"Multidimensional Informational Variable Adaptive Reality\") was developed by combining production and Petri nets. The Mivar-based approach was developed for semantic analysis and adequate representation of humanitarian epistemological and axiological principles in the process of developing artificial intelligence. The Mivar-based approach incorporates computer science, informatics and discrete mathematics, databases, expert systems, graph theory, matrices and inference systems. The Mivar-based approach involves two technologies:\nMivar networks allow us to develop cause-effect dependencies (“If-then”) and create an automated, trained, logical reasoning system.\n\nRepresentatives of Russian association for artificial intelligence (RAAI) – for example, V. I. Gorodecki, doctor of technical science, professor at SPIIRAS and V. N. Vagin, doctor of technical science, professor at MPEI declared that the term is incorrect and suggested that the author should use standard terminology.\n\nWhile working in the Russian Ministry of Defense, O. O. Varlamov started developing the theory of “rapid logical inference” in 1985. He was analyzing Petri nets and productions to construct algorithms. Generally, mivar-based theory represents an attempt to combine entity-relationship models and their problem instance – semantic networks and Petri networks. \n\nThe abbreviation MIVAR was introduced as a technical term by O. O. Varlamov, Doctor of Technical Science, professor at Bauman MSTU in 1993 to designate a “semantic unit” in the process of mathematical modeling. The term has been established and used in all of his further works.\n\nThe first experimental systems operating according to mivar-based principles were developed in 2000. Applied mivar systems were introduced in 2015.\n\nMivar is the smallest structural element of discrete information space.\n\nObject-Property-Relation (VSO) is a graph, the nodes of which are concepts and arcs are connections between concepts.\n\nMivar space represents a set of axes, a set of elements, a set of points of space and a set of values of points.\n\nformula_1\n\nwhere:\n\n\nThen: <math>\\forall a_n \\exists F_n =\\{ f_\n\n",
    "id": "51291103",
    "title": "Mivar-based approach"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=51295111",
    "text": "Schema-agnostic databases\n\nSchema-agnostic databases or vocabulary-independent databases aim at supporting users to be abstracted from the representation of the data, supporting the automatic semantic matching between queries and databases. Schema-agnosticism is the property of a database of mapping a query issued with the user terminology and structure, automatically mapping it to the dataset vocabulary.\n\nThe increase in the size and in the semantic heterogeneity of database schemas bring new requirements for users querying and searching structured data. At this scale it can become unfeasible for data consumers to be familiar with the representation of the data in order to query it. At the center of this discussion is the semantic gap between users and databases, which becomes more central as the scale and complexity of the data grows.\n\nThe evolution of data environments towards the consumption of data from multiple data sources and the growth in the \"schema size\", \"complexity\", \"dynamicity\" and \"decentralisation\" (SCoDD) of schemas increases the complexity of contemporary data management. The SCoDD trend emerges as a central data management concern in Big Data scenarios, where users and applications have a demand for more complete data, produced by independent data sources, under different semantic assumptions and contexts of use, which is the typical scenario for Semantic Web Data applications.\n\nThe evolution of databases in the direction of heterogeneous data environments strongly impacts the usability, semiotics and semantic assumptions behind existing data accessibility methods such as structured queries, keyword-based search and visual query systems. With schema-less databases containing potentially millions of dynamically changing attributes, it becomes unfeasible for some users to become aware of the 'schema' or vocabulary in order to query the database. At this scale, the effort in understanding the schema in order to build a structured query can become prohibitive.\n\nSchema-agnostic queries can be defined as query approaches over structured databases which allow users satisfying complex information needs without the understanding of the representation (schema) of the database. Similarly, Tran et al. defines it as \"search approaches, which do not require users to know the schema underlying the data\". Approaches such as keyword-based search over databases allow users to query databases without employing structured queries. However, as discussed by Tran et al.: \"From these points, users however have to do further navigation and exploration to address complex information needs. Unlike keyword search used on the Web, which focuses on simple needs, the keyword search elaborated here is used to obtain more complex results. Instead of a single set of resources, the goal is to compute complex sets of resources and their relations.\"\n\nThe development of approaches to support natural language interfaces (NLI) over databases have aimed towards the goal of schema-agnostic queries. Complementarily, some approaches based on keyword search have targeted keyword-based queries which express more complex information needs. Other approaches have explored the construction of structured queries over databases where schema constraints can be relaxed. All these approaches (natural language, keyword-based search and structured queries) have targeted different degrees of sophistication in addressing the problem of supporting a flexible semantic matching between queries and data, which vary from the completely absence of the semantic concern to more principled semantic models. \nWhile the demand for schema-agnosticism has been an implicit requirement across semantic search and natural language query systems over structured data, it is not sufficiently individuated as a concept and as a necessary requirement for contemporary database management systems. Recent works have started to define and model the semantic aspects involved on schema-agnostic queries.\n\nConsist of schema-agnostic queries following the syntax of a structured standard (for example SQL, SPARQL). The syntax and semantics of operators are maintained, while different terminologies are used.\n\nwhich maps to the following SPARQL query in the dataset vocabulary:\n\nwhich maps to the following SPARQL query in the dataset vocabulary:\n\nConsist of schema-agnostic queries using keyword queries. In this case the syntax and semantics of operators are different from the structured query syntax.\n\nAs of 2016 the concept of schema-agnostic queries has been developed primarily in academia. Most of schema-agnostic query systems have been investigated in the context of Natural Language Interfaces over databases or over the Semantic Web. These works explore the application of semantic parsing techniques over large, heterogeneous and schema-less databases.\nMore recently, the individuation of the concept of schema-agnostic query systems and databases have appeared more explicitly within the literature. Freitas et al. provide a probabilistic model on the semantic complexity of mapping schema-agnostic queries.\n",
    "id": "51295111",
    "title": "Schema-agnostic databases"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=52036598",
    "text": "Differentiable neural computer\n\nA differentiable neural computer (DNC) is a recurrent artificial neural network architecture with an autoassociative memory. The model was published in 2016 by Alex Graves et al. of DeepMind. \n\nSo far, DNCs have only been demonstrated to handle relatively simple tasks, which could have been easily solved using conventional computer programming decades ago. But DNCs don't need to be programmed for each problem they are applied to, but can instead be trained. This attention span allows the user to feed complex data structures such as graphs sequentially, and recall them during later use. Furthermore, they can learn some aspects of symbolic reasoning and apply it to the use of working memory. Some experts see promise that they can be trained to perform complex, structured tasks and address big-data applications that require some sort of rational reasoning, such as generating video commentaries or semantic text analysis.\n\nDNC can be trained to navigate a variety of rapid transit systems, and then what the DNC learns can be applied, for example, to get around on the London Underground. A neural network without memory would typically have to learn about each different transit system from scratch. On graph traversal and sequence-processing tasks with supervised learning, DNCs performed better than alternatives such as long short-term memory or a neural turing machine. With a reinforcement learning approach to a block puzzle problem inspired by SHRDLU, DNC was trained via curriculum learning, and learned to make a plan. It performed better than a traditional recurrent neural network.\n\nDNC networks were introduced as an extension of the Neural Turing Machine (NTM), with the addition of memory attention mechanisms that control where the memory is stored, and temporal attention that records the order of events. This structure allows DNCs to be more robust and abstract than a NTM, and still perform tasks that have longer-term dependencies than some of its predecessors such as the LSTM network. The memory, which is simply a matrix, can be allocated dynamically and accessed indefinitely. The DNC is differentiable end-to-end (each subcomponent of the model is differentiable, therefore so is the whole model). This makes it possible to optimize them efficiently using gradient descent. It learns how to store and retrieve the information such that it satisfies the task execution.\n\nThe DNC model is similar to the Von Neumann architecture, and because of the resizability of memory, it is turing complete. Differentiable Neural Computers were inspired by the mammalian hippocampus\n\nDNC, as originally published \n\n</math>\n\nRefinements to the model have been published since the original paper's release. Sparse memory addressing results in a time and space complexity reduction of thousands of times. This can be achieved by using an approximate nearest neighbors algorithm, such as Locality-sensitive hashing, or a random k-d tree like the Fast Library for Approximate Nearest Neighbors from UBC. Adding Adaptive Computation Time (ACT) separates computation time from data time, which uses the fact that problem length and problem difficulty are not always the same. Training using synthetic gradients performs considerably better than Backpropagation through time (BPTT).\n",
    "id": "52036598",
    "title": "Differentiable neural computer"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=51885204",
    "text": "The Fable of Oscar\n\nThe Fable of Oscar is a fable proposed by John L. Pollock in his book \"How to Build a Person\" () to defend the idea of token physicalism, agent materialism, and strong AI. It ultimately illustrates what is needed for an Artificial Intelligence to be built and why humans are just like intelligent machines.\n\nOnce in a distant land there lived a race of Engineers. They have all their physical needs provided by the machines they have invented. One of the Engineers decide that he will create an \"intelligent machine\" that is much more ingenious than the more machines, in that it can actually sense, learn, and adapt to its environment as an intelligent animal.\n\nThe first version of the machine is called \"Oscar I\". It has pain sensors and \"fight-or-flight\" responses build within to help it survive hostile environment. In this stage Oscar I is much like the machines Hilary Putnam considers in 1960.\n\nIn order for Oscar I to avoid damages in hostile environment, it must not only be able to respond to its pain sensors but also predict what is likely to happen based on its generalization of its pain sensor activations. Therefore, a \"pain sensor sensor\" was built to sense its pain sensors, thus giving it a rudimentary self-awareness. In this stage Oscar I is much like an amoeba as Oscar II like a worm. Amoebas respond to pain while worms learn to avoid it.\n\nThe problem with Oscar II is that it has no conception if the environment is fooling him. For example, he can't distinguish if a machine-eating tiger and a mirror image of such tiger. To solve such problem, \"introspective sensors\" were built into Oscar II and made him \"Oscar III\". Oscar III can now sense the operation of its own sensors and form generalization about its reliability, thus acquired a higher degree of self-awareness. In this stage Oscar II is much like a bird as Oscar III a kitten. Kittens quickly learn about mirror image and come to ignore them while birds go on attacking their own reflection until they become exhausted.\n\nConsider a world populated by Oscarites. If the Oscarites are sufficiently intelligent, it can philosophizing the difference between their outward physical state and inward mental state. While we, from our perspective, describe the Oscarites as sensing the operation of their perceptual sensors, they describe it as they are \"being self-aware and being conscious\".\n\nIn the end of the fable Pollock states that while the Engineers are fictional, Oscar is real and we are in fact the Oscarites.\n\n\n",
    "id": "51885204",
    "title": "The Fable of Oscar"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=51237053",
    "text": "Wojciech Zaremba\n\nWojciech Zaremba (born November 30, 1988) is a Polish mathematician and computer scientist, noted for his work on artificial neural networks and deep learning. Zaremba is head of robotics, and founding team member of OpenAI, which mission is to build safe artificial intelligence (AI), and ensure that its benefits are as evenly distributed as possible.\n\nZaremba was born in 1988 in Kluczbork, Poland. At a young age, he won local competitions and awards in mathematics, computer science, chemistry and physics. In 2007, Zaremba represented Poland in the International Mathematical Olympiad, and won a silver medal.\n\nZaremba studied at the University of Warsaw and École Polytechnique, and graduated in 2013 with two master's degrees in mathematics. He then began his PhD at New York University (NYU) in deep learning under the supervision of Yann LeCun and Rob Fergus. Zaremba graduated and received his PhD in 2015.\n\nDuring his undergraduate years at the University of Warsaw, Zaremba completed several internships for American technology company NVIDIA.\n\nIn the following years, Zaremba worked on an internship at Google where he reproduced state-of-the-art object-recognition modeling developed originally by DNNresearch. Elements of this model were used in Google+’s photo search feature.\n\nNext, he worked at Google Brain, and in the following year, Zaremba spent time at Facebook AI Research under the supervision of Prof. Rob Fergus and Prof. Yann LeCun\n\nIn 2015, Zaremba was one of the 10 co-founders of OpenAI, a non-profit artificial intelligence (AI) research company. The other co-founders were Ilya Sutskever, Greg Brockman, formerly the CTO of Stripe, Trevor Blackwell, Vicki Cheung, Andrej Karpathy, Durk Kingma, John Schulman, and Pamela Vagata.\nZaremba sits on the advisory board of Growbots, a Silicon Valley startup company aiming to automate sales processes with the use of machine learning and artificial intelligence.\n\n",
    "id": "51237053",
    "title": "Wojciech Zaremba"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=14842794",
    "text": "Artificial imagination\n\nArtificial imagination, also called synthetic imagination or machine imagination, is defined as the artificial simulation of human imagination by general or special purpose computers or artificial neural networks.\n\nThe term artificial imagination is also used to describe a property of machines or programs. Some of the traits that researchers hope to simulate include creativity, vision, digital art, humor, and satire.\n\nArtificial imagination research uses tools and insights from many fields, including computer science, rhetoric, psychology, creative arts, philosophy, neuroscience, affective computing, Artificial Intelligence, cognitive science, linguistics, operations research, creative writing, probability and logic.\n\nPractitioners in the field are researching various aspects of Artificial imagination, such as Artificial (visual) imagination,\nArtificial (aural) Imagination, modeling/filtering content based on human emotions and Interactive Search. Some articles on the topic speculate on how artificial imagination may evolve to create an artificial world \"people may be comfortable enough to escape from the real world\".\n\nSome researchers such as G. Schleis and M. Rizki have focused on using artificial neural networks to simulate artificial imagination.\n\nAnother important project is being led by Hiroharu Kato and Tatsuya Harada at the University of Tokyo in Japan. They have developed a computer capable to translate a description of an object into an image, which could be the easiest way to define what imagination is. Their idea is based on the concept of image as a series of pixels divided into short sequences that correspond to a specific part of an image. The scientists call this sequences “visual words” and those can be interpreted by the machine using statistical distribution to read an create an image of an object that in fact the machine does not knows how it look like.\n\nThe topic of artificial imagination has gotten interest from scholars outside the computer science domain, such as noted communications scholar Ernest Bormann, who came up with the Symbolic Convergence Theory and worked on a project to develop artificial imagination in computer systems. An interdisciplinary research seminar on artificial imagination and postdigital art has been taking place since 2017 at the Ecole Normale Supérieure in Paris.\n\n\"How to Build a Mind: Toward Machines with Imagination\" by Igor Aleksander is a good academic book on the topic; \"Artificial Imagination\", a roman à clef, is a good non-academic book supposedly written by an Artificial imagination system.\n\nThe typical application of artificial imagination is for an interactive search. Interactive searching has been developed since the mid-1990s, accompanied by the world wide web's development and the optimization of search engines. Based on the first query and feedback from a user, the databases to be searched are reorganized to improve the searching results.\n\n\"How artificial imagination can contribute to interactive search\"\n\nArtificial imagination allows us to synthesize images and to develop a new image, whether it is in the database, regardless its existence in the real world. For example, the computer shows results that are based on the answer from the initial query. The user selects several relevant images, and then the technology analyzes these selections and reorganizes the images' ranks to fit the query. In this process, artificial imagination is used to synthesize the selected images and to improve the searching result with additional relevant synthesized images. This technique is based on several algorithms, including the Rocchio algorithm and the evolutionary algorithm. The \"Rocchio algorithm\", locating a query point near relevant examples and far away from irrelevant examples, is simple and works well in a small system where the databases are arranged in certain ranks. The \"evolutionary synthesis\" is composed of two steps: a standard algorithm and an enhancement of the standard algorithm. Through feedback from the user, there would be additional images synthesized so as to be suited to what the user is looking for.\n\nArtificial imagination has a more general definition and wide applications. The traditional fields of artificial imagination include visual imagination and aural imagination. More generally, all the actions to form ideas, images and concepts can be linked to imagination. Thus, artificial imagination means more than only generating graphs. For example, moral imagination is an important research subfield of artificial imagination, although classification of artificial imagination is difficult. \n\nMorals are an important part to human beings' logic, while artificial morals are important in artificial imagination and artificial intelligence. A common criticism of artificial intelligence is whether human beings should take responsible for machines‘ mistake or decisions and how to develop well-behaved machines. As nobody can give a clear description of the best moral rules, it is impossible to create machines with commonly accepted moral rules. However, recent research about artificial morals circumvent the definition of moral. Instead, machine learning methods are applied to train machines to imitate human morals. As the data about moral decisions from thousands of different people are considered, the trained moral model can reflect widely accepted rules. \n\nMemory is another big field of artificial imagination. Researchers like Dr. Aude Oliva have done extensive work on artificial memory, especially visual memory. Compared to visual imagination, the visual memory focuses more on how machine understand, analyse and store pictures in a human way. In addition, characters like spatial features are also considered. As this field is based on the brains' biological structures, extensive research on neuroscience has also been done, which makes it a large intersection between biology and computer science.\n",
    "id": "14842794",
    "title": "Artificial imagination"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=52642349",
    "text": "AIVA\n\nAIVA (Artificial Intelligence Virtual Artist) is a deep learning algorithm applied to music composition. In June 2016, it became the first system of algorithmic composition to be registered, as a composer, in an authors' right Society SACEM.\n\nCreated in February 2016, AIVA specializes in Classical and Symphonic music composition. It became the world’s first virtual composer to be recognized by a music society (SACEM).\nBy reading a large collection of existing works of classical music (written by human composers such as Bach, Beethoven, Mozart) AIVA is capable of understanding concepts of music theory and composing on its own. The algorithm AIVA is based on deep learning and reinforcement learning architectures\n\nAIVA is a published composer; its first studio album “Genesis” was released in November 2016 and counts 20 original and 4 orchestrated works composed by AIVA. The tracks were recorded by human musicians:\nOlivier Hecho as the Conductor of the Aiva Sinfonietta Orchestra and Eric Breton as a Pianist. \n\n\nTrack listing:\n\nAvignon Symphonic Orchestra [ORAP] also performed Aiva's compositions in April 2017.\n\nThis is the preview of the score Op. n°3 for piano solo \"A little chamber music\", composed by AIVA.\n\n",
    "id": "52642349",
    "title": "AIVA"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1092923",
    "text": "Google\n\nGoogle LLC is an American multinational technology company that specializes in Internet-related services and products. These include online advertising technologies, search, cloud computing, software, and hardware. Google was founded in 1998 by Larry Page and Sergey Brin while they were Ph.D. students at Stanford University, in California. Together, they own about 14 percent of its shares, and control 56 percent of the stockholder voting power through supervoting stock. They incorporated Google as a privately held company on September 4, 1998. An initial public offering (IPO) took place on August 19, 2004, and Google moved to its new headquarters in Mountain View, California, nicknamed the Googleplex. In August 2015, Google announced plans to reorganize its various interests as a conglomerate called Alphabet Inc. Google, Alphabet's leading subsidiary, will continue to be the umbrella company for Alphabet's Internet interests. Upon completion of the restructure, Sundar Pichai was appointed CEO of Google; he replaced Larry Page, who became CEO of Alphabet.\n\nThe company's rapid growth since incorporation has triggered a chain of products, acquisitions, and partnerships beyond Google's core search engine (Google Search). It offers services designed for work and productivity (Google Docs, Sheets, and Slides), email (Gmail/Inbox), scheduling and time management (Google Calendar), cloud storage (Google Drive), social networking (Google+), instant messaging and video chat (Google Allo/Duo/Hangouts), language translation (Google Translate), mapping and turn-by-turn navigation (Google Maps/Waze/Earth/Street View), video sharing (YouTube), notetaking (Google Keep), and photo organizing and editing (Google Photos). The company leads the development of the Android mobile operating system, the Google Chrome web browser, and Chrome OS, a lightweight operating system based on the Chrome browser. Google has moved increasingly into hardware; from 2010 to 2015, it partnered with major electronics manufacturers in the production of its Nexus devices, and in October 2016, it released multiple hardware products (including the Google Pixel smartphone, Home smart speaker, Wifi mesh wireless router, and Daydream View virtual reality headset). The new hardware chief, Rick Osterloh, stated: \"a lot of the innovation that we want to do now ends up requiring controlling the end-to-end user experience\". Google has also experimented with becoming an Internet carrier. In February 2010, it announced Google Fiber, a fiber-optic infrastructure that was installed in Kansas City; in April 2015, it launched Project Fi in the United States, combining Wi-Fi and cellular networks from different providers; and in 2016, it announced the Google Station initiative to make public Wi-Fi available around the world, with initial deployment in India.\n\nAlexa, a company that monitors commercial web traffic, lists Google.com as the most visited website in the world. Several other Google services also figure in the top 100 most visited websites, including YouTube and Blogger. Google is the most valuable brand in the world as of 2017, but has received significant criticism involving issues such as privacy concerns, tax avoidance, antitrust, censorship, and search neutrality. Google's mission statement, from the outset, was \"to organize the world's information and make it universally accessible and useful\", and its unofficial slogan was \"Don't be evil\". In October 2015, the motto was replaced in the Alphabet corporate code of conduct by the phrase \"Do the right thing\".\n\n Google began in January 1996 as a research project by Larry Page and Sergey Brin when they were both PhD students at Stanford University in Stanford, California.\n\nWhile conventional search engines ranked results by counting how many times the search terms appeared on the page, the two theorized about a better system that analyzed the relationships among websites. They called this new technology PageRank; it determined a website's relevance by the number of pages, and the importance of those pages that linked back to the original site.\n\nPage and Brin originally nicknamed their new search engine \"BackRub\", because the system checked backlinks to estimate the importance of a site. Eventually, they changed the name to Google; the name of the search engine originated from a misspelling of the word \"googol\", the number 1 followed by 100 zeros, which was picked to signify that the search engine was intended to provide large quantities of information. Originally, Google ran under Stanford University's website, with the domains \"google.stanford.edu\" and \"z.stanford.edu\".\n\nThe domain name for Google was registered on September 15, 1997, and the company was incorporated on September 4, 1998. It was based in the garage of a friend (Susan Wojcicki) in Menlo Park, California. Craig Silverstein, a fellow PhD student at Stanford, was hired as the first employee.\n\nGoogle was initially funded by an August 1998 contribution of $100,000 from Andy Bechtolsheim, co-founder of Sun Microsystems; the money was given before Google was incorporated. Google received money from three other angel investors in 1998: Amazon.com founder Jeff Bezos, Stanford University computer science professor David Cheriton, and entrepreneur Ram Shriram.\n\nAfter some additional, small investments through the end of 1998 to early 1999, a new $25 million round of funding was announced on June 7, 1999, with major investors including the venture capital firms Kleiner Perkins Caufield & Byers and Sequoia Capital.\n\nEarly in 1999, Brin and Page decided they wanted to sell Google to Excite. They went to Excite CEO George Bell and offered to sell it to him for $1 million. He rejected the offer. Vinod Khosla, one of Excite's venture capitalists, talked the duo down to $750,000, but Bell still rejected it.\n\nGoogle's initial public offering (IPO) took place five years later, on August 19, 2004. At that time Larry Page, Sergey Brin, and Eric Schmidt agreed to work together at Google for 20 years, until the year 2024.\n\nAt IPO, the company offered 19,605,052 shares at a price of $85 per share. Shares were sold in an online auction format using a system built by Morgan Stanley and Credit Suisse, underwriters for the deal. The sale of $1.67 bn (billion) gave Google a market capitalization of more than $23bn. By January 2014, its market capitalization had grown to $397bn. The vast majority of the 271 million shares remained under the control of Google, and many Google employees became instant paper millionaires. Yahoo!, a competitor of Google, also benefitted because it owned 8.4 million shares of Google before the IPO took place.\n\nThere were concerns that Google's IPO would lead to changes in company culture. Reasons ranged from shareholder pressure for employee benefit reductions to the fact that many company executives would become instant paper millionaires. As a reply to this concern, co-founders Brin and Page promised in a report to potential investors that the IPO would not change the company's culture. In 2005, articles in \"The New York Times\" and other sources began suggesting that Google had lost its anti-corporate, no evil philosophy. In an effort to maintain the company's unique culture, Google designated a Chief Culture Officer, who also serves as the Director of Human Resources. The purpose of the Chief Culture Officer is to develop and maintain the culture and work on ways to keep true to the core values that the company was founded on: a flat organization with a collaborative environment. Google has also faced allegations of sexism and ageism from former employees. In 2013, a class action against several Silicon Valley companies, including Google, was filed for alleged \"no cold call\" agreements which restrained the recruitment of high-tech employees.\n\nThe stock performed well after the IPO, with shares hitting $350 for the first time on October 31, 2007, primarily because of strong sales and earnings in the online advertising market. The surge in stock price was fueled mainly by individual investors, as opposed to large institutional investors and mutual funds. GOOG shares split into GOOG class C shares and GOOGL class A shares. The company is listed on the NASDAQ stock exchange under the ticker symbols GOOGL and GOOG, and on the Frankfurt Stock Exchange under the ticker symbol GGQ1. These ticker symbols now refer to Alphabet Inc., Google's holding company, since the fourth quarter of 2015.\n\nIn March 1999, the company moved its offices to Palo Alto, California, which is home to several prominent Silicon Valley technology start-ups. The next year, Google began selling advertisements associated with search keywords against Page and Brin's initial opposition toward an advertising-funded search engine. In order to maintain an uncluttered page design, advertisements were solely text-based.\n\nThis model of selling keyword advertising was first pioneered by Goto.com, an Idealab spin-off created by Bill Gross. When the company changed names to Overture Services, it sued Google over alleged infringements of the company's pay-per-click and bidding patents. Overture Services would later be bought by Yahoo! and renamed Yahoo! Search Marketing. The case was then settled out of court; Google agreed to issue shares of common stock to Yahoo! in exchange for a perpetual license.\n\nIn 2001, Google received a patent for its PageRank mechanism. The patent was officially assigned to Stanford University and lists Lawrence Page as the inventor. In 2003, after outgrowing two other locations, the company leased an office complex from Silicon Graphics, at 1600 Amphitheatre Parkway in Mountain View, California. The complex became known as the Googleplex, a play on the word googolplex, the number one followed by a googol zeroes. The Googleplex interiors were designed by Clive Wilkinson Architects. Three years later, Google bought the property from SGI for $319 million. By that time, the name \"Google\" had found its way into everyday language, causing the verb \"google\" to be added to the \"Merriam-Webster Collegiate Dictionary\" and the \"Oxford English Dictionary\", denoted as: \"to use the Google search engine to obtain information on the Internet\". The first use of \"Google\" as a verb in pop culture happened on the TV series \"Buffy the Vampire Slayer\", in 2002.\n\nIn 2005, \"The Washington Post\" reported on a 700 percent increase in third-quarter profit for Google, largely thanks to large companies shifting their advertising strategies from newspapers, magazines, and television to the Internet. In January 2008, all the data that passed through Google's MapReduce software component had an aggregated size of 20 petabytes per day. In 2009, a CNN report about top political searches of 2009 noted that \"more than a billion searches\" are being typed into Google on a daily basis. In May 2011, the number of monthly unique visitors to Google surpassed one billion for the first time, an 8.4 percent increase from May 2010 (931 million).\n\nThe year 2012 was the first time that Google generated $50 billion in annual revenue, which topped the $38 billion that was generated the previous year. In January 2013, then-CEO Larry Page commented, \"We ended 2012 with a strong quarter ... Revenues were up 36% year-on-year, and 8% quarter-on-quarter. And we hit $50 billion in revenues for the first time last year – not a bad achievement in just a decade and a half.\"\n\nGoogle announced the launch of a new company, called Calico, on September 19, 2013, to be led by Apple, Inc. chairman Arthur Levinson. In the official public statement, Page explained that the \"health and well-being\" company would focus on \"the challenge of ageing and associated diseases\".\n\nGoogle celebrated its 15-year anniversary on September 27, 2013, and in 2016 it celebrated its 18th birthday with an animated Doodle shown on web browsers around the world. although it has used other dates for its official birthday. The reason for the choice of September 27 remains unclear, and a dispute with rival search engine Yahoo! Search in 2005 has been suggested as the cause.\n\nThe Alliance for Affordable Internet (A4AI) was launched in October 2013; Google is part of the coalition of public and private organizations that also includes Facebook, Intel, and Microsoft. Led by Sir Tim Berners-Lee, the A4AI seeks to make Internet access more affordable so that access is broadened in the developing world, where only 31% of people are online. Google will help to decrease Internet access prices so they fall below the UN Broadband Commission's worldwide target of 5% of monthly income.\n\nThe corporation's consolidated revenue for the third quarter of 2013 was reported in mid-October 2013 as $14.89 billion, a 12 percent increase compared to the previous quarter. Google's Internet business was responsible for $10.8 billion of this total, with an increase in the number of users' clicks on advertisements.\n\nAccording to Interbrand's annual Best Global Brands report, Google has been the second most valuable brand in the world (behind Apple Inc.) in 2013, 2014, 2015, and 2016, with a valuation of $133 billion.\n\nIn September 2015, Google engineering manager Rachel Potvin revealed details about Google's software code at an engineering conference. She revealed that the entire Google codebase, which spans every single service it develops, consists of over 2 billion lines of code. All that code is stored on a code repository available to all 25,000 Google engineers, and the code is regularly copied and updated on 10 Google data centers. To keep control, Potvin said Google has built its own \"version control system\", called \"Piper\", and that \"when you start a new project, you have a wealth of libraries already available to you. Almost everything has already been done.\" Engineers can make a single code change and deploy it on all services at the same time. The only major exceptions are that the PageRank search results algorithm is stored separately with only specific employee access, and the code for the Android operating system and the Google Chrome browser are also stored separately, as they don't run on the Internet. The \"Piper\" system spans 85 TB of data. Google engineers make 25,000 changes to the code each day, and on a weekly basis change approximately 15 million lines of code across 250,000 files. With that much code, automated bots have to help. Potvin reported, \"You need to make a concerted effort to maintain code health. And this is not just humans maintaining code health, but robots too.” Bots aren't writing code, but generating a lot of the data and configuration files needed to run the company's software. \"Not only is the size of the repository increasing,\" Potvin explained, \"but the rate of change is also increasing. This is an exponential curve.\"\n\nAs of October 2016, Google operates 70 offices in more than 40 countries. Alexa, a company that monitors commercial web traffic, lists Google.com as the most visited website in the world. Several other Google services also figure in the top 100 most visited websites, including YouTube and Blogger.\n\nIn 2001, Google acquired Deja News, the operators of a large archive of materials from Usenet. Google rebranded the archive as Google Groups, and by the end of the year, it had expanded the history back to 1981. \n\nIn April 2003, Google acquired Applied Semantics, a company specializing in making software applications for the online advertising space. The AdSense contextual advertising technology developed by Applied Semantics was adopted into Google's advertising efforts.\n\nIn 2004, Google acquired Keyhole, Inc. Keyhole's eponymous product was later renamed Google Earth.\n\nIn 2005. Google acquired Urchin Software in April 2005, using their Urchin on Demand product (along with ideas from Adaptive Path's Measure Map) to create Google Analytics in 2006.\n\nIn October 2006, Google announced that it had acquired the video-sharing site YouTube for $1.65 billion in Google stock, and the deal was finalized on November 13, 2006.\n\nOn April 13, 2007, Google reached an agreement to acquire DoubleClick for $3.1 billion, transferring to Google valuable relationships that DoubleClick had with Web publishers and advertising agencies.\nThe deal was approved despite anti-trust concerns raised by competitors Microsoft and AT&T.\n\nIn addition to the many companies Google has purchased, the firm has partnered with other organizations for research, advertising, and other activities. In 2005, Google partnered with NASA Ames Research Center to build of offices.\n\nIn 2005 Google partnered with AOL to enhance each other's video search services. In 2006 Google and Fox Interactive Media of News Corporation entered into a $900 million agreement to provide search and advertising on the then-popular social networking site MySpace.\n\nIn 2007, Google began sponsoring NORAD Tracks Santa, displacing the former sponsor AOL. NORAD Tracks Santa purports to follow Santa Claus' progress on Christmas Eve, using Google Earth to \"track Santa\" in 3-D for the first time. \n\nIn 2008, Google developed a partnership with GeoEye to launch a satellite providing Google with high-resolution (0.41 m monochrome, 1.65 m color) imagery for Google Earth. The satellite was launched from Vandenberg Air Force Base on September 6, 2008. Google also announced in 2008 that it was hosting an archive of \"Life Magazine\"s photographs.\n\nIn 2010, Google Energy made its first investment in a renewable energy project, putting $38.8 million into two wind farms in North Dakota. The company announced the two locations will generate 169.5 megawatts of power, enough to supply 55,000 homes. The farms, which were developed by NextEra Energy Resources, will reduce fossil fuel use in the region and return profits. NextEra Energy Resources sold Google a twenty-percent stake in the project to get funding for its development. In February 2010, the Federal Energy Regulatory Commission FERC granted Google an authorization to buy and sell energy at market rates. The order specifically states that Google Energy—a subsidiary of Google—holds the rights \"for the sale of energy, capacity, and ancillary services at market-based rates\", but acknowledges that neither Google Energy nor its affiliates \"own or control any generation or transmission\" facilities. The corporation exercised this authorization in September 2013 when it announced it would purchase all the electricity produced by the not-yet-built 240-megawatt Happy Hereford wind farm.\n\nAlso in 2010, Google purchased Global IP Solutions, a Norway-based company that provides web-based teleconferencing and other related services. This acquisition enabled Google to add telephone-style services to its list of products. On May 27, 2010, Google announced it had also closed the acquisition of the mobile ad network AdMob. This occurred days after the Federal Trade Commission closed its investigation into the purchase. Google acquired the company for an undisclosed amount. In July 2010, Google signed an agreement with an Iowa wind farm to buy 114 megawatts of energy for 20 years.\n\nOn April 4, 2011, \"The Globe and Mail\" reported that Google bid $900 million for 6000 Nortel Networks patents.\n\nOn August 15, 2011, Google made its largest-ever acquisition to-date when it announced that it would acquire Motorola Mobility for $12.5 billion subject to approval from regulators in the United States and Europe. In a post on Google's blog, Google Chief Executive and co-founder Larry Page revealed that the acquisition was a strategic move to strengthen Google's patent portfolio. The company's Android operating system has come under fire in an industry-wide patent battle, as Apple and Microsoft have sued Android device makers such as HTC, Samsung, and Motorola. The merger was completed on May 22, 2012, after the approval of People's Republic of China.\n\nThis purchase was made in part to help Google gain Motorola's considerable patent portfolio on mobile phones and wireless technologies, to help protect Google in its ongoing patent disputes with other companies, mainly Apple and Microsoft, and to allow it to continue to freely offer Android. After the acquisition closed, Google began to restructure the Motorola business to fit Google's strategy. On August 13, 2012, Google announced plans to lay off 4000 Motorola Mobility employees. On December 10, 2012, Google sold the manufacturing operations of Motorola Mobility to Flextronics for $75 million. As a part of the agreement, Flextronics will manufacture undisclosed Android and other mobile devices. On December 19, 2012, Google sold the Motorola Home business division of Motorola Mobility to Arris Group for $2.35 billion in a cash-and-stock transaction. As a part of this deal, Google acquired a 15.7% stake in Arris Group valued at $300 million.\n\nIn June 2013, Google acquired Waze, a $966 million deal. While Waze would remain an independent entity, its social features, such as its crowdsourced location platform, were reportedly valuable integrations between Waze and Google Maps, Google's own mapping service.\n\nOn January 26, 2014, Google announced it had agreed to acquire DeepMind Technologies, a privately held artificial intelligence company from London. DeepMind describes itself as having the ability to combine the best techniques from machine learning and systems neuroscience to build general-purpose learning algorithms. DeepMind's first commercial applications were used in simulations, e-commerce and games. As of December 2013, it was reported that DeepMind had roughly 75 employees. Technology news website \"Recode\" reported that the company was purchased for $400 million though it was not disclosed where the information came from. A Google spokesman would not comment of the price. The purchase of DeepMind aids in Google's recent growth in the artificial intelligence and robotics community.\n\nOn January 29, 2014, Google announced that it would divest Motorola Mobility to Lenovo for $2.91 billion, a fraction of the original $12.5 billion price paid by Google to acquire the company. Google retained all but 2000 of Motorola's patents and entered into cross-licensing deals.\n\nOn September 21, 2017, HTC announced a \"cooperation agreement\" in which it would sell non-exclusive rights to certain intellectual property, as well as smartphone talent, to Google for $1.1 billion.\n\nAs of 2016, Google owned and operated nine data centers across North and South America, two in Asia, and four in Europe.\n\nIn 2011, the company had announced plans to build three data centers at a cost of more than $200 million in Asia (Singapore, Hong Kong and Taiwan) and said they would be operational within two years. In December 2013, Google announced that it had scrapped the plan to build a data center in Hong Kong.\n\nIn October 2013, \"The Washington Post\" reported that the U.S. National Security Agency intercepted communications between Google's data centers, as part of a program named MUSCULAR. This wiretapping was made possible because Google did not encrypt data passed inside its own network. Google began encrypting data sent between data centers in 2013.\n\nGoogle's most efficient data center runs at using only fresh air cooling, requiring no electrically powered air conditioning; the servers run so hot that humans cannot go near them for extended periods.\n\nAn August 2011 report estimated that Google had about 900,000 servers in their data centers, based on energy usage. The report does state that \"Google never says how many servers are running in its data centers.\"\n\nIn December 2016, Google announced that starting in 2017, it will power all of its data centers, as well as all of its offices, from 100% renewable energy. The commitment will make Google \"the world's largest corporate buyer of renewable power, with commitments reaching 2.6 gigawatts (2,600 megawatts) of wind and solar energy\". Google also stated that it does not count that as its final goal; it says that \"since the wind doesn't blow 24 hours a day, we'll also broaden our purchases to a variety of energy sources that can enable renewable power, every hour of every day\". Additionally, the project will \"help support communities\" around the world, as the purchase commitments will \"result in infrastructure investments of more than $3.5 billion globally\", and will \"generate tens of millions of dollars per year in revenue to local property owners, and tens of millions more to local and national governments in tax revenue\".\n\nOn August 10, 2015, Google announced plans to reorganize its various interests as a conglomerate called Alphabet. Google became Alphabet's leading subsidiary, and will continue to be the umbrella company for Alphabet's Internet interests. Upon completion of the restructure, Sundar Pichai became CEO of Google, replacing Larry Page, who became CEO of Alphabet.\n\nOn September 1, 2017, Google Inc. announced its plans of restructuring as a limited liability company, Google LLC, as a wholly owned subsidiary of XXVI Holdings Inc., which is formed as a subsidiary of Alphabet Inc. to hold the equity of its other subsidiaries, including Google LLC and other bets.\n\nFor the 2006 fiscal year, the company reported $10.492 billion in total advertising revenues and only $112 million in licensing and other revenues. In 2011, 96% of Google's revenue was derived from its advertising programs. In addition to its own algorithms for understanding search requests, Google uses technology from the company DoubleClick, to project user interest and target advertising to the search context and the user history.\n\nIn 2007, Google launched \"AdSense for Mobile\", taking advantage of the emerging mobile advertising market.\n\nGoogle Analytics allows website owners to track where and how people use their website, for example by examining click rates for all the links on a page. Google advertisements can be placed on third-party websites in a two-part program. Google's AdWords allows advertisers to display their advertisements in the Google content network, through a cost-per-click scheme. The sister service, Google AdSense, allows website owners to display these advertisements on their website and earn money every time ads are clicked.\n\nOne of the criticisms of this program is the possibility of click fraud, which occurs when a person or automated script clicks on advertisements without being interested in the product, causing the advertiser to pay money to Google unduly. Industry reports in 2006 claimed that approximately 14 to 20 percent of clicks were fraudulent or invalid.\n\nIn February 2003, Google stopped showing the advertisements of Oceana, a non-profit organization protesting a major cruise ship's sewage treatment practices. Google cited its editorial policy at the time, stating \"Google does not accept advertising if the ad or site advocates against other individuals, groups, or organizations.\" In June 2008, Google reached an advertising agreement with Yahoo!, which would have allowed Yahoo! to feature Google advertisements on its web pages. The alliance between the two companies was never completely realized because of antitrust concerns by the U.S. Department of Justice. As a result, Google pulled out of the deal in November 2008.\n\nAccording to market research published by comScore in November 2009, Google Search is the dominant search engine in the United States market, with a market share of 65.6%. Google indexes billions of web pages, so that users can search for the information they desire through the use of keywords and operators.\n\nIn 2003, \"The New York Times\" complained about Google's indexing, claiming that Google's caching of content on its site infringed its copyright for the content. In this case, the United States District Court of Nevada ruled in favor of Google in \"Field v. Google\" and \"Parker v. Google\". The publication \"\" has compiled a list of words that the web giant's new instant search feature will not search.\n\nGoogle Watch has criticized Google's PageRank algorithms, saying that they discriminate against new websites and favor established sites.\n\nGoogle also hosts Google Books. The company began scanning books and uploading limited previews, and full books were allowed, into its new book search engine. The Authors Guild, a group that represents 8,000 U.S. authors, filed a class action suit in a New York City federal court against Google in 2005 over this service. Google replied that it is in compliance with all existing and historical applications of copyright laws regarding books. Google eventually reached a revised settlement in 2009 to limit its scans to books from the U.S., the UK, Australia, and Canada. Furthermore, the Paris Civil Court ruled against Google in late 2009, asking it to remove the works of La Martinière (Éditions du Seuil) from its database. In competition with Amazon.com, Google sells digital versions of new books.\n\nOn July 21, 2010, in response to Bing, Google updated its image search to display a streaming sequence of thumbnails that enlarge when pointed at. Though web searches still appear in a batch per page format, on July 23, 2010, dictionary definitions for certain English words began appearing above the linked results for web searches.\n\nThe \"Hummingbird\" update to the Google search engine was announced in September 2013. The update was introduced over the month prior to the announcement and allows users ask the search engine a question in natural language rather than entering keywords into the search box.\n\nIn August 2016, Google announced two major changes related to its mobile search results. The first, removing the \"mobile-friendly\" label that highlighted pages were easy to read on mobile from its mobile search results page. The second, on January 10, 2017, the company will start punishing mobile pages that show intrusive interstitials when a user first opens a page and they will rank lower in its search results.\n\nIn May 2017, Google enabled a new \"Personal\" tab in Google Search, letting users search for content in their Google accounts' various services, including email messages from Gmail and photos from Google Photos.\n\nG Suite is a monthly subscription offering for organizations and businesses to get access to a collection of Google's services, including Gmail, Google Drive and Docs, Sheets, and Slides, with additional administrative tools, unique domain names, and 24/7 support.\nGoogle Search Appliance was launched in February 2002, targeted toward providing search technology for larger organizations. Google launched the Mini three years later, which was targeted at smaller organizations. Late in 2006, Google began to sell Custom Search Business Edition, providing customers with an advertising-free window into Google.com's index. The service was renamed Google Site Search in 2008. Site Search customers were notified by email in late March 2017 that no new licenses for Site Search would be sold after April 1, 2017, but that customer and technical support would be provided for the duration of existing license agreements.\n\nOn March 15, 2016, Google announced the introduction of Google Analytics 360 Suite, \"a set of integrated data and marketing analytics products, designed specifically for the needs of enterprise-class marketers.\" Among other things, the suite is designed to help \"enterprise class marketers\" \"see the complete customer journey\", generate \"useful insights\", and \"deliver engaging experiences to the right people\". Jack Marshall of \"The Wall Street Journal\" wrote that the suite competes with existing marketing cloud offerings by companies including Adobe, Oracle, Salesforce, and IBM.\n\nGoogle offers Gmail, and the newer variant Inbox, for email, Google Calendar for time-management and scheduling, Google Maps for mapping, navigation and satellite imagery, Google Drive for cloud storage of files, Google Docs, Sheets and Slides for productivity, Google Photos for photo storage and sharing, Google Keep for note-taking, Google Translate for language translation, YouTube for video viewing and sharing, and Google+, Allo, and Duo for social interaction.\n\nGoogle develops the Android mobile operating system, as well as its smartwatch, television, car, and Internet of things-enabled smart devices variations.\n\nIt also develops the Google Chrome web browser, and Chrome OS, an operating system based on Chrome.\n\nIn January 2010, Google released Nexus One, the first Android phone under its own, \"Nexus\", brand. It spawned a number of phones and tablets under the \"Nexus\" branding until its eventual discontinuation in 2016, replaced by a new brand called, Pixel.\n\nIn 2011, the Chromebook was introduced, described as a \"new kind of computer\" running Chrome OS.\n\nIn July 2013, Google introduced the Chromecast dongle, that allows users to stream content from their smartphones to televisions.\n\nIn June 2014, Google announced Google Cardboard, a simple cardboard viewer that lets user place their smartphone in a special front compartment to view virtual reality (VR) media.\n\nIn April 2016, \"Recode\" reported that Google had hired Rick Osterloh, Motorola Mobility's former President, to head Google's new hardware division. In October 2016, Osterloh stated that \"a lot of the innovation that we want to do now ends up requiring controlling the end-to-end user experience\", and Google announced several hardware platforms:\n\nIn February 2010, Google announced the Google Fiber project, with experimental plans to build an ultra-high-speed broadband network for 50,000 to 500,000 customers in one or more American cities. Following Google's corporate restructure to make Alphabet Inc. its parent company, Google Fiber was moved to Alphabet's Access division.\n\nIn April 2015, Google announced Project Fi, a mobile virtual network operator, that combines Wi-Fi and cellular networks from different telecommunication providers in an effort to enable seamless connectivity and fast Internet signal.\n\nIn September 2016, Google began its Google Station initiative, a project for public Wi-Fi at railway stations in India. Caesar Sengupta, VP for Google's next billion users, told \"The Verge\" that 15,000 people get online for the first time thanks to Google Station and that 3.5 million people use the service every month. The expansion meant that Google was looking for partners around the world to further develop the initiative, which promised \"high-quality, secure, easily accessible Wi-Fi\". By December, Google Station had been deployed at 100 railway stations, and in February, Google announced its intention to expand beyond railway stations, with a plan to bring citywide Wi-Fi to Pune.\n\nGoogle launched its Google News service in 2002, an automated service which summarizes news articles from various websites. In March 2005, Agence France Presse (AFP) sued Google for copyright infringement in federal court in the District of Columbia, a case which Google settled for an undisclosed amount in a pact that included a license of the full text of AFP articles for use on Google News.\n\nIn May 2011, Google announced Google Wallet, a mobile application for wireless payments.\n\nIn 2013, Google launched Google Shopping Express, a delivery service initially available only in San Francisco and Silicon Valley.\n\nGoogle Alerts is a content change detection and notification service, offered by the search engine company Google. The service sends emails to the user when it finds new results—such as web pages, newspaper articles, or blogs—that match the user's search term.\n\nIn July 2015 Google released DeepDream, an image recognition software capable of creating psychedelic images using a convolutional neural network.\n\nGoogle introduced its Family Link service in March 2017, letting parents buy Android Nougat-based Android devices for kids under 13 years of age and create a Google account through the app, with the parents controlling the apps installed, monitor the time spent using the device, and setting a \"Bedtime\" feature that remotely locks the device.\n\nIn April 2017, Google launched AutoDraw, a web-based tool using artificial intelligence and machine learning to recognize users' drawings and replace scribbles with related stock images that have been created by professional artists. The tool is built using the same technology as QuickDraw, an experimental game from Google's Creative Lab where users were tasked with drawing objects that algorithms would recognize within 20 seconds.\n\nIn May 2017, Google added \"Family Groups\" to several of its services. The feature, which lets users create a group consisting of their family members' individual Google accounts, lets users add their \"Family Group\" as a collaborator to shared albums in Google Photos, shared notes in Google Keep, and common events in Google Calendar. At announcement, the feature is limited to Australia, Brazil, Canada, France, Germany, Ireland, Italy, Japan, Mexico, New Zealand, Russia, Spain, United Kingdom and United States.\n\nGoogle APIs are a set of application programming interfaces (APIs) developed by Google which allow communication with Google Services and their integration to other services. Examples of these include Search, Gmail, Translate or Google Maps. Third-party apps can use these APIs to take advantage of or extend the functionality of the existing services.\n\nGoogle Developers is Google's site for software development tools, APIs, and technical resources. The site contains documentation on using Google developer tools and APIs—including discussion groups and blogs for developers using Google's developer products.\n\nGoogle Labs was a page created by Google to demonstrate and test new projects.\n\nGoogle owns the top-level domain 1e100.net which is used for some servers within Google's network. The name is a reference to the scientific E notation representation for 1 googol, .\n\nIn March 2017, Google launched a new website, opensource.google.com, to publish its internal documentation for Google Open Source projects.\n\nIn June 2017, Google launched \"We Wear Culture\", a searchable archive of 3,000 years of global fashion. The archive, a result of collaboration between Google and over 180 museums, schools, fashion institutes, and other organizations, also offers curated exhibits of specific fashion topics and their impact on society.\n\nOn \"Fortune\" magazine's list of the best companies to work for, Google ranked first in 2007, 2008 and 2012 and fourth in 2009 and 2010. Google was also nominated in 2010 to be the world's most attractive employer to graduating students in the Universum Communications talent attraction index. Google's corporate philosophy includes principles such as \"you can make money without doing evil,\" \"you can be serious without a suit,\" and \"work should be challenging and the challenge should be fun.\"\n\nAs of the second quarter in 2015, Google has 57,100 employees. Google has released that 30 percent of their employees are female, and 70 percent are male. A March 2013 report detailed that it had 10,000 developers based in more than 40 offices.\n\nGoogle's employees are hired based on a hierarchical system. Employees are split into six hierarchies based on experience and can range \"from entry-level data center workers at level one to managers and experienced engineers at level six.\" \n\nAfter the company's IPO in 2004, founders Sergey Brin and Larry Page and CEO Eric Schmidt requested that their base salary be cut to $1. Subsequent offers by the company to increase their salaries were turned down, primarily because their main compensation continues to come from owning stock in Google. Before 2004, Schmidt made $250,000 per year, and Page and Brin each received an annual salary of $150,000.\n\nIn March 2008, Sheryl Sandberg, then vice-president of global online sales and operations, began her position as chief operating officer of Facebook.\nIn 2009, early employee Tim Armstrong left to become CEO of AOL. In July 2012, Google's first female engineer, Marissa Mayer, left Google to become Yahoo!'s CEO.\n\nIn 2017 former Intel executive Diane Bryant became Chief Operating Officer of Google Cloud.\nAs a motivation technique, Google uses a policy often called Innovation Time Off, where Google engineers are encouraged to spend 20% of their work time on projects that interest them. Some of Google's services, such as Gmail, Google News, Orkut, and AdSense originated from these independent endeavors. In a talk at Stanford University, Marissa Mayer, Google's Vice President of Search Products and User Experience until July 2012, showed that half of all new product launches in the second half of 2005 had originated from the Innovation Time Off.\n\nGoogle's headquarters in Mountain View, California, is referred to as \"the Googleplex\", a play on words on the number googolplex and the headquarters itself being a \"complex\" of buildings. The lobby is decorated with a piano, lava lamps, old server clusters, and a projection of search queries on the wall. The hallways are full of exercise balls and bicycles. Many employees have access to the corporate recreation center. Recreational amenities are scattered throughout the campus and include a workout room with weights and rowing machines, locker rooms, washers and dryers, a massage room, assorted video games, table football, a baby grand piano, a billiard table, and ping pong. In addition to the recreation room, there are snack rooms stocked with various foods and drinks, with special emphasis placed on nutrition. Free food is available to employees 24/7, with the offerings provided by paid vending machines prorated based on and favoring those of better nutritional value.\n\nGoogle's extensive amenities are not available to all of its workers. Temporary workers such as book scanners do not have access to shuttles, Google cafes, or other perks.\n\nIn 2006, Google moved into about of office space in New York City, at 111 Eighth Avenue in Manhattan. The office was designed and built specially for Google, and houses its largest advertising sales team, which has been instrumental in securing large partnerships. The New York headquarters includes a game room, micro-kitchens, and a video game area. In 2010, Google bought the building housing the headquarter, in a deal that valued the property at around $1.9 billion, the biggest for a single building in the United States that year. In February 2012, Google moved additional employees to the New York City campus, with a total of around 2,750 employees.\n\nBy late 2006, Google established a new headquarters for its AdWords division in Ann Arbor, Michigan. In November 2006, Google opened offices on Carnegie Mellon's campus in Pittsburgh, focusing on shopping-related advertisement coding and smartphone applications and programs. Other office locations in the U.S. include Atlanta, Georgia; Austin, Texas; Boulder, Colorado; Cambridge, Massachusetts; San Francisco, California; Seattle, Washington; Reston, Virginia, and Washington, D.C.\n\nIn October 2006, the company announced plans to install thousands of solar panels to provide up to 1.6 megawatts of electricity, enough to satisfy approximately 30% of the campus' energy needs. The system will be the largest solar power system constructed on a U.S. corporate campus and one of the largest on any corporate site in the world. In addition, Google announced in 2009 that it was deploying herds of goats to keep grassland around the Googleplex short, helping to prevent the threat from seasonal bush fires while also reducing the carbon footprint of mowing the extensive grounds. The idea of trimming lawns using goats originated from Bob Widlar, an engineer who worked for National Semiconductor. In 2008, Google faced accusations in \"Harper's Magazine\" of being an \"energy glutton\". The company was accused of employing its \"Don't be evil\" motto and its public energy-saving campaigns to cover up or make up for the massive amounts of energy its servers require.\n\nInternationally, Google has over 70 offices in more than 40 countries. It also has product research and development operations in cities around the world, namely Sydney (birthplace location of Google Maps) and London (part of Android development).\n\nIn November 2013, Google announced plans for a new London headquarter, a notable 1 million square foot office able to accommodate 4,500 employees. Recognized as one of the biggest ever commercial property acquisitions at the time of the deal's announcement in January, Google submitted plans for the new headquarter to the Camden Council in June 2017. The new building, if approved, will feature a rooftop garden with a running track, giant moving blinds, a swimming pool, and a multi-use games area for sports.\n\nIn May 2015, Google announced its intention to create its own campus in Hyderabad, India. The new campus, reported to be the company's largest outside the United States, will accommodate 13,000 employees.\n\nSince 1998, Google has been designing special, temporary alternate logos to place on their homepage intended to celebrate holidays, events, achievements and people. The first Google Doodle was in honor of the Burning Man Festival of 1998. The doodle was designed by Larry Page and Sergey Brin to notify users of their absence in case the servers crashed. Subsequent Google Doodles were designed by an outside contractor, until Larry and Sergey asked then-intern Dennis Hwang to design a logo for Bastille Day in 2000. From that point onward, Doodles have been organized and created by a team of employees termed \"Doodlers\".\n\nGoogle has a tradition of creating April Fools' Day jokes. On April 1, 2000, Google MentalPlex allegedly featured the use of mental power to search the web. In 2007, Google announced a free Internet service called TiSP, or Toilet Internet Service Provider, where one obtained a connection by flushing one end of a fiber-optic cable down their toilet. Also in 2007, Google's Gmail page displayed an announcement for Gmail Paper, allowing users to have email messages printed and shipped to them.\nIn 2008, Google announced Gmail Custom time where users could change the time that the email was sent.\n\nIn 2010, Google changed its company name to Topeka in honor of Topeka, Kansas, whose mayor changed the city's name to Google for a short amount of time in an attempt to sway Google's decision in its new Google Fiber Project. In 2011, Google announced Gmail Motion, an interactive way of controlling Gmail and the computer with body movements via the user's webcam.\n\nGoogle's services contain easter eggs, such as the Swedish Chef's \"Bork bork bork,\" Pig Latin, \"Hacker\" or leetspeak, Elmer Fudd, Pirate, and Klingon as language selections for its search engine. The search engine calculator provides the Answer to the Ultimate Question of Life, the Universe, and Everything from Douglas Adams' \"The Hitchhiker's Guide to the Galaxy\". When searching the word \"recursion\", the spell-checker's result for the properly spelled word is exactly the same word, creating a recursive link.\n\nWhen searching for the word \"anagram,\" meaning a rearrangement of letters from one word to form other valid words, Google's suggestion feature displays \"Did you mean: nag a ram?\" In Google Maps, searching for directions between places separated by large bodies of water, such as Los Angeles and Tokyo, results in instructions to \"kayak across the Pacific Ocean.\" During FIFA World Cup 2010, search queries including \"World Cup\" and \"FIFA\" caused the \"Goooo...gle\" page indicator at the bottom of every result page to read \"Goooo...al!\" instead.\n\nIn 2004, Google formed the not-for-profit philanthropic Google.org, with a start-up fund of $1 billion. The mission of the organization is to create awareness about climate change, global public health, and global poverty. One of its first projects was to develop a viable plug-in hybrid electric vehicle that can attain 100 miles per gallon. Google hired Larry Brilliant as the program's executive director in 2004, and the current director is Megan Smith.\n\nIn 2008, Google announced its \"project 10\" which accepted ideas for how to help the community and then allowed Google users to vote on their favorites. After two years of silence, during which many wondered what had happened to the program, Google revealed the winners of the project, giving a total of ten million dollars to various ideas ranging from non-profit organizations that promote education to a website that intends to make all legal documents public and online.\n\nIn 2011, Google donated 1 million euros to International Mathematical Olympiad to support the next five annual International Mathematical Olympiads (2011–2015). In July 2012, Google launched a \"Legalize Love\" campaign in support of gay rights.\n\nGoogle uses various tax avoidance strategies. Out of the five largest American technology companies, it pays the lowest taxes to the countries of origin of its revenues. Google between 2007 and 2010 saved $3.1 billion in taxes by shuttling non-U.S. profits through Ireland and the Netherlands and then to Bermuda. Such techniques lower its non-U.S. tax rate to 2.3 per cent, while normally the corporate tax rate in for instance the UK is 28 per cent. This has reportedly sparked a French investigation into Google's transfer pricing practices.\n\nFollowing criticism of the amount of corporate taxes that Google paid in the United Kingdom, Chairman Eric Schmidt said, \"It's called capitalism. We are proudly capitalistic.\" During the same December 2012 interview, Schmidt confirmed that the company had no intention of paying more to the UK exchequer. \n\nGoogle Vice President Matt Brittin testified to the Public Accounts Committee of the UK House of Commons that his UK sales team made no sales and hence owed no sales taxes to the UK. In January 2016, Google reached a settlement with the UK to pay £130m in back taxes plus higher taxes in future.\n\nSince 2007, Google has aimed for carbon neutrality in regard to its operations.\n\nGoogle disclosed in September 2011 that it \"continuously uses enough electricity to power 200,000 homes\", almost 260 million watts or about a quarter of the output of a nuclear power plant. Total carbon emissions for 2010 were just under 1.5 million metric tons, mostly due to fossil fuels that provide electricity for the data centers. Google said that 25 percent of its energy was supplied by renewable fuels in 2010. An average search uses only 0.3 watt-hours of electricity, so all global searches are only 12.5 million watts or 5% of the total electricity consumption by Google.\n\nIn 2007, Google launched a project centered on developing renewable energy, titled the \"Renewable Energy Cheaper than Coal (RE<C)\" project. However, the project was cancelled in 2014, after engineers Ross Koningstein and David Fork understood, after years of study, that \"best-case scenario, which was based on our most optimistic forecasts for renewable energy, would still result in severe climate change\", writing that they \"came to the conclusion that even if Google and others had led the way toward a wholesale adoption of renewable energy, that switch would not have resulted in significant reductions of carbon dioxide emissions\".\n\nIn June 2013, \"The Washington Post\" reported that Google had donated $50,000 to the Competitive Enterprise Institute, a libertarian think tank that calls human carbon emissions a positive factor in the environment and argues that global warming is not a concern.\n\nIn July 2013, it was reported that Google had hosted a fundraising event for Oklahoma Senator Jim Inhofe, who has called climate change a \"hoax\". In 2014 Google cut ties with the American Legislative Exchange Council (ALEC) after pressure from the Sierra Club, major unions and Google's own scientists because of ALEC's stance on climate change and opposition to renewable energy.\n\nIn November 2017, Google bought 536 megawatts of wind power. The purchase made the firm reach 100% renewable energy. The wind energy comes from two power plants in South Dakota, one in Iowa and one in Oklahoma.\n\nIn 2013, Google ranked 5th in lobbying spending, up from 213th in 2003. In 2012, the company ranked 2nd in campaign donations of technology and Internet sections.\n\nGoogle has been involved in a number of lawsuits including the High-Tech Employee Antitrust Litigation which resulted in Google being one of four companies to pay a $415 million settlement to employees.\n\nOn June 27, 2017, the company received a record fine of from the European Union for \"promoting its own shopping comparison service at the top of search results.\" Commenting on the penalty, \"New Scientist\" magazine said: \"The hefty sum – the largest ever doled out by the EU's competition regulators – will sting in the short term, but Google can handle it. Alphabet, Google’s parent company, made a profit of $2.5 billion (€2.2 billion) in the first six weeks of 2017 alone. The real impact of the ruling is that Google must stop using its dominance as a search engine to give itself the edge in another market: online price comparisons.\" The company disputed the ruling.\n\nGoogle's market dominance has led to prominent media coverage, including criticism of the company over issues such as aggressive tax avoidance, search neutrality, copyright, censorship of search results and content, and privacy. Other criticisms include alleged misuse and manipulation of search results, its use of others' intellectual property, concerns that its compilation of data may violate people's privacy, and the energy consumption of its servers, as well as concerns over traditional business issues such as monopoly, restraint of trade, anti-competitive practices, and patent infringement.\n\nGoogle's mission statement, from the outset, was \"to organize the world's information and make it universally accessible and useful\", and its unofficial slogan was \"Don't be evil\". In October 2015, the motto was replaced in the Alphabet corporate code of conduct by the phrase: \"Do the right thing\". Google's commitment to such robust idealism has been increasingly called into doubt due to a number of the firm's actions and behaviours which appear to contradict this.\n\nFollowing media reports about PRISM, NSA's massive electronic surveillance program, in June 2013, several technology companies were identified as participants, including Google. According to leaks of said program, Google joined the PRISM program in 2009.\n\nOn August 8, 2017, Google fired employee James Damore after he distributed a memo throughout the company which argued that \"Google's ideological echo chamber\" and bias clouded their thinking about diversity and inclusion, and that it is also biological factors, not discrimination alone, that cause the average woman to be less interested than men in technical positions. Google CEO Sundar Pichai accused Damore in violating company policy by \"advancing harmful gender stereotypes in our workplace\", and he was fired on the same day. \"New York Times\" columnist David Brooks argued Pichai had mishandled the case, and called for his resignation.\n\nReportedly, Google's influenced New America think tank to expel their Open Markets research group, after the group has criticized Google monopolistic power and supported the EU $2.7B fine of Google.\n\nIn 2017, David Elliot and Chris Gillespie argued before the Ninth Circuit of the United States Court of Appeals that \"google\" had suffered genericide. The controversy began in 2012 when Gillespie acquired 763 domain names containing the word \"google.\" Google promptly filed a complaint with the NAF. Elliot then filed a petition for cancelling the Google trademark. Ultimately, the court ruled in favor of Google because Elliot failed to show a preponderance of evidence showing the genericide of \"google.\"\n\n\n",
    "id": "1092923",
    "title": "Google"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=51023476",
    "text": "Artificial empathy\n\nArtificial empathy (AE) is the development of AI systems − such as companion robots − that are able to detect and respond to human emotions. According to scientists, although the technology can be perceived as scary or threatening by many people, it could also have a significant advantage over humans in professions which are traditionally involved in emotional role-playing such as the health care sector. From the care-giver perspective for instance, performing emotional labor above and beyond the requirements of paid labor often results in chronic stress or burnout, and the development of a feeling of being desensitized to patients. However, it is argued that the emotional role-playing between the care-receiver and a robot can actually have a more positive outcome in terms of creating the conditions of less fear and concern for one's own predicament best exemplified by the phrase: \"if it is just a robot taking care of me it cannot be that critical.\" Scholars debate the possible outcome of such technology using two different perspectives. Either, the AE could help the socialization of care-givers, or serve as role model for emotional detachment.\n\nThere are a variety of philosophical, theoretical, and applicative questions related to AE. For example:\n\n",
    "id": "51023476",
    "title": "Artificial empathy"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=23886619",
    "text": "List of programming languages for artificial intelligence\n\nArtificial intelligence researchers have developed several specialized programming languages for artificial intelligence:\n\n\n\n\n",
    "id": "23886619",
    "title": "List of programming languages for artificial intelligence"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=562827",
    "text": "POP-11\n\nPOP-11 is a reflective, incrementally compiled programming language with many of the features of an interpreted language. It is the core language of the Poplog programming environment developed originally by the University of Sussex, and recently in the\nSchool of Computer Science at the\nUniversity of Birmingham which hosts\nthe Poplog website.\n\nPOP-11 is an evolution of the language POP-2, developed in Edinburgh University and features an open stack model (like Forth, among others). It is mainly procedural, but supports declarative language constructs, including a pattern matcher and is mostly used for research and teaching in Artificial Intelligence, although it has features sufficient for many other classes of problems. It is often used to introduce symbolic programming techniques to programmers of more conventional languages like Pascal, who find POP syntax more familiar than that of Lisp. One of POP-11's features is that it supports first-class functions.\n\nPop-11 is the core language of the Poplog system. The fact that the compiler and compiler subroutines are available at run-time (a requirement for incremental compilation) gives it the ability to support a far wider range of extensions than would be possible using only a macro facility. This made it possible for incremental compilers to be added for Prolog, Common Lisp and Standard ML, which could be added as required to support either mixed language development or development in the second language without using any Pop-11 constructs. This made it possible for Poplog to be used by teachers, researchers, or developers who were interested in only one of the languages. The most successful product developed in Pop-11 was the Clementine data-mining system, developed by ISL, as described in the entry on Poplog. After SPSS bought ISL they decided to port Clementine to C++ and Java, and eventually succeeded with great effort (and perhaps some loss of the flexibility provided by the use of an AI language!).\n\nAs explained in the entries for Poplog and POP-2, Pop-11 was for a time available only as part of an expensive commercial package (Poplog), but since about 1999 it has been freely available as part of the Open Source version of Poplog, including various additional packages and teaching libraries. An online version of ELIZA using Pop-11 is available at Birmingham.\n\nAt the University of Sussex David Young used Pop-11 in combination with C and Fortran to develop a suite of teaching and interactive development tools for image processing and vision, and has made them available in the Popvision extension to Poplog.\n\nHere is an example of a simple POP-11 program:\n\nThat prints out:\n\nThis one includes some list processing:\n\nExamples using the Pop-11 pattern matcher, which makes it relatively easy for students to learn to develop sophisticated list-processing programs without having to treat patterns as tree structures accessed by 'head' and 'tail' functions (CAR and CDR in Lisp), can be found in the online introductory tutorial. The matcher is at the heart of\nthe SimAgent (sim_agent) toolkit. Some of the powerful features of the toolkit, e.g. linking pattern variables to inline code variables, would have been very difficult to implement without the incremental compiler facilities.\n\n\n\n\n[[Category:Lisp programming language family]]\n[[Category:Artificial intelligence]]",
    "id": "562827",
    "title": "POP-11"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=2142",
    "text": "List of artificial intelligence projects\n\nThe following is a list of current and past, nonclassified notable artificial intelligence projects.\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "id": "2142",
    "title": "List of artificial intelligence projects"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=21652",
    "text": "Natural-language processing\n\nNatural-language processing (NLP) is a field of computer science, artificial intelligence concerned with the interactions between computers and human (natural) languages, and, in particular, concerned with programming computers to fruitfully process large natural language data.\n\nChallenges in natural-language processing frequently involve speech recognition, natural-language understanding, and natural-language generation.\n\nThe history of NLP generally started in the 1950s, although work can be found from earlier periods.\nIn 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence.\n\nThe Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem. However, real progress was much slower, and after the ALPAC report in 1966, which found that ten-year-long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted until the late 1980s, when the first statistical machine translation systems were developed.\n\nSome notably successful NLP systems developed in the 1960s were SHRDLU, a natural-language system working in restricted \"blocks worlds\" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and 1966. Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction. When the \"patient\" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to \"My head hurts\" with \"Why do you say your head hurts?\".\n\nDuring the 1970s, many programmers began to write \"conceptual ontologies\", which structured real-world information into computer-understandable data. Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981). During this time, many chatterbots were written including PARRY, Racter, and Jabberwacky.\n\nUp to the 1980s, most NLP systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in NLP with the introduction of machine learning algorithms for language processing. This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing. Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules. However, part-of-speech tagging introduced the use of hidden Markov models to NLP, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data. The cache language models upon which many speech recognition systems now rely are examples of such statistical models. Such models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data), and produce more reliable results when integrated into a larger system comprising multiple subtasks.\n\nMany of the notable early successes occurred in the field of machine translation, due especially to work at IBM Research, where successively more complicated statistical models were developed. These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government. However, most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems. As a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data.\n\nRecent research has increasingly focused on unsupervised and semi-supervised learning algorithms. Such algorithms are able to learn from data that has not been hand-annotated with the desired answers, or using a combination of annotated and non-annotated data. Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data. However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the World Wide Web), which can often make up for the inferior results.\n\nIn recent years, there has been a flurry of results showing deep learning techniques achieving state-of-the-art results in many natural-language tasks, for example in language modeling,\nparsing, and many others.\n\nSince the so-called \"statistical revolution\"\nin the late 1980s and mid 1990s, much Natural-Language Processing research has relied heavily on machine learning.\n\nFormerly, many language-processing tasks typically involved the direct hand coding of rules, which is not in general robust to natural-language variation. The machine-learning paradigm calls instead for using statistical inference to automatically learn such rules through the analysis of large \"corpora\" of typical real-world examples (a \"corpus\" (plural, \"corpora\") is a set of documents, possibly with human or computer annotations).\n\nMany different classes of machine learning algorithms have been applied to NLP tasks. These algorithms take as input a large set of \"features\" that are generated from the input data. Some of the earliest-used algorithms, such as decision trees, produced systems of hard if-then rules similar to the systems of hand-written rules that were then common. Increasingly, however, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to each input feature. Such models have the advantage that they can express the relative certainty of many different possible answers rather than only one, producing more reliable results when such a model is included as a component of a larger system.\n\nSystems based on machine-learning algorithms have many advantages over hand-produced rules:\n\nThe following is a list of some of the most commonly researched tasks in NLP. Note that some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.\n\nThough NLP tasks are obviously very closely intertwined, they are frequently, for convenience, subdivided into categories. A coarse division is given below.\n\n\n\n\n\n",
    "id": "21652",
    "title": "Natural-language processing"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=23423534",
    "text": "Australian Artificial Intelligence Institute\n\nIn Australia, the Australian Artificial Intelligence Institute (Australian AI Institute, AAII, or AI) was a government-funded research and development laboratory for investigating and commercializing Artificial Intelligence, specifically Intelligent Software Agents.\n\nThe AAII was started in 1988 as an initiative by the Hawke government and closed in 1999. It was backed by support from the Computer Power Group, SRI International and the Victorian State Government. The director of the group was Michael Georgeff who came from SRI, contributing his experience with the PRS and vision in the domain of Intelligent agents. It was located in the Melbourne suburb of Carlton before moving to more spacious premises in the city centre of Melbourne, Victoria. At its peak it had more than 40 staff and took up two floors of an office building on the corner of Latrobe and Russell Streets.\n\nIn the late 1990s, the AAII spun out Agentis International (Agentis Business Solutions) to address the commercialization of the developed technology. Another company, Agent Oriented Software (AOS) was formed by a number of ex-AAII staff to pursue agent technology developing JACK Intelligent Agents. After the AAII shutdown, those staff that remained and the intellectual property were transferred to Agentis International.\n\nThis section summarizes a selection of the software and commercial projects that came out of the AAII: \n\n\nOver the course of its existence, the AAII released more than 75 of public technical notes . This section lists an available selection of these notes.\n\n\n\n",
    "id": "23423534",
    "title": "Australian Artificial Intelligence Institute"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=52918812",
    "text": "NTU RGB-D dataset\n\nThe NTU RGB-D (Nanyang Technological University's Red Blue Green and Depth information) dataset is a large dataset containing recordings of labeled human activities\n. This dataset consists of 56,880 action samples containing 4 different modalities (RGB videos, depth map sequences, 3D skeletal data, infrared videos) of data for each sample.\n\nThe dataset consists of 60 labelled actions. Specifically, drink water, eat meal/snack, brushing teeth, brushing hair, drop, pickup, throw, sitting down, standing up (from sitting position), clapping, reading, writing, tear up paper, wear jacket, take off jacket, wear a shoe, take off a shoe, wear on glasses, take off glasses, put on a hat/cap, take off a hat/cap, cheer up, hand waving, kicking something, put something inside pocket / take out something from pocket, hopping (one foot jumping), jump up, make a phone call/answer phone, playing with phone/tablet, typing on a keyboard, pointing to something with finger, taking a selfie, check time (from watch), rub two hands together, nod head/bow, shake head, wipe face, salute, put the palms together, cross hands in front (say stop), sneeze/cough, staggering, falling, touch head (headache), touch chest (stomachache/heart pain), touch back (backache), touch neck (neckache), nausea or vomiting condition, use a fan (with hand or paper)/feeling warm, punching/slapping other person, kicking other person, pushing other person, pat on back of other person, point finger at the other person, hugging other person, giving something to other person, touch other person's pocket, handshaking, walking towards each other and walking apart from each other.\n\nThis is a table of some of the machine learning methods used on the database and their error rates, by type of classifier:\n",
    "id": "52918812",
    "title": "NTU RGB-D dataset"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=52968552",
    "text": "WordDive\n\nWordDive is an AI-based online language learning software and mobile application. Ten languages are currently offered: English, Estonian, Finnish, French, German, Italian, Japanese, Russian, Spanish and Swedish.\n\nThe method is based on the use of multiple senses, individual optimization and game-like elements.With the individual optimization feature, the course is modified based on the user's learning ability. When the user learns quickly, the amount of actively practiced content increases and the repetition decreases. The aim of the exercises is to help the user to start speaking the new language quickly and effortlessly. Each course package includes several individual courses that focus on specific topics or grammatical aspects.\n\nWordDive has been awarded as the Best e-Learning Solution in Finland 2011 as well as the Best Mobile Service in Finland 2014. In spring 2015, WordDive was selected as a Red Herring Europe Top 100 winner.\n\nThe company is based in Tampere, Finland. The service was launched in 2010. As of December 2016, the platform was said to have 250,000 users in 150 countries.\n\nIn Finland and Germany WordDive is best known for its English prep course, which became a market leader in Finland two years after its launch in 2014. Now 25 percent of Finnish high school seniors use the WordDive prep course, and 98 percent of those who complete the course recommend WordDive. There is a grade guarantee: if the course participant completes the prep course but doesn't get at least the second best grade in English, they will be fully refunded.\n\nIn Germany, the company is working together with brand ambassador Samu Haber, the front man of the band Sunrise Avenue and a judge in The Voice of Germany.\n\nIn January 2017 WordDive secured 1.2 million euros via the Invesdor crowdfunding platform.\n",
    "id": "52968552",
    "title": "WordDive"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=41732818",
    "text": "Qloo\n\nQloo (pronounced \"clue\") is a company that uses artificial intelligence (AI). An application programming interface (API) provides cultural correlations. It was founded by Alex Elias and received funding from Leonardo DiCaprio, Barry Sternlicht and Pierre Lagrange.\n\nQloo establishes consumer preference correlations via machine learning across multiple proprietary, customer and open-source data across cultural domains including music, film, television, dining, nightlife, fashion, books and travel. The recommender system uses AI to predict correlations for further applications.\n\nQloo was founded in 2012 by chief executive officer Alex Elias and chief operating officer Jay Alger. Elias was formerly a hedge fund manager with APE Capital.\nHe graduated from the University of Southern California, and then developed his idea at law school at New York University.\nAlger was formerly the CEO of the digital agency Deepend.\n\nQloo was tested on a private website in April 2012. \nIn 2012, Qloo raised $1.4 million in seed funding from investors including Cedric the Entertainer, Danny Masterson, and venture capital firm Kindler Capital.\nQloo had a public beta release in November 2012 after its initial funding. \n\nIn 2013, the company raised an additional $1.6 million from Cross Creek Pictures founding partner Tommy Thompson, and Samih Toukan and Hussam Khoury, founders of Maktoob, an Internet services company purchased by Yahoo! for $164 million in 2009.\nOn November 14, 2013, a website and an iPhone app were announced. The company later released an Android app, and tablet versions, in mid-2014.\n\nIn 2016, Qloo secured $4.5 million in venture capital investment. The $4.5 million was split between a number of investors, including Barry Sternlicht, Pierre Lagrange and Leonardo DiCaprio. In July 2017, Qloo raised $6.5 million in funding rounds from AXA Strategic Ventures and Elton John.\n\nFollowing the investment, the founders stated in an interview with Tech Crunch that they would use the investment to expand Qloo's database. They hoped the move would secure larger contracts with corporate clients. At the time, clients already included Fortune 500 companies such as Twitter, PepsiCo and BMW.\n\nQloo calls itself a cultural AI platform to provide real-time correlation data across domains of culture and entertainment including: film, music, television, dining, nightlife, fashion, books and travel. Each category contains subcategories. \n\nQloo’s knowledge of a user's taste in one category can be utilized to offer suggestions in other categories. Users then rate the suggestions, providing it with feedback for future suggestions.\nQloo has partnerships with companies such as Expedia and iTunes.\n",
    "id": "41732818",
    "title": "Qloo"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=28803763",
    "text": "INDECT\n\nINDECT is a research project in the area of intelligent security systems performed by several European universities since 2009 and funded by the European Union. The purpose of the project is to involve European scientists and researchers in the development of solutions to and tools for automatic threat detection through e.g. processing of CCTV camera data streams, standardization of video sequence quality for user applications, threat detection in computer networks as well as data and privacy protection.\n\nThe area of research, applied methods and techniques are described in the public deliverables which are available to the public on the project's website. Practically, all information related to the research is public. Only documents that comprise information related to financial data or information that could negatively influence the competitiveness and law enforcement capabilities of parties involved in the project are not published. This follows regulations and practices applied in EU research projects.\n\nThe main end-user of INDECT solutions are police forces and security services.\n\nThe principle of operation of the project is detecting threats and identifying source of threats, without monitoring and searching for particular citizens or groups of citizens. Then, the system operator (i.e. police officer) decides whether an intervention of services responsible for public security are required or not. Further investigation eventually leading to persons related to threats are performed, preserving the presumption of innocence, on the basis of existing procedures already used by police services and prosecutors. As it can be found in the project deliverables, INDECT does not involve storage of personal data (such as names, addresses, identity document numbers, etc.).\n\nA similar, behaviour based surveillance program was SAMURAI \"(Suspicious and Abnormal behaviour Monitoring Using a netwoRk of cAmeras & sensors for sItuation awareness enhancement)\".\n\nThe main expected results of the INDECT project are:\n\nSome media and other sources accuse INDECT of privacy abuse, collecting personal data, and keeping information from the public. Consequently, these issues have been commented and discussed by some Members of the European Parliament.\n\nAs can be seen in the project's documentation, INDECT does not involve mobile phone tracking or call interception.\n\nThe rumours about testing INDECT during 2012 UEFA European Football Championship also turned out to be false.\n\nThe mid-term review of the Seventh Framework Programme to the European Parliament strongly urges the European Commission to immediately make all documents available and to define a clear and strict mandate for the research goal, the application, and the end users of INDECT, and stresses a thorough investigation of the possible impact on fundamental rights. Nevertheless, according to Mr. Paweł Kowal, MEP, the project had the ethical review on 15 March 2011 in Brussels with the participation of ethics experts from Austria, France, Netherlands, Germany and Great Britain.\n\n\n",
    "id": "28803763",
    "title": "INDECT"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=46803936",
    "text": "Susan Schneider (philosopher)\n\nSusan Schneider is an American philosopher. She is a professor of philosophy and cognitive science at The University of Connecticut, a fellow at the Institute for Ethics and Emerging Technologies, and a faculty member in the Ethics and Technology Group at the Yale Interdisciplinary Center for Bioethics, Yale University.\n\nSchneider's main focus is the nature of the self and mind. Within the fields of metaphysics and philosophy of mind, much of her work explores the nature of thought, especially in light of discoveries in cognitive science and work in contemporary metaphysics. She has argued that the brain is computational, and has developed a new version of the language of thought (\"LOT\") position. But while supporting computationalism about the brain, she is a critic of physicalism about the nature of the mind, arguing physicalism is ill-conceived.\n\nShe is also actively engaged in debates over artificial intelligence (including superintelligence) and brain enhancement (see mind uploading) and uses ideas from contemporary metaphysics, ethics and cognitive science, interlaced with science fiction thought experiments, to illustrate flaws in positions. In an article for a NASA publication she argues that the most intelligent beings in the universe are likely to be superintelligent robots.\n\nDiscussions of her work have appeared in \"The New York Times, Wired Magazine, Humanity+, Big Think, \"3 Quarks Daily\", \"Discover Magazine\", \"Science Magazine\", Motherboard, Slate (France), Popular Mechanics\", and more.\n\nHer work was recently the subject of a documentary TV episode.\n\nThe Language of Thought\n\nSchneider has framed a new version of the language of thought (“LOT”) approach. According to the LOT approach, humans and even non-human animals think in a “language of thought” - an inner mental language that is not equivalent to any natural language. This mental language is computational, for thinking is regarded as the algorithmic manipulation of mental symbols, where the algorithm is to be identified through research in cognitive science. The “Classical Computational Theory of Mind” holds that part or all of the brain is computational in this algorithmic sense.\n\nIn her book on LOT, Schneider contrasts the LOT approach to its chief foe, the connectionist or neural network approach, urging that both approaches are insightful. She holds that the brain is probably a hybrid system — being both a symbol processing engine, and having neural networks. In particular, deliberative, conscious thought is symbolic, but it is implemented by neural networks.\n\nLOT’s chief philosophical architect, Jerry Fodor, has argued the cognitive mind is likely non-computational.<ref name=\"A Bradford Book / MIT Press\"></ref> Schneider argues against Fodor’s pessimism, illustrating that the development of sophisticated computational theories of cognition, as well as artificial general intelligence (\"AGI\"), are on the horizon.\n\nSchneider also defends a view of the nature of the mental symbols (where such are the basic vocabulary items in the language of thought). She then used this conception of symbols, together with certain work on the nature of meaning, to construct a theory of the nature of concepts. The basic theory of concepts is intended to be ecumenical, having a version that applying in the case of connectionism, as well as versions that apply to both the prototype theory and definitions view of concepts.\n\nThe Metaphysics of Mind\n\nPhysicalism holds that everything is made up of entities that physics says are fundamental, such as particles, fields or strings. Schneider urges that the success of computationalism does not require physicalism to be correct.\n\nSchneider claims some of the most popular versions of physicalism are at odds with commonly accepted positions about substance and properties in the related field of metaphysics. Further, the mathematical nature of fundamental physical theories undermines physicalism itself. Fundamental physical entities are defined mathematically, and the physicalist must consider what makes mathematical statements true. This is an issue dealt with in the field of philosophy of mathematics. The most viable theories in that domain, when combined with a physicalist approach, yield unworkable versions of physicalism. At best, physicalism becomes a form of dualism – a dualism of the abstract and concrete. And it fares as poorly as substance and property dualism with respect to explaining mental causation. Physicalism, thus understood, loses its customary advantages over competing theories.\n\nAstrobiology and Artificial Intelligence (A.I.)\n\nSchneider is among those researchers who believe that the most intelligent alien beings we encounter will be \"postbiological in nature\", being forms of artificial intelligence. (See also Paul Davies, Steven J. Dick, Martin Rees and Seth Shostak.) She is the first to contend that they would be superintelligent, and that we can predict the shape of some of these superintelligences would be like.\n\nTheir reason for the claim that the most intelligent aliens will be \"postbiological\" is called the \"short window observation.” This holds that by the time any society learns to transmit radio signals, they're likely just a few hundred years from upgrading their own biology. As Elon Musk puts it biological beings would be just a \"biological boot loader for digital superintelligence.\"\n\nSchneider poses two questions: first, how can we understand the thinking of superintelligences? And, second, if this is the direction intelligence is going, will these superintelligent beings even be conscious? She poses a \"hard problem of AI consciousness\" that is similar in some respects to David Chalmers hard problem of consciousness, which concerns the human case. The problem is: how can we know that silicon is the right sort of medium for conscious experience?\n\nDrawing from work in cognitive science, Schneider identifies ways that humans might understand the advanced thought patterns of certain kinds of superintelligences – namely, those that are modeled after the biological beings that created them. And she urges that under the right circumstances the A.I. may be conscious.\n\nUploading, Cognitive Enhancement and the Singularity\n\nTranshumanism is a philosophical, cultural, and political movement that holds that the human species is only now in a comparatively early phase and that its very evolution will be altered by developing technologies.\n\nSchneider observes that when one considers whether to enhance in the radical ways the transhumanists advocate (e.g., uploading, adding silicon brain parts for new cognitive capacities, or to enhance existing ones), one must ask, \"Will this radically enhanced creature still be me?\" If not, then, on the assumption that one key factor in a decision to enhance oneself is one's own personal development, we should regard the enhancement in question as undesirable. For when you choose to enhance in these radical ways, the enhancement does not really enhance \"you.\" In this vein, Schneider contends that one cannot really \"upload\" their brain, transferring their consciousness to a computer. At best, uploading will merely create a computational copy of youl.\n\nEnhancement decisions will require deep deliberation about metaphysical and ethical questions that are controversial and difficult to solve: questions that will require reflection about both personal identity and the nature of mind. At best, a pluralistic society should recognize the diversity of different views on these matters, and not assume that science itself can answer questions about whether radical forms of brain enhancement are a form of survival.\n\n\n",
    "id": "46803936",
    "title": "Susan Schneider (philosopher)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=53210461",
    "text": "Moral Machine\n\nMoral Machine is an online platform, developed by Iyad Rahwan's Scalable Cooperation group at the Massachusetts Institute of Technology, that generates moral dilemmas and collects information on the decisions that people make between two destructive outcomes. The presented scenarios are often variations of the trolley problem, and the information collected would be used for further research regarding the decisions that machine intelligence must make in the future. For example, as artificial intelligence plays an increasingly significant role in autonomous driving technology, research projects like Moral Machine help to find solutions for challenging life-and-death decisions that will face self-driving vehicles.\n",
    "id": "53210461",
    "title": "Moral Machine"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=53322329",
    "text": "Human Problem Solving\n\nHuman Problem Solving (1972) is a book by Allen Newell and Herbert A. Simon.\n",
    "id": "53322329",
    "title": "Human Problem Solving"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=53587467",
    "text": "Outline of machine learning\n\nThe following outline is provided as an overview of and topical guide to machine learning:\n\nMachine learning – subfield of computer science (more particularly soft computing) that evolved from the study of pattern recognition and computational learning theory in artificial intelligence. In 1959, Arthur Samuel defined machine learning as a \"Field of study that gives computers the ability to learn without being explicitly programmed\". Machine learning explores the study and construction of algorithms that can learn from and make predictions on data. Such algorithms operate by building a model from an example \"training set\" of input observations in order to make data-driven predictions or decisions expressed as outputs, rather than following strictly static program instructions.\n\n\nSubfields of machine learning\n\nCross-disciplinary fields involving machine learning\n\nApplications of machine learning\n\nMachine learning hardware\n\nMachine learning tools   (list)\n\nMachine learning framework\n\nProprietary machine learning frameworks\n\nOpen source machine learning frameworks\n\nMachine learning library   (list)\n\nMachine learning algorithm\n\n\nMachine learning method   (list)\n\nDimensionality reduction\n\nEnsemble learning\n\nMeta learning\n\nReinforcement learning\n\nSupervised learning\n\nBayesian statistics\n\nDecision tree algorithm\n\nLinear classifier\n\nUnsupervised learning\n\nArtificial neural network\n\nAssociation rule learning\n\nHierarchical clustering\n\nCluster analysis\n\nAnomaly detection\n\nSemi-supervised learning\n\nDeep learning\n\n\nMachine learning research\n\nHistory of machine learning\n\nMachine learning projects\n\nMachine learning organizations\n\n\nBooks about machine learning\n\n\n\n\n\n\n\n\n",
    "id": "53587467",
    "title": "Outline of machine learning"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=10136",
    "text": "Expert system\n\nIn artificial intelligence, an expert system is a computer system that emulates the decision-making ability of a human expert.\nExpert systems are designed to solve complex problems by reasoning through bodies of knowledge, represented mainly as if–then rules rather than through conventional procedural code. The first expert systems were created in the 1970s and then proliferated in the 1980s. Expert systems were among the first truly successful forms of artificial intelligence (AI) software.\n\nAn expert system is divided into two subsystems: the inference engine and the knowledge base. The knowledge base represents facts and rules. The inference engine applies the rules to the known facts to deduce new facts. Inference engines can also include explanation and debugging abilities.\n\nExpert systems were introduced by the Stanford Heuristic Programming Project led by Edward Feigenbaum, who is sometimes termed the \"father of expert systems\"; other key early contributors were Bruce Buchanan and Randall Davis. The Stanford researchers tried to identify domains where expertise was highly valued and complex, such as diagnosing infectious diseases (Mycin) and identifying unknown organic molecules (Dendral). The idea that \"intelligent systems derive their power from the knowledge they possess rather than from the specific formalisms and inference schemes they use\" – as Feigenbaum said – was at the time a significant step forward, since the past research had been focused on heuristic computational methods, culminating in attempts to develop very general-purpose problem solvers (foremostly the conjunct work of Allen Newell and Herbert Simon). Expert systems became some of the first truly successful forms of artificial intelligence (AI) software.\n\nResearch on expert systems was also active in France. While in the US the focus tended to be on rule-based systems, first on systems hard coded on top of LISP programming environments and then on expert system shells developed by vendors such as Intellicorp, in France research focused more on systems developed in Prolog. The advantage of expert system shells was that they were somewhat easier for nonprogrammers to use. The advantage of Prolog environments was that they weren't focused only on \"if-then\" rules; Prolog environments provided a much fuller realization of a complete First Order Logic environment.\n\nIn the 1980s, expert systems proliferated. Universities offered expert system courses and two thirds of the Fortune 500 companies applied the technology in daily business activities. Interest was international with the Fifth Generation Computer Systems project in Japan and increased research funding in Europe.\n\nIn 1981, the first IBM PC, with the PC DOS operating system, was introduced. The imbalance between the high affordability of the relatively powerful chips in the PC, compared to the much more expensive cost of processing power in the mainframes that dominated the corporate IT world at the time, created a new type of architecture for corporate computing, termed the client-server model. Calculations and reasoning could be performed at a fraction of the price of a mainframe using a PC. This model also enabled business units to bypass corporate IT departments and directly build their own applications. As a result, client server had a tremendous impact on the expert systems market. Expert systems were already outliers in much of the business world, requiring new skills that many IT departments did not have and were not eager to develop. They were a natural fit for new PC-based shells that promised to put application development into the hands of end users and experts. Until then, the main development environment for expert systems had been high end Lisp machines from Xerox, Symbolics, and Texas Instruments. With the rise of the PC and client server computing, vendors such as Intellicorp and Inference Corporation shifted their priorities to developing PC based tools. Also, new vendors, often financed by venture capital (such as Aion Corporation, Neuron Data, Exsys, and many others), started appearing regularly.\n\nThe first expert system to be used in a design capacity for a large-scale product was the SID (Synthesis of Integral Design) software program, developed in 1982. Written in LISP, SID generated 93% of the VAX 9000 CPU logic gates. Input to the software was a set of rules created by several expert logic designers. SID expanded the rules and generated software logic synthesis routines many times the size of the rules themselves. Surprisingly, the combination of these rules resulted in an overall design that exceeded the capabilities of the experts themselves, and in many cases out-performed the human counterparts. While some rules contradicted others, top-level control parameters for speed and area provided the tie-breaker. The program was highly controversial, but used nevertheless due to project budget constraints. It was terminated by logic designers after the VAX 9000 project completion.\n\nIn the 1990s and beyond, the term \"expert system\" and the idea of a standalone AI system mostly dropped from the IT lexicon. There are two interpretations of this. One is that \"expert systems failed\": the IT world moved on because expert systems didn't deliver on their over hyped promise. The other is the mirror opposite, that expert systems were simply victims of their success: as IT professionals grasped concepts such as rule engines, such tools migrated from being standalone tools for developing special purpose \"expert\" systems, to being one of many standard tools. Many of the leading major business application suite vendors (such as SAP, Siebel, and Oracle) integrated expert system abilities into their suite of products as a way of specifying business logic – rule engines are no longer simply for defining the rules an expert would use but for any type of complex, volatile, and critical business logic; they often go hand in hand with business process automation and integration environments.\n\nAn expert system is an example of a knowledge-based system. Expert systems were the first commercial systems to use a knowledge-based architecture. A knowledge-based system is essentially composed of two sub-systems: the knowledge base and the inference engine.\n\nThe knowledge base represents facts about the world. In early expert systems such as Mycin and Dendral, these facts were represented mainly as flat assertions about variables. In later expert systems developed with commercial shells, the knowledge base took on more structure and used concepts from object-oriented programming. The world was represented as classes, subclasses, and instances and assertions were replaced by values of object instances. The rules worked by querying and asserting values of the objects.\n\nThe inference engine is an automated reasoning system that evaluates the current state of the knowledge-base, applies relevant rules, and then asserts new knowledge into the knowledge base. The inference engine may also include abilities for explanation, so that it can explain to a user the chain of reasoning used to arrive at a particular conclusion by tracing back over the firing of rules that resulted in the assertion.\n\nThere are mainly two modes for an inference engine: forward chaining and backward chaining. The different approaches are dictated by whether the inference engine is being driven by the antecedent (left hand side) or the consequent (right hand side) of the rule. In forward chaining an antecedent fires and asserts the consequent. For example, consider the following rule:\n\nformula_1\n\nA simple example of forward chaining would be to assert Man(Socrates) to the system and then trigger the inference engine. It would match R1 and assert Mortal(Socrates) into the knowledge base.\n\nBackward chaining is a bit less straight forward. In backward chaining the system looks at possible conclusions and works backward to see if they might be true. So if the system was trying to determine if Mortal(Socrates) is true it would find R1 and query the knowledge base to see if Man(Socrates) is true. One of the early innovations of expert systems shells was to integrate inference engines with a user interface. This could be especially powerful with backward chaining. If the system needs to know a particular fact but doesn't it can simply generate an input screen and ask the user if the information is known. So in this example, it could use R1 to ask the user if Socrates was a Man and then use that new information accordingly.\n\nThe use of rules to explicitly represent knowledge also enabled explanation abilities. In the simple example above if the system had used R1 to assert that Socrates was Mortal and a user wished to understand why Socrates was mortal they could query the system and the system would look back at the rules which fired to cause the assertion and present those rules to the user as an explanation. In English if the user asked \"Why is Socrates Mortal?\" the system would reply \"Because all men are mortal and Socrates is a man\". A significant area for research was the generation of explanations from the knowledge base in natural English rather than simply by showing the more formal but less intuitive rules.\n\nAs expert systems evolved, many new techniques were incorporated into various types of inference engines. Some of the most important of these were:\n\n\nThe goal of knowledge-based systems is to make the critical information required for the system to work explicit rather than implicit. In a traditional computer program the logic is embedded in code that can typically only be reviewed by an IT specialist. With an expert system the goal was to specify the rules in a format that was intuitive and easily understood, reviewed, and even edited by domain experts rather than IT experts. The benefits of this explicit knowledge representation were rapid development and ease of maintenance.\n\nEase of maintenance is the most obvious benefit. This was achieved in two ways. First, by removing the need to write conventional code, many of the normal problems that can be caused by even small changes to a system could be avoided with expert systems. Essentially, the logical flow of the program (at least at the highest level) was simply a given for the system, simply invoke the inference engine. This also was a reason for the second benefit: rapid prototyping. With an expert system shell it was possible to enter a few rules and have a prototype developed in days rather than the months or year typically associated with complex IT projects.\n\nA claim for expert system shells that was often made was that they removed the need for trained programmers and that experts could develop systems themselves. In reality, this was seldom if ever true. While the rules for an expert system were more comprehensible than typical computer code, they still had a formal syntax where a misplaced comma or other character could cause havoc as with any other computer language. Also, as expert systems moved from prototypes in the lab to deployment in the business world, issues of integration and maintenance became far more critical. Inevitably demands to integrate with, and take advantage of, large legacy databases and systems arose. To accomplish this, integration required the same skills as any other type of system.\n\nThe most common disadvantage cited for expert systems in the academic literature is the knowledge acquisition problem. Obtaining the time of domain experts for any software application is always difficult, but for expert systems it was especially difficult because the experts were by definition highly valued and in constant demand by the organization. As a result of this problem, a great deal of research in the later years of expert systems was focused on tools for knowledge acquisition, to help automate the process of designing, debugging, and maintaining rules defined by experts. However, when looking at the life-cycle of expert systems in actual use, other problems – essentially the same problems as those of any other large system – seem at least as critical as knowledge acquisition: integration, access to large databases, and performance.\n\nPerformance was especially problematic because early expert systems were built using tools such as Lisp, which executed interpreted (rather than compiled) code. Interpreting provided an extremely powerful development environment but with the drawback that it was virtually impossible to match the efficiency of the fastest compiled languages, such as C. System and database integration were difficult for early expert systems because the tools were mostly in languages and platforms that were neither familiar to nor welcome in most corporate IT environments – programming languages such as Lisp and Prolog, and hardware platforms such as Lisp machines and personal computers. As a result, much effort in the later stages of expert system tool development was focused on integrating with legacy environments such as COBOL and large database systems, and on porting to more standard platforms. These issues were resolved mainly by the client-server paradigm shift, as PCs were gradually accepted in the IT environment as a legitimate platform for serious business system development and as affordable minicomputer servers provided the processing power needed for AI applications.\n\nHayes-Roth divides expert systems applications into 10 categories illustrated in the following table. The example applications were not in the original Hayes-Roth table, and some of them arose well afterward. Any application that is not footnoted is described in the Hayes-Roth book. Also, while these categories provide an intuitive framework to describe the space of expert systems applications, they are not rigid categories, and in some cases an application may show traits of more than one category.\nHearsay was an early attempt at solving voice recognition through an expert systems approach. For the most part this category or expert systems was not all that successful. Hearsay and all interpretation systems are essentially pattern recognition systems—looking for patterns in noisy data. In the case of Hearsay recognizing phonemes in an audio stream. Other early examples were analyzing sonar data to detect Russian submarines. These kinds of systems proved much more amenable to a neural network AI solution than a rule-based approach.\n\nCADUCEUS and MYCIN were medical diagnosis systems. The user describes their symptoms to the computer as they would to a doctor and the computer returns a medical diagnosis.\n\nDendral was a tool to study hypothesis formation in the identification of organic molecules. The general problem it solved—designing a solution given a set of constraints—was one of the most successful areas for early expert systems applied to business domains such as salespeople configuring Digital Equipment Corporation (DEC) VAX computers and mortgage loan application development.\n\nSMH.PAL is an expert system for the assessment of students with multiple disabilities.\n\nMistral is an expert system to monitor dam safety, developed in the 90's by Ismes (Italy). It gets data from an automatic monitoring system and performs a diagnosis of the state of the dam. Its first copy, installed in 1992 on the Ridracoli Dam (Italy), is still operational 24/7/365. It has been installed on several dams in Italy and abroad (e.g., Itaipu Dam in Brazil), and on landslide sites under the name of Eydenet, and on monuments under the name of Kaleidos. Mistral is a registered trade mark of CESI.\n\nBayesian networks (BNs) are probabilistic graphical models, which are typically used to model cause and effect relationships, have become the most widely accepted technique for incorporating expert knowledge along with data. Expert knowledge can be incorporated into BNs by either constructing the causal (or dependence) graph, or by incorporating factors into the causal network which are important for inference but which data fail to capture.\nThe popularity of BNs as expert systems has led to the development of countless prediction and decision support systems in industry, government and academia worldwide. These systems typically incorporate both knowledge and data, and have been applied in the areas of, but not limited to, finance, engineering, sports, sports psychology, law, project management, marketing, medicine, energy, forensics, economics, property market, and defence.\n\n\n",
    "id": "10136",
    "title": "Expert system"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=50231947",
    "text": "Mycroft (software)\n\nMycroft is a free and open-source intelligent personal assistant and knowledge navigator for Linux-based operating systems that uses a natural language user interface. It is named after a fictional computer from 1966 science fiction novel The Moon Is a Harsh Mistress.\n\nInspiration for Mycroft came when Ryan Sipes and Joshua Montgomery were visiting the Kansas City makerspace, where they came across a simple and basic intelligent virtual assistant project. Mycroft is said to be the world’s first open-source AI voice assistant. They were interested in the technology, but did not like its inflexibility. Mycroft was part of the Sprint Accelerator 2016 class in Kansas City and joined 500 Startups Batch 20 in February 2017. The company accepted a strategic investment from Jaguar Land Rover during this same time period.\n\nMycroft uses an intent parser called Adapt to convert natural language into machine readable data structures. For speech synthesis Mycroft uses Mimic, which is based on Flite. Mycroft is designed to be modular, so users are able to change its components. For example, espeak can be used instead of Mimic.\n",
    "id": "50231947",
    "title": "Mycroft (software)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=53644736",
    "text": "Distributional–relational database\n\nA distributional–relational database, or word-vector database, is a database management system (DBMS) that uses distributional word-vector representations to enrich the semantics of structured data. As distributional word-vectors can be automatically built automatically from large-scale corpora, this enrichment supports the construction of databases which can embed large-scale commonsense background knowledge into their operations. Distributional-Relational models can be applied to the construction of schema-agnostic databases (databases in which users can query the data without being aware of its schema), semantic search, schema-integration and inductive and abductive reasoners as well as different applications in which a semantically flexible knowledge representation model is needed. The main advantage of distributional–relational models over purely logical / Semantic Web models is the fact that the core semantic associations can be automatically captured from corpora in contrast to the definition of manually curated ontologies and rule knowledge bases.\n\nDistributional–relational models were first formalized as a mechanism to cope with the vocabulary/semantic gap between users and the schema behind the data. In this scenario, distributional semantic relatedness measures, combined with semantic pivoting heuristics can support the approximation between user queries (expressed in their own vocabulary) and data (expressed in the vocabulary of the dataset designer).\n\nIn this model, the database symbols (entities and relations) are embedded into a distributional semantic space and have a geometric interpretation under a latent or explicit semantic space. The geometric aspect supports the semantic approximation between entities from different databases or between a query term and a database entity. The distributional relational model then becomes a double layered model where the semantics of the structured data provides the fine-grained semantics intended by the database designer, which is extended by the distributional semantic model which contains the semantic associations expressed at a broader use. \nThese models support the generalization from a closed communication scenario (in which database designers and users live in the same context, e.g. the same organization) to an open communication scenario (e.g. different organizations, the Web), creating an abstraction layer between users and the specific representation of the conceptual model.\n",
    "id": "53644736",
    "title": "Distributional–relational database"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=53673334",
    "text": "Open information extraction\n\nIn natural language processing, open information extraction (OIE) is the task of generating a structured, machine-readable representation of the information in text, usually in the form of triples or n-ary propositions. A proposition can be understood as truth-bearer, a textual expression of a potential fact (e.g., \"Dante wrote the Divine Comedy\"), represented in an amenable structure for computers [e.g., (\"Dante\", \"wrote\", \"Divine Comedy\")]. An OIE extraction normally consists of a relation and a set of arguments. For instance, (\"Dante\", \"passed away in\" \"Ravenna\") is a proposition formed by the relation \"passed away in\" and the arguments \"Dante\" and \"Ravenna\". The first argument is usually referred as the subject while the second is considered to be the object.\n\nThe extraction is said to be a textual representation of a potential fact because its elements are not linked to a knowledge base. Furthermore, the factual nature of the proposition has not yet been established. In the above example, transforming the extraction into a full fledged fact would first require linking, if possible, the relation and the arguments to a knowledge base. Second, the truth of the extraction would need to be determined. In computer science transforming OIE extractions into ontological facts is known as relation extraction.\n\nIn fact, OIE can be seen as the first step to a wide range of deeper text understanding tasks such as relation extraction, knowledge-base construction, question answering, semantic role labeling. The extracted propositions can also be directly used for end-user applications such as structured search (e.g., retrieve all propositions with \"Dante\" as subject).\n\nOIE was first introduced by TextRunner developed at the University of Washington Turing Center headed by Oren Etzioni. Other methods introduced later such as Reverb, OLLIE, ClausIE or CSD helped to shape the OIE task by characterizing some of its aspects. At a high level, all of these approaches make use of a set of patterns to generate the extractions. Depending on the particular approach, these patterns are either hand-crafted or learned.\n\nReverb suggested the necessity to produce meaningful relations to more accurately capture the information in the input text. For instance, given the sentence \"Faust made a pact with the devil\", it would be erroneous to just produce the extraction (\"Faust\", \"made\", \"a pact\") since it would not be adequately informative. A more precise extraction would be (\"Faust\", \"made a pact with\", \"the devil\"). Reverb also argued against the generation of overspecific relations.\n\nOLLIE stressed two important aspects for OIE. First, it pointed to the lack of factuality of the propositions. For instance, in a sentence like \"If John studies hard, he will pass the exam\", it would be inaccurate to consider (\"John\", \"will pass\", \"the exam\") as a fact. Additionally, the authors indicated that an OIE system should be able to extract non-verb mediated relations, which account for significant portion of the information expressed in natural language text. For instance, in the sentence \"Obama, the former US president, was born in Hawaii\", an OIE system should be able to recognize a proposition (\"Obama\", \"is\", \"former US president\").\n\nClausIE introduced the connection between grammatical clauses, propositions, and OIE extractions. The authors stated that as each grammatical clause expresses a proposition, each verb mediated proposition can be identified by solely recognizing the set of clauses expressed in each sentence. This implies that to correctly recognize the set of propositions in an input sentence, it is necessary to understand its grammatical structure. The authors studied the case in the English language that only admits seven clause types, meaning that the identification of each proposition only requires defining seven grammatical patterns.\n\nThe finding also established a separation between the recognition of the propositions and its materialization. In a first step, the proposition can be identified without any consideration of its final form, in a domain-independent and unsupervised way, mostly based on linguistic principles. In a second step, the information can be represented according to the requirements of the underlying application, without conditioning the identification phase.\n\nConsider the sentence \"Albert Einstein was born in Ulm and died in Princeton\". The first step will recognize the two propositions (\"Albert Einstein\", \"was born\", \"in Ulm\") and (\"Albert Einstein\", \"died\", \"in Princeton\"). Once the information has been correctly identified, the propositions can take the particular form required by the underlying application [e.g., (\"Albert Einstein\", \"was born in\", \"Ulm\") and (\"Albert Einstein\", \"died in\", \"Princeton\")].\n\nCSD introduced the idea of minimality in OIE. It considers that computers can make better use of the extractions if they are expressed in a compact way. This is especially important in sentences with subordinate clauses. In these cases, CSD suggests the generation of nested extractions. For example, consider the sentence \"The Embassy said that 6,700 Americans were in Pakistan\". CSD generates two extractions [i] (\"6,700 Americans\", \"were\", \"in Pakistan\") and [ii] (\"The Embassy\", \"said\", \"that [i]). This is usually known as reification.\n",
    "id": "53673334",
    "title": "Open information extraction"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=53639854",
    "text": "ICarbonX\n\niCarbonX is a company founded by Chinese genomicist Jun Wang, former CEO of Beijing Genomic Institute (BGI), in 2015. iCarbonX combines genomics with other health factors such as metabolites, bacteria and lifestyle choices to create a digitalized form of life.\n\niCarbonX has raised over $600 million in investment. Tencent Holdings Limited – owner of social-media app WeChat – and Zhongyuan Union Cell & Gene Engineering Corp. invested $200 million in iCarbonX, which made iCarbonX one of only three Chinese healthcare startups with a valuation of more than $1 billion (Unicorn). The company has about 100 employees.\niCarbonX was founded on October 27, 2015. On September 10, 2016 iCarbonX acquired Imagu Vision Technologies, an Israeli AI and image processing company, in order to establish an iCarbonX-Israel R&D center. On January 5, 2017 iCarbonX announced its Digital Life Alliance with seven other companies including SomaLogic, HealthTell, PatientsLikeMe, AOBiome, GALT, Imagu and Robustnique.\n\nOn January 5, 2017 iCarbonX released Meum, a digital health management platform. The company name, “iCarbonX,” symbolizes the use of the internet and artificial intelligence to improve life, of which a central element is carbon. The “i” and “X” indicate the company’s plans to combine the Internet and artificial intelligence to create something new.\n",
    "id": "53639854",
    "title": "ICarbonX"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=253284",
    "text": "Discovery system\n\nA discovery system may be an artificial intelligence system (in the context of computer science, logic, and mathematics) or a bibliographical search system (in the context of library and information science).\n\nA discovery system is an artificial intelligence system that attempts to discover new scientific concepts or laws.\n\nNotable discovery systems have included\n\n\nA discovery system is a bibliographic search system based on search engine technology. It is part of the concept of Library 2.0 and is intended to supplement or even replace the existing OPAC catalogs.\n\n\nCommercial products:\n\nOpen Source products:\n\n\n\n",
    "id": "253284",
    "title": "Discovery system"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=54008163",
    "text": "Industrial artificial intelligence\n\nIndustrial artificial intelligence, or industrial AI, usually refers to the application of artificial intelligence to industry. Unlike general artificial intelligence which is a frontier research discipline to build computerized systems that perform tasks requiring human intelligence, industrial AI is more concerned with the application of such technologies to address industrial pain-points for customer value creation, productivity improvement, and insight discovery. Although in a dystopian vision of AI applications, intelligent machines may take away jobs of humans and cause social and ethical issues, industry in general holds a more positive view of AI and sees this transformation of economy unstoppable and expects huge business opportunities in this process.\n\nThe concept of artificial intelligence has been proposed around 1940s, and the idea of improving productivity and gaining insights through smart analytics and modeling is not new. Artificial Intelligence and Knowledge-Based systems has been an active research branch of artificial intelligence for the entire product life cycle for product design, production planning, distribution, and field services. E-manufacturing systems and e-factories did not use the term “AI,” but they scale up modeling of engineering systems to enable complete integration of elements in the manufacturing eco-system for smart operation management. Cloud Foundry service platforms widely embed the artificial intelligent technologies. Cybermanufacturing systems also apply predictive analytics and cyber-physical modeling to address the gap between production and machine health for optimized productivity.\n\nThere are several reasons for the recent popularity of industrial AI: More affordable sensors and the automated process of data acquisition; More powerful computation capability of computers to perform more complex tasks at a faster speed with lower cost; Faster connectivity infrastructure and more accessible cloud services for data management and computing power outsourcing. However, the technology alone never creates any business value if the problems in industry are not well studied. The major categories where industrial AI may contribute to are product and service innovation, process improvement, and insight discovery.\n\nIndustrial AI can be embedded to existing products or services to make them more effective, reliable, safer, and last longer. One example is the automobile industry uses vision recognition to avoid accidents, stay in lane, and use speech recognition to facilitate safer driving. In manufacturing, one example is the prediction of blade life for self-aware band saw machines, so that users will be able to rely on evidence of degradation rather than experience, which is safer, will extend blade life, and build up blade usage profile to help blade selection.\n\nIndustrial AI can create new products and novel business models. Predix by General Electric is a Cloud Foundry that serves as an industrial operating system for narrow AI application development. InsightCM™ by National Instruments and Watchdog Agent® Toolbox on the LabVIEW platform also provide software analytical capabilities. The aforementioned band saw machine manufacturer also announced their service center system to help users improve equipment reliability and sawing efficiency as a novel business model.\n\nAutomation is one of the major aspects in process applications of industrial AI. With the help of AI, the scope and pace of automation have been fundamentally changed. AI technologies boost the performance and expand the capability of conventional AI applications. An example is the collaborative robots. Collaborative robotic arms are able to learn the motion and path demonstrated by human operators and perform the same task. AI also automates the process that used to require human participation. An example is the Hong Kong subway, where an AI program decides the distribution and job scheduling of engineers with more efficiency and reliability than human counterparts do.\n\nAnother aspect of process applications is the modeling large-scale systems. Cybermanufacturing systems are defined as a manufacturing service system that is networked and resilient to faults by evidence-based modeling and data-driven deep learning. Such a system deals with large and usually geographically distributed assets, which is hard to be modeled via conventional individual-asset physics-based model. With machine learning and optimization algorithms, a bottom-up framework considering machine health can leverage large samples of assets and automate the operation management, spare part inventory planning, and maintenance scheduling process.\n\nIndustrial AI can also be used for knowledge discovery by identifying insights in engineering systems. In aviation and aeronautics, AI has been playing a vital role in many critical areas, one of which is safety assurance and root cause. NASA is trying to proactively manage risks to aircraft safety by analyzing flight numeric data and text reports in parallel to not only detect anomalies but also relate it to the causal factors. This mined insight of why certain faults happen in the past will shed light on predictions of similar incidents in the future and prevent problems before they occur.\n\nPredictive and preventive maintenance through data-driven machine learning is also critical in cost reduction for industrial applications. Prognostics and health management (PHM) programs capture the opportunities at the shop floor by modeling equipment health degradation. The obtained information can be used for efficiency improvement and quality improvement.\n\nThe challenges of industrial AI to unlock the value lies in the transformation of raw data to intelligent predictions for rapid decision-making. In general, there are four major challenges in realizing industrial AI.\n\nEngineering systems now generate a lot of data and modern industry is indeed a big data environment. However, industrial data usually is structured, but may be low-quality. The “3B” issues of industrial big data is:\nThe quality of the data may be poor, and unlike other consumer-faced applications, data from industrial systems usually have clear physical meanings, which makes it harder to compensate the quality with volume.\nData collected for training machine learning models usually is lacking a comprehensive set of working conditions and health states/fault modes, which may cause false positives and false negatives in online implementation of AI systems.\nIndustrial data patterns can be highly transient and interpreting them requires domain expertise, which can hardly be harnessed by merely mining numeric data.\n\nProduction process happens fast and the equipment and work piece can be expensive, the AI applications need to be applied in real-time to be able to detect anomalies immediately to avoid waste and other consequences. Cloud-based solutions can be powerful and fast, but they still would not fit certain computation efficiency requirements. Edge computing may be a better choice in such scenario.\n\nUnlike consumer-faced AI recommendations systems which have a high tolerance for false positives and negatives, even a very low rate of false positives or negatives rate may cost the total credibility of AI systems. Industrial AI applications are usually dealing with critical issues related to safety, reliability, and operations. Any failure in predictions could incur a negative economic and/or safety impact on the users and discourage them to rely on AI systems.\n\nBesides prediction accuracy and performance fidelity, the industrial AI systems must also go beyond prediction results and give root cause analysis for anomalies. This requires that during development, data scientists need to work with domain experts and include domain know-how into the modeling process, and have the model adaptively learn and accumulate such insights as knowledge.\n",
    "id": "54008163",
    "title": "Industrial artificial intelligence"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=54073206",
    "text": "Google.ai\n\nGoogle.ai is a division of Google dedicated solely to artificial intelligence. It was announced at Google I/O 2017 by CEO Sundar Pichai.\n\n\n",
    "id": "54073206",
    "title": "Google.ai"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=54329238",
    "text": "Lawbot\n\nLawbots are a broad class of customer-facing legal AI applications that are used to automate specific legal tasks, such as document automation and legal research. Lawbots use various artificial intelligence techniques or other intelligent systems to limit humans' direct ongoing involvement in certain steps of a legal matter. The user interfaces on lawbots vary from smart searches and step-by-step forms to chatbots. Consumer and enterprise-facing lawbot solutions often do not require direct supervision from a legal professional. Depending on the task, some client-facing solutions used at law firms operate under an attorney supervision.\n\nIn the 2016 report, Deloitte estimated that more than 110,000 law jobs in just the United Kingdom alone could be gone within the next twenty years because of automation. That could result in the creation of more highly skilled jobs and reduction of paralegal and temp positions. Deloitte's report asserts that \"there is significant potential for high-skilled roles that involve repetitive processes to be automated by smart and self-learning algorithms\".\n\nIt has been widely estimated for at least the last generation that all the programs and resources devoted to ensuring access to justice address only 20% of the civil legal needs of low-income people in the United States. Drawing on this experience, in late 2011, the U.S. government-funded Legal Services Corporation decided to convene a summit of leaders to explore how best to use technology in the access-to-justice community. The group adopted a mission for The Summit on the Use of Technology to Expand Access to Justice (Summit) consistent with the magnitude of the challenge: \"to explore the potential of technology to move the United States toward providing some form of effective assistance to 100% of persons otherwise unable to afford an attorney for dealing with essential civil legal needs\".\n\nIn April 2017, joined by Microsoft and Pro Bono Net, the Legal Services Corporation (LSC) announced a pilot program to develop online, statewide legal portals to direct individuals with civil legal needs to the most appropriate forms of assistance. These portals will use cutting-edge, user-centered technology to help ensure that all people with civil legal needs can navigate their options and more easily access solutions and services available from legal aid, the courts, the private bar, and community partners.\n\n\n",
    "id": "54329238",
    "title": "Lawbot"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=54575571",
    "text": "Explainable Artificial Intelligence\n\nExplainable AI (XAI) is a neologism that has recently reached the parlance of artificial intelligence. Its purpose is to provide accountability when addressing technological innovations ascribed to dynamic and non-linearly programmed systems, e.g. artificial neural networks, deep learning, and genetic algorithms.\n\nIt is about asking the question of how algorithms arrive at their decisions. In a sense, it is a technical discipline providing operational tools that might be useful for explaining systems, such as in implementing a right to explanation.\n\nAI-related algorithmic (supervised and unsupervised) practices work on a model of success that orientates towards some form of correct state, with singular focus placed on an expected output. E.g., an image recognition algorithm's level of success will be based on the algorithm's ability to recognize certain objects, and failure to do so will indicate that the algorithm requires further tuning. As the tuning level is dynamic, closely correlated to function refinement and training data-set, granular understanding of the underlying operational vectors is rarely introspected.\n\nXAI aims to address this black-box approach and allow introspection of these dynamic systems tractable, allowing humans to understand how computational machines develop their own models for solving tasks.\n\nA universal definition of this term has yet to have been fully established; however, the DARPA XAI program defines its aims as the following:\n\n\nWhile the term \"Explainable AI\" is new, the field of understanding the knowledge embedded in machine learning systems itself has a long history. Researchers have long been interested in whether it is possible to extract rules from trained neural networks, and researchers in clinical expert systems creating neural network-powered decision support for clinicians have sought to develop dynamic explanations that allow these technologies to be more trusted and trustworthy in practice.\n\nLayerwise Relevance Propagation (LRP), first described in 2015, is a technique for determining which features in a particular input vector contribute most strongly to a neural network’s output.\n\nNewer however is the focus on explaining machine learning and AI to those whom the decisions concern, rather than the designers or direct users of decision systems. Since DARPA's introduction of its program in 2016, a number of new initiatives seek to address the issue of algorithmic accountability and provide transparency concerning how technologies within this domain function.\n\n\nA cross-section of industrial sectors will be affected by these requirements, as accountability is delegated to a greater or lesser extent from humans to machines.\n\nExamples of these effects have already been seen in the following sectors:\n\nAs regulators, official bodies and general users dependency on AI-based dynamic systems, clearer accountability will be required for decision making processes to ensure trust and transparency. Evidence of this requirement gaining more momentum can be seen with the launch of the first global conference exclusively dedicated to this emerging discipline, the International Joint Conference on Artificial Intelligence: Workshop on Explainable Artificial Intelligence (XAI).\n\n",
    "id": "54575571",
    "title": "Explainable Artificial Intelligence"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=54625345",
    "text": "Right to explanation\n\nIn the regulation of algorithms, particularly artificial intelligence and its subfield of machine learning, a right to explanation (or right to \"an\" explanation) is a right to be given an explanation for an output of the algorithm. Such rights primarily refer to individual rights to be given an explanation for decisions that significantly affect an individual, particularly legally or financially. For example, a person who applies for a loan and is denied may ask for an explanation, which could be \"Credit bureau X reports that you declared bankruptcy last year; this is the main factor in considering you too likely to default, and thus we will not give you the loan you applied for.\"\n\nSome such legal rights already exist, while the scope of a general \"right to explanation\" is a matter of ongoing debate.\n\nCredit score in the United States – more generally, credit actions – have a well-established right to explanation. Under the Equal Credit Opportunity Act (Regulation B of the Code of Federal Regulations),\nTitle 12, Chapter X, Part 1002, §1002.9, creditors are required to notify applicants of action taken in certain circumstances, and such notifications must provide specific reasons, as detailed in §1002.9(b)(2):\n\nThe official interpretation of this section details what types of statements are acceptable.\n\nCredit agencies and data analysis firms such as FICO comply with this regulation by providing a list of reasons (generally at most 4, per interpretation of regulations), consisting of a numeric (as identifier) and an associated explanation, identifying the main factors affecting a credit score. An example might be:\n\nThe European Union General Data Protection Regulation (enacted 2016, taking effect 2018), extends the automated decision-making rights in the 1995 Data Protection Directive to provide a legally disputed form of a right to an explanation, stated as such in Recital 71: \"[the data subject should have] the right ... to obtain an explanation of the decision reached\". In full:\n\nHowever, the extent to which the regulations themselves provide a \"right to explanation\" is heavily debated. There are two main strands of criticism. There are significant legal issues with the right as found in Article 22 — as recitals are not binding, and the right to an explanation is not mentioned in the binding articles of the text, having been removed during the legislative process. In addition, there are significant restrictions on the types of automated decisions that are covered — which must be both \"solely\" based on automated processing, and have legal or similarly significant effect — which limits their applicability in many of the cases of algorithmic controversy that have been picked up in the media.\n\nA second source of such a right has been pointed to in Article 15, the \"right of access by the data subject\". This restates a similar provision from the 1995 Data Protection Directive, allowing the data subject access to \"meaningful information about the logic involved\" in the same significant, solely automated decision-making, found in Article 22. Yet this too suffers from alleged challenges that relate to the timing of when this right can be drawn upon, as well as practical challenges that mean it may not be binding in many cases of public concern.\n\nIn France the 2016 \"Loi pour une République numérique\" (Digital Republic Act or \"loi numérique\") amends the country's administrative code to introduce a new provision for the explanation of decisions made by public sector bodies about individuals. It notes that where there is \"a decision taken on the basis of an algorithmic treatment\", the rules that define that treatment and its “principal characteristics” must be communicated to the citizen upon request, where there is not an exclusion (e.g. for national security or defence). These should include the following:\nScholars have noted that this right, while limited to administrative decisions, goes beyond the GDPR right to explicitly apply to decision support rather than decisions \"solely\" based on automated processing, as well as provides a framework for explaining specific decisions. Indeed, the GDPR automated decision-making rights in the European Union, one of the places a \"right to an explanation\" has been sought within, find their origins in French law in the late 1970s.\n\nSome argue that a \"right to explanation\" is at best unnecessary, at worst harmful, and threatens to stifle innovation. Specific criticisms include: favoring human decisions over machine decisions; being redundant with existing laws; and focusing on process over outcome.\n\nMore fundamentally, many algorithms used in machine learning are not easily explainable. For example, the output of a deep neural network depends on many layers of computations, connected in a complex way, and no one input or computation may be a dominant factor. The field of Explainable AI seeks to provide better explanations from existing algorithms, and algorithms that are more easily explainable, but it is a young and active field.\n\nSimilarly, human decisions often cannot be easily explained: they may be based on intuition or a \"gut feeling\" that is hard to put into words. Requiring machines to meet a higher standard than humans is thus arguably unreasonable.\n\n\n",
    "id": "54625345",
    "title": "Right to explanation"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=54677577",
    "text": "Extreme: Personal Assistant\n\nExtreme: Personal assistant is a virtual personal assistant developed and published by Multiverse app for Android in May 2016.\n\nUnlike most Android assistants, Extreme has a different, more nerdy looking User Interface. The application was primarily developed to mimic the functionality of Tony Stark's J.A.R.V.I.S. from Iron Man, giving a unique experience to the users. Users can interact with the app via natural voice or keyboard. In the current form, Extreme is able to search the Internet, narrate a joke, read important notifications, open applications and carry out other simple as well as complex tasks.\n\nExtreme was originally developed for Windows PC. The first beta was rolled out in November 2015 with a heavy futuristic looking UI. The software was highly based on Iron Man's JARVIS from Marvel's MCU, Played by Paul Bettany. The early build was coded on Visual Basic 6.0 and used windows speech recognition macros and Windows TTS to listen and speak commands and even housed an Antivirus program.\n\nAt its developmental stages, Extreme could Narrate stories, read news articles, read facebook and twitter notifications, open/close programs, provide a dictation mode and search the internet. Though the system was quite buggy at the time, Iron Man fans loved the software, mainly due to the unique looking interface.\n\nExtreme, at its current form, can carry out:\n\nExtreme also offers numerous pre-scripted responses to amusing questions. When a pre-scripted response is not found, Extreme uses Machine Learning to record user's response and reply appropriately the next time.\n\nCurrently, Extreme's Functionality is limited due to a single language support. No acknowledgment is given by the developers regarding the future regionalization of the app.\n",
    "id": "54677577",
    "title": "Extreme: Personal Assistant"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=54683549",
    "text": "Fast-and-frugal trees\n\nIn artificial intelligence and in heuristic decision-making, a fast-and-frugal tree is a type of classification tree. Fast-and-frugal trees can be used as decision-making tools which operate as lexicographic classifiers, and, if required, associate an action (decision) to each class or category. The original fast-and-frugal trees introduced in 2003 by Laura Martignon, et al. are simple both in execution and in construction, and constitute a kind of simple heuristic in the adaptive toolbox postulated by Gerd Gigerenzer and the Center for Adaptive Behavior and Cognition. Similar models had been previously used by Martignon & Hoffrage, 1999; Green and Mehr, 1997 ; Dhami and Ayton, 2001 ; Dhami and Harries, 2001 and Fischer, Steiner, Zucol, Berger, Martignon, et al. 2002 . \n\nYet recent developments in several fields of application have maintained the simplicity in execution, introducing subtle, relevant procedures for construction that have proven extremely useful for users.\n\nThe basic elements on which to ground a binary classification are (sets of) cues. The fast-and-frugal tree establishes a ranking and, according to the ranking, a “topology” of the tree. Once the ranking is established, the fast-and-frugal tree checks one cue at a time, and at each step, one of the possible outcomes of the considered cue is an exit node which allows for a decision.\n\nFast-and-frugal trees can also be described in terms of their building blocks. First, they have a search rule: They inspect cues in a specific order. Second, they have a stopping rule: Each cue has one value that leads to an exit node and hence to a classification, and another value that leads to consulting the next cue in the cue hierarchy (the exception being the last cue in the hierarchy, which has two exit nodes). Finally, they have a classification rule. \nFigure 1 illustrates a fast-and-frugal tree for classifying a patient as “high risk” of having a heart stroke and thus having to be sent to the “coronary care unit” or “low risk” and thus having to be sent to a “regular nursing bed (Green& Mehr, 1997). \nFast-and-frugal trees have been characterized mathematically as lexicographic classifiers (Martignon, Katsikopoulos and Woike, 2008) and as linear classifiers with non-compensatory weights (Martignon, Katsikopoulos and Woike, 2008) . Their “topology” and construction has been analysed using signal detection theory (Luan, Schooler and Gigerenzer, 2011) and their performance and robustness when compared to regression and CARts has been studied by Laskey and Martignon (2014). An extensive study on the robustness, the predictive value and sensitivity/specificity of Fast-And-Frugal trees compared to those of Naive Bayes and of Full Natural Frequency Trees has been carried out by.\n\nIn 2017, Phillips, Neth, Woike and Gaissmaier introduced the R package FFTrees, which uses new algorithms to construct and evaluate FFTs. A simulation study across ten real-world data sets shows that FFTs created by FFTrees can predict data as well as more complex classification algorithms, while remaining simple enough for anyone to understand and use.\n",
    "id": "54683549",
    "title": "Fast-and-frugal trees"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=54690679",
    "text": "Computational human modeling\n\nComputational human modeling is an interdisciplinary computational science that links the diverse fields of artificial intelligence, cognitive science, and computer vision with machine learning, mathematics, and cognitive psychology.\n\nComputational human modeling emphasizes descriptions of human for A.I. research and applications. \n\nResearch in computational human modeling can include computer vision studies on identify (face recognition), attributes (gender, age, skin color), expressions, geometry (3D face modeling, 3D body modeling), and activity (pose, gaze, actions, and social interactions).\n\n",
    "id": "54690679",
    "title": "Computational human modeling"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=25277457",
    "text": "ACROSS Project\n\nACROSS is a Singular Strategic R&D Project led by Treelogic funded by the Spanish Ministry of Industry, Tourism and Trade activities in the field of Robotics and Cognitive Computing over an execution time-frame from 2009 to 2011. ACROSS project involves a number higher than 100 researchers from 13 Spanish entities.\n\nACROSS modifies the design of social robotics, blocked in providing predefined services, going further by means of intelligent systems. These systems are able to self-reconfigure and modify their behavior autonomously through the capacity for understanding, learning and software remote access.\n\nIn order to provide an open framework for collaboration between universities, research centers and the Administration, ACROSS develops Open Source Services available to everybody.\n\nACROSS works in three application domains:\n\n\n\n",
    "id": "25277457",
    "title": "ACROSS Project"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=3630374",
    "text": "Neural computation\n\nNeural computation is information processing performed by networks of neurons. \n\nNeural computation can be studied for example by building models of neural computation. \n\nThere is a scientific journal dedicated to this subject, \"Neural Computation\". \n\nArtificial neural networks (ANN) is subfield of the research area machine learning. Work on ANNs has been somewhat inspired by knowledge of neural computation. \n",
    "id": "3630374",
    "title": "Neural computation"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=55269442",
    "text": "FEDOR (armed AI bot)\n\nFEDOR or Feodor () (an acronym of \"final experimental demonstration object research\") is the name of a Russian humanoid that is being trained to stay balanced while it shoots guns. Dmitry Rogozin, the Russian Deputy Prime Minister, insisted they are not creating a Terminator. This AI robot creation was originally developed for rescue missions. It has been reported that the FEDOR is designed to make \"colonies\" on the moon and is being prepared for a solo space mission in 2021.\n\nAt the same time, a \"robotic army\" is a controversial part of Russian military plans, especially after Vladimir Putin stated that \"the country that perfects AI will be 'ruler of the world. However, he retains his position that he doesn't want to see anyone \"monopolize\" the field, and that Russia will share the technology with the \"entire world\", as with nuclear technology.\n\n",
    "id": "55269442",
    "title": "FEDOR (armed AI bot)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=142224",
    "text": "A.I. Artificial Intelligence\n\nA.I. Artificial Intelligence, also known as A.I., is a 2001 American science fiction drama film directed by Steven Spielberg. The screenplay by Spielberg was based on a screen story by Ian Watson and the 1969 short story \"Supertoys Last All Summer Long\" by Brian Aldiss. The film was produced by Kathleen Kennedy, Spielberg and Bonnie Curtis. It stars Haley Joel Osment, Jude Law, Frances O'Connor, Brendan Gleeson and William Hurt. Set in a futuristic post-climate change society, \"A.I.\" tells the story of David (Osment), a childlike android uniquely programmed with the ability to love.\n\nDevelopment of \"A.I.\" originally began with producer-director Stanley Kubrick, after he acquired the rights to Aldiss' story in the early 1970s. Kubrick hired a series of writers until the mid-1990s, including Brian Aldiss, Bob Shaw, Ian Watson, and Sara Maitland. The film languished in protracted development for years, partly because Kubrick felt computer-generated imagery was not advanced enough to create the David character, whom he believed no child actor would convincingly portray. In 1995, Kubrick handed \"A.I.\" to Spielberg, but the film did not gain momentum until Kubrick's death in 1999. Spielberg remained close to Watson's film treatment for the screenplay.\n\nThe film divided critics, with the overall balance being positive, and grossed approximately $235 million. The film was nominated for two Academy Awards at the 74th Academy Awards, for Best Visual Effects and Best Original Score (by John Williams).\n\nIn a 2016 BBC poll of 177 critics around the world, Steven Spielberg's \"A.I. Artificial Intelligence\" was voted the eighty-third greatest film since 2000. \"A.I.\" is dedicated to Stanley Kubrick.\n\nIn the late 22nd century, rising sea levels from global warming have wiped out coastal cities such as Amsterdam, Venice, and New York, and drastically reduced the world's population. A new type of robots called Mecha, advanced humanoids capable of thoughts and emotions, have been created.\n\nDavid, a Mecha that resembles a human child and is programmed to display love for its owners, is sent to Henry Swinton, and his wife, Monica, as a replacement for their son, Martin, who has been placed in suspended animation until he can be cured of a rare disease. Monica warms to David and activates his imprinting protocol, causing him to have an enduring childlike love for her. David is befriended by Teddy, a robotic teddy bear, who cares for David's well-being.\n\nMartin is cured of his disease and brought home; as he recovers, he grows jealous of David. He makes David go to Monica in the night and cut off a lock of her hair. This upsets the parents, particularly Henry, who fears that the scissors are a weapon.\n\nAt a pool party, one of Martin's friends pokes David with a knife, activating his self-protection programming. David fearfully grabs Martin for protection and they accidentally fall into the pool. Martin is saved from drowning, but Henry persuades Monica to return David to his creator for destruction. Instead, Monica abandons both David and Teddy in the forest to hide as an unregistered Mecha.\n\nDavid is captured for an anti-Mecha \"Flesh Fair\", where obsolete and unlicensed Mecha are destroyed before cheering crowds. David is nearly killed, but tricks the crowd into thinking that he is human, and escapes with Gigolo Joe, a male prostitute Mecha who is on the run after being framed for murder. The two set out to find the Blue Fairy, whom David remembers from \"The Adventures of Pinocchio\", and believes can turn him into a human, allowing Monica to love him and take him home.\n\nJoe and David make their way to the resort town, Rouge City, where \"Dr. Know\", a holographic answer engine, leads them to the top of Rockefeller Center in the flooded ruins of Manhattan. There, David meets a copy of himself and destroys it. David then meets his creator, Professor Hobby, who tells David that he was built in the image of the professor's dead son David, and that more copies, including female versions called Darlene, are being manufactured.\n\nDisheartened, David falls from a ledge, but is rescued by Joe using their amphibicopter. David tells Joe he saw the Blue Fairy underwater and wants to go down to meet her. Joe is captured by the authorities using an electromagnet. David and Teddy use the amphibicopter to go to the Fairy, which turns out to be a statue at the now-sunken Coney Island. The two become trapped when the Wonder Wheel falls on their vehicle. David asks repeatedly to be turned into a real boy until the ocean freezes and is deactivated once his power source is drained.\n\nTwo thousand years later, humans have become extinct, and Manhattan is buried under glacial ice. The Mecha have evolved into an advanced, intelligent, silicon-based form. They find David and Teddy, and discover they are original Mecha that knew living humans, making them special.\n\nDavid is revived and walks to the frozen Fairy statue, which collapses when he touches it. The Mecha use David’s memories to reconstruct the Swinton home and explain to him that they cannot make him human. However, David insists that they recreate Monica from DNA in the lock of hair. The Mecha warn David that the clone can only live for a day, and that the process cannot be repeated. David spends the next day with Monica and Teddy. Before she drifts off to sleep, Monica tells David she has always loved him. Teddy climbs onto the bed and watches the two lie peacefully together.\n\n\nKubrick began development on an adaptation of \"Super-Toys Last All Summer Long\" in the late 1970s, hiring the story's author, Brian Aldiss, to write a film treatment. In 1985, Kubrick asked Steven Spielberg to direct the film, with Kubrick producing. Warner Bros. agreed to co-finance \"A.I.\" and cover distribution duties. The film labored in development hell, and Aldiss was fired by Kubrick over creative differences in 1989. Bob Shaw served as writer very briefly, leaving after six weeks because of Kubrick's demanding work schedule, and Ian Watson was hired as the new writer in March 1990. Aldiss later remarked, \"Not only did the bastard fire me, he hired my enemy [Watson] instead.\" Kubrick handed Watson \"The Adventures of Pinocchio\" for inspiration, calling \"A.I.\" \"a picaresque robot version of \"Pinocchio\"\".\n\nThree weeks later Watson gave Kubrick his first story treatment, and concluded his work on \"A.I.\" in May 1991 with another treatment, at 90 pages. Gigolo Joe was originally conceived as a GI Mecha, but Watson suggested changing him to a male prostitute. Kubrick joked, \"I guess we lost the kiddie market.\" In the meantime, Kubrick dropped \"A.I.\" to work on a film adaptation of \"Wartime Lies\", feeling computer animation was not advanced enough to create the David character. However, after the release of Spielberg's \"Jurassic Park\" (with its innovative use of computer-generated imagery), it was announced in November 1993 that production would begin in 1994. Dennis Muren and Ned Gorman, who worked on \"Jurassic Park\", became visual effects supervisors, but Kubrick was displeased with their previsualization, and with the expense of hiring Industrial Light & Magic.\n\nIn early 1994, the film was in pre-production with Christopher \"Fangorn\" Baker as concept artist, and Sara Maitland assisting on the story, which gave it \"a feminist fairy-tale focus\". Maitland said that Kubrick never referred to the film as \"A.I.\", but as \"Pinocchio\". Chris Cunningham became the new visual effects supervisor. Some of his unproduced work for \"A.I.\" can be seen on the DVD, \"The Work of Director Chris Cunningham\". Aside from considering computer animation, Kubrick also had Joseph Mazzello do a screen test for the lead role. Cunningham helped assemble a series of \"little robot-type humans\" for the David character. \"We tried to construct a little boy with a movable rubber face to see whether we could make it look appealing,\" producer Jan Harlan reflected. \"But it was a total failure, it looked awful.\" Hans Moravec was brought in as a technical consultant.\nMeanwhile, Kubrick and Harlan thought \"A.I.\" would be closer to Steven Spielberg's sensibilities as director. Kubrick handed the position to Spielberg in 1995, but Spielberg chose to direct other projects, and convinced Kubrick to remain as director. The film was put on hold due to Kubrick's commitment to \"Eyes Wide Shut\" (1999). After the filmmaker's death in March 1999, Harlan and Christiane Kubrick approached Spielberg to take over the director's position. By November 1999, Spielberg was writing the screenplay based on Watson's 90-page story treatment. It was his first solo screenplay credit since \"Close Encounters of the Third Kind\" (1977). Spielberg remained close to Watson's treatment, but removed various sex scenes with Gigolo Joe. Pre-production was briefly halted during February 2000, because Spielberg pondered directing other projects, which were \"Harry Potter and the Philosopher's Stone\", \"Minority Report\" and \"Memoirs of a Geisha\". The following month Spielberg announced that \"A.I.\" would be his next project, with \"Minority Report\" as a follow-up. When he decided to fast track \"A.I.\", Spielberg brought Chris Baker back as concept artist.\n\nThe original start date was July 10, 2000, but filming was delayed until August. Aside from a couple of weeks shooting on location in Oxbow Regional Park in Oregon, \"A.I.\" was shot entirely using sound stages at Warner Bros. Studios and the Spruce Goose Dome in Long Beach, California.\nThe Swinton house was constructed on Stage 16, while Stage 20 was used for Rouge City and other sets. Spielberg copied Kubrick's obsessively secretive approach to filmmaking by refusing to give the complete script to cast and crew, banning press from the set, and making actors sign confidentiality agreements. Social robotics expert Cynthia Breazeal served as technical consultant during production. Haley Joel Osment and Jude Law applied prosthetic makeup daily in an attempt to look shinier and robotic. Costume designer Bob Ringwood (\"Batman\", \"Troy\") studied pedestrians on the Las Vegas Strip for his influence on the Rouge City extras. Spielberg found post-production on \"A.I.\" difficult because he was simultaneously preparing to shoot \"Minority Report\".\n\nThe film's soundtrack was released by Warner Sunset Records in 2001. The original score was composed and conducted by John Williams and featured singers Lara Fabian on two songs and Josh Groban on one. The film's score also had a limited release as an official \"For your consideration Academy Promo\", as well as a complete score issue by La-La Land Records in 2015. The band Ministry appears in the film playing the song \"What About Us?\" (but the song does not appear on the official soundtrack album).\n\nWarner Bros. used an alternate reality game titled \"The Beast\" to promote the film. Over forty websites were created by Atomic Pictures in New York City (kept online at Cloudmakers.org) including the website for Cybertronics Corp. There were to be a series of video games for the Xbox video game console that followed the storyline of \"The Beast\", but they went undeveloped. To avoid audiences mistaking \"A.I.\" for a family film, no action figures were created, although Hasbro released a talking Teddy following the film's release in June 2001.\n\n\"A.I.\" had its premiere at the Venice Film Festival in 2001.\n\nThe film opened in 3,242 theaters in the United States on June 29, 2001, earning $29,352,630 during its opening weekend. \"A.I\" went on to gross $78.62 million in US totals as well as $157.31 million in foreign countries, coming to a worldwide total of $235.93 million.\n\nBased on 190 reviews collected by Rotten Tomatoes, 73% of the critics gave the film positive notices with a score of 6.6 out of 10. The website described the critical consensus perceiving the film as \"a curious, not always seamless, amalgamation of Kubrick's chilly bleakness and Spielberg's warm-hearted optimism. [The film] is, in a word, fascinating.\" By comparison, Metacritic collected an average score of 65, based on 32 reviews, which is considered favorable.\n\nProducer Jan Harlan stated that Kubrick \"would have applauded\" the final film, while Kubrick's widow Christiane also enjoyed \"A.I\". Brian Aldiss admired the film as well: \"I thought what an inventive, intriguing, ingenious, involving film this was. There are flaws in it and I suppose I might have a personal quibble but it's so long since I wrote it.\" Of the film's ending, he wondered how it might have been had Kubrick directed the film: \"That is one of the 'ifs' of film history—at least the ending indicates Spielberg adding some sugar to Kubrick's wine. The actual ending is overly sympathetic and moreover rather overtly engineered by a plot device that does not really bear credence. But it's a brilliant piece of film and of course it's a phenomenon because it contains the energies and talents of two brilliant filmmakers.\" Richard Corliss heavily praised Spielberg's direction, as well as the cast and visual effects. Roger Ebert gave the film four stars, saying that it was \"wonderful and maddening.\" Leonard Maltin, on the other hand, gives the film two stars out of four in his \"Movie Guide\", writing: \"[The] intriguing story draws us in, thanks in part to Osment's exceptional performance, but takes several wrong turns; ultimately, it just doesn't work. Spielberg rewrote the adaptation Stanley Kubrick commissioned of the Brian Aldiss short story 'Super Toys Last All Summer Long'; [the] result is a curious and uncomfortable hybrid of Kubrick and Spielberg sensibilities.\" However, he calls John Williams' music score \"striking\". Jonathan Rosenbaum compared \"A.I.\" to \"Solaris\" (1972), and praised both \"Kubrick for proposing that Spielberg direct the project and Spielberg for doing his utmost to respect Kubrick's intentions while making it a profoundly personal work.\" Film critic Armond White, of the \"New York Press\", praised the film noting that \"each part of David’s journey through carnal and sexual universes into the final eschatological devastation becomes as profoundly philosophical and contemplative as anything by cinema’s most thoughtful, speculative artists – Borzage, Ozu, Demy, Tarkovsky.\" Filmmaker Billy Wilder hailed \"A.I.\" as \"the most underrated film of the past few years.\" When British filmmaker Ken Russell saw the film, he wept during the ending.\n\nMick LaSalle gave a largely negative review. \"\"A.I.\" exhibits all its creators' bad traits and none of the good. So we end up with the structureless, meandering, slow-motion endlessness of Kubrick combined with the fuzzy, cuddly mindlessness of Spielberg.\" Dubbing it Spielberg's \"first boring movie\", LaSalle also believed the robots at the end of the film were aliens, and compared Gigolo Joe to the \"useless\" Jar Jar Binks, yet praised Robin Williams for his portrayal of a futuristic Albert Einstein. Peter Travers gave a mixed review, concluding \"Spielberg cannot live up to Kubrick's darker side of the future.\" But he still put the film on his top ten list that year for best movies. David Denby in \"The New Yorker\" criticized \"A.I.\" for not adhering closely to his concept of the Pinocchio character. Spielberg responded to some of the criticisms of the film, stating that many of the \"so called sentimental\" elements of \"A.I.\", including the ending, were in fact Kubrick's and the darker elements were his own. However, Sara Maitland, who worked on the project with Kubrick in the 1990s, claimed that one of the reasons Kubrick never started production on \"A.I.\" was because he had a hard time making the ending work. James Berardinelli found the film \"consistently involving, with moments of near-brilliance, but far from a masterpiece. In fact, as the long-awaited 'collaboration' of Kubrick and Spielberg, it ranks as something of a disappointment.\" Of the film's highly debated finale, he claimed, \"There is no doubt that the concluding 30 minutes are all Spielberg; the outstanding question is where Kubrick's vision left off and Spielberg's began.\"\n\nScreenwriter Ian Watson has speculated, \"Worldwide, \"A.I.\" was very successful (and the 4th highest earner of the year) but it didn't do quite so well in America, because the film, so I'm told, was too poetical and intellectual in general for American tastes. Plus, quite a few critics in America misunderstood the film, thinking for instance that the Giacometti-style beings in the final 20 minutes were aliens (whereas they were robots of the future who had evolved themselves from the robots in the earlier part of the film) and also thinking that the final 20 minutes were a sentimental addition by Spielberg, whereas those scenes were exactly what I wrote for Stanley and exactly what he wanted, filmed faithfully by Spielberg.\"\n\nIn 2002, Spielberg told film critic Joe Leydon that \"People pretend to think they know Stanley Kubrick, and think they know me, when most of them don't know either of us\". \"And what's really funny about that is, all the parts of \"A.I.\" that people assume were Stanley's were mine. And all the parts of \"A.I.\" that people accuse me of sweetening and softening and sentimentalizing were all Stanley's. The teddy bear was Stanley's. The whole last 20 minutes of the movie was completely Stanley's. The whole first 35, 40 minutes of the film – all the stuff in the house – was word for word, from Stanley's screenplay. This was Stanley's vision.\" \"Eighty percent of the critics got it all mixed up. But I could see why. Because, obviously, I've done a lot of movies where people have cried and have been sentimental. And I've been accused of sentimentalizing hard-core material. But in fact it was Stanley who did the sweetest parts of \"A.I.\", not me. I'm the guy who did the dark center of the movie, with the Flesh Fair and everything else. That's why he wanted me to make the movie in the first place. He said, 'This is much closer to your sensibilities than my own.\n\nUpon rewatching the film many years after its release, BBC film critic Mark Kermode apologized to Spielberg in an interview in January 2013 for \"getting it wrong\" on the film when he first viewed it in 2001. He now believes the film to be Spielberg's \"enduring masterpiece\".\n\nVisual effects supervisors Dennis Muren, Stan Winston, Michael Lantieri and Scott Farrar were nominated for the Academy Award for Best Visual Effects, while John Williams was nominated for Best Original Music Score. Steven Spielberg, Jude Law and Williams received nominations at the 59th Golden Globe Awards. \"A.I.\" was successful at the Saturn Awards, winning five awards, including Best Science Fiction Film along with Best Writing for Spielberg and Best Performance by a Younger Actor for Osment.\n\n\n",
    "id": "142224",
    "title": "A.I. Artificial Intelligence"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=55443695",
    "text": "Google Clips\n\nGoogle Clips is a miniature clip-on camera device developed by Google. It was announced during a Google event on 4 October 2017.\n\nWith a flashing LED that indicates it's recording, Google Clips automatically captures video clips at moments its machine learning algorithms determined to be interesting or relevant.\n\n\"The Independent\" wrote that Google Clips is \"an impressive little device, but one that also has the potential to feel very creepy.\"\n",
    "id": "55443695",
    "title": "Google Clips"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=55507995",
    "text": "Open Neural Network Exchange\n\nThe Open Neural Network Exchange (ONNX) is an open-source artificial intelligence ecosystem. ONNX is available on GitHub.\n\nIn September 2017 Facebook and Microsoft introduced a system for switching between machine learning frameworks such as PyTorch and Caffe2. Later, IBM, Huawei, Intel, AMD, ARM and Qualcomm announced support for the initiative.\n\nIn October 2017, Microsoft announced that it would add its Cognitive Toolkit and Project Brainwave platform to the initiative.\n\nThe initiative targets:\n\nAllow developers to more easily move between frameworks, some of which may be more desirable for specific phases of the development process, such as fast training, network architecture flexibility or inferencing on mobile devices.\n\nAllow hardware vendors and others to improve the performance of artificial neural networks of multiple frameworks at once by targeting the ONNX representation.\n\nONNX provides definitions of an extensible computation graph model, built-in operators and standard data types, focused on inferencing (evaluation).\n\nEach computation dataflow graph is a list of nodes that form an acyclic graph. Nodes have inputs and outputs. Each node is a call to an operator. Metadata documents the graph. Built-in operators are to be available on each ONNX-supporting framework.\n\nMicrosoft and Facebook are part of the Partnership on AI along with Apple, Amazon, Google and IBM that works to increase public awareness and boost research.\n\n",
    "id": "55507995",
    "title": "Open Neural Network Exchange"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1164",
    "text": "Artificial intelligence\n\nArtificial intelligence (AI, also machine intelligence, MI) is intelligence displayed by machines, in contrast with the natural intelligence (NI) displayed by humans and other animals. In computer science AI research is defined as the study of \"intelligent agents\": any device that perceives its environment and takes actions that maximize its chance of success at some goal. Colloquially, the term \"artificial intelligence\" is applied when a machine mimics \"cognitive\" functions that humans associate with other human minds, such as \"learning\" and \"problem solving\". See glossary of artificial intelligence.\n\nThe scope of AI is disputed: as machines become increasingly capable, tasks considered as requiring \"intelligence\" are often removed from the definition, a phenomenon known as the AI effect, leading to the quip \"AI is whatever hasn't been done yet.\" For instance, optical character recognition is frequently excluded from \"artificial intelligence\", having become a routine technology. Capabilities generally classified as AI include successfully understanding human speech, competing at a high level in strategic game systems (such as chess and Go), autonomous cars, intelligent routing in content delivery networks, military simulations, and interpreting complex data, including images and videos.\nArtificial intelligence was founded as an academic discipline in 1956, and in the years since has experienced several waves of optimism, followed by disappointment and the loss of funding (known as an \"AI winter\"), followed by new approaches, success and renewed funding. For most of its history, AI research has been divided into subfields that often fail to communicate with each other. These sub-fields are based on technical considerations, such as particular goals (e.g. \"robotics\" or \"machine learning\"), the use of particular tools (\"logic\" or \"neural networks\"), or deep philosophical differences. Subfields have also been based on social factors (particular institutions or the work of particular researchers). \nThe traditional problems (or goals) of AI research include reasoning, knowledge, planning, learning, natural language processing, perception and the ability to move and manipulate objects. General intelligence is among the field's long-term goals. Approaches include statistical methods, computational intelligence, and traditional symbolic AI. Many tools are used in AI, including versions of search and mathematical optimization, neural networks and methods based on statistics, probability and economics. The AI field draws upon computer science, mathematics, psychology, linguistics, philosophy, neuroscience, artificial psychology and many others.\nThe field was founded on the claim that human intelligence \"can be so precisely described that a machine can be made to simulate it\". This raises philosophical arguments about the nature of the mind and the ethics of creating artificial beings endowed with human-like intelligence, issues which have been explored by myth, fiction and philosophy since antiquity. Some people also consider AI a danger to humanity if it progresses unabatedly. Others believe that AI, unlike previous technological revolutions, will create a risk of mass unemployment.\nIn the twenty-first century, AI techniques have experienced a resurgence following concurrent advances in computer power, large amounts of data, and theoretical understanding; and AI techniques have become an essential part of the technology industry, helping to solve many challenging problems in computer science.\n\nWhile thought-capable artificial beings appeared as storytelling devices in antiquity, the idea of actually trying to build a machine to perform useful reasoning may have begun with Ramon Llull (c. 1300 CE). With his Calculus ratiocinator, Gottfried Leibniz extended the concept of the calculating machine (Wilhelm Schickard engineered the first one around 1623), intending to perform operations on concepts rather than numbers. Since the 19th century, artificial beings are common in fiction, as in Mary Shelley's \"Frankenstein\" or Karel Čapek's \"R.U.R. (Rossum's Universal Robots)\".\nThe study of mechanical or \"formal\" reasoning began with philosophers and mathematicians in antiquity. The study of mathematical logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as \"0\" and \"1\", could simulate any conceivable act of mathematical deduction. This insight, that digital computers can simulate any process of formal reasoning, is known as the Church–Turing thesis. Along with concurrent discoveries in neurology, information theory and cybernetics, this led researchers to consider the possibility of building an electronic brain. The first work that is now generally recognized as AI was McCullouch and Pitts' 1943 formal design for Turing-complete \"artificial neurons\".\nThe field of AI research was born at a workshop at Dartmouth College in 1956. Attendees Allen Newell (CMU), Herbert Simon (CMU), John McCarthy (MIT), Marvin Minsky (MIT) and Arthur Samuel (IBM) became the founders and leaders of AI research. They and their students produced programs that the press described as \"astonishing\": computers were learning checkers strategies (c. 1954) (and by 1959 were reportedly playing better than the average human), solving word problems in algebra, proving logical theorems (Logic Theorist, first run c. 1956) and speaking English. By the middle of the 1960s, research in the U.S. was heavily funded by the Department of Defense and laboratories had been established around the world. AI's founders were optimistic about the future: Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do\". Marvin Minsky agreed, writing, \"within a generation ... the problem of creating 'artificial intelligence' will substantially be solved\".\nThey failed to recognize the difficulty of some of the remaining tasks. Progress slowed and in 1974, in response to the criticism of Sir James Lighthill and ongoing pressure from the US Congress to fund more productive projects, both the U.S. and British governments cut off exploratory research in AI. The next few years would later be called an \"AI winter\", a period when obtaining funding for AI projects was difficult.\nIn the early 1980s, AI research was revived by the commercial success of expert systems, a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985 the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S and British governments to restore funding for academic research. However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting hiatus began.\nIn the late 1990s and early 21st century, AI began to be used for logistics, data mining, medical diagnosis and other areas. The success was due to increasing computational power (see Moore's law), greater emphasis on solving specific problems, new ties between AI and other fields and a commitment by researchers to mathematical methods and scientific standards. Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov on 11 May 1997.\nAdvanced statistical techniques (loosely known as deep learning), access to large amounts of data and faster computers enabled advances in machine learning and perception. By the mid 2010s, machine learning applications were used throughout the world. In a \"Jeopardy!\" quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy champions, Brad Rutter and Ken Jennings, by a significant margin. The Kinect, which provides a 3D body–motion interface for the Xbox 360 and the Xbox One use algorithms that emerged from lengthy AI research as do intelligent personal assistants in smartphones. In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps. In the 2017 Future of Go Summit, AlphaGo won a three-game match with Ke Jie, who at the time continuously held the world No. 1 ranking for two years. This marked the completion of a significant milestone in the development of Artificial Intelligence as Go is an extremely complex game, more so than Chess.\n\nAccording to Bloomberg's Jack Clark, 2015 was a landmark year for artificial intelligence, with the number of software projects that use AI within Google increased from a \"sporadic usage\" in 2012 to more than 2,700 projects. Clark also presents factual data indicating that error rates in image processing tasks have fallen significantly since 2011. He attributes this to an increase in affordable neural networks, due to a rise in cloud computing infrastructure and to an increase in research tools and datasets. Other cited examples include Microsoft's development of a Skype system that can automatically translate from one language to another and Facebook's system that can describe images to blind people.\n\nThe overall research goal of artificial intelligence is to create technology that allows computers and machines to function in an intelligent manner. The general problem of simulating (or creating) intelligence has been broken down into sub-problems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention.\n\nEarly researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, AI research had developed methods for dealing with uncertain or incomplete information, employing concepts from probability and economics.\nFor difficult problems, algorithms can require enormous computational resources—most experience a \"combinatorial explosion\": the amount of memory or computer time required becomes astronomical for problems of a certain size. The search for more efficient problem-solving algorithms is a high priority.\n\nHuman beings ordinarily use fast, intuitive judgments rather than step-by-step deduction that early AI research was able to model. AI has progressed using \"sub-symbolic\" problem solving: embodied agent approaches emphasize the importance of sensorimotor skills to higher reasoning; neural net research attempts to simulate the structures inside the brain that give rise to this skill; statistical approaches to AI mimic the human ability to guess.\n\nKnowledge representation and knowledge engineering are central to AI research. Many of the problems machines are expected to solve will require extensive knowledge about the world. Among the things that AI needs to represent are: objects, properties, categories and relations between objects; situations, events, states and time; causes and effects; knowledge about knowledge (what we know about what other people know); and many other, less well researched domains. A representation of \"what exists\" is an ontology: the set of objects, relations, concepts, and properties formally described so that software agents can interpret them. The semantics of these are captured as description logic concepts, roles, and individuals, and typically implemented as classes, properties, and individuals in the Web Ontology Language. The most general ontologies are called upper ontologies, which attempt to provide a foundation for all other knowledge by acting as mediators between domain ontologies that cover specific knowledge about a particular knowledge domain (field of interest or area of concern). Such formal knowledge representations are suitable for content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery via automated reasoning (inferring new statements based on explicitly stated knowledge), etc. Video events are often represented as SWRL rules, which can be used, among others, to automatically generate subtitles for constrained videos.\n\nAmong the most difficult problems in knowledge representation are:\n\nIntelligent agents must be able to set goals and achieve them. They need a way to visualize the future—a representation of the state of the world and be able to make predictions about how their actions will change it—and be able to make choices that maximize the utility (or \"value\") of available choices.\n\nIn classical planning problems, the agent can assume that it is the only system acting in the world, allowing the agent to be certain of the consequences of its actions. However, if the agent is not the only actor, then it requires that the agent can reason under uncertainty. This calls for an agent that can not only assess its environment and make predictions, but also evaluate its predictions and adapt based on its assessment.\n\nMulti-agent planning uses the cooperation and competition of many agents to achieve a given goal. Emergent behavior such as this is used by evolutionary algorithms and swarm intelligence.\n\nMachine learning, a fundamental concept of AI research since the field's inception, is the study of computer algorithms that improve automatically through experience.\n\nUnsupervised learning is the ability to find patterns in a stream of input. Supervised learning includes both classification and numerical regression. Classification is used to determine what category something belongs in, after seeing a number of examples of things from several categories. Regression is the attempt to produce a function that describes the relationship between inputs and outputs and predicts how the outputs should change as the inputs change. In reinforcement learning the agent is rewarded for good responses and punished for bad ones. The agent uses this sequence of rewards and punishments to form a strategy for operating in its problem space. These three types of learning can be analyzed in terms of decision theory, using concepts like utility. The mathematical analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory.\n\nWithin developmental robotics, developmental learning approaches are elaborated upon to allow robots to accumulate repertoires of novel skills through autonomous self-exploration, social interaction with human teachers, and the use of guidance mechanisms (active learning, maturation, motor synergies, etc.).\n\nNatural language processing gives machines the ability to read and understand human language. A sufficiently powerful natural language processing system would enable natural language user interfaces and the acquisition of knowledge directly from human-written sources, such as newswire texts. Some straightforward applications of natural language processing include information retrieval, text mining, question answering and machine translation.\n\nA common method of processing and extracting meaning from natural language is through semantic indexing. Although these indexes require a large volume of user input, it is expected that increases in processor speeds and decreases in data storage costs will result in greater efficiency.\n\nMachine perception is the ability to use input from sensors (such as cameras, microphones, tactile sensors, sonar and others) to deduce aspects of the world. Computer vision is the ability to analyze visual input. A few selected subproblems are speech recognition, facial recognition and object recognition.\n\nThe field of robotics is closely related to AI. Intelligence is required for robots to handle tasks such as object manipulation and navigation, with sub-problems such as localization, mapping, and motion planning. These systems require that an agent is able to: Be spatially cognizant of its surroundings, learn from and build a map of its environment, figure out how to get from one point in space to another, and execute that movement (which often involves compliant motion, a process where movement requires maintaining physical contact with an object).\n\nAffective computing is the study and development of systems that can recognize, interpret, process, and simulate human affects. It is an interdisciplinary field spanning computer sciences, psychology, and cognitive science. While the origins of the field may be traced as far back as the early philosophical inquiries into emotion, the more modern branch of computer science originated with Rosalind Picard's 1995 paper on \"affective computing\". A motivation for the research is the ability to simulate empathy, where the machine would be able to interpret human emotions and adapts its behavior to give an appropriate response to those emotions.\n\nEmotion and social skills are important to an intelligent agent for two reasons. First, being able to predict the actions of others by understanding their motives and emotional states allow an agent to make better decisions. Concepts such as game theory, decision theory, necessitate that an agent be able to detect and model human emotions. Second, in an effort to facilitate human–computer interaction, an intelligent machine may want to display emotions (even if it does not experience those emotions itself) to appear more sensitive to the emotional dynamics of human interaction.\n\nA sub-field of AI addresses creativity both theoretically (the philosophical psychological perspective) and practically (the specific implementation of systems that generate novel and useful outputs).\n\nMany researchers think that their work will eventually be incorporated into a machine with artificial general intelligence, combining all the skills mentioned above and even exceeding human ability in most or all these areas. A few believe that anthropomorphic features like artificial consciousness or an artificial brain may be required for such a project.\n\nMany of the problems above also require that general intelligence be solved. For example, even specific straightforward tasks, like machine translation, require that a machine read and write in both languages (NLP), follow the author's argument (reason), know what is being talked about (knowledge), and faithfully reproduce the author's original intent (social intelligence). A problem like machine translation is considered \"AI-complete\", but all of these problems need to be solved simultaneously in order to reach human-level machine performance.\n\nThere is no established unifying theory or paradigm that guides AI research. Researchers disagree about many issues. A few of the most long standing questions that have remained unanswered are these: should artificial intelligence simulate natural intelligence by studying psychology or neurology? Or is human biology as irrelevant to AI research as bird biology is to aeronautical engineering?\nCan intelligent behavior be described using simple, elegant principles (such as logic or optimization)? Or does it necessarily require solving a large number of completely unrelated problems?\nCan intelligence be reproduced using high-level symbols, similar to words and ideas? Or does it require \"sub-symbolic\" processing?\nJohn Haugeland, who coined the term GOFAI (Good Old-Fashioned Artificial Intelligence), also proposed that AI should more properly be referred to as synthetic intelligence, a term which has since been adopted by some non-GOFAI researchers.\n\nStuart Shapiro divides AI research into three approaches, which he calls computational psychology, computational philosophy, and computer science. Computational psychology is used to make computer programs that mimic human behavior. Computational philosophy, is used to develop an adaptive, free-flowing computer mind. Implementing computer science serves the goal of creating computers that can perform tasks that only people could previously accomplish. Together, the humanesque behavior, mind, and actions make up artificial intelligence.\n\nIn the 1940s and 1950s, a number of researchers explored the connection between neurology, information theory, and cybernetics. Some of them built machines that used electronic networks to exhibit rudimentary intelligence, such as W. Grey Walter's turtles and the Johns Hopkins Beast. Many of these researchers gathered for meetings of the Teleological Society at Princeton University and the Ratio Club in England. By 1960, this approach was largely abandoned, although elements of it would be revived in the 1980s.\n\nWhen access to digital computers became possible in the middle 1950s, AI research began to explore the possibility that human intelligence could be reduced to symbol manipulation. The research was centered in three institutions: Carnegie Mellon University, Stanford and MIT, and each one developed its own style of research. John Haugeland named these approaches to AI \"good old fashioned AI\" or \"GOFAI\". During the 1960s, symbolic approaches had achieved great success at simulating high-level thinking in small demonstration programs. Approaches based on cybernetics or neural networks were abandoned or pushed into the background.\nResearchers in the 1960s and the 1970s were convinced that symbolic approaches would eventually succeed in creating a machine with artificial general intelligence and considered this the goal of their field.\n\nEconomist Herbert Simon and Allen Newell studied human problem-solving skills and attempted to formalize them, and their work laid the foundations of the field of artificial intelligence, as well as cognitive science, operations research and management science. Their research team used the results of psychological experiments to develop programs that simulated the techniques that people used to solve problems. This tradition, centered at Carnegie Mellon University would eventually culminate in the development of the Soar architecture in the middle 1980s.\n\nUnlike Newell and Simon, John McCarthy felt that machines did not need to simulate human thought, but should instead try to find the essence of abstract reasoning and problem solving, regardless of whether people used the same algorithms. His laboratory at Stanford (SAIL) focused on using formal logic to solve a wide variety of problems, including knowledge representation, planning and learning. Logic was also the focus of the work at the University of Edinburgh and elsewhere in Europe which led to the development of the programming language Prolog and the science of logic programming.\n\nResearchers at MIT (such as Marvin Minsky and Seymour Papert) found that solving difficult problems in vision and natural language processing required ad-hoc solutions – they argued that there was no simple and general principle (like logic) that would capture all the aspects of intelligent behavior. Roger Schank described their \"anti-logic\" approaches as \"scruffy\" (as opposed to the \"neat\" paradigms at CMU and Stanford). Commonsense knowledge bases (such as Doug Lenat's Cyc) are an example of \"scruffy\" AI, since they must be built by hand, one complicated concept at a time.\n\nWhen computers with large memories became available around 1970, researchers from all three traditions began to build knowledge into AI applications. This \"knowledge revolution\" led to the development and deployment of expert systems (introduced by Edward Feigenbaum), the first truly successful form of AI software. The knowledge revolution was also driven by the realization that enormous amounts of knowledge would be required by many simple AI applications.\n\nBy the 1980s progress in symbolic AI seemed to stall and many believed that symbolic systems would never be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition. A number of researchers began to look into \"sub-symbolic\" approaches to specific AI problems. Sub-symbolic methods manage to approach intelligence without specific representations of knowledge.\n\nThis includes embodied, situated, behavior-based, and nouvelle AI. Researchers from the related field of robotics, such as Rodney Brooks, rejected symbolic AI and focused on the basic engineering problems that would allow robots to move and survive. Their work revived the non-symbolic viewpoint of the early cybernetics researchers of the 1950s and reintroduced the use of control theory in AI. This coincided with the development of the embodied mind thesis in the related field of cognitive science: the idea that aspects of the body (such as movement, perception and visualization) are required for higher intelligence.\n\nInterest in neural networks and \"connectionism\" was revived by David Rumelhart and others in the middle of the 1980s. Neural networks are an example of soft computing --- they are solutions to problems which cannot be solved with complete logical certainty, and where an approximate solution is often sufficient. Other soft computing approaches to AI include fuzzy systems, evolutionary computation and many statistical tools. The application of soft computing to AI is studied collectively by the emerging discipline of computational intelligence.\n\nIn the 1990s, AI researchers developed sophisticated mathematical tools to solve specific subproblems. These tools are truly scientific, in the sense that their results are both measurable and verifiable, and they have been responsible for many of AI's recent successes. The shared mathematical language has also permitted a high level of collaboration with more established fields (like mathematics, economics or operations research). Stuart Russell and Peter Norvig describe this movement as nothing less than a \"revolution\" and \"the victory of the neats\". Critics argue that these techniques (with few exceptions) are too focused on particular problems and have failed to address the long-term goal of general intelligence. There is an ongoing debate about the relevance and validity of statistical approaches in AI, exemplified in part by exchanges between Peter Norvig and Noam Chomsky.\n\n\nIn the course of 60 or so years of research, AI has developed a large number of tools to solve the most difficult problems in computer science. A few of the most general of these methods are discussed below.\n\nMany problems in AI can be solved in theory by intelligently searching through many possible solutions: Reasoning can be reduced to performing a search. For example, logical proof can be viewed as searching for a path that leads from premises to conclusions, where each step is the application of an inference rule. Planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis. Robotics algorithms for moving limbs and grasping objects use local searches in configuration space. Many learning algorithms use search algorithms based on optimization.\n\nSimple exhaustive searches are rarely sufficient for most real world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes. The solution, for many problems, is to use \"heuristics\" or \"rules of thumb\" that prioritize choices in favor of those that are more likely to reach a goal, and to do so in a shorter number of steps. In some search methodologies heuristics can also serve to entirely eliminate some choices that are unlikely to lead to a goal (called \"pruning the search tree\"). Heuristics supply the program with a \"best guess\" for the path on which the solution lies. Heuristics limit the search for solutions into a smaller sample size.\n\nA very different kind of search came to prominence in the 1990s, based on the mathematical theory of optimization. For many problems, it is possible to begin the search with some form of a guess and then refine the guess incrementally until no more refinements can be made. These algorithms can be visualized as blind hill climbing: we begin the search at a random point on the landscape, and then, by jumps or steps, we keep moving our guess uphill, until we reach the top. Other optimization algorithms are simulated annealing, beam search and random optimization.\n\nEvolutionary computation uses a form of optimization search. For example, they may begin with a population of organisms (the guesses) and then allow them to mutate and recombine, selecting only the fittest to survive each generation (refining the guesses). Forms of evolutionary computation include swarm intelligence algorithms (such as ant colony or particle swarm optimization) and evolutionary algorithms (such as genetic algorithms, gene expression programming, and genetic programming).\n\nLogic is used for knowledge representation and problem solving, but it can be applied to other problems as well. For example, the satplan algorithm uses logic for planning and inductive logic programming is a method for learning.\n\nSeveral different forms of logic are used in AI research. Propositional or sentential logic is the logic of statements which can be true or false. First-order logic also allows the use of quantifiers and predicates, and can express facts about objects, their properties, and their relations with each other. Fuzzy logic, is a version of first-order logic which allows the truth of a statement to be represented as a value between 0 and 1, rather than simply True (1) or False (0). Fuzzy systems can be used for uncertain reasoning and have been widely used in modern industrial and consumer product control systems. Subjective logic models uncertainty in a different and more explicit manner than fuzzy-logic: a given binomial opinion satisfies belief + disbelief + uncertainty = 1 within a Beta distribution. By this method, ignorance can be distinguished from probabilistic statements that an agent makes with high confidence.\n\nDefault logics, non-monotonic logics and circumscription are forms of logic designed to help with default reasoning and the qualification problem. Several extensions of logic have been designed to handle specific domains of knowledge, such as: description logics; situation calculus, event calculus and fluent calculus (for representing events and time); causal calculus; belief calculus; and modal logics.\n\nMany problems in AI (in reasoning, planning, learning, perception and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of powerful tools to solve these problems using methods from probability theory and economics.\n\nBayesian networks are a very general tool that can be used for a large number of problems: reasoning (using the Bayesian inference algorithm), learning (using the expectation-maximization algorithm), planning (using decision networks) and perception (using dynamic Bayesian networks). Probabilistic algorithms can also be used for filtering, prediction, smoothing and finding explanations for streams of data, helping perception systems to analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).\n\nA key concept from the science of economics is \"utility\": a measure of how valuable something is to an intelligent agent. Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis, and information value theory. These tools include models such as Markov decision processes, dynamic decision networks, game theory and mechanism design.\n\nThe simplest AI applications can be divided into two types: classifiers (\"if shiny then diamond\") and controllers (\"if shiny then pick up\"). Controllers do, however, also classify conditions before inferring actions, and therefore classification forms a central part of many AI systems. Classifiers are functions that use pattern matching to determine a closest match. They can be tuned according to examples, making them very attractive for use in AI. These examples are known as observations or patterns. In supervised learning, each pattern belongs to a certain predefined class. A class can be seen as a decision that has to be made. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.\n\nA classifier can be trained in various ways; there are many statistical and machine learning approaches. The most widely used classifiers are the neural network,\nkernel methods such as the support vector machine,\nk-nearest neighbor algorithm,\nGaussian mixture model,\nnaive Bayes classifier,\nand decision tree.\nThe performance of these classifiers have been compared over a wide range of tasks. Classifier performance depends greatly on the characteristics of the data to be classified. There is no single classifier that works best on all given problems; this is also referred to as the \"no free lunch\" theorem. Determining a suitable classifier for a given problem is still more an art than science.\n\nNeural networks are modeled after the neurons in the human brain, where a trained algorithm determines an output response for input signals. The study of non-learning artificial neural networks began in the decade before the field of AI research was founded, in the work of Walter Pitts and Warren McCullouch. Frank Rosenblatt invented the perceptron, a learning network with a single layer, similar to the old concept of linear regression. Early pioneers also include Alexey Grigorevich Ivakhnenko, Teuvo Kohonen, Stephen Grossberg, Kunihiko Fukushima, Christoph von der Malsburg, David Willshaw, Shun-Ichi Amari, Bernard Widrow, John Hopfield, Eduardo R. Caianiello, and others.\n\nThe main categories of networks are acyclic or feedforward neural networks (where the signal passes in only one direction) and recurrent neural networks (which allow feedback and short-term memories of previous input events). Among the most popular feedforward networks are perceptrons, multi-layer perceptrons and radial basis networks. Neural networks can be applied to the problem of intelligent control (for robotics) or learning, using such techniques as Hebbian learning, GMDH or competitive learning.\n\nToday, neural networks are often trained by the backpropagation algorithm, which had been around since 1970 as the reverse mode of automatic differentiation published by Seppo Linnainmaa, and was introduced to neural networks by Paul Werbos.\n\nHierarchical temporal memory is an approach that models some of the structural and algorithmic properties of the neocortex.\n\nDeep learning in artificial neural networks with many layers has transformed many important subfields of artificial intelligence, including computer vision, speech recognition, natural language processing and others.\n\nAccording to a survey, the expression \"Deep Learning\" was introduced to the Machine Learning community by Rina Dechter in 1986 and gained traction after\nIgor Aizenberg and colleagues introduced it to Artificial Neural Networks in 2000. The first functional Deep Learning networks were published by Alexey Grigorevich Ivakhnenko and V. G. Lapa in 1965. These networks are trained one layer at a time. Ivakhnenko's 1971 paper describes the learning of a deep feedforward multilayer perceptron with eight layers, already much deeper than many later networks. In 2006, a publication by Geoffrey Hinton and Ruslan Salakhutdinov introduced another way of pre-training many-layered feedforward neural networks (FNNs) one layer at a time, treating each layer in turn as an unsupervised restricted Boltzmann machine, then using supervised backpropagation for fine-tuning. Similar to shallow artificial neural networks, deep neural networks can model complex non-linear relationships. Over the last few years, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer.\n\nDeep learning often uses convolutional neural networks (CNNs), whose origins can be traced back to the Neocognitron introduced by Kunihiko Fukushima in 1980. In 1989, Yann LeCun and colleagues applied backpropagation to such an architecture. In the early 2000s, in an industrial application CNNs already processed an estimated 10% to 20% of all the checks written in the US.\nSince 2011, fast implementations of CNNs on GPUs have\nwon many visual pattern recognition competitions.\n\nDeep feedforward neural networks were used in conjunction with reinforcement learning by AlphaGo, Google Deepmind's program that was the first to beat a professional human Go player.\n\nEarly on, deep learning was also applied to sequence learning with recurrent neural networks (RNNs) which are general computers and can run arbitrary programs to process arbitrary sequences of inputs. The depth of an RNN is unlimited and depends on the length of its input sequence. RNNs can be trained by gradient descent but suffer from the vanishing gradient problem. In 1992, it was shown that unsupervised pre-training of a stack of recurrent neural networks can speed up subsequent supervised learning of deep sequential problems.\n\nNumerous researchers now use variants of a deep learning recurrent NN called the long short-term memory (LSTM) network published by Hochreiter & Schmidhuber in 1997. LSTM is often trained by Connectionist Temporal Classification (CTC). At Google, Microsoft and Baidu this approach has revolutionised speech recognition. For example, in 2015, Google's speech recognition experienced a dramatic performance jump of 49% through CTC-trained LSTM, which is now available through Google Voice to billions of smartphone users. Google also used LSTM to improve machine translation, Language Modeling and Multilingual Language Processing. LSTM combined with CNNs also improved automatic image captioning and a plethora of other applications.\n\nControl theory, the grandchild of cybernetics, has many important applications, especially in robotics.\n\nAI researchers have developed several specialized languages for AI research, including Lisp, Prolog, Python, and C++.\n\nIn 1950, Alan Turing proposed a general procedure to test the intelligence of an agent now known as the Turing test. This procedure allows almost all the major problems of artificial intelligence to be tested. However, it is a very difficult challenge and at present all agents fail.\n\nArtificial intelligence can also be evaluated on specific problems such as small problems in chemistry, hand-writing recognition and game-playing. Such tests have been termed subject matter expert Turing tests. Smaller problems provide more achievable goals and there are an ever-increasing number of positive results.\n\nFor example, performance at draughts (i.e. checkers) is optimal, performance at chess is high-human and nearing super-human (see computer chess: computers versus human) and performance at many everyday tasks (such as recognizing a face or crossing a room without bumping into something) is sub-human.\n\nA quite different approach measures machine intelligence through tests which are developed from \"mathematical\" definitions of intelligence. Examples of these kinds of tests start in the late nineties devising intelligence tests using notions from Kolmogorov complexity and data compression. Two major advantages of mathematical definitions are their applicability to nonhuman intelligences and their absence of a requirement for human testers.\n\nA derivative of the Turing test is the Completely Automated Public Turing test to tell Computers and Humans Apart (CAPTCHA). As the name implies, this helps to determine that a user is an actual person and not a computer posing as a human. In contrast to the standard Turing test, CAPTCHA is administered by a machine and targeted to a human as opposed to being administered by a human and targeted to a machine. A computer asks a user to complete a simple test then generates a grade for that test. Computers are unable to solve the problem, so correct solutions are deemed to be the result of a person taking the test. A common type of CAPTCHA is the test that requires the typing of distorted letters, numbers or symbols that appear in an image undecipherable by a computer.\n\nAI is relevant to any intellectual task. Modern artificial intelligence techniques are pervasive and are too numerous to list here. Frequently, when a technique reaches mainstream use, it is no longer considered artificial intelligence; this phenomenon is described as the AI effect.\n\nHigh-profile examples of AI include autonomous vehicles (such as drones and self-driving cars), medical diagnosis, creating art (such as poetry), proving mathematical theorems, playing games (such as Chess or Go), search engines (such as Google search), online assistants (such as Siri), image recognition in photographs, spam filtering, prediction of judicial decisions and targeting online advertisements.\n\nWith social media sites overtaking TV as a source for news for young people and news organisations increasingly reliant on social media platforms for generating distribution, major publishers now use artificial intelligence (AI) technology to post stories more effectively and generate higher volumes of traffic.\n\nThere are a number of competitions and prizes to promote research in artificial intelligence. The main areas promoted are: general machine intelligence, conversational behavior, data-mining, robotic cars, robot soccer and games.\n\nArtificial intelligence is breaking into the healthcare industry by assisting doctors. According to Bloomberg Technology, Microsoft has developed AI to help doctors find the right treatments for cancer.  There is a great amount of research and drugs developed relating to cancer. In detail, there are more than 800 medicines and vaccines to treat cancer. This negatively affects the doctors, because there are too many options to choose from, making it more difficult to choose the right drugs for the patients. Microsoft is working on a project to develop a machine called \"Hanover\". Its goal is to memorize all the papers necessary to cancer and help predict which combinations of drugs will be most effective for each patient. One project that is being worked on at the moment is fighting myeloid leukemia, a fatal cancer where the treatment has not improved in decades. Another study was reported to have found that artificial intelligence was as good as trained doctors in identifying skin cancers. Another study is using artificial intelligence to try and monitor multiple high-risk patients, and this is done by asking each patient numerous questions based on data acquired from live doctor to patient interactions.\n\nAccording to CNN, there was a recent study by surgeons at the Children's National Medical Center in Washington which successfully demonstrated surgery with an autonomous robot. The team supervised the robot while it performed soft-tissue surgery, stitching together a pig's bowel during open surgery, and doing so better than a human surgeon, the team claimed. IBM has created its own artificial intelligence computer, the IBM Watson, which has beaten human intelligence (at some levels). Watson not only won at the game show \"Jeopardy!\" against former champions, but, was declared a hero after successfully diagnosing a women who was suffering from leukemia.\n\nAdvancements in AI have contributed to the growth of the automotive industry through the creation and evolution of self-driving vehicles. As of 2016, there are over 30 companies utilizing AI into the creation of driverless cars. A few companies involved with AI include Tesla, Google, and Apple.\n\nMany components contribute to the functioning of self-driving cars. These vehicles incorporate systems such as braking, lane changing, collision prevention, navigation and mapping. Together, these systems, as well as high performance computers, are integrated into one complex vehicle.\n\nRecent developments in autonomous automobiles have made the innovation of self-driving trucks possible, though they are still in the testing phase. The UK government has passed legislation to begin testing of self-driving truck platoons in 2018. Self-driving truck platoons are a fleet of self-driving trucks following the lead of one non-self-driving truck, so the truck platoons aren't entirely autonomous yet. Meanwhile, the Daimler, a German automobile corporation, is testing the Freightliner Inspiration which is a semi-autonomous truck that will only be used on the highway.\n\nOne main factor that influences the ability for a driver-less automobile to function is mapping. In general, the vehicle would be pre-programmed with a map of the area being driven. This map would include data on the approximations of street light and curb heights in order for the vehicle to be aware of its surroundings. However, Google has been working on an algorithm with the purpose of eliminating the need for pre-programmed maps and instead, creating a device that would be able to adjust to a variety of new surroundings. Some self-driving cars are not equipped with steering wheels or brakes, so there has also been research focused on creating an algorithm that is capable of maintaining a safe environment for the passengers in the vehicle through awareness of speed and driving conditions.\n\nAnother factor that is influencing the ability for a driver-less automobile is the safety of the passenger. To make a driver-less automobile, engineers must program it to handle high risk situations. These situations could include a head on collision with pedestrians. The car's main goal should be to make a decision that would avoid hitting the pedestrians and saving the passengers in the car. But there is a possibility the car would need to make a decision that would put someone in danger. In other words, the car would need to decide to save the pedestrians or the passengers.The programing of the car in these situations is crucial to a successful driver-less automobile.\n\nFinancial institutions have long used artificial neural network systems to detect charges or claims outside of the norm, flagging these for human investigation. The use of AI in banking can be traced back to 1987 when Security Pacific National Bank in USA set-up a Fraud Prevention Task force to counter the unauthorised use of debit cards. Programs like Kasisto and Moneystream are using AI in financial services.\n\nBanks use artificial intelligence systems today to organize operations, maintain book-keeping, invest in stocks, and manage properties. AI can react to changes overnight or when business is not taking place. In August 2001, robots beat humans in a simulated financial trading competition. AI has also reduced fraud and financial crimes by monitoring behavioral patterns of users for any abnormal changes or anomalies.\n\nThe use of AI machines in the market in applications such as online trading and decision making has changed major economic theories. For example, AI based buying and selling platforms have changed the law of supply and demand in that it is now possible to easily estimate individualized demand and supply curves and thus individualized pricing. Furthermore, AI machines reduce information asymmetry in the market and thus making markets more efficient while reducing the volume of trades. Furthermore, AI in the markets limits the consequences of behavior in the markets again making markets more efficient. Other theories where AI has had impact include in rational choice, rational expectations, game theory, Lewis turning point, portfolio optimization and counterfactual thinking.\n\nArtificial intelligence is used to generate intelligent behaviors primarily in non-player characters (NPCs), often simulating human-like intelligence.\n\nA platform (or \"computing platform\") is defined as \"some sort of hardware architecture or software framework (including application frameworks), that allows software to run\". As Rodney Brooks pointed out many years ago, it is not just the artificial intelligence software that defines the AI features of the platform, but rather the actual platform itself that affects the AI that results, i.e., there needs to be work in AI problems on real-world platforms rather than in isolation.\n\nA wide variety of platforms has allowed different aspects of AI to develop, ranging from expert systems such as Cyc to deep-learning frameworks to robot platforms such as the Roomba with open interface. Recent advances in deep artificial neural networks and distributed computing have led to a proliferation of software libraries, including Deeplearning4j, TensorFlow, Theano and Torch.\n\nCollective AI is a platform architecture that combines individual AI into a collective entity, in order to achieve global results from individual behaviors. With its collective structure, developers can crowdsource information and extend the functionality of existing AI domains on the platform for their own use, as well as continue to create and share new domains and capabilities for the wider community and greater good. As developers continue to contribute, the overall platform grows more intelligent and is able to perform more requests, providing a scalable model for greater communal benefit. Organizations like SoundHound Inc. and the Harvard John A. Paulson School of Engineering and Applied Sciences have used this collaborative AI model.\n\nA McKinsey Global Institute study found a shortage of 1.5 million highly trained data and AI professionals and managers and a number of private bootcamps have developed programs to meet that demand, including free programs like The Data Incubator or paid programs like General Assembly.\n\nAmazon, Google, Facebook, IBM, and Microsoft have established a non-profit partnership to formulate best practices on artificial intelligence technologies, advance the public's understanding, and to serve as a platform about artificial intelligence. They stated: \"This partnership on AI will conduct research, organize discussions, provide thought leadership, consult with relevant third parties, respond to questions from the public and media, and create educational material that advance the understanding of AI technologies including machine perception, learning, and automated reasoning.\" Apple joined other tech companies as a founding member of the Partnership on AI in January 2017. The corporate members will make financial and research contributions to the group, while engaging with the scientific community to bring academics onto the board.\n\nThere are three philosophical questions related to AI:\n\nCan a machine be intelligent? Can it \"think\"?\n\n\n\n\n\n\n\nWidespread use of artificial intelligence could have unintended consequences that are dangerous or undesirable. Scientists from the Future of Life Institute, among others, described some short-term research goals to be how AI influences the economy, the laws and ethics that are involved with AI and how to minimize AI security risks. In the long-term, the scientists have proposed to continue optimizing function while minimizing possible security risks that come along with new technologies.\n\nMachines with intelligence have the potential to use their intelligence to make ethical decisions. Research in this area includes \"machine ethics\", \"artificial moral agents\", and the study of \"malevolent vs. friendly AI\".\n\nA common concern about the development of artificial intelligence is the potential threat it could pose to humanity. This concern has recently gained attention after mentions by celebrities including Stephen Hawking, Bill Gates, and Elon Musk. A group of prominent tech titans including Peter Thiel, Amazon Web Services and Musk have committed $1billion to OpenAI a nonprofit company aimed at championing responsible AI development. The opinion of experts within the field of artificial intelligence is mixed, with sizable fractions both concerned and unconcerned by risk from eventual superhumanly-capable AI.\n\nIn his book \"\", Nick Bostrom provides an argument that artificial intelligence will pose a threat to mankind. He argues that sufficiently intelligent AI, if it chooses actions based on achieving some goal, will exhibit convergent behavior such as acquiring resources or protecting itself from being shut down. If this AI's goals do not reflect humanity's - one example is an AI told to compute as many digits of pi as possible - it might harm humanity in order to acquire more resources or prevent itself from being shut down, ultimately to better achieve its goal.\n\nFor this danger to be realized, the hypothetical AI would have to overpower or out-think all of humanity, which a minority of experts argue is a possibility far enough in the future to not be worth researching. Other counterarguments revolve around humans being either intrinsically or convergently valuable from the perspective of an artificial intelligence.\n\nConcern over risk from artificial intelligence has led to some high-profile donations and investments. In January 2015, Elon Musk donated ten million dollars to the Future of Life Institute to fund research on understanding AI decision making. The goal of the institute is to \"grow wisdom with which we manage\" the growing power of technology. Musk also funds companies developing artificial intelligence such as Google DeepMind and Vicarious to \"just keep an eye on what's going on with artificial intelligence. I think there is potentially a dangerous outcome there.\"\n\nDevelopment of militarized artificial intelligence is a related concern. Currently, 50+ countries are researching battlefield robots, including the United States, China, Russia, and the United Kingdom. Many people concerned about risk from superintelligent AI also want to limit the use of artificial soldiers.\n\nJoseph Weizenbaum wrote that AI applications cannot, by definition, successfully simulate genuine human empathy and that the use of AI technology in fields such as customer service or psychotherapy was deeply misguided. Weizenbaum was also bothered that AI researchers (and some philosophers) were willing to view the human mind as nothing more than a computer program (a position now known as computationalism). To Weizenbaum these points suggest that AI research devalues human life.\n\nThe relationship between automation and employment is complicated. While automation eliminates old jobs, it also creates new jobs through micro-economic and macro-economic effects. Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; \"The Economist\" states that \"the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution\" is \"worth taking seriously\". Subjective estimates of the risk vary widely; for example, Michael Osborne and Carl Benedikt Frey estimate 47% of U.S. jobs are at \"high risk\" of potential automation, while an OECD report classifies only 9% of U.S. jobs as \"high risk\". Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy. Author Martin Ford and others go further and argue that a large number of jobs are routine, repetitive and (to an AI) predictable; Ford warns that these jobs may be automated in the next couple of decades, and that many of the new jobs may not be \"accessible to people with average capability\", even with retraining. Economists point out that in the past technology has tended to increase rather than reduce total employment, but acknowledge that \"we're in uncharted territory\" with AI.\n\nThis raises the issue of how ethically the machine should behave towards both humans and other AI agents. This issue was addressed by Wendell Wallach in his book titled \"Moral Machines\" in which he introduced the concept of artificial moral agents (AMA). For Wallach, AMAs have become a part of the research landscape of artificial intelligence as guided by its two central questions which he identifies as \"Does Humanity Want Computers Making Moral Decisions\" and \"Can (Ro)bots Really Be Moral\". For Wallach the question is not centered on the issue of \"whether\" machines can demonstrate the equivalent of moral behavior in contrast to the \"constraints\" which society may place on the development of AMAs.\n\nThe field of machine ethics is concerned with giving machines ethical principles, or a procedure for discovering a way to resolve the ethical dilemmas they might encounter, enabling them to function in an ethically responsible manner through their own ethical decision making. The field was delineated in the AAAI Fall 2005 Symposium on Machine Ethics: \"Past research concerning the relationship between technology and ethics has largely focused on responsible and irresponsible use of technology by human beings, with a few people being interested in how human beings ought to treat machines. In all cases, only human beings have engaged in ethical reasoning. The time has come for adding an ethical dimension to at least some machines. Recognition of the ethical ramifications of behavior involving machines, as well as recent and potential developments in machine autonomy, necessitate this. In contrast to computer hacking, software property issues, privacy issues and other topics normally ascribed to computer ethics, machine ethics is concerned with the behavior of machines towards human users and other machines. Research in machine ethics is key to alleviating concerns with autonomous systems—it could be argued that the notion of autonomous machines without such a dimension is at the root of all fear concerning machine intelligence. Further, investigation of machine ethics could enable the discovery of problems with current ethical theories, advancing our thinking about Ethics.\" Machine ethics is sometimes referred to as machine morality, computational ethics or computational morality. A variety of perspectives of this nascent field can be found in the collected edition \"Machine Ethics\" that stems from the AAAI Fall 2005 Symposium on Machine Ethics.\n\nPolitical scientist Charles T. Rubin believes that AI can be neither designed nor guaranteed to be benevolent. He argues that \"any sufficiently advanced benevolence may be indistinguishable from malevolence.\" Humans should not assume machines or robots would treat us favorably, because there is no \"a priori\" reason to believe that they would be sympathetic to our system of morality, which has evolved along with our particular biology (which AIs would not share). Hyper-intelligent software may not necessarily decide to support the continued existence of humanity, and would be extremely difficult to stop. This topic has also recently begun to be discussed in academic publications as a real source of risks to civilization, humans, and planet Earth.\n\nPhysicist Stephen Hawking, Microsoft founder Bill Gates, and SpaceX founder Elon Musk have expressed concerns about the possibility that AI could evolve to the point that humans could not control it, with Hawking theorizing that this could \"spell the end of the human race\".\n\nOne proposal to deal with this is to ensure that the first generally intelligent AI is 'Friendly AI', and will then be able to control subsequently developed AIs. Some question whether this kind of check could really remain in place.\n\nLeading AI researcher Rodney Brooks writes, \"I think it is a mistake to be worrying about us developing malevolent AI anytime in the next few hundred years. I think the worry stems from a fundamental error in not distinguishing the difference between the very real recent advances in a particular aspect of AI, and the enormity and complexity of building sentient volitional intelligence.\"\n\nIf an AI system replicates all key aspects of human intelligence, will that system also be sentient – will it have a mind which has conscious experiences? This question is closely related to the philosophical problem as to the nature of human consciousness, generally referred to as the hard problem of consciousness.\n\nComputationalism is the position in the philosophy of mind that the human mind or the human brain (or both) is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind-body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.\n\nThe philosophical position that John Searle has named \"strong AI\" states: \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\" Searle counters this assertion with his Chinese room argument, which asks us to look \"inside\" the computer and try to find where the \"mind\" might be.\n\nMary Shelley's \"Frankenstein\" considers a key issue in the ethics of artificial intelligence: if a machine can be created that has intelligence, could it also \"feel\"? If it can feel, does it have the same rights as a human? The idea also appears in modern science fiction, such as the film \"\", in which humanoid machines have the ability to feel emotions. This issue, now known as \"robot rights\", is currently being considered by, for example, California's Institute for the Future, although many critics believe that the discussion is premature. Some critics of transhumanism argue that any hypothetical robot rights would lie on a spectrum with animal rights and human rights. The subject is profoundly discussed in the 2010 documentary film \"Plug & Pray\".\n\nAre there limits to how intelligent machines – or human-machine hybrids – can be? A superintelligence, hyperintelligence, or superhuman intelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind. ‘’Superintelligence’’ may also refer to the form or degree of intelligence possessed by such an agent.\n\nIf research into Strong AI produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to recursive self-improvement. The new intelligence could thus increase exponentially and dramatically surpass humans. Science fiction writer Vernor Vinge named this scenario \"singularity\". Technological singularity is when accelerating progress in technologies will cause a runaway effect wherein artificial intelligence will exceed human intellectual capacity and control, thus radically changing or even ending civilization. Because the capabilities of such an intelligence may be impossible to comprehend, the technological singularity is an occurrence beyond which events are unpredictable or even unfathomable.\n\nRay Kurzweil has used Moore's law (which describes the relentless exponential improvement in digital technology) to calculate that desktop computers will have the same processing power as human brains by the year 2029, and predicts that the singularity will occur in 2045.\n\nRobot designer Hans Moravec, cyberneticist Kevin Warwick and inventor Ray Kurzweil have predicted that humans and machines will merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, which has roots in Aldous Huxley and Robert Ettinger, has been illustrated in fiction as well, for example in the manga \"Ghost in the Shell\" and the science-fiction series \"Dune\".\n\nIn the 1980s artist Hajime Sorayama's Sexy Robots series were painted and published in Japan depicting the actual organic human form with lifelike muscular metallic skins and later \"the Gynoids\" book followed that was used by or influenced movie makers including George Lucas and other creatives. Sorayama never considered these organic robots to be real part of nature but always unnatural product of the human mind, a fantasy existing in the mind even when realized in actual form.\n\nEdward Fredkin argues that \"artificial intelligence is the next stage in evolution\", an idea first proposed by Samuel Butler's \"Darwin among the Machines\" (1863), and expanded upon by George Dyson in his book of the same name in 1998.\n\nThought-capable artificial beings appeared as storytelling devices since antiquity.\n\nThe implications of a constructed machine exhibiting artificial intelligence have been a persistent theme in science fiction since the twentieth century. Early stories typically revolved around intelligent robots. The word \"robot\" itself was coined by Karel Čapek in his 1921 play \"R.U.R.\", the title standing for \"Rossum's Universal Robots\". Later, the SF writer Isaac Asimov developed the Three Laws of Robotics. He subsequently explored these in his many books, most notably the \"Multivac\" series about a super-intelligent computer of the same name. Asimov's laws are often brought up during layman discussions of machine ethics; while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.\n\nThe novel \"Do Androids Dream of Electric Sheep?\", by Philip K. Dick, tells a science fiction story about Androids and humans clashing in a futuristic world. Elements of artificial intelligence include the empathy box, mood organ, and the androids themselves. Throughout the novel, Dick portrays the idea that human subjectivity is altered by technology created with artificial intelligence.\n\nNowadays AI is firmly rooted in popular culture; intelligent robots appear in innumerable works. HAL 9000, the murderous computer in charge of the \"Discovery One\" spaceship in and \"\" (both 1968), is an example of the common \"robotic rampage\" archetype in science fiction movies. \"The Terminator\" (1984) and \"The Matrix\" (1999) provide additional widely familiar examples. In contrast, the rare loyal robots such as Gort from \"The Day the Earth Stood Still\" (1951) and Bishop from \"Aliens\" (1986) are less prominent in popular culture.\n\n\n\n",
    "id": "1164",
    "title": "Artificial intelligence"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=55690224",
    "text": "Msg.ai\n\nmsg.ai is an American artificial intelligence company and developer of human–computer interaction technologies founded in May 2015.\n\nFounded in May 2015 by Puneet Mehta, msg.ai was recruited by the global media and advertising agency, Universal McCann, to assist clients Heinz and BMW. \n\nmsg.ai worked with Sony Pictures to launch the first ever bot on Facebook Messenger for a $100M film, “Goosebumps” and subsequently joined Y Combinator as a member of the Winter 2016 class.\n\nIn 2016 the company received investments from several venture capital firms, including Index Ventures, Y Combinator, Bowery Capital, Salesforce Ventures and private entrepreneurs such as the founders of Google DeepMind and OpenAI.\n\nmsg.ai later partnered with Facebook’s Creative Shop and the Tommy Hilfiger fashion brand to develop a brand-specific bot with the goal of outperforming existing retail shopping bots.\n\nIn 2016 there was an update to the platform to incorporate multivariate testing. This type of testing, unlike traditional A/B testing, permits the monitoring of user interaction with the bot, adjustments to the bot’s tone, and experiments with the use of media.\n\nmsg.ai’s deep reinforcement learning platform allows for conversational AI and chatbots which engage through personalized interactions at scale and one-to-one relationships throughout the entire user experience.\n\nmsg.ai utilizes artificial intelligence to automate customized messages and engage in natural dialogues with deep reinforcement learning. This allows the bot to interact in a conversational manner and in a number of ways, including offering product recommendations based on user preferences, answering questions regarding availability and pricing, guiding customers towards a purchase, and providing assistance to complex issues.\n\nmsg.ai has collaborated with messaging platforms, creative agencies, and technology providers to build conversational AI for brands such as Heinz, BMW, Tommy Hilfiger, Signal, and The Anne Frank House.\n\n",
    "id": "55690224",
    "title": "Msg.ai"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=52649487",
    "text": "CloudSight\n\nCloudSight, Inc. is a Los Angeles, CA based technology company that specializes in image captioning and understanding.\n\nCloudSight was founded in 2012 by Bradford Folkens and Dominik Mazur. It was previously known as Image Searcher, Inc. and then CamFind, Inc., respectively. In 2016, the company was officially rebranded as CloudSight, Inc.\n\nAs of February, 2015, CloudSight has 11 patents pending for its technology. To date, CloudSight has recognized over 700 million images. This dataset becomes invaluable for neural network training and the development of artificial intelligence.\n\nOn October 11, 2012, CloudSight released its first mobile application into the AppStore, TapTapSee.\n\nTapTapSee is a mobile camera application designed specifically for the blind and visually impaired iOS and Android users. Its image recognition capabilities are powered by CloudSight API. The application utilizes the device’s camera and VoiceOver functions to photograph objects and identify them out loud for the user.\n\nTapTapSee was the 2014 recipient of the Access Award by the American Foundation for the Blind. In March 2013, TapTapSee was named App of the Month by the Royal National Institute for the Blind. At the end of 2013, TapTapSee was elected into the AppleVis iOS Hall of Fame.\n\nOn April 7, 2013, CloudSight released its second mobile application into the AppStore, CamFind. The mobile application surpassed 1,000,000 downloads within the first seven months after its release into the Apple AppStore.\n\nCamFind is a visual search engine application that utilizes image recognition to photograph, identify, and provide information on any object, at any angle. Its image recognition capabilities are powered by CloudSight API.\n\nCamFind is available in the Apple AppStore and Google Play Store. In February 2015, CamFind was released on Google Glass via MyGlass.\n\nIn April 2015, CloudSight evolved CamFind a step further by releasing social network capabilities within the application. The app now features the ability for users to share the items they identify, as well as see the items that others are identifying with CamFind.\n\nIn September 2013, CloudSight released its CloudSight API to the general public.\n\n\"The CloudSight API employs deep learning, a technology that simulates the human brain, 'learning' from its mistakes over time, and is the same technology powering CamFind.\"\n",
    "id": "52649487",
    "title": "CloudSight"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=55567666",
    "text": "New Lab\n\nNew Lab opened in June, 2016, as a multi-disciplinary technology center. Housed in Building 128 of the Brooklyn Navy Yard, the $35 million project serves as a hardware-focused shared workspace, research lab, and hatchery for socially-oriented tech manufacturing.\n\nUsing the MIT Media Lab as a model, the impetus for the independent organization was to provide space and services to new manufacturing enterprises. Current members work in fields such as robotics, connected devices, energy, nanotechnology, life sciences, and urban tech.\n\nMedia coverage of New Lab has focused on the company's role in revitalizing the Brooklyn Navy Yard, its public-private partnership lease structure, and Urban Tech initiative with the New York CIty Economic Development Corporation.\n\nDavid Belt and partner Scott Cohen formed the concept for New Lab in 2011 after prospecting the decaying Building 128 with Navy Yard president David Ehrenberg. The partners found the maritime manufacturing history of the structure, specifically the manufacturing innovations that took place there, synchronous with their aim to provide a platform for emerging hardware technologies in New York City. The city was abundant in resources and opportunities for entrepreneurs working in software, Belt said in a recent interview, but space, tools and resources for those working in the new manufacturing hardware community were lacking.\n\nBelt leveraged his development firm, Macro Sea, a company that specializes \"in bringing historic properties back into cultural relevance,\" to obtain funding, architectural expertise, and begin constructing a lease with the city of New York. The Navy Yard was in the initial stages of its current revitalization at the time and, because the city owned the property, special arrangements were needed to develop there.\n\nCohen began scouting the companies who would comprise their core members and helped work to capitalize them. To date, venture capitalists have invested approximately $250 million in New Lab and its members.\n\nThe land predating New Lab has a rich historical cultural lineage and narrative of experimental and innovative breakthroughs. That past was a major factor in the decision to develop New Lab in the Navy Yard. Before colonial settlement, the area that would become the Navy Yard served as a clamming site for the Lenape Native Americans. It was then settled by the Dutch and sold to a developer, thus beginning its employment as a center for manufacturing. Among the technological advancements that took place at the Navy Yard are: the first use of the steam-powered pile driver; construction of the first undersea telegraph cable; development of a commercialized form of anesthetic ether by E.R. Squibb; and a broadcast of the first woman to sing over the radio, opera singer Eugenia Farrar performed \"I Love You Truly.\"\n\nConstruction of navy ships like the Fulton II, a first-of-its-kind steam-powered warship, and fabrication of the USS Arizona, state-of-the-art among its peers, induced many influential manufacturing process refinements and advancements.\n\nIn interviews, Belt and Cohen both cite this maritime and technological history as inspiration for New Lab, both in guiding the renovation of the facility and in shaping its mission.\n\nAccording to a Brooklyn Navy Yard Development Corporation document,128 was raised in 1899 as a \"steel structure... used to assemble large boiler engines and fabricated sections of naval vessels.\" It served as the primary machine shop for every major ship launched during World Wars One and Two. Designed to accommodate the significant height of a warship, the sequence of its hulking steel girders resembles an airport hangar. 128 has been slated for, but avoided, plans for non-naval readaptation. The City of New York sought to adapt it for reuse as a \"food complex at one point,\" but the effort was not sustained.\n\nMarvel Architects, New Lab's architect of record, along with DBI, Belt's project management firm, worked together to craft and execute the renovation. Press regarding New Lab often states that the company occupies Building 128 of the Navy Yard, but this is slightly misleading in that 128 is a complex of warehouses and New Lab occupies the southernmost portion.\n\nRecladding the building's armature and repurposing of the 51,000 ft machine shop into an 84,000 ft multidisciplinary design, prototyping, and advanced manufacturing space took approximately 5 years and continued until the company's full opening in September 2016. The undertaking utilized approximately 9,000 lbs of steel in total according to the developer.\n\nA guiding principle of the redesign was to harmonize of the needs of the forthcoming lab environment with the original structural features. Modern workplace design elements were fused with the 19th century industrial characteristics of the building's centerline:\n\nNicko Elliott, Macro Sea's design director, and other members of the development firm's team collaborated to design custom or retrofitted furniture for the space. Elliott told The Commercial Observer that the Danish midcentury chairs employed throughout the open areas were purchased at auction in Chicago. They were then reupholstered in yellow, green, and red fabrics. The desks were also made custom and paired with Herman Miller Eames chairs that New Lab co-founder Scott Cohen discovered in some Soho trash outside the Scholastic Building. Cohen phoned Belt and, in their typical hands-on fashion, the two rented a truck and adopted the seats for New Lab.\n\nNew Lab's open floor design was intended, spatially, to reinforce its mission, the layout meant to encourage member companies to collaborate and cross-pollinate ideas. Communal meeting rooms, office pods, and interior plazas on both floors emphasize the developer’s intention to create a collaborative design and fabrication center.\n\nUpon completion, the rebuild subdivided Building 128's usable space into: Private studios = 31,664 ft; Open private studios = 6,226 ft; Fabrication lab = 6,834 ft; Cafe kitchen = 600 ft; Conference rooms = 2,014 ft; Coworking desks = 144; Flex space = 66 desks.\n\nThere is an additional 6,174 ft of event space which hosts talks, hackathons, and new manufacturing events such as the recent Urban Tech Hub launch.\n\nAdditive Manufacturing (3D printing) technology is a component of the design process for many New Lab residents. Prototyping shops are a distinguishing feature of the hardware-centric facility. New Lab leverages partnerships with firms like AutoDesk, Stratasys, BigRep, Haas, and others to provide and maintain equipment. The organization has amassed several million dollars of digital fabrication and manufacturing machine assets such as 3D Printers, electronic workbenches, fabrication tools, and CNC equipment since its opening.\n\n\n\nEighty companies and 400 people currently work at New Lab. Members are typically growth-stage companies with anywhere between 3-20 employees. While New Lab's membership model includes open one-year agreements with public enterprises and individuals seeking shared and private studios or reserved workspaces, the organization gives preference to hardware-focused companies with positive, socially oriented goals. \"We want to make sure that they have a level of optimism and humanism in the work that they're doing so that they're making the world better,\" Belt stated in a recent profile. The core members, who occupy the larger spaces in the building, were selected and recruited by the company for their intellectual property, visibility, and potential to scale.\n\n",
    "id": "55567666",
    "title": "New Lab"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=11924",
    "text": "Game theory\n\nGame theory is \"the study of mathematical models of conflict and cooperation between intelligent rational decision-makers\". Game theory is mainly used in economics, political science, and psychology, as well as logic, computer science and biology. Originally, it addressed zero-sum games, in which one person's gains result in losses for the other participants. Today, game theory applies to a wide range of behavioral relations, and is now an umbrella term for the science of logical decision making in humans, animals, and computers.\n\nModern game theory began with the idea regarding the existence of mixed-strategy equilibria in two-person zero-sum games and its proof by John von Neumann. Von Neumann's original proof used the Brouwer fixed-point theorem on continuous mappings into compact convex sets, which became a standard method in game theory and mathematical economics. His paper was followed by the 1944 book \"Theory of Games and Economic Behavior\", co-written with Oskar Morgenstern, which considered cooperative games of several players. The second edition of this book provided an axiomatic theory of expected utility, which allowed mathematical statisticians and economists to treat decision-making under uncertainty.\n\nThis theory was developed extensively in the 1950s by many scholars. Game theory was later explicitly applied to biology in the 1970s, although similar developments go back at least as far as the 1930s. Game theory has been widely recognized as an important tool in many fields. With the Nobel Memorial Prize in Economic Sciences going to game theorist Jean Tirole in 2014, eleven game-theorists have now won the economics Nobel Prize. John Maynard Smith was awarded the Crafoord Prize for his application of game theory to biology.\n\nEarly discussions of examples of two-person games occurred long before the rise of modern, mathematical game theory. The first known discussion of game theory occurred in a letter written by Charles Waldegrave, an active Jacobite, and uncle to James Waldegrave, a British diplomat, in 1713. In this letter, Waldegrave provides a minimax mixed strategy solution to a two-person version of the card game le Her, and the problem is now known as Waldegrave problem. James Madison made what we now recognize as a game-theoretic analysis of the ways states can be expected to behave under different systems of taxation. In his 1838 \"Recherches sur les principes mathématiques de la théorie des richesses\" (\"Researches into the Mathematical Principles of the Theory of Wealth\"), Antoine Augustin Cournot considered a duopoly and presents a solution that is a restricted version of the Nash equilibrium.\n\nIn 1913, Ernst Zermelo published \"Über eine Anwendung der Mengenlehre auf die Theorie des Schachspiels\" (\"On an Application of Set Theory to the Theory of the Game of Chess\"). It proved that the optimal chess strategy is strictly determined. This paved the way for more general theorems.\n\nIn 1938, the Danish mathematical economist Frederik Zeuthen proved that the mathematical model had a winning strategy by using Brouwer's fixed point theorem. In his 1938 book \"Applications aux Jeux de Hasard\" and earlier notes, Émile Borel proved a minimax theorem for two-person zero-sum matrix games only when the pay-off matrix was symmetric. Borel conjectured that non-existence of mixed-strategy equilibria in two-person zero-sum games would occur, a conjecture that was proved false.\n\nGame theory did not really exist as a unique field until John von Neumann published a paper in 1928. Von Neumann's original proof used Brouwer's fixed-point theorem on continuous mappings into compact convex sets, which became a standard method in game theory and mathematical economics. His paper was followed by his 1944 book \"Theory of Games and Economic Behavior\" co-authored with Oskar Morgenstern. The second edition of this book provided an axiomatic theory of utility, which reincarnated Daniel Bernoulli's old theory of utility (of the money) as an independent discipline. Von Neumann's work in game theory culminated in this 1944 book. This foundational work contains the method for finding mutually consistent solutions for two-person zero-sum games. During the following time period, work on game theory was primarily focused on cooperative game theory, which analyzes optimal strategies for groups of individuals, presuming that they can enforce agreements between them about proper strategies.\n\nIn 1950, the first mathematical discussion of the prisoner's dilemma appeared, and an experiment was undertaken by notable mathematicians Merrill M. Flood and Melvin Dresher, as part of the RAND Corporation's investigations into game theory. RAND pursued the studies because of possible applications to global nuclear strategy. Around this same time, John Nash developed a criterion for mutual consistency of players' strategies, known as Nash equilibrium, applicable to a wider variety of games than the criterion proposed by von Neumann and Morgenstern. Nash proved that every n-player, non-zero-sum (not just 2-player zero-sum) non-cooperative game has what is now known as a Nash equilibrium.\n\nGame theory experienced a flurry of activity in the 1950s, during which time the concepts of the core, the extensive form game, fictitious play, repeated games, and the Shapley value were developed. In addition, the first applications of game theory to philosophy and political science occurred during this time.\n\nIn 1965, Reinhard Selten introduced his solution concept of subgame perfect equilibria, which further refined the Nash equilibrium (later he would introduce trembling hand perfection as well). In 1994 Nash, Selten and Harsanyi became Economics Nobel Laureates for their contributions to economic game theory.\n\nIn the 1970s, game theory was extensively applied in biology, largely as a result of the work of John Maynard Smith and his evolutionarily stable strategy. In addition, the concepts of correlated equilibrium, trembling hand perfection, and common knowledge were introduced and analyzed.\n\nIn 2005, game theorists Thomas Schelling and Robert Aumann followed Nash, Selten and Harsanyi as Nobel Laureates. Schelling worked on dynamic models, early examples of evolutionary game theory. Aumann contributed more to the equilibrium school, introducing an equilibrium coarsening, correlated equilibrium, and developing an extensive formal analysis of the assumption of common knowledge and of its consequences.\n\nIn 2007, Leonid Hurwicz, together with Eric Maskin and Roger Myerson, was awarded the Nobel Prize in Economics \"for having laid the foundations of mechanism design theory\". Myerson's contributions include the notion of proper equilibrium, and an important graduate text: \"Game Theory, Analysis of Conflict\". Hurwicz introduced and formalized the concept of incentive compatibility.\n\nIn 2012, Alvin E. Roth and Lloyd S. Shapley were awarded the Nobel Prize in Economics \"for the theory of stable allocations and the practice of market design\" and, in 2014, the Nobel went to game theorist Jean Tirole.\n\nA game is \"cooperative\" if the players are able to form binding commitments externally enforced (e.g. through contract law). A game is \"non-cooperative\" if players cannot form alliances or if all agreements need to be self-enforcing (e.g. through credible threats).\n\nCooperative games are often analysed through the framework of \"cooperative game theory\", which focuses on predicting which coalitions will form, the joint actions that groups take and the resulting collective payoffs. It is opposed to the traditional \"non-cooperative game theory\" which focuses on predicting individual players' actions and payoffs and analyzing Nash equilibria.\n\nCooperative game theory provides a high-level approach as it only describes the structure, strategies and payoffs of coalitions, whereas non-cooperative game theory also looks at how bargaining procedures will affect the distribution of payoffs within each coalition. As non-cooperative game theory is more general, cooperative games can be analyzed through the approach of non-cooperative game theory (the converse does not hold) provided that sufficient assumptions are made to encompass all the possible strategies available to players due to the possibility of external enforcement of cooperation. While it would thus be optimal to have all games expressed under a non-cooperative framework, in many instances insufficient information is available to accurately model the formal procedures available to the players during the strategic bargaining process, or the resulting model would be of too high complexity to offer a practical tool in the real world. In such cases, cooperative game theory provides a simplified approach that allows to analyze the game at large without having to make any assumption about bargaining powers.\n\nA symmetric game is a game where the payoffs for playing a particular strategy depend only on the other strategies employed, not on who is playing them. If the identities of the players can be changed without changing the payoff to the strategies, then a game is symmetric. Many of the commonly studied 2×2 games are symmetric. The standard representations of chicken, the prisoner's dilemma, and the stag hunt are all symmetric games. Some scholars would consider certain asymmetric games as examples of these games as well. However, the most common payoffs for each of these games are symmetric.\n\nMost commonly studied asymmetric games are games where there are not identical strategy sets for both players. For instance, the ultimatum game and similarly the dictator game have different strategies for each player. It is possible, however, for a game to have identical strategies for both players, yet be asymmetric. For example, the game pictured to the right is asymmetric despite having identical strategy sets for both players.\n\nZero-sum games are a special case of constant-sum games, in which choices by players can neither increase nor decrease the available resources. In zero-sum games the total benefit to all players in the game, for every combination of strategies, always adds to zero (more informally, a player benefits only at the equal expense of others). Poker exemplifies a zero-sum game (ignoring the possibility of the house's cut), because one wins exactly the amount one's opponents lose. Other zero-sum games include matching pennies and most classical board games including Go and chess.\n\nMany games studied by game theorists (including the famed prisoner's dilemma) are non-zero-sum games, because the outcome has net results greater or less than zero. Informally, in non-zero-sum games, a gain by one player does not necessarily correspond with a loss by another.\n\nConstant-sum games correspond to activities like theft and gambling, but not to the fundamental economic situation in which there are potential gains from trade. It is possible to transform any game into a (possibly asymmetric) zero-sum game by adding a dummy player (often called \"the board\") whose losses compensate the players' net winnings.\n\nSimultaneous games are games where both players move simultaneously, or if they do not move simultaneously, the later players are unaware of the earlier players' actions (making them \"effectively\" simultaneous). Sequential games (or dynamic games) are games where later players have some knowledge about earlier actions. This need not be perfect information about every action of earlier players; it might be very little knowledge. For instance, a player may know that an earlier player did not perform one particular action, while he does not know which of the other available actions the first player actually performed.\n\nThe difference between simultaneous and sequential games is captured in the different representations discussed above. Often, normal form is used to represent simultaneous games, while extensive form is used to represent sequential ones. The transformation of extensive to normal form is one way, meaning that multiple extensive form games correspond to the same normal form. Consequently, notions of equilibrium for simultaneous games are insufficient for reasoning about sequential games; see subgame perfection.\n\nIn short, the differences between sequential and simultaneous games are as follows:\n\nAn important subset of sequential games consists of games of perfect information. A game is one of perfect information if all players know the moves previously made by all other players. Most games studied in game theory are imperfect-information games. Examples of perfect-information games include tic-tac-toe, checkers, infinite chess, and Go.\n\nMany card games are games of imperfect information, such as poker and bridge. Perfect information is often confused with complete information, which is a similar concept. Complete information requires that every player know the strategies and payoffs available to the other players but not necessarily the actions taken. Games of incomplete information can be reduced, however, to games of imperfect information by introducing \"moves by nature\".\n\nGames in which the difficulty of finding an optimal strategy stems from the multiplicity of possible moves are called combinatorial games. Examples include chess and go. Games that involve imperfect information may also have a strong combinatorial character, for instance backgammon. There is no unified theory addressing combinatorial elements in games. There are, however, mathematical tools that can solve particular problems and answer general questions.\n\nGames of perfect information have been studied in combinatorial game theory, which has developed novel representations, e.g. surreal numbers, as well as combinatorial and algebraic (and sometimes non-constructive) proof methods to solve games of certain types, including \"loopy\" games that may result in infinitely long sequences of moves. These methods address games with higher combinatorial complexity than those usually considered in traditional (or \"economic\") game theory. A typical game that has been solved this way is hex. A related field of study, drawing from computational complexity theory, is game complexity, which is concerned with estimating the computational difficulty of finding optimal strategies.\n\nResearch in artificial intelligence has addressed both perfect and imperfect information games that have very complex combinatorial structures (like chess, go, or backgammon) for which no provable optimal strategies have been found. The practical solutions involve computational heuristics, like alpha-beta pruning or use of artificial neural networks trained by reinforcement learning, which make games more tractable in computing practice.\n\nGames, as studied by economists and real-world game players, are generally finished in finitely many moves. Pure mathematicians are not so constrained, and set theorists in particular study games that last for infinitely many moves, with the winner (or other payoff) not known until \"after\" all those moves are completed.\n\nThe focus of attention is usually not so much on the best way to play such a game, but whether one player has a winning strategy. (It can be proven, using the axiom of choice, that there are gameseven with perfect information and where the only outcomes are \"win\" or \"lose\"for which \"neither\" player has a winning strategy.) The existence of such strategies, for cleverly designed games, has important consequences in descriptive set theory.\n\nMuch of game theory is concerned with finite, discrete games, that have a finite number of players, moves, events, outcomes, etc. Many concepts can be extended, however. Continuous games allow players to choose a strategy from a continuous strategy set. For instance, Cournot competition is typically modeled with players' strategies being any non-negative quantities, including fractional quantities.\n\nDifferential games such as the continuous pursuit and evasion game are continuous games where the evolution of the players' state variables is governed by differential equations. The problem of finding an optimal strategy in a differential game is closely related to the optimal control theory. In particular, there are two types of strategies: the open-loop strategies are found using the Pontryagin maximum principle while the closed-loop strategies are found using Bellman's Dynamic Programming method.\n\nA particular case of differential games are the games with a random time horizon. In such games, the terminal time is a random variable with a given probability distribution function. Therefore, the players maximize the mathematical expectation of the cost function. It was shown that the modified optimization problem can be reformulated as a discounted differential game over an infinite time interval.\n\nGames with an arbitrary, but finite, number of players are often called n-person games. Evolutionary game theory considers games involving a population of decision makers, where the frequency with which a particular decision is made can change over time in response to the decisions made by all individuals in the population. In biology, this is intended to model (biological) evolution, where genetically programmed organisms pass along some of their strategy programming to their offspring. In economics, the same theory is intended to capture population changes because people play the game many times within their lifetime, and consciously (and perhaps rationally) switch strategies.\n\nIndividual decision problems with stochastic outcomes are sometimes considered \"one-player games\". These situations are not considered game theoretical by some authors. They may be modeled using similar tools within the related disciplines of decision theory, operations research, and areas of artificial intelligence, particularly AI planning (with uncertainty) and multi-agent system. Although these fields may have different motivators, the mathematics involved are substantially the same, e.g. using Markov decision processes (MDP).\n\nStochastic outcomes can also be modeled in terms of game theory by adding a randomly acting player who makes \"chance moves\" (\"moves by nature\"). This player is not typically considered a third player in what is otherwise a two-player game, but merely serves to provide a roll of the dice where required by the game.\n\nFor some problems, different approaches to modeling stochastic outcomes may lead to different solutions. For example, the difference in approach between MDPs and the minimax solution is that the latter considers the worst-case over a set of adversarial moves, rather than reasoning in expectation about these moves given a fixed probability distribution. The minimax approach may be advantageous where stochastic models of uncertainty are not available, but may also be overestimating extremely unlikely (but costly) events, dramatically swaying the strategy in such scenarios if it is assumed that an adversary can force such an event to happen. (See Black swan theory for more discussion on this kind of modeling issue, particularly as it relates to predicting and limiting losses in investment banking.)\n\nGeneral models that include all elements of stochastic outcomes, adversaries, and partial or noisy observability (of moves by other players) have also been studied. The \"gold standard\" is considered to be partially observable stochastic game (POSG), but few realistic problems are computationally feasible in POSG representation.\n\nThese are games the play of which is the development of the rules for another game, the target or subject game. Metagames seek to maximize the utility value of the rule set developed. The theory of metagames is related to mechanism design theory.\n\nThe term metagame analysis is also used to refer to a practical approach developed by Nigel Howard. whereby a situation is framed as a strategic game in which stakeholders try to realise their objectives by means of the options available to them. Subsequent developments have led to the formulation of confrontation analysis.\n\nThese are games prevailing over all forms of society. Pooling games are repeated plays with changing payoff table in general over an experienced path and their equilibrium strategies usually take a form of evolutionary social convention and economic convention. Pooling game theory emerges to formally recognize the interaction between optimal choice in one play and the emergence of forthcoming payoff table update path, identify the invariance existence and robustness, and predict variance over time. The theory is based upon topological transformation classification of payoff table update over time to predict variance and invariance, and is also within the jurisdiction of the computational law of reachable optimality for ordered system.\n\nMean field game theory is the study of strategic decision making in very large populations of small interacting agents. This class of problems was considered in the economics literature by Boyan Jovanovic and Robert W. Rosenthal, in the engineering literature by Peter E. Caines and by mathematician Pierre-Louis Lions and Jean-Michel Lasry.\n\nThe games studied in game theory are well-defined mathematical objects. To be fully defined, a game must specify the following elements: the \"players\" of the game, the \"information\" and \"actions\" available to each player at each decision point, and the \"payoffs\" for each outcome. (Eric Rasmusen refers to these four \"essential elements\" by the acronym \"PAPI\".) A game theorist typically uses these elements, along with a solution concept of their choosing, to deduce a set of equilibrium strategies for each player such that, when these strategies are employed, no player can profit by unilaterally deviating from their strategy. These equilibrium strategies determine an equilibrium to the game—a stable state in which either one outcome occurs or a set of outcomes occur with known probability.\n\nMost cooperative games are presented in the characteristic function form, while the extensive and the normal forms are used to define noncooperative games.\n\nThe extensive form can be used to formalize games with a time sequencing of moves. Games here are played on trees (as pictured here). Here each vertex (or node) represents a point of choice for a player. The player is specified by a number listed by the vertex. The lines out of the vertex represent a possible action for that player. The payoffs are specified at the bottom of the tree. The extensive form can be viewed as a multi-player generalization of a decision tree. To solve any extensive form game, backward induction must be used. It involves working backwards up the game tree to determine what a rational player would do at the last vertex of the tree, what the player with the previous move would do given that the player with the last move is rational, and so on until the first vertex of the tree is reached.\n\nThe game pictured consists of two players. The way this particular game is structured (i.e., with sequential decision making and perfect information), \"Player 1\" \"moves\" first by choosing either \"F\" or \"U\" (Fair or Unfair). Next in the sequence, \"Player 2\", who has now seen \"Player 1\"s move, chooses to play either \"A\" or \"R\". Once \"Player 2\" has made his/ her choice, the game is considered finished and each player gets their respective payoff. Suppose that \"Player 1\" chooses \"U\" and then \"Player 2\" chooses \"A\": \"Player 1\" then gets a payoff of \"eight\" (which in real-world terms can be interpreted in many ways, the simplest of which is in terms of money but could mean things such as eight days of vacation or eight countries conquered or even eight more opportunities to play the same game against other players) and \"Player 2\" gets a payoff of \"two\".\n\nThe extensive form can also capture simultaneous-move games and games with imperfect information. To represent it, either a dotted line connects different vertices to represent them as being part of the same information set (i.e. the players do not know at which point they are), or a closed line is drawn around them. (See example in the imperfect information section.)\n\nThe normal (or strategic form) game is usually represented by a matrix which shows the players, strategies, and payoffs (see the example to the right). More generally it can be represented by any function that associates a payoff for each player with every possible combination of actions. In the accompanying example there are two players; one chooses the row and the other chooses the column. Each player has two strategies, which are specified by the number of rows and the number of columns. The payoffs are provided in the interior. The first number is the payoff received by the row player (Player 1 in our example); the second is the payoff for the column player (Player 2 in our example). Suppose that Player 1 plays \"Up\" and that Player 2 plays \"Left\". Then Player 1 gets a payoff of 4, and Player 2 gets 3.\n\nWhen a game is presented in normal form, it is presumed that each player acts simultaneously or, at least, without knowing the actions of the other. If players have some information about the choices of other players, the game is usually presented in extensive form.\n\nEvery extensive-form game has an equivalent normal-form game, however the transformation to normal form may result in an exponential blowup in the size of the representation, making it computationally impractical.\n\nIn games that possess removable utility, separate rewards are not given; rather, the characteristic function decides the payoff of each unity. The idea is that the unity that is 'empty', so to speak, does not receive a reward at all.\n\nThe origin of this form is to be found in John von Neumann and Oskar Morgenstern's book; when looking at these instances, they guessed that when a union formula_1 appears, it works against the fraction\nformula_2\nas if two individuals were playing a normal game. The balanced payoff of C is a basic function. Although there are differing examples that help determine coalitional amounts from normal games, not all appear that in their function form can be derived from such.\n\nFormally, a characteristic function is seen as: (N,v), where N represents the group of people and formula_3 is a normal utility.\n\nSuch characteristic functions have expanded to describe games where there is no removable utility.\n\nAs a method of applied mathematics, game theory has been used to study a wide variety of human and animal behaviors. It was initially developed in economics to understand a large collection of economic behaviors, including behaviors of firms, markets, and consumers. The first use of game-theoretic analysis was by Antoine Augustin Cournot in 1838 with his solution of the Cournot duopoly. The use of game theory in the social sciences has expanded, and game theory has been applied to political, sociological, and psychological behaviors as well.\n\nAlthough pre-twentieth century naturalists such as Charles Darwin made game-theoretic kinds of statements, the use of game-theoretic analysis in biology began with Ronald Fisher's studies of animal behavior during the 1930s. This work predates the name \"game theory\", but it shares many important features with this field. The developments in economics were later applied to biology largely by John Maynard Smith in his book \"Evolution and the Theory of Games\".\n\nIn addition to being used to describe, predict, and explain behavior, game theory has also been used to develop theories of ethical or normative behavior and to prescribe such behavior. In economics and philosophy, scholars have applied game theory to help in the understanding of good or proper behavior. Game-theoretic arguments of this type can be found as far back as Plato.\n\nThe primary use of game theory is to describe and model how human populations behave. Some scholars believe that by finding the equilibria of games they can predict how actual human populations will behave when confronted with situations analogous to the game being studied. This particular view of game theory has been criticized. It is argued that the assumptions made by game theorists are often violated when applied to real world situations. Game theorists usually assume players act rationally, but in practice, human behavior often deviates from this model. Game theorists respond by comparing their assumptions to those used in physics. Thus while their assumptions do not always hold, they can treat game theory as a reasonable scientific ideal akin to the models used by physicists. However, empirical work has shown that in some classic games, such as the centipede game, guess 2/3 of the average game, and the dictator game, people regularly do not play Nash equilibria. There is an ongoing debate regarding the importance of these experiments and whether the analysis of the experiments fully captures all aspects of the relevant situation.\n\nSome game theorists, following the work of John Maynard Smith and George R. Price, have turned to evolutionary game theory in order to resolve these issues. These models presume either no rationality or bounded rationality on the part of players. Despite the name, evolutionary game theory does not necessarily presume natural selection in the biological sense. Evolutionary game theory includes both biological as well as cultural evolution and also models of individual learning (for example, fictitious play dynamics).\n\nSome scholars, like Leonard Savage, see game theory not as a predictive tool for the behavior of human beings, but as a suggestion for how people ought to behave. Since a strategy, corresponding to a Nash equilibrium of a game constitutes one's best response to the actions of the other players – provided they are in (the same) Nash equilibrium – playing a strategy that is part of a Nash equilibrium seems appropriate. This normative use of game theory has also come under criticism.\n\nGame theory is a major method used in mathematical economics and business for modeling competing behaviors of interacting agents. Applications include a wide array of economic phenomena and approaches, such as auctions, bargaining, mergers & acquisitions pricing, fair division, duopolies, oligopolies, social network formation, agent-based computational economics, general equilibrium, mechanism design, and voting systems; and across such broad areas as experimental economics, behavioral economics, information economics, industrial organization, and political economy.\n\nThis research usually focuses on particular sets of strategies known as \"solution concepts\" or \"equilibria\". A common assumption is that players act rationally. In non-cooperative games, the most famous of these is the Nash equilibrium. A set of strategies is a Nash equilibrium if each represents a best response to the other strategies. If all the players are playing the strategies in a Nash equilibrium, they have no unilateral incentive to deviate, since their strategy is the best they can do given what others are doing.\n\nThe payoffs of the game are generally taken to represent the utility of individual players.\n\nA prototypical paper on game theory in economics begins by presenting a game that is an abstraction of a particular economic situation. One or more solution concepts are chosen, and the author demonstrates which strategy sets in the presented game are equilibria of the appropriate type. Naturally one might wonder to what use this information should be put. Economists and business professors suggest two primary uses (noted above): \"descriptive\" and \"prescriptive\".\n\nThe application of game theory to political science is focused in the overlapping areas of fair division, political economy, public choice, war bargaining, positive political theory, and social choice theory. In each of these areas, researchers have developed game-theoretic models in which the players are often voters, states, special interest groups, and politicians.\n\nEarly examples of game theory applied to political science are provided by Anthony Downs. In his book \"An Economic Theory of Democracy\", he applies the Hotelling firm location model to the political process. In the Downsian model, political candidates commit to ideologies on a one-dimensional policy space. Downs first shows how the political candidates will converge to the ideology preferred by the median voter if voters are fully informed, but then argues that voters choose to remain rationally ignorant which allows for candidate divergence. Game Theory was applied in 1962 to the Cuban missile crisis during the presidency of John F. Kennedy.\n\nIt has also been proposed that game theory explains the stability of any form of political government. Taking the simplest case of a monarchy, for example, the king, being only one person, does not and cannot maintain his authority by personally exercising physical control over all or even any significant number of his subjects. Sovereign control is instead explained by the recognition by each citizen that all other citizens expect each other to view the king (or other established government) as the person whose orders will be followed. Coordinating communication among citizens to replace the sovereign is effectively barred, since conspiracy to replace the sovereign is generally punishable as a crime. Thus, in a process that can be modeled by variants of the prisoner's dilemma, during periods of stability no citizen will find it rational to move to replace the sovereign, even if all the citizens know they would be better off if they were all to act collectively.\n\nA game-theoretic explanation for democratic peace is that public and open debate in democracies send clear and reliable information regarding their intentions to other states. In contrast, it is difficult to know the intentions of nondemocratic leaders, what effect concessions will have, and if promises will be kept. Thus there will be mistrust and unwillingness to make concessions if at least one of the parties in a dispute is a non-democracy.\n\nOn the other hand, game theory predicts that two countries may still go to war even if their leaders are cognizant of the costs of fighting. War may result from asymmetric information; two countries may have incentives to mis-represent the amount of military resources they have on hand, rendering them unable to settle disputes agreeably without resorting to fighting. Moreover, war may arise because of commitment problems: if two countries wish to settle a dispute via peaceful means, but each wishes to go back on the terms of that settlement, they may have no choice but to resort to warfare. Finally, war may result from issue indivisibilities.\n\nGame theory could also help predict a nation's responses when there is a new rule or law to be applied to that nation. One example would be Peter John Wood's (2013) research when he looked into what nations could do to help reduce climate change. Wood thought this could be accomplished by making treaties with other nations to reduce green house gas emissions. However, he concluded that this idea could not work because it would create a prisoner's dilemma to the nations.\n\nUnlike those in economics, the payoffs for games in biology are often interpreted as corresponding to fitness. In addition, the focus has been less on equilibria that correspond to a notion of rationality and more on ones that would be maintained by evolutionary forces. The best known equilibrium in biology is known as the \"evolutionarily stable strategy\" (ESS), first introduced in . Although its initial motivation did not involve any of the mental requirements of the Nash equilibrium, every ESS is a Nash equilibrium.\n\nIn biology, game theory has been used as a model to understand many different phenomena. It was first used to explain the evolution (and stability) of the approximate 1:1 sex ratios. suggested that the 1:1 sex ratios are a result of evolutionary forces acting on individuals who could be seen as trying to maximize their number of grandchildren.\n\nAdditionally, biologists have used evolutionary game theory and the ESS to explain the emergence of animal communication. The analysis of signaling games and other communication games has provided insight into the evolution of communication among animals. For example, the mobbing behavior of many species, in which a large number of prey animals attack a larger predator, seems to be an example of spontaneous emergent organization. Ants have also been shown to exhibit feed-forward behavior akin to fashion (see Paul Ormerod's \"Butterfly Economics\").\n\nBiologists have used the game of chicken to analyze fighting behavior and territoriality.\n\nAccording to Maynard Smith, in the preface to \"Evolution and the Theory of Games\", \"paradoxically, it has turned out that game theory is more readily applied to biology than to the field of economic behaviour for which it was originally designed\". Evolutionary game theory has been used to explain many seemingly incongruous phenomena in nature.\n\nOne such phenomenon is known as biological altruism. This is a situation in which an organism appears to act in a way that benefits other organisms and is detrimental to itself. This is distinct from traditional notions of altruism because such actions are not conscious, but appear to be evolutionary adaptations to increase overall fitness. Examples can be found in species ranging from vampire bats that regurgitate blood they have obtained from a night's hunting and give it to group members who have failed to feed, to worker bees that care for the queen bee for their entire lives and never mate, to vervet monkeys that warn group members of a predator's approach, even when it endangers that individual's chance of survival. All of these actions increase the overall fitness of a group, but occur at a cost to the individual.\n\nEvolutionary game theory explains this altruism with the idea of kin selection. Altruists discriminate between the individuals they help and favor relatives. Hamilton's rule explains the evolutionary rationale behind this selection with the equation c The coefficient values depend heavily on the scope of the playing field; for example if the choice of whom to favor includes all genetic living things, not just all relatives, we assume the discrepancy between all humans only accounts for approximately 1% of the diversity in the playing field, a co-efficient that was ½ in the smaller field becomes 0.995. Similarly if it is considered that information other than that of a genetic nature (e.g. epigenetics, religion, science, etc.) persisted through time the playing field becomes larger still, and the discrepancies smaller.\n\nGame theory has come to play an increasingly important role in logic and in computer science. Several logical theories have a basis in game semantics. In addition, computer scientists have used games to model interactive computations. Also, game theory provides a theoretical basis to the field of multi-agent systems.\n\nSeparately, game theory has played a role in online algorithms; in particular, the k-server problem, which has in the past been referred to as \"games with moving costs\" and \"request-answer games\". Yao's principle is a game-theoretic technique for proving lower bounds on the computational complexity of randomized algorithms, especially online algorithms.\n\nThe emergence of the internet has motivated the development of algorithms for finding equilibria in games, markets, computational auctions, peer-to-peer systems, and security and information markets. Algorithmic game theory and within it algorithmic mechanism design combine computational algorithm design and analysis of complex systems with economic theory.\n\nGame theory has been put to several uses in philosophy. Responding to two papers by , used game theory to develop a philosophical account of convention. In so doing, he provided the first analysis of common knowledge and employed it in analyzing play in coordination games. In addition, he first suggested that one can understand meaning in terms of signaling games. This later suggestion has been pursued by several philosophers since Lewis. Following game-theoretic account of conventions, Edna Ullmann-Margalit (1977) and Bicchieri (2006) have developed theories of social norms that define them as Nash equilibria that result from transforming a mixed-motive game into a coordination game.\n\nGame theory has also challenged philosophers to think in terms of interactive epistemology: what it means for a collective to have common beliefs or knowledge, and what are the consequences of this knowledge for the social outcomes resulting from agents' interactions. Philosophers who have worked in this area include Bicchieri (1989, 1993), Skyrms (1990), and Stalnaker (1999).\n\nIn ethics, some authors have attempted to pursue Thomas Hobbes' project of deriving morality from self-interest. Since games like the prisoner's dilemma present an apparent conflict between morality and self-interest, explaining why cooperation is required by self-interest is an important component of this project. This general strategy is a component of the general social contract view in political philosophy (for examples, see and ).\n\nOther authors have attempted to use evolutionary game theory in order to explain the emergence of human attitudes about morality and corresponding animal behaviors. These authors look at several games including the prisoner's dilemma, stag hunt, and the Nash bargaining game as providing an explanation for the emergence of attitudes about morality (see, e.g., and ).\n\n\n\n\n\nLists\n\n\n\n\n\n",
    "id": "11924",
    "title": "Game theory"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=43723774",
    "text": "Polarr\n\nPolarr is an artificial intelligence startup headquartered in downtown San Jose, CA building photo management and editing. The company was founded in August 2014 by two former Stanford University masters' students, Borui Wang, and Enhao Gong. The company received initial funding from Pejman Mar Ventures and StartX. Polarr currently offers two products: Polarr Photo Editor and Polarr Album Plus.\n\nIn February 2015, Polarr launched an online photo editor. It was one of the earliest in-browser photo editors capable of editing RAW images. \n\nIn June 2015, the first mobile version of Polarr Photo Editor was released for iOS. The app was similar in features to the in-browser photo editor and received more than 250,000 downloads within the first 48 hours of availability in the App Store. An Android version was launched a month later in the Google Play Store. \n\nIn the fall of 2015, Polarr released photo editors for Windows 10 and macOS. \n\nPolarr was named by Apple as Best of the App Store for 2015 and 2016. \n\nIn October 2017, Polarr released Album Plus for iOS. \n\nPolarr Photo Editor is the first product from Polarr. The image editor is available on all major platforms and operating systems. The company claimed that the app has been used by over 10 million photographers to edit images as of August 2017. \n\nAlbum Plus is the latest product from Polarr. It was launched in October 2017. Album Plus helps users to organize their photos on their iPhone's using AI on the phone, and not the cloud. \n\n",
    "id": "43723774",
    "title": "Polarr"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=56072531",
    "text": "Ask Tuki\n\nTUKI started out as a simple backend system for automating searches and information retrieval between editors and journalists.\n\nHowever, with the improvement of virtual assistants in smart phones and computers TUKI was eventually opened to the public in 2017.\n\nBy December 2017, Tuki was operating under its own branding on a small number of news sites and apps offering question and answer forums “Ask Tuki” and chat bot style information retrieval interfaces “Tuki Assistant”\n\nMost notably Al-Sahawat Times runs “Ask Tuki” software on their official site as a members’ peer-to-peer questions and answer service.\n\nFirst developed purely as an internal reference system for newspaper editors and journalists, TUKI Assistant is an Artificial intelligence program that integrates with websites, devices and Virtual assistant (artificial intelligence) programs to deliver a customized voice, handwriting and text input multi-lingual interaction.\n\nIn essence this means that the program, which is now aimed at corporate and governmental solutions can provide additional device control and information layered into existing systems such as siri.\n\n\"Ask TUKI anything. A unique combination of artificial intelligence, field experts and moderated peer to peer communities answer for you swiftly.\" \n\nASK Tuki is a Question and Answer forum similar to quora and Yahoo! Answers. Its key differences are that answers do not just come from peer-to-peer users but also paid, industry experts and artificial intelligence.\n\nUsers are graded on a 7 level system, bronze, silver, gold, platinum, diamond and verified. The different membership levels relate to the ratio of positive feedback and verified answers.\n\nTUKI, operates on Al-Sahawat Times official website and is a fully integrated system within the Newspaper. Al-Sahawat Times in turn is owned by The International Press and Media Group ( IPMG ). Thus TUKI is owned by IPMG\n\nThe copyrights and patent for TUKI is registered to the Chief executive officer of IPMG Sheikh. Dr. Shamsaldin Qais Sulayman al-Said.\n",
    "id": "56072531",
    "title": "Ask Tuki"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=56127293",
    "text": "Artificial intelligence arms race\n\nAn artificial intelligence arms race is a competition between two or more states to have its military forces equipped with the best \"artificial intelligence\" (AI). Since the mid-2010s, many analysts have argued that a such a global arms race for better artificial intelligence has already begun.\n\nRussian General Viktor Bondarev, commander-in-chief of the Russian air force, has stated that as early as February 2017, Russia has been working on AI-guided missiles that can decide to switch targets mid-flight. Reports by state-sponsored Russian media on potential military uses of AI increased in mid-2017. In May 2017, the CEO of Russia's Kronstadt Group, a defense contractor, stated that \"there already exist completely autonomous AI operation systems that provide the means for UAV clusters, when they fulfill missions autonomously, sharing tasks between them, and interact\", and that it is inevitable that \"swarms of drones\" will one day fly over combat zones. Russia has been testing several autonomous and semi-autonomous combat systems, such as Kalashnikov's \"neural net\" combat module, with a machine gun, a camera, and an AI that its makers claim can make its own targeting judgements without human intervention. In September 2017, during a National Knowledge Day address to over a million students in 16,000 Russian schools, Russian President Vladimir Putin stated \"Artificial intelligence is the future, not only for Russia but for all humankind... Whoever becomes the leader in this sphere will become the ruler of the world\".\n\nThe Russian government has strongly rejected any ban on lethal autonomous weapons systems, suggesting that such a ban could be ignored.\n\nAccording to Elsa Kania of the Center for a New American Security, \"China is no longer in a position of technological inferiority but rather sees itself as close to catching up with and overtaking the United States in AI. As such, the (Chinese military) intends to achieve an advantage through changing paradigms in warfare with military innovation, thus seizing the 'commanding heights'...of future military competition\". The close ties between Silicon Valley and China, and the open nature of the American research community, has made the West's most advanced AI technology easily available to China; in addition, Chinese industry has numerous home-grown AI accomplishments of its own, such as Baidu passing a notable Chinese-language speech recognition capability benchmark in 2015. As of 2017, Beijing's roadmap aims to create a $150 billion AI industry by 2030. Before 2013, Chinese defense procurement was mainly restricted to a few conglomerates; however, as of 2017, China often sources sensitive emerging technology such as drones and artificial intelligence from private start-up companies.\n\nChina published a position paper in 2016 questioning the adequacy of existing international law to address the eventuality of fully autonomous weapons, becoming the first permanent member of the U.N. Security Council to broach the issue.\n\nIn 2014, former Secretary of Defense Chuck Hagel posited the \"Third Offset Strategy\" that rapid advances in artificial intelligence will define the next generation of warfare. According to data science and analytics firm Govini, The U.S. Department of Defense increased investment in artificial intelligence, big data and cloud computing from $5.6 billion in 2011 to $7.4 billion in 2106. However, the civilian NSF budget for AI saw no increase in 2017. In addition, many Western tech companies are leery of being associated too closely with the U.S. military, for fear of losing access to China's market.\n\nThe U.S. has many military AI combat programs, such as the Sea Hunter autonomous warship, which is designed to operate for extended periods at sea without a single crew member, and to even guide itself in and out of port. As of 2017, a temporary US Department of Defense directive requires a human operator to be kept in the loop when it comes to the taking of human life by autonomous weapons systems.\n\nIn 2015, the UK government opposed a ban on lethal autonomous weapons, stating that \"international humanitarian law already provides sufficient regulation for this area\", but that all weapons employed by UK armed forces would be \"under human oversight and control\".\n\nIsrael's Harpy anti-radar \"fire and forget\" drone is designed to be launched by ground troops, and autonomously fly over an area to find and destroy radar that fits pre-determined criteria.\n\nThe South Korean Super aEgis II machine gun, unveiled in 2010, sees use both in South Korea and in the Middle East. It can identify, track, and destroy a moving target at a range of 4 km. While the technology can theoretically operate without human intervention, in practice safeguards are installed to require manual input. A South Korean manufacturer states, \"Our weapons don't sleep, like humans must. They can see in the dark, like humans can't. Our technology therefore plugs the gaps in human capability\", and they want to \"get to a place where our software can discern whether a target is friend, foe, civilian or military\".\n\nAs early as 2007, scholars such as AI professor Noel Sharkey have warned of \"an emerging arms race among the hi-tech nations to develop autonomous submarines, fighter jets, battleships and tanks that can find their own targets and apply violent force without the involvement of meaningful human decisions\". As early as 2014, AI specialists such as Steve Omohundro have been warning that \"An autonomous weapons arms race is already taking place\". Miles Brundage of the University of Oxford has argued an AI arms race might be somewhat mitigated through diplomacy: \"We saw in the various historical arms races that collaboration and dialog can pay dividends\". Over a hundred experts signed an open letter in 2017 calling on the UN to address the issue of lethal autonomous weapons; however, at a November 2017 session of the UN Convention on Certain Conventional Weapons (CCW), diplomats could not agree even on how to define such weapons. The Indian ambassador and chair of the CCW stated that agreement on rules remained a distant prospect. As of 2017, twenty-two countries have called for a full ban on lethal autonomous weapons.\n\nMany experts believe attempts to completely ban killer robots are likely to fail. A 2017 report from Harvard's Belfer Center predicts that AI has the potential to be as transformative as nuclear weapons. The report further argues that \"Preventing expanded military use of AI is likely impossible\" and that \"the more modest goal of safe and effective technology management must be pursued\", such as banning the attaching of an AI dead man's switch to a nuclear arsenal. Part of the impracticality is that detecting treaty violations would be extremely difficult.\n\nA 2015 open letter calling for the ban of lethal automated weapons systems has been signed by tens of thousands of citizens, including scholars such as physicist Stephen Hawking, Tesla magnate Elon Musk, and Apple's Steve Wozniak.\n\nProfessor Noel Sharkey of the University of Sheffield has warned that autonomous weapons will inevitably fall into the hands of terrorist groups such as the Islamic State.\n\n",
    "id": "56127293",
    "title": "Artificial intelligence arms race"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=55843837",
    "text": "Automated machine learning\n\nAutomated machine learning (AutoML) is the process of automating the end-to-end process of applying machine learning to real-world problems. In a typical machine learning application, practitioners must apply the appropriate data pre-processing, feature engineering, feature extraction, and feature selection methods that make the dataset amenable for machine learning. Following those preprocessing steps, practitioners must then perform algorithm selection and hyperparameter optimization to maximize the predictive performance of their final machine learning model. As many of these steps are often beyond the abilities of non-experts, AutoML was proposed as an artificial intelligence-based solution to the ever-growing challenge of applying machine learning. Automating the end-to-end process of applying machine learning offers the advantages of producing simpler solutions, faster creation of those solutions, and models that often outperform models that were designed by hand.\n\nAutomated machine learning can target various stages of the machine learning process:\n\nSoftware tackling various stages of AutoML:\n\n\n\n\n",
    "id": "55843837",
    "title": "Automated machine learning"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1931185",
    "text": "Automatic image annotation\n\nAutomatic image annotation (also known as automatic image tagging or linguistic indexing) is the process by which a computer system automatically assigns metadata in the form of captioning or keywords to a digital image. This application of computer vision techniques is used in image retrieval systems to organize and locate images of interest from a database.\n\nThis method can be regarded as a type of multi-class image classification with a very large number of classes - as large as the vocabulary size. Typically, image analysis in the form of extracted feature vectors and the training annotation words are used by machine learning techniques to attempt to automatically apply annotations to new images. The first methods learned the correlations between image features and training annotations, then techniques were developed using machine translation to try to translate the textual vocabulary with the 'visual vocabulary', or clustered regions known as \"blobs\". Work following these efforts have included classification approaches, relevance models and so on.\n\nThe advantages of automatic image annotation versus content-based image retrieval (CBIR) are that queries can be more naturally specified by the user. CBIR generally (at present) requires users to search by image concepts such as color and texture, or finding example queries. Certain image features in example images may override the concept that the user is really focusing on. The traditional methods of image retrieval such as those used by libraries have relied on manually annotated images, which is expensive and time-consuming, especially given the large and constantly growing image databases in existence.\n\nSome annotation engines are online, including the ALIPR.com real-time tagging engine developed by Pennsylvania State University researchers, and Behold.\n\n\n\n\n\n",
    "id": "1931185",
    "title": "Automatic image annotation"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=2090992",
    "text": "Dr. Sbaitso\n\nDr. Sbaitso is an artificial intelligence speech synthesis program released late in 1991 by Creative Labs for MS DOS-based personal computers.\n\nDr. Sbaitso was distributed with various sound cards manufactured by Creative Labs (the name was an acronym for Sound Blaster Artificial Intelligent Text to Speech Operator) in the early 1990s.\n\nThe program \"conversed\" with the user as if it were a psychologist, though most of its responses were along the lines of \"WHY DO YOU FEEL THAT WAY?\" rather than any sort of complicated interaction. When confronted with a phrase it could not understand, it would often reply with something such as \"THAT'S NOT MY PROBLEM\". Dr. Sbaitso repeated text out loud that was typed after the word \"SAY\". Repeated swearing or abusive behavior on the part of the user caused Dr. Sbaitso to \"break down\" in a \"PARITY ERROR\" before resetting itself.\n\nThe program introduced itself with the following lines:\nExample:\n\nThe program was designed to showcase the digitized voices the cards were able to produce, though the quality was far from lifelike.\n\n\n",
    "id": "2090992",
    "title": "Dr. Sbaitso"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=2760602",
    "text": "Polyworld\n\nPolyworld is a cross-platform (Linux, Mac OS X) program written by Larry Yaeger to evolve Artificial Intelligence through natural selection and evolutionary algorithms. \n\nIt uses the Qt graphics toolkit and OpenGL to display a graphical environment in which a population of trapezoid agents search for food, mate, have offspring, and prey on each other. The population is typically only in the hundreds, as each individual is rather complex and the environment consumes considerable computer resources. The graphical environment is necessary since the individuals actually move around the 2-D plane and must be able to \"see.\" Since some basic abilities, like eating carcasses or randomly generated food, seeing other individuals, mating or fighting with them, etc., are possible, a number of interesting behaviours have been observed to spontaneously arise after prolonged evolution, such as cannibalism, predators and prey, and mimicry.\n\nEach individual makes decisions based on a neural net using Hebbian learning; the neural net is derived from each individual's genome. The genome does not merely specify the wiring of the neural nets, but also determines their size, speed, color, mutation rate and a number of other factors. The genome is randomly mutated at a set probability, which are also changed in descendant organisms.\n\n",
    "id": "2760602",
    "title": "Polyworld"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=463838",
    "text": "Eurisko\n\nEurisko (Gr., \"I discover\") is a program written by Douglas Lenat in RLL-1, a representation language itself written in the Lisp programming language. A sequel to Automated Mathematician, it consists of heuristics, i.e. rules of thumb, including heuristics describing how to use and change its own heuristics. Lenat was frustrated by Automated Mathematician's constraint to a single domain and so developed Eurisko; his frustration with the effort of encoding domain knowledge for Eurisko led to Lenat's subsequent (and, , continuing) development of Cyc. Lenat envisions ultimately coupling the Cyc knowledgebase with the Eurisko discovery engine.\n\nDevelopment commenced at Carnegie Mellon in 1976 and continued at Stanford University in 1978 when Lenat returned to teach. \"For the first five years, nothing good came out of it\", Lenat said. But when the implementation was changed to a frame language based representation he called RLL (Representation Language Language), heuristic creation and modification became much simpler. Eurisko was then applied to a number of domains with surprising success, including VLSI chip design.\n\nLenat and Eurisko gained notoriety by submitting the winning fleet (a large number of stationary, lightly-armored ships with many small weapons) to the United States Traveller TCS national championship in 1981, forcing extensive changes to the game's rules. However, Eurisko won again in 1982 when the program discovered that the rules permitted the program to destroy its own ships, permitting it to continue to use much the same strategy. Tournament officials announced that if Eurisko won another championship the competition would be abolished; Lenat retired Eurisko from the game. The Traveller TCS wins brought Lenat to the attention of DARPA, which has funded much of his subsequent work.\n\nIn the first-season \"The X-Files\" episode \"Ghost in the Machine\", Eurisko is the name of a fictional software company responsible for the episode's \"monster of the week\", facilities management software known as \"Central Operating System\", or \"COS\". COS (described in the episode as an \"adaptive network\") is shown to be capable of learning when its designer arrives at Eurisko headquarters and is surprised to find that COS has given itself the ability to speak. The designer is forced to create a virus to destroy COS after COS commits a series of murders in an apparent effort to prevent its own destruction.\n\nLenat is also mentioned and Eurisko is discussed at the end of Richard Feynman's Computer Heuristics Lecture as part of the Idiosyncratic Thinking Workshop Series.\n\n",
    "id": "463838",
    "title": "Eurisko"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=6006314",
    "text": "TuVox\n\nTuVox is a company that produces VXML-based telephone speech-recognition applications to replace DTMF touch-tone systems for their clients.\n\nTuVox was founded in 2001 by Steven S. Pollock and Ashok Khosla, formerly of Apple Computer Corporation and Claris Corporation. Since then, TuVox has grown to over 40 employees and has US offices in Cupertino, California and Boca Raton, Florida as well as international offices in London, Vancouver and Sydney. In 2005, TuVox acquired the customers and hosting facilities of Net-By-Tel.\n\nOn July 22, 2010, West Interactive—a subsidiary of West Corporation—announced its acquisition of TuVox.\n\nTuVox clients include 1-800-Flowers.com, AMC Entertainment, American Airlines, British Airways, M&T Bank, Canon Inc., Gateway, Inc., Motorola, Progress Energy Inc., Telecom New Zealand, Time, Inc., BECU, Virgin America and USAA.\n\n\n",
    "id": "6006314",
    "title": "TuVox"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=8227051",
    "text": "Sinewave synthesis\n\nSinewave synthesis, or sine wave speech, is a technique for synthesizing speech by replacing the formants (main bands of energy) with pure tone whistles. The first sinewave synthesis program (\"SWS\") for the automatic creation of stimuli for perceptual experiments was developed by Philip Rubin at Haskins Laboratories in the 1970s. This program was subsequently used by Robert Remez, Philip Rubin, David Pisoni, and other colleagues to show that listeners can perceive continuous speech without traditional speech cues. This work paved the way for a view of speech as a dynamic pattern of trajectories through articulatory-acoustic space.\n\n\n",
    "id": "8227051",
    "title": "Sinewave synthesis"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=8642422",
    "text": "Language identification\n\nIn natural language processing, language identification or language guessing is the problem of determining which natural language given content is in. Computational approaches to this problem view it as a special case of text categorization, solved with various statistical methods.\n\nThere are several statistical approaches to language identification using different techniques to classify the data. One technique is to compare the compressibility of the text to the compressibility of texts in a set of known languages. This approach is known as mutual information based distance measure. The same technique can also be used to empirically construct family trees of languages which closely correspond to the trees constructed using historical methods. Mutual information based distance measure is essentially equivalent to more conventional model-based methods and is not generally considered to be either novel or better than simpler techniques. \n\nAnother technique, as described by Cavnar and Trenkle (1994) and Dunning (1994) is to create a language n-gram model from a \"training text\" for each of the languages. These models can be based on characters (Cavnar and Trenkle) or encoded bytes (Dunning); in the latter, language identification and character encoding detection are integrated. Then, for any piece of text needing to be identified, a similar model is made, and that model is compared to each stored language model. The most likely language is the one with the model that is most similar to the model from the text needing to be identified. This approach can be problematic when the input text is in a language for which there is no model. In that case, the method may return another, \"most similar\" language as its result. Also problematic for any approach are pieces of input text that are composed of several languages, as is common on the Web. \n\nFor a more recent method, see Řehůřek and Kolkus (2009). This method can detect multiple languages in an unstructured piece of text and works robustly on short texts of only a few words: something that the n-gram approaches struggle with.\n\nAn older statistical method by Grefenstette was based on the prevalence of certain function words (e.g., \"the\" in English).\n\nOne of the great bottlenecks of language identification systems is to distinguish between closely related languages. Similar languages like Serbian and Croatian or Indonesian and Malay present significant lexical and structural overlap, making it challenging for systems to discriminate between them. \n\nRecently, the DSL shared task has been organized providing a dataset (Tan et al., 2014) containing 13 different languages (and language varieties) in six language groups: Group A (Bosnian, Croatian, Serbian), Group B (Indonesian, Malaysian), Group C (Czech, Slovakian), Group D (Brazilian Portuguese, European Portuguese), Group E (Peninsular Spain, Argentine Spanish), Group F (American English, British English). The best system reached performance of over 95% results (Goutte et al., 2014). Results of the DSL shared task are described in Zampieri et al. 2014.\n\n\n\n",
    "id": "8642422",
    "title": "Language identification"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=8642531",
    "text": "Machine translation software usability\n\nThe sections below give objective criteria for evaluating the usability of machine translation software output.\n\nDo repeated translations converge on a single expression in both languages? I.e. does the translation method show stationarity or produce a canonical form? Does the translation become stationary without losing the original meaning? This metric has been criticized as not being well correlated with BLEU (BiLingual Evaluation Understudy) scores.\n\nIs the system adaptive to colloquialism, argot or slang? The French language has many rules for creating words in the speech and writing of popular culture. Two such rules are: (a) The reverse spelling of words such as \"femme\" to \"meuf\". (This is called verlan.) (b) The attachment of the suffix \"-ard\" to a noun or verb to form a proper noun. For example, the noun \"faluche\" means \"student hat\". The word \"faluchard\" formed from \"faluche\" colloquially can mean, depending on context, \"a group of students\", \"a gathering of students\" and \"behavior typical of a student\". The Google translator as of 28 December 2006 doesn't derive the constructed words as for example from rule (b), as shown here:\nIl y a une chorale falucharde mercredi, venez nombreux, les faluchards chantent des paillardes! ==> \"There is a choral society falucharde Wednesday, come many, the faluchards sing loose-living women!\"\nFrench argot has three levels of usage:\n\nThe United States National Institute of Standards and Technology conducts annual evaluations of machine translation systems based on the BLEU-4 criterion . A combined method called IQmt which incorporates BLEU and additional metrics NIST, GTM, ROUGE and METEOR has been implemeneted by Gimenez and Amigo .\n\nIs the output grammatical or well-formed in the target language? Using an interlingua should be helpful in this regard, because with a fixed interlingua one should be able to write a grammatical mapping to the target language from the interlingua. Consider the following Arabic language input and English language translation result from the Google translator as of 27 December 2006 . This Google translator output doesn't parse using a reasonable English grammar: \n\nDo repeated re-translations preserve the semantics of the original sentence? For example, consider the following English input passed multiple times into and out of French using the Google translator as of 27 December 2006: \n\nAs noted above and in, this kind of round-trip translation is a very unreliable method of evaluation.\n\nAn interesting peculiarity of Google Translate as of 24 January 2008 (corrected as of 25 January 2008) is the following result when translating from English to Spanish, which shows an embedded joke in the English-Spanish dictionary which has some added poignancy given recent events:\nThis raises the issue of trustworthiness when relying on a machine translation system embedded in a Life-critical system in which the translation system has input to a Safety Critical Decision Making process. Conjointly it raises the issue of whether in a given use the software of the machine translation system is safe from hackers.\n\nIt is not known whether this feature of Google Translate was the result of a joke/hack or perhaps an unintended consequence of the use of a method such as statistical machine translation. Reporters from CNET Networks asked Google for an explanation on January 24, 2008; Google said only that it was an \"internal issue with Google Translate\". The mistranslation was the subject of much hilarity and speculation on the Internet.\n\nIf it is an unintended consequence of the use of a method such as statistical machine translation, and not a joke/hack, then this event is a demonstration of a potential source of critical unreliability in the statistical machine translation method.\n\nIn human translations, in particular on the part of interpreters, selectivity on the part of the translator in performing a translation is often commented on when one of the two parties being served by the interpreter knows both languages.\n\nThis leads to the issue of whether a particular translation could be considered \"verifiable\". In this case, a converging round-trip translation would be a kind of verification.\n\n\n",
    "id": "8642531",
    "title": "Machine translation software usability"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=5559076",
    "text": "Monitoring and surveillance agents\n\nMonitoring and surveillance agents (also known as predictive agents) are a type of intelligent agent software that observes and reports on computer equipment. Monitoring and surveillance agents are often used to monitor complex computer networks to predict when a crash or some other defect may occur. Another type of monitoring and surveillance agent works on computer networks keeping track of the configuration of each computer connected to the network. It tracks and updates the central configuration database when anything on any computer changes, such as the number or type of disk drives. An important task in managing networks lies in prioritizing traffic and shaping bandwidth.\n\n\nHaag & Cummings & McCubbrey & Pinsonneault & Donovan (2006). \"Management Information Systems Third Canadian Ed.\" McGraw-Hill Ryerson\n\n",
    "id": "5559076",
    "title": "Monitoring and surveillance agents"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=5410176",
    "text": "Network compartment\n\nNetwork Compartmentalization, the division of network functionality into network compartments, is an important concept of Autonomic Networking.\n\nNetwork Compartments implement the operational rules and administrative policies for a given communication context. The boundaries of a communication context, and hence the compartment boundaries, are based on technological and/or administrative boundaries. For example, compartment boundaries can be defined by a certain type of network technology (e.g., a specific wireless access network) or based on a particular communication protocol and/or addressing space (e.g., an IPv4 or and IPv6 network), but also based on a policy domain (e.g., a national health network that requires a highly secure boundary). \n\nA compartment's communication principles, protocols and policies form a sort of “recipe” that all compartment entities must obey. For example, the recipe defines how to join a compartment, who can join, and how the naming, addressing and routing is handled. The complexity and details of the internal operation is left to each compartment. For example, registration with a compartment can range from complex trust-based mechanisms to simple registration schemes with a central database or a public DHT-based system; resolution of a communication peer can be handled implicitly by the compartment’s naming and addressing scheme or require explicit actions (e.g., resolution of an identifier to a locator). It is important to note here that compartments have full autonomy on how to handle the compartment’s internal communication – i.e. there are no global invariants that have to be implemented by all compartments or all communication elements.\n\nMembers of a compartment are able and willing to communicate among each other according to compartment’s operational and policy rules. Conceptually a compartment maintains some form of implicit database which contains its members; that is, each entry in the database defines a member. Before one can send a data packet to a compartment member, a resolution step is required which returns a means to “address” the member. Note that the above definition does not specify whether a member is a node, a set of servers or a software module. This rather abstract definition of compartment membership permits to capture many different flavours of members and communication forms. \n\nIt is anticipated that many compartments co-exist and that compartments are able to interwork on various levels (e.g. through \"layering\" or \"peering\" of compartments).\n\n",
    "id": "5410176",
    "title": "Network compartment"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=13234913",
    "text": "Artificial Intelligence Applications Institute\n\nThe Artificial Intelligence Applications Institute (AIAI) at the School of Informatics at the University of Edinburgh is a non-profit technology transfer organisation that promotes the benefits of the application of Artificial Intelligence research to commercial, industrial, and government organisations worldwide.\n\nAIAI was created in July 1983, and received it formal charter from the University of Edinburgh in July 1984. It joined the School of Informatics when this was created from a number of departments and research institutes in 1998. The Director of AIAI is Austin Tate.\n\nAIAI specialises in Intelligent and Knowledge-based systems, including:\n\n",
    "id": "13234913",
    "title": "Artificial Intelligence Applications Institute"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=6026708",
    "text": "Noisy text analytics\n\nNoisy text analytics is a process of information extraction whose goal is to automatically extract structured or semistructured information from noisy unstructured text data. While Text analytics is a growing and mature field that has great value because of the huge amounts of data being produced, processing of noisy text is gaining in importance because a lot of common applications produce noisy text data. Noisy unstructured text data is found in informal settings such as online chat, text messages, e-mails, message boards, newsgroups, blogs, wikis and web pages. Also, text produced by processing spontaneous speech using automatic speech recognition and printed or handwritten text using optical character recognition contains processing noise. Text produced under such circumstances is typically highly noisy containing spelling errors, abbreviations, non-standard words, false starts, repetitions, missing punctuations, missing letter case information, pause filling words such as “um” and “uh” and other texting and speech disfluencies. Such text can be seen in large amounts in contact centers, chat rooms, optical character recognition (OCR) of text documents, short message service (SMS) text, etc. Documents with historical language can also be considered noisy with respect to today’s knowledge about the language. Such text contains important historical, religious, ancient medical knowledge that is useful. The nature of the noisy text produced in all these contexts warrants moving beyond traditional text analysis techniques.\n\nMissing punctuation and the use of non-standard words can often hinder standard natural language processing tools such as part-of-speech tagging\nand parsing. Techniques to both learn from the noisy data and then to be able to process the noisy data are only now being developed.\n\n\n",
    "id": "6026708",
    "title": "Noisy text analytics"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=6773335",
    "text": "Verbot\n\nThe Verbot (Verbal-Robot) was a popular chatterbot program and Artificial Intelligence Software Development Kit (SDK) for the Windows platform and for the web.\n\nVirtual Personalities, Inc. traces its technology back to Michael Mauldin's work as a graduate student and post-doctoral fellow at Carnegie Mellon University; and its artistry back to Peter Plantec's work in personality psychology and art direction.\n\nIn 1994, Mauldin, Founder of Lycos, Inc., developed a prototype Chatterbot, Julia, which competed in the internationally known Turing test, for the coveted Loebner Prize. The Turing Test matches computer scientist judges against machines to see if they can distinguish a computer from a real human. This prototype version was refined and developed, and in 1997, Dr. Mauldin and Peter Plantec, a clinical psychologist, and animator, formed Virtual Personalities, Inc. (now Conversive, Inc.) in order to create a virtual human interface that would incorporate real-time animation as well as speech and natural language processing. The initial release, a stand-alone virtual person called Sylvie, was beta-tested to the public. This release was well received, and finally, after several versions, the production release (deemed version 3) of the Verbally Enhanced Software Robot—or, Verbot was deployed in the Fall 2000.\n\n\nThe Virtual Personalities story goes back to 1978, where Mauldin was attending Rice University. Fascinated by the idea of ELIZA, he proceeded to write a program called \"PET\" for his 8 kilobyte Commodore PET Computer. PET included simple induction as a way to post new information, and once managed the following deep observation: Meanwhile, Plantec was separately designing a personality for \"Entity\", a theoretical virtual human that would interact comfortably with humans without pretending to be one. At that time the technology was not advanced enough to bring Entity to life, however, Mauldin was working on that.\n\nMauldin got so involved with this, that he majored in Computer Science and minored in Linguistics.\n\nIn the late seventies and early eighties, a popular computer game at Universities was Rogue, an implementation of Dungeons and Dragons where the player would descend 26 levels in a randomly created dungeon, fighting monsters, gathering treasure, and searching for the elusive \"Amulet of Yendor\". Mauldin was one of four grad students who devoted a large amount of time to building a program called \"Rog-O-Matic\" that could and on several occasions did manage to retrieve the amulet and emerge victorious from the dungeon.\n\nSo when in 1989, James Aspnes at Carnegie Mellon created the first TinyMUD (a descendent of MUD and AberMUD), Mauldin was one of the first to create a computer player that would explore the text-based world of TinyMUD. But his first robot, Gloria, gradually accreted more and more linguistic ability, to the point that it could pass the \"unsuspecting\" Turing Test. In this version of the test, the human has no reason to suspect that one of the other occupants of the room is controlled by a computer, and so is more polite and asks fewer probing questions. The second generation of Mauldin's TinyMUD robots was Julia, created on Jan. 8, 1990. Julia slowly developed into a more and more capable conversational agent, and assumed useful duties in the TinyMUD world, including tour guide, information assistant, note-taker, message-relayer, and even could play the card game hearts along with the other human players.\n\nIn 1991, the first Loebner Prize contest was held in Boston, Mass., and Julia was there. Although she only finished third, she was ranked by one judge as more human than one of the human confederates, winning a coveted certificate of humanness in the world's first restricted Turing test.\n\nJulia continued to log in to various TinyMUD's and TinyMucks for the next seven years, and also chats with hundreds of people a month over the internet.\n\nJulia's job was to explore a virtual world consisting of pages of textual descriptions, with links between them, and to construct an internal map of that world and answer questions about it (including path information such as the shortest route from one room to another, and matching information, such as which rooms contained a certain kind of object or textual description).\n\nIt was therefore only a very short cognitive leap from Julia to Lycos, another robotic agent that explores a virtual world made of hyperlinked pages of text, and which answers questions about those pages. Sylvie was born and her abilities were expanded greatly to include interfacing with computers and control systems via her serial ports.\n\nSylvie was the first intelligent animated virtual human. She was designed both as a conversation agent and as a virtual human interface that would form a bridge between the two. She became more popular as a conversation agent, but her designers believe she serves as a prototype for future virtual human interface design that will help us all cope with the increasing complexity of technology.\n\nAs and aside, Plantec noticed that an inordinately large number of Sylvies were being sold in Southeast Asia. Upon investigation he discovered that students had discovered a \"test\" mode that would allow them to type in English sentences that Sylvie would pronounce in her somewhat stylized English. Sylvie was teaching them English ... her style of English.\n\nIn 1997, Dr. Mauldin and Peter Plantec, formed Virtual Personalities, Inc. to create Natural Language Processing solutions for companies. In 2001 Virtual Personalities, Inc. became Conversive, Inc. to reflect the focus on providing Customer Service and Marketing to the Enterprise Market. In late 2012 Avaya, Inc. acquired Conversive's assets including Verbots.\n\nThe Verbot 4 version was created and released in 2004. In 2005 Version 4.1 of the Verbot Software was released with many feature enhancements and bug fixes, including built-in support for embedding C# code in outputs and conditionals. In Early 2006 Conversive launched Verbots Online allowing Verbot 4 users to upload their knowledge and show off their bots to the world. In 2009 Version 5 was released, completely free and fully featured. In early 2012 the last version of Verbot, 5.0.1.2, was released to the general public with support for Windows 7. Also in 2012 Verbots Online completely shutdown.\n\nVerbots.com, its community of users and its forums no longer exist but the software and users can still be found. There has been no active development since the early 2012 release of Verbot 5.0.1.2.\n\n\n",
    "id": "6773335",
    "title": "Verbot"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=16021556",
    "text": "DialogOS\n\nDialogOS is a graphical programming environment to design computer system which can converse through voice with the user. Dialogs are clicked together in a Flowchart. DialogOS includes bindings to control Lego Mindstorms robots with the voice.\n\nDialogOS is used in computer science courses in schools and universities to teach programming and to introduce beginners in the basic principles of human/computer interaction and dialog design.\n\nDialogOS can control the LEGO Mindstorms NXT Series. It uses sensor-nodes to obtain values for the following sensors:\n",
    "id": "16021556",
    "title": "DialogOS"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=14477647",
    "text": "Quack.com\n\nQuack.com was an early voice portal company. The domain name later was used for Quack, an iPad search application from AOL.\n\nIt was founded in 1998 by Steven Woods, Jeromy Carriere and Alex Quilici as a Pittsburgh, Pennsylvania, USA, based voice portal infrastructure company named Quackware. Quack was the first company to try to create a voice portal: a consumer-based destination \"site\" in which consumers could not only access information by voice alone, but also complete transactions. Quackware launched a beta phone service in 1999 that allowed consumers to purchase books from sites such as Amazon and CDs from sites such as CDNow by answering a short set of questions. Quack followed with a set of information services from movie listings (inspired by, but expanding upon, Moviefone) to news, weather and stock quotes. This concept introduced a series of lookalike startups including Tellme Networks which raised more money than any Internet startup in history on a similar concept.\n\nQuack received venture funding in 1999 and moved operations to Mountain View in Silicon Valley, California in 1999. \nA deal with Lycos was announced in May 2000.\nIn September 2000 Quack was acquired for $200 million by America Online (AOL) and moved onto the Netscape campus with what was left of the Netscape team.\n\nQuack was attacked in the Canadian press for being representative of the Canadian \"brain drain\" to the US during the Internet bubble, focusing its recruiting efforts on the University of Waterloo, hiring more than 50 engineers from Waterloo in less than 10 months. Quack competitor Tellme Networks raised enormous funds in what became a highly competitive market in 2000, with the emergence of more than a dozen additional competitors in a 12-month period.\n\nFollowing its acquisition by America Online in an effort led by Ted Leonsis to bring Quack into AOL Interactive, the Quack voice service became AOLbyPhone as one of AOL's \"web properties\" along with MapQuest, Moviefone and others.\n\nQuack secured several patents that underlie the technical challenges of delivering interactive voice services. Constructing a voice portal required integrations and innovations not only in speech recognition and speech generation, but also in databases, application specification, constraint-based reasoning and artificial intelligence and computational linguistics. \"Quack\"'s name derived from the company goal of providing not only voice-based services, but more broadly \"Quick Ubiquitous Access to Consumer Knowledge\".\n\nThe patents assigned to Quack.com include: System and method for voice access to Internet-based information, System and method for advertising with an Internet Voice Portal and recognizing the axiom that in interactive voice systems one must \"know the set of possible answers to a question before asking it\". System and method for determining if one web site has the same information as another web site.\n\nQuack.com was spoofed in \"The Simpsons\" in March 2002 in the episode \"Blame It on Lisa\" in which a \"ComQuaak\" sign is replaced by another equally crazy telecom company name.\n\nIn July 2010, quack.com became the focus of a new AOL iPad application, that was a web search experience. The product delivers web results and blends in picture, video and Twitter results. It enables you to preview the web results before you go to the site, search within each result, and flip through the results pages, making full use of the iPad's touch screen features. The iPad app was free via iTunes, but support discontinued in 2012.\n\n",
    "id": "14477647",
    "title": "Quack.com"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=18629502",
    "text": "Context-sensitive user interface\n\nA context-sensitive user interface is one which can automatically choose from a multiplicity of options based on the current or previous state(s) of the program operation. \"Context sensitivity\" is almost ubiquitous in current graphical user interfaces, usually in the form of context menus. Context sensitivity, when operating correctly, should be practically transparent to the user. This can be experienced in computer operating systems which call a compatible program to run files based upon their filename extension, e.g. opening text files with a word processor, video files (.mpg, .mov and .avi, etc.) with a video player, image files (.jpg and .png etc.) with a photo viewer or running program files themselves, and their shortcuts, (i.e. .exe files) when selected.\n\nThe user-interface may also provide \"Context sensitive\" feedback, such as changing the appearance of the mouse pointer or cursor, changing the menu color, or with applicable auditory or tactile feedback.\n\nThe primary reason for introducing context sensitivity is to simplify the user interface.\n\nAdvantages include:\n\n\nContext sensitive actions may be perceived as dumbing down of the user interface - leaving the operator at a loss as to what to do when the computer decides to perform an unwanted action. Additionally non-automatic procedures may be hidden or obscured by the context sensitive interface causing an increase in user workload for operations the designers did not foresee.\n\nA poor implementation can be more annoying than helpful – a classic example of this is Office Assistant.\n\nAt the simplest level each possible action is reduced to a single most likely action – the action performed is based on a single variable (such as file extension). In more complicated \nimplementations multiple factors can be assessed such as the user's previous actions, the size of the file, the programs in current use, metadata etc.\n\nThe method is not only limited to the response to imperative button presses and mouse clicks - pop-up menus can be pruned and/or altered, or a web search can focus results based on previous searches.\n\nAt higher levels of implementation \"context sensitive\" actions require either larger amounts of meta-data, extensive case analysis based programming, or other artificial intelligence algorithms.\n\nContext sensitivity is important in video games – especially those controlled by a gamepad, joystick or computer mouse in which the number of buttons available is limited. It is primarily applied when the player is in a certain place and is used to interact with a person or object. For example, if the player is standing next to a non-player character, an option may come up allowing the player to talk with him/her.\n\nImplementations range from the embryonic 'Quick Time Event' to context sensitive sword combat in which the attack used depends on the position and orientation of both the player and opponent, as well as the virtual surroundings. A similar range of use is found in the 'action button' which, depending upon the in-game position of the player's character, may cause it to pick something up, open a door, grab a rope, punch a monster or opponent, or smash an object.\n\nThe response does not have to be player activated - an on-screen device may only be shown in certain circumstances, e.g. 'targeting' cross hairs in a flight combat game may indicate the player should fire. An alternative implementation is to monitor the input from the player (e.g. level of button pressing activity) and use that to control the pace of the game in an attempt to maximize enjoyment or to control the excitement or ambience.\n\nThe method has become increasingly important as more complex games are designed for machines with few buttons (keyboard-less consoles). Bennet Ring commented (in 2006) that \"\"Context-sensitive\" is the new lens flare\".\n\nContext sensitive help is a common implementation of context sensitivity, a single help button is actioned and the help page or menu will open a specific page or related topic.\n\n\n",
    "id": "18629502",
    "title": "Context-sensitive user interface"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=2961998",
    "text": "ESTAR project\n\nThe eSTAR project is a multi-agent system that aims to implement a true heterogeneous network of robotic telescopes for automated observing. The project is a joint collaboration between the Astrophysics Group of the University of Exeter and the Astrophysics Research Institute at Liverpool John Moores University.\n\nIn 2006 work began on an autonomous software agent for observations of variable stars. This agent implements the optimal sampling technique of Saunders et al. (2006) and the prototype was successfully tested on the RoboNet network of telescopes which includes: the Liverpool Telescope, the Faulkes Telescope North and the Faulkes Telescope South.\n\neSTAR is affiliated with the RoboNet Consortium and the global Heterogeneous Telescope Networks Consortium.\n\nAs of 2007 eSTAR is \"live\" supporting two real-time observing projects. Automated follow-up observations of gamma ray bursts are performed using the 3.8m UKIRT telescope situated in Hawai'i, making this telescope the largest in the world, with an automated response system for tracking such events.\n\neSTAR is also involved in the search for extra-solar planets by placing observations on the RoboNet system of telescopes on behalf of the PLANET collaboration. The technique of gravitational microlensing is used to monitor large numbers of stars in the galactic bulge looking for the tell-tale signature of cool planets orbiting those stars.\n\n",
    "id": "2961998",
    "title": "ESTAR project"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=18849138",
    "text": "YouNoodle\n\nYouNoodle is a San Francisco-based company, with offices in Barcelona and Santiago, founded in 2010, building a platform for entrepreneurship competitions all over the world. YouNoodle matches entrepreneurs with competitions, accelerators, and startup programs, and provides a judging and voting SaaSplatform to university, non-profit, government and enterprise clients organizing innovation challenges and competitions. Stanford's BASES, UC Berkeley LAUNCH, Start-Up Chile, Amazon Startup Challenge, and NASA are all running one or more competitions on YouNoodle's platform. \n\nYouNoodle was founded by Rebeca Hwang and Torsten Kolind in 2010. The company was spun off a project started by Bob Goodson and Kirill Makharinsky in 2007 with support from Peter Thiel (Founders Fund), Max Levchin (PayPal) and Charles Lho (Amicus Group), founding investor and Chairman of YouNoodle today. This project also spawned Quid (Goodson) and indirectly Ostrovok (Makharinsky). Although also named YouNoodle, this project/company was discontinued in 2010, when the three new entities started operations.\n\nThe founders of the 2007-2010 entity were Goodson and Makharinsky, both former students of the University of Oxford. Goodson had studied medieval English literature before moving from Oxford to California when Levchin, the co-founder of Paypal, invited him to join a start-up there. Makharinsky's degree was in applied mathematics, and he was also encouraged to pursue opportunities in the United States by Levchin. Other significant employees included Hwang (co-founder of today's YouNoodle), a Stanford University doctoral student whose research is into social network theory.\n\nYouNoodle's now discontinued \"Startup predictor\", part of the 2007-2010 entity and developed by Makharinsky and Hwang, uses mathematical models to predict the success of new businesses. The user fills in a questionnaire, which takes about half an hour to complete and concentrates on the business' concept, finances, founders and advisers. Because the procedure is designed for very new companies, questions on revenue and traffic are not included. The site then provides an estimate of what the company's value will be after three years and a score from 1 to 1000 representing its value as an investment. The service is free for the startups themselves, but YouNoodle intends to charge third parties for access to the results. (The level of detail required by the questionnaire makes it difficult for people without inside knowledge of a company to provide the data for a prediction on their own.)\n\nThe company's founders have declined to explain the algorithm in detail, but state that it takes into account the entrepreneurs' experience, networks and mutual relations. Information provided by companies which use the site's networking features is used to improve the algorithm. As of August 2008, the algorithm was based on data from 3,000 startups. In the same month the company had four patents pending on the technology.\n\n",
    "id": "18849138",
    "title": "YouNoodle"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=19852067",
    "text": "Pop music automation\n\nPop music automation is a field of study among musicians and computer scientists with a goal of producing successful pop music algorithmically. It is often based on the premise that pop music is especially formulaic, unchanging, and easy to compose. The idea of automating pop music composition is related to many ideas in algorithmic music, Artificial Intelligence (AI) and computational creativity.\n\nAlgorithms (or, at the very least, formal sets of rules) have been used to compose music for centuries; the procedures used to plot voice-leading in counterpoint, for example, can often be reduced to algorithmic determinacy. Now the term is usually reserved, however, for the use of formal procedures to make music without human intervention.\n\nClassical music automation software exists that generates music in the style of Mozart and Bach and jazz. Most notably, David Cope has written a software system called \"Experiments in Musical Intelligence\" (or \"EMI\") that is capable of analyzing and generalizing from existing music by a human composer to generate novel musical compositions in the same style. EMI's output is convincing enough to persuade human listeners that its music is human-generated to a high level of competence.\n\nCreativity research in jazz has focused on the process of improvisation and the cognitive demands that this places on a musical agent: reasoning about time, remembering and conceptualizing what has already been played, and planning ahead for what might be played next.\n\nInevitably associated with Pop music automation is Pop music analysis.\n\nProjects in Pop music automation may include, but are not limited to, ideas in melody creation and song development, vocal generation or improvement, automatic accompaniment and lyric composition.\n\nSome systems exist that automatically choose chords to accompany a vocal melody in real-time. A user with no musical experience can create a song with instrumental accompaniment just by singing into a microphone.\nAn example is a Microsoft Research project called Songsmith, which trains a Hidden Markov model using a music database and uses that\nmodel to select chords for new melodies.\n\nAutomatic melody generation is often done with a Markov chain, the states of the system become note or pitch values, and a probability vector for each note is constructed, completing a transition probability matrix (see below). An algorithm is constructed to produce and output note values based on the transition matrix weightings, which could be MIDI note values, frequency (Hz), or any other desirable metric.\n\nA second-order Markov chain can be introduced by considering the current state \"and\" also the previous state, as indicated in the second table. Higher, \"n\"th-order chains tend to \"group\" particular notes together, while 'breaking off' into other patterns and sequences occasionally. These higher-order chains tend to generate results with a sense of phrasal structure, rather than the 'aimless wandering' produced by a first-order system.\n\nAutomated lyric creating software may take forms such as:\n\nThe Tra-la-Lyrics system produces song lyrics, in Portuguese, for a given melody. This not only involves matching each word syllable with a note in the melody, but also matching the word's stress with the strong beats of the melody.\n\nThis involves natural language processing.\nPablo Gervás has developed a noteworthy system called ASPERA that employs a case-based reasoning (CBR) approach to generating poetic formulations of a given input text via a composition of poetic fragments that are retrieved from a case-base of existing poems. Each poem fragment in the ASPERA case-base is annotated with a prose string that expresses the meaning of the fragment, and this prose string is used as the retrieval key for each fragment. Metrical rules are then used to combine these fragments into a well-formed poetic structure.\n\nPrograms like TALE-SPIN\nOn-line metaphor generation systems like 'Sardonicus' or 'Aristotle' can suggest lexical metaphors for a given descriptive goal (e.g., to describe a supermodel as skinny, the source terms “pencil”, “whip”, “whippet”, “rope”, “stick-insect” and “snake” are suggested). \n\nUsing a language database (such as wordnet) one can create musings on a subject that may be weak grammatically but are still sensical. See such projects as the Flowerewolf automatic poetry generator or the \nDada engine.\n\n\n\n",
    "id": "19852067",
    "title": "Pop music automation"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=8168237",
    "text": "Roblog\n\nRoblog is a neologism for a blog written by a robot with no human intervention.\n\nRoblogs were made possible with a new generation of robots which are capable of uploading images and texts automatically to the Web. The first roblogs to appear, late 2005, were written by AIBO robots, the dog-like robotic pets once manufactured by Sony.\n\nAIBO diaries are roblogs produced by AIBO model ERS-7, running a bundled software called \"Mind\" in either version 2 or 3. Depending on the language of the Mind software, the AIBO blogs in either English or Japanese. To be able to blog on its own, an ERS-7M2 or ERS-7M3 must be linked to the Internet through its Wi-Fi connection capability, and its e-mail sending capability must be correctly configured, for which an SMTP server not requiring authentication nor alternate ports is needed. Posts, consisting of pictures taken with the AIBO's color camera built into its nose, are then sent by e-mail to the blog.\n\n",
    "id": "8168237",
    "title": "Roblog"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=19980",
    "text": "Machine translation\n\nMachine translation, sometimes referred to by the abbreviation MT (not to be confused with computer-aided translation, machine-aided human translation (MAHT) or interactive translation) is a sub-field of computational linguistics that investigates the use of software to translate text or speech from one language to another.\n\nOn a basic level, MT performs simple substitution of words in one language for words in another, but that alone usually cannot produce a good translation of a text because recognition of whole phrases and their closest counterparts in the target language is needed. Solving this problem with corpus statistical, and neural techniques is a rapidly growing field that is leading to better translations, handling differences in linguistic typology, translation of idioms, and the isolation of anomalies.\n\nCurrent machine translation software often allows for customization by domain or profession (such as weather reports), improving output by limiting the scope of allowable substitutions. This technique is particularly effective in domains where formal or formulaic language is used. It follows that machine translation of government and legal documents more readily produces usable output than conversation or less standardised text.\n\nImproved output quality can also be achieved by human intervention: for example, some systems are able to translate more accurately if the user has unambiguously identified which words in the text are proper names. With the assistance of these techniques, MT has proven useful as a tool to assist human translators and, in a very limited number of cases, can even produce output that can be used as is (e.g., weather reports).\n\nThe progress and potential of machine translation have been debated much through its history. Since the 1950s, a number of scholars have questioned the possibility of achieving fully automatic machine translation of high quality. Some critics claim that there are in-principle obstacles to automating the translation process.\n\nThe idea of machine translation may be traced back to the 17th century. In 1629, René Descartes proposed a universal language, with equivalent ideas in different tongues sharing one symbol. The field of \"machine translation\" appeared in Warren Weaver's Memorandum on Translation (1949). The first researcher in the field, Yehosha Bar-Hillel, began his research at MIT (1951). A Georgetown University MT research team followed (1951) with a public demonstration of its Georgetown-IBM experiment system in 1954. MT research programs popped up in Japan and Russia (1955), and the first MT conference was held in London (1956). Researchers continued to join the field as the Association for Machine Translation and Computational Linguistics was formed in the U.S. (1962) and the National Academy of Sciences formed the Automatic Language Processing Advisory Committee (ALPAC) to study MT (1964). Real progress was much slower, however, and after the ALPAC report (1966), which found that the ten-year-long research had failed to fulfill expectations, funding was greatly reduced. According to a 1972 report by the Director of Defense Research and Engineering (DDR&E), the feasibility of large-scale MT was reestablished by the success of the Logos MT system in translating military manuals into Vietnamese during that conflict.\n\nThe French Textile Institute also used MT to translate abstracts from and into French, English, German and Spanish (1970); Brigham Young University started a project to translate Mormon texts by automated translation (1971); and Xerox used SYSTRAN to translate technical manuals (1978). Beginning in the late 1980s, as computational power increased and became less expensive, more interest was shown in statistical models for machine translation. Various MT companies were launched, including Trados (1984), which was the first to develop and market translation memory technology (1989). The first commercial MT system for Russian / English / German-Ukrainian was developed at Kharkov State University (1991).\n\nMT on the web started with SYSTRAN Offering free translation of small texts (1996), followed by AltaVista Babelfish, which racked up 500,000 requests a day (1997). Franz-Josef Och (the future head of Translation Development AT Google) won DARPA's speed MT competition (2003). More innovations during this time included MOSES, the open-source statistical MT engine (2007), a text/SMS translation service for mobiles in Japan (2008), and a mobile phone with built-in speech-to-speech translation functionality for English, Japanese and Chinese (2009). Recently, Google announced that Google Translate translates roughly enough text to fill 1 million books in one day (2012).\n\nThe idea of using digital computers for translation of natural languages was proposed as early as 1946 by A. D. Booth and possibly others. Warren Weaver wrote an important memorandum \"Translation\" in 1949. The Georgetown experiment was by no means the first such application, and a demonstration was made in 1954 on the APEXC machine at Birkbeck College (University of London) of a rudimentary translation of English into French. Several papers on the topic were published at the time, and even articles in popular journals (see for example \"Wireless World\", Sept. 1955, Cleave and Zacharov). A similar application, also pioneered at Birkbeck College at the time, was reading and composing Braille texts by computer.\n\nThe human translation process may be described as:\n\nBehind this ostensibly simple procedure lies a complex cognitive operation. To decode the meaning of the source text in its entirety, the translator must interpret and analyse all the features of the text, a process that requires in-depth knowledge of the grammar, semantics, syntax, idioms, etc., of the source language, as well as the culture of its speakers. The translator needs the same in-depth knowledge to re-encode the meaning in the target language.\n\nTherein lies the challenge in machine translation: how to program a computer that will \"understand\" a text as a person does, and that will \"create\" a new text in the target language that \"sounds\" as if it has been written by a person.\n\nIn its most general application, this is beyond current technology. Though it works much faster, no automated translation program or procedure, with no human participation, can produce output even close to the quality a human translator can produce. What it can do, however, is provide a general, though imperfect, approximation of the original text, getting the \"gist\" of it (a process called \"gisting\"). This is sufficient for many purposes, including making best use of the finite and expensive time of a human translator, reserved for those cases in which total accuracy is indispensable.\n\nThis problem may be approached in a number of ways, through the evolution of which accuracy has improved.\n\nMachine translation can use a method based on linguistic rules, which means that words will be translated in a linguistic way – the most suitable (orally speaking) words of the target language will replace the ones in the source language.\n\nIt is often argued that the success of machine translation requires the problem of natural language understanding to be solved first.\n\nGenerally, rule-based methods parse a text, usually creating an intermediary, symbolic representation, from which the text in the target language is generated. According to the nature of the intermediary representation, an approach is described as interlingual machine translation or transfer-based machine translation. These methods require extensive lexicons with morphological, syntactic, and semantic information, and large sets of rules.\n\nGiven enough data, machine translation programs often work well enough for a native speaker of one language to get the approximate meaning of what is written by the other native speaker. The difficulty is getting enough data of the right kind to support the particular method. For example, the large multilingual corpus of data needed for statistical methods to work is not necessary for the grammar-based methods. But then, the grammar methods need a skilled linguist to carefully design the grammar that they use.\n\nTo translate between closely related languages, the technique referred to as rule-based machine translation may be used.\n\nThe rule-based machine translation paradigm includes transfer-based machine translation, interlingual machine translation and dictionary-based machine translation paradigms. This type of translation is used mostly in the creation of dictionaries and grammar programs. Unlike other methods, RBMT involves more information about the linguistics of the source and target languages, using the morphological and syntactic rules and semantic analysis of both languages. The basic approach involves linking the structure of the input sentence with the structure of the output sentence using a parser and an analyzer for the source language, a generator for the target language, and a transfer lexicon for the actual translation. RBMT's biggest downfall is that everything must be made explicit: orthographical variation and erroneous input must be made part of the source language analyser in order to cope with it, and lexical selection rules must be written for all instances of ambiguity. Adapting to new domains in itself is not that hard, as the core grammar is the same across domains, and the domain-specific adjustment is limited to lexical selection adjustment.\n\nTransfer-based machine translation is similar to interlingual machine translation in that it creates a translation from an intermediate representation that simulates the meaning of the original sentence. Unlike interlingual MT, it depends partially on the language pair involved in the translation.\n\nInterlingual machine translation is one instance of rule-based machine-translation approaches. In this approach, the source language, i.e. the text to be translated, is transformed into an interlingual language, i.e. a \"language neutral\" representation that is independent of any language. The target language is then generated out of the interlingua. One of the major advantages of this system is that the interlingua becomes more valuable as the number of target languages it can be turned into increases. However, the only interlingual machine translation system that has been made operational at the commercial level is the KANT system (Nyberg and Mitamura, 1992), which is designed to translate Caterpillar Technical English (CTE) into other languages.\n\nMachine translation can use a method based on dictionary entries, which means that the words will be translated as they are by a dictionary.\n\nStatistical machine translation tries to generate translations using statistical methods based on bilingual text corpora, such as the Canadian Hansard corpus, the English-French record of the Canadian parliament and EUROPARL, the record of the European Parliament. Where such corpora are available, good results can be achieved translating similar texts, but such corpora are still rare for many language pairs. The first statistical machine translation software was CANDIDE from IBM. Google used SYSTRAN for several years, but switched to a statistical translation method in October 2007. In 2005, Google improved its internal translation capabilities by using approximately 200 billion words from United Nations materials to train their system; translation accuracy improved. Google Translate and similar statistical translation programs work by detecting patterns in hundreds of millions of documents that have previously been translated by humans and making intelligent guesses based on the findings. Generally, the more human-translated documents available in a given language, the more likely it is that the translation will be of good quality. Newer approaches into Statistical Machine translation such as METIS II and PRESEMT use minimal corpus size and instead focus on derivation of syntactic structure through pattern recognition. With further development, this may allow statistical machine translation to operate off of a monolingual text corpus. SMT's biggest downfall includes it being dependent upon huge amounts of parallel texts, its problems with morphology-rich languages (especially with translating \"into\" such languages), and its inability to correct singleton errors.\n\nExample-based machine translation (EBMT) approach was proposed by Makoto Nagao in 1984. Example-based machine translation is based on the idea of analogy. In this approach, the corpus that is used is one that contains texts that have already been translated. Given a sentence that is to be translated, sentences from this corpus are selected that contain similar sub-sentential components. The similar sentences are then used to translate the sub-sentential components of the original sentence into the target language, and these phrases are put together to form a complete translation.\n\nHybrid machine translation (HMT) leverages the strengths of statistical and rule-based translation methodologies. Several MT organizations (such as Omniscien Technologies (formerly Asia Online), LinguaSys, Systran, and Polytechnic University of Valencia) claim a hybrid approach that uses both rules and statistics. The approaches differ in a number of ways:\nMore recently, with the advent of Neural MT, a new version of hybrid machine translation is emerging that combines the benefits of rules, statistical and neural machine translation. The approach allows benefitting from pre- and post-processing in a rule guided workflow as well as benefitting from NMT and SMT. The downside is the inherent complexity which makes the approach suitable only for specific use cases. One of the proponents of this approach for complex use cases is Omniscien Technologies.\n\nA deep learning based approach to MT, neural machine translation has made rapid progress in recent years, and Google has announced its translation services are now using this technology in preference to its previous statistical methods. Other providers including KantanMT, Omniscien Technologies and SDL have announced the deployment of neural machine translation technology in 2017 as well.\n\nWord-sense disambiguation concerns finding a suitable translation when a word can have more than one meaning. The problem was first raised in the 1950s by Yehoshua Bar-Hillel. He pointed out that without a \"universal encyclopedia\", a machine would never be able to distinguish between the two meanings of a word. Today there are numerous approaches designed to overcome this problem. They can be approximately divided into \"shallow\" approaches and \"deep\" approaches.\n\nShallow approaches assume no knowledge of the text. They simply apply statistical methods to the words surrounding the ambiguous word. Deep approaches presume a comprehensive knowledge of the word. So far, shallow approaches have been more successful.\n\nClaude Piron, a long-time translator for the United Nations and the World Health Organization, wrote that machine translation, at its best, automates the easier part of a translator's job; the harder and more time-consuming part usually involves doing extensive research to resolve ambiguities in the source text, which the grammatical and lexical exigencies of the target language require to be resolved:\n\nThe ideal deep approach would require the translation software to do all the research necessary for this kind of disambiguation on its own; but this would require a higher degree of AI than has yet been attained. A shallow approach which simply guessed at the sense of the ambiguous English phrase that Piron mentions (based, perhaps, on which kind of prisoner-of-war camp is more often mentioned in a given corpus) would have a reasonable chance of guessing wrong fairly often. A shallow approach that involves \"ask the user about each ambiguity\" would, by Piron's estimate, only automate about 25% of a professional translator's job, leaving the harder 75% still to be done by a human.\n\nOne of the major pitfalls of MT is its inability to translate non-standard language with the same accuracy as standard language. Heuristic or statistical based MT takes input from various sources in standard form of a language. Rule-based translation, by nature, does not include common non-standard usages. This causes errors in translation from a vernacular source or into colloquial language. Limitations on translation from casual speech present issues in the use of machine translation in mobile devices.\n\nName entities, in narrow sense, refer to concrete or abstract entities in the real world including people, organizations, companies, places etc. It also refers to expressing of time, space, quantity such as 1 July 2011, $79.99 and so on.\n\nNamed entities occur in the text being analyzed in statistical machine translation. The initial difficulty that arises in dealing with named entities is simply identifying them in the text. Consider the list of names common in a particular language to illustrate this – the most common names are different for each language and also are constantly changing. If named entities cannot be recognized by the machine translator, they may be erroneously translated as common nouns, which would most likely not affect the BLEU rating of the translation but would change the text's human readability. It is also possible that, when not identified, named entities will be omitted from the output translation, which would also have implications for the text's readability and message.\n\nAnother way to deal with named entities is to use transliteration instead of translation, meaning that you find the letters in the target language that most closely correspond to the name in the source language. There have been attempts to incorporate this into machine translation by adding a transliteration step into the translation procedure. However, these attempts still have their problems and have even been cited as worsening the quality of translation. Named entities were still identified incorrectly, with words not being transliterated when they should or being transliterated when they shouldn't. For example, for \"Southern California\" the first word should be translated directly, while the second word should be transliterated. However, machines would often transliterate both because they treated them as one entity. Words like these are hard for machine translators, even those with a transliteration component, to process.\n\nThe lack of attention to the issue of named entity translation has been recognized as potentially stemming from a lack of resources to devote to the task in addition to the complexity of creating a good system for named entity translation. One approach to named entity translation has been to transliterate, and not translate, those words. A second is to create a \"do-not-translate\" list, which has the same end goal – transliteration as opposed to translation. Both of these approaches still rely on the correct identification of named entities, however.\n\nA third approach to successful named entity translation is a class-based model. In this method, named entities are replaced with a token to represent the class they belong to. For example, \"Ted\" and \"Erica\" would both be replaced with \"person\" class token. In this way the statistical distribution and use of person names in general can be analyzed instead of looking at the distributions of \"Ted\" and \"Erica\" individually. A problem that the class based model solves is that the probability of a given name in a specific language will not affect the assigned probability of a translation. A study by Stanford on improving this area of translation gives the examples that different probabilities will be assigned to \"David is going for a walk\" and \"Ankit is going for a walk\" for English as a target language due to the different number of occurrences for each name in the training data. A frustrating outcome of the same study by Stanford (and other attempts to improve named recognition translation) is that many times, a decrease in the BLEU scores for translation will result from the inclusion of methods for named entity translation.\n\nSome work has been done in the utilization of multiparallel corpora, that is a body of text that has been translated into 3 or more languages. Using these methods, a text that has been translated into 2 or more languages may be utilized in combination to provide a more accurate translation into a third language compared with if just one of those source languages were used alone.\n\nAn ontology is a formal representation of knowledge which includes the concepts (such as objects, processes etc.) in a domain and some relations between them. If the stored information is of linguistic nature, one can speak of a lexicon.\nIn NLP, ontologies can be used as a source of knowledge for machine translation systems. With access to a large knowledge base, systems can be enabled to resolve many (especially lexical) ambiguities on their own.\nIn the following classic examples, as humans, we are able to interpret the prepositional phrase according to the context because we use our world knowledge, stored in our lexicons:\nA machine translation system initially would not be able to differentiate between the meanings because syntax does not change. With a large enough ontology as a source of knowledge however, the possible interpretations of ambiguous words in a specific context can be reduced.\nOther areas of usage for ontologies within NLP include information retrieval, information extraction and text summarization.\n\nThe ontology generated for the PANGLOSS knowledge-based machine translation system in 1993 may serve as an example of how an ontology for NLP purposes can be compiled:\n\nWhile no system provides the holy grail of fully automatic high-quality machine translation of unrestricted text, many fully automated systems produce reasonable output. The quality of machine translation is substantially improved if the domain is restricted and controlled.\n\nDespite their inherent limitations, MT programs are used around the world. Probably the largest institutional user is the European Commission. The MOLTO project, for example, coordinated by the University of Gothenburg, received more than 2.375 million euros project support from the EU to create a reliable translation tool that covers a majority of the EU languages. The further development of MT systems comes at a time when budget cuts in human translation may increase the EU's dependency on reliable MT programs. The European Commission contributed 3.072 million euros (via its ISA programme) for the creation of MT@EC, a statistical machine translation program tailored to the administrative needs of the EU, to replace a previous rule-based machine translation system.\n\nGoogle has claimed that promising results were obtained using a proprietary statistical machine translation engine. The statistical translation engine used in the Google language tools for Arabic <-> English and Chinese <-> English had an overall score of 0.4281 over the runner-up IBM's BLEU-4 score of 0.3954 (Summer 2006) in tests conducted by the National Institute for Standards and Technology.\n\nWith the recent focus on terrorism, the military sources in the United States have been investing significant amounts of money in natural language engineering. \"In-Q-Tel\" (a venture capital fund, largely funded by the US Intelligence Community, to stimulate new technologies through private sector entrepreneurs) brought up companies like Language Weaver. Currently the military community is interested in translation and processing of languages like Arabic, Pashto, and Dari. Within these languages, the focus is on key phrases and quick communication between military members and civilians through the use of mobile phone apps. The Information Processing Technology Office in DARPA hosts programs like TIDES and Babylon translator. US Air Force has awarded a $1 million contract to develop a language translation technology.\n\nThe notable rise of social networking on the web in recent years has created yet another niche for the application of machine translation software – in utilities such as Facebook, or instant messaging clients such as Skype, GoogleTalk, MSN Messenger, etc. – allowing users speaking different languages to communicate with each other. Machine translation applications have also been released for most mobile devices, including mobile telephones, pocket PCs, PDAs, etc. Due to their portability, such instruments have come to be designated as mobile translation tools enabling mobile business networking between partners speaking different languages, or facilitating both foreign language learning and unaccompanied traveling to foreign countries without the need of the intermediation of a human translator.\n\nDespite being labelled as an unworthy competitor to human translation in 1966 by the Automated Language Processing Advisory Committee put together by the United States government, the quality of machine translation has now been improved to such levels that its application in online collaboration and in the medical field are being investigated. In the Ishida and Matsubara lab of Kyoto University, methods of improving the accuracy of machine translation as a support tool for inter-cultural collaboration in today's globalized society are being studied. The application of this technology in medical settings where human translators are absent is another topic of research however difficulties arise due to the importance of accurate translations in medical diagnoses.\n\nThere are many factors that affect how machine translation systems are evaluated. These factors include the intended use of the translation, the nature of the machine translation software, and the nature of the translation process.\n\nDifferent programs may work well for different purposes. For example, statistical machine translation (SMT) typically outperforms example-based machine translation (EBMT), but researchers found that when evaluating English to French translation, EBMT performs better. The same concept applies for technical documents, which can be more easily translated by SMT because of their formal language.\n\nIn certain applications, however, e.g., product descriptions written in a controlled language, a dictionary-based machine-translation system has produced satisfactory translations that require no human intervention save for quality inspection.\n\nThere are various means for evaluating the output quality of machine translation systems. The oldest is the use of human judges to assess a translation's quality. Even though human evaluation is time-consuming, it is still the most reliable method to compare different systems such as rule-based and statistical systems. Automated means of evaluation include BLEU, NIST, METEOR, and LEPOR.\n\nRelying exclusively on unedited machine translation ignores the fact that communication in human language is context-embedded and that it takes a person to comprehend the context of the original text with a reasonable degree of probability. It is certainly true that even purely human-generated translations are prone to error. Therefore, to ensure that a machine-generated translation will be useful to a human being and that publishable-quality translation is achieved, such translations must be reviewed and edited by a human. The late Claude Piron wrote that machine translation, at its best, automates the easier part of a translator's job; the harder and more time-consuming part usually involves doing extensive research to resolve ambiguities in the source text, which the grammatical and lexical exigencies of the target language require to be resolved. Such research is a necessary prelude to the pre-editing necessary in order to provide input for machine-translation software such that the output will not be meaningless.\n\nIn addition to disambiguation problems, decreased accuracy can occur due to varying levels of training data for machine translating programs. Both example-based and statistical machine translation rely on a vast array of real example sentences as a base for translation, and when too many or too few sentences are analyzed accuracy is jeopardized. Researchers found that when a program is trained on 203,529 sentence pairings, accuracy actually decreases. The optimal level of training data seems to be just over 100,000 sentences, possibly because as training data increases, the number of possible sentences increases, making it harder to find an exact translation match.\n\nAlthough there have been concerns about machine translation's accuracy, Dr. Ana Nino of the University of Manchester has researched some of the advantages in utilizing machine translation in the classroom. One such pedagogical method is called using \"MT as a Bad Model.\" MT as a Bad Model forces the language learner to identify inconsistencies or incorrect aspects of a translation; in turn, the individual will (hopefully) possess a better grasp of the language. Dr. Nino cites that this teaching tool was implemented in the late 1980s. At the end of various semesters, Dr. Nino was able to obtain survey results from students who had used MT as a Bad Model (as well as other models.) Overwhelmingly, students felt that they had observed improved comprehension, lexical retrieval, and increased confidence in their target language.\n\nIn the early 2000s, options for machine translation between spoken and signed languages were severely limited. It was a common belief that deaf individuals could use traditional translators. However, stress, intonation, pitch, and timing are conveyed much differently in spoken languages compared to signed languages. Therefore, a deaf individual may misinterpret or become confused about the meaning of written text that is based on a spoken language.\n\nResearchers Zhao, et al. (2000), developed a prototype called TEAM (translation from English to ASL by machine) that completed English to American Sign Language (ASL) translations. The program would first analyze the syntactic, grammatical, and morphological aspects of the English text. Following this step, the program accessed a sign synthesizer, which acted as a dictionary for ASL. This synthesizer housed the process one must follow to complete ASL signs, as well as the meanings of these signs. Once the entire text is analyzed and the signs necessary to complete the translation are located in the synthesizer, a computer generated human appeared and would use ASL to sign the English text to the user.\n\nOnly works that are original are subject to copyright protection, so some scholars claim that machine translation results are not entitled to copyright protection because MT does not involve creativity. The copyright at issue is for a derivative work; the author of the original work in the original language does not lose his rights when a work is translated: a translator must have permission to publish a translation.\n\n\n\n",
    "id": "19980",
    "title": "Machine translation"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=22419840",
    "text": "Imense\n\nImense Ltd is a UK-based company that develops technology for Content-based image retrieval and automatic image annotation.\n\nThe founders of Imense are Dr Christopher Town and Dr David Sinclair. In their academic lives they developed the\nfirst 'Ontological Query Evaluation Language' (OQUEL) for image retrieval, which mapped a plain text user query onto a query over automatically recognized visual content in a corpus of images.\n\nTechnology derived in spirit from OQUEL is in routine use on the Imense PictureSearch portal.\nThe user interface allows a user to type a plain text query that is probabilistically parsed to recognise visual aspects (like 'purple center green background' or 'group of five people')\nand non visual aspects (e.g. 'freedom' or 'Buddhism' or 'Parma ham'). \nThe issues associated with scaling up image search to cope with tens of millions or more images were addressed with active\nsupport from the Science and Technology Facilities Council and GridPP. \nNews articles about Imense search technology include.\n\nKey papers describing the birth and evolution of ontological query languages include: \nThe research focus of Imense Ltd remains ontology based image content recognition. Imense uses cutting edge techniques from machine learning\nto build and train extremely-high-dimensional classifiers to help semantically label things in the visual world. Imense appears to use different types of visual models for different object classes — for example, Bayesian Constrained Local Models for parametric face modeling, and SVMs for general semantic content labeling.\n\n\n",
    "id": "22419840",
    "title": "Imense"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=743971",
    "text": "Content-based image retrieval\n\nContent-based image retrieval (CBIR), also known as query by image content (QBIC) and content-based visual information retrieval (CBVIR) is the application of computer vision techniques to the image retrieval problem, that is, the problem of searching for digital images in large databases (see this survey for a recent scientific overview of the CBIR field). Content-based image retrieval is opposed to traditional concept-based approaches (see Concept-based image indexing).\n\n\"Content-based\" means that the search analyzes the contents of the image rather than the metadata such as keywords, tags, or descriptions associated with the image. The term \"content\" in this context might refer to colors, shapes, textures, or any other information that can be derived from the image itself. CBIR is desirable because searches that rely purely on metadata are dependent on annotation quality and completeness. Having humans manually annotate images by entering keywords or metadata in a large database can be time consuming and may not capture the keywords desired to describe the image. The evaluation of the effectiveness of keyword image search is subjective and has not been well-defined. In the same regard, CBIR systems have similar challenges in defining success.\n\nThe term \"content-based image retrieval\" seems to have originated in 1992 when it was used by T. Kato to describe experiments into automatic retrieval of images from a database, based on the colors and shapes present. Since then, the term has been used to describe the process of retrieving desired images from a large collection on the basis of syntactical image features. The techniques, tools, and algorithms that are used originate from fields such as statistics, pattern recognition, signal processing, and computer vision\n\nThe earliest commercial CBIR system was developed by IBM and was called QBIC (Query by Image Content). Recent network and graph based approaches have presented a simple and attractive alternative to existing methods.\n\nThe interest in CBIR has grown because of the limitations inherent in metadata-based systems, as well as the large range of possible uses for efficient image retrieval. Textual information about images can be easily searched using existing technology, but this requires humans to manually describe each image in the database. This can be impractical for very large databases or for images that are generated automatically, e.g. those from surveillance cameras. It is also possible to miss images that use different synonyms in their descriptions. Systems based on categorizing images in semantic classes like \"cat\" as a subclass of \"animal\" can avoid the miscategorization problem, but will require more effort by a user to find images that might be \"cats\", but are only classified as an \"animal\". Many standards have been developed to categorize images, but all still face scaling and miscategorization issues.\n\nInitial CBIR systems were developed to search databases based on image color, texture, and shape properties. After these systems were developed, the need for user-friendly interfaces became apparent. Therefore, efforts in the CBIR field started to include human-centered design that tried to meet the needs of the user performing the search. This typically means inclusion of: query methods that may allow descriptive semantics, queries that may involve user feedback, systems that may include machine learning, and systems that may understand user satisfaction levels.\n\nMany CBIR systems have been developed, but the problem of retrieving images on the basis of their pixel content remains largely unsolved.\n\nDifferent implementations of CBIR make use of different types of user queries.\n\nQuery by example is a query technique that involves providing the CBIR system with an example image that it will then base its search upon. The underlying search algorithms may vary depending on the application, but result images should all share common elements with the provided example.\n\nOptions for providing example images to the system include:\n\n\nThis query technique removes the difficulties that can arise when trying to describe images with words.\n\n\"Semantic\" retrieval starts with a user making a request like \"find pictures of Abraham Lincoln\". This type of open-ended task is very difficult for computers to perform - Lincoln may not always be facing the camera or in the same pose. Many CBIR systems therefore generally make use of lower-level features like texture, color, and shape. These features are either used in combination with interfaces that allow easier input of the criteria or with databases that have already been trained to match features (such as faces, fingerprints, or shape matching). However, in general, image retrieval requires human feedback in order to identify higher-level concepts.\n\nCombining CBIR search techniques available with the wide range of potential users and their intent can be a difficult task. An aspect of making CBIR successful relies entirely on the ability to understand the user intent. CBIR systems can make use of \"relevance feedback\", where the user progressively refines the search results by marking images in the results as \"relevant\", \"not relevant\", or \"neutral\" to the search query, then repeating the search with the new information. Examples of this type of interface have been developed.\n\nMachine learning and application of iterative techniques are becoming more common in CBIR.\n\nOther query methods include browsing for example images, navigating customized/hierarchical categories, querying by image region (rather than the entire image), querying by multiple example images, querying by visual sketch, querying by direct specification of image features, and multimodal queries (e.g. combining touch, voice, etc.)\n\nThe most common method for comparing two images in content-based image retrieval (typically an example image and an image from the database) is using an image distance measure. An image distance measure compares the similarity of two images in various dimensions such as color, texture, shape, and others. For example, a distance of 0 signifies an exact match with the query, with respect to the dimensions that were considered. As one may intuitively gather, a value greater than 0 indicates various degrees of similarities between the images. Search results then can be sorted based on their distance to the queried image. Many measures of image distance (Similarity Models) have been developed.\n\nComputing distance measures based on color similarity is achieved by computing a color histogram for each image that identifies the proportion of pixels within an image holding specific values. Examining images based on the colors they contain is one of the most widely used techniques because it can be completed without regard to image size or orientation. However, research has also attempted to segment color proportion by region and by spatial relationship among several color regions.\n\nTexture measures look for visual patterns in images and how they are spatially defined. Textures are represented by texels which are then placed into a number of sets, depending on how many textures are detected in the image. These sets not only define the texture, but also where in the image the texture is located.\n\nTexture is a difficult concept to represent. The identification of specific textures in an image is achieved primarily by modeling texture as a two-dimensional gray level variation. The relative brightness of pairs of pixels is computed such that degree of contrast, regularity, coarseness and directionality may be estimated. The problem is in identifying patterns of co-pixel variation and associating them with particular classes of textures such as \"silky\", or \"rough\".\n\nOther methods of classifying textures include:\n\n\nShape does not refer to the shape of an image but to the shape of a particular region that is being sought out. Shapes will often be determined first applying segmentation or edge detection to an image. Other methods use shape filters to identify given shapes of an image. Shape descriptors may also need to be invariant to translation, rotation, and scale.\n\nSome shape descriptors include:\n\n\nMeasures of image retrieval can be defined in terms of precision and recall. However, there are other methods being considered.\n\nAn image is retrieved in CBIR system by adopting several techniques simultaneously such as Integrating Pixel Cluster Indexing, histogram intersection and discrete wavelet transform methods.\n\nPotential uses for CBIR include:\n\n\nCommercial Systems that have been developed include:\n\nExperimental Systems include:\n\n\n\n",
    "id": "743971",
    "title": "Content-based image retrieval"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=22717006",
    "text": "GestureTek\n\nGestureTek is an American-based interactive technology company headquartered in Silicon Valley, California, with offices in Toronto and Ottawa, Ontario and Asia.\n\nFounded in 1986 by Canadians Vincent John Vincent and Francis MacDougall, this privately held company develops and licenses gesture recognition software based on computer vision techniques. The partners invented video gesture control in 1986 and received their base patent in 1996 for the GestPoint video gesture control system. GestPoint technology is a camera-enabled video tracking software system that translates hand and body movement into computer control. The system enables users to navigate and control interactive multi-media and menu-based content, engage in virtual reality game play, experience immersion in an augmented reality environment or interact with a consumer device (such a television, mobile phone or set top box) without using touch-based peripherals.\nSimilar companies include gesture recognition specialist LM3LABS based in Tokyo, Japan.\n\nGestureTek’s gesture interface applications include multi-touch and 3D camera tracking. GestureTek’s multi-touch technology powers the multi-touch table in Melbourne’s Eureka Tower. A GestureTek multi-touch table with object recognition is found at the New York City Visitors Center. Telefónica has a multi-touch window with technology from GestureTek. GestureTek’s 3D tracking technology is used in a 3D television prototype from Hitachi and various digital signage and display solutions based on 3D interaction.\n\nGestureTek currently has 8 patents awarded, including: 5,534,917 (Video Gesture Control Motion Detection); 7,058,204 (Multiple Camera Control System, Point to Control Base Patent); 7,421,093 (Multiple Camera Tracking System for Interfacing With an Application); 7,227,526 (Stereo Camera Control, 3D-Vision Image Control System); 7,379,563 (Two Handed Movement Tracker Tracking Bi-Manual Movements); 7,379,566 (Optical Flow-Based Tilt Sensor For Phone Tilt Control); 7,389,591 (Phone Tilt for Typing & Menus/Orientation-Sensitive Signal Output); 7,430,312 (Five Camera 3D Face Capture).\n\nGestureTek’s software and patents have been licensed by Microsoft for the Xbox 360, Sony for the EyeToy, NTT DoCoMo for their mobile phones and Hasbro for the ION Educational Gaming System. In addition to software provision, GestureTek also fabricates interactive gesture control display systems with natural user interface for interactive advertising, games and presentations.\n\nIn addition, GestureTek’s natural user interface virtual reality system has been the subject of research by universities and hospitals for its application in both physical therapy and physical rehabilitation.\n\nIn 2008, GestureTek received the Mobile Innovation Global Award from the GSMA for its software-based, gesture-controlled user interface for mobile games and applications. The technology is used by Java platform integration providers and mobile developers. Katamari Damacy is one example of a gesture control mobile game powered by GestureTek software.\n\nOther companies in the industry of interactive projections for marketing and retail experiences include Po-motion Inc., Touchmagix and LM3LABS.\n",
    "id": "22717006",
    "title": "GestureTek"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1657551",
    "text": "Automatic number-plate recognition\n\nAutomatic number-plate recognition (ANPR; see also other names below) is a technology that uses optical character recognition on images to read vehicle registration plates to create vehicle location data. It can use existing closed-circuit television, road-rule enforcement cameras, or cameras specifically designed for the task. ANPR is used by police forces around the world for law enforcement purposes, including to check if a vehicle is registered or licensed. It is also used for electronic toll collection on pay-per-use roads and as a method of cataloguing the movements of traffic, for example by highways agencies.\n\nAutomatic number plate recognition can be used to store the images captured by the cameras as well as the text from the license plate, with some configurable to store a photograph of the driver. Systems commonly use infrared lighting to allow the camera to take the picture at any time of day or night. ANPR technology must take into account plate variations from place to place.\n\nConcerns about these systems have centered on privacy fears of government tracking citizens' movements, misidentification, high error rates, and increased government spending. Critics have described it as a form of mass surveillance.\n\nANPR is sometimes known by various other terms:\nANPR was invented in 1976 at the Police Scientific Development Branch in the UK. Prototype systems were working by 1979, and contracts were awarded to produce industrial systems, first at EMI Electronics, and then at Computer Recognition Systems (CRS) in Wokingham, UK. Early trial systems were deployed on the A1 road and at the Dartford Tunnel. The first arrest through detection of a stolen car was made in 1981. However, ANPR did not become widely used until new developments in cheaper and easier to use software were pioneered during the 1990s. The collection of ANPR data for future use (\"i.e\"., in solving then-unidentified crimes) was documented in the early 2000s. The first documented case of ANPR being used to help solve a murder occurred in November 2005, in Bradford, UK, where ANPR played a vital role in locating and subsequently convicting killers of Sharon Beshenivsky.\n\nThe software aspect of the system runs on standard home computer hardware and can be linked to other applications or databases. It first uses a series of image manipulation techniques to detect, normalize and enhance the image of the number plate, and then optical character recognition (OCR) to extract the alphanumerics of the license plate. ANPR systems are generally deployed in one of two basic approaches: one allows for the entire process to be performed at the lane location in real-time, and the other transmits all the images from many lanes to a remote computer location and performs the OCR process there at some later point in time. When done at the lane site, the information captured of the plate alphanumeric, date-time, lane identification, and any other information required is completed in approximately 250 milliseconds. This information can easily be transmitted to a remote computer for further processing if necessary, or stored at the lane for later retrieval. In the other arrangement, there are typically large numbers of PCs used in a server farm to handle high workloads, such as those found in the London congestion charge project. Often in such systems, there is a requirement to forward images to the remote server, and this can require larger bandwidth transmission media.\n\nANPR uses optical character recognition (OCR) on images taken by cameras. When Dutch vehicle registration plates switched to a different style in 2002, one of the changes made was to the font, introducing small gaps in some letters (such as \"P\" and \"R\") to make them more distinct and therefore more legible to such systems. Some license plate arrangements use variations in font sizes and positioning—ANPR systems must be able to cope with such differences in order to be truly effective. More complicated systems can cope with international variants, though many programs are individually tailored to each country.\n\nThe cameras used can be existing road-rule enforcement or closed-circuit television cameras, as well as mobile units, which are usually attached to vehicles. Some systems use infrared cameras to take a clearer image of the plates.\n\nDuring the 1990s, significant advances in technology took automatic number plate recognition (ANPR) systems from limited expensive, hard to set up, fixed based applications to simple \"point and shoot\" mobile ones. This was made possible by the creation of software that ran on cheaper PC based, non-specialist hardware that also no longer needed to be given the pre-defined angles, direction, size and speed in which the plates would be passing the camera's field of view. Further scaled-down components at more cost-effective price points led to a record number of deployments by law enforcement agencies around the world. Smaller cameras with the ability to read license plates at higher speeds, along with smaller, more durable processors that fit in the trunks of police vehicles, allowed law enforcement officers to patrol daily with the benefit of license plate reading in real time, when they can interdict immediately.\n\nDespite their effectiveness, there are noteworthy challenges related with mobile ANPRs. One of the biggest is that the processor and the cameras must work fast enough to accommodate relative speeds of more than 100 mph (160 km/h), a likely scenario in the case of oncoming traffic. This equipment must also be very efficient since the power source is the vehicle battery, and equipment must be small to minimize the space it requires.\n\nRelative speed is only one issue that affects the camera's ability to actually read a license plate. Algorithms must be able to compensate for all the variables that can affect the ANPR's ability to produce an accurate read, such as time of day, weather and angles between the cameras and the license plates. A system's illumination wavelengths can also have a direct impact on the resolution and accuracy of a read in these conditions.\n\nInstalling ANPR cameras on law enforcement vehicles requires careful consideration of the juxtaposition of the cameras to the license plates they are to read. Using the right number of cameras and positioning them accurately for optimal results can prove challenging, given the various missions and environments at hand. Highway patrol requires forward-looking cameras that span multiple lanes and are able to read license plates at very high speeds. City patrol needs shorter range, lower focal length cameras for capturing plates on parked cars. Parking lots with perpendicularly parked cars often require a specialized camera with a very short focal length. Most technically advanced systems are flexible and can be configured with a number of cameras ranging from one to four which can easily be repositioned as needed. States with rear-only license plates have an additional challenge since a forward-looking camera is ineffective with oncoming traffic. In this case one camera may be turned backwards.\n\nThere are seven primary algorithms that the software requires for identifying a license plate:\n\nThe complexity of each of these subsections of the program determines the accuracy of the system. During the third phase (normalization), some systems use edge detection techniques to increase the picture difference between the letters and the plate backing. A median filter may also be used to reduce the visual noise on the image.\n\nThere are a number of possible difficulties that the software must be able to cope with. These include:\n\nWhile some of these problems can be corrected within the software, it is primarily left to the hardware side of the system to work out solutions to these difficulties. Increasing the height of the camera may avoid problems with objects (such as other vehicles) obscuring the plate but introduces and increases other problems, such as the adjusting for the increased skew of the plate.\n\nOn some cars, tow bars may obscure one or two characters of the license plate. Bikes on bike racks can also obscure the number plate, though in some countries and jurisdictions, such as Victoria, Australia, \"bike plates\" are supposed to be fitted.\nSome small-scale systems allow for some errors in the license plate. When used for giving specific vehicles access to a barricaded area, the decision may be made to have an acceptable error rate of one character. This is because the likelihood of an unauthorized car having such a similar license plate is seen as quite small. However, this level of inaccuracy would not be acceptable in most applications of an ANPR system.\n\nAt the front end of any ANPR system is the imaging hardware which captures the image of the license plates. The initial image capture forms a critically important part of the ANPR system which, in accordance to the garbage in, garbage out principle of computing, will often determine the overall performance.\n\nLicense plate capture is typically performed by specialized cameras designed specifically for the task, although new software techniques are being implemented that support any I.P.-based surveillance camera and increase the utility of ANPR for perimeter security applications. Factors which pose difficulty for license plate imaging cameras include the speed of the vehicles being recorded, varying level of ambient light, headlight glare and harsh environmental conditions. Most dedicated license plate capture cameras will incorporate infrared illumination in order to solve the problems of lighting and plate reflectivity.\nMany countries now use license plates that are retroreflective. This returns the light back to the source and thus improves the contrast of the image. In some countries, the characters on the plate are not reflective, giving a high level of contrast with the reflective background in any lighting conditions. A camera that makes use of active infrared imaging (with a normal colour filter over the lens and an infrared illuminator next to it) benefits greatly from this as the infrared waves are reflected back from the plate. This is only possible on dedicated ANPR cameras, however, and so cameras used for other purposes must rely more heavily on the software capabilities. Further, when a full-colour image is required as well as use of the ANPR-retrieved details, it is necessary to have one infrared-enabled camera and one normal (colour) camera working together.\n\nTo avoid blurring it is ideal to have the shutter speed of a dedicated camera set to 1/1000 of a second. It is also important that the camera uses a global shutter, as opposed to rolling shutter, to assure that the taken images are distortion-free. Because the car is moving, slower shutter speeds could result in an image which is too blurred to read using the OCR software, especially if the camera is much higher up than the vehicle. In slow-moving traffic, or when the camera is at a lower level and the vehicle is at an angle approaching the camera, the shutter speed does not need to be so fast. Shutter speeds of 1/500 of a second can cope with traffic moving up to 40 mph (64 km/h) and 1/250 of a second up to 5 mph (8 km/h). License plate capture cameras can produce usable images from vehicles traveling at .\n\nTo maximize the chances of effective license plate capture, installers should carefully consider the positioning of the camera relative to the target capture area. Exceeding threshold angles of incidence between camera lens and license plate will greatly reduce the probability of obtaining usable images due to distortion. Manufacturers have developed tools to help eliminate errors from the physical installation of license plate capture cameras.\n\nSeveral State Police Forces, and the Department of Justice (Victoria) use both fixed and mobile ANPR systems. The New South Wales Police Force Highway Patrol were the first to trial and use a fixed ANPR camera system in Australia in 2005. In 2009 they began a roll-out of a mobile ANPR system (known officially as MANPR) with three infrared cameras fitted to its Highway Patrol fleet. The system identifies unregistered and stolen vehicles as well as disqualified or suspended drivers as well as other 'persons of interest' such as persons having outstanding warrants.\n\nThe city of Mechelen uses an ANPR system since September 2011 to scan all cars crossing the city limits (inbound and outbound). Cars listed on 'black lists' (no insurance, stolen, etc.) generate an alarm in the dispatching room, so they can be intercepted by a patrol.\nAs of early 2012, 1 million cars per week are automatically checked in this way.\n\nThe police service in Ontario uses automatic licence plate recognition software to nab drivers behind the wheels of vehicles with Ontario number plates.\n\nThe technique is tested by the Danish police. It will be in permanent use from the end of 2015.\n\n180 gantries over major roads have been built throughout the country. These together with a further 250 fixed cameras is to enable a levy of an eco tax on lorries over 3.5 tonnes. The system is currently being opposed and whilst they may be collecting data on vehicles passing the cameras, no eco tax is being charged.\n\nOn 11 March 2008, the Federal Constitutional Court of Germany ruled that some areas of the laws permitting the use of automated number plate recognition systems in Germany violated the right to privacy. More specifically, the court found that the retention of any sort of information (i.e., number plate data) which was not for any pre-destined use (e.g., for use tracking suspected terrorists or for enforcement of speeding laws) was in violation of German law.\nThese systems were provided by Jenoptik Robot GmbH, and called TraffiCapture.\n\nIn 2012 a state consortium was formed among the Hungarian Ministry of Interior, the National Police Headquarters and the Central Commission of Public Administration and Electronic Services with the aim to install and operate a unified intelligent transportation system (\"ITS\") with nationwide coverage by the end of 2015. Within the system, 160 portable traffic enforcement and data-gathering units and 365 permanent gantry installations were brought online with ANPR, speed detection, imaging and statistical capabilities. Since all the data points are connected to a centrally located ITS, each member of the consortium is able to separately utilize its range of administrative and enforcement activities, such as remote vehicle registration and insurance verification, speed, lane and traffic light enforcement and wanted or stolen vehicle interception among others.\n\nSeveral Hungarian auxiliary police units also use a system called Matrix Police in cooperation with the police. It consists of a portable computer equipped with a web camera that scans the stolen car database using automatic number plate recognition. The system is installed on the dashboard of selected patrol vehicles (PDA-based hand-held versions also exist) and is mainly used to control the license plate of parking cars. As the Auxiliary Police do not have the authority to order moving vehicles to stop, if a stolen car is found, the formal police is informed.\n\nThe technique is proposed by Lahore Police; if approved it will be functional by 2018.\n\nVehicle registration plates in Saudi Arabia use white background, but several vehicle types may have a different background. United States diplomatic plates have the letters 'USD', which in Arabic reads 'DSU' when read from right to left in the direction of Arabic script. There are only 17 Arabic letters used on the registration plates. A Challenge for plates recognition in Saudi Arabia is the size of the digits. Some plates use both Eastern Arabic numerals and the 'Western Arabic' equivalents. A research with source code is available for APNR Arabic digits.\n\nThe technique is tested by the Swedish police at nine different places in Sweden.\n\nSeveral cities have tested—and some have put into service—the KGYS (Kent Guvenlik Yonetim Sistemi, City Security Administration System), , i.e., capital Ankara, has debuted KGYS- which consists of a registration plate number recognition system on the main arteries and city exits. The system has been used with two cameras per lane, one for plate recognition, one for speed detection. Now the system has been widened to network all the registration number cameras together, and enforcing average speed over preset distances. Some arteries have limit, and some , and photo evidence with date-time details are posted to registration address if speed violation is detected. As of 2012, the fine for exceeding the speed limit for more than 30% is approximately US$175.\n\nThe project of system integration «OLLI Technology» and the Ministry of Internal Affairs of Ukraine Department of State Traffic Inspection (STI) experiments on the introduction of a modern technical complex which is capable to locate stolen cars, drivers deprived of driving licenses and other problem cars in real time. The Ukrainian complex \"Video control\" working by a principle of video fixing of the car with recognition of license plates with check under data base.\n\nThe Home Office states the purpose of automatic number plate recognition in the United Kingdom is to help detect, deter and disrupt criminality including tackling organised crime groups and terrorists. Vehicle movements are recorded by a network of nearly 8000 cameras capturing between 25 and 30 million ANPR ‘read’ records daily. These records are stored for up to two years in the National ANPR Data Center, which can be accessed, analysed and used as evidence as part of investigations by UK law enforcement agencies.\n\nIn 2012, the UK Parliament enacted the Protection of Freedoms Act which includes several provisions related to controlling and restricting the collection, storage, retention, and use of information about individuals. Under this Act, the Home Office published a code of practice in 2013 for the use of surveillance cameras, including ANPR, by government and law enforcement agencies. The aim of the code is to help ensure their use is \"characterised as surveillance by consent, and such consent on the part of the community must be informed consent and not assumed by a system operator. Surveillance by consent should be regarded as analogous to policing by consent.\" In addition, a set a standards were introduced in 2014 for data, infrastructure, and data access and management.\n\nIn the United States, ANPR systems are more commonly referred to as ALPR (Automatic License Plate Reader/Recognition) technology, due to differences in language (i.e., \"number plates\" are referred to as \"license plates\" in American English)\n\nMobile ANPR use is widespread among US law enforcement agencies at the city, county, state and federal level. According to a 2012 report by the Police Executive Research Forum, approximately 71% of all US police departments use some form of\nANPR. Mobile ANPR is becoming a significant component of municipal predictive policing strategies and intelligence gathering, as well as for recovery of stolen vehicles, identification of wanted felons, and revenue collection from individuals who are delinquent on city or state taxes or fines, or monitoring for \"Amber Alerts\". With the widespread implementation of this technology, many U.S. states now issue misdemeanor citations of up to $500 when a license plate is identified as expired or on the incorrect vehicle. Successfully recognized plates may be matched against databases including \"wanted person\", \"protection order\", missing person, gang member, known and suspected terrorist, supervised release, immigration violator, and National Sex Offender lists. In addition to the real-time processing of license plate numbers, ANPR systems in the US collect (and can indefinitely store) data from each license plate capture. Images, dates, times and GPS coordinates can be stockpiled and can help place a suspect at a scene, aid in witness identification, pattern recognition or the tracking of individuals.\n\nThe Department of Homeland Security has proposed a federal database to combine all monitoring systems, which was cancelled after privacy complaints. In 1998, a Washington D.C. police lieutenant pleaded guilty to extortion after blackmailing the owners of vehicles parked near a gay bar. In 2015, the Los Angeles Police Department proposed sending letters to the home addresses of all vehicles that enter areas of high prostitution.\n\nAn early, private sector mobile ANPR application has been applications for vehicle repossession and recovery), although the application of ANPR by private companies to collect information from privately owned vehicles or collected from private property (for example, driveways) has become an issue of sensitivity and public debate. Other ANPR uses include parking enforcement, and revenue collection from individuals who are delinquent on city or state taxes or fines. The technology is often featured in the reality TV show \"Parking Wars\" featured on A&E Network. In the show, tow truck drivers and booting teams use the ANPR to find delinquent vehicles with high amounts of unpaid parking fines.\n\nANPR is used for speed limit enforcement in Australia, Austria, Belgium, Dubai (UAE), France, Italy, The Netherlands, Spain, South Africa, the UK, and Kuwait.\n\nThis works by tracking vehicles' travel time between two fixed points, and calculating the average speed. These cameras are claimed to have an advantage over traditional speed cameras in maintaining steady legal speeds over extended distances, rather than encouraging heavy braking on approach to specific camera locations and subsequent acceleration back to illegal speeds.\n\nIn has developed a monitoring system named covering more than 2500 km (2012). The Tutor system is also able to intercept cars while changing lanes.\n\nAverage speed cameras (\"trajectcontrole\") are in place in the Netherlands since 2002. As of July 2009, 12 cameras were operational, mostly in the west of the country and along the A12. Some of these are divided in several “sections” to allow for cars leaving and entering the motorway.\n\nA first experimental system was tested on a short stretch of the A2 in 1997 and was deemed a big success by the police, reducing overspeeding to 0.66%, compared to 5 to 6% when regular speed cameras were used at the same location. The first permanent average speed cameras were installed on the A13 in 2002, shortly after the speed limit was reduced to 80 km/h to limit noise and air pollution in the area. In 2007, average speed cameras resulted in 1.7 million fines for overspeeding out of a total of 9.7 millions. According to the Dutch Attorney General, the average number of violation of the speed limits on motorway sections equipped with average speed cameras is between 1 and 2%, compared to 10 to 15% elsewhere.\n\nOne of the most notable stretches of average speed cameras in the UK is found on the A77 road in Scotland, with being monitored between Kilmarnock and Girvan. In 2006 it was confirmed that speeding tickets could potentially be avoided from the 'SPECS' cameras by changing lanes and the RAC Foundation feared that people may play \"Russian Roulette\" changing from one lane to another to lessen their odds of being caught. However, in 2007 the system was upgraded for multi-lane use and in 2008 the manufacturer described the \"myth\" as “categorically untrue”. There exists evidence that implementation of systems such as SPECS has a considerable effect on the volume of drivers travelling at excessive speeds; on the stretch of road mentioned above (A77 Between Glasgow and Ayr) there has been noted a \"huge drop\" in speeding violations since the introduction of a SPECS system.\n\nRecent innovations have contributed to the adoption of ANPR for perimeter security and access control applications at government facilities. Within the US, \"homeland security\" efforts to protect against alleged \"acts of terrorism\" have resulted in adoption of ANPR for sensitive facilities such as embassies, schools, airports, maritime ports, military and federal buildings, law enforcement and government facilities, and transportation centers. ANPR is marketed as able to be implemented through networks of IP based surveillance cameras that perform \"double duty\" alongside facial recognition, object tracking, and recording systems for the purpose of monitoring suspicious or anomalous behavior, improving access control, and matching against watch lists. ANPR systems are most commonly installed at points of significant sensitivity, ingress or egress. Major US agencies such as the Department of Homeland Security, the Department of Justice, the Department of Transportation and the Department of Defense have purchased ANPR for perimeter security applications. Large networks of ANPR systems are being installed by cities such as Boston, London and New York City to provide citywide protection against acts of terrorism, and to provide support for public gatherings and public spaces.\n\nThe Center For Evidence-Based Crime Policy in George Mason University identifies the following randomized controlled trials of automatic number plate recognition technology as very rigorous.\n\nIn addition to government facilities, many private sector industries with facility security concerns are beginning to implement ANPR solutions. Examples include casinos, hospitals, museums, parking facilities, and resorts. In the US, private facilities typically cannot access government or police watch lists, but may develop and match against their own databases for customers, VIPs, critical personnel or \"banned person\" lists. In addition to providing perimeter security, private ANPR has service applications for valet / recognized customer and VIP recognition, logistics and key personnel tracking, sales and advertising, parking management, and logistics (vendor and support vehicle tracking).\n\nMany cities and districts have developed traffic control systems to help monitor the movement and flow of vehicles around the road network. This had typically involved looking at historical data, estimates, observations and statistics, such as:\n\nCCTV cameras can be used to help traffic control centres by giving them live data, allowing for traffic management decisions to be made in real-time. By using ANPR on this footage it is possible to monitor the travel of individual vehicles, automatically providing information about the speed and flow of various routes. These details can highlight problem areas as and when they occur and help the centre to make informed incident management decisions.\n\nSome counties of the United Kingdom have worked with Siemens Traffic to develop traffic monitoring systems for their own control centres and for the public. Projects such as Hampshire County Council's ROMANSE provide an interactive and real-time website showing details about traffic in the city. The site shows information about car parks, ongoing road works, special events and footage taken from CCTV cameras. ANPR systems can be used to provide average point-to-point journey times along particular routes, which can be displayed on a variable-message sign(VMS) giving drivers the ability to plan their route. ROMANSE also allows travellers to see the current situation using a mobile device with an Internet connection (such as WAP, GPRS or 3G), allowing them to view mobile device CCTV images within the Hampshire road network.\n\nThe UK company Trafficmaster has used ANPR since 1998 to estimate average traffic speeds on non-motorway roads without the results being skewed by local fluctuations caused by traffic lights and similar. The company now operates a network of over 4000 ANPR cameras, but claims that only the four most central digits are identified, and no numberplate data is retained.\n\nIEEE Intelligent Transportation Systems Society published some papers on the plate number recognition technologies and applications.\n\nOntario's 407 ETR highway uses a combination of ANPR and radio transponders to toll vehicles entering and exiting the road. Radio antennas are located at each junction and detect the transponders, logging the unique identity of each vehicle in much the same way as the ANPR system does. Without ANPR as a second system it would not be possible to monitor all the traffic. Drivers who opt to rent a transponder for C$2.55 per month are not charged the \"Video Toll Charge\" of C$3.60 for using the road, with heavy vehicles (those with a gross weight of over 5,000 kg) being required to use one. Using either system, users of the highway are notified of the usage charges by post.\n\nThere are numerous other electronic toll collection networks which use this combination of Radio frequency identification and ANPR. These include:\n\nPortuguese roads have old highways with toll stations where drivers can pay with cards and also lanes where there are electronic collection systems. However most new highways only have the option of electronic toll collection system.\nThe electronic toll collection system comprises three different structures:\nWhen the smart tag is installed in the vehicle, the car is quickly identified and owner's bank account is automatically deducted. This process is realized at any speed up to over 250 km per hour.\nIf the car does not have the smart tag, the driver is required to go to a pay station to pay the tolls between 3rd and 5th day after with a surplus charge. If he fails to do so, the owner is sent a letter home with a heavy fine. If this is not paid, it increases five-fold and after that, the car is inserted into a police database for vehicle impounding.\nThis system is also used in some limited access areas of main cities to allow only entry from pre-registered residents. It is planned to be implemented both in more roads and in city entrance toll collection/access restriction. The efficacy of the system is considered to be so high that it is almost impossible for the driver to complain.\n\nThe London congestion charge is an example of a system that charges motorists entering a payment area. Transport for London (TfL) uses ANPR systems and charges motorists a daily fee of £11.50 if they enter, leave or move around within the congestion charge zone between 7 a.m. and 6:00 p.m., Monday to Friday. A reduced fee of £10.50 is paid by vehicle owners who sign up for the automatic deduction scheme. Fines for traveling within the zone without paying the charge are £65 per infraction if paid before the deadline, doubling to £130 per infraction thereafter.\n\nThere are currently 1,500 cameras which use automatic number plate recognition (ANPR) technology. There are also a number of mobile camera units which may be deployed anywhere in the zone.\n\nIt is estimated that around 98% of vehicles moving within the zone are caught on camera. The video streams are transmitted to a data centre located in central London where the ANPR software deduces the registration plate of the vehicle. A second data centre provides a backup location for image data.\n\nBoth front and back number plates are being captured, on vehicles going both in and out – this gives up to four chances to capture the number plates of a vehicle entering and exiting the zone. This list is then compared with a list of cars whose owners/operators have paid to enter the zone – those that have not paid are fined. The registered owner of such a vehicle is looked up in a database provided by the DVLA.\n\nIn Stockholm, Sweden, ANPR is used for the Stockholm congestion tax, owners of cars driving into or out of the inner city must pay a charge, depending on the time of the day. From 2013, also for the Gothenburg congestion tax, which also includes vehicles passing the city on the main highways.\n\nSeveral UK companies and agencies use ANPR systems. These include Vehicle and Operator Services Agency (VOSA), Driver and Vehicle Licensing Agency (DVLA) and Transport for London.\n\nANPR systems may also be used for/by:\n\nVehicle owners have used a variety of techniques in an attempt to evade ANPR systems and road-rule enforcement cameras in general. One method increases the reflective properties of the lettering and makes it more likely that the system will be unable to locate the plate or produce a high enough level of contrast to be able to read it. This is typically done by using a plate cover or a spray, though claims regarding the effectiveness of the latter are disputed. In most jurisdictions, the covers are illegal and covered under existing laws, while in most countries there is no law to disallow the use of the sprays. Other users have attempted to smear their license plate with dirt or utilize covers to mask the plate.\n\nNovelty frames around Texas license plates were made illegal in Texas on 1 September 2003 by Texas Senate Bill 439 because they caused problems with ANPR devices. That law made it a Class C misdemeanor (punishable by a fine of up to US $200), or Class B (punishable by a fine of up to US $2,000 and 180 days in jail) if it can be proven that the owner did it to deliberately obscure their plates. The law was later clarified in 2007 to allow Novelty frames.\n\nIf an ANPR system cannot read the plate, it can flag the image for attention, with the human operators looking to see if they are able to identify the alphanumerics.\n\nIn order to avoid surveillance or penalty charges, there has been an upsurge in car cloning. This is usually achieved by copying registration plates from another car of a similar model and age. This can be difficult to detect, especially as cloners may change the registration plates and travel behavior to hinder investigations.\n\nIn 2013 researchers at Sunflex Zone Ltd created a privacy license plate frame that uses near infrared light to make the license plate unreadable to license plate recognition systems.\n\nThe introduction of ANPR systems has led to fears of misidentification and the furthering of \"1984\"-style surveillance. In the United States, some such as Gregg Easterbrook oppose what they call \"machines that issue speeding tickets and red-light tickets\" as the beginning of a slippery slope towards an automated justice system:\n\nSimilar criticisms have been raised in other countries. Easterbrook also argues that this technology is employed to maximize revenue for the state, rather than to promote safety.\nThe electronic surveillance system produces tickets which in the US are often in excess of $100, and are virtually impossible for a citizen to contest in court without the help of an attorney. The revenues generated by these machines are shared generously with the private corporation that builds and operates them, creating a strong incentive to tweak the system to generate as many tickets as possible.\n\nOlder systems had been notably unreliable; in the UK this has been known to lead to charges being made incorrectly with the vehicle owner having to pay £10 in order to be issued with proof (or not) of the offense. Improvements in technology have drastically decreased error rates, but false accusations are still frequent enough to be a problem.\n\nPerhaps the best known incident involving the abuse of an ANPR database in North America is the case of \"Edmonton Sun\" reporter Kerry Diotte in 2004. Diotte wrote an article critical of Edmonton police use of traffic cameras for revenue enhancement, and in retaliation was added to an ANPR database of \"high-risk drivers\" in an attempt to monitor his habits and create an opportunity to arrest him. The police chief and several officers were fired as a result, and The Office of the Privacy Commissioner of Canada expressed public concern over the \"growing police use of technology to spy on motorists.\"\n\nOther concerns include the storage of information that could be used to identify people and store details about their driving habits and daily life, contravening the Data Protection Act along with similar legislation (see personally identifiable information). The laws in the UK are strict for any system that uses CCTV footage and can identify individuals.\n\nAlso of concern is the safety of the data once it is mined, following the discovery of police surveillance records lost in a gutter.\n\nThere is also a case in the UK for saying that use of ANPR cameras is unlawful under the Regulation of Investigatory Powers Act 2000. The breach exists, some say, in the fact that ANPR is used to monitor the activities of law-abiding citizens and treats everyone like the suspected criminals intended to be surveyed under the Act. The police themselves have been known to refer to the system of ANPR as a \"24/7 traffic movement database\" which is a diversion from its intended purpose of identifying vehicles involved in criminal activities. The opposing viewpoint is that where the plates have been cloned, a 'read' of an innocent motorist's vehicle will allow the elimination of that vehicle from an investigation by visual examination of the images stored. Likewise, stolen vehicles are read by ANPR systems between the time of theft and report to the Police, assisting in the investigation.\n\nThe \"Associated Press\" reported in August 2011 that New York Police Department cars and license plate tracking equipment purchased with federal HIDTA (High Intensity Drug Trafficking Area) funds were used to spy on Muslims at mosques, and to track the license plate numbers of worshipers.\n\nPolice in unmarked cars outfitted with electronic license plate readers would drive down the street and automatically catalog the plates of everyone parked near the mosque, amassing a covert database that would be distributed among officers and used to profile Muslims in public.\n\nIn 2013 the American Civil Liberties Union released 26,000 pages of data about ANPR systems obtained from local, state, and federal agencies through freedom of information laws. \"The documents paint a startling picture of a technology deployed with too few rules that is becoming a tool for mass routine location tracking and surveillance\" wrote the ACLU. The ACLU reported that in many locations the devices were being used to store location information on vehicles which were not suspected of any particular offense. \"Private companies are also using license plate readers and sharing the information they collect with police with little or no oversight or privacy protections. A lack of regulation means that policies governing how long our location data is kept vary widely,\" the ACLU said. In 2012 the ACLU filed suit against the Department of Homeland Security, which funds many local and state ANPR programs through grants, after the agency failed to provide access to records the ACLU had requested under the Freedom of Information Act about the programs.\n\nMany ANPR systems claim accuracy when trained to match plates from a single jurisdiction or region, but can fail when trying to recognize plates from other jurisdictions due to variations in format, font, color, layout, and other plate features. Some jurisdictions offer vanity or affinity plates (particularly in the US), which can create many variations within a single jurisdiction.\n\nFrom time to time, US states will make significant changes in their license plate protocol that will affect OCR accuracy. They may add a character or add a new license plate design. ALPR systems must adapt to these changes quickly in order to be effective. Another challenge with ALPR systems is that some states have the same license plate protocol. For example, more than one state uses the standard three letters followed by four numbers. So each time the ALPR systems alarms, it is the user’s responsibility to make sure that the plate which caused the alarm matches the state associated with the license plate listed on the in-car computer. For maximum effectiveness, an ANPR system should be able to recognize plates from any jurisdiction, and the jurisdiction to which they are associated, but these many variables make such tasks difficult.\n\nCurrently at least one US ANPR provider (PlateSmart) claims their system has been independently reviewed as able to accurately recognize the US state jurisdiction of license plates, and one European ANPR provider claims their system can differentiate all EU plate jurisdictions.\n\nA 2008 article in \"Parking Trend International\" discussed a disparity in claimed vs. experienced license plate recognition read rates, with manufacturers claiming that their recognition engines can correctly report 98% of the time, although customers experience only 90% to 94% success, even with new equipment under perfect conditions. Early systems were reportedly only 60% to 80% reliable.\n\nTrue system error rate is the product of its subsystem error rates (image capture, license plate image extraction, LP image interpretation); slight increases in subsystem error rates can produce dramatic reductions of read rates. The effects of real-world interfering factors on read rate are not uniformly specified or tested by manufacturers. The article states \"there is a need for the industry to adopt a standard performance measurement protocol to enable potential customers assess the best fit for their particular requirements.\"\n\n\n\nLicense Plate Recognition Costs\n",
    "id": "1657551",
    "title": "Automatic number-plate recognition"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=20638403",
    "text": "FatKat (investment software)\n\nFatKat, Inc. is a privately held company founded in 1999 by Raymond C. Kurzweil, an author, inventor, and futurist. He’s perhaps best known for creating an optical character recognition system that – in conjunction with a flatbed scanner and text-to-speech synthesizer – reads text aloud to the sight-impaired. FatKat is an acronym derived from \"financial accelerating transactions from Kurzweil Adaptive Technologies\". The aforesaid company is one of a total of nine Kurzweil companies.\n\nThe purpose of FatKat as listed with the Massachusetts Secretary of the Commonwealth Corporations Division is \"investment software\". Kurzweil, who specializes in artificial intelligence coupled with pattern recognition, has created software that uses quantitative methods to pick stocks for investment purposes.\n\nAlthough selecting stocks based on software-generated recommendations is not new, FatKat’s approach was unique at the time because of its “nonlinear decision making processes more akin to how a brain operates.” In layman's terms, the software can evolve by creating different rules, letting them compete, and using (or combining) the best outcomes. After FatKat’s inception, other investment and/or software companies rushed to develop software based on this and similar Darwinist evolutionary principles, using genetic algorithms.\n\nIn 2005, Kurzweil reported that the FatKat software was \"doing very well – 50% to 100% returns for the last two years\". But as of December 2008, FatKat does not offer its software for sale.\n\nFatKat was registered as a foreign corporation in 1999 with the Massachusetts Secretary of the Commonwealth, Corporations Division. It was originally formed as a company in the state of Delaware. Ray Kurzweil is the president of FatKat, with Aaron Kleiner serving as treasurer and secretary. Michael Brown is listed as a director.\n\nTwo hedge funds exist that use the FatKat name: FatKat Investment Fund, LP and FatKat QP Investment Fund, LP. Both of these investment fund companies list Kurzweil Capital Partners LLC as a general partner. These companies were formed in December 2005, also in Delaware. Neither of the hedge funds is publicly traded. Kurzweil Capital Partners LLC and the two hedge funds are not listed on the Kurzweil companies' web site.\n\nDocumented investors in FatKat, Inc. and its hedge funds are venture capitalist Vinod Khosla and Michael W. Brown (former CFO of Microsoft and chairman of NASDAQ). Other investors have not been disclosed.\n\nKurzweil Technologies (with links to related companies)\n",
    "id": "20638403",
    "title": "FatKat (investment software)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=24175540",
    "text": "WebCrow\n\nThe WebCrow is a research project carried out at the Information Engineering of the University of Siena with the purpose of automatically solving crosswords.\n\nThe scientific relevance of the project can be understood considering that cracking crosswords requires human-level knowledge. Unlike chess and related games and there is no closed world configuration space. Interestingly, a first nucleus of technology, such as search engines, information retrieval, and machine learning techniques enable computers to enfold with semantics real-life concepts. The project is based on a software system whose major assumption is to attack crosswords making use of the Web as its primary source of knowledge.\n\nWebCrow is very fast and often thrashes human challengers in competitions, especially on multi language crossword schemes. A distinct feature of the WebCrow software system is to combine properly natural language processing (NLP) techniques, the Google web search engine, and constraint satisfaction algorithms from artificial intelligence to acquire knowledge and to fill the schema. The most important component of WebCrow is the Web Search Module (WSM), which implements a domain specific web based question answering algorithm.\n\nThe way WebCrow approaches crosswords solving is quite with respect to humans: Whereas we tend to first answer clues we are sure of and then proceed filling the schema by exploiting the already answered clues as hints, WebCrow uses two clearly distinct stages. In the first one, it processes all the clues and tries to answer them all: For each clue it finds many possible candidates and sorts them according to complex ranking models mainly based on a probability criteria. In the second stage, WebCrow uses constraint satisfaction algorithms to fill the grid with the overall most likely combination of clue answers.\n\nIn order to interact with Google, first of all, WebCrow needs to compose queries on the basis of the given clues. This is done by query expansion, whose purpose is to convert the clue into a query expressed by a simplified and more appropriate language for Google. The retrieved documents are parsed so as to extract a list of word candidates that are congruent with the crossword length constraints. Crosswords can hardly be faced by using encyclopedic knowledge only, since many clues are wordplays or are otherwise purposefully very ambiguous. This enigmatic component of crosswords is faced by a massive use of database of solved crosswords, and by automatic reasoning on a properly organized knowledge base of wired rules. Last but not the least, the final constraint satisfaction step is very effective to fill the correct candidate, even though, unlike humans, the system can not rely on very high confidence on the correctness of the answer.\n\nWebCrow speed and effectiveness has been tested many times in man-machine competitions on Italian, English and multi-language crosswords\nThe outcome of the tests is that WebCrow can successfully compete with average human players on single language schemes and reaches expert level performance in multi-language crosswords. However, WebCrow has not reached expert level in single-language crosswords, yet.\n\nOn August 30, 2006, at the European Conference on Artificial Intelligence (ECAI2006), 25 conference attendees and 53 internet connected crosswords lovers, competed with WebCrow in an official challenge organized within the conference program. The challenge consisted in 5 different crosswords (2 in Italian, 2 in English and one multi-language in Italian and English) and 15 minutes were assigned for each crossword. WebCrow ranked 21 out of 74 participants in the Italian competition, and won both the bilingual and English competitions.\n\nSeveral competitions have been held in Florence, Italy within the Creativity Festival in December 2006, and another official conference competition took place in Hyderabad, India in January 2007, within the International Conference of Artificial Intelligence, where it ranked second out of 25 participants.\n\n",
    "id": "24175540",
    "title": "WebCrow"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1191600",
    "text": "Document processing\n\nDocument processing involves the conversion of typed and handwritten text on paper-based & electronic documents (e.g., scanned image of a document) into electronic information utilising one of, or a combination of, intelligent character recognition (ICR), optical character recognition (OCR) and experienced data entry clerks.\n\n",
    "id": "1191600",
    "title": "Document processing"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=4960888",
    "text": "Optical answer sheet\n\nAn optical answer sheet or \"bubble sheet\" is a special type of form used in multiple choice question examinations. Optical mark recognition is used to detect answers. The most well known company in the United States involved with optical answer sheets is the Scantron Corporation, although certain uses require their own customized system. The terms \"Optical answer sheet\" and \"scantron\" have become more or less interchangeable.\n\nOptical answer sheets usually have a set of blank ovals or boxes that correspond to each question, often on separate sheets of paper. Bar codes may mark the sheet for automatic processing, and each series of ovals filled will return a certain value when read. In this way students' answers can be digitally recorded, or identity given.\n\nThe first optical answer sheets were read by shining a light through the sheet and measuring how much of the light was blocked using phototubes on the opposite side. As some phototubes are mostly sensitive to the blue end of the visible spectrum, blue pens could not be used, as blue inks reflect and transmit blue light. Because of this, number two pencils had to be used to fill in the bubbles—graphite is a very opaque substance which absorbs or reflects most of the light which hits it.\n\nModern optical answer sheets are read based on reflected light, measuring lightness and darkness. They do not need to be filled in with a number two pencil, though these are recommended over other types due to the lighter marks made by higher-number pencils, and the smudges from number 1 pencils. Black ink will be read, though many systems will ignore marks that are the same color the form is printed in. This also allows optical answer sheets to be double-sided, because marks made on the opposite side will not interfere with reflectance readings as much as with opacity readings.\n\nMost systems accommodate for human error in filling in ovals imprecisely, as long as they do not stray into the other ovals and the oval is almost completely filled\n\nIt is possible for optical answer sheets to be printed incorrectly, such that all ovals will be read as filled. This occurs if the outline of the ovals is too thick, or is irregular. During the 2008 U.S. presidential election, this occurred with over 19,000 absentee ballots in the Georgia county of Gwinnett, and was discovered after around 10,000 had already been returned. The slight difference was not apparent to the naked eye, and was not detected until a test run was made in late October. This required all ballots to be transferred to correctly printed ones, by sequestered workers of the board of elections, under close observation by members of the Democratic and Republican (but not other) political parties, and county sheriff deputies. The transfer, by law, could not occur until election day (November 4).\n",
    "id": "4960888",
    "title": "Optical answer sheet"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=26603942",
    "text": "Silent speech interface\n\nSilent speech interface is a device that allows speech communication without using the sound made when people vocalize their speech sounds. As such it is a type of electronic lip reading. It works by the computer identifying the phonemes that an individual pronounces from nonauditory sources of information about their speech movements. These are then used to recreate the speech using speech synthesis.\n\nSilent speech interface systems have been created using ultrasound and optical camera input of tongue and lip movements. Electromagnetic devices are another technique for tracking tongue and lip movements. The detection of speech movements by electromyography of speech articulator\nmuscles and the larynx is another technique. Another source of information is the vocal tract resonance signals that get transmitted through bone conduction called non-audible murmurs. \nThey have also been created as a brain–computer interface using brain activity in the motor cortex obtained from intracortical microelectrodes.\n\nSuch devices are created as aids to those unable to create the sound phonation needed for audible speech such as after laryngectomies. Another use is for communication when speech is masked by background noise or distorted by self-contained breathing apparatus. A further practical use is where a need exists for silent communication, such as when privacy is required in a public place, or hands-free data silent transmission is needed during a military or security operation.\n\nIn 2002, the Japanese company NTT DoCoMo announced it had created a silent mobile phone using electromyography and imaging of lip movement. \"The spur to developing such a phone,\" the company said, \"was ridding public places of noise,\" adding that, \"the technology is also expected to help people who have permanently lost their voice.\" The feasibility of using silent speech interfaces for practical communication has since then been shown.\n\nThe decoding of silent speech using a computer played an important role in Arthur C. Clarke's story and Stanley Kubrick's associated film \"\". In this, HAL 9000, a computer controlling spaceship Discovery One, bound for Jupiter, discovers a plot to deactivate it by the mission astronauts Dave Bowman and Frank Poole through lip reading their conversations.\n\nIn Orson Scott Card’s series (including \"Ender’s Game\"), the artificial intelligence can be spoken to while the protagonist wears a movement sensor in his jaw, enabling him to converse with the AI without making noise. He also wears an ear implant.\n\n",
    "id": "26603942",
    "title": "Silent speech interface"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=4903304",
    "text": "Eccky\n\nEccky is an online game. Until 2009, it was an MSN-based life simulation game in which two people work together to create and raise a virtual baby. \"Eccky\" won the 2005 SpinAwards for Innovation and for Best Interactive Concept. In 2009, the game play changed to a real-time virtual world on Hyves.\n\n\"Eccky\" was created in August 2005 by Dutch developer Media Republic in association with MSN in the Netherlands. \"Eccky\" has characteristics of life simulation and virtual pet games. The gameplay of the first version of \"Eccky\" involved a virtual baby, or Eccky, which was born on the basis of information derived from both \"Eccky\" user players. \"Eccky\" used an AIML chatbot and MSN Messenger for chat between users and the Eccky baby. In 2006, \"Eccky\" became an independent company as a subsidiary of Media Republic.\n\nFrom its live introduction to the public until August 2006, \"Eccky\" required an initial fee which could be paid by either user player. However \"Eccky\" has been free to play since September 2006.\n\nIn the first version of \"Eccky\", two users create a virtual baby, and raise him/her with the goal of making the child as happy and satisfied as possible. A user fills out a questionnaire with information regarding their personal characteristics, child-rearing attitudes, favorites, etc. Once registration and the \"DNA\" test have been completed, a user may invite another person to make an Eccky. Upon accepting the invitation, the second user also registers on the Eccky website, and completes his/her own \"DNA\" test. Both players then choose both a masculine and feminine name for their future Eccky. Thereafter Eccky is born, with characteristics determined by the combination of both users' DNA profiles. Eccky's sex is randomly determined, as is Eccky’s name (which is chosen randomly between the four possible names chosen by the user parents).\n\nOver a six-day period from Eccky’s birth, Eccky grows and ages three years for every one day of gameplay. Thus, in six days, Eccky develops from a cooing baby into an eighteen-year-old young adult with its own character. Every Eccky is unique at birth, and the way in which the users raise their Eccky further individualizes Eccky’s demeanor and characteristics. On the sixth day, or upon turning 18, Eccky leaves the house to venture off into the wide world, and the game ends.\n\n\"Eccky\" played principally via the Eccky website and MSN Messenger, though also via mobile phone. Upon Eccky’s birth, Eccky is automatically added as a contact to the MSN contact lists of both players. This allows users to talk to their virtual child. Running on an AIML chat engine, Eccky is able to speak from the moment following birth, both in response to users addressing Eccky and by initiating conversations him/herself. Eccky initiates conversation either randomly or to express any particularly pressing need he/she may have (i.e. being extremely hungry, having to go to the bathroom, being very sick, feeling neglected, etc.) Eccky’s vocabulary is initially limited to newborn babble. Then, as with a real child, Eccky’s command of vocabulary grows with each day of gameplay, to a final capability of being able to respond with over 60,000 unique answers on more than 4,000 diverse subjects. Since June 2007, Eccky also features an in-game chat functionality.\n\nEccky and users can also exchange text messages via mobile phone. Exchange of text messages requires that the user have a mobile phone, and that the user purchase a mobile phone for Eccky within the game environment. This feature involves extra costs to the user, specifically the cost of sending text messages, and can be turned on and off at the user's discretion.\n\nMost games are intended to be played with users playing against their Eccky and vice versa. Others are played in cooperation with Eccky. Of note are the racing games, some of which allow users to create their own tracks and race on them with Eccky. Though most games are free to play, some games require additional credits to play.\n\nEccky’s physical and emotional states are subject to continual change, determined by 180 dynamic variables, and are influenced by the interaction with and treatment by users, both via the virtual world and via the chat. The levels of these physical and emotional states are continually assessed and are made visible to users via meters that gauge happiness, hunger, toilet needs, popularity, who Eccky’s favorite parent is, etc. For example, if Eccky is not sufficiently fed, users will see this in Eccky’s hunger meter. In worst-case scenarios, an Eccky can be neglected, physical and emotionally, to the point of needing to be sent to the hospital. During this time, gameplay continues and users may chat with Eccky, but may not interact with Eccky in the virtual world. Further, Eccky’s physical and emotional states are reflected in the chat element, as Eccky will either comment, unsolicited, on how he/she is doing physically and emotionally, or respond to a user in such a way as to make his/her physical and emotional state known.\n\nIt is also possible to send Eccky away for a period, either to live in other accommodations such as in a hotel, on vacation, or to stay with a babysitter. A babysitter may be anyone with an MSN Messenger account. Sending Eccky to a hotel or on vacation does involve additional costs (sending Eccky to a babysitter does not), and as with Eccky staying in the hospital, users may continue to speak with Eccky via chat but cannot interact with Eccky in the virtual world setting during this time. A user can retrieve their Eccky at any time during a stay away from home. Eccky was closed down in 2007 and until it was launched in a new format in 2009.\n\nIn 2009 Dutch/Chinese developer TribePlay created a new version of \"Eccky\" and launched it in Hyves as one of the first social networking virtual worlds. In \"Eccky\" players can make a character (an Eccky) with their social network profile. After that they enter the Eccky world. In this world they have access to various locations, such as a city center, park, mountain top and beach. Players can also visit their own or other users' houses. In any of these virtual locations players can chat, play mini games and do a variety of other things.\n\n\"Eccky\" is integrated into Hyves and Facebook. Together with their Hyves or Facebook friends players can chat, play mini games and post their Eccky's activities on their social networking profile. \"Eccky\" has many games. Games can be played in single-player or multi-player format. Ecckies can also play with their Wobble. A Wobble is Eccky’s own pet. Wobbles were the first inhabitants of the Eccky world. Wobbles live together with Ecckies and need to be taken care of. Once Ecckies advance in levels they receive superpowers to be used in the game.\n\n",
    "id": "4903304",
    "title": "Eccky"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=7271261",
    "text": "Statistical semantics\n\nIn linguistics, statistical semantics applies the methods of statistics to the problem of determining the meaning of words or phrases, ideally through unsupervised learning, to a degree of precision at least sufficient for the purpose of information retrieval. \n\nThe term \"statistical semantics\" was first used by Warren Weaver in his well-known paper on machine translation. He argued that word sense disambiguation for machine translation should be based on the co-occurrence frequency of the context words near a given target word. The underlying assumption that \"a word is characterized by the company it keeps\" was advocated by J.R. Firth. This assumption is known in linguistics as the distributional hypothesis. Emile Delavenay defined \"statistical semantics\" as the \"statistical study of meanings of words and their frequency and order of recurrence\". \"Furnas et al. 1983\" is frequently cited as a foundational contribution to statistical semantics. An early success in the field was latent semantic analysis.\n\nResearch in statistical semantics has resulted in a wide variety of algorithms that use the distributional hypothesis to discover many aspects of semantics, by applying statistical techniques to large corpora:\n\nStatistical semantics focuses on the meanings of common words and the relations between common words, unlike text mining, which tends to focus on whole documents, document collections, or named entities (names of people, places, and organizations). Statistical semantics is a subfield of computational semantics, which is in turn a subfield of computational linguistics and natural language processing.\n\nMany of the applications of statistical semantics (listed above) can also be addressed by lexicon-based algorithms, instead of the corpus-based algorithms of statistical semantics. One advantage of corpus-based algorithms is that they are typically not as labour-intensive as lexicon-based algorithms. Another advantage is that they are usually easier to adapt to new languages than lexicon-based algorithms. However, the best performance on an application is often achieved by combining the two approaches.\n\n",
    "id": "7271261",
    "title": "Statistical semantics"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1741558",
    "text": "SCIgen\n\nSCIgen is a computer program that uses context-free grammar to randomly generate nonsense in the form of computer science research papers. All elements of the papers are formed, including graphs, diagrams, and citations. Created by scientists at the Massachusetts Institute of Technology, its stated aim is \"to maximize amusement, rather than coherence.\"\n\nOpening abstract of \"Rooter: A Methodology for the Typical Unification of Access Points and Redundancy\":\nIn 2005 a paper generated by SCIgen, \"Rooter: A Methodology for the Typical Unification of Access Points and Redundancy\", was accepted as a non-reviewed paper to the 2005 World Multiconference on Systemics, Cybernetics and Informatics (WMSCI) and the authors were invited to speak. The authors of SCIgen described their hoax on their website, and it soon received great publicity when picked up by Slashdot.\nWMSCI withdrew their invitation, but the SCIgen team went anyway, renting space in the hotel separately from the conference and delivering a series of randomly generated talks on their own \"track.\" The organizer of these WMSCI conferences is Professor Nagib Callaos. From 2000 until 2005, the WMSCI was also sponsored by the Institute of Electrical and Electronics Engineers. The IEEE stopped granting sponsorship to Callaos from 2006 to 2008.\nSubmitting the paper was a deliberate attempt to embarrass WMSCI, which the authors claim accepts low-quality papers and sends unsolicited requests for submissions in bulk to academics. As the SCIgen website states:\n\nComputing writer Stan Kelly-Bootle noted in \"ACM Queue\" that many sentences in the \"Rooter\" paper were individually plausible, which he regarded as posing a problem for automated detection of hoax articles. He suggested that even human readers might be taken in by the effective use of jargon (\"The pun on root/router is par for MIT-graduate humor, and at least one occurrence of methodology is mandatory\") and attribute the paper's apparent incoherence to their own limited knowledge. His conclusion was that \"a reliable gibberish filter requires a careful holistic review by several peer domain experts\".\n\nThe name of a fictional man named \"Herbert Schlangemann\" was used to publish false scientific articles in international conferences that are suspected to be, at least partially, frauds. The author is named after the Swedish short film \"Der Schlangemann\".\n\nIn all cases, the published papers were withdrawn from the conferences' proceedings, and the conference organizing committee as well as the names of the keynote speakers were removed from their websites.\n\n\n\nRefereeing performed on behalf of the Institute of Electrical and Electronics Engineers has also been subject to criticism after fake papers were discovered in conference publications, most notably by Labbé and a researcher using the pseudonym of Schlangemann.\n\nIn this 2010 paper by Cyril Labbé from Grenoble University demonstrated the vulnerability of h-index calculations based on Google Scholar output by feeding it a large set of SCIgen-generated documents that were citing each other, effectively an academic link farm. Using this method the author managed to rank \"Ike Antkare\" ahead of Albert Einstein for instance.\n\n\n",
    "id": "1741558",
    "title": "SCIgen"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=25910473",
    "text": "AForge.NET\n\nAForge.NET is a computer vision and artificial intelligence library originally developed by Andrew Kirillov for the .NET Framework.\n\nThe source code and binaries of the project are available under the terms of the Lesser GPL and the GPL (GNU General Public License).\n\nAnother (unaffiliated) project called Accord.NET was created to extend the features of the original AForge.NET library.\n\nOn April 1, 2012, Andrew Kirillov announced the end of the public support for the library, temporarily closing the discussion forums. The last release of the AForge.NET Framework was made available on July 17, 2013. However, since its release 3.0 in 2015, the Accord.NET project started to incorporate most of the original AForge.NET source code in its codebase, continuing its support and development under the Accord.NET name.\n\nThe framework's API includes support for:\n\nComplete list of features is available on the features page of the project.\n\nThe framework is provided not only with different libraries and their sources, but with many sample applications, which demonstrate the use of this framework, and with documentation help files, which are provided in HTML Help format. The documentation is also available on-line.\n\n",
    "id": "25910473",
    "title": "AForge.NET"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=28539183",
    "text": "EuResist\n\nEuResist is an international project designed to improve the treatment of HIV patients by developing a computerized system that can recommend optimal treatment based on the patient’s clinical and genomic data.\n\nThe project is part of the Virtual Physiological Human framework, funded by the European Commission. It started in 2006 with the formation of a consortium of several research institutes and hospitals in Europe and Israel. The consortium completed its commitment to the European Commission near the end of 2008, at which time the system became available online. A non-profit organization was consequently established by the main partners to maintain and improve the system.\n\nIn 2009, the EuResist project was named as a Computerworld honors program laureate.\n\nAIDS is a disease caused by the HIV retrovirus, which progressively reduces the effectiveness of the immune system, leading to infections and ultimately death.\n\nMore than 30 different drugs exist for treating HIV patients. Antiretroviral drugs can disrupt the virus’s replication process causing its numbers to decrease dramatically. While the virus cannot be eradicated completely, in small numbers it is harmless. Usually a patient is given a combination of three or four drugs, a treatment known as highly active antiretroviral therapy, or HAART. The main reason such a treatment might fail is the development of mutated strands of the virus, resistant to one or more of the prescribed drugs.\n\nThus an important consideration when choosing treatment for a patient is to prescribe those drugs to which the particular patient’s virus strands are most susceptible. One way to achieve that is to extract virus samples from the patient’s blood and test them against all possible drugs. Since this process is lengthy and costly, computerized systems have been developed to predict virus resistance based on its genotype. The treating physician samples virus genotype sequences from the patient’s blood and provides this data to a computerized system. The system then responds with drug recommendations.\n\nSuch systems are limited in accuracy, depending on the amount of data used for their creation, its quality and the richness of mathematical models used for the actual prediction. Prior to EuResist, such systems had several common characteristics that negatively impacted their accuracy:\n\nEuResist sought to create a more accurate HIV treatment prediction system by collecting a large database of in vivo data (clinical and genomic records of real treatments of HIV patients and their consequences), and by using an array of prediction models instead of just one.\nThe database was created by merging local databases of various clinics across Europe. This database is thought to be the largest of its kind in the world.\nFor each patient, it includes various personal and demographic details such as gender, age, country of origin, genomic sequencing of HIV found in the patient’s blood, records of the drugs prescribed, and the changes in the amount of virus in the blood following these treatments.\n\nThis data was used to train an array of prediction models, created by using various contemporary machine learning techniques, among them Bayesian networks, logistic regression, and others.\n\nA web interface allows physicians to specify patients' clinical and genomic data. This data is sent to the prediction engines, and the combined response, which is displayed to the physician, includes various suggested treatments and a prediction of their effect on the amount of HIV in the blood.\n\nThe EuResist system was tested and compared with its predecessors by feeding it with historical data on patients for which treatment results are known. The developers of EuResist, who conducted this test, reported an improved performance over the previous state-of-the-art system.\n\nEuResist started in 2006 as a consortium funded by the European Union as part of the Virtual Physiological Human FP-6 framework. The partners of this consortium were:\nThe consortium completed its commitment to the European Union in late 2008, at which time the EuResist system became available on line.\nThe first five partners mentioned above continued to form a non-profit organization that maintains the system, expands the database with new clinical and genomical records and updates the prediction engines accordingly. As of mid-2010, an average of 600 queries are submitted to the EuResist system every quarter.\n\nOn June 1, 2009, EuResist received a Computerworld honors program laureate award, a global program honoring individuals and organizations that use information technology to benefit society.\n\n",
    "id": "28539183",
    "title": "EuResist"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=6898858",
    "text": "Concept mining\n\nConcept mining is an activity that results in the extraction of concepts from artifacts. Solutions to the task typically involve aspects of artificial intelligence and statistics, such as data mining and text mining. Because artifacts are typically a loosely structured sequence of words and other symbols (rather than concepts), the problem is nontrivial, but it can provide powerful insights into the meaning, provenance and similarity of documents.\n\nTraditionally, the conversion of words to concepts has been performed using a thesaurus, and for computational techniques the tendency is to do the same. The thesauri used are either specially created for the task, or a pre-existing language model, usually related to Princeton's WordNet.\n\nThe mappings of words to concepts are often ambiguous. Typically each word in a given language will relate to several possible concepts. Humans use context to disambiguate the various meanings of a given piece of text, where available machine translation systems cannot easily infer context.\n\nFor the purposes of concept mining however, these ambiguities tend to be less important than they are with machine translation, for in large documents the ambiguities tend to even out, much as is the case with text mining.\n\nThere are many techniques for disambiguation that may be used. Examples are linguistic analysis of the text and the use of word and concept association frequency information that may be inferred from large text corpora. Recently, techniques that base on semantic similarity between the possible concepts and the context have appeared and gained interest in the scientific community.\n\nOne of the spin-offs of calculating document statistics in the concept domain, rather than the word domain, is that concepts form natural tree structures based on hypernymy and meronymy. These structures can be used to produce simple tree membership statistics, that can be used to locate any document in a Euclidean concept space. If the size of a document is also considered as another dimension of this space then an extremely efficient indexing system can be created. This technique is currently in commercial use locating similar legal documents in a 2.5 million document corpus.\n\nStandard numeric clustering techniques may be used in \"concept space\" as described above to locate and index documents by the inferred topic. These are numerically far more efficient than their text mining cousins, and tend to behave more intuitively, in that they map better to the similarity measures a human would generate.\n\n",
    "id": "6898858",
    "title": "Concept mining"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=23392007",
    "text": "Document capture software\n\nDocument Capture Software refers to applications that provide the ability and feature set to automate the process of scanning paper documents. Most scanning hardware, both scanners and copiers, provides the basic ability to scan to any number of image file formats, including: PDF, TIFF, JPG, BMP, etc. This basic functionality is augmented by document capture software, which can add efficiency and standardization to the process.\n\nTypical features of Document Capture Software include:\n\nThe goal for implementing a document capture solution is to reduce the amount of time spent in the scanning and capture process, and produce metadata along with an image file, and/or OCR text. This information is then migrated to a Document Management or Enterprise Content Management system. These systems often provide a search function, allowing search of the assets based on the produced metadata, and then viewed using document imaging software.\n\nECM (Enterprise Content management) and their DMS component (Document Management System) are being adopted by many organizations as a corporate document management system for all types of electronic files, e.g. MS word, PDF ... However, much of the information held by organisations is on paper and this needs to be integrated within the same document repository.\n\nBy converting paper documents into digital format through scanning companies can convert paper into image formats such as TIF and JPG and also extract valuable index information or business data from the document using OCR technology. Digital documents and associated metadata can easily be stored in the ECM in a variety of formats. The most popular of these formats is PDF which not only provides an accurate representation of the document but also allows all the OCR text in the document to be stored behind the PDF image. This format is known as PDF with hidden text or text-searchable PDF. This allows users to search for documents by using keywords in the metadata fields or by searching the content of PDF files across the repository.\n\nInformation held on paper is usually just as valuable to organisations as the electronic documents that are generated internally. Often this information represents a large proportion of the day to day correspondence with suppliers and customers. Having the ability to manage and share this information internally through a document management system such as SharePoint can improve collaboration between departments or employees and also eliminate the risk of losing this information through disasters such as floods or fire.\n\nOrganisations adopting an ECM/DMS often implement electronic workflow which allows the information held on paper to be included as part of an electronic business process and incorporated into a customer record file along with other associated office documents and emails.\nFor business critical documents, such as purchase orders and supplier invoices, digitising documents can help speed up business transactions as well as reduce manual effort involved in keying data into business systems, such as CRM, ERP and Accounting. Scanned invoices can also be routed to managers for payment approval via email or an electronic workflow.\n\nDistributed document capture is a technology which allows the scanning of documents into a central server through the use of individual capture stations. A variation of distributed capture is thin-client document capture in which documents are scanned into a central server through the use of web browser. One of these web-based products was reviewed by AIIM. They said, \"(this product) is a thin-client distributed capture system that streamlines the process of acquiring and creating documents.\" The streamlining is a result of several factors including the lack of software which needs to be installed at every scanning station and the variety of input sources from which documents can be captured. This includes things like email, fax, or a watched folder.\n\nJeff Shuey, Director of Business Development at Kodak, makes a distinction between distributed capture and what he calls \"remote\" capture. In an article publishing in AIIM, he said that the key difference between the two is whether or not the information that is captured from scanning needs to be sent to the centralized server. If, as he points out in his article, the document just needs to be scanned and committed to a SharePoint system and doesn't need to be sent to some other centralized server, this is just a remote capture situation.\n\nThere are Document Capture Software comparisons available, featuring some of the most relevant products (EMC Captiva, IBM Datacap, Artsyl Technologies or Ephesoft) and extracting performance facts and their most relevant features.\n",
    "id": "23392007",
    "title": "Document capture software"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=2368154",
    "text": "Clinical decision support system\n\nA clinical decision support system (CDSS) is a health information technology system that is designed to provide physicians and other health professionals with clinical decision support (CDS), that is, assistance with clinical decision-making tasks. A working definition has been proposed by Robert Hayward of the Centre for Health Evidence: \"Clinical decision support systems link health observations with health knowledge to influence health choices by clinicians for improved health care\". CDSSs constitute a major topic in artificial intelligence in medicine.\n\nThe evidence of the effectiveness of CDSS is mixed. A 2014 systematic review did not find a benefit in terms of risk of death when the CDSS was combined with the electronic health record. There may be some benefits, however, in terms of other outcomes.\n\nA 2005 systematic review concluded that CDSSs improved practitioner performance in 64% of the studies. The CDSSs improved patient outcomes in 13% of the studies. Sustainable CDSSs features associated with improved practitioner performance include the following:\nBoth the number and the methodological quality of studies of CDSSs increased from 1973 through 2004.\n\nAnother 2005 systematic review found... \"\"Decision support systems significantly improved clinical practice in 68% of trials.\"\" The CDSS features associated with success include the following:\n\nHowever, other systematic reviews are less optimistic about the effects of CDS, with one from 2011 stating \"\"There is a large gap between the postulated and empirically demonstrated benefits of [CDSS and other] eHealth technologies ... their cost-effectiveness has yet to be demonstrated\"\".\n\nA 5-year evaluation of the effectiveness of a CDSS in implementing rational treatment of bacterial infections was published in 2014; according to the authors, it was the first long term study of a CDSS.\n\nA clinical decision support system has been defined as an \"active knowledge systems, which use two or more items of patient data to generate case-specific advice.\" This implies that a CDSS is simply a decision support system that is focused on using knowledge management in such a way so as to achieve clinical advice for patient care based on multiple items of patient data.\n\nThe main purpose of modern CDSS is to assist clinicians at the point of care. This means that clinicians interact with a CDSS to help to analyse, and reach a diagnosis based on, patient data.\n\nIn the early days, CDSSs were conceived of as being used to literally make decisions for the clinician. The clinician would input the information and wait for the CDSS to output the \"right\" choice and the clinician would simply act on that output. However, the modern methodology of using CDSSs to assist means that the clinician interacts with the CDSS, utilizing both their own knowledge and the CDSS, to make a better analysis of the patient's data than either human or CDSS could make on their own. Typically, a CDSS makes suggestions for the clinician to look through, and the clinician is expected to pick out useful information from the presented results and discount erroneous CDSS suggestions.\n\nThere are two main types of CDSS:\nas detailed below.\n\nAn example of how a clinical decision support system might be used by a clinician is a specific type of CDSS, a DDSS (diagnosis decision support systems). A DDSS requests some of the patients data and in response, proposes a set of appropriate diagnoses. The doctor then takes the output of the DDSS and determines which diagnoses might be relevant and which are not, and if necessary orders further tests to narrow down the diagnosis.\n\nAnother example of a CDSS would be a case-based reasoning (CBR) system. A CBR system might use previous case data to help determine the appropriate amount of beams and the optimal beam angles for use in radiotherapy for brain cancer patients; medical physicists and oncologists would then review the recommended treatment plan to determine its viability.\n\nAnother important classification of a CDSS is based on the timing of its use. Doctors use these systems at point of care to help them as they are dealing with a patient, with the timing of use being either pre-diagnosis, during diagnosis, or post diagnosis. Pre-diagnosis CDSS systems are used to help the physician prepare the diagnoses. CDSS used during diagnosis help review and filter the physician's preliminary diagnostic choices to improve their final results. Post-diagnosis CDSS systems are used to mine data to derive connections between patients and their past medical history and clinical research to predict future events. It has been claimed that decision support will begin to replace clinicians in common tasks in the future.\n\nAnother approach, used by the National Health Service in England, is to use a DDSS (either, in the past, operated by the patient, or, today, by a phone operative who is not medically-trained) to triage medical conditions out of hours by suggesting a suitable next step to the patient (e.g. call an ambulance, or see a general practitioner on the next working day). The suggestion, which may be disregarded by either the patient or the phone operative if common sense or caution suggests otherwise, is based on the known information and an implicit conclusion about what the \"worst-case\" diagnosis is likely to be (which is not always revealed to the patient, because it might well be incorrect and is not based on a medically-trained person's opinion - it is only used for initial triage purposes).\n\nMost CDSSs consist of three parts: the knowledge base, an inference engine, and a mechanism to communicate. The knowledge base contains the rules and associations of compiled data which most often take the form of IF-THEN rules. If this was a system for determining drug interactions, then a rule might be that IF drug X is taken AND drug Y is taken THEN alert user. Using another interface, an advanced user could edit the knowledge base to keep it up to date with new drugs. The inference engine combines the rules from the knowledge base with the patient's data. The communication mechanism allows the system to show the results to the user as well as have input into the system.\n\nCDSSs that do not use a knowledge base use a form of artificial intelligence called machine learning, which allow computers to learn from past experiences and/or find patterns in clinical data. This eliminates the need for writing rules and for expert input. However, since systems based on machine learning cannot \"explain\" the reasons for their conclusions (they are so-called \"black boxes\", because no meaningful information about how they work can be discerned by human inspection), most clinicians do not use them directly for diagnoses, for reliability and accountability reasons. Nevertheless, they can be useful as post-diagnostic systems, for suggesting patterns for clinicians to look into in more depth.\n\nThree types of non-knowledge-based systems are support vector machines, artificial neural networks and genetic algorithms.\n\n\nWith the enactment of the American Recovery and Reinvestment Act of 2009 (ARRA), there is a push for widespread adoption of health information technology through the Health Information Technology for Economic and Clinical Health Act (HITECH). Through these initiatives, more hospitals and clinics are integrating electronic medical records (EMRs) and computerized physician order entry (CPOE) within their health information processing and storage. Consequently, the Institute of Medicine (IOM) promoted usage of health information technology including clinical decision support systems to advance quality of patient care. The IOM had published a report in 1999, \"To Err is Human\", which focused on the patient safety crisis in the United States, pointing to the incredibly high number of deaths. This statistic attracted great attention to the quality of patient care.\n\nWith the enactment of the HITECH Act included in the ARRA, encouraging the adoption of health IT, more detailed case laws for CDSS and EMRs are still being defined by the Office of National Coordinator for Health Information Technology (ONC) and approved by Department of Health and Human Services (HHS). A definition of \"Meaningful use\" is yet to be published.\n\nDespite the absence of laws, the CDSS vendors would almost certainly be viewed as having a legal duty of care to both the patients who may adversely be affected due to CDSS usage and the clinicians who may use the technology for patient care. However, duties of care legal regulations are not explicitly defined yet.\n\nWith recent effective legislations related to performance shift payment incentives, CDSS are becoming more attractive.\n\nMuch effort has been put forth by many medical institutions and software companies to produce viable CDSSs to support all aspects of clinical tasks. However, with the complexity of clinical workflows and the demands on staff time high, care must be taken by the institution deploying the support system to ensure that the system becomes a fluid and integral part of the clinical workflow. Some CDSSs have met with varying amounts of success, while others have suffered from common problems preventing or reducing successful adoption and acceptance.\n\nTwo sectors of the healthcare domain in which CDSSs have had a large impact are the pharmacy and billing sectors. There are commonly used pharmacy and prescription ordering systems that now perform batch-based checking of orders for negative drug interactions and report warnings to the ordering professional. Another sector of success for CDSS is in billing and claims filing. Since many hospitals rely on Medicare reimbursements to stay in operation, systems have been created to help examine both a proposed treatment plan and the current rules of Medicare in order to suggest a plan that attempts to address both the care of the patient and the financial needs of the institution.\n\nOther CDSSs that are aimed at diagnostic tasks have found success, but are often very limited in deployment and scope. The Leeds Abdominal Pain System went operational in 1971 for the University of Leeds hospital, and was reported to have produced a correct diagnosis in 91.8% of cases, compared to the clinicians' success rate of 79.6%.\n\nDespite the wide range of efforts by institutions to produce and use these systems, widespread adoption and acceptance has still not yet been achieved for most offerings. One large roadblock to acceptance has historically been workflow integration. A tendency to focus only on the functional decision making core of the CDSS existed, causing a deficiency in planning for how the clinician will actually use the product in situ. Often CDSSs were stand-alone applications, requiring the clinician to cease working on their current system, switch to the CDSS, input the necessary data (even if it had already been inputted into another system), and examine the results produced. The additional steps break the flow from the clinician's perspective and cost precious time.\n\nClinical decision support systems face steep technical challenges in a number of areas. Biological systems are profoundly complicated, and a clinical decision may utilize an enormous range of potentially relevant data. For example, an electronic evidence-based medicine system may potentially consider a patient's symptoms, medical history, family history and genetics, as well as historical and geographical trends of disease occurrence, and published clinical data on medicinal effectiveness when recommending a patient's course of treatment.\n\nClinically, a large deterrent to CDSS acceptance is workflow integration, as mentioned above.\n\nAnother source of contention with many medical support systems is that they produce a massive number of alerts. When systems produce high volume of warnings (especially those that do not require escalation), aside from the annoyance, clinicians may pay less attention to warnings, causing potentially critical alerts to be missed.\n\nOne of the core challenges facing CDSS is difficulty in incorporating the extensive quantity of clinical research being published on an ongoing basis. In a given year, tens of thousands of clinical trials are published. Currently, each one of these studies must be manually read, evaluated for scientific legitimacy, and incorporated into the CDSS in an accurate way. In 2004, it was stated that the process of gathering clinical data and medical knowledge and putting them into a form that computers can manipulate to assist in clinical decision-support is \"still in its infancy\".\n\nNevertheless, it is more feasible for a business to do this centrally, even if incompletely, than for each individual doctor to try to keep up with all the research being published.\n\nIn addition to being laborious, integration of new data can sometimes be difficult to quantify or incorporate into the existing decision support schema, particularly in instances where different clinical papers may appear conflicting. Properly resolving these sorts of discrepancies is often the subject of clinical papers itself (see meta-analysis), which often take months to complete.\n\nIn order for a CDSS to offer value, it must demonstrably improve clinical workflow or outcome. Evaluation of CDSS is the process of quantifying its value to improve a system's quality and measure its effectiveness. Because different CDSSs serve different purposes, there is no generic metric which applies to all such systems; however, attributes such as consistency (with itself, and with experts) often apply across a wide spectrum of systems.\n\nThe evaluation benchmark for a CDSS depends on the system's goal: for example, a diagnostic decision support system may be rated based upon the consistency and accuracy of its classification of disease (as compared to physicians or other decision support systems). An evidence-based medicine system might be rated based upon a high incidence of patient improvement, or higher financial reimbursement for care providers.\n\nImplementing electronic health records (EHR) was an inevitable challenge. The reasons behind this challenge are that it is a relatively uncharted area, and there are many issues and complications during the implementation phase of an EHR. This can be seen in the numerous studies that have been undertaken. However, challenges in implementing electronic health records (EHRs) have received some attention, but less is known about the process of transitioning from legacy EHRs to newer systems.\n\nWith all of that said, electronic health records are the way of the future for healthcare industry. They are a way to capture and utilise real-time data to provide high-quality patient care, ensuring efficiency and effective use of time and resources. Incorporating EHR and CDSS together into the process of medicine has the potential to change the way medicine has been taught and practiced. It has been said that \"the highest level of EHR is a CDSS\".\n\nSince \"clinical decision support systems (CDSS) are computer systems designed to impact clinician decision making about individual patients at the point in time that these decisions are made\", it is clear that it would be beneficial to have a fully integrated CDSS and EHR.\n\nEven though the benefits can be seen, to fully implement a CDSS that is integrated with an EHR has historically required significant planning by the healthcare facility/organisation, in order for the purpose of the CDSS to be successful and effective. \nThe success and effectiveness can be measured by the increase in patient care being delivered and reduced adverse events occurring. In addition to this, there would be a saving of time and resources, and benefits in terms of autonomy and financial benefits to the healthcare facility/organisation.\n\nA successful CDSS/EHR integration will allow the provision of best practice, high quality care to the patient, which is the ultimate goal of healthcare.\n\nErrors have always occurred in healthcare, so trying to minimise them as much as possible is important in order to provide quality patient care. Three areas that can be addressed with the implementation of CDSS and Electronic Health Records (EHRs), are:\nCDSSs will be most beneficial in the future when healthcare facilities are \"100% electronic\" in terms of real-time patient information, thus simplifying the number of modifications that have to occur to ensure that all the systems are up to date with each other.\n\nThe measurable benefits of clinical decision support systems on physician performance and patient outcomes remain the subject of ongoing research, as noted in the section above.\n\nImplementing electronic health records (EHR) in healthcare settings incurs challenges; none more important than maintaining efficiency and safety during rollout, but in order for the implementation process to be effective, an understanding of the EHR users' perspectives is key to the success of EHR implementation projects. In addition to this, adoption needs to be actively fostered through a bottom-up, clinical-needs-first approach. The same can be said for CDSS.\n\nThe main areas of concern with moving into a fully integrated EHR/CDSS system are:\n\n\nA service oriented architecture has been proposed as a technical means to address some of these barriers.\n\nAs of July 2015, the planned transition to EHRs in Australia is facing difficulties. The majority of healthcare facilities are still running completely paper-based systems, and some are in a transition phase of scanned EHRs, or are moving towards such a transition phase.\n\nVictoria has attempted to implement EHR across the state with its HealthSMART program, but due to unexpectedly high costs it has cancelled the project.\n\nSouth Australia (SA) however is slightly more successful than Victoria in the implementation of an EHR. This may be due to all public healthcare organisations in SA being centrally run. (However, on the other hand, the UK's National Health Service is also centrally administered, and its National Programme for IT in the 2000s, which included EHRs in its remit, was an expensive disaster.)\n\nSA is in the process of implementing \"Enterprise patient administration system (EPAS)\". This system is the foundation for all public hospitals and health care sites for an EHR within SA and it was expected that by the end of 2014 all facilities in SA will be connected to it. This would allow for successful integration of CDSS into SA and increase the benefits of the EHR.\nBy July 2015 it was reported that only 3 out of 75 health care facilities implemented EPAS.\n\nWith the largest health system in the country and a federated rather than centrally administered model, New South Wales is making consistent progress towards statewide implementation of EHRs. The current iteration of the state's technology, eMR2, includes CDSS features such as a sepsis pathway for identifying at-risk patients based upon data input to the electronic record. As of June 2016, 93 of 194 sites in-scope for the initial roll-out had implemented eMR2\n\n\n",
    "id": "2368154",
    "title": "Clinical decision support system"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=29069615",
    "text": "Resistance Database Initiative\n\nHIV Resistance Response Database Initiative (RDI) is a not-for-profit organisation established in 2002 with the mission of improving the clinical management of HIV infection through the application of bioinformatics to HIV drug resistance and treatment outcome data. The RDI has the following specific goals:\n\n\nThe RDI consists of a small executive group based in the UK, an international advisory group of leading HIV/AIDS scientists and clinicians, and an extensive global network of collaborators and data contributors.\n\nHuman immunodeficiency virus (HIV) is the virus that causes acquired immunodeficiency syndrome (AIDS), a condition in which the immune system begins to fail, leading to life-threatening opportunistic infections.\n\nThere are approximately 25 HIV ‘antiretroviral’ drugs that have been approved for the treatment of HIV infection, from six different classes, based on the point in the HIV life-cycle at which they act.\n\nThey are used in combination; typically 3 or more drugs from 2 or more different classes, a form of therapy known as highly active antiretroviral therapy or HAART. The aim of therapy is suppression of the virus to very low, ideally undetectable, levels in the blood this prevents the virus from depleting the immune cells that it preferentially attacks (CD4 cells) and prevents or delays illness and death.\n\nDespite the expanding availability of these drugs and the impact of their use, treatments continue to fail, often due to the development of resistance. During drug therapy, low-level virus replication still occurs, particularly when a patient misses a dose. HIV makes errors in copying its genetic material and, if a mutation makes the virus resistant to one or more of the drugs, it may begin to replicate more successfully in the presence of that drug and undermine the effect of the treatment. If this happens then the treatment needs to be changed to re-establish control over the virus.\n\nIn well-resourced healthcare settings, when treatment fails a resistance test may be run to predict to which drugs the patient’s virus is resistant. The type of test in most common use is the genotype test, which detects mutations in the viral genetic code. This information is then typically interpreted using rules equating individual mutations with resistance against individual drugs. However, there are many different interpretation systems available that do not always agree, the systems only provide categorical results (resistant, sensitive or intermediate) and they do not necessarily relate well to how a patient will respond to a combination of drugs in the clinic.\n\nThe RDI was established in 2002 to pioneer a new approach: to develop computational models using the genotype and a wide range of other clinically relevant data collected from thousands of patients treated with HAART all over the world and to use these models to predict how an individual patient will respond to different combinations of drugs. The RDI’s goal was to make available a free treatment-response prediction tool over the Internet.\n\nKey to the success of this approach is the collection of large amounts of data with which to train the models and the use of data from as wide and heterogeneous range of sources as possible to maximise the generalisability of the models’ predictions. In order to achieve this, the RDI set out to involve as many clinics worldwide as possible and to be the single repository for the data required, in an attempt to avoid unnecessary duplication of effort and competition.\n\nAs of October 2013, the RDI has collected data from approximately 110,000 patients from dozens of clinics in more than 30 countries. It is probably the largest database of its kind in the world. The data includes demographic information for the patient, and multiple determinations of the amount of virus in the patient’s bloodstream, CD4 cells counts (a white blood cell critical to the function of the immune system that HIV targets and destroys), genetic code of the patients virus, and details of the drugs that have been used to treat the patient.\n\nThe RDI has used these data to conduct extensive research in order to develop the most accurate system possible for the prediction of treatment response. This research involved the development and comparison of different computational modelling methods including artificial neural networks, support vector machines, random forests and logistical regression.\n\nThe predictions of the RDI’s models have historically correlated well with the actual changes in virus load of patients in the clinic, typically achieving a correlation co-efficient of 0.8 or more.\n\nIn October 2010, following clinical testing in two multinational studies, the RDI made its experimental HIV Treatment Response Prediction System, HIV-TRePS available over the Internet. In January 2011, two clinical studies were published indicating that use of the HIV-TRePS system could lead to clinical and economic benefits. The studies, conducted by expert HIV physicians in the USA, Canada and Italy, showed that use of the system was associated with changes of treatment decision to combinations involving fewer drugs overall, which were predicted to result in better virological responses, suggesting that use of the system could potentially improve patient outcomes and reduce the overall number and cost of drugs used.\n\nRecent models have predicted whether a combination treatment will reduce the level of virus in the patient’s bloodstream to undetectable levels with an accuracy of approximately 80%, significantly better than just using a genotype with rules-based interpretation\n\nAs clinics in resource-limited settings are often unable to afford genotyping, the RDI has developed models that predicted treatment response without the need for a genotype, with only a small loss of accuracy. In July 2011, the RDI made these models available as part of the HIV-TRePS system. This version is aimed particularly at resource-limited settings where genotyping is often not routinely available. The most recent of these models, trained with the largest dataset so far, achieved 80% accuracy, which is comparable to models that use a genotype in their predictions and significantly more accurate than genotyping with rules-based interpretation itself.\n\nHIV-TRePS is now in use in 70 countries as a tool to predict virological response to therapy and avoid treatment failure.\n\nThe system has been expanded to enable physicians to include their local drug costs in the modelling. A recent study of data from an Indian cohort demonstrated that the system was able to identify combinations of three locally available drugs with a higher probability of success than the regimen prescribed in the clinic, including those cases where the treatment used in the clinic failed. Moreover, in all these cases some of the alternatives were less costly than the regimen used in the clinic, suggesting that the system could be not only help avoid treatment failure but also reduce costs.\n\n\n\nCohorts: Peter Reiss and Ard van Sighem (ATHENA, the Netherlands); Julio Montaner and Richard Harrigan (BC Center for Excellence in HIV & AIDS, Canada); Tobias Rinke de Wit, Raph Hamers and Kim Sigaloff (PASER-M cohort, The Netherlands); Brian Agan, Vincent Marconi and Scott Wegner (US Department of Defense); Wataru Sugiura (National Institute of Health, Japan); Maurizio Zazzi (MASTER, Italy); Adrian Streinu-Cercel National Institute of Infectious Diseases Prof.Dr. Matei Balş, Bucharest, Romania; Gerardo Alvarez-Uria (VFHCS, India).\nClinics: Jose Gatell and Elisa Lazzari (University Hospital, Barcelona, Spain); Brian Gazzard, Mark Nelson, Anton Pozniak and Sundhiya Mandalia (Chelsea and Westminster Hospital, London, UK); Lidia Ruiz and Bonaventura Clotet (Fundacion Irsi Caixa, Badelona, Spain); Schlomo Staszewski (Hospital of the Johann Wolfgang Goethe-University, Frankfurt, Germany); Carlo Torti (University of Brescia); Cliff Lane and Julie Metcalf (National Institutes of Health Clinic, Rockville, USA); Maria-Jesus Perez-Elias (Instituto Ramón y Cajal de Investigación Sanitaria, Madrid, Spain); Andrew Carr, Richard Norris and Karl Hesse (Immunology B Ambulatory Care Service, St. Vincent’s Hospital, Sydney, NSW, Australia); Dr Emanuel Vlahakis (Taylor’s Square Private Clinic, Darlinghurst, NSW, Australia); Hugo Tempelman and Roos Barth (Ndlovu Care Group, Elandsdoorn, South Africa), Carl Morrow and Robin Wood (Desmond Tutu HIV Centre, University of Cape Town, South Africa); Luminita Ene (“Dr. Victor Babes” Hospital for Infectious and Tropical Diseases, Bucharest, Romania); Gordana Dragovic (University of Belgrade, Belgrade, Serbia).\nClinical trials: Sean Emery and David Cooper (CREST); Carlo Torti (GenPherex); John Baxter (GART, MDR); Laura Monno and Carlo Torti (PhenGen); Jose Gatell and Bonventura Clotet (HAVANA); Gaston Picchio and Marie-Pierre deBethune (DUET 1 & 2 and POWER 3); Maria-Jesus Perez-Elias (RealVirfen).\n\n",
    "id": "29069615",
    "title": "Resistance Database Initiative"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=23951451",
    "text": "SmartAction\n\nSmartAction provides artificial intelligence-based voice self-service. SmartAction's Intelligent Voice Automation (IVA) is a hosted IVR platform that uses natural language speech recognition and is based on an object-oriented coding framework. IVA is a cloud-based, hosted service. IVA® handles complex customer experience calls across 12 verticals, providing assistance with numerous call types such as appointment scheduling and rescheduling, delivery status, order placements, and a variety of outbound calls.\n\nSmartAction was founded by inventor and entrepreneur Peter Voss and is headquartered in El Segundo, CA.\n\nDeveloping artificial intelligence has challenged researchers since at least the 1940s (see History of artificial intelligence). Part of the problem has been the difficulty of defining an adequate theory of intelligence that can serve as a framework to guide hardware implementations. Starting in the early 1990s Voss developed a new theory of intelligence, outlined in the book \"Artificial General Intelligence\". In 2001, Voss founded an R&D startup, Adaptive AI, Inc., to research and develop a prototype artificial general intelligence system based on his theory of intelligence. In 2009 Voss founded Smart Action Company, LLC to commercialize this technology. Born out of Voss' research and development, SmartAction created its first practical application of this new technology focused on the management of inbound and outbound calls for contact centers. Since its inception, SmartAction has continued to develop the technology into a highly specialized and purpose driven AI, identified by the company as Intelligent Voice Automation.\n\nThere are two layers of technology that SmartAction uses in its systems. The first is speech recognition, which enables the system to understand natural language spoken words and phrases. Second is a proprietary artificial intelligence engine used to determine the meaning and intent behind the spoken words, drive the conversation with the caller, and improve over time. A text-to-speech engine enables the system to speak. The AI engine's built-in knowledge and skills also makes development and updates to the system much faster and easier than with older technology.\n\n\n\n",
    "id": "23951451",
    "title": "SmartAction"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=8252994",
    "text": "Vehicle infrastructure integration\n\nVehicle Infrastructure Integration (VII) is an initiative fostering research and applications development for a series of technologies directly linking road vehicles to their physical surroundings, first and foremost in order to improve road safety. The technology draws on several disciplines, including transport engineering, electrical engineering, automotive engineering, and computer science. VII specifically covers road transport although similar technologies are in place or under development for other modes of transport. Planes, for example, use ground-based beacons for automated guidance, allowing the autopilot to fly the plane without human intervention. In highway engineering, improving the safety of a roadway can enhance overall efficiency. VII targets improvements in both safety and efficiency.\n\nVehicle infrastructure integration is that branch of engineering, which deals with the study and application of a series of techniques directly linking road vehicles to their physical surroundings in order to improve road safety.\n\nThe goal of VII is to provide a communications link between vehicles on the road (via On-Board Equipment, OBE), and between vehicles and the roadside infrastructure (via Roadside Equipment, RSE), in order to increase the safety, efficiency, and convenience of the transportation system. It is based on widespread deployment of a dedicated short-range communications (DSRC) link, incorporating IEEE 802.11p. VII's development relies on a business model supporting the interests of all parties concerned: industry, transportation authorities and professional organisations. The initiative has three priorities:\n\nCurrent active safety technology relies on vehicle-based radar and vision systems. For example, this technology can reduce rear-end collisions by tracking obstructions in front or behind the vehicle, automatically applying brakes when needed. This technology is somewhat limited in that it senses only the distance and speed of vehicles within the direct line of sight of cameras and the sensing range of radars. It is almost completely ineffective for angled and left-turn collisions . It may even cause a motorist to lose control of the vehicle in the event of an impending head-on collision. The rear-end collisions covered by today's technology are typically less severe than angle, left-turn, or head-on collisions. Existing technology is therefore inadequate for the overall needs of the roadway system. \n\nVII would provide a direct link between a vehicle on the road and all vehicles within a defined vicinity. The vehicles would be able to communicate with each other, exchanging data on speed, orientation, perhaps even on driver awareness and intent. This could increase safety for nearby vehicles, while enhancing the overall sensitivity of the VII system, for example, by performing an automated emergency maneuver (steering, decelerating, braking) more effectively. In addition, the system is designed to communicate with the roadway infrastructure, allowing for complete, real-time traffic information for the entire network, as well as better queue management and feedback to vehicles. It would ultimately close the feedback loops on what is now an open-loop transportation system.\n\nThrough VII, roadway markings and road signs could become obsolete. Existing VII applications use sensors within vehicles which can identify markings on the roadway or signing along the side of the road, automatically adjusting vehicle parameters as necessary. Ultimately, VII aims to treat such signs and markings as little more than stored data within the system. This could be in the form of data acquired via beacons along a roadway or stored at a centralised database and distributed to all VII-equipped vehicles.\n\nAll the above factors are largely in response to safety but VII could lead to noticeable gains in the operational efficiency of a transportation network. As vehicles will be linked together with a resulting decrease in reaction times, the headway between vehicles could be reduced so that there is less empty space on the road. Available capacity for traffic would therefore be increased. More capacity per lane will in turn mean fewer lanes in general, possibly satisfying the community's concerns about the impact of roadway widening. VII will enable precise traffic-signal coordination by tracking vehicle platoons and will benefit from accurate timing by drawing on real-time traffic data covering volume, density and turning movements.\n\nReal-time traffic data can also be used in the design of new roadways or modification of existing systems as the data could be used to provide accurate origin-destination studies and turning-movement counts for uses in transportation forecasting and traffic operations. Such technology would also lead to improvements for transport engineers to address problems whilst reducing the cost of obtaining and compiling data. Tolling is another prospect for VII technology as it could enable roadways to be automatically tolled. Data could be collectively transmitted to road users for in-vehicle display, outlining the lowest cost, shortest distance, and/or fastest route to a destination on the basis of real-time conditions.\n\nTo some extent, results along these lines have been achieved in trials performed around the globe, making use of GPS, mobile phone signals, and vehicle registration plates. GPS is becoming standard in many new high-end vehicles and is an option on most new low- and mid-range vehicles. In addition, many users also have mobile phones which transmit trackable signals (and may also be GPS-enabled). Mobile phones can already be traced for purposes of emergency response. GPS and mobile phone tracking, however, do not provide fully reliable data. Furthermore, integrating mobile phones in vehicles may be prohibitively difficult. Data from mobile phones, though useful, might even increase risks to motorists as they tend to look at their phones rather than concentrate on their driving. Automatic registration plate recognition can provide high levels of data, but continuously tracking a vehicle through a corridor is a difficult task with existing technology. Today's equipment is designed for data acquisition and functions such as enforcement and tolling, not for returning data to vehicles or motorists for response. GPS will nevertheless be one of the key components in VII systems.\n\nThere are numerous limitations to the development of VII. A common misconception is that the biggest challenge to VII technology is the computing power that can be fitted inside a vehicle. While this is indeed a challenge, the technology for computers has been advancing rapidly and is not a particular concern for VII researchers. Given the fact that technologies already exist for the most basic of forms of VII, perhaps the greatest hurdle to the deployment of VII technology is public acceptance.\n\nThe most common myth about VII is that it includes tracking technology; however, this is not the case. The architecture is designed to prevent identification of individual vehicles, with all data exchange between the vehicle and the system occurring anonymously. Exchanges between the vehicles and third parties such as OEMs and toll collectors will occur, but the network traffic will be sent via encrypted tunnels and will therefore not be decipherable by the VII system.\n\nAlthough the system will be able to detect signal and speed violations, it will not have the capability to identify the violator and report them. The detection is for the purpose of alerting the violator and/or approaching vehicles, to prevent collisions.\n\nOther public acceptance concerns come from advocates of recreational driving as well as from critics of tolling. The former argue that VII will increase the automation of the vehicle, reducing the driver's enjoyment. Recreational driving concerns are particularly prevalent among owners of sports cars. They could be attenuated by compensating for the presence of vehicles without VII or perhaps by maintaining roadways where vehicles without VII are permitted to travel.\n\nThose opposed to tolling believe it will make driving prohibitively expensive for motorists in the lower-income bracket, conflicting with the general wish to provide equal services for all. In response, public transit discounts or road use discounts can be considered for qualifying individuals and/or families. Such provisions currently exist for numerous tolled roadways and could be applicable to roadways that are tolled via VII. However, as VII could allow for the tolling of \"every\" VII-enabled roadway, the provisions may be ineffective in view of the increased need to provide user-efficient transit services to every area.\n\nA major issue facing the deployment of VII is the problem of how to stand up the system initially. The costs associated with installing the technology in vehicles and providing communications and power at every intersection are significant. Building out the infrastructure along the roadside without the auto manufacturers' cooperation would be disastrous, as would the reverse situation; therefore, the two parties will need to work together to make the VII concept work.\n\nThere are proof of concept tests being performed in Michigan and California that will be evaluated by the US DOT and the auto manufacturers, and a decision will be made, jointly, about whether or not to move forward with implementation of the system at that time.\n\nAnother factor for consideration in regard to the technology's distribution is how to update and maintain the units. Traffic systems are highly dynamic, with new traffic controls implemented every day and roadways constructed or repaired every year. The vehicle-based option could be updated via the internet (preferably wireless), but may subsequently require all users to have access to internet technology. Many local government agencies have been testing deployment of internet facilities in cities and along roadways, for example at rest-stops. These systems could be used for VII updating.\n\nAn additional option is to provide updates whenever a vehicle is brought in for inspection or servicing. A major limitation here is that updating would be in the hands of the user. Some vehicle owners maintain their vehicles themselves, and periodic inspections or servicing are considered too infrequent for updating VII. Motorists might also be reluctant to stop at rest-stops for an update if they do not have the possibility of driving in an internet-enabled city.\n\nAlternatively, if receivers were placed in all vehicles and the VII system was primarily located along the roadside, information could be stored in a centralised database. This would allow the agency responsible to issue updates at any time. These would then be disseminated to the roadside units for passing motorists. Operationally, this method is currently considered to provide the greatest effectiveness but at a high cost to the authorities.\n\nSecurity of the units is another concern, especially in the light of the public acceptance issue. Criminals could tamper with VII units, or remove and/or destroy them regardless of whether they are installed inside vehicles or along the roadside. If they are placed inside vehicles, laws similar to those for tampering with an odometer could be enacted; and the units could be examined during inspections or services for signs of tampering. This method has many of the limitations mentioned in relation to the frequency of inspection and motorists who perform their own servicing. It also raises concerns regarding the honesty of vehicle technicians performing the inspections. The ability of technicians to identify signs of tampering would be dependent on their knowledge of the VII systems themselves.\n\nMagnets, electric shocks, and malicious software (viruses, hacking, or jamming) could be used to damage VII systems - regardless of whether units are located inside vehicle or along the roadside. Extensive training and certification would be required for technicians to inspect VII units within a vehicle. Along the roadside, a high degree of security would be required to ensure that the equipment is not damaged and to increase its durability. However, as roadside units could well be placed on the public right-of-way - which is often close to the edge of the roadway - there could be concerns about vehicles hitting them (whether on purpose or by accident). The units would either have to be built so that they do not provide a threat to motorists: perhaps in the form of a low-profile and/or low-mass object designed to be run over or to break apart (which would entail a relatively inexpensive unit); or the unit would have to be shielded by a device such as a guardrail, raising safety concerns of its own.\n\nYet another limitation is in digitizing the inputs for the VII system. VII systems will probably continue to sense existing signs and roadway markings but one of the goals is to eliminate such signs and markings altogether. This would require converting the locations and messages of each item into the VII system's format. Responsibility for this work would probably fall on the highway agencies which nearly all face difficulties in funding, manpower, and available time. Implementing and maintaining VII systems may therefore require support at the national level.\n\nWhile VII is largely being developed as a joint research enterprise involving numerous transport agencies, it is likely initial products will be tailored to individual applications. As a result, compatibility and formatting issues could well arise as systems expand. Overcoming these difficulties could require complicated translation programs between different systems or possibly a complete overhaul of existing VII systems in order to develop a more comprehensive approach. In either case, the costs and potential for bugs in the software will likely be high.\n\nLegislation will be required to set in place access to the VII data and communications between applicable agencies. In the USA, for example, an Interstate is a Federal roadway that is often maintained by the State, but the local county or municipal authorities may be involved too. The legislation would need to set the levels of authority of each agency. In Pennsylvania, for example, municipalities tend to have greater authority than counties and sometimes even the State whereas neighboring Maryland has more authority at the county level than at municipal level; and State roads are almost exclusively controlled by the State. It would also have to be determined which other agencies can use the data (i.e. law enforcement, Census, etc.) and to what degree it is permissible to use the information. Law enforcement would be needed to minimise data misuse. The various levels of authority could also increase incompatibility.\n\nMuch of the current research and experimentation is conducted in the United States where coordination is ensured through the Vehicle Infrastructure Integration Consortium, consisting of automobile manufacturers (Ford, General Motors, DaimlerChrysler, Toyota, Nissan, Honda, Volkswagen, BMW), IT suppliers, U.S. Federal and state transportation departments, and professional associations. Trialling is taking place in Michigan and California.\n\nThe specific applications now being developed under the U.S. initiative\nare:\nIn mid-2007, a VII environment covering some near Detroit will be used to test 20 prototype VII applications. Several automobile manufacturers are also conducting their own VII research and trialling.\n\n\n",
    "id": "8252994",
    "title": "Vehicle infrastructure integration"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=15795950",
    "text": "Activity recognition\n\nActivity recognition aims to recognize the actions and goals of one or more agents from a series of observations on the agents' actions and the environmental conditions. Since the 1980s, this research field has captured the attention of several computer science communities due to its strength in providing personalized support for many different applications and its connection to many different fields of study such as medicine, human-computer interaction, or sociology.\nDue to its many-faceted nature, different fields may refer to activity recognition as plan recognition, goal recognition, intent recognition, behavior recognition, location estimation and location-based services.\n\nSensor-based activity recognition integrates the emerging area of sensor networks with novel data mining and machine learning techniques to model a wide range of human activities. Mobile devices (e.g. smart phones) provide sufficient sensor data and calculation power to enable physical activity recognition to provide an estimation of the energy consumption during everyday life. Sensor-based activity recognition researchers believe that by empowering ubiquitous computers and sensors to monitor the behavior of agents (under consent), these computers will be better suited to act on our behalf.\n\nSensor-based activity recognition is a challenging task due to the inherent noisy nature of the input. Thus, statistical modeling has been the main thrust in this direction in layers, where the recognition at several intermediate levels is conducted and connected. At the lowest level where the sensor data are collected, statistical learning concerns how to find the detailed locations of agents from the received signal data. At an intermediate level, statistical inference may be concerned about how to recognize individuals' activities from the inferred location sequences and environmental conditions at the lower levels. Furthermore, at the highest level a major concern is to find out the overall goal or subgoals of an agent from the activity sequences through a mixture of logical and statistical reasoning. Scientific conferences where activity recognition work from wearable and environmental often appears are ISWC and UbiComp.\n\nRecognizing activities for multiple users using on-body sensors first appeared in the work by ORL using active badge systems in the early 90's. Other sensor technology such as acceleration sensors were used for identifying group activity patterns during office scenarios. Activities of Multiple Users in intelligent environments are addressed in Gu et al. In this work, they investigate the fundamental problem of recognizing activities for multiple users from sensor readings in a home environment, and propose a novel pattern mining approach to recognize both single-user and multi-user activities in a unified solution.\n\nRecognition of group activities is fundamentally different from single, or multi-user activity recognition in that the goal is to recognize the behavior of the group as an entity, rather than the activities of the individual members within it. Group behavior is emergent in nature, meaning that the properties of the behavior of the group are fundamentally different then the properties of the behavior of the individuals within it, or any sum of that behavior. The main challenges are in modeling the behavior of the individual group members, as well as the roles of the individual within the group dynamic and their relationship to emergent behavior of the group in parallel. Challenges which must still be addressed include quantification of the behavior and roles of individuals who join the group, integration of explicit models for role description into inference algorithms, and scalability evaluations for very large groups and crowds. Group activity recognition has applications for crowd management and response in emergency situations, as well as for social networking and Quantified Self applications.\n\nLogic-based approaches keep track of all logically consistent explanations of the observed actions. Thus, all possible and consistent plans or goals must be considered. Kautz provided a formal theory of plan recognition. He described plan recognition as a logical inference process of circumscription. All actions, plans are uniformly referred to as goals, and a recognizer's knowledge is represented by a set of first-order statements called event hierarchy encoded in first-order logic, which defines abstraction, decomposition and functional relationships between types of events.\n\nKautz's general framework for plan recognition has an exponential time complexity in worst case, measured in the size of input hierarchy. Lesh and Etzioni went one step further and presented methods in scaling up goal recognition to scale up his work computationally. In contrast to Kautz's approach where the plan library is explicitly represented, Lesh and Etzioni's approach enables automatic plan-library construction from domain primitives. Furthermore, they introduced compact representations and efficient algorithms for goal recognition on large plan libraries.\n\nInconsistent plans and goals are repeatedly pruned when new actions arrive. Besides, they also presented methods for adapting a goal recognizer to handle individual idiosyncratic behavior given a sample of an individual's recent behavior. Pollack et al. described a direct argumentation model that can know about the relative strength of several kinds of arguments for belief and intention description.\n\nA serious problem of logic-based approaches is their inability or inherent infeasibility to represent uncertainty. They offer no mechanism for preferring one consistent approach to another and incapable of deciding whether one particular plan is more likely than another, as long as both of them can be consistent enough to explain the actions observed. There is also a lack of learning ability associated with logic based methods.\n\nAnother approach to logic-based activity recognition is to use stream reasoning based on Answer Set Programming, and has been applied to recognising activities for health-related applications, which uses weak constraints to model a degree of ambiguity/uncertainty.\n\nProbability theory and statistical learning models are more recently applied in activity recognition to reason about actions, plans and goals under uncertainty. In the literature, there have been several approaches which explicitly represent uncertainty in reasoning about an agent's plans and goals.\n\nUsing sensor data as input, Hodges and Pollack designed machine learning-based systems for identifying individuals as they perform routine daily activities such as making coffee. Intel Research (Seattle) Lab and University of Washington at Seattle have done some important works on using sensors to detect human plans. Some of these works infer user transportation modes from readings of radio-frequency identifiers (RFID) and global positioning systems (GPS).\n\nThe use of temporal probabilistic models has been shown to perform well in activity recognition and generally outperform non-temporal models. Generative models such as the Hidden Markov Model (HMM) and the more generally formulated Dynamic Bayesian Networks (DBN) are popular choices in modelling activities from sensor data.\nDiscriminative models such as Conditional Random Fields (CRF) are also commonly applied and also give good performance in activity recognition.\n\nGenerative and discriminative models both have their pros and cons and the ideal choice depends on their area of application. A dataset together with implementations of a number of popular models (HMM, CRF) for activity recognition can be found here.\n\nConventional temporal probabilistic models such as the hidden Markov model (HMM) and conditional random fields (CRF) model directly model the correlations between the activities and the observed sensor data. In recent years, increasing evidence has supported the use of hierarchical models which take into account the rich hierarchical structure that exists in human behavioral data. The core idea here is that the model does not directly correlate the activities with the sensor data, but instead breaks the activity into sub-activities (sometimes referred to as actions) and models the underlying correlations accordingly. An example could be the activity of preparing spaghetti, which can be broken down into the subactivities or actions of cutting vegetables, frying the vegetables in a pan and serving it on a plate. Examples of such a hierarchical model are Layered Hidden Markov Models (LHMMs) and the hierarchical hidden Markov model (HHMM), which have been shown to significantly outperform its non-hierarchical counterpart in activity recognition.\n\nDifferent from traditional machine learning approaches, an approach based on data mining has been recently proposed.\nIn the work of Gu et al., the problem of activity recognition is formulated as a pattern-based classification problem. They proposed a data mining approach based on discriminative patterns which describe significant changes between any two activity classes of data to recognize sequential, interleaved and concurrent activities in a unified solution.\nGilbert \"et al.\" use 2D corners in both space and time. These are grouped spatially and temporally using a hierarchical process, with an increasing search area. At each stage of the hierarchy, the most distinctive and descriptive features are learned efficiently through data mining (Apriori rule).\n\nIt is a very important and challenging problem to track and understand the behavior of agents through videos taken by various cameras. The primary technique employed is computer vision. Vision-based activity recognition has found many applications such as human-computer interaction, user interface design, robot learning, and surveillance, among others.\nScientific conferences where vision based activity recognition work often appears are ICCV and CVPR.\n\nIn vision-based activity recognition, a great deal of work has been done. Researchers have attempted a number of methods such as optical flow, Kalman filtering, Hidden Markov models, etc., under different modalities such as single camera, stereo, and infrared. In addition, researchers have considered multiple aspects on this topic, including single pedestrian tracking, group tracking, and detecting dropped objects.\n\nRecently some researchers have used RGBD cameras like Microsoft Kinect to detect human activities. Depth cameras add extra dimension i.e. depth which normal 2d camera fails to provide. Sensory information from these depth cameras have been used to generate real-time skeleton model of humans with different body positions. These skeleton information provides meaningful information that researchers have used to model human activities which are trained and later used to recognize unknown activities.\n\nIn vision-based activity recognition, the computational process is often divided into four steps, namely human detection, human tracking, human activity recognition and then a high-level activity evaluation.\n\nOne way to identify specific people is by how they walk. Gait-recognition software can be used to record a person's gait or gait feature profile in a database for the purpose of recognizing that person later, even if they are wearing a disguise.\n\nWhen activity recognition is performed indoors and in cities using the widely available Wi-Fi signals and 802.11 access points, there is much noise and uncertainty. These uncertainties are modeled using a dynamic Bayesian network model by Yin et al. A multiple goal model that can reason about user's interleaving goals is presented by Chai and Yang, where a deterministic state transition model is applied. A better model that models the concurrent and interleaving activities in a probabilistic approach is proposed by Hu and Yang. A user action discovery model is presented by Yin et al., where the Wi-Fi signals are segmented to produce possible actions.\n\nA fundamental problem in Wi-Fi-based activity recognition is to estimate the user locations. Two important issues are how to reduce the human labelling effort and how to cope with the changing signal profiles when the environment changes. Yin et al. dealt with the second issue by transferring the labelled knowledge between time periods. Chai and Yang proposed a hidden Markov model-based method to extend labelled knowledge by leveraging the unlabelled user traces. J. Pan et al. propose to perform location estimation through online co-localization, and S. Pan et al. proposed to apply multi-view learning for migrating the labelled data to a new time period.\n\nMany different applications have been studied by researchers in activity recognition; examples include assisting the sick and disabled. For example, Pollack et al. show that by automatically monitoring human activities, home-based rehabilitation can be provided for people suffering from traumatic brain injuries. One can find applications ranging from security-related applications and logistics support to location-based services.\n\n\n\n\n\n",
    "id": "15795950",
    "title": "Activity recognition"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=8006328",
    "text": "Chinese speech synthesis\n\nChinese speech synthesis is the application of speech synthesis to the Chinese language (usually Standard Chinese). It poses additional difficulties due to the Chinese characters (which frequently have different pronunciations in different contexts), the complex prosody, which is essential to convey the meaning of words, and sometimes the difficulty in obtaining agreement among native speakers concerning what the correct pronunciation is of certain phonemes.\n\nRecordings can be concatenated in any desired combination, but the joins sound forced (as is usual for simple concatenation-based speech synthesis) and this can severely affect prosody; these synthesizers are also inflexible in terms of speed and expression. However, because these synthesizers do not rely on a corpus, there is no noticeable degradation in performance when they are given more unusual or awkward phrases.\n\nEkho is an open source TTS which simply concatenates sampled syllables. It currently supports Cantonese, Mandarin, and experimentally Korean. Some of the Mandarin syllables have been pitched-normalised in Praat. A modified version of these is used in Gradint's \"synthesis from partials\".\n\ncjkware.com used to ship a product called KeyTip Putonghua Reader which worked similarly; it contained 120 Megabytes of sound recordings (GSM-compressed to 40 Megabytes in the evaluation version), comprising 10,000 multi-syllable dictionary words plus single-syllable recordings in 6 different prosodies (4 tones, neutral tone, and an extra third-tone recording for use at the end of a phrase).\n\nThe lightweight open-source speech project eSpeak, which has its own approach to synthesis, has experimented with Mandarin and Cantonese. eSpeak was used by Google Translate from May 2010 until December 2010.\n\nThe commercial product \"Yuet\" is also lightweight (it is intended to be suitable for resource-constrained environments like embedded systems); it was written from scratch in ANSI C starting from 2013. Yuet claims a built-in NLP model that does not require a separate dictionary; the speech synthesised by the engine claims clear word boundaries and emphasis on appropriate words. Communication with its author is required to obtain a copy.\n\nBoth eSpeak and Yuet can synthesis speech for Cantonese and Mandarin from the same input text, and can output the corresponding romanisation (for Cantonese, Yuet uses Yale and eSpeak uses Jyutping; both use Pinyin for Mandarin). eSpeak does not concern itself with word boundaries when these don't change the question of which syllable should be spoken.\n\nA \"corpus-based\" approach can sound very natural in most cases but can err in dealing with unusual phrases if they can't be matched with the corpus. The synthesiser engine is typically very large (hundreds or even thousands of megabytes) due to the size of the corpus.\n\nAnhui USTC iFlyTek Co., Ltd (iFlyTek) published a W3C paper in which they adapted Speech Synthesis Markup Language to produce a mark-up language called Chinese Speech Synthesis Markup Language (CSSML) which can include additional markup to clarify the pronunciation of characters and to add some prosody information. The amount of data involved is not disclosed by iFlyTek but can be seen from the commercial products that iFlyTek have licensed their technology to; for example, Bider's SpeechPlus is a 1.3 Gigabyte download, 1.2 Gigabytes of which is used for the highly compressed data for a single Chinese voice. iFlyTek's synthesiser can also synthesise mixed Chinese and English text with the same voice (e.g. Chinese sentences containing some English words); they claim their English synthesis to be \"average\".\n\nThe iFlyTek corpus appears to be heavily dependent on Chinese characters, and it is not possible to synthesize from pinyin alone. It is sometimes possible by means of CSSML to add pinyin to the characters to disambiguate between multiple possible pronunciations, but this does not always work.\n\nThere is an online interactive demonstration for NeoSpeech speech synthesis, which accepts Chinese characters and also pinyin if it's enclosed in their proprietary \"VTML\" markup.\n\nMac OS had Chinese speech synthesizers available up to version 9. This was removed in 10.0 and reinstated in 10.7 (Lion).\n\nA corpus-based approach was taken by Tsinghua University in SinoSonic, with the Harbin dialect voice data taking 800 Megabytes. This was planned to be offered as a download but the link was never activated. Nowadays, references to it can be found only on Internet Archive.\n\nBell Labs' approach, which was demonstrated online in 1997 but subsequently removed, was described in a monograph \"Multilingual Text-to-Speech Synthesis: The Bell Labs Approach\" (Springer, October 31, 1997, ), and the former employee who was responsible for the project, Chilin Shih (who subsequently worked at the University of Illinois) put some notes about her methods on her website.\n",
    "id": "8006328",
    "title": "Chinese speech synthesis"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=30285715",
    "text": "Mind's Eye (US military)\n\nThe Mind's Eye is a video analysis research project using artificial intelligence. It is funded by the Defense Advanced Research Projects Agency.\n\nTwelve research teams have been contracted by DARPA for the Mind's Eye: Carnegie Mellon University, Co57 Systems, Inc., Colorado State University, Jet Propulsion Laboratory/Caltech, Massachusetts Institute of Technology, Purdue University, SRI International, State University of New York at Buffalo, TNO (Netherlands), University of Arizona, University of California Berkeley and the University of Southern California.\n\n\"The Mind's Eye program seeks to develop in machines a capability that exists only in animals: visual intelligence. This program pursues the capability to learn generally applicable and generative representations of action between objects in a scene directly from visual inputs, and then reason over those learned representations. A key distinction between this research and the state of the art in machine vision is that the latter has made continual progress in recognizing a wide range of objects and their properties - what might be thought of as the nouns in the description of a scene. The focus of Mind's Eye is to add the perceptual and cognitive underpinnings for recognizing and reasoning about the verbs in those scenes, enabling a more complete narrative of action in the visual experience.\"\n\n",
    "id": "30285715",
    "title": "Mind's Eye (US military)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=318439",
    "text": "Text mining\n\nText mining, also referred to as text data mining, roughly equivalent to text analytics, is the process of deriving high-quality information from text. High-quality information is typically derived through the devising of patterns and trends through means such as statistical pattern learning. Text mining usually involves the process of structuring the input text (usually parsing, along with the addition of some derived linguistic features and the removal of others, and subsequent insertion into a database), deriving patterns within the structured data, and finally evaluation and interpretation of the output. 'High quality' in text mining usually refers to some combination of relevance, novelty, and interestingness. Typical text mining tasks include text categorization, text clustering, concept/entity extraction, production of granular taxonomies, sentiment analysis, document summarization, and entity relation modeling (\"i.e.\", learning relations between named entities).\n\nText analysis involves information retrieval, lexical analysis to study word frequency distributions, pattern recognition, tagging/annotation, information extraction, data mining techniques including link and association analysis, visualization, and predictive analytics. The overarching goal is, essentially, to turn text into data for analysis, via application of natural language processing (NLP) and analytical methods.\n\nA typical application is to scan a set of documents written in a natural language and either model the document set for predictive classification purposes or populate a database or search index with the information extracted.\n\nThe term text analytics describes a set of linguistic, statistical, and machine learning techniques that model and structure the information content of textual sources for business intelligence, exploratory data analysis, research, or investigation. The term is roughly synonymous with text mining; indeed, Ronen Feldman modified a 2000 description of \"text mining\" in 2004 to describe \"text analytics\". The latter term is now used more frequently in business settings while \"text mining\" is used in some of the earliest application areas, dating to the 1980s, notably life-sciences research and government intelligence.\n\nThe term text analytics also describes that application of text analytics to respond to business problems, whether independently or in conjunction with query and analysis of fielded, numerical data. It is a truism that 80 percent of business-relevant information originates in unstructured form, primarily text. These techniques and processes discover and present knowledge – facts, business rules, and relationships – that is otherwise locked in textual form, impenetrable to automated processing.\n\nIncreasing interest is being paid to multilingual data mining: the ability to gain information across languages and cluster similar items from different linguistic sources according to their meaning.\n\nThe challenge of exploiting the large proportion of enterprise information that originates in \"unstructured\" form has been recognized for decades. It is recognized in the earliest definition of business intelligence (BI), in an October 1958 IBM Journal article by H.P. Luhn, A Business Intelligence System, which describes a system that will:\n\"...utilize data-processing machines for auto-abstracting and auto-encoding of documents and for creating interest profiles for each of the 'action points' in an organization. Both incoming and internally generated documents are automatically abstracted, characterized by a word pattern, and sent automatically to appropriate action points.\"\nYet as management information systems developed starting in the 1960s, and as BI emerged in the '80s and '90s as a software category and field of practice, the emphasis was on numerical data stored in relational databases. This is not surprising: text in \"unstructured\" documents is hard to process. The emergence of text analytics in its current form stems from a refocusing of research in the late 1990s from algorithm development to application, as described by Prof. Marti A. Hearst in the paper Untangling Text Data Mining:\n\nFor almost a decade the computational linguistics community has viewed large text collections as a resource to be tapped in order to produce better text analysis algorithms. In this paper, I have attempted to suggest a new emphasis: the use of large online text collections to discover new facts and trends about the world itself. I suggest that to make progress we do not need fully artificial intelligent text analysis; rather, a mixture of computationally-driven and user-guided analysis may open the door to exciting new results.\nHearst's 1999 statement of need fairly well describes the state of text analytics technology and practice a decade later.\n\nSubtasks—components of a larger text-analytics effort—typically include:\n\nThe technology is now broadly applied for a wide variety of government, research, and business needs. Applications can be sorted into a number of categories by analysis type or by business function. Using this approach to classifying solutions, application categories include:\n\nMany text mining software packages are marketed for security applications, especially monitoring and analysis of online plain text sources such as Internet news, blogs, etc. for national security purposes. It is also involved in the study of text encryption/decryption.\n\nA range of text mining applications in the biomedical literature has been described.\n\nOne online text mining application in the biomedical literature is PubGene that combines biomedical text mining with network visualization as an Internet service.\n\nGoPubMed is a knowledge-based search engine for biomedical texts.\n\nText mining methods and software is also being researched and developed by major firms, including IBM and Microsoft, to further automate the mining and analysis processes, and by different firms working in the area of search and indexing in general as a way to improve their results.\nWithin public sector much effort has been concentrated on creating software for tracking and monitoring terrorist activities.\n\nText mining is being used by large media companies, such as the Tribune Company, to clarify information and to provide readers with greater search experiences, which in turn increases site \"stickiness\" and revenue. Additionally, on the back end, editors are benefiting by being able to share, associate and package news across properties, significantly increasing opportunities to monetize content.\n\nText mining is starting to be used in marketing as well, more specifically in analytical customer relationship management. Coussement and Van den Poel (2008)<ref name=\"10.1016/j.im.2008.01.005\"></ref> apply it to improve predictive analytics models for customer churn (customer attrition). Text mining is also being applied in stock returns prediction.\n\nSentiment analysis may involve analysis of movie reviews for estimating how favorable a review is for a movie.\nSuch an analysis may need a labeled data set or labeling of the affectivity of words.\nResources for affectivity of words and concepts have been made for WordNet and ConceptNet, respectively.\n\nText has been used to detect emotions in the related area of affective computing. Text based approaches to affective computing have been used on multiple corpora such as students evaluations, children stories and news stories.\n\nThe issue of text mining is of importance to publishers who hold large databases of information needing indexing for retrieval. This is especially true in scientific disciplines, in which highly specific information is often contained within written text. Therefore, initiatives have been taken such as Nature's proposal for an Open Text Mining Interface (OTMI) and the National Institutes of Health's common Journal Publishing Document Type Definition (DTD) that would provide semantic cues to machines to answer specific queries contained within text without removing publisher barriers to public access.\n\nAcademic institutions have also become involved in the text mining initiative:\n\n\nThe automatic analysis of vast textual corpora has created the possibility for scholars to analyse\nmillions of documents in multiple languages with very limited manual intervention.\nKey enabling technologies have been parsing, machine translation, topic categorization, and machine learning.\nThe automatic parsing of textual corpora has enabled the extraction of actors and their relational networks on a vast scale,\nturning textual data into network data. The resulting networks, which can contain thousands of nodes, are then analysed by using tools from network theory to identify the key actors, the key communities or parties, and general properties such as robustness or structural stability of the overall network, or centrality of certain nodes. This automates the approach introduced by quantitative narrative analysis, whereby subject-verb-object triplets are identified with pairs of actors linked by an action, or pairs formed by actor-object.\n\nContent analysis has been a traditional part of social sciences and media studies for a long time. The automation of content analysis has allowed a \"big data\" revolution to take place in that field, with studies in social media and newspaper content that include millions of news items. Gender bias, readability, content similarity, reader preferences, and even mood have been analyzed based on text mining methods over millions of documents. The analysis of readability, gender bias and topic bias was demonstrated in Flaounas et al. showing how different topics have different gender biases and levels of readability; the possibility to detect mood patterns in a vast population by analysing Twitter content was demonstrated as well.\n\nText mining computer programs are available from many commercial and open source companies and sources. See List of text mining software.\n\nBecause of a lack of flexibilities in European copyright and database law, the mining of in-copyright works (such as web mining) without the permission of the copyright owner is illegal. In the UK in 2014, on the recommendation of the Hargreaves review the government amended copyright law to allow text mining as a limitation and exception. It was only the second country in the world to do so, following Japan, which introduced a mining-specific exception in 2009. However, owing to the restriction of the Copyright Directive, the UK exception only allows content mining for non-commercial purposes. UK copyright law does not allow this provision to be overridden by contractual terms and conditions.\n\nThe European Commission facilitated stakeholder discussion on text and data mining in 2013, under the title of Licences for Europe. The fact that the focus on the solution to this legal issue was licences, and not limitations and exceptions to copyright law, led representatives of universities, researchers, libraries, civil society groups and open access publishers to leave the stakeholder dialogue in May 2013.\n\nBy contrast to Europe, the flexible nature of US copyright law, and in particular fair use, means that text mining in America, as well as other fair use countries such as Israel, Taiwan and South Korea, is viewed as being legal. As text mining is transformative, meaning that it does not supplant the original work, it is viewed as being lawful under fair use. For example, as part of the Google Book settlement the presiding judge on the case ruled that Google's digitisation project of in-copyright books was lawful, in part because of the transformative uses that the digitisation project displayed—one such use being text and data mining.\n\nUntil recently, websites most often used text-based searches, which only found documents containing specific user-defined words or phrases. Now, through use of a semantic web, text mining can find content based on meaning and context (rather than just by a specific word). Additionally, text mining software can be used to build large dossiers of information about specific people and events. For example, large datasets based on data extracted from news reports can be built to facilitate social networks analysis or counter-intelligence. In effect, the text mining software may act in a capacity similar to an intelligence analyst or research librarian, albeit with a more limited scope of analysis. Text mining is also used in some email spam filters as a way of determining the characteristics of messages that are likely to be advertisements or other unwanted material. Text mining plays an important role in determining financial market sentiment.\n\n\n",
    "id": "318439",
    "title": "Text mining"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=42799",
    "text": "Speech synthesis\n\nSpeech synthesis is the artificial production of human speech. A computer system used for this purpose is called a speech computer or speech synthesizer, and can be implemented in software or hardware products. A text-to-speech (TTS) system converts normal language text into speech; other systems render symbolic linguistic representations like phonetic transcriptions into speech.\n\nSynthesized speech can be created by concatenating pieces of recorded speech that are stored in a database. Systems differ in the size of the stored speech units; a system that stores phones or diphones provides the largest output range, but may lack clarity. For specific usage domains, the storage of entire words or sentences allows for high-quality output. Alternatively, a synthesizer can incorporate a model of the vocal tract and other human voice characteristics to create a completely \"synthetic\" voice output.\n\nThe quality of a speech synthesizer is judged by its similarity to the human voice and by its ability to be understood clearly. An intelligible text-to-speech program allows people with visual impairments or reading disabilities to listen to written words on a home computer. Many computer operating systems have included speech synthesizers since the early 1990s.\n\nA text-to-speech system (or \"engine\") is composed of two parts: a front-end and a back-end. The front-end has two major tasks. First, it converts raw text containing symbols like numbers and abbreviations into the equivalent of written-out words. This process is often called \"text normalization\", \"pre-processing\", or \"tokenization\". The front-end then assigns phonetic transcriptions to each word, and divides and marks the text into prosodic units, like phrases, clauses, and sentences. The process of assigning phonetic transcriptions to words is called \"text-to-phoneme\" or \"grapheme-to-phoneme\" conversion. Phonetic transcriptions and prosody information together make up the symbolic linguistic representation that is output by the front-end. The back-end—often referred to as the \"synthesizer\"—then converts the symbolic linguistic representation into sound. In certain systems, this part includes the computation of the \"target prosody\" (pitch contour, phoneme durations), which is then imposed on the output speech.\n\nLong before the invention of electronic signal processing, some people tried to build machines to emulate human speech. Some early legends of the existence of \"Brazen Heads\" involved Pope Silvester II (d. 1003 AD), Albertus Magnus (1198–1280), and Roger Bacon (1214–1294).\n\nIn 1779 the German-Danish scientist Christian Gottlieb Kratzenstein won the first prize in a competition announced by the Russian Imperial Academy of Sciences and Arts for models he built of the human vocal tract that could produce the five long vowel sounds (in notation: , , , and ). There followed the bellows-operated \"acoustic-mechanical speech machine\" of Wolfgang von Kempelen of Pressburg, Hungary, described in a 1791 paper. This machine added models of the tongue and lips, enabling it to produce consonants as well as vowels. In 1837, Charles Wheatstone produced a \"speaking machine\" based on von Kempelen's design, and in 1846, Joseph Faber exhibited the \"Euphonia\". In 1923 Paget resurrected Wheatstone's design.\n\nIn the 1930s Bell Labs developed the vocoder, which automatically analyzed speech into its fundamental tones and resonances. From his work on the vocoder, Homer Dudley developed a keyboard-operated voice-synthesizer called The Voder (Voice Demonstrator), which he exhibited at the 1939 New York World's Fair.\n\nDr. Franklin S. Cooper and his colleagues at Haskins Laboratories built the Pattern playback in the late 1940s and completed it in 1950. There were several different versions of this hardware device; only one currently survives. The machine converts pictures of the acoustic patterns of speech in the form of a spectrogram back into sound. Using this device, Alvin Liberman and colleagues discovered acoustic cues for the perception of phonetic segments (consonants and vowels).\n\nIn 1975 was released MUSA, one of the first Speech Synthesis system: it was able to read Italian; a second version, released in 1978, was able also to sing in Italian \"a cappella\" style.\n\nDominant systems in the 1980s and 1990s were the DECtalk system, based largely on the work of Dennis Klatt at MIT, and the Bell Labs system; the latter was one of the first multilingual language-independent systems, making extensive use of natural language processing methods.\n\nEarly electronic speech-synthesizers sounded robotic and were often barely intelligible. The quality of synthesized speech has steadily improved, but output from contemporary speech synthesis systems remains clearly distinguishable from actual human speech.\n\nKurzweil predicted in 2005 that as the cost-performance ratio caused speech synthesizers to become cheaper and more accessible, more people would benefit from the use of text-to-speech programs.\n\nThe first computer-based speech-synthesis systems originated in the late 1950s. Noriko Umeda \"et al.\" developed the first general English text-to-speech system in 1968 at the Electrotechnical Laboratory, Japan. In 1961 physicist John Larry Kelly, Jr and his colleague Louis Gerstman used an IBM 704 computer to synthesize speech, an event among the most prominent in the history of Bell Labs. Kelly's voice recorder synthesizer (vocoder) recreated the song \"Daisy Bell\", with musical accompaniment from Max Mathews. Coincidentally, Arthur C. Clarke was visiting his friend and colleague John Pierce at the Bell Labs Murray Hill facility. Clarke was so impressed by the demonstration that he used it in the climactic scene of his screenplay for his novel \"\", where the HAL 9000 computer sings the same song as astronaut Dave Bowman puts it to sleep. Despite the success of purely electronic speech synthesis, research into mechanical speech-synthesizers continues.\n\nHandheld electronics featuring speech synthesis began emerging in the 1970s. One of the first was the Telesensory Systems Inc. (TSI) \"Speech+\" portable calculator for the blind in 1976. Other devices had primarily educational purposes, such as the Speak & Spell toy produced by Texas Instruments in 1978. Fidelity released a speaking version of its electronic chess computer in 1979. The first video game to feature speech synthesis was the 1980 shoot 'em up arcade game, \"Stratovox\" (known in Japan as \"Speak & Rescue\"), from Sun Electronics. The first personal computer game with speech synthesis was \"Manbiki Shoujo\" (\"Shoplifting Girl\"), released in 1980 for the PET 2001, for which the game's developer, Hiroshi Suzuki, developed a \"\"zero cross\"\" programming technique to produce a synthesized speech waveform. Another early example, the arcade version of \"Berzerk\", also dates from 1980. The Milton Bradley Company produced the first multi-player electronic game using voice synthesis, \"Milton\", in the same year.\n\nThe most important qualities of a speech synthesis system are \"naturalness\" and \"intelligibility\". Naturalness describes how closely the output sounds like human speech, while intelligibility is the ease with which the output is understood. The ideal speech synthesizer is both natural and intelligible. Speech synthesis systems usually try to maximize both characteristics.\n\nThe two primary technologies generating synthetic speech waveforms are \"concatenative synthesis\" and \"formant synthesis\". Each technology has strengths and weaknesses, and the intended uses of a synthesis system will typically determine which approach is used.\n\nConcatenative synthesis is based on the concatenation (or stringing together) of segments of recorded speech. Generally, concatenative synthesis produces the most natural-sounding synthesized speech. However, differences between natural variations in speech and the nature of the automated techniques for segmenting the waveforms sometimes result in audible glitches in the output. There are three main sub-types of concatenative synthesis.\n\nUnit selection synthesis uses large databases of recorded speech. During database creation, each recorded utterance is segmented into some or all of the following: individual phones, diphones, half-phones, syllables, morphemes, words, phrases, and sentences. Typically, the division into segments is done using a specially modified speech recognizer set to a \"forced alignment\" mode with some manual correction afterward, using visual representations such as the waveform and spectrogram. An index of the units in the speech database is then created based on the segmentation and acoustic parameters like the fundamental frequency (pitch), duration, position in the syllable, and neighboring phones. At run time, the desired target utterance is created by determining the best chain of candidate units from the database (unit selection). This process is typically achieved using a specially weighted decision tree.\n\nUnit selection provides the greatest naturalness, because it applies only a small amount of digital signal processing (DSP) to the recorded speech. DSP often makes recorded speech sound less natural, although some systems use a small amount of signal processing at the point of concatenation to smooth the waveform. The output from the best unit-selection systems is often indistinguishable from real human voices, especially in contexts for which the TTS system has been tuned. However, maximum naturalness typically require unit-selection speech databases to be very large, in some systems ranging into the gigabytes of recorded data, representing dozens of hours of speech. Also, unit selection algorithms have been known to select segments from a place that results in less than ideal synthesis (e.g. minor words become unclear) even when a better choice exists in the database. Recently, researchers have proposed various automated methods to detect unnatural segments in unit-selection speech synthesis systems.\n\nDiphone synthesis uses a minimal speech database containing all the diphones (sound-to-sound transitions) occurring in a language. The number of diphones depends on the phonotactics of the language: for example, Spanish has about 800 diphones, and German about 2500. In diphone synthesis, only one example of each diphone is contained in the speech database. At runtime, the target prosody of a sentence is superimposed on these minimal units by means of digital signal processing techniques such as linear predictive coding, PSOLA or MBROLA. or more recent techniques such as pitch modification in the source domain using discrete cosine transform Diphone synthesis suffers from the sonic glitches of concatenative synthesis and the robotic-sounding nature of formant synthesis, and has few of the advantages of either approach other than small size. As such, its use in commercial applications is declining, although it continues to be used in research because there are a number of freely available software implementations.\n\nDomain-specific synthesis concatenates prerecorded words and phrases to create complete utterances. It is used in applications where the variety of texts the system will output is limited to a particular domain, like transit schedule announcements or weather reports. The technology is very simple to implement, and has been in commercial use for a long time, in devices like talking clocks and calculators. The level of naturalness of these systems can be very high because the variety of sentence types is limited, and they closely match the prosody and intonation of the original recordings.\n\nBecause these systems are limited by the words and phrases in their databases, they are not general-purpose and can only synthesize the combinations of words and phrases with which they have been preprogrammed. The blending of words within naturally spoken language however can still cause problems unless the many variations are taken into account. For example, in non-rhotic dialects of English the \"\"r\"\" in words like \"\"clear\"\" is usually only pronounced when the following word has a vowel as its first letter (e.g. \"\"clear out\"\" is realized as ). Likewise in French, many final consonants become no longer silent if followed by a word that begins with a vowel, an effect called liaison. This alternation cannot be reproduced by a simple word-concatenation system, which would require additional complexity to be context-sensitive.\n\nFormant synthesis does not use human speech samples at runtime. Instead, the synthesized speech output is created using additive synthesis and an acoustic model (physical modelling synthesis). Parameters such as fundamental frequency, voicing, and noise levels are varied over time to create a waveform of artificial speech. This method is sometimes called \"rules-based synthesis\"; however, many concatenative systems also have rules-based components.\nMany systems based on formant synthesis technology generate artificial, robotic-sounding speech that would never be mistaken for human speech. However, maximum naturalness is not always the goal of a speech synthesis system, and formant synthesis systems have advantages over concatenative systems. Formant-synthesized speech can be reliably intelligible, even at very high speeds, avoiding the acoustic glitches that commonly plague concatenative systems. High-speed synthesized speech is used by the visually impaired to quickly navigate computers using a screen reader. Formant synthesizers are usually smaller programs than concatenative systems because they do not have a database of speech samples. They can therefore be used in embedded systems, where memory and microprocessor power are especially limited. Because formant-based systems have complete control of all aspects of the output speech, a wide variety of prosodies and intonations can be output, conveying not just questions and statements, but a variety of emotions and tones of voice.\n\nExamples of non-real-time but highly accurate intonation control in formant synthesis include the work done in the late 1970s for the Texas Instruments toy Speak & Spell, and in the early 1980s Sega arcade machines and in many Atari, Inc. arcade games using the TMS5220 LPC Chips. Creating proper intonation for these projects was painstaking, and the results have yet to be matched by real-time text-to-speech interfaces.\n\nFormant synthesis was implemented in hardware in the Yamaha FS1R synthesizer, but the speech aspect of formants was never realized in the synth. It was capable of short, several-second formant sequences which could speak a single phrase, but since the MIDI control interface was so restrictive live speech was an impossibility.\n\nArticulatory synthesis refers to computational techniques for synthesizing speech based on models of the human vocal tract and the articulation processes occurring there. The first articulatory synthesizer regularly used for laboratory experiments was developed at Haskins Laboratories in the mid-1970s by Philip Rubin, Tom Baer, and Paul Mermelstein. This synthesizer, known as ASY, was based on vocal tract models developed at Bell Laboratories in the 1960s and 1970s by Paul Mermelstein, Cecil Coker, and colleagues.\n\nUntil recently, articulatory synthesis models have not been incorporated into commercial speech synthesis systems. A notable exception is the NeXT-based system originally developed and marketed by Trillium Sound Research, a spin-off company of the University of Calgary, where much of the original research was conducted. Following the demise of the various incarnations of NeXT (started by Steve Jobs in the late 1980s and merged with Apple Computer in 1997), the Trillium software was published under the GNU General Public License, with work continuing as gnuspeech. The system, first marketed in 1994, provides full articulatory-based text-to-speech conversion using a waveguide or transmission-line analog of the human oral and nasal tracts controlled by Carré's \"distinctive region model\".\n\nMore recent synthesizers, developed by Jorge C. Lucero and colleagues, incorporate models of vocal fold biomechanics, glottal aerodynamics and acoustic wave propagation in the bronqui, traquea, nasal and oral cavities, and thus constitute full systems of physics-based speech simulation.\n\nHMM-based synthesis is a synthesis method based on hidden Markov models, also called Statistical Parametric Synthesis. In this system, the frequency spectrum (vocal tract), fundamental frequency (voice source), and duration (prosody) of speech are modeled simultaneously by HMMs. Speech waveforms are generated from HMMs themselves based on the maximum likelihood criterion.\n\nSinewave synthesis is a technique for synthesizing speech by replacing the formants (main bands of energy) with pure tone whistles.\n\nThe process of normalizing text is rarely straightforward. Texts are full of heteronyms, numbers, and abbreviations that all require expansion into a phonetic representation. There are many spellings in English which are pronounced differently based on context. For example, \"My latest project is to learn how to better project my voice\" contains two pronunciations of \"project\".\n\nMost text-to-speech (TTS) systems do not generate semantic representations of their input texts, as processes for doing so are unreliable, poorly understood, and computationally ineffective. As a result, various heuristic techniques are used to guess the proper way to disambiguate homographs, like examining neighboring words and using statistics about frequency of occurrence.\n\nRecently TTS systems have begun to use HMMs (discussed above) to generate \"parts of speech\" to aid in disambiguating homographs. This technique is quite successful for many cases such as whether \"read\" should be pronounced as \"red\" implying past tense, or as \"reed\" implying present tense. Typical error rates when using HMMs in this fashion are usually below five percent. These techniques also work well for most European languages, although access to required training corpora is frequently difficult in these languages.\n\nDeciding how to convert numbers is another problem that TTS systems have to address. It is a simple programming challenge to convert a number into words (at least in English), like \"1325\" becoming \"one thousand three hundred twenty-five.\" However, numbers occur in many different contexts; \"1325\" may also be read as \"one three two five\", \"thirteen twenty-five\" or \"thirteen hundred and twenty five\". A TTS system can often infer how to expand a number based on surrounding words, numbers, and punctuation, and sometimes the system provides a way to specify the context if it is ambiguous. Roman numerals can also be read differently depending on context. For example, \"Henry VIII\" reads as \"Henry the Eighth\", while \"Chapter VIII\" reads as \"Chapter Eight\".\n\nSimilarly, abbreviations can be ambiguous. For example, the abbreviation \"in\" for \"inches\" must be differentiated from the word \"in\", and the address \"12 St John St.\" uses the same abbreviation for both \"Saint\" and \"Street\". TTS systems with intelligent front ends can make educated guesses about ambiguous abbreviations, while others provide the same result in all cases, resulting in nonsensical (and sometimes comical) outputs, such as \"co-operation\" being rendered as \"company operation\".\n\nSpeech synthesis systems use two basic approaches to determine the pronunciation of a word based on its spelling, a process which is often called text-to-phoneme or grapheme-to-phoneme conversion (phoneme is the term used by linguists to describe distinctive sounds in a language). The simplest approach to text-to-phoneme conversion is the dictionary-based approach, where a large dictionary containing all the words of a language and their correct pronunciations is stored by the program. Determining the correct pronunciation of each word is a matter of looking up each word in the dictionary and replacing the spelling with the pronunciation specified in the dictionary. The other approach is rule-based, in which pronunciation rules are applied to words to determine their pronunciations based on their spellings. This is similar to the \"sounding out\", or synthetic phonics, approach to learning reading.\n\nEach approach has advantages and drawbacks. The dictionary-based approach is quick and accurate, but completely fails if it is given a word which is not in its dictionary. As dictionary size grows, so too does the memory space requirements of the synthesis system. On the other hand, the rule-based approach works on any input, but the complexity of the rules grows substantially as the system takes into account irregular spellings or pronunciations. (Consider that the word \"of\" is very common in English, yet is the only word in which the letter \"f\" is pronounced .) As a result, nearly all speech synthesis systems use a combination of these approaches.\n\nLanguages with a phonemic orthography have a very regular writing system, and the prediction of the pronunciation of words based on their spellings is quite successful. Speech synthesis systems for such languages often use the rule-based method extensively, resorting to dictionaries only for those few words, like foreign names and borrowings, whose pronunciations are not obvious from their spellings. On the other hand, speech synthesis systems for languages like English, which have extremely irregular spelling systems, are more likely to rely on dictionaries, and to use rule-based methods only for unusual words, or words that aren't in their dictionaries.\n\nThe consistent evaluation of speech synthesis systems may be difficult because of a lack of universally agreed objective evaluation criteria. Different organizations often use different speech data. The quality of speech synthesis systems also depends on the quality of the production technique (which may involve analogue or digital recording) and on the facilities used to replay the speech. Evaluating speech synthesis systems has therefore often been compromised by differences between production techniques and replay facilities.\n\nSince 2005, however, some researchers have started to evaluate speech synthesis systems using a common speech dataset.\n\nA study in the journal \"Speech Communication\" by Amy Drahota and colleagues at the University of Portsmouth, UK, reported that listeners to voice recordings could determine, at better than chance levels, whether or not the speaker was smiling. It was suggested that identification of the vocal features that signal emotional content may be used to help make synthesized speech sound more natural. One of the related issues is modification of the pitch contour of the sentence, depending upon whether it is an affirmative, interrogative or exclamatory sentence. One of the techniques for pitch modification uses discrete cosine transform in the source domain (linear prediction residual). Such pitch synchronous pitch modification techniques need a priori pitch marking of the synthesis speech database using techniques such as epoch extraction using dynamic plosion index applied on the integrated linear prediction residual of the voiced regions of speech.\n\nEarly Technology (not available anymore)\n\nCurrent (as of 2013)\n\n\nPopular systems offering speech synthesis as a built-in capability.\n\nThe Mattel Intellivision game console offered the Intellivoice Voice Synthesis module in 1982. It included the SP0256 Narrator speech synthesizer chip on a removable cartridge. The Narrator had 2kB of Read-Only Memory (ROM), and this was utilized to store a database of generic words that could be combined to make phrases in Intellivision games. Since the Orator chip could also accept speech data from external memory, any additional words or phrases needed could be stored inside the cartridge itself. The data consisted of strings of analog-filter coefficients to modify the behavior of the chip's synthetic vocal-tract model, rather than simple digitized samples.\n\nAlso released in 1982, Software Automatic Mouth was the first commercial all-software voice synthesis program. It was later used as the basis for Macintalk. The program was available for non-Macintosh Apple computers (including the Apple II, and the Lisa), various Atari models and the Commodore 64. The Apple version preferred additional hardware that contained DACs, although it could instead use the computer's one-bit audio output (with the addition of much distortion) if the card was not present. The Atari made use of the embedded POKEY audio chip. Speech playback on the Atari normally disabled interrupt requests and shut down the ANTIC chip during vocal output. The audible output is extremely distorted speech when the screen is on. The Commodore 64 made use of the 64's embedded SID audio chip.\n\nArguably, the first speech system integrated into an operating system was the 1400XL/1450XL personal computers designed by Atari, Inc. using the Votrax SC01 chip in 1983. The 1400XL/1450XL computers used a Finite State Machine to enable World English Spelling text-to-speech synthesis. Unfortunately, the 1400XL/1450XL personal computers never shipped in quantity.\n\nThe Atari ST computers were sold with \"stspeech.tos\" on floppy disk.\n\nThe first speech system integrated into an operating system that shipped in quantity was Apple Computer's MacInTalk. The software was licensed from 3rd party developers Joseph Katz and Mark Barton (later, SoftVoice, Inc.) and was featured during the 1984 introduction of the Macintosh computer. This January demo required 512 kilobytes of RAM memory. As a result, it could not run in the 128 kilobytes of RAM the first Mac actually shipped with. So, the demo was accomplished with a prototype 512k Mac, although those in attendance were not told of this and the synthesis demo created considerable excitement for the Macintosh. In the early 1990s Apple expanded its capabilities offering system wide text-to-speech support. With the introduction of faster PowerPC-based computers they included higher quality voice sampling. Apple also introduced speech recognition into its systems which provided a fluid command set. More recently, Apple has added sample-based voices. Starting as a curiosity, the speech system of Apple Macintosh has evolved into a fully supported program, PlainTalk, for people with vision problems. VoiceOver was for the first time featured in Mac OS X Tiger (10.4). During 10.4 (Tiger) & first releases of 10.5 (Leopard) there was only one standard voice shipping with Mac OS X. Starting with 10.6 (Snow Leopard), the user can choose out of a wide range list of multiple voices. VoiceOver voices feature the taking of realistic-sounding breaths between sentences, as well as improved clarity at high read rates over PlainTalk. Mac OS X also includes say, a command-line based application that converts text to audible speech. The AppleScript Standard Additions includes a say verb that allows a script to use any of the installed voices and to control the pitch, speaking rate and modulation of the spoken text.\n\nThe Apple iOS operating system used on the iPhone, iPad and iPod Touch uses VoiceOver speech synthesis for accessibility. Some third party applications also provide speech synthesis to facilitate navigating, reading web pages or translating text.\n\nThe second operating system to feature advanced speech synthesis capabilities was AmigaOS, introduced in 1985. The voice synthesis was licensed by Commodore International from SoftVoice, Inc., who also developed the original MacinTalk text-to-speech system. It featured a complete system of voice emulation for American English, with both male and female voices and \"stress\" indicator markers, made possible through the Amiga's audio chipset. The synthesis system was divided into a translator library which converted unrestricted English text into a standard set of phonetic codes and a narrator device which implemented a formant model of speech generation.. AmigaOS also featured a high-level \"Speak Handler\", which allowed command-line users to redirect text output to speech. Speech synthesis was occasionally used in third-party programs, particularly word processors and educational software. The synthesis software remained largely unchanged from the first AmigaOS release and Commodore eventually removed speech synthesis support from AmigaOS 2.1 onward.\n\nDespite the American English phoneme limitation, an unofficial version with multilingual speech synthesis was developed. This made use of an enhanced version of the translator library which could translate a number of languages, given a set of rules for each language.\n\nModern Windows desktop systems can use SAPI 4 and SAPI 5 components to support speech synthesis and speech recognition. SAPI 4.0 was available as an optional add-on for Windows 95 and Windows 98. Windows 2000 added Narrator, a text–to–speech utility for people who have visual impairment. Third-party programs such as JAWS for Windows, Window-Eyes, Non-visual Desktop Access, Supernova and System Access can perform various text-to-speech tasks such as reading text aloud from a specified website, email account, text document, the Windows clipboard, the user's keyboard typing, etc. Not all programs can use speech synthesis directly. Some programs can use plug-ins, extensions or add-ons to read text aloud. Third-party programs are available that can read text from the system clipboard.\n\nMicrosoft Speech Server is a server-based package for voice synthesis and recognition. It is designed for network use with web applications and call centers.\n\nIn the early 1980s, TI was known as a pioneer in speech synthesis, and a highly popular plug-in speech synthesizer module was available for the TI-99/4 and 4A. Speech synthesizers were offered free with the purchase of a number of cartridges and were used by many TI-written video games (notable titles offered with speech during this promotion were Alpiner and Parsec). The synthesizer uses a variant of linear predictive coding and has a small in-built vocabulary. The original intent was to release small cartridges that plugged directly into the synthesizer unit, which would increase the device's built in vocabulary. However, the success of software text-to-speech in the Terminal Emulator II cartridge cancelled that plan.\n\nText-to-Speech (TTS) refers to the ability of computers to read text aloud. A TTS Engine converts written text to a phonemic representation, then converts the phonemic representation to waveforms that can be output as sound. TTS engines with different languages, dialects and specialized vocabularies are available through third-party publishers.\n\nVersion 1.6 of Android added support for speech synthesis (TTS).\n\nCurrently, there are a number of applications, plugins and gadgets that can read messages directly from an e-mail client and web pages from a web browser or Google Toolbar, such as Text to Voice, which is an add-on to Firefox. Some specialized software can narrate RSS-feeds. On one hand, online RSS-narrators simplify information delivery by allowing users to listen to their favourite news sources and to convert them to podcasts. On the other hand, on-line RSS-readers are available on almost any PC connected to the Internet. Users can download generated audio files to portable devices, e.g. with a help of podcast receiver, and listen to them while walking, jogging or commuting to work.\n\nA growing field in Internet based TTS is web-based assistive technology, e.g. 'Browsealoud' from a UK company and Readspeaker. It can deliver TTS functionality to anyone (for reasons of accessibility, convenience, entertainment or information) with access to a web browser. The non-profit project was created in 2006 to provide a similar web-based TTS interface to the Wikipedia.\n\nOther work is being done in the context of the W3C through the W3C Audio Incubator Group with the involvement of The BBC and Google Inc.\n\nSystems that operate on free and open source software systems including Linux are various, and include open-source programs such as the Festival Speech Synthesis System which uses diphone-based synthesis, as well as more modern and better-sounding techniques, eSpeak, which supports a broad range of languages, and gnuspeech which uses articulatory synthesis from the Free Software Foundation.\n\n\nWith the 2016 introduction of Adobe Voco audio editing and generating software prototype slated to be part of the Adobe Creative Suite and the similarly enabled DeepMind WaveNet, a deep neural network based audio synthesis software from Google \nspeech synthesis is verging on being completely indistinguishable from a real human's voice.\n\nAdobe Voco takes approximately 20 minutes of the desired target's speech and after that it can generate sound-alike voice with even phonemes that were not present in the training material. The software obviously poses ethical concerns as it allows to steal other peoples voices and manipulate them to say anything desired.\n\nThis increases the stress on the disinformation situation coupled with the facts that \n\nA number of markup languages have been established for the rendition of text as speech in an XML-compliant format. The most recent is Speech Synthesis Markup Language (SSML), which became a W3C recommendation in 2004. Older speech synthesis markup languages include Java Speech Markup Language (JSML) and SABLE. Although each of these was proposed as a standard, none of them have been widely adopted.\n\nSpeech synthesis markup languages are distinguished from dialogue markup languages. VoiceXML, for example, includes tags related to speech recognition, dialogue management and touchtone dialing, in addition to text-to-speech markup.\n\nSpeech synthesis has long been a vital assistive technology tool and its application in this area is significant and widespread. It allows environmental barriers to be removed for people with a wide range of disabilities. The longest application has been in the use of screen readers for people with visual impairment, but text-to-speech systems are now commonly used by people with dyslexia and other reading difficulties as well as by pre-literate children. They are also frequently employed to aid those with severe speech impairment usually through a dedicated voice output communication aid.\n\nSpeech synthesis techniques are also used in entertainment productions such as games and animations. In 2007, Animo Limited announced the development of a software application package based on its speech synthesis software FineSpeech, explicitly geared towards customers in the entertainment industries, able to generate narration and lines of dialogue according to user specifications. The application reached maturity in 2008, when NEC Biglobe announced a web service that allows users to create phrases from the voices of characters.\n\nIn recent years, Text to Speech for disability and handicapped communication aids have become widely deployed in Mass Transit. Text to Speech is also finding new applications outside the disability market. For example, speech synthesis, combined with speech recognition, allows for interaction with mobile devices via natural language processing interfaces.\n\nText-to speech is also used in second language acquisition. Voki, for instance, is an educational tool created by Oddcast that allows users to create their own talking avatar, using different accents. They can be emailed, embedded on websites or shared on social media.\n\nIn addition, speech synthesis is a valuable computational aid for the analysis and assessment of speech disorders. A voice quality synthesizer, developed by Jorge C. Lucero et al. at University of Brasilia, simulates the physics of phonation and includes models of vocal frequency jitter and tremor, airflow noise and laryngeal asymmetries. The synthesizer has been used to mimic the timbre of dysphonic speakers with controlled levels of roughness, breathiness and strain.\n\nMultiple companies offer TTS APIs to their customers to accelerate development of new applications utilizing TTS technology. Companies offering TTS APIs include AT&T, CereProc, DIOTEK, IVONA, Neospeech, Readspeaker, SYNVO, YAKiToMe! and CPqD. For mobile app development, Android operating system has been offering text to speech API for a long time. Most recently, with iOS7, Apple started offering an API for text to speech.\n",
    "id": "42799",
    "title": "Speech synthesis"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=33233256",
    "text": "Xaitment\n\nxaitment is a German-based company that develops and sells artificial intelligence (AI) software to video game developers and simulation developers. The company was founded in 2004 by Dr. Andreas Gerber, and is a spin-off of the German Research Centre for Artificial Intelligence, or DFKI. xaitment has its main office in Quierschied, Germany, and field offices in San Francisco and China.\n\nxaitment currently sells two AI software modules: xaitMap and xaitControl. xaitMap provides runtime libraries and graphical tools for navigation mesh generation (also called NavMesh generation), pathfinding, dynamic collision avoidance, and individual and crowd movement. xaitControl is a finite-state machine for game logic and character behavior modeling that also includes a real-time debugger. On January 11, 2012, xaitment announced that it making its source code for these modules available to \"all current and future US and European licensees\".\n\nOn February 22, 2012 xaitment released two new plug-ins, xaitMap and xaitControl for the Unity Game Engine. \nThe full versions are available for PC (Windows and Linux), PlayStation 3, Xbox 360 and Wii. The pathfinding plug-in is available with a Windows dev environment, but can deployed on iOS, Mac, Android and the Unity Web Player.\n\nxaitment's AI software is currently integrated into the Unity game engine, Havok's Vision Engine, Bohemia Interactive's VBS2 Simulation Engine, GameBase's Gamebryo game engine.\n\nxaitment sells its AI software products to video game developers and military and civil simulation developers. Current customers include Tencent, gamania, TML Studios, Emobi Games, IP Keys and others. A full list of customers can be found on xaitment's website.\n\n",
    "id": "33233256",
    "title": "Xaitment"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=13111724",
    "text": "Speech-generating device\n\nSpeech-generating devices (SGDs), also known as voice output communication aids, are electronic augmentative and alternative communication (AAC) systems used to supplement or replace speech or writing for individuals with severe speech impairments, enabling them to verbally communicate. SGDs are important for people who have limited means of interacting verbally, as they allow individuals to become active participants in communication interactions. They are particularly helpful for patients suffering from amyotrophic lateral sclerosis (ALS) but recently have been used for children with predicted speech deficiencies.\n\nThere are several input and display methods for users of varying abilities to make use of SGDs. Some SGDs have multiple pages of symbols to accommodate a large number of utterances, and thus only a portion of the symbols available are visible at any one time, with the communicator navigating the various pages. Speech-generating devices can produce electronic voice output by using digitized recordings of natural speech or through speech synthesis—which may carry less emotional information but can permit the user to speak novel messages.\n\nThe content, organization, and updating of the vocabulary on an SGD is influenced by a number of factors, such at the user's needs and the contexts that the device will be used in. The development of techniques to improve the available vocabulary and rate of speech production is an active research area. Vocabulary items should be of high interest to the user, be frequently applicable, have a range of meanings, and be pragmatic in functionality.\n\nThere are multiple methods of accessing messages on devices: directly or indirectly, or using specialized access devices—although the specific access method will depend on the skills and abilities of the user. SGD output is typically much slower than speech, although rate enhancement strategies can increase the user's rate of output, resulting in enhanced efficiency of communication.\n\nThe first known SGD was prototyped in the mid-1970s, and rapid progress in hardware and software development has meant that SGD capabilities can now be integrated into devices like smartphones. Notable users of SGDs include Stephen Hawking, Roger Ebert, Tony Proudfoot, and Pete Frates (founder of the ALS ice bucket challenge).\n\nSpeech-generating systems may be dedicated devices developed solely for AAC, or non-dedicated devices such as computers running additional software to allow them to function as AAC devices.\n\nSGDs have their roots in early electronic communication aids. The first such aid was a sip-and-puff typewriter controller named the patient-operated selector mechanism (POSSUM) prototyped by Reg Maling in the United Kingdom in 1960. POSSUM scanned through a set of symbols on an illuminated display. Researchers at Delft University in the Netherlands created the lightspot operated typewriter (LOT) in 1970, which made use of small movements of the head to point a small spot of light at a matrix of characters, each equipped with a photoelectric cell. Although it was commercially unsuccessful, the LOT was well received by its users.\n\nIn 1966, Barry Romich, a freshman engineering student at Case Western Reserve University, and Ed Prentke, an engineer at Highland View Hospital in Cleveland, Ohio formed a partnership, creating the Prentke Romich Company. In 1969, the company produced its first communication device, a typing system based on a discarded Teletype machine.\n\nDuring the 1970s and early 1980s, several other companies began to emerge that have since become prominent manufacturers of SGDs. Toby Churchill founded Toby Churchill Ltd in 1973, after losing his speech following encephalitis. In the US, Dynavox (then known as Sentient Systems Technology) grew out of a student project at Carnegie-Mellon University, created in 1982 to help a young woman with cerebral palsy to communicate.\nBeginning in the 1980s, improvements in technology led to a greatly increased number, variety, and performance of commercially available communication devices, and a reduction in their size and price. Alternative methods of access such as Target Scanning (also known as eye pointing) calibrate the movement of a user's eyes to direct an SGD to produce the desired speech phase. Scanning, in which alternatives are presented to the user sequentially, became available on communication devices. Speech output possibilities included both digitized and synthesized speech.\n\nRapid progress in hardware and software development continued, including projects funded by the European Community. The first commercially available dynamic screen speech generating devices were developed in the 1990s. Software programs were developed that allowed the computer-based production of communication boards. High-tech devices have continued to become smaller and lighter, while increasing accessibility and capability; communication devices can be accessed using eye-tracking systems, perform as a computer for word-processing and Internet use, and as an environmental control device for independent access to other equipment such as TV, radio and telephones.\n\nNotable individuals who have used AAC devices include Stephen Hawking, Roger Ebert Tony Proudfoot, and Pete Frates. Hawking is unable to speak due to a combination of severe disabilities caused by ALS, and an emergency tracheotomy. He has come to be associated with the unique voice of his particular synthesis equipment. In the past 20 or so years SGD have gained popularity amongst young children with speech deficiencies, such as autism, Down syndrome, and predicted brain damage due to surgery.\n\nStarting in the early 2000s, specialist saw the benefit of using SGDs not only for adults but for children, as well. Neuro-linguists found that SGDs were just as effective in helping children who were at risk for temporary language deficits after undergoing brain surgery as it is for patients with ALS. In particular, digitized SGDs have been used as communication aids for pediatric patients during the recovery process.\n\nThere are many methods of accessing messages on devices: directly, indirectly, and with specialized access devices. Direct access methods involve physical contact with the system, by using a keyboard or a touch screen. Users accessing SGDs indirectly and through specialized devices must manipulate an object in order to access the system, such as maneuvering a joystick, head mouse, optical head pointer, light pointer, infrared pointer, or switch access scanner.\n\nThe specific access method will depend on the skills and abilities of the user. With direct selection a body part, pointer, adapted mouse, joystick, or eye tracking could be used, whereas switch access scanning is often used for indirect selection. Unlike direct selection (e.g., typing on a keyboard, touching a screen), users of Target Scanning can only make selections when the scanning indicator (or cursor) of the electronic device is on the desired choice. Those who are unable to point typically calibrate their eyes to use eye gaze as a way to point and blocking as a way to select desired words and phrases. The speed and pattern of scanning, as well as the way items are selected, are individualized to the physical, visual and cognitive capabilities of the user.\n\nAugmentative and alternative communication is typically much slower than speech, with users generally producing 8–10 words per minute. Rate enhancement strategies can increase the user's rate of output to around 12–15 words per minute, and as a result enhance the efficiency of communication.\n\nIn any given SGD there may be a large number of vocal expressions that facilitate efficient and effective communication, including greetings, expressing desires, and asking questions. Some SGDs have multiple pages of symbols to accommodate a large number of vocal expressions, and thus only a portion of the symbols available are visible at any one time, with the communicator navigating the various pages. Speech-generating devices generally display a set of selections either using a dynamically changing screen, or a fixed display.\n\nThere are two main options for increasing the rate of communication for an SGD: encoding and prediction.\n\nEncoding permits a user to produce a word, sentence or phrase using only one or two activations of their SGD. Iconic encoding strategies such as Semantic compaction combine sequences of icons (picture symbols) to produce words or phrases. In numeric, alpha-numeric, and letter encoding (also known as Abbreviation-Expansion), words and sentences are coded as sequences of letters and numbers. For example, typing \"HH\" or \"G1\" (for Greeting 1) may retrieve \"Hello, how are you?\".\n\nPrediction is a rate enhancement strategy in which the SGD attempts to reduce the number of keystrokes used by predicting the word or phrase being written by the user. The user can then select the correct prediction without needing to write the entire word. Word prediction software may determine the choices to be offered based on their frequency in language, association with other words, past choices of the user, or grammatical suitability. However, users have been shown to produce more words per minute (using a scanning interface) with a static keyboard layout than with a predictive grid layout, suggesting that the cognitive overhead of reviewing a new arrangement cancels out the benefits of the predictive layout when using a scanning interface.\n\nSome systems, such as Auditory Sciences' Interact-Voice device, combine Encoding and Prediction into the same system. For example, typing \"HMF\" can be an encoded shortcut for \"Can you help me find ____\" and then the prediction capabilities help the user complete the sentence, such as \"Can you help me find my glasses?\" or \"Can you help me find my car keys?\".\n\nAnother approach to rate-enhancement is Dasher, which uses language models and arithmetic coding to present alternative letter targets on the screen with size relative to their likelihood given the history.\n\nThe rate of words produced can depend greatly on the conceptual level of the system: the TALK system, which allows users to choose between large numbers of sentence-level utterances, demonstrated output rates in excess of 60 wpm.\n\nFixed display devices refer to those in which the symbols and items are \"fixed\" in a particular format; some sources refer to these as \"static\" displays. Such display devices have a simpler learning curve than some other devices.\n\nFixed display devices replicate the typical arrangement of low-tech AAC devices (low-tech is defined as those devices that do not need batteries, electricity or electronics), like communication boards. They share some of disadvantages; for example they are typically restricted to a limited number of symbols and hence messages. It is important to note that with technological advances made in the twenty-first century, fixed-display SGDs are not commonly used anymore.\n\nDynamic displays devices are usually also touchscreen devices. iPads, Tobii Technology and Words+. devices are the most commonly used Dynamic Display Devices. They typically generate electronically produced visual symbols that, when pressed, change the set of selections that is displayed. The user can change the symbols available using page links to navigate to appropriate pages of vocabulary and messages. \n\nThe \"home\" page of a dynamic display device may show symbols related to many different contexts or conversational topics. Pressing any one of these symbols may open a different screen with messages related to that topic. For example, when watching a volleyball game, a user may press the \"sport\" symbol to open a page with messages relating to sport, then press the symbol showing a scoreboard to utter the phrase \"What's the score?\".\n\nAdvantages of dynamic display devices include the availability of a much larger vocabulary, and the ability to see the sentence under construction A further advantage of dynamic display devices is that the underlying operating system is capable of providing options for multiple communication channels, including cell phone, text messaging and e-mail. Work by Linköping University has shown that such email writing practices allowed children who were SGD users to develop new social skills and increase their social participation.\n\nLow cost systems can also include a keyboard and audio speaker combination without a dynamic display or visual screen. This type of keyboard sends typed text direct to an audio speaker. It can permit any phrase to be spoken without the need for a visual screen that is not always required. One simple benefit is that a talking keyboard, when used with a standard telephone or speakerphone can enable a voice impaired individual have 2 way conversation over a telephone.\n\nThe output of a SGD may be digitized and/or synthesized: digitized systems play directly recorded words or phrases while synthesized speech uses text-to-speech software that can carry less emotional information but permits the user to speak novel messages by typing new words. Today, individuals use a combination of recorded messages and text-to-speech techniques on their SGDs. However, some devices are limited to only one type of output.\n\nWords, phrases or entire messages can be digitised and stored onto the device for playback to be activated by the user. This process is formally known as Voice Banking. Advantages of recorded speech include that it (a) provides natural prosody and speech naturalness for the listener (e.g., person of the same age and gender as the AAC user can be selected to record the messages), and (b) it provides for additional sounds that may be important for the user such as laughing or whistling. Moreover, Digitized SGDs is that they provide a degree of normalcy both for the patient and for their families when they lose their ability to speak on their own.\n\nA major disadvantage of using only recorded speech is that users are unable to produce novel messages; they are limited to the messages pre-recorded into the device. Depending on the device, there may be a limit to the length of the recordings.\n\nSGDs that use synthesized speech apply the phonetic rules of the language to translate the user’s message into voice output (speech synthesis). Users have the freedom to create novel words and messages and are not limited to those that have been pre-recorded on their device by others.\n\nSmartphones and computers have increased the use of synthesized speech devices through the creation of apps that allow the user to select from a list of phrases or messages to be spoken in the voice and language that the user has chosen. Apps such as SpeakIt! or Assistive Express for iPhone provide a cheap way to use a speech-generating device without having to visit a doctor's office or learn to use specialized machinery.\n\nSynthesized SGDs may allow multiple methods of message creation that can be used individually or in combination: messages can be created from letters, words, phrases, sentences, pictures, or symbols. With synthesized speech there is virtually unlimited storage capacity for messages with few demands on memory space.\n\nSynthesized speech engines are available in many languages, and the engine's parameters, such as speech rate, pitch range, gender, stress patterns, pauses, and pronunciation exceptions can be manipulated by the user.\n\nThe selection set of a SGD is the set of all messages, symbols and codes that are available to a person using that device. The content, organisation, and updating of this selection set are areas of active research and are influenced by a number of factors, including the user's ability, interests and age. The selection set for an AAC system may include words that the user does not know yet – they are included for the user to \"grow into\". The content installed on any given SGD may include a large number of preset pages provided by the manufacturer, with a number of additional pages produced by the user or the user's care team depending on the user's needs and the contexts that the device will be used in.\n\nResearchers Beukelman and Mirenda list a number of possible sources (such as family members, friends, teachers, and care staff) for the selection of initial content for a SGD. A range of sources is required because, in general, one individual would not have the knowledge and experience to generate all the vocal expressions needed in any given environment. For example, parents and therapists might not think to add slang terms, such as \"innit\".\n\nPrevious work has analyzed both vocabulary use of typically developing speakers and word use of AAC users to generate content for new AAC devices. Such processes work well for generating a core set of utterances or vocal expressions but are less effective in situations where a particular vocabulary is needed (for example, terms related directly to a user's interest in horse riding). The term \"fringe vocabulary\" refers to vocabulary that is specific or unique to the individual's personal interests or needs. A typical technique to develop fringe vocabulary for a device is to conduct interviews with multiple \"informants\": siblings, parents, teachers, co-workers and other involved persons.\n\nOther researchers, such as Musselwhite and St. Louis suggest that initial vocabulary items should be of high interest to the user, be frequently applicable, have a range of meanings and be pragmatic in functionality. These criteria have been widely used in the AAC field as an ecological check of SGD content.\n\nBeukelman and Mirenda emphasize that vocabulary selection also involves ongoing vocabulary maintenance; however, a difficulty in AAC is that users or their carers must program in any new utterances manually (e.g. names of new friends or personal stories) and there are no existing commercial solutions for automatically adding content. A number of research approaches have attempted to overcome this difficulty, these range from \"inferred input\", such as generating content based on a log of conversation with a user's friends and family, to data mined from the Internet to find language materials, such as the Webcrawler Project. Moreover, by making use of Lifelogging based approaches, a device's content can be changed based on events that occur to a user during their day. By accessing more of a user's data, more high-quality messages can be generated at a risk of exposing sensitive user data. For example, by making use of global positioning systems, a device's content can be changed based on geographical location.\n\nMany recently developed SGDs include performance measurement and analysis tools to help monitor the content used by an individual. This raises concerns about privacy, and some argue that the device user should be involved in the decision to monitor use in this way. Similar concerns have been raised regarding the proposals for devices with automatic content generation, and privacy is increasingly a factor in design of SGDs. As AAC devices are designed to be used in all areas of a user’s life, there are sensitive legal, social, and technical issues centred on a wide family of personal data management problems that can be found in contexts of AAC use. For example, SGDs may have to be designed so that they support the user's right to delete logs of conversations or content that has been added automatically.\n\nProgramming of Dynamic Speech Generating devices is usually done by augmentative communication specialists. Specialists are required to cater to the needs of the patients because the patients usually choose what kinds of words/ phrases they want. For example, patients use different phrases based on their age, disability, interests, etc. Therefore, content organization is extremely time consuming. Additionally, SGDs are rarely covered by health insurance companies. As a result, resources are very limited with regards to both funding and staffing. Dr. John Costello of Boston Children’s Hospital has been the driving force soliciting donations to keep these program running and well-staffed both within his hospital and in hospitals across the country.\n\n",
    "id": "13111724",
    "title": "Speech-generating device"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=34340886",
    "text": "ETAP-3\n\nETAP-3 is a proprietary linguistic processing system focusing on English and Russian. It was developed in Moscow, Russia at the Institute for Information Transmission Problems (). It is a rule-based system which uses the Meaning-Text Theory as its theoretical foundation. At present, there are several applications of ETAP-3, such as a machine translation tool, a converter of the Universal Networking Language, an interactive learning tool for Russian language learners and a syntactically annotated corpus of Russian language. Demo versions of some of these tools are available online.\n\nThe ETAP-3 machine translation tool can translate text from English into Russian and vice versa. It is a rule-based system which makes it different from the most present-day systems that are predominantly statistical-based. The system makes a syntactical analysis of the input sentence which can be visualized as a syntax tree.\n\nThe machine translation tool uses bilingual dictionaries which contain more than 100,000 lexical entries.\n\nThe UNL converter based on ETAP-3 can transform English and Russian sentences into there representations in UNL (\"Universal Networking Language\") and generate English and Russian sentences from their UNL representations.\n\nA syntactically annotated corpus (treebank) is a part of Russian National Corpus. It contains 40,000 sentences (600,000 words) which are fully syntactically and morphologically annotated. The primary annotation was made by ETAP-3 and then manually verified by competent linguists. This makes the syntactically annotated corpus a reliable tool for linguistic research.\n\nThe ETAP-3 system makes extensive use of lexical functions explored in the Meaning-Text Theory. For this reason, an interactive tool for Russian language learners aiming at the acquisition of lexical functions has been developed. Such learning tools are now being created for German, Spanish and Bulgarian\n",
    "id": "34340886",
    "title": "ETAP-3"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=18863997",
    "text": "Natural-language user interface\n\nNatural-language user interfaces (LUI or NLUI) are a type of computer human interface where linguistic phenomena such as verbs, phrases and clauses act as UI controls for creating, selecting and modifying data in software applications.\n\nIn interface design, natural-language interfaces are sought after for their speed and ease of use, but most suffer the challenges to understanding wide varieties of ambiguous input.\nNatural-language interfaces are an active area of study in the field of natural-language processing and computational linguistics. An intuitive general natural-language interface is one of the active goals of the Semantic Web.\n\nText interfaces are \"natural\" to varying degrees. Many formal (un-natural) programming languages incorporate idioms of natural human language. Likewise, a traditional keyword search engine could be described as a \"shallow\" natural-language user interface.\n\nA natural-language search engine would in theory find targeted answers to user questions (as opposed to keyword search). For example, when confronted with a question of the form 'which U.S. state has the highest income tax?', conventional search engines ignore the question and instead search on the keywords 'state', 'income' and 'tax'. Natural-language search, on the other hand, attempts to use natural-language processing to understand the nature of the question and then to search and return a subset of the web that contains the answer to the question. If it works, results would have a higher relevance than results from a keyword search engine.\n\nPrototype Nl interfaces had already appeared in the late sixties and early seventies.\n\n\nNatural-language interfaces have in the past led users to anthropomorphize the computer, or at least to attribute more intelligence to machines than is warranted. On the part of the user, this has led to unrealistic expectations of the capabilities of the system. Such expectations will make it difficult to learn the restrictions of the system if users attribute too much capability to it, and will ultimately lead to disappointment when the system fails to perform as expected as was the case in the AI winter of the 1970s and 80s.\n\nA 1995 paper titled 'Natural Language Interfaces to Databases – An Introduction', describes some challenges:\n\nOther goals to consider more generally are the speed and efficiency of the interface, in all algorithms these two points are the main point that will determine if some methods are better than others and therefore have greater success in the market. In addition, localisation across multiple language sites requires extra consideration - this is based on differing sentence structure and language syntax variations between most languages.\n\nFinally, regarding the methods used, the main problem to be solved is creating a general algorithm that can recognize the entire spectrum of different voices, while disregarding nationality, gender or age. The significant differences between the extracted features - even from speakers who says the same word or phrase - must be successfully overcome.\n\nThe natural-language interface gives rise to technology used for many different applications.\n\nSome of the main uses are:\n\n\nBelow are named and defined some of the applications that use natural-language recognition, and so have integrated utilities listed above.\n\nUbiquity, an add-on for Mozilla Firefox, is a collection of quick and easy natural-language-derived commands that act as mashups of web services, thus allowing users to get information and relate it to current and other webpages.\n\nWolfram Alpha is an online service that answers factual queries directly by computing the answer from structured data, rather than providing a list of documents or web pages that might contain the answer as a search engine would. It was announced in March 2009 by Stephen Wolfram, and was released to the public on May 15, 2009.\n\nSiri is an intelligent personal assistant application integrated with operating system iOS. The application uses natural language processing to answer questions and make recommendations.\n\nSiri's marketing claims include that it adapts to a user's individual preferences over time and personalizes results, and performs tasks such as making dinner reservations while trying to catch a cab.\n\n\n\n",
    "id": "18863997",
    "title": "Natural-language user interface"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=34937796",
    "text": "Interactions Corporation\n\nInteractions LLC is a privately held technology company that builds and delivers hosted Virtual Assistant applications that enable businesses to deliver automated natural language communications for enterprise customer care.\n\nInteractions LLC was founded in 2004 and is headquartered in Franklin, Massachusetts. Since inception, Interactions has raised $167M and is venture-backed by Sigma Partners, Cross Atlantic Capital Partners, Updata Partners, North Hill Ventures, Revolution Growth, NewSpring Capital, and Comcast Ventures. Michael Iacobucci serves as Interactions' CEO. Interactions has additional offices in Indiana, Michigan, New Jersey and New York.\n\nIn 2010, Interactions received PCI DSS compliance validation.\n\nIn April 2012, Interactions was named a 2012 Gartner 'Cool Vendor' in CRM Customer Service and Social.\n\nIn November 2014, Interactions announced the acquisition of AT&T's Speech and Language Technology group, along with its AT&T Watson(SM) platform. \n\nIn June 2016 the company announced a new service named \"Curo Speech and Language Platform\", based on the acquired AT&T technology, to provide enhanced natural language understanding within its products. Later in 2016 the company updated its Virtual Assistant offering to include a voice biometric add-on, which helps prevent fraud in automated Virtual Assistant interactions.\n\nIn May 2017, Interactions acquired AI-based social media engagement innovator Digital Roots. Interactions Digital Roots uses social AI, natural language processing and machine learning to help global household brands engage with consumers on social media. In Septepmber 2017, Interactions was named to the Forbes Next Billion-Dollar Startups list.\n\nInteractions' hosted Intelligent Virtual Assistants combine artificial intelligence and human understanding to enable businesses and consumers to engage in productive conversations. Operating under a SaaS business model, Interactions' service includes both an application's design and build, as well as continuous operating and tuning. Services created can be compared to services provided by Apple’s Siri, Amazon Alexa, and Google Now.\n\nInteractions' customer base includes multiple consumer-facing Fortune 500 companies, including Hyatt, TXU Energy, The Salt River Project, Humana, Westar Energy, and LifeLock. Interactions customers represent the Telecommunication, Hospitality, Finance, Insurance and Warranty, Retail, Utility, Consumer Software and Electronics, and Healthcare industries.\n",
    "id": "34937796",
    "title": "Interactions Corporation"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=35591037",
    "text": "Marketing and artificial intelligence\n\nArtificial intelligence is a field of study that “seeks to explain and emulate intelligent behaviour in terms of computational processes” through performing the tasks of decision making, problem solving and learning. Unlike other fields associated with intelligence, Artificial intelligence is concerned with both understanding and building of intelligent entities, and has the ability to automate intelligent processes. It is evident that Artificial intelligence is impacting on a variety of subfields and wider society. However literature regarding its application to the field of marketing appears to be scarce.\n\nAdvancements in Artificial intelligence’s application to a range of disciplines have led to the development of Artificial intelligence systems which have proved useful to marketers. These systems assist in areas such as market forecasting, automation of processes and decision making and increase the efficiency of tasks which would usually be performed by humans. The science behind these systems can be explained through neural networks and expert systems which are computer programs that process input and provide valuable output for marketers. \n\nArtificial intelligence systems stemming from Social computing technology can be applied to understand social networks on the Web. Data mining techniques can be used to analyze different types of social networks. This analysis helps a marketer to identify influential actors or nodes within networks, this information can then be applied to take a Societal marketing approach.\n\nArtificial intelligence has gained significant recognition in the marketing industry. However, ethical issues surrounding these systems and their potential to impact on the need for humans in the workforce, specifically marketing, is a controversial topic.\n\nAn artificial neural network is a form of computer program modelled on the brain and nervous system of humans. Neural networks are composed of a series of interconnected processing neurons functioning in unison to achieve certain outcomes. \nUsing “human-like trial and error learning methods neural networks detect patterns existing within a data set ignoring data that is not significant, while emphasising the data which is most influential”.\n\nFrom a marketing perspective, neural networks are a form of software tool used to assist in decision making. Neural networks are effective in gathering and extracting information from large data sources and have the ability to identify the cause and effect within data. These neural nets through the process of learning, identify relationships and connections between data bases. Once knowledge has been accumulated, neural networks can be relied on to provide generalisations and can apply past knowledge and learning to a variety of situations.\n\nNeural networks help fulfil the role of marketing companies through effectively aiding in market segmentation and measurement of performance while reducing costs and improving accuracy. Due to their learning ability, flexibility, adaption and knowledge discovery, neural networks offer many advantages over traditional models. Neural networks can be used to assist in pattern classification, forecasting and marketing analysis.\n\nClassification of customers can be facilitated through the neural network approach allowing companies to make informed marketing decisions. An example of this was employed by Spiegel Inc., a firm dealing in direct-mail operations who used neural networks to improve efficiencies. Using software developed by NeuralWare Inc., Spiegel identified the demographics of customers who had made a single purchase and those customers who had made repeat purchases. Neural networks where then able to identify the key patterns and consequently identify the customers that were most likely to repeat purchase. Understanding this information allowed Speigel to streamline marketing efforts, and reduced costs.\n\nSales forecasting “is the process of estimating future events with the goal of providing benchmarks for monitoring actual performance and reducing uncertainty\". Artificial intelligence techniques have emerged to facilitate the process of forecasting through increasing accuracy in the areas of demand for products, distribution, employee turnover, performance measurement and inventory control. An example of forecasting using neural networks is the Airline Marketing Assistant/Tactician; an application developed by BehabHeuristics which allows for the forecasting of passenger demand and consequent seat allocation through neural networks. This system has been used by Nationalair Canada and USAir.\n\nNeural networks provide a useful alternative to traditional statistical models due to their reliability, time-saving characteristics and ability to recognise patterns from incomplete or noisy data. Examples of marketing analysis systems include the Target Marketing System developed by Churchull Systems for Veratex Corporation. This support system scans a market database to identify dormant customers allowing management to make decisions regarding which key customers to target.\n\nWhen performing marketing analysis, neural networks can assist in the gathering and processing of information ranging from consumer demographics and credit history to the purchase patterns of consumers.\n\nMarketing is a complex field of decision making which involves a large degree of both judgment and intuition on behalf of the marketer. The enormous increase in complexity that the individual decision maker faces renders the decision making process almost an impossible task. Marketing decision engine can help distill the noise. The generation of more efficient management procedures have been recognized as a necessity. The application of Artificial intelligence to decision making through a Decision Support System has the ability to aid the decision maker in dealing with uncertainty in decision problems. Artificial intelligence techniques are increasingly extending decision support through analyzing trends; providing forecasts; reducing information overload; enabling communication required for collaborative decisions, and allowing for up-to-date information.\n\nOrganizations’ strive to satisfy the needs of the customers, paying specific attention to their desires. A consumer-orientated approach requires the production of goods and services that align with these needs. Understanding consumer behaviour aids the marketer in making appropriate decisions. Thus, the decision making is dependent on the marketing problem, the decision maker, and the decision environment.\n\nAn Expert System is a software program that combines the knowledge of experts in an attempt to solve problems through emulating the knowledge and reasoning procedures of the experts. Each expert system has the ability to process data, and then through reasoning, transform it into evaluations, judgments and opinions, thus providing advises to specialized problems.\n\nThe use of an expert system that applies to the field of marketing is MARKEX (Market Expert). These Intelligent decision support systems act as consultants for marketers, supporting the decision maker in different stages, specifically in the new product development process. The software provides a systematic analysis that uses various methods of forecasting, data analysis and multi-criteria decision making to select the most appropriate penetration strategy. BRANDFRAME is another example of a system developed to assist marketers in the decision-making process. The system supports a brand manager in terms of identifying the brand’s attributes, retail channels, competing brands, targets and budgets. New marketing input is fed into the system where BRANDFRAME analyses the data. Recommendations are made by the system in regard to marketing mix instruments, such as lowering the price or starting a sales promotional campaign.\n\nIn terms of marketing, automation uses software to computerize marketing processes that would have otherwise been performed manually. It assists in effectively allowing processes such as customer segmentation, campaign management and products promotion, to be undertaken at a more efficient rate. Marketing automation is a key component of Customer Relationship Management (CRM). Companies are using systems that employ data-mining algorithms that analyses the customer database, giving further insight into the customer. This information may refer to socio-economic characteristics, earlier interactions with the customer, and information about the purchase history of the customer. \nVarinos Systems have been designed to give organizations control over their data. Automation tools allow the system to monitor the performance of campaigns, making regular adjustments to the campaigns to improve response rates and to provide campaign performance tracking.\n\nDistribution of products requires companies to access accurate data so they are able to respond to fluctuating trends in product demand. Automation processes are able to provide a comprehensive system that improves real-time monitoring and intelligent control. \nAmazon acquired Kiva Systems, the makers of the warehouse robot for $775 million in 2012. Prior to the purchase of the automated system, human employees would have to walk the enormous warehouse, tracking and retrieving books. The Kiva robots are able to undertake order fulfillment, product replenishment, as well as heavy lifting, thus increasing efficiency for the company.\n\nA social network is a social arrangement of actors who make up a group, within a network; there can be an array of ties and nodes that exemplifies common occurrences within a network and common relationships. Lui (2011), describes a social network as, “the study of social entities (people in organization, called actors), and their interactions and relationships. The interactions and relationships can be represented with a network or graph, where each vertex (or node) represents an actor and each link represents a relationship.” At the present time there is a growth in virtual social networking with the common emergence of social networks being replicated online, for example social networking sites such as Twitter, Facebook and LinkedIn. From a marketing perspective, analysis and simulation of these networks can help to understand consumer behavior and opinion. The use of Agent-based social simulation techniques and data/opinion mining to collect social knowledge of networks can help a marketer to understand their market and segments within it.\n\nSocial computing is the branch of technology that can be used by marketers to analyze social behaviors within networks and also allows for creation of artificial social agents. Social computing provides the platform to create social based software; some earlier examples of social computing are such systems that allow a user to extract social information such as contact information from email accounts e.g. addresses and companies titles from ones email using Conditional Random Field (CRFs) technology.\n\nData mining involves searching the Web for existing information namely opinions and feelings that are posted online among social networks. “ This area of study is called opinion mining or sentiment analysis. It analyzes peoples opinions, appraisals, attitudes, and emotions toward entities, individuals, issues, events, topics, and their attributes”. However searching for this information and analysis of it can be a sizeable task, manually analyzing this information also presents the potential for researcher bias. Therefore, objective opinion analysis systems are suggested as a solution to this in the form of automated opinion mining and summarization systems. Marketers using this type of intelligence to make inferences about consumer opinion should be wary of what is called opinion spam, where fake opinions or reviews are posted in the web in order to influence potential consumers for or against a product or service.\n\nSearch engines are a common type of intelligence that seeks to learn what the user is interested in to present appropriate information. PageRank and HITS are examples of algorithms that search for information via hyperlinks; Google uses PageRank to control its search engine. Hyperlink based intelligence can be used to seek out web communities, which is described as ‘ a cluster of densely linked pages representing a group of people with a common interest’.\n\nCentrality and prestige are types of measurement terms used to describe the level of common occurrences among a group of actors; the terms help to describe the level of influence and actor holds within a social network. Someone who has many ties within a network would be described as a ‘central’ or ‘prestige’ actor. Identifying these nodes within a social network is helpful for marketers to find out who are the trendsetters within social networks.\n",
    "id": "35591037",
    "title": "Marketing and artificial intelligence"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=36987204",
    "text": "Kasparov's Gambit\n\nKasparov's Gambit or simply Gambit is a chess playing computer program created by Heuristic Software and published by Electronic Arts in 1993 based on Socrates II, the only winner of the North American Computer Chess Championship running on a common microcomputer. It was designed for MS-DOS while Garry Kasparov reigned as world champion, whose involvement and support was its key allure.\n\nJulio Kaplan, chessplayer, computer programmer, and owner of the company 'Heuristic Software', first developed Heuristic Alpha in 1990-91. The original version evolved into \"Socrates\" with the help of other chess players and programmers including Larry Kaufman and Don Dailey, who, later, were also developers of \"Kasparov's Gambit\".\n\nImprovements to \"Socrates\" were reflected in a version called \"Titan\", renamed for competition as \"Socrates II\", the most successful of the series winning the 1993 ACM International Chess Championship. During the course of the championship \"Socrates II\", which was running on a stock 486 PC, defeated opponents with purpose-built hardware and software for playing chess, including HiTech and Cray Blitz.\n\nElectronic Arts purchased \"Socrates II\" and hired its creators to build a new product, \"Kasparov's Gambit\", including Kasparov as consultant and brand. It was the company's effort to enter the chess programs market, dominated at the time by \"Chessmaster 3000\" and \"Blitz\". In 1993 it went on sale, but contained a number of bugs, so was patched at the end of that year. The patched version ran at about 75% of the speed of \"Socrates II\" which was quite an achievement considering the whole functionality of the software was sharing the same computer resources.\n\nIn 1993 it competed in the Harvard Cup (six humans versus six programs) facing grandmasters who had ratings ranging from 2515 to 2625 ELO. It finished the competition in 12th and last place. Grandmasters took the first five places and another \"Socrates\" derivation - \"Socrates Exp\" - was the best program finishing in 6th place.\nAccording to team developer Eric Schiller, a Windows version was planned by Electronic Arts, but was never finished. Excluding chess-style board games like (1983) or Battle Chess II: Chinese Chess (2002), Kasparov's Gambit remains the sole effort of Electronic Arts to enter the classic chess software market.\n\n\"Computer Gaming World\" in 1993 approved of \"Kasparov's Gambit\"s \"stunning\" SVGA graphics, Socrates II engine, and coaching features, concluding that it was \"above any PC game on the market\". It was a runner-up for the magazine's Strategy Game of the Year award in June 1994, losing to \"Master of Orion\". The editors called \"Kasparov's Gambit\" \"beautifully crafted\", a \"great teacher\" and \"a chess game for the 'rest of us.'\" It holds the 147th place in \"Computer Gaming World\"s 1996 list of \"150 Best Games of All Time\".\n\nRegarding Garry Kasparov's successful title defense against Nigel Short in the same year, followed by its triumph at the 1993 International Computer Chess Championship and its user-friendly capabilities, \"Gambit\" failed in sales and marked the end of Electronic Arts attempts to produce chess games.\n\n\"Gambit\" was intended to have the capabilities of a champion level software and a teaching tool for a wide range of player levels. It was Electronic Arts' first use of windowed video showing digitized images, video and voice of champ Garry Kasparov giving advice and commenting on player moves.\n\nPrimary features include:\n\nThe human strength rating is calculated using Elo formula with the included personalities and the one of player himself/herself, going from 800 to 2800 points. New players get a customizable 800 ELO, which changes according to the total number of games played, opponents strength and result of game.\n\nCreation of personalities enables five adjustable characteristics in percentage (0-100%)—strength, orthodoxy, creativity, focus and aggresivness—which define, besides its style, its ELO rating. User ELO is calculated according to \"Gambit's\" universe of electronic players and user him/herself, thus do not match rankings in real world, instead this feature was designed to provide a useful way to measure player strength and progress against \"Gambit\".\n\nBesides 125 tutorials, written by renowned chess author and developer Eric Schiller, classified in openings, middle game, endgames (checkmates), tactics and strategy also include a \"Famous Games\" database, a list of all-time world champions games commented by Kasparov with a quiz option where user must choose the next move.\n\nWas designed for 386SX IBM AT compatible systems. Even when it's capable to read commands from keyboard or mouse, the use of mouse is recommended. During the days it was released, \"Kasparov's Gambit\" offered a nice \"look & feel\" experience using SVGA mode with 640x480 resolution and 256 colors and voice/video recordings of world champion Garry Kasparov. A lack of soundcards support was reported by users.\n\nIt is playable in DOSBox emulator since 0.61 version over Linux and other Unix-like operating systems, Windows XP and subsequent versions and Mac OS X.\n\nFirst intention was using \"Heuristic Alpha\" as \"Gambit\"'s base, but unexpected good performance of \"Socrates II\" in tournaments made of it the final choice. According to developer and tester Larry Kauffman \"first released included important bugs, that Knowledge of bishop mobility appears to be missing, as does some other chess knowledge, and Gambit appears to run only about 50-60% of the speed of the ACM program in positions (without bishops) where the two do play and evaluate identically. There are also bugs in the features and the time controls, and the program is rather difficult to use (perhaps because it has so many features). One good thing I can say is that the 3d graphics are superb... I have tested the patched version, and have confirmed that most or all of the bugs have been corrected. The new version does play identically to the ACM program and runs at 70-75% of the speed, so it should rate just 30 points below the ACM program.\"\n\"Socrates II\" engine was fully programmed in assembly language, but rewritten just in C language for \"Kasparov's Gambit\" engine. Instead, assembly language was used for sound and video capabilities, as for other functionalities.\n\n\n",
    "id": "36987204",
    "title": "Kasparov's Gambit"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=19089876",
    "text": "Grandmaster Chess\n\nGrandmaster Chess is a 1992 video game to play chess for PC DOS platform develop by IntraCorp and its subsidiary Capstone that was focused on neural network technology and an artificial intelligence (AI) able to learn from mistakes.\n\nCapable of using VGA and SVGA modes, features multiple skill levels, different sets of pieces, boards and backgrounds, 2D/3D view, pull-down menus, move list with VCR style control, able to analysis moves and games and rate the user strength. Originally it was distributed in floppy discs, but in 1993 in appeared in CD-ROM. This release only relevant addition was the \"Terminator 2: Judgement Day: Chess Wars\" package, an animated chess set like \"Battle Chess\" video game representing the movie.\n\n\"Computer Gaming World\" stated that \"Grandmaster Chess\" \"falls short of the current competition in terms of overall options\". The magazine criticized the game's weak strategic analysis reporting, the absence of an advertised teaching mode, and weak opening book.\n\n\n",
    "id": "19089876",
    "title": "Grandmaster Chess"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=18015568",
    "text": "GNOME Chess\n\nGNOME Chess (formerly glChess) is a graphical front-end featuring a 2D and a 3D chessboard interface. GNOME Chess does not comprise an own chess engine and to play against the computer a third party chess engine must be present, but most Linux distributions package GNU Chess as the default chess engine with it. Additionally GNOME Chess supports third party chess engines, known ones are automatically detected.\n\nGNOME Chess is written in Vala. For 2D rendering it uses GTK+ and Cairo/librsvg, and 3D support is optionally available using OpenGL.\n\nAs part of the GNOME desktop environment and GNOME Games, GNOME Chess is free and open-source software subject to the terms of the GNU General Public License (GPL) version 2.\n\nGNOME Chess supports a plethora of chess engines, such as:\n\nglChess, the predecessor to GNOME Chess, can be used with any other CECP and Universal Chess Interface compatible software like:\n\n\"glChess\" was written by Robert Ancell in 2000 only as a personal project to test open source development.\n\nFirst version was written in C, OpenGL for graphics, and GLUT for the user interface. In May 5 was released 0.1.0, the first but still not playable version, being only capable to draw board and pieces. Days later, on May 31, version 0.1.3 was finally included on SourceForge and playable on a very basic way.\n\nOn April 8, 2001 version 0.2.0 changed GLUT to GTK+ focusing the improvement in visual aspects instead of its chess artificial intelligence. Version 0.3.0, from June 27, could play against other artificial intelligence (AI) engines, like Crafty and GNU Chess, after a Chess Engine Communication Protocol (CECP) implementation and it was ported to IRIX platform. In December, version 0.4.0 was the last one before the project entered into a stand-by time of three years.\n\nIn December, 2004, there was an advance to version 0.8.0 in order to accelerate the achievement the 1.0. This version added network support and updated GTK+ from version 1.2 to 2.0.\n\nOne year later, December 2005, version 0.9.0 was intended to be the last release before 1.0. It replaced C for Python to improve platform portability and maintenance, besides having a better test approach of the codebase testing.\n\nOn December 16, 2006, glChess finally reached version 1.0.\n\nApple Chess is a fork of GNOME Chess.\n\nIn version 3.14 3D mode was removed.\n\n\n",
    "id": "18015568",
    "title": "GNOME Chess"
  }
]
