[
  {
    "url": "https://en.wikipedia.org/wiki?curid=3541535",
    "text": "Infradian rhythm\n\nIn chronobiology, an infradian rhythm is a rhythm with a period longer than the period of a circadian rhythm, i.e. with a frequency less than one cycle in 28 hours, such as menstruation, breeding, tidal or seasonal rhythms. In contrast, ultradian rhythms have periods shorter than the period of a circadian rhythm.\n\n",
    "id": "3541535",
    "title": "Infradian rhythm"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=5635076",
    "text": "Syntrophy\n\nSyntrophy, synthrophy, cross-feeding, or cross feeding [Greek \"syn\" meaning together, \"trophe\" meaning nourishment] is the phenomenon that one species lives off the products of another species. In this association, the growth of one partner is improved, or depends on the nutrients, growth factors or substrate provided by the other partner. Jan Dolfing described syntrophy as \"the critical interdependency between producer and consumer\". This term for nutritional interdependence is often used in microbiology to describe this symbiotic relationship between bacterial species. Morris \"et al\". have described the process as \"obligately mutualistic metabolism\".\n\nSyntrophy plays an important role in a large number of microbial processes.\n\nThe defining feature of ruminants, such as cows and goats, is a stomach called a rumen. The rumen contains billions of microbes, many of which are syntrophic. One excellent example of this syntrophy is interspecies hydrogen transfer. Some anaerobic fermenting microbes in the rumen (and other gastrointestinal tracts) are capable of degrading organic matter to short chain fatty acids, and hydrogen. The accumulating hydrogen inhibits the microbe's ability to continue degrading organic matter, but syntrophic hydrogen-consuming microbes allow continued growth by metabolizing the waste products. In addition, fermentative bacteria gain maximum energy yield when protons are used as electron acceptor with concurrent H production. Hydrogen-consuming organisms include methanogens, sulfate-reducers, acetogens, and others. Some fermentation products, such as fatty acids longer than two carbon atoms, alcohols longer than one carbon atom, and branched-chain and aromatic fatty acids, cannot directly be used in methanogenesis. In acetogenesis process, these products are oxidized to acetate and H by obligated proton reducing bacteria in syntrophic relationship with methanogenic archaea as low H partial pressure is essential for acetogenic reactions to be thermodynamically favorable (ΔG < 0). (Stams et al., 2005)\n\nThe number of bacterial cells that live on or in the human body, for example throughout the alimentary canal and on the skin, is in the region of 10 times the total number of human cells in it. These microbes are vital, for instance for the digestive and the immune system to function.\n\nAnother example is the many organisms that feed on faeces or dung. A cow diet consists mainly of grass, the cellulose of which is transformed into lipids by micro-organisms in the cow's large intestine. These micro-organisms cannot use the lipids because of lack of oxygen in the intestine, so the cow does not take up all lipids produced. When the processed grass leaves the intestine as dung and comes into open air, many organisms, such as the dung beetle, feed on it.\n\nYet another example is the community of micro-organisms in soil that live off leaf litter. Leaves typically last one year and are then replaced by new ones. These micro-organisms mineralize the discarded leaves and release nutrients that are taken up by the plant. Such relationships are called reciprocal syntrophy because the plant lives off the products of micro-organisms. Many symbiotic relationships are based on syntrophy.\n\nSyntrophic microbial food webs can play an integral role in the breakdown of organic pollutants such as oils, aromatic compounds, and amino acids.\n\nEnvironmental contamination with oil is of high ecological importance, but can be mediated through syntrophic degradation. Alkanes are hydrocarbon chains that are the major chemical component of crude oils, and have been experimentally verified to be broken down by syntrophic microbial food webs. The hydrocarbons of the oil are broken down after activation by fumarate, a chemical compound that is regenerated by other microorganisms. Without regeneration, the microbes degrading the oil would eventually run out of fumarate and the process would cease. This breakdown is crucial in the processes of bioremediation and global carbon cycling.\n\nSyntrophic microbial communities are key players in the breakdown of aromatic compounds, which are common pollutants. The degradation of aromatic benzoate to methane produces many intermediate compounds such as formate, acetate, CO2 and H2. The build up of these products makes benzoate degradation progressively less favourable. These intermediates can then be taken up and metabolized syntrophically by methanogens to make the whole process more thermodynamically favourable.\n\nStudies have shown that bacterial degradation of amino acids can be significantly enhanced through the process of syntrophy. Microbes growing poorly on amino acid substrates alanine, aspartate, serine, leucine, valine, and glycine can have their rate of growth dramatically increased by syntrophic H2 scavengers. These scavengers, like \"Methanospirillum and Acetobacterium,\" metabolize the H2 waste produced during amino acid breakdown, preventing a toxic build-up. Another way to improve amino acid breakdown is through interspecies electron transfer mediated by formate. Species like \"Desulfovibrio\" employ this method.\n\nThe main motive behind a syntrophic relation between two bacterial organisms is generalized as a relationship where each participant's metabolic activity cannot independently overcome the thermodynamic pressure of the reaction under standard conditions even when a cosubstrate or nutrient is added into the environment. Therefore, the cooperation of the other participant is required to reduce the intermediate pool size.\nThe \"Methanobacillus omelianskii\" culture is a classic example in demonstrating how two separate unfavourable reactions can be carried out by syntrophic interactions. Strain S and strain M.o.H of \"Methanobacillus omelianskii\" oxidize ethanol into acetate and methane by a process called interspecies hydrogen transfer. Individuals of strain S are observed as obligate anaerobic bacteria that use ethanol as an electron donor, whereas organisms of strain M.o.H are methanogens that oxidize hydrogen gas to produce methane. These two metabolic reactions can be shown as follows:\n<br>\nComplex organic compounds such as ethanol, propionate, butyrate, and lactate cannot be directly used as substrates for methanogenesis by methanogens. On the other hand, fermentation of these organic compounds cannot occur in fermenting microorganisms unless the hydrogen concentration is reduced to a low level by the methanogens. In this case, hydrogen, an electron-carrying compound (mediator) is transported from the fermenting bacteria to the methanogen through a process called mediated interspecies electron transfer (MIET), where the mediator is carried down a concentration gradient created by a thermodynamically favourable coupled redox reaction.\n",
    "id": "5635076",
    "title": "Syntrophy"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=6920635",
    "text": "Biological constraints\n\nBiological constraints are factors which make populations resistant to evolutionary change. One proposed definition of constraint is \"A property of a trait that, although possibly adaptive in the environment in which it originally evolved, acts to place limits on the production of new phenotypic variants.\" Constraint has played an important role in the development of such ideas as homology and body plans.\n\nAny aspect of an organism that has not changed over a certain period of time could be considered to provide evidence for \"constraint\" of some sort. To make the concept more useful, it is therefore necessary to divide it into smaller units. First, one can consider the pattern of constraint as evidenced by phylogenetic analysis and the use of phylogenetic comparative methods; this is often termed phylogenetic inertia, or phylogenetic constraint. It refers to the tendency of related taxa sharing traits based on phylogeny. Charles Darwin spoke of this concept in his 1859 book \"On the Origin of Species\", as being \"Unity of Type\" and went on to explain the phenomenon as existing because organisms do not start over from scratch, but have characteristics that are built upon already existing ones that were inherited from their ancestors; and these characteristics likely limit the amount of evolution seen in that new taxa due to these constraints. \n\nIf one sees particular features of organisms that have not changed over rather long periods of time (many generations), then this could suggest some constraint on their ability to change (evolve). However, it is not clear that mere documentation of lack of change in a particular character is good evidence for constraint in the sense of the character being unable to change. For example, long-term stabilizing selection related to stable environments might cause stasis. It has often been considered more fruitful, to consider constraint in its causal sense: what are the causes of lack of change?\n\nThe most common explanation for biological constraint is that stabilizing selection acts on an organism to prevent it changing, for example, so that it can continue to function in a tightly-defined niche. This may be considered to be a form of external constraint, in the sense that the organism is constrained not by its makeup or genetics, but by its environment. The implication would be that if the population was in a new environment, its previously constrained features would potentially begin to evolve.\n\nRelated to the idea of stabilizing selection is that of the requirement that organisms function adequately in their environment. Thus, where stabilizing selection acts because of the particular niche that is occupied, mechanical and physico-chemical constraints act in a more general manner. For example, the acceleration caused by gravity places constraints on the minimum bone density and strength for an animal of a particular size. Similarly, the properties of water mean that tissues must have certain osmotic properties in order to function properly.\n\nFunctional coupling takes the idea that organisms are integrated networks of functional interactions (for example, the vertebral column of vertebrates is involved in the muscle, nerve, and vascular systems as well as providing support and flexibility) and therefore cannot be radically altered without causing severe functional disruption. This may be viewed as one type of trade-off. As Rupert Riedl pointed out, this degree of functional constraint — or burden — generally varies according to position in the organism. Structures literally in the centre of the organism — such as the vertebral column — are often more burdened than those at the periphery, such as hair or toes.\n\nThis class of constraint depends on certain types of phenotype not being produced by the genotype (compare stabilizing selection, where there is no constraint on what is produced, but rather on what is naturally selected). For example, for a highly homozygous organism, the degree of observed phenotypic variability in its descendents would be lower than those of a heterozygous one. Similarly, developmental systems may be highly canalised, to prevent the generation of certain types of variation.\n\nAlthough they are separate, the types of constraints discussed are nevertheless relatable to each other. In particular, stabilizing selection, mechanical, and physical constraints might lead through time to developmental integration and canalisation. However, without any clear idea of any of these mechanisms, deducing them from mere patterns of stasis as deduced from phylogenetic patterns or the fossil record remains problematic. In addition, the terminology used to describe constraints has led to confusion.\n\n\n",
    "id": "6920635",
    "title": "Biological constraints"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=2686634",
    "text": "Polyphenism\n\nA polyphenic trait is a trait for which multiple, discrete phenotypes can arise from a single genotype as a result of differing environmental conditions. It is therefore a special case of phenotypic plasticity.\n\nThere are several types of polyphenism in animals, from having sex determined by the environment to the castes of honey bees and other social insects. Some polyphenisms are seasonal, as in some butterflies which have different patterns during the year, and some Arctic animals like the snowshoe hare and Arctic fox, which are white in winter. Other animals have predator-induced or resource polyphenisms, allowing them to exploit variations in their environment. Some nematode worms can develop either into adults or into resting dauer larvae according to resource availability.\n\nA polyphenism is a biological mechanism that causes a trait to be polyphenic. For example, crocodiles possess a sex-determining polyphenism, and therefore their gender is a polyphenic trait.\n\nWhen polyphenic forms exist at the same time in the same panmictic (interbreeding) population they can be compared to genetic polymorphism. With polyphenism the switch between morphs is environmental, but with genetic polymorphism the determination of morph is genetic. These two cases have in common that more than one morph is part of the population at any one time. This is rather different from cases where one morph predictably follows another during, for instance, the course of a year. In essence the latter is normal ontogeny where young forms can and do have different forms, colours and habits to adults.\n\nThe discrete nature of polyphenic traits differentiates them from traits like weight and height, which are also dependent on environmental conditions but vary continuously across a spectrum. When a polyphenism is present, an environmental cue causes the organism to develop along a separate pathway, resulting in distinct morphologies; thus, the response to the environmental cue is “all or nothing.” The nature of these environmental conditions varies greatly, and includes seasonal cues like temperature and moisture, pheromonal cues, kairomonal cues (signals released from one species that can be recognized by another), and nutritional cues.\n\nSex-determining polyphenisms allow a species to benefit from sexual reproduction while permitting an unequal gender ratio. This can be beneficial to a species because a large female-to-male ratio maximizes reproductive capacity. However, temperature-dependent sex determination (as seen in crocodiles) limits the range in which a species can exist, and makes the species susceptible to endangerment by changes in weather pattern. Temperature-dependent sex determination has been proposed as an explanation for the extinction of the dinosaurs.\n\nPopulation-dependent and reversible sex determination, found in animals such as the blue wrasse fish, have less potential for failure. In the blue wrasse, only one male is found in a given territory: larvae within the territory develop into females, and adult males will not enter the same territory. If a male dies, one of the females in his territory becomes male, replacing him. While this system ensures that there will always be a mating couple when two animals of the same species are present, it could potentially decrease genetic variance in a population, for example if the females remain in a single male's territory.\n\nThe caste system of insects enables eusociality, the division of labor between non-breeding and breeding individuals. A series of polyphenisms determines whether larvae develop into queens, workers, and, in some cases soldiers. In the case of the ant, \"P. morrisi\", an embryo must develop under certain temperature and photoperiod conditions in order to become a reproductively-active queen. This allows for control of the mating season but, like sex determination, limits the spread of the species into certain climates.\nIn bees, royal jelly provided by worker bees causes a developing larva to become a queen. Royal jelly is only produced when the queen is aging or has died. This system is less subject to influence by environmental conditions, yet prevents unnecessary production of queens.\n\nPolyphenic pigmentation is adaptive for insect species that undergo multiple mating seasons each year. Different pigmentation patterns provide appropriate camouflage throughout the seasons, as well as alter heat retention as temperatures change. Because insects cease growth and development after eclosion, their pigment pattern is invariable in adulthood: thus, a polyphenic pigment adaptation would be less valuable for species whose adult form survives longer than one year.\n\nBirds and mammals are capable of continued physiological changes in adulthood, and some display reversible seasonal polyphenisms, such as in the Arctic fox, which becomes all white in winter as snow camouflage.\n\nPredator-induced polyphenisms allow the species to develop in a more reproductively-successful way in a predator's absence, but to otherwise assume a more defensible morphology. However, this can fail if the predator evolves to stop producing the kairomone to which the prey responds. For example, the fly larvae that feed on \"Daphnia cucullata\" (a water flea) release a kairomone that \"Daphnia\" can detect. When the fly larvae are present, \"Daphnia\" grow large helmets that protect them from being eaten. However, when the predator is absent, \"Daphnia\" have smaller heads and are therefore more agile swimmers.\n\nOrganisms with resource polyphenisms show alternative phenotypes that allow differential use of food or other resources. One example is the western spadefoot toad, which maximizes its reproductive capacity in temporary desert ponds. While the water is at a safe level, the tadpoles develop slowly on a diet of other opportunistic pond inhabitants. However, when the water level is low and desiccation is imminent, the tadpoles develop a morphology (wide mouth, strong jaw) that permits them to cannibalize. Cannibalistic tadpoles receive better nutrition and thus metamorphose more quickly, avoiding death as the pond dries up.\n\nAmong invertebrates, the nematode \"Pristionchus pacificus\" has one morph that primarily feeds on bacteria and a second morph that produces large teeth, enabling it to feed on other nematodes, including competitors for bacterial food. In this species, cues of starvation and crowding by other nematodes, as sensed by pheromones, trigger a hormonal signal that ultimately activates a developmental switch gene that specifies formation of the predatory morph.\n\nDensity-dependent polyphenism allows species to show a different phenotype based on the population density in which it was reared. In Lepidoptera, African armyworm larvae exhibit one of two appearances: the gregarious or solitary phase. Under crowded or \"gregarious\" conditions, the larvae have black bodies and yellow stripes along their bodies. However, under solitary conditions, they have green bodies with a brown stripe down their backs. The different phenotypes emerge during the third instar and remain until the last instar. \n\nUnder conditions of stress such as crowding and high temperature, L2 larvae of some free living nematodes such as \"Caenorhabditis elegans\" can switch development to the so-called dauer larva state, instead of going the normal molts into a reproductive adult. These dauer larvae are a stress-resistant, non-feeding, long-lived stage, enabling the animals to survive harsh conditions. On return to favorable conditions, the animal resumes reproductive development from L3 stage onwards.\n\nA mechanism has been proposed for the evolutionary development of polyphenisms:\n\n\nEvolution of novel polyphenisms through this mechanism has been demonstrated in the laboratory. Suzuki and Nijhout used an existing mutation (\"black\") in a monophenic green hornworm (\"Manduca sexta\") that causes a black phenotype. They found that if larvae from an existing population of \"black\" mutants were raised at 20˚C, then all the final instar larvae were black; but if the larvae were instead raised at 28˚C, the final instar larvae ranged in color from black to green. By selecting for larvae that were black if raised at 20˚C but green if raised at 28˚C, they produced a polyphenic strain after thirteen generations.\n\nThis fits the model described above because a new mutation (black) was required to reveal pre-existing genetic variation and to permit selection. Furthermore, the production of a polyphenic strain was only possible because of background variation within the species: two alleles, one temperature-sensitive and one stable, were present for a single gene upstream of \"black\" (in the pigment production pathway) before selection occurred. The temperature-sensitive allele was not observable because at high temperatures, it caused an increase in green pigment in hornworms that were already bright green. However, introduction of the black mutant caused the temperature-dependent changes in pigment production to become obvious. The researchers could then select for larvae with the temperature-sensitive allele, resulting in a polyphenism.\n\n\n",
    "id": "2686634",
    "title": "Polyphenism"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=277810",
    "text": "Trace metal\n\nTrace metals are the metals subset of trace elements; that is, metals normally present in small but measurable amounts in animal and plant cells and tissues and that are a necessary part of nutrition and physiology. Many biometals are trace metals. Ingestion of, or exposure to, excessive quantities can be toxic. However, insufficient plasma or tissue levels of certain trace metals can cause pathology, as is the case with iron.\n\nTrace metals within the human body include iron, lithium, zinc, copper, chromium, nickel, cobalt, vanadium, molybdenum, manganese and others.\n\nTrace metals are metals needed by living organisms to function properly and are depleted through the expenditure of energy by various metabolic processes of living organisms. They are replenished in animals through diet as well as environmental exposure, and in plants through the uptake of nutrients from the soil in which the plant grows. Human vitamin pills and plant fertilizers can be a source of trace metals.\n\nTrace metals are sometimes referred to as trace elements, although the latter includes minerals and is a broader category. See also Dietary mineral. Trace elements are required by the body for specific functions. Things such as vitamins, sports drinks, fresh fruits and vegetables are sources. Taken in excessive amounts, trace elements can cause problems. For example fluorine is required for the formation of bones and enamel on teeth. However, when taken in an excessive amount can cause a disease called \"Fluorosis', in which bone deformations and yellowing of teeth are seen. Fluorine can occur naturally in some areas in ground water.\n\nTrace metals include iron that can help to prevent anemia, and zinc that is a cofactor in over 100 enzyme reactions.\n",
    "id": "277810",
    "title": "Trace metal"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=10869371",
    "text": "Supercompensation\n\nIn sports science theory, supercompensation is the post training period during which the trained function/parameter has a higher performance capacity than it did prior to the training period.\n\nThe fitness level of a human body in training can be broken down into four periods: initial fitness, parietal fitness training, recovery, and supercompensation. During the initial fitness period, the target of the training has a base level of fitness (shown by the first time sector in the graph). Upon entering the training period, the target's level of fitness decreases (shown by the second time sector in the graph). After training, the body enters the recovery period during which level of fitness increases up to the initial fitness level (shown by the third time sector in the graph). Because the human body is an adjustable organism, it will feel the need to adjust itself to a higher level of fitness in anticipation of the next training session. Accordingly, the increase in fitness following a training session does not stop at the initial fitness level. Instead the body enters a period of supercompensation during which fitness surpasses the initial fitness level (shown by the fourth time sector in the graph). If there are no further workouts, this fitness level will slowly decline back towards the initial fitness level (shown by the last time sector in the graph). First put forth by Russian scientist Nikolai N. Yakovlev (1911–1992) in 1949-1959, this theory is a basic principle of athletic training.\n\nIf the next workout takes place during the recovery period, overtraining may occur. If the next workout takes place during the supercompensation period, the body will advance to a higher level of fitness. If the next workout takes place after the supercompensation period, the body will remain at the base level.\n\nMore complex variations are possible; for instance, sometimes a few workouts are intentionally made in the recovery period to achieve greater supercompensation effects.\n\nAt a first glance, creating effective training programs might look simple. All you need is to determine the intensity level and how long it takes you to get to the supercompensation period. Afterwards, continue training with the intensity level that was determined previously and keep the necessary intervals between workouts required for supercompensation. However, things become more complex because training affects many different bodily functions and parameters. Each bodily function or parameter has a different recovery time, a different amount of time needed to reach peak supercompensation, a different amount of time between supercompensation peak and return to base fitness.\n\nThe aforementioned functions and parameters are basic ones. Muscle strength and mass are complex parameters. For instance, muscle mass is a function of many different simple parameters. For example, amount of glycogen in muscles is a basic parameter that influences muscle mass.\n\nIn classical sport science, the yearly (sometimes multi-yearly) period is divided to micro and macro cycles, where each microcycle is responsible for the development of a specific (sometimes several) basic training function and parameter, whereas macrocycles are responsible for the development of complex parameters/functions (such as muscle strength). During each microcycle, the resting period is the same as the amount of time needed for reaching the supercompensation stage of the current training parameter/function (also during such a micro cycle there shouldn't be any negative influence on the recovery of the main function). Such a training method will work only when the developed functions/parameters are non-related. Unfortunately, for muscle strength and mass this is not the case (functions/parameters are related). Therefore, for muscle strength and mass different approaches are needed. During a training cycle the intensity and volume of training varies, waves of different functions are overlaid so that until the end of the microcycle supercompensation of the main required functions is achieved.\n\n",
    "id": "10869371",
    "title": "Supercompensation"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=11801835",
    "text": "Kin recognition\n\nKin recognition, also called kin detection, is an organism's ability to distinguish between close genetic kin and non-kin. In evolutionary biology and psychology, such an ability is presumed to have evolved for inbreeding avoidance.\n\nAn additional adaptive function sometimes posited for kin recognition is a role in kin selection. There is debate over this, since in strict theoretical terms kin recognition is not necessary for kin selection or the cooperation associated with it. Rather, social behaviour can emerge by kin selection in the demographic conditions of 'viscous populations' with organisms interacting in their natal context, without active kin discrimination, since social participants by default typically share recent common origin. Since kin selection theory emerged, much research has been produced investigating the possible role of kin recognition mechanisms in mediating altruism. Taken as a whole, this research suggests that active powers of recognition play a negligible role in mediating social cooperation relative to less elaborate cue-based and context-based mechanisms, such as familiarity, imprinting and phenotype matching.\n\nBecause cue-based 'recognition' predominates in social mammals, outcomes are non-deterministic in relation to actual genetic kinship, instead outcomes simply reliably correlate with genetic kinship in an organism's typical conditions. A well-known human example of an inbreeding avoidance mechanism is the Westermarck effect, in which unrelated individuals who happen to spend their childhood in the same household find each other sexually unattractive. Similarly, due to the cue-based mechanisms that mediate social bonding and cooperation, unrelated individuals who grow up together in this way are also likely to demonstrate strong social and emotional ties, and enduring altruism.\n\nThe English evolutionary biologist W. D. Hamilton's theory of inclusive fitness, and the related theory of kin selection, were formalized in the 1960s and 1970s to explain the evolution of social behaviours. Hamilton's early papers, as well as giving a mathematical account of the selection pressure, discussed possible implications and behavioural manifestations. Hamilton considered potential roles of cue-based mechanisms mediating altruism versus 'positive powers' of kin discrimination:\n\nThese two possibilities, altruism mediated via 'passive situation' or via 'sophisticated discrimination', stimulated a generation of researchers to look for evidence of any 'sophisticated' kin discrimination. However, Hamilton later (1987) developed his thinking to consider that \"an innate kin recognition adaptation\" was unlikely to play a role in mediating altruistic behaviours:\n\nThe implication that the inclusive fitness criterion can be met by mediating mechanisms of cooperative behaviour that are context and location-based has been clarified by recent work by West \"et al.\":\n\nFor a recent review of the debates around kin recognition and their role in the wider debates about how to interpret inclusive fitness theory, including its compatibility with ethnographic data on human kinship, see Holland (2012).\n\nLeading inclusive fitness theorists such as Grafen have argued that the whole research program around kin recognition is somewhat misguided:\n\nOthers have cast similar doubts over the enterprise:\n\nKin recognition is a behavioral adaptation noted in many species but proximate level mechanisms are not well documented. Recent studies have shown that kin recognition can result from a multitude of sensory input. Jill Mateo notes that there are three components are prominent in kin recognition. First, \"production of unique phenotypic cues or labels\". Second, \"perception of these labels and the degree of correspondence of these labels with a 'recognition template'\", and finally the recognition of the phenotypes should lead to \"action taken by the animal as a function of the perceived similarity between its template and an encountered phenotype\".\n\nThe three components allow for several possible mechanisms of kin recognition. Sensory information gathered from visual, olfactory and auditory stimuli are the most prevalent. The belding ground squirrel kin produce similar odors in comparison to non-kin. Mateo notes that the squirrels spent longer investigating non-kin scents suggesting recognition of kin odor. It's also noted that belding's ground squirrels produce at least two scents arising from dorsal and oral secretions, giving two opportunities for kin recognition. Auditory distinctions have been noted among avian species. Long-tailed tits (\"Aegithalos caudatus\") are capable of discriminating kin and non-kin based on contact calls. Distinguishing calls are often learned from adults during the nestling period. Studies suggest that the bald-faced hornet, \"Dolichovespula maculata\", can recognize nest mates by their cuticular hydrocarbon profile, which produces a distinct smell.\n\nKin recognition in some species may also be mediated by immunogenetic similarity of the major histocompatibility complex (MHC). For a discussion of the interaction of these social and biological kin recognition factors see Lieberman, Tooby, and Cosmides (2007). Some have suggested that, as applied to humans, this nature-nurture interactionist perspective allows a synthesis between theories and evidence of social bonding and cooperation across the fields of evolutionary biology, psychology (attachment theory) and cultural anthropology (nurture kinship).\n\nKin recognition is an adaptive behavior observed in living beings to prevent inbreeding, and increase fitness of populations, individuals and genes. Kin recognition is the key to successful reciprocal altruism, a behavior that increases reproductive success of both organisms involved. Reciprocal altruism as a product of kin recognition has been observed and studied in many animals, and more recently, plants. Due to the nature of plant reproduction and growth, plants are more likely than animals to live in close proximity to family members, and therefore stand to gain more from the ability to differentiate kin from strangers (Waldman, 1988).\n\nIn recent years, botanists have been conducting studies to determine which plant species can recognize kin, and discover the responses of plants to neighboring kin. Murphy and Dudley (2009) shows that \"Impatiens pallida\" has the ability to recognize individuals closely related to them and those not related to them. The physiological response to this recognition is increasingly interesting. \"I. pallida\" responds to kin by increasing branchiness and stem elongation, to prevent shading relatives, and responds to strangers by increasing leaf to root allocation, as a form of competition.\n\nSimilarly, Bhatt et al. (2010) show that \"Cakile edentula\", the American sea rocket, has the ability to allocate more energy to root growth, and competition, in response to growing next to a stranger, and allocates less energy to root growth when planted next to a sibling. This reduces competition between siblings and increases fitness of relatives growing next to each other, while still allowing competition between non-relative plants.\n\nLittle is known about the mechanisms involved in kin recognition. They most likely vary between species as well as within species. A study by Bierdrzycki et al. (2010) shows that root secretions are necessary for \"Arabidopsis thaliana\" to recognize kin vs. strangers, but not necessary to recognize self vs. non-self roots. This study was performed using secretion inhibitors, which disabled the mechanism responsible for kin recognition in this species, and showed similar growth patterns to Bhatt et al., (2010) and Murphy and Dudley (2009) in control groups. The most interesting result of this study was that inhibiting root secretions did not reduce the ability of \"Arabidopsis\" to recognize their own roots, which implicates a separate mechanism for self/non-self recognition than that for kin/stranger recognition.\n\nWhile this mechanism in the roots responds to exudates and involves competition over resources like nitrogen and phosphorus, another mechanism has been recently proposed, which involves competition over light, in which kin recognition takes place in leaves. In their 2014 study, Crepy and Casal conducted multiple experiments on different accessions of \"A. thaliana\". These experiments showed that \"Arabidopsis\" accessions have distinct R:FR and blue light signatures, and that these signatures can be detected by photoreceptors, which allows the plant to recognize its neighbor as a relative or non-relative. Not much is known about the pathway that \"Arabidopsis\" uses to associate these light patterns with kin, however, researchers ascertained that photoreceptors phyB, cry 1, cry 2, phot1, and phot2 are involved in the process by performing a series of experiments with knock-out mutants. Researchers also concluded that the auxin-synthesis gene TAA1 is involved in the process, downstream of the photoreceptors, by performing a similar experiments using Sav3 knock-out mutants. This mechanism leads to altered leaf direction to prevent shading of related neighbors and to reduce competition for sunlight.\n\nWhen mice inbreed with close relatives in their natural habitat, there is a significant detrimental effect on progeny survival. Since inbreeding is detrimental, it tends to be avoided. In the house mouse, the major urinary protein (MUP) gene cluster provides a highly polymorphic scent signal of genetic identity that appears to underlie kin recognition and inbreeding avoidance. Thus there are fewer matings between mice sharing MUP haplotypes than would be expected if there were random mating. Another mechanism for avoiding inbreeding is evident when a female house mouse mates with multiple males. In such a case, there appears to be egg-driven sperm selection against sperm from related males.\n\nIn toads, male advertisement vocalizations may serve as cues by which females recognize their kin and thus avoid inbreeding.\n\nIn dioecious plants, the stigma may receive pollen from several different potential donors. As multiple pollen tubes from the different donors grow through the stigma to reach the ovary, the receiving maternal plant may carry out pollen selection favoring pollen from less related donor plants. Thus, kin recognition at the level of the pollen tube apparently leads to post-pollination selection to avoid inbreeding depression. Also, seeds may be aborted selectively depending on donor–recipient relatedness.\n\n",
    "id": "11801835",
    "title": "Kin recognition"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=2747470",
    "text": "Biological thermodynamics\n\nBiological thermodynamics is the quantitative study of the energy transductions that occur in or between living organisms, structures, and cells and of the nature and function of the chemical processes underlying these transductions. Biological thermodynamics may address the question of whether the benefit associated with any particular phenotypic trait is worth the energy investment it requires.\n\nGerman-British medical doctor and biochemist Hans Krebs' 1957 book \"Energy Transformations in Living Matter\" (written with Hans Kornberg) was the first major publication on the thermodynamics of biochemical reactions. In addition, the appendix contained the first-ever published thermodynamic tables, written by Kenneth Burton, to contain equilibrium constants and Gibbs free energy of formations for chemical species, able to calculate biochemical reactions that had not yet occurred.\n\nNon-equilibrium thermodynamics has been applied for explaining how biological organisms can develop from disorder. Ilya Prigogine developed methods for the thermodynamic treatment of such systems. He called these systems dissipative systems, because they are formed and maintained by the dissipative processes that exchange energy between the system and its environment, and because they disappear if that exchange ceases. It may be said that they live in symbiosis with their environment. Energy transformations in biology are dependent primarily on photosynthesis. The total energy captured by photosynthesis in green plants from the solar radiation is about 2 x 10 joules of energy per year. Annual energy captured by photosynthesis in green plants is about 4% of the total sunlight energy that reaches Earth. The energy transformations in biological communities surrounding hydrothermal vents are exceptions; they oxidize sulfur, obtaining their energy via chemosynthesis rather than photosynthesis.\n\nThe field of biological thermodynamics is focused on principles of chemical thermodynamics in biology and biochemistry. Principles covered include the first law of thermodynamics, the second law of thermodynamics, Gibbs free energy, statistical thermodynamics, reaction kinetics, and on hypotheses of the origin of life. Presently, biological thermodynamics concerns itself with the study of internal biochemical dynamics as: ATP hydrolysis, protein stability, DNA binding, membrane diffusion, enzyme kinetics, and other such essential energy controlled pathways. In terms of thermodynamics, the amount of energy capable of doing work during a chemical reaction is measured quantitatively by the change in the Gibbs free energy. The physical biologist Alfred Lotka attempted to unify the change in the Gibbs free energy with evolutionary theory.\n\nThe sun is the primary source of energy for living organisms. Some living organisms like plants need sunlight directly while other organisms like humans can acquire energy from the sun indirectly. There is however evidence that some bacteria can thrive in harsh environments like Antarctica as evidence by the blue-green algae beneath thick layers of ice in the lakes. No matter what the type of living species, all living organisms must capture, transduce, store, and use energy to live.\n\nThe relationship between the energy of the incoming sunlight and its wavelength or frequency is given by\n\nwhere \"h\" is the Planck constant (6.63x10Js) and \"c\" is the speed of light (2.998x10 m/s). Plants trap this energy from the sunlight and undergo photosynthesis, effectively converting solar energy into chemical energy. To transfer the energy once again, animals will feed on plants and use the energy of digested plant materials to create biological macromolecules.\n\nThe First Law of Thermodynamics is a statement of the conservation of energy; though it can be changed from one form to another, energy can be neither created nor destroyed. From the first law, a principle called Hess's Law arises. Hess’s Law states that the heat absorbed or evolved in a given reaction must always be constant and independent of the manner in which the reaction takes place. Although some intermediate reactions may be endothermic and others may be exothermic, the total heat exchange is equal to the heat exchange had the process occurred directly. This principle is the basis for the calorimeter, a device used to determine the amount of heat in a chemical reaction. Since all incoming energy enters the body as food and is ultimately oxidized, the total heat production may be estimated by measuring the heat produced by the oxidation of food in a calorimeter. This heat is expressed in kilocalories, which are the common unit of food energy found on nutrition labels.\n\nThe Second Law of Thermodynamics is concerned primarily with whether or not a given process is possible. The Second Law states that no natural process can occur unless it is accompanied by an increase in the entropy of the universe. Stated differently, an isolated system will always tend to disorder. Living organisms are often mistakenly believed to defy the Second Law because they are able to increase their level of organization. To correct this misinterpretation, one must refer simply to the definition of systems and boundaries. A living organism is an open system, able to exchange both matter and energy with its environment. For example, a human being takes in food, breaks it down into its components, and then uses those to build up cells, tissues, ligaments, etc. This process increases order in the body, and thus decreases entropy. However, humans also 1) conduct heat to clothing and other objects they are in contact with, 2) generate convection due to differences in body temperature and the environment, 3) radiate heat into space, 4) consume energy-containing substances (i.e., food), and 5) eliminate waste (e.g., carbon dioxide, water, and other components of breath, urine, feces, sweat, etc.). When taking all these processes into account, the total entropy of the greater system (i.e., the human and her/his environment) increases. When the human ceases to live, none of these processes (1-5) take place, and any interruption in the processes (esp. 4 or 5) will quickly lead to morbidity and/or mortality.\n\nIn biological systems, in general energy and entropy change together. Therefore, it is necessary to be able to define a state function that accounts for these changes simultaneously. This state function is the Gibbs Free Energy, \"G\".\n\nwhere: \nThe change in Gibbs Free Energy can be used to determine whether a given chemical reaction can occur spontaneously. If ∆\"G\" is negative, the reaction can occur spontaneously. Likewise, if ∆\"G\" is positive, the reaction is nonspontaneous. Chemical reactions can be “coupled” together if they share intermediates. In this case, the overall Gibbs Free Energy change is simply the sum of the ∆\"G\" values for each reaction. Therefore, an unfavorable reaction (positive ∆\"G\") can be driven by a second, highly favorable reaction (negative ∆\"G\" where the magnitude of ∆\"G\" > magnitude of ∆\"G\"). For example, the reaction of glucose with fructose to form sucrose has a ∆\"G\" value of +5.5 kcal/mole. Therefore, this reaction will not occur spontaneously. The breakdown of ATP to form ADP and inorganic phosphate has a ∆\"G\" value of -7.3 kcal/mole. These two reactions can be coupled together, so that glucose binds with ATP to form glucose-1-phosphate and ADP. The glucose-1-phosphate is then able to bond with fructose yielding sucrose and inorganic phosphate. The ∆\"G\" value of the coupled reaction is -1.8 kcal/mole, indicating that the reaction will occur spontaneously. This principle of coupling reactions to alter the change in Gibbs Free Energy is the basic principle behind all enzymatic action in biological organisms.\n\n\n\n",
    "id": "2747470",
    "title": "Biological thermodynamics"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=13478480",
    "text": "Biological exponential growth\n\nBiological exponential growth is the exponential growth of biological organisms. When the resources availability is unlimited in the habitat, the population of an organism living in the habitat grows in an exponential or geometric fashion. Population growth in which the number of individuals increase by a constant multiple in each generation. The potential for population growth can be demonstrated in the laboratory under conditions that provide abundant resources and space. For example, a few fruit flies in a large culture jar containing an abundant food source may reproduce rapidly. One female fruit fly may lay more than 50 eggs. Reproductive adults develop in about 14 days, with approximately equal numbers of male and female offspring. For each female that began the population, 50 flies are expected 2 weeks later. Each female in the second generation produces 50 more flies after 2 more weeks, and so on. In other words, the population is experiencing exponential growth. Slow exponential growth is when a population grows slowly yet exponential because the population has long live spans. While a rapid exponential growth refers to a population that grows ( and dies ) rapidly because the population has short life spans.\n\nResource availability is obviously essential for the unimpeded growth of a population. Ideally, when resources in the habitat are unlimited, each species has the ability to realise fully its innate potential to grow in number, as Charles Darwin observed while developing his theory of natural selection.\n\nIf, in a hypothetical population of size \"N\", the birth rates (per capita) are represented as \"b\" and death rates (per capita) as \"d\", then the increase or decrease in \"N\" during a time period \"t\" will be:\n\nformula_1\n\n(b-d) is called the 'intrinsic rate of natural increase' and is a very important parameter chosen for assessing the impacts of any biotic or abiotic factor on population growth.\n\nAny species growing exponentially under unlimited resource conditions can reach enormous population densities in a short time. Darwin showed how even a slow growing animal like the elephant could reach an enormous population if there were unlimited resources for its growth in its habitat.\n\nIf birth giving takes two parents we get Nurgaliev's law.\n\n",
    "id": "13478480",
    "title": "Biological exponential growth"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1556918",
    "text": "Phase response curve\n\nA phase response curve (PRC) illustrates the transient change in the cycle period of an oscillation induced by a perturbation as a function of the phase at which it is received. PRCs are used in various fields; examples of biological oscillations are the heartbeat, circadian rhythms, and the regular, repetitive firing observed in some neurons in the absence of noise.\n\nIn humans and animals, there is a regulatory system that governs the phase relationship of an organism's internal circadian clock to a regular periodicity in the external environment (usually governed by the solar day). In most organisms, a stable phase relationship is desired, though in some cases the desired phase will vary by season, especially among mammals with seasonal mating habits.\n\nIn circadian rhythm research, a PRC illustrates the relationship between a chronobiotic's time of administration (relative to the internal circadian clock) and the magnitude of the treatment's effect on circadian phase. Specifically, a PRC is a graph showing, by convention, time of the subject's endogenous day along the \"x\"-axis and the amount of the phase shift (in hours) along the \"y\"-axis. The curve has one peak and one nadir in each 24-hour cycle. Relative circadian time is plotted vs. phase shift magnitude. The treatment is usually narrowly specified as a set intensity and colour and duration of light exposure to the retina and skin, or a set dose and formulation of melatonin.\n\nThese curves are often consulted in the therapeutic setting. Normally, the body's various physiological rhythms will be synchronized within an individual organism (human or animal), usually with respect to a master biological clock. Of particular importance is the sleep–wake cycle. Various sleep disorders and externals stresses (such as jet lag) can interfere with this. People with non-24-hour sleep–wake disorder often experience an inability to maintain a consistent internal clock. Extreme chronotypes usually maintain a consistent clock, but find that their natural clock does not align with the expectations of their social environment. PRC curves provide a starting point for therapeutic intervention. The two common treatments used to shift the timing of sleep are light therapy, directed at the eyes, and administration of the hormone melatonin, usually taken orally. Either or both can be used daily. The phase adjustment is generally cumulative with consecutive daily administrations, and—at least partially—additive with concurrent administrations of distinct treatments. If the underlying disturbance is stable in nature, ongoing daily intervention is usually required. For jet lag, the intervention serves mainly to accelerate natural alignment, and ceases once desired alignment is achieved.\n\nNote that PRC curves from the experimental setting are usually aggregates of the test population, that there can be mild or significant variation within the test population, that individuals with sleep disorders often respond atypically, and that the formulation of the chronobiotic might be specific to the experimental setting and not generally available in clinical practice (e.g. for melatonin, one sustained-release formulation might differ in its release rate as compared to another); also, while the magnitude is dose-dependent, not all PRC graphs cover a range of doses. The discussions below are restricted to the PRCs for the light and melatonin in humans.\n\nStarting about two hours before an individual's regular bedtime, exposure of the eyes to light will delay the circadian phase, causing later wake-up time and later sleep onset. The delaying effect gets stronger as evening progresses; it is also dependent on the wavelength and illuminance (\"brightness\") of the light. The effect is small in dim indoor lighting.\n\nAbout five hours after usual bedtime, coinciding with the body temperature nadir (the lowest point of the core body temperature during sleep) the PRC peaks and the effect changes abruptly from phase delay to phase advance. Immediately after this peak, light exposure has its greatest phase-advancing effect, causing earlier wake-up and sleep onset. Again, illuminance greatly affects results; indoor light may be less than 500 lux while light therapy uses up to 10,000 lux. The effect diminishes until about two hours after spontaneous wake-up time, when it reaches approximately zero.\n\nDuring the period between two hours after usual wake-up time and two hours before usual bedtime, light exposure has little or no effect on circadian phase (slight effects generally cancelling each other out).\n\nAnother image of the PRC for light is here (Figure 1). Within that image, the explanatory text is \n\nLight therapy, typically with a light box producing 10,000 lux at a prescribed distance, can be used in the evening to delay or in the morning to advance a person's sleep timing. Because losing sleep to obtain bright light exposure is considered undesirable by most people, and because it is very difficult to estimate exactly when the greatest effect (the PRC peak) will occur in an individual, the treatment is usually applied daily just prior to bedtime (to achieve phase delay), or just after spontaneous awakening (to achieve phase advance).\n\nIn addition to its use in the adjustment of circadian rhythms, light therapy is used as treatment for several affective disorders including seasonal affective disorder (SAD).\n\nIn 2002 Brown University researchers led by David Berson announced the discovery of special cells in the human eye, ipRGCs (intrinsically photosensitive retinal ganglion cells), which many researchers now believe control the light entrainment effect of the phase response curve. In the human eye, the ipRGCs have the greatest response to light in the 460–480 nm (blue) range. In one experiment, 400 lux of blue light produced the same effects as 10,000 lux of white light. A theory of spectral opponency, in which the addition of other spectral colors renders blue light less effective for circadian phototransduction, was supported by research reported in 2005.\n\nThe phase response curve for melatonin is roughly twelve hours out of phase with the phase response curve for light. At spontaneous wake-up time, exogenous (externally administered) melatonin has a slight phase-delaying effect. The amount of phase-delay increases until about eight hours after wake-up time, when the effect swings abruptly from strong phase delay to strong phase advance. The phase-advance effect diminishes as the day goes on until it reaches zero about bedtime. From usual bedtime until wake-up time, exogenous melatonin has no effect on circadian phase.\n\nThe human body produces its own (endogenous) melatonin starting about two hours before bedtime, provided the lighting is dim. This is known as \"dim-light melatonin onset\", DLMO. This stimulates the phase-advance portion of the PRC and helps keep the body on a regular sleep-wake schedule. It also helps prepare the body for sleep.\n\nAdministration of melatonin at any time may have a mild hypnotic (sleep-inducing) effect. The expected effect on sleep phase timing, if any, is predicted by the PRC.\n\nIn a 2006 study Victoria L. Revell \"et al.\" showed that a combination of morning bright light and afternoon melatonin, both timed to phase advance according to the respective PRCs, produce a larger phase advance shift than bright light alone, for a total of up to 2 hours. All times are approximate and vary from one person to another. In particular, there is no convenient way to accurately determine the times of the peaks and zero-crossings of these curves in an individual. Administration of light or melatonin close to the time at which the effect is expected to change sense abruptly may, if the changeover time is not accurately known, produce an opposite effect to that desired.\n\nThe first published usage of the term \"phase response curve\" was in 1960 by Patricia DeCoursey. The \"daily\" activity rhythms of her flying squirrels, kept in constant darkness, responded to pulses of light exposure. The response varied according to the time of day—that is, the animals' subjective \"day\"—when light was administered. When DeCoursey plotted all her data relating the quantity and direction (advance or delay) of phase-shift on a single curve, she created the PRC. It has since been a standard tool in the study of biological rhythms.\n\nPhase response curve analysis can be used to understand the intrinsic properties and oscillatory behavior of regular-spiking neurons. The neuronal PRCs can be classified as being purely positive (PRC type I) or as having negative parts (PRC type II). Importantly, the PRC type exhibited by a neuron is indicative of its input–output function (excitability) as well as synchronization behavior: networks of PRC type II neurons can synchronize their activity via mutual excitatory connections, but those of PRC type I can not.\n\nExperimental estimation of PRC in living, regular-spiking neurons involves measuring the changes in inter-spike interval in response to a small perturbation, such as a transient pulse of current. Notably, the PRC of a neuron is not fixed but may change when firing frequency or neuromodulatory state of the neuron is changed.\n\n\n",
    "id": "1556918",
    "title": "Phase response curve"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=18101603",
    "text": "High throughput biology\n\nHigh throughput cell biology is the use of automation equipment with classical cell biology techniques to address biological questions that are otherwise unattainable using conventional methods. It may incorporate techniques from optics, chemistry, biology or image analysis to permit rapid, highly parallel research into how cells function, interact with each other and how pathogens exploit them in disease.\n\nHigh-throughput biology serves as one facet of what has also been called \"omics research\" - the interface between large scale biology (genome, proteome, transcriptome), technology and researchers. High throughput cell biology has a definite focus on the cell, and methods accessing the cell such as imaging, gene expression microarrays, or genome wide screening. The basic idea is to take methods normally performed on their own and do a very large number of them without impacting their quality.\n\nHigh throughput research can be defined as the automation of experiments such that large scale repetition becomes feasible. This is important because many of the questions faced by life science researchers now involve large numbers. For example, the Human Genome contains at least 21,000 genes, all of which can potentially contribute to cell function, or disease. To be able to capture an idea of how these genes interact with one another, which genes are involved in and where they are, methods that encompass from the cell to the genome are of interest.\n\nClassical High throughput screening robotics are now being tied closer to cell biology, principally using technologies such as High-content screening. High throughput cell biology dictates methods that can take routine cell biology from low scale research to the speed and scale necessary to investigate complex systems, achieve high sample size, or efficiently screen through a collection.\n\nIt has a greater emphasis on function rather than discovery, and will have its most significant impact in exploring biology as we progress toward models of the cell as a system rather than isolated pathways.\n\nHigh-content screening technology is mainly based on automated digital microscopy and flow cytometry, in combination with IT-systems for the analysis and storage of the data.\n\"High-content\" or visual biology technology has two purposes, first to acquire spatially or temporally resolved information on an event and second to automatically quantify it. Spatially resolved instruments are typically automated microscopes, and temporal resolution still requires some form of fluorescence measurement in most cases.This means that a lot of HCS instruments are (fluorescence) microscopes that are connected to some form of image analysis package. These take care of all the steps in taking fluorescent images of cells and provide rapid, automated and unbiased assessment of experiments.\n\nThe technology can be defined as being at the same development point as the first automated DNA sequencers in the early 1990s. Automated DNA sequencing was a disruptive technology when it became practical and -even if early devices had shortcomings- it enabled genome scale sequencing projects and created the field of bioinformatics. The impact of a similarly disruptive and powerful technology on molecular cell biology and translational research is hard to predict but what is clear is that it will cause a profound change in the way cell biologists research and medicines are discovered.\n\n\n",
    "id": "18101603",
    "title": "High throughput biology"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=21167788",
    "text": "Comparator system\n\nA comparator system, or simply comparator, in the fields of biophysics, biology, and neurology is a particular organisation of neurons. Comparators, as their name suggests, compare several inputs of internal or external information, and are important to the field of neural learning. In biological systems, comparators help an organism adapt to changes in its surroundings.\n",
    "id": "21167788",
    "title": "Comparator system"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=23826998",
    "text": "Compensatory growth (organ)\n\nCompensatory growth is a type of regenerative growth that can take place in a number of human organs after the organs are either damaged, removed, or cease to function. Additionally, increased functional demand can also stimulate this growth in tissues and organs. The growth can be a result of increased cell size (compensatory hypertrophy) or an increase in cell division (compensatory hyperplasia) or both. For instance, if one kidney is surgically removed, the cells of other kidney divide at an increased rate. Eventually, the remaining kidney can grow until its mass approaches the combined mass of two kidneys. Along with the kidneys, compensatory growth has also been characterized in a number of other tissues and organs including:\n\nA large number of growth factors and hormones are involved with compensatory growth, but the exact mechanism is not fully understood and probably varies between different organs. Nevertheless, angiogenic growth factors which control the growth of blood vessels are particularly important because blood flow significantly determines the maximum growth of an organ.\n\nCompensatory growth may also refer to the accelerated growth following a period of slowed growth, particularly as a result of nutrient deprivation.\n\n",
    "id": "23826998",
    "title": "Compensatory growth (organ)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=10086055",
    "text": "Test (biology)\n\nIn biology, a test is the hard shell of some spherical marine animals, notably sea urchins and microorganisms such as testate foraminiferans, radiolarians, and testate amoebae.\n\nThe anatomical term \"test\" derives from the Latin \"testa\" (which means a rounded bowl, amphora or bottle). It is distinct from the term \"test\" as in \"examination\", which derives from \"testis\", related to the idea of testimony.\n\nThe test is a skeletal structure, made of hard material such as calcium carbonate, silica, chitin or composite materials. As such, it allows the protection of the internal organs and the attachment of soft flesh.\n\nThe test of sea urchins is made of calcium carbonate, strengthened by a framework of calcite monocrystals, in a characteristic \"stereomic\" structure. These two ingredients provide sea urchins with a great solidity and a moderate weight, as well as the capacity to regenerate the mesh from the cuticle. According to a 2012 study, the skeletal structures of sea urchins consist in \"92% of bricks of calcite monocrystals (conferring solidity and hardness) and 8 % of a \"mortar\" of amorphous lime (allowing flexibility and lightness). This lime is constituted itself of 99.9% of calcium carbonate, with 0.1% structural proteins, which make sea urchins animals with an extremely mineralized skeleton (which also explains their excellent conservation as fossils).\n\nThe test of foraminiferan initially consists of gradually mineralized organic matter, but also sometimes exogenous agglomerated particles (in particular for species living in the sediment or in great depth). It can be of many types, like agglutinated (exogenous agglomerate), microgranular (with calcite grains), porcelain-like (smooth calcite) or hyalin (lens). Foraminiferans develops by building new rooms in their test. These are arranged according to a geometry particular to each species: they can be rectilinear, curved, rolled up or cyclic, and every time uniserial or multiserial. These organizational types can also be mixed, or even more complex. Miliolids have a particular arrangement. The surface of the test can be smooth or textured.\n\nIn ascidians the sheath is sometimes called test as well, and is composed largely of a particular type of cellulose historically termed \"tunicine\". From 1845 (when this was discovered by Schmidt) until 1958 (when cellulose fibres were found in mammalian connective tissue), ascidians were believed to be the only animals that synthesised cellulose.\n\nOn a strictly scientific point of view, the term \"test\" should be restricted to the hard shell protecting sea urchins and foraminiferans. For sessile echinoderms (like crinoids, but also many fossile groups such as cistoids or blastoids), the correct word is \"theca\". For diatomea, the term in use is \"frustule\", and for radiolarians it should be \"capsule\". The more common word \"shell\" is used for mollusks, arthropods and turtles (even if the latter ones belong to the order \"Testudines\").\n\n",
    "id": "10086055",
    "title": "Test (biology)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=24809644",
    "text": "Suture materials comparison chart\n\nNumerous different surgical suture materials exist. The following table compares some of the most common absorbable sutures.\n\n3.Types of sutures and suture materials\n",
    "id": "24809644",
    "title": "Suture materials comparison chart"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=20445291",
    "text": "Potassium spatial buffering\n\nPotassium spatial buffering is a mechanism for the regulation of extracellular potassium concentration by astrocytes. Other mechanisms for astrocytic potassium clearance are carrier-operated or channel-operated potassium chloride uptake.\nThe repolarization of neurons tends to raise potassium concentration in the extracellular fluid. If a significant rise occurs, it will interfere with neuronal signaling by depolarizing neurons. Astrocytes have large numbers of potassium ion channels facilitating removal of potassium ions from the extracellular fluid. They are taken up at one region of the astrocyte and then distributed throughout the cytoplasm of the cell, and further to its neighbors via gap junctions. This keeps extracellular potassium at levels that prevent interference with normal propagation of an action potential.\n\nGlial cells, once believed to have a passive role in CNS, are active regulators of numerous functions in the brain, including clearance of the neurotransmitter from the synapses, guidance during neuronal migration, control of neuronal synaptic transmission, and maintaining ideal ionic environment for active communications between neurons in central nervous system.\n\nNeurons are surrounded by extracellular fluid rich in sodium ions and poor in potassium ions. The concentrations of these ions are reversed inside the cells. Due to the difference in concentration, there is a chemical gradient across the cell membrane, which leads to sodium influx and potassium efflux. When the action potential takes place, a considerable change in extracellular potassium concentration occurs due to the limited volume of the CNS extracellular space. The change in potassium concentration in the extracellular space impacts a variety of neuronal processes, such as maintenance of membrane potential, activation and inactivation of voltage gated channels, synaptic transmission, and electrogenic transport of neurotransmitter. Change of extracellular potassium concentration of from 3mM can affect neural activity. Therefore, there are diverse cellular mechanisms for tight control of potassium ions, the most widely accepted mechanisms being K+ spatial buffering mechanism. Orkand and his colleagues who first theorized spatial buffering stated “if a Glial cell becomes depolarized by K+ that has accumulated in the clefts, the resulting current carries K+ inward in the high [K+] region and out again, through electrically coupled Glial cells in low [K+] regions” \nIn the model presented by Orkand and his colleagues, glial cells intake and traverse potassium ions from region of high concentrations to region of low concentration maintaining potassium concentration to be low in extracellular space. Glial cells are well suited for transportation of potassium ions since it has unusually high permeability to potassium ions and traverse long distance by its elongated shape or by being coupled to one another.\n\nPotassium buffering can be broadly categorized into two categories: Potassium uptake and Potassium spatial buffering. For potassium uptake, excess potassium ions are temporarily taken into glial cells through transporters, or potassium channels. In order to preserve electroneutrality, potassium influxes into glial cells are accompanied by influx of chlorine or efflux of sodium. It is expected that when potassium accumulates within glial cells, water influx and swelling occurs. For potassium spatial buffering, functionally coupled glial cells with high potassium permeability transfer potassium ions from regions of elevated potassium concentration to regions of lower potassium concentration. The potassium current is driven by the difference in glial syncytium membrane potential and local potassium equilibrium potential. When one region of potassium concentration increases, there is a net driving force causing potassium to flow into the glial cells. The entry of potassium causes a local depolarization that propagates electrotonically through the glial cell network which causes net driving force of potassium out of the glial cells. This process causes dispersion of local potassium with little net gain of potassium ions within the glial cells, which in turn prevents swelling. Glial cell depolarization caused by neuronal activity releases potassium onto blood stream, which was once widely hypothesized to be cause of vessel relaxation, was found to have little affect on neurovascular coupling. Despite the efficiency of potassium spatial buffering mechanisms, in certain regions of CNS, potassium buffering seems more dependent on active uptake mechanisms rather than spatial buffering. Therefore, the exact role of glial potassium spatial buffering in the various regions of our brain still remains uncertain.\n\nThe high permeability of glial cell membranes to potassium ions is a result of expression of high densities of potassium-selective channels with high open-probability at resting membrane potentials. Kir channels, potassium inward-rectifying channels, allow passage of potassium ions inward much more readily than outward. They also display a variable conductance that positively correlates with extracellular potassium concentration: the higher the potassium concentration outside the cell, the higher the conductance. \n\nKir channels are categorized into seven major subfamilies, Kir1 to Kir7, with a variety of gating mechanisms. Kir3 and Kir6 are primarily activated by intracellular G-proteins. Because they have a relatively low open-probability compared to the other families, they have little impact on potassium buffering. Kir1 and Kir7 are mainly expressed in epithelial cells, such as those in kidney, choroid plexus, or retinal pigment epithelium, and have no impact on spatial buffering. Kir2, however, are expressed in brain neurons and glial cells. Kir4 and Kir5 are, along with Kir2, located in Muller glia and play important roles in potassium siphoning. There are some discrepancies among studies on expression of these channels in the stated locations.\n\nThe panglial syncytium is a large network of interconnected glial cells, which are extensively linked by gap junctions. The panglial syncytium spreads through central nervous system where it provides metabolic and osmotic support, as well as ionic regulation of myelinated axons in white matter tracts. The three types of macroglial cells within network of panglial syncytium are astrocytes, oligodendrocytes, and ependymocytes. Originally it was believed that there was homologous gap junction between oligodendrocytes. It was later found through untrastructural analysis that gap junctions do not directly link adjacent oligodendrocytes, rather it gap junctions with adjacent astrocytes, providing secondary pathway to nearby oligodendrocytes. With direct gap junction between myelin sheaths to surrounding astrocytes, excess potassium and osmotic water directly enters astrocyte syncytium, where it downstream to astrocyte endfeet at capillaries and the glia limitans. \n\nPotassium spatial buffering that occurs in the retina is called potassium siphoning, where the Muller cell is the principal glial cell type. Muller cells have important role in retinal physiology. It maintains retinal cell metabolism and are critical in maintaining potassium homeostasis in extracellular space during neuronal activity. Like cells responsible for spatial buffering, Muller cells are distinctively permeable to potassium ions through Kir channels. Like other glial cells, the high selectivity of Muller cell membranes to potassium ions is due to the high density of Kir channels. Potassium conductance is unevenly distributed in Muller cells. By focally increasing potassium ions along amphibian Muller cells and recording the resulting depolarization, the observed potassium conductance was concentrated in the endfoot process of 94% of the total potassium conductance localized to the small subcellular domain. The observation lead to hypothesis that excess potassium in extracellular space is “siphoned” by the Muller cells to the vitreous humor. Potassium siphoning is a specialized form of spatial buffering mechanisms where large reservoir of potassium ions is emptied into vitreous humor. Similar distribution pattern of Kir channels could be found in amphibians.\n\nExistence of potassium siphoning was first reported in 1966 study by Orkand et al. In the study, optic nerve of Necturus was dissected to document the long-distance movement of potassium after the nerve stimulation. Following the low frequency stimulation of .5 Hz at the retinal end of the dissected optic nerve, depolarization 1-2mV was measured at astrocytes at the opposite end of the nerve bundle, which was up to several millimeters from the electrode. With higher frequency stimulation, higher plateau of depolarization was observed. Therefore, they hypothesized that the potassium released to extracellular compartment during axonal activity entered and depolarized nearby astrocytes, where it was transported away by unfamiliar mechanism, which caused depolarization on astrocytes distant from site of stimulation. The proposed model was actually inappropriate since at the time neither gap junctions nor syncytium among glial cells were known, and optic nerve of Necturus are unmyelinated, which means that potassium efflux occurred directly into the periaxonal extracellular space, where potassium ions in extracellular space would be directly absorbed into the abundant astrocytes around axons.\n\nIn patients with Tuberous Sclerosis Complex (TSC), abnormalities occur in astrocyte, which leads to pathogenesis of neurological dysfunction in this disease. TSC is a multisystem genetic disease with mutation in either TSC1 or TSC2 gene. It results in disabling neurological symptoms such as mental retardation, autism, and seizures. Glial cells have important physiological roles of regulating neuronal excitability and preventing epilepsy. Astrocytes maintain homeostasis of excitatory substances, such as extracellular potassium, by immediate uptake through specific potassium channels and sodium potassium pumps. It is also regulated by potassium spatial buffering via astrocyte networks where astrocytes are coupled through gap junctions. Mutations in TSC1 or TSC2 gene often results in decreased expression of the astrocytic connexin protein, Cx43. With impairment in gap junction coupling between astrocytes, myriad of abnormalities in potassium buffering occurs which results in increased extracellular potassium concentration and may predispose to neuronal hyperexcitability and seizures. According to a study done on animal model, connexin43-deficient mice showed decreased threshold for the generation of epileptiform events. The study also demonstrated role of gap junction in accelerating potassium clearance, limiting potassium accumulation during neuronal firing, and relocating potassium concentrations.\n\nDemyelinating Diseases of the central nervous system, such as Neuromyelitis Optica, often leads to molecular components of the panglial syncytium being compromised, which leads to blocking of potassium spatial buffering. Without mechanism of potassium buffering, potassium induced osmotic swelling of myelin occurs where myelins are destroyed and axonal salutatory conduction ceases.\n",
    "id": "20445291",
    "title": "Potassium spatial buffering"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=8355163",
    "text": "Radioactivity in the life sciences\n\nRadioactivity is generally used in life sciences for highly sensitive and direct measurements of biological phenomena, and for visualizing the location of biomolecules radiolabelled with a radioisotope.\n\nAll atoms exist as stable or unstable isotopes and the latter decay at a given half-life ranging from attoseconds to billions of years; radioisotopes useful to biological and experimental systems have half-lives ranging from minutes to months. In the case of the hydrogen isotope tritium (half-life = 12.3 years) and carbon-14 (half-life = 5,730 years), these isotopes derive their importance from all organic life containing hydrogen and carbon and therefore can be used to study countless living processes, reactions, and phenomena. Most short lived isotopes are produced in cyclotrons, linear particle accelerators, or nuclear reactors and their relatively short half-lives give them high maximum theoretical specific activities which is useful for detection in biological systems.\nRadiolabeling is a technique used to track the passage of a molecule that incorporates a radioisotope through a reaction, metabolic pathway, cell, tissue, organism, or biological system. The reactant is 'labeled' by replacing specific atoms by their isotope. Replacing an atom with its own radioisotope is an intrinsic label that does not alter the structure of the molecule. Alternatively, molecules can be radiolabeled by chemical reactions that introduce an atom, moiety, or functional group that contains a radionuclide. For example, radio-iodination of peptides and proteins with biologically useful iodine isotopes is easily done by an oxidation reaction that replaces the hydroxyl group with iodine on tyrosine and histadine residues. Another example is to use chelators such DOTA that can be chemically coupled to a protein; the chelator in turn traps radiometals thus radiolabeling the protein. This has been used for introducing Yttrium-90 onto a monoclonal antibody for therapeutic purposes and for introducing Gallium-68 onto the peptide Octreotide for diagnostic imaging by PET imaging. (See DOTA uses.)\n\nRadiolabeling is not necessary for some applications. For some purposes, soluble ionic salts can be used directly without further modification (e.g., gallium-67, gallium-68, and radioiodine isotopes). These uses rely on the chemical and biological properties of the radioisotope itself, to localize it within the organism or biological system.\n\nMolecular imaging is the biomedical field that employs radiotracers to visualize and quantify biological processes using positron emission tomography (PET) and single-photon emission computed tomography (SPECT) imaging. Again, a key feature of using radioactivity in life science applications is that it is a quantitative technique, so PET/SPECT not only reveals where a radiolableled molecule is but how much is there.\n\nRadiobiology (also known as radiation biology) is a field of clinical and basic medical sciences that involves the study of the action of radioactivity on biological systems. The controlled action of deleterious radioactivity on living systems is the basis of radiation therapy.\n\nTritium (Hydrogen-3) is a very low beta energy emitter that can be used to label proteins, nucleic acids, drugs and almost any organic biomolecule. The maximum theoretical specific activity of tritium is 28.8 Ci/mmol (1.066 PBq/mol). However, there is often more than one tritium atom per molecule: for example, tritiated UTP is sold by most suppliers with carbons 5 and 6 each bonded to a tritium atom.\n\nFor tritium detection, liquid scintillation counters have been classically employed, in which the energy of a tritium decay is transferred to a scintillant molecule in solution which in turn gives off photons whose intensity and spectrum can be measured by a photomultiplier array. The efficiency of this process is 4–50%, depending on the scintillation cocktail used. \n\nCarbon-14 has a long half-life of 5,730±40 years. Its maximum specific activity is 0.0624 Ci/mmol (2.31 TBq/mol). It is used in applications such as radiometric dating or drug tests. C-14 labeling is common in drug development to do ADME (absorption, distribution, metabolism and excretion) studies in animal models and in human toxicology and clinical trials. Since tritium exchange may occur in some radiolabeled compounds, this does not happen with C-14 and may thus be preferred.\n\nSodium-22 and chlorine-36 are commonly used to study ion transporters. However, sodium-22 is hard to screen off and chlorine-36, with a half-life of 300,000 years, has low activity.\n\nSulfur-35 is used to label proteins and nucleic acids. Cysteine is an amino acid containing a thiol group which can be labeled by S-35. For nucleotides that do not contain a sulfur group, the oxygen on one of the phosphate groups can be substituted with a sulfur. This thiophosphate acts the same as a normal phosphate group, although there is a slight bias against it by most polymerases. The maximum theoretical specific activity is 1,494 Ci/mmol (55.28 PBq/mol).\n\nPhosphorus-33 is used to label nucleotides. It is less energetic than P-32 and does not require protection with plexi glass. A disadvantage is its higher cost compared to P-32, as most of the bombarded P-31 will have acquired only one neutron, while only some will have acquired two or more. Its maximum specific activity is 5,118 Ci/mmol (189.4 PBq/mol).\n\nPhosphorus-32 is widely used for labeling nucleic acids and phosphoproteins. It has the highest emission energy (1.7 MeV) of all common research radioisotopes. This is a major advantage in experiments for which sensitivity is a primary consideration, such as titrations of very strong interactions (i.e., very low dissociation constant), footprinting experiments, and detection of low-abundance phosphorylated species. 32P is also relatively inexpensive. Because of its high energy, however, its safe use requires a number of engineering controls (e.g., acrylic glass) and administrative controls. The half-life of 32P is 14.2 days, and its maximum specific activity is 9131 Ci/mmol.\n\nIodine-125 is commonly used for labeling proteins, usually at tyrosine residues. Unbound iodine is volatile and must be handled in a fume hood. Its maximum specific activity is 2,176 Ci/mmol (80.51 PBq/mol).\n\nA good example of the difference in energy of the various radionuclei is the detection window ranges used to detect them, which are generally proportional to the energy of the emission, but vary from machine to machine: in a Perkin elmer TriLux Beta scintillation counter , the H-3 energy range window is between channel 5–360; C-14, S-35 and P-33 are in the window of 361–660; and P-32 is in the window of 661–1024.\n\nIn liquid scintillation counting, a small aliquot, filter or swab is added to scintillation fluid and the plate or vial is placed in a scintillation counter to measure the radioactive emissions. Manufacturers have incorporated solid scintillants into multi-well plates to eliminate the need for scintillation fluid and make this into a high-throughput technique.\n\nA gamma counter is similar in format to scintillation counting but it detects gamma emissions directly and does not require a scintillant.\n\nA Geiger counter is a quick and rough approximation of activity. Lower energy emitters such as tritium can not be detected.\n\nAutoradiography: A tissue section affixed to a microscope slide or a membrane such as a Northern blot or a hybridized slot blot can be placed against x-ray film or phosphor screens to acquire a photographic or digital image. The density of exposure, if calibrated, can supply exacting quantitative information.\n\nPhosphor storage screen: The slide or membrane is placed against a phosphor screen which is then scanned in a phosphorimager. This is many times faster than film/emulsion techniques and outputs data in a digital form, thus it has largely replaced film/emulsion techniques.\n\nElectron microscopy: The sample is not exposed to a beam of electrons but detectors picks up the expelled electrons from the radionuclei.\n\nMicro-autoradiography: A tissue section, typically cryosectioned, is placed against a phosphor screen as above.\n\nQuantitative Whole Body Autoradiography (QWBA): Larger than micro-autoradiography, whole animals, typically rodents, can be analyzed for biodistribution studies.\n\nSchild regression is a radioligand binding assay. It is used for DNA labelling (5' and 3'), leaving the nucleic acids intact.\n\nA vial of radiolabel has a \"total activity\". Taking as an example γ32P ATP, from the catalogues of the two major suppliers, Perkin Elmer NEG502H500UC or GE AA0068-500UCI, in this case, the total activity is 500 μCi (other typical numbers are 250 μCi or 1 mCi). This is contained in a certain volume, depending on the radioactive concentration, such as 5 to 10 mCi/mL (185 to 370 TBq/m); typical volumes include 50 or 25 μL.\n\nNot all molecules in the solution have a P-32 on the last (i.e., gamma) phosphate: the \"specific activity\" gives the radioactivity concentration and depends on the radionuclei's half-life. If every molecule were labelled, the maximum theoretical specific activity is obtained that for P-32 is 9131 Ci/mmol. Due to pre-calibration and efficiency issues this number is never seen on a label; the values often found are 800, 3000 and 6000 Ci/mmol. With this number it is possible to calculate the total chemical concentration and the hot-to-cold ratio.\n\n\"Calibration date\" is the date in which the vial’s activity is the same as on the label. \"Pre-calibration\" is when the activity is calibrated in a future date to compensate for the decay occurred during shipping.\n\nPrior to the widespread use of fluorescence in the past three decades radioactivity was the most common label.\n\nThe primary advantage of fluorescence over radiotracers is that it does not require radiological controls and their associated expenses and safety measures. The decay of radioisotpes may limit the shelf life of a reagent, requiring its replacement and thus increasing expenses. Several fluorescent molecules can be used simultaneously (given that they do not overlap, cf. FRET), whereas with radioactivity two isotopes can be used (tritium and a low energy isotope, e.g. P due to different intensities) but require special equipment (a tritium screen and a regular phosphor-imaging screen, a specific dual channel detector, e.g. ).\n\nFluorescence is not necessary easier or more convenient to use because fluorescence requires specialized equipment of its own and because quenching makes absolute and/or reproducible quantification difficult.\n\nThe primary disadvantage of fluorescence versus radiotracers is a significant biological problem: chemically tagging a molecule with a fluorescent dye radically changes the structure of the molecule, which in turn can radically change the way that molecule interacts with other molecules. In contrast, intrinsic radiolabeling of a molecule can be done without altering its structure in any way. For example, substituting a H-3 for a hydrogen atom or C-14 for a carbon atom does not change the conformation, structure, or any other property of the molecule, it's just switching forms of the same atom. Thus an intrinscially radiolabeled molecule is identical to its unlabeled counterpart.\n\nMeasurement of biological phenomona by radiotracers is always direct. In contrast, many life science fluorescence applications are indirect, consisting of a fluorescent dye increasing, decreasing, or shifting in wavelength emission upon binding to the molecule of interest.\n\nIf good health physics controls are maintained in a laboratory where radionuclides are used, it is unlikely that the overall radiation dose received by workers will be of much significance. Nevertheless, the effects of low doses are mostly unknown so many regulations exist to avoid unnecessary risks, such as skin or internal exposure. Due to the low penetration power and many variables involved it is hard to convert a radioactive concentration to a dose.\n1 μCi of P-32 on a square centimetre of skin (through a dead layer of a thickness of 70 μm) gives 7961 rads (79.61 grays) per hour . Similarly a mammogram gives an exposure of 300 mrem (3 mSv) on a larger volume (in the US, the average annual dose is 620 mrem or 6.2 mSv ).\n\n",
    "id": "8355163",
    "title": "Radioactivity in the life sciences"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=29880293",
    "text": "Gender typing\n\nGender typing is the process by which a child becomes aware of their gender and thus behaves accordingly by adopting values and attributes of members of the sex that they identify as their own. This process is extremely important for a child’s social and personality development because it largely impacts the child’s understanding of expected social behaviour and influences social judgments.\n\nGiven the definition, once the child is aware of their gender they will start to behave in gender roles that their same-sex models would normally adopt. Therefore, these individual responses become internalized and function according to the appropriate gender-role standards. The responses that individuals receive from their social group will mold their identity – becoming more feminine or masculine – and thus affect the way they view the world. Other facets of the process can also result in atypical development. Albeit, whether a child develops shared traits, cross-gender identities, or androgyny, their decision begins with the identification of a gender and the models he or she chooses to emulate. The behaviors they adopt will ultimately shape their knowledge and identity for who they are and how they should behave.\n\nSigmund Freud believed that as a child progresses through each of the different stages in psychosexual development, during the third year of the child’s life, genitals are thought to become active. As children develop a greater understanding of their own sex they also develop either a castration complex (in boys) or penis envy (in girls). For boys, during the “phallic” stage, they are at the height of childhood sexuality. During this the Oedipus complex occurs, where the boy feels erotic love for his mother (Electra complex in girls where love is directed towards the father). As time progresses and the boy matures, he is slowly able to let go of the rival feelings he has towards his father and free himself from his love for his parents. At this time, the boy learns to emulate masculine attributes from his father and subsequently to identify with him.\nThe girl’s development, Freud argued, is more complicated. Generally, as with males, the first object of interest is also the mother figure and for the first four years and beyond a girl remains attached to her mother. However, when the female child learns about “castration” it sparks disappointment and she blames her mother for her lack of a penis. Because of this, the girl gives up masturbation and in turn shifts focus from her mother to her father. By abandoning masturbation the girl can no longer be active, thus displaying a passive nature. The father then assists her by smoothly transitioning her towards a more feminine path. Furthermore, the girl’s affection towards her father will also influence her to emulate her mother’s feminine qualities and eventually adopt more gender-typed behaviours.\n\nThe cognitive-developmental theory is also closely linked to Jean Piaget’s analysis for the age-related cognitive changes a child goes through. Lawrence Kohlberg suggested that cognition comes before action and behavior (“I am a boy so I do boy-like things”). This emphasizes the importance of a child’s understanding about gender roles and their permanent placement in it. After a child can fully grasp this concept, gender-specific information will become more relevant. This idea of gender consistency, similar to Piaget’s concrete-operational stage, is represented by three stages:\n\nWhen the child is able to fully grasp gender stable understanding about themselves, usually between the ages of five and seven, the motivation to master their orientation and to socialize themselves allows them to seek out same-sexed models to learn more about gender-stereotypic behaviors.\n\nA schema is a cognitively organized network of associations that is readily available to help guide an individual’s perception. Gender schema acts as a guide or standard for consistent behavior in a given scenario. Labels such as “girls are weak and boys are strong,” classifies what stereotypically acceptable actions for the gender groups are. Therefore, the theory proposes the idea that once the child has developed basic knowledge on gender behaviors they will begin to construct gender schemas. This is acquired first through the basic understanding of gender-specific roles. In order words, the child learns the contents of the society, things that are related to their own and the opposite sex, and incorporates it into their gender schemas. The child then learns to apply the appropriate attributes respectively to the right gender by selectively using this knowledge to conceptualize their own actions. Hence, categorizing how they should perform in various situations by molding their capabilities to match the schematic labels.\n\nSocial learning theorists, like Albert Bandura, suggest that adults not only provide models for children to imitate, but that they also are actively involved in influencing a child’s gender-role identification. The Social learning theory proposes that gender-identities and gender-role preferences are acquired through two concepts.\n\n\n\nThere are many opportunities for a child to learn and develop their own understanding of what “gender” is. Thus as children progresses from childhood into adolescence they will already have been exposed to many factors that will influence their ideas and attitudes for normative social behaviors regarding gender roles. Social models, such as parents, siblings, and the media become extremely important during different stages of the child’s development.\n\nParents:\nParents play a vital part in a child’s early life, for they are the first group of people that a child meets and learns from. The information that surrounds a child at home becomes reinforcements for desired behaviors of a male or female. Studies have shown that as immediate as 24 hours after a child is born most parents have already engaged in gender stereotypic expectations of sons or daughters. Through examples such as painting a room pink or blue, encouragement to participate in shared sex-typed activities, offering gender differentiated toys, or treating the opposite sex child differently, these parent-child interactions have long lasting influence on how a child connects to certain gender-specific behaviors.\nFurthermore, various evidence suggest that certain household differences affecting how a child is raised can influence how similar (or different) a child is to the opposite sex, therefore how “feminine” or “masculine” the child can potentially become. For instance, as certain research demonstrates, in the absence of a father figure boys are generally more “feminine” than those living with a father. This demonstrates the significance of father-son modeling. The same research reveals that boyhood femininity is more strongly correlated with parental reinforcement such as a father or mother’s desire for a girl and/or their approval for feminine behaviours.\n\nSiblings:\nApart from parents, children also seek reinforcement from their older siblings. Therefore, inconsistencies for gender behaviors can also be as a result of children emulating their siblings of opposite sex.\nSibling’s influence is often most effective when the sibling is of an age that is more advanced than the child himself, therefore increasing the motivation for the child to model after their brother or sister. The impact of older siblings are power predictors for the younger sibling’s gender role attitudes, sex-typed personality qualities, and masculine leisure activities. Findings suggest that girls develop less traditional attitudes than boys, thus, relative to stereotypically traditional development, older male siblings are more conscientious towards masculine activities, which are evidently modeled after by younger siblings more than feminine activities. Moreover, older brothers tend to have stronger influence over a younger sibling’s sexual development. There is evidence that the relationship between older brother and younger sister can, in fact, influence the sister to become more feminine, abiding to more stereotypical gender-typed development than girls with older sisters. Interestingly, biological studies show that dizygotic twins with the opposite- sex co-twin gender show more sex-typed behaviors than same-sex twins.\nConversely, this can also develop in an opposite direction in which children may try to diverge themselves from their siblings as much as possible, making their differences more salient; however, this may be more evident in first-borns possibly due to the birth order.\n\nPeers:\nAt a young age, children can already utilize their knowledge of different social categories to form stereotypes about what they understand about men and women. Interaction with peer groups will often shape people’s behavior to fit normative expectations. Children often group together with other children of the same-gender. Belonging in a group that shares the same gender identity will often endorse more gender appropriate traits. An example of this is the fact that girls have more expressive traits than boys. Research also show that children often engage in play with same-sex peers, and exclude others who are different from the norm. For example, children who want to join a group will only be allowed to engage in play if they are a same-gender peer who has had prior experience with the activity, or an opposite-gender peer who has not had prior experience. The desire to engage in play can influence a child to behave like to their peers. In later life, as children starts to move away from their parents, the role of friendship becomes much more influential.\n\nMedia:\nChildren learn about different gender categories by observing various forms of media. They often look for gender roles, with whom they can relate to, from books and television. Conversely, these sources of media will also stereotypically shape a child’s understanding for gender acceptable behaviors. Studies examining the effects of gender stereotyping in children’s literature describe that, most often than not, gender views are affiliated with stereotypes which are both culturally and individually constructed. In children’s literature, male characters appear to be more central as well as less emotional and stronger. Female characters on the other hand worked out their feelings through expression, they are more dependent and usually adopt the roles of more domesticated characters.\nMoreover, these characteristics are also seen in television programing. For example, in most prime-time television shows women receive twice as many comments about her appearance than men. Similarly, a study investigating the cartoon “Pokémon” and gender role expresses the differences in “good” and “bad” characters. Jesse and James (villains of the story) are portrayed to have adopted counter-stereotypical portrayal. While Jesse is seen as more aggressive and James as more feminine, it subtly teaches children that nontraditional or nonstereotypical gender role behaviors are bad. Furthermore, children in the story have difficultly recalling a male Pokémon revealing that there is an imbalance in which prominence is given to male characters.\nThese depictions of male and female character roles can potentially become unconsciously influential in the way a child constructs gender views.\n\nIt is expected for children to develop gender-typed behavioural cues given that children often use gender-related information to make judgments. Society often rewards shared traditional behaviours, especially for men, and thus having progressed to a cross-gender development can lead to unwanted criticisms and punishment. Therefore, when children undergo atypical gender development, due to both genetic and environmental contributors, it can drastically alter normal development from a person’s sense of self-worth, self-esteem, to feelings of acceptance. When the child identifies as the opposite sex, he or she is then diagnosed with gender dysphoria (often termed \"gender identity disorder\" or GID).\n\nGenetics vs. environmental contributors:\nThe development of gender roles has been associated with both genetic and social factors.\nCurrent biological research has suggested that testosterone differences can affect sexual orientation, gender identity, and personality. For example, girls with increased prenatal testosterone levels, due to congenital adrenal hyperplasia (CAH), show more male-typed behaviors than the average female. This may cause a decrease in empathy levels, which is proven to be higher in the average women than men, and an increase in physical aggression, typically higher in men than women. Furthermore, multiple twin studies have also shown that homosexual traits displayed greater congruence in monozygotic (MZ) twins than dizygotic (DZ) twins displaying genetic contributions. Further studies reveal that this is especially true for girls' gender-typed values. Similarly, other studies have summarized that the prevalence of cross-gender behavior a highly heritable trait where MZ twins display the same cross-gender behaviors more so than DZ twins.\nWhile atypical gender role development may be heritable, this does not mean that it is independent. However, environmental effects can differ largely for different genders. Because society is more accepting of male traits (girls playing soccer) than female traits (boys doing ballet), society’s negativity typically forces males to try to stay within the stereotypic norms. That being said however, male influence accounted by environmental factors that lead to atypical development is also stronger for boys than girls in terms of atypical development.\n\nGender dysphoria:\nGender dysphoria or gender identity disorder (GID) occurs when the child identifies himself or herself as the opposite sex. GID, previously known as transsexualism, occurs when a person has a strong desire to be the opposite sex because they feel uncomfortable in their own body. This may lead to cross-dressing or the desire to rid of their physical characteristic. GID is accompanied with a distress that the individual cannot change their sex and a strong rejection for sex-typical behaviours. Onset of most of these behaviors occurs as early as two to four years of age.\n\nControversies about gender in DSM-5:\nFor the fifth edition of the \"Diagnostic and Statistical Manual of Mental Disorders\" (DSM-5), there has been a tremendous amount of debate around whether the sexual and gender identity, which includes the GID, qualifies as a mental disorder. One of the most notable discussions for this occurred in 2003 during the annual APA meeting, where Darryl B. Hill, Ph.D., argued that the GID should not be considered a mental disorder. He explains that due to the limited amount of reliable and valid evidence, given the role of parents and therapies, for whether GID meets the conditions as a mental disorder among children and adolescents. Furthermore, other researchers have stated that the diagnosis does not give recognition to the individual’s discomfort with their biological sex which is completely detached from their gender assignment. This leaves patients susceptible to social changes for what acceptably feminine or masculine. Lastly, it does not provide evidence for nonconformity to traditional gender behaviours which varies between cultures, life stages, genders, and ethnic groups.\n\nAndrogyny:\nRecent studies have shown that androgynous people are able to enhance performance cross-situationally because they can alter their behaviors appropriately to becoming more “masculine” or “feminine” in the given context. According to a study, a person’s activity preference in games and interests are purely based on their gender-typed stereotypes, making the person significantly more stereotyped than androgynous people. In a self-esteem test, when individuals were given opposite-sex activities, gender-typed individuals generally felt more uncomfortable, leading to decreased levels of self-esteem. However, androgynous subjects did not feel discomfort or pessimism about themselves. Therefore, gender typing can often lead to specific gender-rule stereotypes to better facilitate decision making which can adhere to certain benefits as well as limitations. Therefore, parents who foster more non-traditional views in sex role orientation tend to encourage a less discriminatory environment. The child can then freely engage in more choices that are not affected by gender limitations. Androgynous children have been found to have higher self-esteem and higher self-worth.\n\nDrawbacks\nRecent studies have shown that gender typing is, in fact, not just related to gender-related characteristics that is congruous with the person’s biological sex, but rather holds different dimensions. Several studies have also revealed the perks of androgynous individuals such as having more adaptability towards gender-specific situations as well as more flexible attitudes about sex roles. Therefore, while there appear to be many benefits for a gender-congruent identity, it can also result in limitations. Because gender typing often reinforces stereotypes, it tends to attract negative and rigid attitudes towards atypical gender characteristics, activities, and interests. This reinforces the idea that gender-typing does result in certain limitations towards the construction of one’s identity.\n\nBoy Raised as Girl\nAfter a circumcision accident, Bruce’s parents turned to Psychologist Dr. John Money who suggested they raise the boy as a girl. Money believes that biology does not determine our gender identification; our environment does. At 17 months old, Bruce was renamed \"Brenda\". However, at age 13, \"Brenda\" became unhappy, lonely, and refused to pursue anything female-typed. Brenda’s parents chose to tell Brenda the truth, and from then on he named himself David. This story ended in tragedy; David committed suicide after his wife left him and he lost his job.\n\nKate:\nBen had never shown interest in “boy” toys, they wanted to wear girl’s clothing and grow long hair. Soon they started to make their gender clear: they talked about being a girl and their identity not matching anatomy. Finally, Ben told family they would go by Kate. In the beginning, the child lived a double life. In school, she was “Ben”; at home, she was Kate. Characteristics between the two identities drastically varied. Ben was quiet, shy and withdrawn, while Kate was extroverted and happy. Her parents could see that this was taking a toll on Kate and so they changed to a school where Kate could attend as herself. As the parent learned through this experience, supporting a child’s gender identity is extremely important.\n\nPop:\nIn Sweden, there is a couple who raised their child, Pop, by keeping the child’s gender a secret, despite a few people who changed the baby’s diaper. The parents stated that they wanted the baby to grow up free and without a gender mold. Pop’s wardrobe contains everything from pants to dresses. Pop’s parents avoid using pronouns and just refer to their child as “Pop.”\n",
    "id": "29880293",
    "title": "Gender typing"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=31351275",
    "text": "Pulsatile flow\n\nIn fluid dynamics, a flow with periodic variations is known as pulsatile flow, or as Womersley flow. The flow profiles was first derived by John R. Womersley (1907–1958) in his work with blood flow in arteries. The cardiovascular system of chordate animals is a very good example where pulsatile flow is found, but pulsatile flow is also observed in engines and hydraulic systems, as a result of rotating mechanisms pumping the fluid.\n\nThe pulsatile flow profile is given in a straight pipe by\n\nwhere:\n\nThe pulsatile flow profile changes its shape depending on the Womersley number\n\nFor formula_3, viscous forces dominate the flow, and the pulse is considered quasi-static with a parabolic profile.\nFor formula_4, the inertial forces are dominant in the central core, whereas viscous forces dominate near the boundary layer. Thus, the velocity profile gets flattened, and phase between the pressure and velocity waves gets shifted towards the core.\n\nThe Bessel function at its lower limit becomes\n\nwhich converges to the Hagen-Poiseuille flow profile for steady flow for\n\nor to a quasi-static pulse with parabolic profile when\n\nIn this case, the function is real, because the pressure and velocity waves are in phase.\n\nThe Bessel function at its upper limit it becomes\n\nwhich converges to\n\nThis is highly reminiscent of the Stokes layer on an oscillating flat plate, or the skin-depth penetration of an alternating magnetic field into an electrical conductor.\nOn the surface formula_10, but the exponential term becomes negligible once formula_11 becomes large, the velocity profile becomes almost constant and independent of the viscosity. Thus, the flow simply oscillates as a plug profile in time according to the pressure gradient,\n\nHowever, close to the walls, in a layer of thickness formula_13, the velocity adjusts rapidly to zero. Furthermore, the phase of the time oscillation varies quickly with position across the layer. The exponential decay of the higher frequencies is faster.\n\nFor deriving the analytical solution of this non-stationary flow velocity profile, the following assumptions are taken:\n\nThus, the Navier-Stokes equation and the continuity equation are simplified as\n\nand\n\nrespectively. The pressure gradient driving the pulsatile flow is decomposed in Fourier series,\n\nwhere formula_17 is the imaginary number, formula_18 is the angular frequency of the first harmonic (i.e., formula_19), and formula_20 are the amplitudes of each harmonic formula_21. Note that, formula_22 (standing for formula_23) is the steady-state pressure gradient, whose sign is opposed to the steady-state velocity (i.e., a negative pressure gradient yields positive flow). Similarly, the velocity profile is also decomposed in Fourier series in phase with the pressure gradient, because the fluid is incompressible,\n\nwhere formula_25 are the amplitudes of each harmonic of the periodic function, and the steady component (formula_23) is simply Poiseuille flow\n\nThus, the Navier-Stokes equation for each harmonic reads as\n\nWith the boundary conditions satisfied, the general solution of this ordinary differential equation for the oscillatory part (formula_29) is\n\nwhere formula_31 is the Bessel function of first kind and order zero, formula_32 is the Bessel function of second kind and order zero, formula_33 and formula_34 are arbitrary constants, and formula_35 is the dimensionless Womersley number. The axisymetic boundary condition (formula_36) is applied to show that formula_37 for the derivative of above equation to be valid, as the derivatives formula_38 and formula_39 approach infinity. Next, the wall non-slip boundary condition (formula_40) yields formula_41. Hence, the amplitudes of the velocity profile of the harmonic formula_21 becomes\n\nwhere formula_44 is used for simplification.\nThe velocity profile itself is obtained by taking the real part of the complex function resulted from the summation of all harmonics of the pulse,\n\nFlow rate is obtained by integrating the velocity field on the cross-section. Since,\n\nthen\n\nTo compare the shape of the velocity profile, it can be assumed that\n\nwhere\n\nis the shape function.\nIt is important to notice that this formulation ignores the inertial effects. The velocity profile approximates a parabolic profile or a plug profile, for low or high Womersley numbers, respectively.\n\nFor straight pipes, wall shear stress is\n\nThe derivative of a Bessel function is\n\nHence,\n\nIf the pressure gradient formula_20 is not measured, it can still be obtained by measuring the velocity at the centre line. The measured velocity has only the real part of the full expression in the form of\n\nNoting that formula_55, the full physical expression becomes\n\nat the centre line. The measured velocity is compared with the full expression by applying some properties of complex number. For any product of complex numbers (formula_57), the amplitude and phase have the relations formula_58 and formula_59, respectively. Hence,\n\nand\n\nwhich finally yield\n\n",
    "id": "31351275",
    "title": "Pulsatile flow"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=30906179",
    "text": "Allometric engineering\n\nAllometric engineering is the process of experimentally shifting the scaling relationships, for body size or shape, in a population of organisms. More specifically, the process of experimentally breaking the tight covariance evident among component traits of a complex phenotype by altering the variance of one trait relative to another. Typically, body size is one of the two traits. The measurements of the two traits are plotted against each other and the scaling relationship can be represented as: formula_1. Manipulations of this sort alter the scaling relationships either by shifting the intercept (\"b\"), slope (\"m\") or both to create novel variants (see: Allometry, for more details). These novel variants can then be tested for differences in performance or fitness. Through careful testing, one could sequentially test each component of a trait suite to determine how each part contributes to the function of the entire complex phenotype, and ultimately the fitness of the organism. This technique allows for comparison within or among biological groups differing in size by adjusting morphology to match one another and comparing their performances.\n\nAllometric engineering has been used to test David Lack's hypothesis in the lizard \"Sceloporus occidentalis\". In this study, two populations were \"engineered\" to fit the morphology of the other by manipulating egg yolk quantity, removing effect of size difference between groups. After manipulation, they found that speed was inversely proportional to body size.\n\nMaternal investment was \"allometrically engineered\" by surgically removing an ovary in cockroaches (\"Diploptera punctata\"). This effectively reduced number of progeny and increased resource allocation to each offspring. They coupled this manipulation with group effects (faster development in large groups), and found that maternal investment can overcome group effect.\n\nThe male long-tailed widowbird (\"Euplectes progne\") has exceptionally long tail feathers roughly half a meter in length. Male tail feathers were cropped and glued and those with artificially enhanced tail lengths secured the most matings, demonstrating female preference.\n\nThe fly \"Zonosemata vittigera\" has a banding pattern on its wings that was found to mimic movements of a jumping spider. Greene \"et al.\" engineered novel phenotypes, breaking correlation between a behavior and morphology, by cutting and transplanting the wings of this fly with the common housefly. This manipulation demonstrated that it was behavior coupled with the banding pattern that deterred jumping spiders from attacking, though not other predators.\n\nCurrent uses have involved truncation or cropping, yolk manipulation, hormonal treatments, maternal allocation, temperature manipulation, or altering the nutritional states. Each method undoubtedly has its merits and pitfalls to consider before designing an experiment, but these techniques are opening new avenues of research in comparative and evolutionary biology.\n\n",
    "id": "30906179",
    "title": "Allometric engineering"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=32000094",
    "text": "Compensatory growth (organism)\n\nCompensatory growth, known as catch-up growth and compensatory gain, is an accelerated growth of an organism following a period of slowed development, particularly as a result of nutrient deprivation. The growth may be with respect to weight or length (or height in humans). For example, oftentimes the body weights of animals who experience nutritional restriction will over time become similar to those of animals who did not experience such stress. It is possible for high compensatory growth rates to result in overcompensation, where the organism exceeds normal weight and often has excessive fat deposition.\n\nAn organism can recover to normal weight without additional time. Sometimes when the nutrient restriction is severe, the growth period is extended to reach the normal weight. If the nutrient restriction is severe enough, the organism may have permanent stunted growth where it does not ever reach normal weight. Usually in animals, complete recovery from carbohydrate and protein restriction occurs.\n\nCompensatory growth has been observed in a number of organisms including humans, other species of mammals, birds, reptiles, fish, plants (especially grasses and young tree seedlings and saplings), fungi, microbes, and damselflies.\n\nIn 1911, Hans Aron performed the earliest study of growth after periods of undernourishment. He underfed a dog and found that it still had the capacity to rapidly gain weight, though it did not reach the final weight of a dog that was fed normally. In 1915, Osborne and Mendel were the first to demonstrate that rats fed after growth restriction had an accelerated growth rate. In 1945, Brody developed the idea of “homoestasis of growth” in the book \"Bioenergetics and Growth\". In 1955, Verle Bohman was the first to use the term “compensatory growth” in an article pertaining to beef cattle.\n\nIn animals, homeostatic and homeorhetic processes are involved in the abnormally high growth rates. Homeostatic processes usually affect compensatory growth in the short term, whereas homeorhetic processes usually have a long-term effect.\n\nThe exact biological mechanisms for compensatory growth are poorly understood, though it is clear that in some animals the endocrine system is involved in the metabolism and nutrient partitioning in the tissues.\nFirst, during nutrient starvation, a reduction of basal metabolism takes place. The gut tissues are the first tissues to be reduced in weight and activity. Then, during the realimentation (re-feeding) phase, an increase in feeding enables more dietary protein and energy to be contributed for tissue growth instead of basal metabolism. The gut tissues are the first to increase in weight, followed by muscle tissue and finally adipose tissue.\n\nAnorexia nervosa can have serious implications if its duration and severity are significant and if onset occurs before the completion of growth, pubertal maturation or prior to attaining peak bone mass. Both height gain and pubertal development are dependent on the release of growth hormone and gonadotrophins (LH and FSH) from the pituitary gland. Suppression of gonadotropins in patients with anorexia nervosa has been frequently documented. In some cases, especially where onset is pre-pubertal, physical consequences such as stunted growth and pubertal delay are usually fully reversible. Height potential is normally preserved if the duration and severity of anorexia nervosa are not significant and/or if the illness is accompanied with delayed bone age (especially prior to a bone age of approximately 15 years), as hypogonadism may negate the deleterious effects of undernutrition on stature by allowing for a longer duration of growth compared to controls. In such cases, appropriate early treatment can preserve height potential and may even help to increase it in some post-anorexic subjects due to the aforementioned reasons in addition to factors such as long-term reduced estrogen-producing adipose tissue levels compared to premorbid levels.\n\nIn 1960, Wilson and Osborne outlined six factors that could affect compensatory growth in a review article. The importance of each, some, or all of these factors is not well understood. These factors are as follows:\n\nAnimal factors that can affect compensatory growth may include the maturity level and fat proportion of the animal at the time of nutrient deprivation, the genotype, the gender, and the metabolic changes. The stage of development of the animal when the nutrient restriction occurs greatly affects its body composition.\n\n",
    "id": "32000094",
    "title": "Compensatory growth (organism)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=35942883",
    "text": "Biological dark matter\n\nBiological dark matter is an informal term for genetic material or microorganisms that are unclassified or poorly understood.\n\nBiological dark matter includes non-coding DNA (junk DNA) and non-coding RNA. Much of the genomic dark matter is thought to originate from ancient transposable elements and from other low-complexity repetitive elements. Uncategorized genetic material is found in humans and in several other organisms. Their phylogenetic novelty could indicate the cellular organisms or viruses from which they evolved.\n\nBiologists are unable to culture and grow 99% of all living microorganisms, so few functional insights exist about the metabolic potential of these organisms.\n\n",
    "id": "35942883",
    "title": "Biological dark matter"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=38045209",
    "text": "Despeciation\n\nDespeciation is the loss of a unique species of animal due to its combining with another previously distinct species. It is the opposite of Speciation and is much more rare. It is similar to extinction in that there is a loss of a unique species but without the associated loss of a biological lineage.\n\nFor example, Taylor et al.'s genetic analysis of three-spined sticklebacks across six lakes in southwestern British Columbia found two distinct species in 1977 and 1988 but only one combined species in data from 1997, 2000, and 2002. They concluded that external factors had imperiled the living conditions of the two species, thus eliminating the evolutionary specializations that had kept them unique.\n",
    "id": "38045209",
    "title": "Despeciation"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=38523023",
    "text": "Unconventional protein secretion\n\nUnconventional protein secretion (known as ER/Golgi-independent protein secretion or nonclassical protein export ) represents a manner in which the proteins are delivered to the surface of plasma membrane or to extracellular matrix regardless of ER/Golgi- dependent protein secretion. This includes cytokines and mitogens with crucial function in complex processes such as inflammatory response or tumor-induced angiogenesis. Most of these proteins are involved in processes in higher eukaryotes, however unconventional export mechanism was found in lower eukaryotes too. Even proteins folded in their correct conformation can pass plasma membrane this way, unlike proteins transported via ER/Golgi pathway. Two types of unconventional protein secretion are these: signal-peptid-containing proteins and cytoplasmatic and nuclear proteins that are missing an ER-signal peptide (1).\n\nThese proteins contain a specific signal-peptide sequence, which is to be translated into the endoplasmic reticulum, but are, however, able to reach the cell surface unconventionally. They can be packed into COPII-coated vesicle and directly fuse with plasma membrane or can fuse with endosomal or lysosomal compartment. Alternatively they can be packed into non-COPII-coated vesicle as well and fuse with Golgi (before reaching plasma membrane) or directly delivered to plasma membrane.\n\nSoluble proteins can reach the surface of the cell both by non-vesicular and vesicular mechanism. Non-vesicular mechanism use a carrier to get protein into extracellular space (for example phosphatidylinositol-4,5-bisphoshate). Vesicular mechanism can use lysosome-dependent pathway, microvesicle shedding or biogenesis of multivesicular bodies.\n",
    "id": "38523023",
    "title": "Unconventional protein secretion"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=38260038",
    "text": "Bridged nucleic acid\n\nBridged nucleic acids (BNAs) are modified RNA nucleotides. They are sometimes also referred to as constrained or inaccessible RNA molecules. BNA monomers can contain a five-membered, six-membered or even a seven-membered bridged structure with a “fixed” C’-endo sugar puckering. The bridge is synthetically incorporated at the 2’, 4’-position of the ribose to afford a 2’, 4’-BNA monomer. The monomers can be incorporated into oligonucleotide polymeric structures using standard phosphoamidite chemistry. BNAs are structurally rigid oligo-nucleotides with increased binding affinities and stability.\n\nChemical structures of BNA monomers containing a bridge at the 2’, 4’-position of the ribose to afford a 2’, 4’-BNA monomer as synthesized by Takeshi Imanishi’s group. The nature of the bridge can vary for different types of monomers. The 3D structures for A-RNA and B-DNA were used as a template for the design of the BNA monomers. The goal for the design was to find derivatives that possess high binding affinities with complementary RNA and/or DNA strands.\n\nThe presence of 2’-hydroxyls in the RNA backbone favors a structure that resembles the A-form structure of DNA. The flexible five-membered furanose ring in nucleotides exists in equilibrium of two preferred conformations of the N- (C’-endo, A-form) and the S-type (C’-endo, B-form) as illustrated in the next figure.\n\nAn increased conformational inflexibility of the sugar moiety in nucleosides (oligonucleotides) results in a gain of high binding affinity with complementary single-stranded RNA and/or double-stranded DNA. The first 2’,4’-BNA (LNA) monomers were first synthesized by Takeshi Imanishi’s group in 1997 followed independently by Jesper Wengel’s group in 1998.\nChemical structures of other BNAs that were synthesized in the past years as indicated below the structures.\n\nBNA nucleotides can be incorporated into DNA or RNA oligonucleotides at any desired position. Such oligomers are synthesized chemically and are now commercially available. The bridged ribose conformation enhances base stacking and pre-organizes the backbone of the oligonucleotide significantly increasing their hybridization properties. \nThe incorporation of BNAs into oligonucleotides allows the production of modified synthetic oligonucleotides with:\n\n\nChemical structures of BNAs were introduced in 2007 by Imanishi’s group. These new generation of BNAs analogues are called 2’,4’-BNA[NH], 2’,4’-BNA[NMe], and 2’,4’-BNA[NBn].\n\nNew BNA analogs introduced by Imanishi’s group were designed by taking the length of the bridged moiety into account. A six-membered bridged structure with a unique structural feature (N-O bond) in the sugar moiety was designed to have a nitrogen atom. This atom improves the formation of duplexes and triplexes by lowering the repulsion between the negatively charged backbone phosphates. These modifications allow to control the affinity towards complementary strands, regulate resistance against nuclease degradation and the synthesis of functional molecules designed for specific applications in genomics. The properties of these analogs were investigated and compared to those of previous 2’,4’-BNA (LNA) modified oligonucleotides by Imanishi’s group. Imanishi’s results show that “2’,4’-BNA-modified oligonucleotides with these profiles show great promise for applications in antisense and antigene technologies.”\n\nMakoto Koizumi in 2004 reviewed the properties of BNAs with focus on ENAs as antisense and antigen oligonucleotides (AONs) and proposed an action mechanism for these compounds that may involve translation arrest, mRNA degradation mediated by RNase H and splicing arrest. This is illustrated in the following figure.\n\nYamamoto et al. in 2012 demonstrated that BNA-based antisense therapeutics inhibited hepatic PCSK9 expression, resulting in a strong reduction of the serum LDL-C levels of mice. The findings supported the hypothesis that PCSK9 is a potential therapeutic target for hypercholesterolemia and the researchers were able to show that BNA-based antisense oligonucleotides (AONs) induced cholesterol-lowering action in hypercholesterolemic mice. A moderate increase of aspartate aminotransferase, ALT, and blood urea nitrogen levels was observed whereas the histopathological analysis revealed no severe hepatic toxicities. The same group, also in 2012, reported that the 2’,4’-BNA[NMe] analog when used in antisense oligonucleotides showed significantly stronger inhibitory activities which is more pronounced in shorter (13- to 16mer) oligonucleotides. Their data led the researchers to conclude that the 2’,4’-BNA[NMe] analog may be a better alternative to conventional LNAs.\n\nSome of the benefits of BNAs include:\n\nApplication of BNAs include:\n\n",
    "id": "38260038",
    "title": "Bridged nucleic acid"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=39269199",
    "text": "Heterolysis (biology)\n\nIn biology, heterolysis refers to apoptosis induced by hydrolytic enzymes from surrounding (usually inflammatory) cells. Autolysis is apoptosis of a cell by its own enzymes.\n",
    "id": "39269199",
    "title": "Heterolysis (biology)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=39269279",
    "text": "Homolysis (biology)\n\nIn biology, homolysis refers to a dividing cell resulting in two equal-size daughter cells.\n",
    "id": "39269279",
    "title": "Homolysis (biology)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=39207238",
    "text": "Dorsal nexus\n\nThe dorsal nexus is an area within the dorsal medial prefrontal cortex that serves as an intersection point for multiple brain networks. Research suggests it plays a role in the maintenance and manipulation of information, as well as supporting the control of cognitive functions such as behavior, memory, and conflict resolution. Abnormally increased connectivity between these networks through the Dorsal Nexus has been associated with certain types of depression. The activity generated by this abnormally high level of connectivity during a depressive state can be identified through Magnetic resonance imaging (MRI) and Positron emission tomography (PET).\n\nThe brain's intrinsic connections are divided into different networks that enable communication between the different structures: The cognitive control network, or Executive network (EN), the affective network or somatic network, and the default mode network. These regions are dependent on the dorsal nexus to communicate.\n\nThe EN is located in the dorsolateral prefrontal cortex and lateral parietal cortex, and is responsible for the maintenance and manipulation of the information in working memory. The EN also plays an important role as support of adaptive, goal-directed behaviors, which is why it is colloquially referred to as \"the problem solver.\"\n\nThe affective (or salience) network includes connections between the limbic area and subcortical areas, and is important during fear and vigilance states, as well as for autonomic and visceral regulation. It also generates the somatic sensations that accompany emotions.\n\nThe default mode is most active when the brain is at rest, or when a person is communicating socially. Its activity decreases during the performance of cognitively demanding tasks.\n\nNeuroimaging studies have shown that many neurological diseases and psychiatric disorders are associated with abnormalities in the functional connectivity of neural networks. MRIs indicate that the dorsal nexus is responsible for connecting these networks, and this might explain how symptoms of depression are influenced by the state of brain networks. The increased connectivity can produce symptoms of decreased focus and increased vigilance, which can present as paranoia, rumination or autonomic, visceral and emotional imbalance.\n\nSubjects with depression were observed to have abnormal connectivity in the bilateral parahipocampal cortex, as well as an increase of hyperintensity of white matter. Increased default-mode network connectivity, mediated via a region of the dorsomedial prefrontal cortex, may underline the characteristics of depression. In this pathology, the dorsal nexus is strongly connected to the task-positive, task-negative and affective networks. The function of this node is to allow enhanced “cross-talk” between networks, and this may explain how the diverse symptoms seen in depression converge The dorsal nexus can be related to two different types of depression: decreased and major depression. It is important to mention that there is a big difference between these two types: With decreased depression, the connectivity between the cingulated subgenual cortex and amygdala, pale striatum, and medial thalamus is diminished. In the case of major depression, connectivity is normal. This could explain differences in response to drugs and psychotherapy.\n\nNeuroimaging techniques allow of imaging of the nervous system in vivo, and permit scientists to explore the structures and functions of the human brain. In neuropsychiatry, neuroimaging techniques such as MRI and Positron Emission Tomography allow the identification of different networks that are implicated in various pathologies.\n\nIn the case of depression, portions of three different networks (the cognitive control network, the default mode network and the affective network) which are related with conflict resolution, making decisions, behavior, regulate memory and future planning present increased function in MRI’s. These three increased connectivity networks converged specifically on the dorsal nexus. The dorsal nexus has extremely high connectivity with large regions including dorso lateral prefrontal cortex, dorso medial prefrontal cortex, ventral medial prefrontal cortex, pregenual an subgenual cortex, posterior cingulate, and precuneus. Because these networks can be determined for each individual based on the strength of correlation to an a priori seed location, group statistical differences in networks can be evaluated on an image – wide basis.\n\nTreating the symptoms of depression has the purpose of reduce and control the dysfunction that patients could have in any areas of their life. The choice of treatment is based on the needs of the patient and can include drugs, therapy and other similar treatments. Regardless of the chosen treatment, if is necessary to consider possible side effects .\nIn the case of depression associated with dorsal nexus and other associated structures, reducing the increased connectivity might play a critical role reducing depression symptomatology and thus represent a potential therapy target for affective disorders \n\nSince glutamate is the most abundant and major excitatory neurotransmitter in the brain, pathophysiological changes in glutamatergic signaling are likely to affect neurobehavioral plasticity, information processing and large-scales changes in functional brain connectivity.Ketamine, a fast-acting general anesthetic derived from phencyclidine and use as a pediatric inductor, plays a non-well known role in the neural network dynamics at the healthy brain . The administration of ketamine in abnormal brain has the potential of reduce the increased function of the networks that are seen in depression. The therapeutic potential of ketamine may be explained by reversing disturbances in the glutamatergic system and restoring parts of a disrupted neurobehavioral homeostasis where several structural, metabolic, and functional abnormalities have taken place. Long term ketamine treatments lead to cognitive impairment including problems of short-term memory, visual and verbal memory. On the other hand, short term treatments are generally well tolerated and any damage may be reversible.\n\nElectroconvulsive therapy (ECT) significantly reduces functional connectivity between the dorsolateral prefrontal cortex (Dorsal nexus) and the anterior cingulate cortex, the medial prefrontal cortex, and other areas implicated in major depression. Although electroconvulsive therapy has been used as a treatment for depression since 1930, it has several side effects as loss of memory, confusion and difficulties in forming new memories. Because of this reasons, this kind of treatment is limited to severely damaged patients.\n",
    "id": "39207238",
    "title": "Dorsal nexus"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=39392026",
    "text": "Modes of toxic action\n\nA mode of toxic action is a common set of physiological and behavioral signs that characterize a type of adverse biological response. A mode of action should not be confused with mechanism of action, which refer to the biochemical processes underlying a given mode of action. Modes of toxic action are important, widely used tools in ecotoxicology and aquatic toxicology because they classify toxicants or pollutants according to their type of toxic action. There are two major types of modes of toxic action: non-specific acting toxicants and specific acting toxicants. Non-specific acting toxicants are those that produce narcosis, while specific acting toxicants are those that are non-narcotic and that produce a specific action at a specific target site.\n\n result in narcosis; therefore, narcosis is a mode of toxic action. Narcosis is defined as a generalized depression in biological activity due to the presence of toxicant molecules in the organism. The target site and mechanism of toxic action through which narcosis affects organisms are still unclear, but there are hypotheses that support that it occurs through alterations in the cell membranes at specific sites of the membranes, such as the lipid layers or the proteins bound to the membranes. Even though continuous exposure to a narcotic toxicant can produce death, if the exposure to the toxicant is stopped, narcosis can be reversible.\n\nToxicants that at low concentrations modify or inhibit some biological process by binding at a specific site or molecule have a specific acting mode of toxic action. However, at high enough concentrations, toxicants with specific acting modes of toxic actions can produce narcosis that may or may not be reversible. Nevertheless, the specific action of the toxicant is always shown first because it requires lower concentrations.\n\nThere are several specific acting modes of toxic action:\n\nThe pioneer work of identifying the major categories of modes of toxic action (see description above) was conducted by investigators from the U.S. Environmental Protection Agency (EPA) at the Duluth Laboratory using fish, reason why they named the categories as \"\" (FATS). They proposed the FATS by assessing the behavioral and physiological responses of the fish when subjected to toxicity tests, such as locomotive activities, body color, ventilation patterns, cough rate, heart rate, and others.\n\nIt has been proposed that modes of toxic action could be estimated by developing a data set of critical body residues (CBR). The CBR is the whole-body concentration of a chemical that is associated with a given adverse biological response and it is estimated using a partition coefficient and a bioconcentration factor. The whole-body residues are reasonable first approximations of the amount of chemical present at the toxic action site(s). Because different modes of toxic action generally appear to be associated with different ranges of body residues, modes of toxic action can then be separated into categories. However, it is unlikely that every chemical has the same mode of toxic action in every organism, so this variability should be considered. The effects of mixture toxicity should be considered as well, even though mixture toxicity it's generally additive, chemicals with more than one mode of toxic action may contribute to toxicity.\n\nModeling has become a common used tool to predict modes of toxic action in the last decade. The models are based in Quantitative Structure-Activity Relationships (QSARs), which are mathematical models that relate the biological activity of molecules to their chemical structures and corresponding chemical and physicochemical properties. QSARs can then predict modes of toxic action of unknown compounds by comparing its characteristic toxicity profile and chemical structure to reference compounds with known toxicity profiles and chemical structures. Russom and colleagues were one of the first group of researchers being able to classify modes of toxic action with the use of QSARs; they classified 600 chemicals as narcotics. Even though QSARs are a useful tool for predicting modes of toxic action, chemicals having multiple modes of toxic action can obscure QSAR analyses. Therefore, these models are continuously being developed.\n\nThe objective of environmental risk assessment is to protect the environment from adverse effects. Researchers are further developing QSAR models with the ultimate goal providing a clear insight about a mode of toxic action, but also about what the actual target site is, the concentration of the chemical at this target site, and the interaction occurring at the target site, as well as to predict the modes of toxic action in mixtures. Information on the mode of toxic action is crucial not only in understanding joint toxic effects and potential interactions between chemicals in mixtures, but also for developing assays for the evaluation of complex mixtures in the field.\n\nThe combination of behavioral and physiological responses, CBR estimates, and chemical fate and bioaccumulation QSAR models can be a powerful regulatory tool to address pollution and toxicity in areas where effluents are discharged.\n",
    "id": "39392026",
    "title": "Modes of toxic action"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=39337116",
    "text": "Biomaterial Surface Modifications\n\nBiomaterials exhibit various degrees of compatibility with the harsh environment within a living organism. They need to be nonreactive chemically and physically with the body, as well as integrate when with tissue. The extent of compatibility varies based on the application and material required. Often modifications to the surface of a biomaterial system are required to maximize performance. The surface can be modified in many ways, including plasma modification and applying coatings to the substrate. Surface modifications can be used to affect surface energy, adhesion, biocompatibility, chemical inertness, lubricity, sterility, asepsis, thrombogenicity, susceptibility to corrosion, degradation, and hydrophilicity.\n\nTeflon is a hydrophobic polymer composed of a carbon chain saturated with fluorine atoms. The fluorine-carbon bond is largely ionic producing a strong dipole. The dipole prevents Teflon from being susceptible to Van der Waals forces, so other materials will not stick to the surface. Teflon is commonly used to reduce friction in biomaterial applications such as in arterial grafts, catheters, and guide wire coatings.\n\nPEEK is a semicrystalline polymer composed of benzene, ketone, and ether groups. PEEK is known for having good physical properties including high wear resistance and low moisture absorption and has been used for biomedical implants due to its relative inertness inside of the human body.\n\nPlasma modification is one way to alter the surface of biomaterials to enhance their properties. During plasma modification techniques, the surface is subjected to high levels of excited gases that alter the surface of the material. Plasma's are generally generated with a radio frequency (RF) field. Additional methods include applying a large (~1KV) DC voltage across electrodes engulfed in a gas. The plasma is then used to expose the biomaterial surface, which can break or form chemical bonds. This is the result of physical collisions or chemical reactions of the excited gas molecules with the surface. This changes the surface chemistry and therefore surface energy of the material which affects the adhesion, biocompatibility, chemical inertness, lubricity, and sterilization of the material. The table below shows several biomaterial applications of plasma treatments. \n\nThe surface energy is equal to the sum of disrupted molecular bonds that occur at the interface between two different phases. Surface energy can be estimated by contact angle measurements using a version of the Young–Laplace equation:\n\nformula_1 \n\nWhere formula_2 is the surface tension at the interface of solid and vapor, formula_3 is the surface tension at the interface of solid and liquid, and formula_4 is the surface tension at the interface of liquid and vapor. Plasma modification techniques alter the surface of the material, and subsequently the surface energy. Changes in surface energy then alter the surface properties of the material.\n\nSurface modification techniques have been extensively researched for the application of adsorbing biological molecules. Surface functionalization can be performed by exposing surfaces to RF plasma. Many gases can be excited and used to functionalize surfaces for a wide variety of applications. Common techniques include using air plasma, oxygen plasma, and ammonia plasma as well as other exotic gases. Each gas can have varying effects on a substrate. These effects decay with time as reactions with molecules in air and contamination occur. \n\nAmmonia plasma treatment can be used to attach amine functional groups. These functional groups lock on to anticoagulants like Heparin decreasing thrombogenicity.\n\nPolysaccharides have been used as thin film coatings for biomaterial surfaces. Polysaccharides are extremely hydrophilic and will have small contact angles. They can be used for a wide range of applications due to their wide range of compositions. They can be used to reduce the adsorption of proteins to biomaterial surfaces. Additionally, they can be used as receptor sites, targeting specific biomolecules. This can be used to activate specific biological responses.\n\nCovalent attachment to a substrate is necessary to immobilize polysaccharides, otherwise they will rapidly desorb in a biological environment. This can be a challenge due to the fact that the majority of biomaterials do not possess the surface properties to covalently attach polysaccharides. This can be achieved by the introduction of amine groups by RF glow discharge plasma. Gases used to form amine groups, including ammonia or n-heptylamine vapor, can be used to deposit a thin film coating containing surface amines. Polysaccharides must also be activated by oxidation of anhydroglucopyranoside subunits. This can be completed with sodium metaperiodate (NaIO). This reaction converts anhydroglucopyranoside subunits to cyclic hemiacetal structures, which can be reacted with amine groups to form a Schiff base linkage (a carbon-nitrogen double bond). These linkages are unstable and will easily dissociate. Sodium cyanoborohydride (NaBHCN) can be used as a stabilizer by reducing the linkages back to an amine.\n\nThere are many examples of contamination of biomaterials that are specific to the preparation or manufacturing process. Additionally, nearly all surfaces are prone to contamination of organic impurities in the air. Contamination layers are usually limited to a monolayer or less of atoms and are thus only detectable by surface analysis techniques, such as XPS. It is unknown whether this sort of contamination is harmful, yet it is still regarded as contamination and will most certainly affect surface properties.\n\nGlow discharge plasma treatment is a technique that is used for cleaning contamination from biomaterial surfaces. Plasma treatment has been used for various biological evaluation studies to increase the surface energy of biomaterial surfaces, as well as cleaning. Plasma treatment has also been proposed for sterilization of biomaterials for potential implants.\n\nAnother method of altering surface properties of biomaterials is to coat the surface. Coatings are used in many applications to improve biocompatibility and alter properties such as adsorption, lubricity, thrombogenicity, degradation, and corrosion.\n\nIn general, the lower the surface tension of a liquid coating, the easier it will be to form a satisfactory wet film from it. The difference between the surface tension of a coating and the surface energy of a solid substrate to which a coating is applied affects how the liquid coating flows out over the substrate. It also affects the strength of the adhesive bond between the substrate and the dry film. If for instance, the surface tension of the coating is higher than the surface tension of the substrate, then the coating will not spread out and form a film. As the surface tension of the substrate is increased, it will reach a point to where the coating will successfully wet the substrate but have poor adhesion. Continuous increase in the coating surface tension will result in better wetting in film formation and better dry film adhesion.\n\nMore specifically whether a liquid coating will spread across a solid substrate can be determined from the surface energies of the involved materials by using the following equation:\n\nformula_5\n\nWhere S is the coefficient of spreading, formula_6 is the surface energy of the substrate in air, formula_7 is the surface energy of the liquid coating in air and formula_8 is the interfacial energy between the coating and the substrate. If S is positive the liquid will cover the surface and the coating will adhere well. If S is negative the coating will not completely cover the surface, producing poor adhesion.\n\nOrganic coatings are a common way to protect a metallic substrate from corrosion. Up until ~1950 it was thought that coatings act as a physical barrier which disallows moisture and oxygen to contact the metallic substrate and form a corrosion cell. This cannot be the case because the permeability of paint films is very high. It has since been discovered that corrosion protection of steel depends greatly upon the adhesion of a noncorrosive coating when in the presence of water. With low adhesion, osmotic cells form underneath the coating with high enough pressures to form blisters, which expose more unprotected steel. Additional non-osmotic mechanisms have also been proposed. In either case, sufficient adhesion to resist displacement forces is required for corrosion protection.\n\nGuide wires are an example of an application for biomedical coatings. Guide wires are used in coronary angioplasty to correct the effects of coronary artery disease, a disease that allows plaque build up on the walls of the arteries. The guide wire is threaded up through the femoral artery to the obstruction. The guide wire guides the balloon catheter to the obstruction where the catheter is inflated to press the plaque against the arterial walls. Guide wires are commonly made from stainless steel or Nitinol and require polymer coatings as a surface modification to reduce friction in the arteries. The coating of the guide wire can affect the trackability, or the ability of the wire to move through the artery without kinking, the tactile feel, or the ability of the doctor to feel the guide wire's movements, and the thrombogenicity of the wire.\n\nHydrophilic coatings can reduce friction in the arteries by up to 83% when compared to bare wires due to their high surface energy. When the hydrophilic coatings come into contact with bodily fluids they form a waxy surface texture that allows the wire to slide easily through the arteries. Guide wires with hydrophilic coatings have increased trackability and are not very thrombogenic; however the low coefficient of friction increases the risk of the wire slipping and perforating the artery.\n\nTeflon and Silicone are commonly used hydrophobic coatings for coronary guide wires. Hydrophobic coatings have a lower surface energy and reduce friction in the arteries by up to 48%. Hydrophobic coatings do not need to be in contact with fluids to form a slippery texture. Hydrophobic coatings maintain tactile sensation in the artery, allowing doctors full control of the wire at all times and reducing the risk of perforation; though, the coatings are more thrombogenic than hydrophilic coatings. The thrombogenicity is due to the proteins in the blood adapting to the hydrophobic environment when they adhere to the coating. This causes an irreversible change for the protein, and the protein remains stuck to the coating allowing for a blood clot to form.\n\nUsing an MRI to image the guide wire during use would have an advantage over using x-rays because the surrounding tissue can be examined while the guide wire is advanced. Because most guide wires' core materials are stainless steel they are not capable of being imaged with an MRI. Nitinol wires are not magnetic and could potentially be imaged, but in practice the conductive nitinol heats up under the magnetic radiation which would damage surrounding tissues. An alternative that is being examined is to replace contemporary guide wires with PEEK cores, coated with iron particle embedded synthetic polymers.\n",
    "id": "39337116",
    "title": "Biomaterial Surface Modifications"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=39310035",
    "text": "Dependence receptor\n\nIn cellular biology, dependence receptors are proteins that mediate programmed cell death by monitoring the absence of certain trophic factors (or, equivalently, the presence of anti-trophic factors) that otherwise serve as ligands (interactors) for the dependence receptors.\nA trophic ligand is a molecule whose protein binding stimulates cell growth, differentiation, and/or survival.\nCells depend for their survival on stimulation that is mediated by various receptors and sensors, and integrated via signaling within the cell and between cells. \nThe withdrawal of such trophic support leads to a form of cellular suicide.\n\nVarious dependence receptors are involved in a range of biological events: developmental cell death (naturally occurring cell death), trophic factor withdrawal-induced cell death, the spontaneous regression characteristic of type IV-S neuroblastoma, neurodegenerative cell death, inhibition of new tumor cells (tumorigenesis) and metastasis, and therapeutic antibody-mediated tumor cell death, as well as programmed cell death in other instances. \nSince these receptors may support either cell death or cell survival, they initiate a new type of tumor suppressor, a conditional tumor suppressor.\nIn addition, events such as cellular atrophy and process retraction may also be mediated by dependence receptors, although this has not been as well documented as the induction of programmed cell death.\n\nDependence receptors were first identified in 1993 in the laboratory of Dale Bredesen, and their importance in tumorigenesis first described by Mehlen, Bredesen, and their colleagues.\n\nThe following is the list of known dependence receptors:\n\nCells depend for their survival on stimulation that is mediated by various receptors and sensors. For any required stimulus, its withdrawal leads to a form of cellular suicide; that is, the cell plays an active role in its own demise. The term programmed cell death was first suggested by Lockshin & Williams in 1964.\nApoptosis, a form of programmed cell death, was first described by Kerr et al. in 1972, \nalthough the earliest references to the morphological appearance of such cells may date back to the late 19th century.\n\nCells require different stimuli for survival, depending on their type and state of differentiation. \nFor example, prostate epithelial cells require testosterone for survival, and the withdrawal of testosterone leads to apoptosis in these cells.\nHow do cells recognize a lack of stimulus? While positive survival signals are clearly important, a complementary form of signal transduction is pro-apoptotic, and is activated or propagated by stimulus withdrawal or by the addition of an “anti-trophin.”\n\nThe dependence receptor notion was based on the observation that the effects of a number of receptors that function in both nervous system development and the production of tumors (especially metastasis) cannot be explained simply by a positive effect of signal transduction induced by ligand binding, but rather must also include cell death signaling in response to trophic withdrawal.\n\nPositive survival signals involve classical signal transduction, initiated by interactions between ligands and receptors. Negative survival signals involve an alternative form of signal transduction that is initiated by the withdrawal of ligands from dependence receptors. This process is seen in developmental cell death, carcinogenesis (especially metastasis), neurodegeneration, and possibly non-lethal (sub-apoptotic) events such as neurite retraction and somal atrophy. Mechanistic studies of dependence receptors suggest that these receptors form complexes that activate and amplify caspase activity. In at least some cases, the caspase activation is via a pathway that is dependent on caspase-9 but not on mitochondria. \nSome of the downstream mediators have been identified, such as DAP kinase and the DRAL gene.\n\nDependence receptors display the common property that they mediate two different intracellular signals: in the presence of ligand, these receptors transduce a positive signal leading to survival, differentiation or migration; conversely, in the absence of ligand, the receptors initiate and/or amplify a signal for programmed cell death. Thus cells that express these proteins at sufficient concentrations manifest a state of dependence on their respective ligands. The signaling that mediates cell death induction upon ligand withdrawal is incompletely defined, but typically includes a required interaction with, and cleavage by, specific caspases. \nMutation of the caspase site(s) in the receptor, of which there is typically one or two, prevents the trophic ligand withdrawal-induced programmed cell death.\n\nComplex formation appears to be a function of ligand-receptor interaction, and dependence receptors appear to exist in at least two conformational states. \nComplex formation in the absence of ligand leads to caspase activation by a mechanism that is usually dependent on caspase cleavage of the receptor itself, releasing pro-apoptotic peptides. \nThus these receptors may serve in caspase amplification, and in so doing create cellular states of dependence on their respective ligands.\nThese states of dependence are not absolute, since they can be blocked downstream in some cases by the expression of anti-apoptotic genes such as Bcl-2 or P35. \nHowever, they result in a shift toward an increased likelihood of a cell's undergoing apoptosis.\n\nResearch has highlighted the role of the dependence receptor UNC5D in the phenomenon of spontaneous regression of type IV-S neuroblastoma.\nTrkA and TrkC have been shown to function as dependence receptors,\n\nwith TrkC mediating both neural cell death and tumorigenesis.\nIn addition, although dependence receptors have been described as mediating programmed cell death in the absence of binding of trophic ligand, the possibility that a similar effect might be achieved by the binding of a physiological anti-trophin has been raised, and it has been suggested that the Alzheimer's disease-associated peptide, Aβ, may play such a role.\n\n\n",
    "id": "39310035",
    "title": "Dependence receptor"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=39984881",
    "text": "Phytogeomorphology\n\nPhytogeomorphology is the study of how terrain features affect plant growth. It was the subject of a treatise by Howard and Mitchell in 1985, who were considering the growth and varietal temporal and spatial variability found in forests, but recognized that their work also had application to farming, and the relatively new science (at that time) of precision agriculture. The premise of Howard and Mitchell is that landforms, or features of the land's 3D topography significantly affect how and where plants (or trees in their case) grow. Since that time, the ability to map and classify landform shapes and features has increased greatly. The advent of GPS has made it possible to map almost any variable one might wish to measure. Thus, a very increased awareness of the spatial variability of the environment that plants gown in has arisen. The development of technology like airborne LiDAR has enabled the detailed measurement of landform features to better than sub-meter, and when combined with RTK-GPS (accuracies to 1mm) enables the creation of very accurate maps of where these features are. Comparison of these landform maps with mapping of variables related to crop or plant growth show a strong correlation (see below for examples and references for precision agriculture).\n\nWhile phytogeomorphology is a broad term that applies to the relationship between plants and terrain attributes in general (see Howard et al., (1985)), it is also very applicable to the aspect of precision agriculture that studies crop growth temporal and spatial variability within farm fields. There is already a volume of work, although they don't use the term phytogeomorphology specifically, that considers farm field terrain attributes as affecting crop yield and growth, Moore et al. (1991) provide an early overview of the application of terrain features to precision agriculture, but one of the earliest references to this phenomenon in farming is that of Whittaker in 1967. More recent work includes a six-year study of temporal and spatial yield stability over 11 years (Kaspar et al., (2003), and references therein), and a detailed study of the same on a small patch farm in Portugal (and references therein). This variability can be exploited to produce higher yields and reduce the environmental impact of farming - consequently returning a higher profit to the farmer in terms of higher overall yields and lesser amounts of inputs. The new science of Sustainable Intensification of Agriculture which is addressing the need for higher yields from existing fields can be fulfilled by some of the practical applications of phytogeomorphology applied to precision agriculture.\n\nWork in this area has been happening for some years (see Reuter et al., (2005), Marquas de Silva et al., (2008), and especially Moore et al., (1991)), but it is slow and sometimes tedious work that necessarily involves multiple years of data, very specialized software tools, and long compute times to produce the resulting maps.\n\nTypically, the objective of precision agriculture is to divide the farm field into distinct management zones based on yield performance at each point in the field. 'Variable rate technology' is a relatively new term in farming technology that refers to spreaders, seeders, sprayers, etc. that are able to adjust their rates of flow on the fly. The idea is to create a 'recipe map' for variable rate farm machinery to deliver the exact quantity of amendments required at that location (within that zone of the field). The literature is divided on how to properly define management zones.\n\nIn the geomorphological approach to defining management zones it is found that topography aids in at least partially defining how much yield comes from which part of the field. This is true in fields where there are permanently limiting characteristics to parts of the field, but not true in fields where the growth potential is the same all over the field (Blackmore et al., (2003)). It can be shown that an index map of yield (shows areas of consistent over-performance of yield and areas of consistent under-performance) correlates well with a landform classification map (personal communication, Aspinall (2011)). Landforms can be classified a number of ways, but the simplest to use software tool is LandMapR (MacMillan (2003)). An early version of the LandMapR software is available through the Opengeomorphometry project hosted under the Google Code project.\n\n",
    "id": "39984881",
    "title": "Phytogeomorphology"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=40624269",
    "text": "Trabecular cartilage\n\nTrabecular cartilages (trabeculae cranii, sometimes simply trabeculae, prechordal cartilages) are paired, rod-shaped cartilages, which develop in the head of the vertebrate embryo. They are the primordia of the anterior part of the cranial base, and are derived from the cranial neural crest cells.\n\nThe trabecular cartilages generally appear as a paired, rod-shaped cartilages at the ventral side of the forebrain and lateral side of the adenohypophysis in the vertebrate embryo. During development, their anterior ends fuse and form the \"trabecula communis\". Their posterior ends fuse with the caudal-most parachordal cartilages.\n\nMost skeletons are of mesodermal origin in vertebrates. Especially axial skeletal elements, such as the vertebrae, are derived from the paraxial mesoderm (e.g., somites), which is regulated by molecular signals from the notochord. Trabecular cartilages, however, originate from the neural crest, and since they are located anterior to the rostral tip of the notochord, they cannot receive signals from the notochord. Due to these specialisations, and their essential role in cranial development, many comparative morphologists and embryologists have argued their developmental or evolutionary origins. The general theory is that the trabecular cartilage is derived from the neural crest mesenchyme which fills anterior to the mandibular arch (premandibular domain).\n\nAs clearly seen in the lamprey, Cyclostome also has a pair of cartilaginous rods in the embryonic head which is similar to the trabecular cartilages in jawed vertebrates.\nHowever, in 1916, Alexej Nikolajevich Sewertzoff pointed out that the cranial base of the lamprey is exclusively originated from the paraxial mesoderm. Then in 1948, reported the detail of the skeletogenesis of the lamprey, and showed that the “trabecular cartilages” in lamprey appear just beside the notochord, in a similar position to the parachordal cartilages in jawed vertebrates. Recent experimental studies also showed that the cartilages are derived from the head mesoderm. The “trabecular cartilages” in the Cyclostome is no longer considered to be the homologue of the trabecular in the jawed vertebrates: the (true) trabecular cartilages were firstly acquired in the Gnathostome lineage.\n\nThe trabecular cartilages were first described in the grass snake by at 1839. In 1874, Thomas Henry Huxley suggested that the trabecular cartilages are a modified part of the splanchnocranium: they arose as the serial homologues of the pharyngeal arches.\n\nThe vertebrate jaw is generally thought to be the modification of the mandibular arch (1st pharyngeal arch). Since the trabecular cartilages appear anterior to the mandibular arch, if the trabecular cartilages are serial homologues of the pharyngeal arches, ancestral vertebrates should possess more than one pharyngeal arch (so-called \"premandibular arches\") anterior to the mandibular arch. The existence of premandibular arch(es) has been accepted by many comparative embryologists and morphologists (e.g., Edwin Stephen Goodrich, Gavin de Beer). Moreover, reported premandibular arches and the corresponding branchiomeric nerves by the reconstruction of the Osteostracans (e.g., \"Cephalaspis\"; recently this arch was reinterpreted as the mandibular arch)\n\nHowever, the existence of the premandibular arch(es) has been rejected, and the trabecular cartilages are no longer assumed to be one of the pharyngeal arches.\n\n",
    "id": "40624269",
    "title": "Trabecular cartilage"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=790808",
    "text": "Endogeny (biology)\n\nEndogenous substances and processes are those that originate from within an organism, tissue, or cell.\n\nEndogenous viral elements are DNA sequences derived from viruses that are ancestrally inserted into the genomes of germ cells. These sequences, which may be fragments of viruses or entire viral genomes (proviruses), can persist in the germline, being passed on from one generation to the next as host alleles.\n\nEndogenous processes include senescence, the menstrual cycle and the self-sustained circadian rhythms of plants and animals.\n\nIn some biological systems, endogeneity pertains to the recipient of DNA (usually in prokaryotes). However, because of homeostasis, discerning between internal and external influences is often difficult.\n\nEndogenous transcription factors are those manufactured by the cell, as distinguished from cloned transcription factors.\n\nThe word endogenous \n\n",
    "id": "790808",
    "title": "Endogeny (biology)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=65132",
    "text": "Inverted repeat\n\nAn inverted repeat (or IR) is a single stranded sequence of nucleotides followed downstream by its reverse complement. The intervening sequence of nucleotides between the initial sequence and the reverse complement can be any length including zero. When the intervening length is zero, the composite sequence is a palindromic sequence. For example, is an inverted repeat sequence.\n\nBoth inverted repeats and direct repeats constitute types of nucleotide sequences that occur repetitively. These repeated DNA sequences often range from a pair of nucleotides to a whole gene, while the proximity of the repeat sequences varies between widely dispersed and simple tandem arrays. The short tandem repeat sequences may exist as just a few copies in a small region to thousands of copies dispersed all over the genome of most eukaryotes. Repeat sequences with about 10–100 base pairs are known as minisatellites, while shorter repeat sequences having mostly 2–4 base pairs are known as microsatellites. The most common repeats include the dinucleotide repeats, which have the bases AC on one DNA strand, and GT on the complementary strand. Some elements of the genome with unique sequences function as exons, introns and regulatory DNA. Though the most familiar loci of the repetitive sequences are the centromere, and the telomere, a large portion of the repeated sequences in the genome are found among the noncoding DNA.\n\nInverted repeats have a number of important biological functions. They define the boundaries in transposons and indicate regions capable of self-complementary base pairing (regions within a single sequence which can base pair with each other). These properties play an important role in genome instability and contribute not only to cellular evolution and genetic diversity but also to mutation and disease. In order to study these effects in detail, a number of programs and databases have been developed to assist in discovery and annotation of inverted repeats in various genomes.\n\nBeginning with this initial sequence:\n\nThe complement created by base pairing is:\n\nThe reverse complement is:\n\nAnd, the inverted repeat sequence is:\n\n\"nnnnnn\" represents any number of intervening nucleotides.\n\nA direct repeat occurs when a sequence is repeated with the same pattern downstream. There is no inversion and no reverse complement associated with a direct repeat. The nucleotide sequence written in bold characters signifies the repeated sequence. It may or may not have intervening nucleotides.\nLinguistically, a typical direct repeat is comparable to saying \"bye-bye\".\n\nAn inverted repeat sequence with no intervening nucleotides between the initial sequence and its downstream reverse complement is a palindrome.     EXAMPLE:\n        Step 1: start with an inverted repeat: \n        Step 2: remove intervening nucleotides: \n        This resulting sequence is palindromic because it is the reverse complement of itself.\n\nThe diverse genome-wide repeats are derived from transposable elements, which are now understood to \"jump\" about different genomic locations, without transferring their original copies. Subsequent shuttling of the same sequences over numerous generations ensures their multiplicity throughout the genome. The limited recombination of the sequences between two distinct sequence elements known as conservative site-specific recombination (CSSR) results in inversions of the DNA segment, based on the arrangement of the recombination recognition sequences on the donor DNA and recipient DNA. Again, the orientation of two of the recombining sites within the donor DNA molecule relative to the asymmetry of the intervening DNA cleavage sequences, known as the crossover region, is pivotal to the formation of either inverted repeats or direct repeats. Thus, recombination occurring at a pair of inverted sites will invert the DNA sequence between the two sites. Very stable chromosomes have been observed with comparatively fewer numbers of inverted repeats than direct repeats, suggesting a relationship between chromosome stability and the number of repeats.\nTerminal inverted repeats have been observed in the DNA of various eukaryotic transposons, even though their source remains unknown. Inverted repeats are principally found at the origins of replication of cell organism and organelles that range from phage plasmids, mitochondria, and eukaryotic viruses to mammalian cells. The replication origins of the phage G4 and other related phages comprise a segment of nearly 139 nucleotide bases that include three inverted repeats that are essential for replication priming.\n\nTo a large extent, portions of nucleotide repeats are quite often observed as part of rare DNA combinations. The three main repeats which are largely found in particular DNA constructs include the closely precise homopurine-homopyrimidine inverted repeats, which is otherwise referred to as H palindromes, a common occurrence in triple helical H conformations that may comprise either the TAT or CGC nucleotide triads. The others could be described as long inverted repeats having the tendency to produce hairpins and cruciform, and finally direct tandem repeats, which commonly exist in structures described as slipped-loop, cruciform and left-handed Z-DNA.\n\nPast studies suggest that repeats are a common feature of eukaryotes unlike the prokaryotes and archaea. Other reports suggest that irrespective of the comparative shortage of repeat elements in prokaryotic genomes, they nevertheless contain hundreds or even thousands of large repeats. Current genomic analysis seem to suggest the existence of a large excess of perfect inverted repeats in many prokaryotic genomes as compared to eukaryotic genomes. Recently (2017) was found that inverted repeats are present in DNA of all mitochondrial genomes (sequenced so far) and are enriched especially in important regulatory regions like replication origin and D-loops.\n\nPseudoknots are common structural motifs found in RNA. They are formed by two nested stem-loops such that the stem of one structure is formed from the loop of the other. There are multiple folding topologies among pseudoknots and great variation in loop lengths, making them a structurally diverse group.\n\nInverted repeats are a key component of pseudoknots as can be seen in the illustration of a naturally occurring pseudoknot found in the human telomerase RNA component. Four different sets of inverted repeats are involved in this structure. Sets 1 and 2 are the stem of stem-loop A and are part of the loop for stem-loop B. Similarly, sets 3 and 4 are the stem for stem-loop B and are part of the loop for stem-loop A.\n\nPseudoknots play a number of different roles in biology. The telomerase pseudoknot in the illustration is critical to that enzyme's activity. The ribozyme for the \"hepatitis delta virus (HDV)\" folds into a double-pseudoknot structure and self-cleaves its circular genome to produce a single-genome-length RNA. Pseudoknots also play a role in programmed ribosomal frameshifting found in some viruses and required in the replication of retroviruses.\n\nInverted repeats play an important role in riboswitches, which are RNA regulatory elements that control the expression of genes that produce the mRNA, of which they are part. A simplified example of the flavin mononucleotide (FMN) riboswitch is shown in the illustration. This riboswitch exists in the mRNA transcript and has several stem-loop structures upstream from the coding region. However, only the key stem-loops are shown in the illustration, which has been greatly simplified to help show the role of the inverted repeats. There are multiple inverted repeats in this riboswitch as indicated in green (yellow background) and blue (orange background). \n\nIn the absence of FMN, the Anti-termination structure is the preferred conformation for the mRNA transcript. It is created by base-pairing of the inverted repeat region circled in red. When FMN is present, it may bind to the loop and prevent formation of the Anti-termination structure. This allows two different sets of inverted repeats to base-pair and form the Termination structure. The stem-loop on the 3' end is a transcriptional terminator because the sequence immediately following it is a string of uracils (U). If this stem-loop forms (due to the presence of FMN) as the growing RNA strand emerges from the RNA polymerase complex, it will create enough structural tension to cause the RNA strand to dissociate and thus terminate transcription. The dissociation occurs easily because the base-pairing between the U's in the RNA and the A's in the template strand are the weakest of all base-pairings. Thus, at higher concentration levels, FMN down-regulates its own transcription by increasing the formation of the termination structure.\n\nInverted repeats are often described as \"hotspots\" of eukaryotic and prokaryotic genomic instability. Long inverted repeats are deemed to greatly influence the stability of the genome of various organisms. This is exemplified in \"E. coli\", where genomic sequences with long inverted repeats are seldom replicated, but rather deleted with rapidity. Again, the long inverted repeats observed in yeast greatly favor recombination within the same and adjacent chromosomes, resulting in an equally very high rate of deletion. Finally, a very high rate of deletion and recombination were also observed in mammalian chromosomes regions with inverted repeats. Reported differences in the stability of genomes of interrelated organisms are always an indication of a disparity in inverted repeats. The instability results from the tendency of inverted repeats to fold into hairpin- or cruciform-like DNA structures. These special structures can hinder or confuse DNA replication and other genomic activities. Thus, inverted repeats lead to special configurations in both RNA and DNA that can ultimately cause mutations and disease.\n\nThe illustration shows an inverted repeat undergoing cruciform extrusion. DNA in the region of the inverted repeat unwinds and then recombines, forming a four-way junction with two stem-loop structures. The cruciform structure occurs because the inverted repeat sequences self-pair to each other on their own strand.\n\nExtruded cruciforms can lead to frameshift mutations when a DNA sequence has inverted repeats in the form of a palindrome combined with regions of direct repeats on either side. During transcription, slippage and partial dissociation of the polymerase from the template strand can lead to both deletion and insertion mutations. Deletion occurs when a portion of the unwound template strand forms a stem-loop that gets \"skipped\" by the transcription machinery. Insertion occurs when a stem-loop forms in a dissociated portion of the nascent (newly synthesized) strand causing a portion of the template strand to be transcribed twice.\n\nImperfect inverted repeats can lead to mutations through intrastrand and interstrand switching. The antithrombin III gene's coding region is an example of an imperfect inverted repeat as shown in the figure on the right.\nThe stem-loop structure forms with a bump at the bottom because the G and T do not pair up. A strand switch event could result in the G (in the bump) being replaced by an A which removes the \"imperfection\" in the inverted repeat and provides a stronger stem-loop structure. However, the replacement also creates a point mutation converting the GCA codon to ACA. If the strand switch event is followed by a second round of DNA replication, the mutation may become fixed in the genome and lead to disease. Specifically, the missense mutation would lead to a defective gene and a deficiency in antithrombin which could result in the development of venous thromboembolism (blood clots within a vein).\n\nMutations in the collagen gene can lead to the disease Osteogenesis Imperfecta, which is characterized by brittle bones. In the illustration, a stem-loop formed from an imperfect inverted repeat is mutated with a thymine (T) nucleotide insertion as a result of an inter- or intrastrand switch. The addition of the T creates a base-pairing \"match up\" with the adenine (A) that was previously a \"bump\" on the left side of the stem. While this addition makes the stem stronger and perfects the inverted repeat, it also creates a frameshift mutation in the nucleotide sequence which alters the reading frame and will result in an incorrect expression of the gene.\nThe following list provides information and external links to various programs and databases for inverted repeats:\n\n",
    "id": "65132",
    "title": "Inverted repeat"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=26305461",
    "text": "Parabiosis\n\nParabiosis, meaning \"living beside\", is a technical term in various contexts in fields of study related to ecology and physiology. It accordingly has been defined independently in at least three disciplines, namely experimental or medical physiology, the ecology of inactive physiological states, and the ecology of certain classes of social species that share nests.\n\nParabiosis derives most directly from new Latin, but the Latin in turn derives from two classical Greek roots. The first is \"παρά\" (\"para\") for \"beside\" or \"next to\". In modern etymology, this root appears in various senses, such as \"close to\", \"outside of\", and \"different\".\n\n\nThe second classical Greek root from which the Latin derives is \"βίος\" (\"bios\"), meaning \"life\".\n\nIn the field of experimental physiology, parabiosis is a class of techniques in which two living organisms are joined together surgically and develop single, shared physiological systems, such as a shared circulatory system. Through surgically connecting two animals, researchers can prove that the feedback system in one animal is circulated and affects the second animal via blood and plasma exchange. Total blood volume is exchanged approximately ten times per day in rat experiments using parabiosis. One limitation of the experiments is that outbred rats cannot be used because it can lead to a significant loss of pairs due to intoxication of the blood supply from a dissimilar rat.\n\nIn the mid-1800s, parabiotic experiments were pioneered by Paul Bert. He postulated that surgically connected animals could share a circulatory system. Bert was awarded the Prize of Experimental Physiology of the French Academy of Science in 1866 for his discoveries. Parabiotic experiments were scarcely revisited until the 20th century.\n\nMany of the parabiotic experiments since 1950 involve research regarding metabolism. One of these experiments was published in 1959 by G. R. Hervey in the \"Journal of Physiology\". This experiment was to support the theory that damage to the hypothalamus, particularly the ventromedial hypothalamus, leads to obesity caused by the overconsumption of food. This results from the ventromedial hypothalamus failing to respond to physiological signals that suppress appetite. The result is attributed to the feedback control system in the brain. Rats in the study were from the same litter, which had been a closed colony for multiple years. The two rats in each pair had no more than 3% difference in weight. Rats were paired at four weeks old. Unpaired rats were used as controls. The rats were conjoined in three ways. First, the peritoneal cavities were opened and connected between the two rats. Later, to avoid the risk of tangling the two rats’ intestines together, smaller cuts were made. After more refinement of the experimental procedure, the abdominal cavities were not opened and the rats were conjoined at the hip bone with minimal cutting. In order to prove that the two animals were sharing blood, researchers injected dye into the veins of one rat and the pigment would show up in the conjoined rat. It was necessary to verify the exchange of blood and plasma. The scientists refined the lesion placement by practicing the procedure on other rats. The rats were killed with ether and weighed at the conclusion of the experiment and the amount of fat in each animal was quantified.\n\nMany of the parabiotic pairs died throughout the experiment before its conclusion. In each pair, one rat became obese and exhibited hyperphagia. The weight of the rat with the surgical lesion rose rapidly for a few months, then reached a plateau as a direct result of the surgical procedure. After the procedure, the rat with the impaired hypothalamus voraciously ate and the paired rat decreased their appetite. The paired rat became obviously thin throughout the experiment, even rejecting food when it was offered. Some of the paired rats starved to death. Lesions subsequently made in the hypothalamus of two paired rats resulted in hyperphagia and obesity. That result verifies that the paired rat decreased its eating in a direct response to the signals in the blood from the rat with the lesion. If its brain was similarly altered, it also ate voraciously and became obese. The control rats who were given surgical lesions in the hypothalamus became similarly obese to the parabiotic counterparts.\n\nLater studies linked the effects of the previous parabiotic experiments about metabolism to the discovery of leptin. Many hormones and metabolites were proven to not be the satiety factor that caused one rat to starve in the experiments. Leptin seemed like a viable candidate. Starting in 1977, Ruth B.S. Harris, a graduate student under Hervey, repeated previous studies about parabiosis in rats and mice. Due to the discovery of leptin, she analyzed leptin concentrations of the mice in the parabiotic experiments. After injecting leptin into the obese mouse of each pair, she found that leptin circulated between the conjoined animals, but the circulation of leptin took some time to reach equilibrium. As a result of the injections, almost immediate weight loss resulted in the parabiotic pairs due to increased inhibition. Approximately 50–70% of fat was lost in the pairs. The obese mouse lost only fat. The lean mouse lost muscle mass and fat. Harris concluded that leptin levels are increased in the obese animal, but other factors could also affect the animals. Also, leptin was determined to decrease fat storage in both the obese and thin animals.\n\nParabiotic experiments have also been used to study diabetes. Douglas Coleman did further parabiotic experiments to determine diabetic chromosomal relationships. A major gene that causes obesity in mice was identified on chromosome 6. The obese gene (ob/ob) was determined from one mutant mouse, discovered in 1950. Coleman and researchers further identified a gene on chromosome 4 that led to hyperphagia and obesity in mice. The gene also correlated to the severe onset of diabetes (db/db gene). The experimenters used parabiosis to conjoin a db/db mouse to a normal mouse, and the normal mouse would starve to death after one week. The db/db mouse would still have a high blood sugar and food in its system. The db/db mouse was determined to have a satiety factor so potent that the other mouse would starve to death. A db/db mouse was conjoined with an ob/ob mouse and the result was the same as the first experiment. The obese mouse starved in 20–30 days, similar to the thin counterpart in the first experiment. The db/db mutant mouse overproduced a satiety factor but could not respond to it, perhaps due to a defective receptor, whereas the ob/ob mutant recognized and responded to the factor but could not reproduce it. Further studies with db/db mice with lesions in the arcuate nucleus of the hypothalamus suggested that the receptor for the satiety factor was found in these brain receptors.\n\nEarly parabiotic experiments also included cancer research. One study, published in 1966 by Friedell, studied the effects of radiation with X-rays on ovarian tumors. To study the tumors, two adult female rats were conjoined. The left rat was shielded and the right rat was exposed to high levels of radiation. The rats were given a controlled amount of food and water for the remainder of their natural lives. After death, the rats were autopsied and 149 of 328 pairs showed the presence of possible ovarian tumors in one or both of the two animals. This result matched previous studies of single rats. Since Friedell’s experiment, other parabiotic experiments have been useful in researching many types of cancer.\n\nChronic diseases of age have been saluted as prime candidates for parabiotic research because of the potential to conjoin an older animal with a younger animal. This process could be used to research cardiovascular disease, diabetes, osteoarthritis, and Alzheimer’s disease. As animals age, their oligodendrocytes reduce in efficiency, resulting in decreased myelination, causing negative effects on the central nervous system (CNS). Julia Ruckh and fellow researchers have used parabiosis to study remyelination from adult stem cells to see if conjoining young with older mice could reverse or delay this process. In the experiment, the two mice were conjoined and demyelination was induced via injection into the older mice. The experiment determined that factors from the younger mice reversed CNS demyelination in older mice by revitalizing the oligodendrocytes. The monocytes from the younger mice also enhanced the ability of the older mice to clear myelin debris because the young monocytes can clear lipids from myelin sheaths more effectively than older monocytes. The conjoining of the two animals reversed the effects of age on the myelination cells. The ability of the young mouse’s cells was unaffected. Enhanced immunity from the younger mouse also promoted the general health of the older mouse in each pair. The results of this experiment could lead to therapy processes for people with demyelinating diseases like multiple sclerosis.\n\nParabiotic research can be controversial. Due to accusations of animal cruelty, the practice is now shunned in many countries. Compared to other research techniques, there are relatively few parabiotic experiments. Some scientists argue for the benefits of parabiotic research. Eggel and Wyss-Coray argue that conjoining animals mimics naturally occurring parabiosis in nature due to the shared blood supply of conjoined twins. The experiments give insight into the way that bodily systems circulate, which has led to many advances in the study of a variety of diseases. Parabiotic experiments have been used to study obesity, chronic diseases of age, stem cell research, tissue regeneration, diabetes, transplants, tumor biology, and endocrinology.\n\nA two-year human trial to evaluate the beneficial effects of infusions of plasma from young donors (16–25 years of age) using blood biomarkers was started in June 2016 in Monterey, California. A panel of age-associated biomarkers will be measured before and after treatment, representing a spectrum of physiologic pathways with evidence-based connections to aging.\n\nOther than in experimental physiology, the term also is applicable to spontaneously occurring conditions such as in conjoined twins.\n\nLogically the word parabiosis would be equally applicable to various forms of parasitism such as the obligate parasitic reproduction of Anglerfish of the family Ceratiidae, in which the circulatory systems of the males and females unite completely. Without the attachment of males to a female, the endocrine functions cannot mature, the individuals fail to develop properly and die young and without reproducing.\n\nSimilarly, in plants growing closely together roots or stems in intimate contact sometimes form natural grafts. More commonly, in parasitic plants such as mistletoe and dodder the haustoria unite the circulatory systems of the host and the parasite so intimately that parasitic twiners such as \"Cassytha\" may act as vectors carrying disease organisms from one host plant to another.\n\nParabiosis as a term also applies to the states assumed by many organisms in various kingdoms of life, such as some bacteria, the bear animalcules (Tardigrada), and Rotifera. At least some members of all these taxa can survive drying out, often for decades or longer. Some bacteria, such as those causing anthrax, produce inactive, resistant spores that survive underground for many years. The eggs of some animals, such as Anostraca, the brine shrimps and their close relatives, have resistant shells and can survive in seasonally dried-out pools. In such a state the organisms show no sign of life until suitable conditions return. In this sense the \"para\" root apparently refers to the state as being \"outside\" of life.\n\nIn the late 19th century the first examples were discovered, of colonies of ants that more or less routinely shared their nests with essentially unrelated species of ants. They did not obviously share anything beyond the upkeep of the nests, even segregating their brood, so these were a very surprising observations; most ants are radically intolerant of intruders, usually including even intruders of their own species.\n\nIn the early 20th century Auguste-Henri Forel coined the term \"parabiosis\" for such associations, and it was adopted by the likes of William Morton Wheeler. The term has remained in currency, for example: \"Parabiosis is defined as a special symbiosis, so far known only from a few Neotropical ant species, in which two or more species occupy the same nest site and forage together while keeping brood separate\".\n\nAs is to be expected in matters concerning ethology and ecology however, such a class of behaviour patterns cannot be distinguished puristically from every other; there are exceptions and differences of kind and degree. Parabiosis ranges from the sharing of common trails to sharing common nests, and from effectively tolerating another species in general, to toleration only of the single colony of that species that shares the same nest. Furthermore, there is evidence for the partitioning of functions and unequal sharing of work between the two species in the nest. Early reports that parabiotic ant colonies forage and feed together peacefully also have been qualified by observations that revealed ants of one species in such an association aggressively displacing members of the other species from artificially provided food, while also profiting by following their recruitment trails to new food sources.\n\nIn practice parabiosis hardly could be a purely neutral interaction. There can be consequent benefits from shared nest defence and maintenance even when there is neither direct cooperation nor inimical interaction between the two associated populations in a nest.\n\n",
    "id": "26305461",
    "title": "Parabiosis"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=41439747",
    "text": "Pseudohypoxia\n\nPseudohypoxia refers to increased cytosolic ratio of free NAD to NADH in cells.\nResearch has shown that declining levels of NAD+ during aging cause pseudohypoxia, and that raising nuclear NAD+ in old mice reverses pseudohypoxia and metabolic dysfunction, thus reversing the aging process. It is expected that human NAD trials will begin in 2014.\n\n",
    "id": "41439747",
    "title": "Pseudohypoxia"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=41163070",
    "text": "Fast-scan cyclic voltammetry\n\nFast-scan cyclic voltammetry (FSCV) is cyclic voltammetry with a very high scan rate (up to ). Application of high scan rate allows rapid acquisition of a voltammogram within several milliseconds and ensures high temporal resolution of this electroanalytical technique. An acquisition rate of 10 Hz is routinely employed.\n\nFSCV in combination with carbon-fiber microelectrodes became a very popular method for detection of neurotransmitters, hormones and metabolites in biological systems. Initially, FSCV was successfully used for detection of electrochemically active biogenic amines release in chromaffin cells (adrenaline and noradrenaline), brain slices (5-HT, dopamine, norepinephrine) and in vivo in anesthetized or awake and behaving animals (dopamine). Further refinements of the method have enabled detection of 5-HT, norepinephrine, adenosine, oxygen, pH changes in vivo in rats and mice as well as measurement of dopamine and serotonin concentration in fruit flies.\n\nIn fast-scan cyclic voltammetry (FSCV), a small carbon fiber electrode (micrometer scale) is inserted into living cells, tissue, or extracellular space. The electrode is then used to quickly raise and lower the voltage in a triangular wave fashion. When the voltage is in the correct range (typically ±1 Volt) the compound of interest will be repeatedly oxidized and reduced. This will result in a movement of electrons in solution that will ultimately create a small alternating current (nano amps scale). By subtracting the background current created by the probe from the resulting current, it is possible to generate a voltage vs. current plot that is unique to each compound. Since the time scale of the voltage oscillations is known, this can then be used to calculate a plot of the current in solution as a function of time. The relative concentrations of the compound may be calculated as long as the number of electrons transferred in each oxidation and reduction reaction is known.\n\nAdvantages such as chemical specificity, high resolution, and noninvasive probes make FSCV a powerful technique for detecting changing chemical concentrations in vivo. The chemical specificity of FSCV is derived from reduction potentials. Every compound has a unique reduction potential, and so the alternating voltage can be set to select for a particular compound. As a result, FSCV can be used to measure a variety of electrically active biological compounds such as catacholamines, indolamines, and neurotransmitters. Concentration changes regarding ascorbic acid, oxygen, nitric oxide, and hydrogen ions (pH) can also be detected. It can even be used to measure multiple compounds at the same time, as long as one has a positive and the other has a negative redox potential. High resolution is achieved by changing the voltage at very high speeds, referred to as a fast scan rate. Scan rates for FSCV are on the sub-second scale, oxidizing and reducing compounds in microseconds. Another advantage of FSCV is its ability to be used in vivo. Typical electrodes consist of small carbon fiber needles that are micrometers in diameter and able to be noninvasively inserted into live tissues. The size of the electrode also permits it to probe very specific brain regions. Thus, FSCV has proved to be effective in measuring chemical fluctuations of living organisms and has been used in conjunction with several behavioral studies.\n\nAcceptable voltage and current ranges are common limitations of FSCV. To start, the electric potential must stay within the voltage range of the electrolysis of water (Eo = ± 1.23). Additionally, the resulting current must remain low in order to avoid cell lysis as well as cell depolarization. Fast scan cyclic voltammetry is also limited in that it only makes differential measurements; the currents it measures are only relative to the background, so they cannot be used to quantify resting concentrations. This is partially due to the fact that the basal current levels are largely effected by factors such as pH, so over longer periods of time these values tend to drift. The age of the electrode is also important, and probes tend to be less accurate the longer they are used.\n\nThis technique is also limited to quantifying the concentrations of electrically active compounds, and can only be used with select molecules in biological systems. In spite of this, there have been methods developed to measure levels of non-electric enzymes that have an electroactive substrate. However, in this scenario, the electrode probes are also a limiting factor in the data resolution. When measuring an electroactive substrate, the probe is often coated with its corresponding enzyme. In order to avoid the enzyme interacting with different substrates, the electrode is also coated with a polymer that acts as a selective filter against particular types of ions. However, when this polymer is added it lowers the speed at which the voltage scans can be made and effectively lowers the data resolution.\n\nFSCV is used to monitor changes in concentration of dopamine in mammalian brain in real time with sensitivity down to 1 nM. Using an acquisition rate of 10 Hz is fast enough to sample dynamics of neurotransmitter release and clearance. Pharmacological action of dopaminergic drugs such as D1 and D2 receptors agonists and antagonist (raclopride, haloperidol), dopamine transporter blockers (cocaine, nomifensine, GBR 12909) could be evaluated with FSCV. The fast acquisition rate also allows the study of dopamine dynamics during behavior.\n\nThe effects of psychostimulants (cocaine, amphetamine and methamphetamine), opioids (morphine and heroin), cannabinoids, alcohol and nicotine on dopaminergic neurotransmission and development of drug addiction was studied with FSCV.\n\nDopamine is a primary neurotransmitter mediating learning, goal-directing behavior and decision making. Monitoring of dopamine concentration in vivo in behaving animals with FSCV reveals dopamine coding of the brain's decision making process.\n\nFSCV is used to study dynamics of exocytosis of noradrenaline and adrenaline from chromaffin cells; release of serotonin from mast cells; release of 5-HT in brain slices; release of 5-HT in brain of anesthetized rodents and fruite flies; release of norepinephrine in brain of anesthetized and freely moving rodents.\n\n",
    "id": "41163070",
    "title": "Fast-scan cyclic voltammetry"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=20433613",
    "text": "Universality–diversity paradigm\n\nThe universality–diversity paradigm is the analysis of biological materials based on the universality and diversity of its fundamental structural elements and functional mechanisms. The analysis of biological systems based on this classification has been a cornerstone of modern biology.\n\nFor example, proteins constitute the elementary building blocks of a vast variety of biological materials such as cells, spider silk or bone, where they create extremely robust, multi-functional materials by self-organization of structures over many length- and time scales, from nano to macro. Some of the structural features are commonly found in many different tissues, that is, they are highly conserved. Examples of such universal building blocks include alpha-helices, beta-sheets or tropocollagen molecules. In contrast, other features are highly specific to tissue types, such as particular\nfilament assemblies, beta-sheet nanocrystals in spider silk or tendon fascicles. This coexistence of universality and diversity—referred to as the universality–diversity paradigm (UDP)—is an overarching feature in biological materials and a crucial component of materiomics. It might provide guidelines for bioinspired and biomimetic material development, where this concept is translated into the use of inorganic or hybrid organic-inorganic building blocks.\n\n\n",
    "id": "20433613",
    "title": "Universality–diversity paradigm"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=9127632",
    "text": "Biology\n\nBiology is the natural science that involves the study of life and living organisms, including their physical structure, chemical composition, function, development and evolution. Modern biology is a vast field, composed of many branches. Despite the broad scope and the complexity of the science, there are certain unifying concepts that consolidate it into a single, coherent field. Biology recognizes the cell as the basic unit of life, genes as the basic unit of heredity, and evolution as the engine that propels the creation of new species. Living organisms are open systems that survive by transforming energy and decreasing their local entropy to maintain a stable and vital condition defined as homeostasis. See glossary of biology. \n\nSub-disciplines of biology are defined by the scale at which life is studied, the kinds of organisms studied, and the methods used to study them: biochemistry examines the rudimentary chemistry of life; molecular biology studies the complex interactions among biological molecules; cellular biology examines the basic building-block of all life, the cell; physiology examines the physical and chemical functions of tissues, organs, and organ systems; ecology examines how organisms interact in their environment; and evolutionary biology examines the processes that produced the diversity of life. \n\nThe term \"biology\" is derived from the Greek word , \"bios\", \"life\" and the suffix , \"-logia\", \"study of.\" The Latin-language form of the term first appeared in 1736 when Swedish scientist Carl Linnaeus (Carl von Linné) used \"biologi\" in his \"Bibliotheca botanica\". It was used again in 1766 in a work entitled \"Philosophiae naturalis sive physicae: tomus III, continens geologian, biologian, phytologian generalis\", by Michael Christoph Hanov, a disciple of Christian Wolff. The first German use, \"Biologie\", was in a 1771 translation of Linnaeus' work. In 1797, Theodor Georg August Roose used the term in the preface of a book, \"Grundzüge der Lehre van der Lebenskraft\". Karl Friedrich Burdach used the term in 1800 in a more restricted sense of the study of human beings from a morphological, physiological and psychological perspective (\"Propädeutik zum Studien der gesammten Heilkunst\"). The term came into its modern usage with the six-volume treatise \"Biologie, oder Philosophie der lebenden Natur\" (1802–22) by Gottfried Reinhold Treviranus, who announced:\n\nAlthough modern biology is a relatively recent development, sciences related to and included within it have been studied since ancient times. Natural philosophy was studied as early as the ancient civilizations of Mesopotamia, Egypt, the Indian subcontinent, and China. However, the origins of modern biology and its approach to the study of nature are most often traced back to ancient Greece. While the formal study of medicine dates back to Hippocrates (ca. 460 BC – ca. 370 BC), it was Aristotle (384 BC – 322 BC) who contributed most extensively to the development of biology. Especially important are his \"History of Animals\" and other works where he showed naturalist leanings, and later more empirical works that focused on biological causation and the diversity of life. Aristotle's successor at the Lyceum, Theophrastus, wrote a series of books on botany that survived as the most important contribution of antiquity to the plant sciences, even into the Middle Ages.\n\nScholars of the medieval Islamic world who wrote on biology included al-Jahiz (781–869), Al-Dīnawarī (828–896), who wrote on botany, and Rhazes (865–925) who wrote on anatomy and physiology. Medicine was especially well studied by Islamic scholars working in Greek philosopher traditions, while natural history drew heavily on Aristotelian thought, especially in upholding a fixed hierarchy of life.\n\nBiology began to quickly develop and grow with Anton van Leeuwenhoek's dramatic improvement of the microscope. It was then that scholars discovered spermatozoa, bacteria, infusoria and the diversity of microscopic life. Investigations by Jan Swammerdam led to new interest in entomology and helped to develop the basic techniques of microscopic dissection and staining.\n\nAdvances in microscopy also had a profound impact on biological thinking. In the early 19th century, a number of biologists pointed to the central importance of the cell. Then, in 1838, Schleiden and Schwann began promoting the now universal ideas that (1) the basic unit of organisms is the cell and (2) that individual cells have all the characteristics of life, although they opposed the idea that (3) all cells come from the division of other cells. Thanks to the work of Robert Remak and Rudolf Virchow, however, by the 1860s most biologists accepted all three tenets of what came to be known as cell theory.\n\nMeanwhile, taxonomy and classification became the focus of natural historians. Carl Linnaeus published a basic taxonomy for the natural world in 1735 (variations of which have been in use ever since), and in the 1750s introduced scientific names for all his species. Georges-Louis Leclerc, Comte de Buffon, treated species as artificial categories and living forms as malleable—even suggesting the possibility of common descent. Though he was opposed to evolution, Buffon is a key figure in the history of evolutionary thought; his work influenced the evolutionary theories of both Lamarck and Darwin.\n\nSerious evolutionary thinking originated with the works of Jean-Baptiste Lamarck, who was the first to present a coherent theory of evolution. He posited that evolution was the result of environmental stress on properties of animals, meaning that the more frequently and rigorously an organ was used, the more complex and efficient it would become, thus adapting the animal to its environment. Lamarck believed that these acquired traits could then be passed on to the animal's offspring, who would further develop and perfect them. However, it was the British naturalist Charles Darwin, combining the biogeographical approach of Humboldt, the uniformitarian geology of Lyell, Malthus's writings on population growth, and his own morphological expertise and extensive natural observations, who forged a more successful evolutionary theory based on natural selection; similar reasoning and evidence led Alfred Russel Wallace to independently reach the same conclusions. Although it was the subject of controversy (which continues to this day), Darwin's theory quickly spread through the scientific community and soon became a central axiom of the rapidly developing science of biology.\n\nThe discovery of the physical representation of heredity came along with evolutionary principles and population genetics. In the 1940s and early 1950s, experiments pointed to DNA as the component of chromosomes that held the trait-carrying units that had become known as genes. A focus on new kinds of model organisms such as viruses and bacteria, along with the discovery of the double helical structure of DNA in 1953, marked the transition to the era of molecular genetics. From the 1950s to present times, biology has been vastly extended in the molecular domain. The genetic code was cracked by Har Gobind Khorana, Robert W. Holley and Marshall Warren Nirenberg after DNA was understood to contain codons. Finally, the Human Genome Project was launched in 1990 with the goal of mapping the general human genome. This project was essentially completed in 2003, with further analysis still being published. The Human Genome Project was the first step in a globalized effort to incorporate accumulated knowledge of biology into a functional, molecular definition of the human body and the bodies of other organisms.\n\nCell theory states that the cell is the fundamental unit of life, that all living things are composed of one or more cells, and that all cells arise from other cells through cell division. In multicellular organisms, every cell in the organism's body derives ultimately from a single cell in a fertilized egg. The cell is also considered to be the basic unit in many pathological processes. In addition, the phenomenon of energy flow occurs in cells in processes that are part of the function known as metabolism. Finally, cells contain hereditary information (DNA), which is passed from cell to cell during cell division. Research into the origin of life, abiogenesis, amounts to an attempt to discover the origin of the first cells.\n\nA central organizing concept in biology is that life changes and develops through evolution, and that all life-forms known have a common origin. The theory of evolution postulates that all organisms on the Earth, both living and extinct, have descended from a common ancestor or an ancestral gene pool. This universal common ancestor of all organisms is believed to have appeared about 3.5 billion years ago. Biologists regard the ubiquity of the genetic code as definitive evidence in favor of the theory of universal common descent for all bacteria, archaea, and eukaryotes (see: origin of life).\n\nThe term \"evolution\" was introduced into the scientific lexicon by Jean-Baptiste de Lamarck in 1809, and fifty years later Charles Darwin posited a scientific model of natural selection as evolution's driving force. (Alfred Russel Wallace is recognized as the co-discoverer of this concept as he helped research and experiment with the concept of evolution.) Evolution is now used to explain the great variations of life found on Earth.\n\nDarwin theorized that species flourish or die when subjected to the processes of natural selection or selective breeding. Genetic drift was embraced as an additional mechanism of evolutionary development in the modern synthesis of the theory.\n\nThe evolutionary history of the species—which describes the characteristics of the various species from which it descended—together with its genealogical relationship to every other species is known as its phylogeny. Widely varied approaches to biology generate information about phylogeny. These include the comparisons of DNA sequences, a product of molecular biology (more particularly genomics), and comparisons of fossils or other records of ancient organisms, a product of paleontology. Biologists organize and analyze evolutionary relationships through various methods, including phylogenetics, phenetics, and cladistics. (For a summary of major events in the evolution of life as currently understood by biologists, see evolutionary timeline.) \n\nEvolution is relevant to the understanding of the natural history of life forms and to the understanding of the organization of current life forms. But, those organizations can only be understood in the light of how they came to be by way of the process of evolution. Consequently, evolution is central to all fields of biology. \n\nGenes are the primary units of inheritance in all organisms. A gene is a unit of heredity and corresponds to a region of DNA that influences the form or function of an organism in specific ways. All organisms, from bacteria to animals, share the same basic machinery that copies and translates DNA into proteins. Cells transcribe a DNA gene into an RNA version of the gene, and a ribosome then translates the RNA into a sequence of amino acids known as a protein. The translation code from RNA codon to amino acid is the same for most organisms. For example, a sequence of DNA that codes for insulin in humans also codes for insulin when inserted into other organisms, such as plants.\n\nDNA is found as linear chromosomes in eukaryotes, and circular chromosomes in prokaryotes. A chromosome is an organized structure consisting of DNA and histones. The set of chromosomes in a cell and any other hereditary information found in the mitochondria, chloroplasts, or other locations is collectively known as a cell's genome. In eukaryotes, genomic DNA is localized in the cell nucleus, or with small amounts in mitochondria and chloroplasts. In prokaryotes, the DNA is held within an irregularly shaped body in the cytoplasm called the nucleoid. The genetic information in a genome is held within genes, and the complete assemblage of this information in an organism is called its genotype.\n\nHomeostasis is the ability of an open system to regulate its internal environment to maintain stable conditions by means of multiple dynamic equilibrium adjustments that are controlled by interrelated regulation mechanisms. All living organisms, whether unicellular or multicellular, exhibit homeostasis.\n\nTo maintain dynamic equilibrium and effectively carry out certain functions, a system must detect and respond to perturbations. After the detection of a perturbation, a biological system normally responds through negative feedback that stabilize conditions by reducing or increasing the activity of an organ or system. One example is the release of glucagon when sugar levels are too low.\n\nThe survival of a living organism depends on the continuous input of energy. Chemical reactions that are responsible for its structure and function are tuned to extract energy from substances that act as its food and transform them to help form new cells and sustain them. In this process, molecules of chemical substances that constitute food play two roles; first, they contain energy that can be transformed and reused in that organism's biological, chemical reactions; second, food can be transformed into new molecular structures (biomolecules) that are of use to that organism.\n\nThe organisms responsible for the introduction of energy into an ecosystem are known as producers or autotrophs. Nearly all such organisms originally draw their energy from the sun. Plants and other phototrophs use solar energy via a process known as photosynthesis to convert raw materials into organic molecules, such as ATP, whose bonds can be broken to release energy. A few ecosystems, however, depend entirely on energy extracted by chemotrophs from methane, sulfides, or other non-luminal energy sources.\n\nSome of the energy thus captured produces biomass and energy that is available for growth and development of other life forms. The majority of the rest of this biomass and energy are lost as waste molecules and heat. The most important processes for converting the energy trapped in chemical substances into energy useful to sustain life are metabolism and cellular respiration.\n\nMolecular biology is the study of biology at the molecular level. This field overlaps with other areas of biology, particularly those of genetics and biochemistry. Molecular biology is a study of the interactions of the various systems within a cell, including the interrelationships of DNA, RNA, and protein synthesis and how those interactions are regulated.\n\nThe next larger scale, cell biology, studies the structural and physiological properties of cells, including their internal behavior, interactions with other cells, and with their environment. This is done on both the microscopic and molecular levels, for unicellular organisms such as bacteria, as well as the specialized cells of multicellular organisms such as humans. Understanding the structure and function of cells is fundamental to all of the biological sciences. The similarities and differences between cell types are particularly relevant to molecular biology.\n\nAnatomy is a treatment of the macroscopic forms of such structures organs and organ systems.\n\nGenetics is the science of genes, heredity, and the variation of organisms. Genes encode the information needed by cells for the synthesis of proteins, which in turn play a central role in influencing the final phenotype of the organism. Genetics provides research tools used in the investigation of the function of a particular gene, or the analysis of genetic interactions. Within organisms, genetic information is physically represented as chromosomes, within which it is represented by a particular sequence of amino acids in particular DNA molecules. \n\nDevelopmental biology studies the process by which organisms grow and develop. Developmental biology, originated from embryology, studies the genetic control of cell growth, cellular differentiation, and \"cellular morphogenesis,\" which is the process that progressively gives rise to tissues, organs, and anatomy.\nModel organisms for developmental biology include the round worm \"Caenorhabditis elegans,\" the fruit fly \"Drosophila melanogaster,\" the zebrafish \"Danio rerio,\" the mouse \"Mus musculus\", and the weed \"Arabidopsis thaliana\". (A model organism is a species that is extensively studied to understand particular biological phenomena, with the expectation that discoveries made in that organism provide insight into the workings of other organisms.)\n\nPhysiology is the study of the mechanical, physical, and biochemical processes of living organisms function as a whole. The theme of \"structure to function\" is central to biology. Physiological studies have traditionally been divided into plant physiology and animal physiology, but some principles of physiology are universal, no matter what particular organism is being studied. For example, what is learned about the physiology of yeast cells can also apply to human cells. The field of animal physiology extends the tools and methods of human physiology to non-human species. Plant physiology borrows techniques from both research fields.\n\nPhysiology is the study the interaction of how, for example, the nervous, immune, endocrine, respiratory, and circulatory systems, function and interact. The study of these systems is shared with such medically oriented disciplines as neurology and immunology.\n\nEvolutionary research is concerned with the origin and descent of species, and their change over time. It employs scientists from many taxonomically oriented disciplines, for example, those with special training in particular organisms such as mammalogy, ornithology, botany, or herpetology, but are of use in answering more general questions about evolution.\n\nEvolutionary biology is partly based on paleontology, which uses the fossil record to answer questions about the mode and tempo of evolution, and partly on the developments in areas such as population genetics. In the 1980s, developmental biology re-entered evolutionary biology after its initial exclusion from the modern synthesis through the study of evolutionary developmental biology. Phylogenetics, systematics, and taxonomy are related fields often considered part of evolutionary biology.\n\nMultiple speciation events create a tree structured system of relationships between species. The role of systematics is to study these relationships and thus the differences and similarities between species and groups of species.\nHowever, systematics was an active field of research long before evolutionary thinking was common.\n\nTraditionally, living things have been divided into five kingdoms: Monera; Protista; Fungi; Plantae; Animalia. However, many scientists now consider this five-kingdom system outdated. Modern alternative classification systems generally begin with the three-domain system: Archaea (originally Archaebacteria); Bacteria (originally Eubacteria) and Eukaryota (including protists, fungi, plants, and animals) These domains reflect whether the cells have nuclei or not, as well as differences in the chemical composition of key biomolecules such as ribosomes.\n\nFurther, each kingdom is broken down recursively until each species is separately classified. The order is:\nDomain; Kingdom; Phylum; Class; Order; Family; Genus; Species.\n\nOutside of these categories, there are obligate intracellular parasites that are \"on the edge of life\" in terms of metabolic activity, meaning that many scientists do not actually classify such structures as alive, due to their lack of at least one or more of the fundamental functions or characteristics that define life. They are classified as viruses, viroids, prions, or satellites.\n\nThe scientific name of an organism is generated from its genus and species. For example, humans are listed as \"Homo sapiens\". \"Homo\" is the genus, and \"sapiens\" the species. When writing the scientific name of an organism, it is proper to capitalize the first letter in the genus and put all of the species in lowercase. Additionally, the entire term may be italicized or underlined.\n\nThe dominant classification system is called the Linnaean taxonomy. It includes ranks and binomial nomenclature. How organisms are named is governed by international agreements such as the International Code of Nomenclature for algae, fungi, and plants (ICN), the International Code of Zoological Nomenclature (ICZN), and the International Code of Nomenclature of Bacteria (ICNB). The classification of viruses, viroids, prions, and all other sub-viral agents that demonstrate biological characteristics is conducted by the International Committee on Taxonomy of Viruses (ICTV) and is known as the International Code of Viral Classification and Nomenclature (ICVCN). However, several other viral classification systems do exist.\n\nA merging draft, BioCode, was published in 1997 in an attempt to standardize nomenclature in these three areas, but has yet to be formally adopted. The BioCode draft has received little attention since 1997; its originally planned implementation date of January 1, 2000, has passed unnoticed. A revised BioCode that, instead of replacing the existing codes, would provide a unified context for them, was proposed in 2011. However, the International Botanical Congress of 2011 declined to consider the BioCode proposal. The ICVCN remains outside the BioCode, which does not include viral classification.\n\nEcology is the study of the distribution and abundance of living organisms, the interaction between them and their environment. An organism shares an environment that includes other organisms and biotic factors as well as local abiotic factors (non-living) such as climate and ecology. One reason that biological systems can be difficult to study is that so many different interactions with other organisms and the environment are possible, even on small scales. A microscopic bacterium responding to a local sugar gradient is responding to its environment as much as a lion searching for food in the African savanna. For any species, behaviors can be co-operative, competitive, parasitic, or symbiotic. Matters become more complex when two or more species interact in an ecosystem.\n\nEcological systems are studied at several different levels, from the scale of the ecology of individual organisms, to those of populations, to the ecosystems and finally the biosphere. The term population biology is often used interchangeably with population ecology, although \"population biology\" is more frequently used in the case of diseases, viruses, and microbes, while the term population ecology is more commonly applied to the study of plants and animals. Ecology draws on many subdisciplines.\n\nEthology is the study of animal behavior (particularly that of social animals such as primates and canids), and is sometimes considered a branch of zoology. Ethologists have been particularly concerned with the evolution of behavior and the understanding of behavior in terms of the theory of natural selection. In one sense, the first modern ethologist was Charles Darwin, whose book, \"The Expression of the Emotions in Man and Animals,\" influenced many ethologists to come.\n\nBiogeography studies the spatial distribution of organisms on the Earth, focusing on such topics as plate tectonics, climate change, dispersal and migration, and cladistics.\n\nDespite the profound advances made over recent decades in our understanding of life's fundamental processes, some basic problems have remained unresolved. One of the major unresolved problems in biology is the primary adaptive function of sex, and particularly its key processes in eukaryotes of meiosis and homologous recombination. One view is that sex evolved primarily as an adaptation that promoted increased genetic diversity (see references e.g.). An alternative view is that sex is an adaptation for promoting accurate DNA repair in germ-line DNA, and that increased genetic diversity is primarily a byproduct that may be useful in the long run. (See also Evolution of sexual reproduction).\n\nAnother basic unresolved problem in biology is the biologic basis of aging. At present, there is no consensus view on the underlying cause of aging. Various competing theories are outlined in Ageing Theories.\n\nThese are the main branches of biology:\nFor a more detailed list, see outline of biology.\n\n\n\n",
    "id": "9127632",
    "title": "Biology"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=44057611",
    "text": "Tokogeny\n\nTokogeny or tocogeny is the biological relationship between parent and offspring, or more generally between ancestors and descendants. In contradistinction to phylogeny it applies to individual organisms as opposed to species.\n\nIn the tokogentic system shared characteristics are called \"traits\".\n",
    "id": "44057611",
    "title": "Tokogeny"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=44055899",
    "text": "Triploid block\n\nTriploid block is a phenomenon describing the formation of nonviable progeny after hybridization of flowering plants that differ in ploidy. The barrier is established in the endosperm, a nutritive tissue supporting embryo growth. This phenomenon usually happens when autopolyploidy occurs in diploid plants. Triploid blocks lead to reproductive isolation. The triploid block effects have been explained as possibly due to genomic imprinting in the endosperm.\n",
    "id": "44055899",
    "title": "Triploid block"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=44961465",
    "text": "Pinch-induced behavioral inhibition\n\nPinch-induced behavioral inhibition (PIBI) or clipnosis is a partially inert state which results from a gentle squeeze of the skin behind the neck. It is mostly observed among Felidae and allows the mother cat to easily carry her kitten with her jaws. It can be used to effectively restrain most cats in a domestic or veterinary context.\n",
    "id": "44961465",
    "title": "Pinch-induced behavioral inhibition"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=319610",
    "text": "Biological life cycle\n\nIn biology, a biological life cycle (or just life cycle when the biological context is clear) is a series of changes in form that an organism undergoes, returning to the starting state. \"The concept is closely related to those of the life history, development and ontogeny, but differs from them in stressing renewal.\" Transitions of form may involve growth, asexual reproduction, or sexual reproduction.\n\nIn some organisms, different \"generations\" of the species succeed each other during the life cycle. For plants and many algae, there are two multicellular stages, and the life cycle is referred to as alternation of generations. The term life history is often used, particularly for organisms such as the red algae which have three multicellular stages (or more), rather than two.\n\nLife cycles that include sexual reproduction involve alternating haploid (\"n\") and diploid (2\"n\") stages, i.e., a change of ploidy is involved. To return from a diploid stage to a haploid stage, meiosis must occur. In regard to changes of ploidy, there are 3 types of cycles:\n\nThe cycles differ in when mitosis (growth) occurs. Zygotic meiosis and gametic meiosis have one mitotic stage: mitosis occurs during the \"n\" phase in zygotic meiosis and during the 2\"n\" phase in gametic meiosis. Therefore, zygotic and gametic meiosis are collectively termed haplobiontic (single mitotic phase, not to be confused with haplontic). Sporic meiosis, on the other hand, has mitosis in two stages, both the diploid and haploid stages, termed diplobiontic (not to be confused with diplontic).\n\nThe study of reproduction and development in organisms was carried out by many botanists and zoologists.\n\nWilhelm Hofmeister demonstrated that alternation of generations is a feature that unites plants, and published this result in 1851 (see plant sexuality).\n\nSome terms (haplobiont and diplobiont) used for the description of life cycles were proposed initially for algae by Nils Svedelius, and then became used for other organisms. Other terms (autogamy and gamontogamy) used in protist life cycles were introduced by Karl Gottlieb Grell. The description of the complex life cycles of various organisms contributed to the disproof of the ideas of spontaneous generation in the 1840s and 1850s.\n\nA zygotic meiosis is a meiosis of a zygote immediately after karyogamy, which is the fusion of two cell nuclei. This way, the organism ends its diploid phase and produces several haploid cells. These cells divide mitotically to form either larger, multicellular individuals, or more haploid cells. Two opposite types of gametes (e.g., male and female) from these individuals or cells fuse to become a zygote.\n\nIn the whole cycle, zygotes are the only diploid cell; mitosis occurs only in the haploid phase.\n\nThe individuals or cells as a result of mitosis are haplonts, hence this life cycle is also called haplontic life cycle. Haplonts are:\n\n\nIn gametic meiosis, instead of immediately dividing \"meiotically\" to produce haploid cells, the zygote divides \"mitotically\" to produce a multicellular diploid individual or a group of more unicellular diploid cells. Cells from the diploid individuals then undergo meiosis to produce haploid cells or gametes. Haploid cells may divide again (by mitosis) to form more haploid cells, as in many yeasts, but the haploid phase is not the predominant life cycle phase. In most diplonts, mitosis occurs only in the diploid phase, i.e. gametes usually form quickly and fuse to produce diploid zygotes.\n\nIn the whole cycle, gametes are usually the only haploid cells, and mitosis usually occurs only in the diploid phase.\n\nThe diploid multicellular individual is a diplont, hence a gametic meiosis is also called a diplontic life cycle. Diplonts are:\n\n\nIn sporic meiosis (also commonly known as intermediary meiosis), the zygote divides mitotically to produce a multicellular diploid sporophyte. The sporophyte creates spores via meiosis which \"also\" then divide mitotically producing haploid individuals called gametophytes. The gametophytes produce gametes via mitosis. In some plants the gametophyte is not only small-sized but also short-lived; in other plants and many algae, the gametophyte is the \"dominant\" stage of the life cycle.\n\nHaplodiplonts are:\n\n\nSome animals have a sex-determination system called haplodiploid, but this is not related to the haplodiplontic life cycle.\n\nSome red algae (such as \"Bonnemaisonia\" and \"Lemanea\") and green algae (such as \"Prasiola\") have vegetative meiosis, also called somatic meiosis, which is a rare phenomenon. Vegetative meiosis can occur in haplodiplontic and also in diplontic life cycles. The gametophytes remain attached to and part of the sporophyte. Vegetative (non-reproductive) diploid cells undergo meiosis, generating vegetative haploid cells. These undergo many mitosis, and produces gametes.\n\nA different phenomenon, called vegetative diploidization, a type of apomixis, occurs in some brown algae (e.g., \"Elachista stellaris\"). Cells in a haploid part of the plant spontaneously duplicate their chromosomes to produce diploid tissue.\n\nThe primitive type of life cycle probably had haploid individuals with asexual reproduction. Bacteria and archaea exhibit a life cycle like this, and some eukaryotes apparently do too (e.g., Cryptophyta, Choanoflagellata, many Euglenozoa, many Amoebozoa, some red algae, some green algae, the imperfect fungi, some rotifers and many other groups, not necessarily haploid). However, these eukaryotes probably are not primitively asexual, but have lost their sexual reproduction, or it just was not observed yet. Many eukaryotes (including animals and plants) exhibit asexual reproduction, which may be facultative or obligate in the life cycle, with sexual reproduction occurring more or less frequently.\n\n\n",
    "id": "319610",
    "title": "Biological life cycle"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1989020",
    "text": "Indeterminate growth\n\nIn biology and botany, indeterminate growth is growth that is not terminated in contrast to determinate growth that stops once a genetically pre-determined structure has completely formed. Thus, a plant that grows and produces flowers and fruit until killed by frost or some other external factor is called indeterminate. For example, the term is applied to tomato varieties that grow in a rather gangly fashion, producing fruit throughout the growing season, and in contrast to a determinate tomato plant, which grows in a more bushy shape and is most productive for a single, larger harvest, then either tapers off with minimal new growth/fruit, or dies.\n\nIn reference to an inflorescence (a shoot specialised for bearing flowers, and bearing no leaves other than bracts), an indeterminate type (such as a raceme) is one in which the first flowers to develop and open are from the buds at the base, followed progressively by buds nearer to the growing tip. The growth of the shoot is not impeded by the opening of the early flowers or development of fruits and its appearance is of growing, producing, and maturing flowers and fruit indefinitely. In practice the continued growth of the terminal end necessarily peters out sooner or later, though without producing any definite terminal flower, and in some species it may stop growing before any of the buds have opened. \n\nNot all plants produce indeterminate inflorescences however; some produce a definite terminal flower that terminates the development of new buds towards the tip of that inflorescence. In most species that produce a \"'determinate\" inflorescence in this way, all of the flower buds are formed before the first ones begin to open, and all open more or less at the same time. In some species with determinate inflorescences however, the terminal flower blooms first, which stops the elongation of the main axis, but side buds develop lower down. One type of example is Dianthus another type is exemplified by Allium and yet others by Daucus.\n\nIn zoology, indeterminate growth refers to the condition where animals grow rapidly when young, and continue to grow after reaching adulthood although at a slower pace. It is common in fish, amphibians, reptiles, and many molluscs. The term also refers to the pattern of hair growth sometimes seen in humans and a few domestic breeds, where hair continues to grow in length until it is cut.\n\nSome mushrooms – notably \"Cantharellus californicus\" – also exhibit indeterminate growth.\n\n",
    "id": "1989020",
    "title": "Indeterminate growth"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=46439815",
    "text": "Feminist biology\n\nFeminist biology is an approach to biology that is concerned with the influence of gender values, the removal of gender bias, and the understanding of the overall role of social values in biological research and practices. Feminist Biology, was founded by, among others, Dr. Ruth Bleier of the University of Wisconsin-Madison (who authored the 1984 work “Science and Gender: A Critique of Biology and Its Theories on Women” and inspired the university’s endowed fellowship for feminist biology), it aims to enhance biology by incorporating feminist critique in matters varying from the mechanisms of cell biology and sex selection to the assessment of the meaning of words such as “gender” and “sex.”\nOverall, the field is broadly defined and pertains itself to philosophies behind both biological and feminist practice. These considerations make feminist biology debatable and conflictive with itself, particularly when concerning matters of biological determinism, whereby descriptive sex terms of male and female are intrinsically confining, or extreme post-modernism, whereby the body is viewed more as a social construct. Despite opinions ranging from determinist to postmodernist, however, biologists, feminists, and feminist biologists of varying labels alike have made claims to the utility of applying feminist ideology to biological practice and procedure.\n\nDonna J. Haraway, a biologist and primatologist hailing from the University of California, put forth male bias criticisms in 1989 concerning the study of human evolution and culture via primatology by denoting a prominent lack of focus in female primates. Haraway contributed to a large discovery of behaviors in primate groups regarding mate selection, and female-female interactions derived from observing female primates, citing feminist influences as she studied the female primates in their own merit.\n\nSimilarly, feminist cell biologists of The Biology and Gender Study Group have criticized androcentrism in the study of behavior between sexes and “come to look at feminist critique as [they] would any experimental control.” They cite a general trend of “active” biological description associated with the sperm gamete and “passive” description to the ovum, comparing such description to an archetypal hero facing many challenges before it finding its static, female home. The group criticized the diction employed by biological readings and textbooks, stating that the more active and risk-associated traits of the ovum (such as its own survival from the whittling of 2 million oocytes) are dismissed for the sake of a patterned narrative.\n\nAnne Fausto-Sterling, a professor of Biology and Gender Studies at Brown University, assesses the complexity of defining sex through a dichotomous lens in a variety of her works such as Sexing the Body: Gender Politics and the Construction of Sexuality as well as in an article piece she wrote titled “The Five Sexes: Why Male and Female Are Not Enough.” She addresses the existence of intersex individuals and the lack of acknowledgment of their state of being in the context of a dichotomously defined world of sexes – even, if not especially, by medical professionals and surgeons who understand intersex anatomy to a point to where they can surgically alter it to one of the sexes. She states: “Ironically, a more sophisticated knowledge of the complexity of sexual systems has led to the repression of such intricacy.” Fausto-Sterling continues by advocating the reevaluation of what is considered urgent medical intervention in light of the influence she believes social stigma has had on standard medical procedure – which in turn could help open up the possible directions that science could take.\n\nVarying motivations are at work for those who advocate feminist biology. Many individuals are motivated by the need to discern a more objective, scientific truth from culturally influenced practices (such as with Fausto-Sterling’s five sexes), while others are concerned dispelling stigmatic prejudices that influential figures have accepted as scientifically – and objectively – true. While extreme cases such as Edward Clarke’s controversial, 19th century assessment that women’s education diverts a harmful amount of energy away from the uterus are no longer as prevalent, more contemporary examples such as The Biology and Gender Study’s issue with gamete depiction exemplify this.\n\nIndividuals such as Carla Fehr offer constructive criticism for the future of feminist philosophy in the field of biology; Fehr proposes feminist biologists to consider novel questions pertaining to subjects such as the research of genomics in relation to gender. More skeptical criticism exists in sources such as the American Enterprise Institute, who called to question the legitimacy of feminist biology altogether in its reception to the University of Wisconsin’s endowed fellowship.\n",
    "id": "46439815",
    "title": "Feminist biology"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=46984631",
    "text": "Heteromorphosis\n\nHeteromorphosis ( – other; morphe – form) – atypical reparative (traumatic) regeneration than new organ or a tissue is different from the old one or (embryonic) development anomalies, including an abnormal location, or an abnormal shape. It should not be confused with homeosis, which means big change in tissue structure of an organ.\n\nJacques Loeb offered this term in 1892, then he was in experiments of distortion of polarity of hydroids.\nMany organisms from protozoans to the chordate may have heteromorphosis examples, but it is easier to find in lower forms of animals:\n\n",
    "id": "46984631",
    "title": "Heteromorphosis"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=46840954",
    "text": "FMRFamide in Biomphalaria glabrata\n\nBiomphalaria glabrata is a species of a freshwater snail, an aquatic gastropod mollusk in the family Planorbidae, the rams horn snails. \"B. glabrata\" is best known for its role as the intermediate host for the human-infecting trematode parasite \"Schistosoma mansoni\".\n\nThis freshwater snail species is used as a model organism, in other words, a non-human species which is extensively studied to understand a biological phenomenon, with the expectation that discoveries made in the model will provide insight into the workings of other organisms. Model organisms are in vivo models and are widely used to research human disease when human experimentation would be unfeasible or unethical.\n\nThis snail has been studied in relation to human pathology and the epidemiology of schistosomiasis. \"S. masoni\" is known to change its host’s (\"B. glabrata\"'s) behavior via the upregulation/downregulation of neuropeptides such as schistosomin and NPY, and some studies have reported that FMRFamide is aminergic, and may be implicated in the secretion of molecules to respond to infection with parasites.\n\nThe ganglionic central nervous system (CNS) of \"B. glabrata\" consists of paired cerebral, pedal, pleural, parietal, and buccal ganglia, and one unpaired visceral ganglion. FMRFamide is concentrated in the concentrated in the cerebral and visceral ganglia, although evidence from current research suggests that FMRFamide moves downward of the head-foot region of the snail as embryonic development proceeds.\n\nThe exact role of FMRFamide during early development of the embryonic central nervous system is not well studied. Detection of this neuropeptide is important because its expression lays down the foundation of the CNS in the early stages of development in invertebrates. In recent years, neuromodulatory actions of FMRFamide in invertebrates have become more apparent. This is in part due to the extensive studies done on the Planorbidae and Lymnaeidae families of pond snails.\n\nFMRFamide expression in \"B. glabrata\" can be detected as early 72 hours, post-cleavage. Studies have shown its expression pattern in the posterior of the ganglionic nervous system, as the first FMRFamide immunoreactive cell appears at 25-28% of development and is located at the extreme posterior of the embryo. The cell sends a single process on each side and each process follows the body curvature.\n\nAs these processes elongate, two lateral FMRFamide-expressing cells are apparent on either side of the body wall. Some studies have suggested that FMRFamide these structures may innervate muscles that originate in trochopore larvae and expand during development. This neuromodulator helps to regulate cardiac activity. Several FMRFamid related peptides are known, regulating various cellular functions and possessing pharmacological actions, such as anti-opiate effects. FMRFamide may also play a role in osmoregulation and developmental patterning. This neuropeoptide has multiple functions and controls many processes that allow the embryo to mature into an adult snail.\n\nFMRFamide patterning over the course of embryonic life has been recorded through immunofluorescence. Embryos at 0 to 144 hours (6 days) post cleavage were extracted from the egg mass and fixed in 4% PFA for 2 hrs at room temperature. Embryos older than 4 days were pre-treated with 0.5% ethylenediaminetetraacetic acid (EDTA) solution in PBS for 5 minutes to improve primary antibody penetration, and then fixed in 4% PFA for 2 hours at room temperature. Embryos were then washed overnight in 4% Triton X-100 in PBS and incubated in blocking solution, 1% normal goal serum in PBS, overnight at 4 °C.\n\nAfter blocking overnight, embryos were treated with a primary antibody solution at 1:2000 (in NGS/PBS). Secondary antibody tagged with fluorescein isothiocyanate (FITC)-conjugated anti-rabbit IgG (polyclonal) was incubated at 1:20, overnight at 4 °C. Embryos were then mounted on a slide and viewed with fluorescent microscopy.\n\n",
    "id": "46840954",
    "title": "FMRFamide in Biomphalaria glabrata"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=27919",
    "text": "Sociobiology\n\nSociobiology is a field of biology that aims to examine and explain social behavior in terms of evolution. It draws from disciplines including ethology, anthropology, evolution, zoology, archaeology, and population genetics. Within the study of human societies, sociobiology is closely allied to Darwinian anthropology, human behavioral ecology and evolutionary psychology.\n\nSociobiology investigates social behaviors such as mating patterns, territorial fights, pack hunting, and the hive society of social insects. It argues that just as selection pressure led to animals evolving useful ways of interacting with the natural environment, so also it led to the genetic evolution of advantageous social behavior.\n\nWhile the term \"sociobiology\" originated at least as early as the 1940s, the concept did not gain major recognition until the publication of E. O. Wilson's book \"\" in 1975. The new field quickly became the subject of controversy. Critics, led by Richard Lewontin and Stephen Jay Gould, argued that genes played a role in human behavior, but that traits such as aggressiveness could be explained by social environment rather than by biology. Sociobiologists responded by pointing to the complex relationship between nature and nurture.\n\nE. O. Wilson defined sociobiology as \"the extension of population biology and evolutionary theory to social organization\".\n\nSociobiology is based on the premise that some behaviors (social and individual) are at least partly inherited and can be affected by natural selection. It begins with the idea that behaviors have evolved over time, similar to the way that physical traits are thought to have evolved. It predicts that animals will act in ways that have proven to be evolutionarily successful over time. This can, among other things, result in the formation of complex social processes conducive to evolutionary fitness.\n\nThe discipline seeks to explain behavior as a product of natural selection. Behavior is therefore seen as an effort to preserve one's genes in the population. Inherent in sociobiological reasoning is the idea that certain genes or gene combinations that influence particular behavioral traits can be inherited from generation to generation\n\nFor example, newly dominant male lions often kill cubs in the pride that they did not sire. This behavior is adaptive because killing the cubs eliminates competition for their own offspring and causes the nursing females to come into heat faster, thus allowing more of his genes to enter into the population. Sociobiologists would view this instinctual cub-killing behavior as being inherited through the genes of successfully reproducing male lions, whereas non-killing behavior may have died out as those lions were less successful in reproducing.\n\nThe ethologist John Paul Scott coined the word \"sociobiology\" at a 1948 conference on genetics and social behaviour, and it became widely used after it was popularized by Edward O. Wilson in his 1975 book, \"Sociobiology: The New Synthesis\". However, the influence of evolution on behavior has been of interest to biologists and philosophers since soon after the discovery of evolution itself. Peter Kropotkin's \"\", written in the early 1890s, is a popular example. Antecedents of modern sociobiological thinking can be traced to the 1960s and the work of such biologists as Richard D. Alexander, Robert Trivers and William D. Hamilton.\nThe idea of the inheritance of behaviour arose from J. B. S. Haldane's idea about how \"altruistic behaviour\" (see Altruism) could be passed from generation to generation.\nWilson's book pioneered and popularized the attempt to explain the evolutionary mechanics behind social behaviors such as altruism, aggression, and nurturance, primarily in ants (Wilson's own research specialty) and other Hymenoptera, but also in other animals. The final chapter of the book is devoted to sociobiological explanations of human behavior, and Wilson later wrote a Pulitzer Prize winning book, \"On Human Nature\", that addressed human behavior specifically.\n\nEdward H. Hagen writes in \"The Handbook of Evolutionary Psychology\" that sociobiology is, despite the public controversy regarding the applications to humans, \"one of the scientific triumphs of the twentieth century.\" \"Sociobiology is now part of the core research and curriculum of virtually all biology departments, and it is a foundation of the work of almost all field biologists\" Sociobiological research on nonhuman organisms has increased dramatically and continuously in the world's top scientific journals such as \"Nature\" and \"Science\". The more general term behavioral ecology is commonly substituted for the term sociobiology in order to avoid the public controversy.\n\nSociobiologists believe that human behavior, as well as nonhuman animal behavior, can be partly explained as the outcome of natural selection. They contend that in order to fully understand behavior, it must be analyzed in terms of evolutionary considerations.\n\nNatural selection is fundamental to evolutionary theory. Variants of hereditary traits which increase an organism's ability to survive and reproduce will be more greatly represented in subsequent generations, i.e., they will be \"selected for\". Thus, inherited behavioral mechanisms that allowed an organism a greater chance of surviving and/or reproducing in the past are more likely to survive in present organisms. That inherited adaptive behaviors are present in nonhuman animal species has been multiply demonstrated by biologists, and it has become a foundation of evolutionary biology. However, there is continued resistance by some researchers over the application of evolutionary models to humans, particularly from within the social sciences, where culture has long been assumed to be the predominant driver of behavior.\n\nSociobiology is based upon two fundamental premises:\n\nSociobiology uses Nikolaas Tinbergen's four categories of questions and explanations of animal behavior. Two categories are at the species level; two, at the individual level. The species-level categories (often called \"ultimate explanations\") are\nThe individual-level categories (often called \"proximate explanations\") are\n\nSociobiologists are interested in how behavior can be explained logically as a result of selective pressures in the history of a species. Thus, they are often interested in instinctive, or intuitive behavior, and in explaining the similarities, rather than the differences, between cultures. For example, mothers within many species of mammals – including humans – are very protective of their offspring. Sociobiologists reason that this protective behavior likely evolved over time because it helped the offspring of the individuals which had the characteristic to survive. This parental protection would increase in frequency in the population. The social behavior is believed to have evolved in a fashion similar to other types of nonbehavioral adaptations, such as a coat of fur, or the sense of smell.\n\nIndividual genetic advantage fails to explain certain social behaviors as a result of gene-centred selection. E.O. Wilson argued that evolution may also act upon groups. The mechanisms responsible for group selection employ paradigms and population statistics borrowed from evolutionary game theory. Altruism is defined as \"a concern for the welfare of others\". If altruism is genetically determined, then altruistic individuals must reproduce their own altruistic genetic traits for altruism to survive, but when altruists lavish their resources on non-altruists at the expense of their own kind, the altruists tend to die out and the others tend to increase. An extreme example is a soldier losing his life trying to help a fellow soldier. This example raises the question of how altruistic genes can be passed on if this soldier dies without having any children.\n\nWithin sociobiology, a social behavior is first explained as a sociobiological hypothesis by finding an evolutionarily stable strategy that matches the observed behavior. Stability of a strategy can be difficult to prove, but usually, it will predict gene frequencies. The hypothesis can be supported by establishing a correlation between the gene frequencies predicted by the strategy, and those expressed in a population.\n\nAltruism between social insects and littermates has been explained in such a way. Altruistic behavior, behavior that increases the reproductive fitness of others at the apparent expense of the altruist, in some animals has been correlated to the degree of genome shared between altruistic individuals. A quantitative description of infanticide by male harem-mating animals when the alpha male is displaced as well as rodent female infanticide and fetal resorption are active areas of study. In general, females with more bearing opportunities may value offspring less, and may also arrange bearing opportunities to maximize the food and protection from mates.\n\nAn important concept in sociobiology is that temperament traits exist in an ecological balance. Just as an expansion of a sheep population might encourage the expansion of a wolf population, an expansion of altruistic traits within a gene pool may also encourage increasing numbers of individuals with dependent traits.\n\nStudies of human behavior genetics have generally found behavioral traits such as creativity, extroversion, aggressiveness, and IQ have high heritability. The researchers who carry out those studies are careful to point out that heritability does not constrain the influence that environmental or cultural factors may have on those traits.\n\nCriminality is actively under study, but extremely controversial. There are arguments that in some environments criminal behavior might be adaptive. The novelist Elias Canetti also has noted applications of sociobiological theory to cultural practices such as slavery and autocracy.\n\nGenetic mouse mutants illustrate the power that genes exert on behaviour. For example, the transcription factor FEV (aka Pet1), through its role in maintaining the serotonergic system in the brain, is required for normal aggressive and anxiety-like behavior. Thus, when FEV is genetically deleted from the mouse genome, male mice will instantly attack other males, whereas their wild-type counterparts take significantly longer to initiate violent behaviour. In addition, FEV has been shown to be required for correct maternal behaviour in mice, such that offspring of mothers without the FEV factor do not survive unless cross-fostered to other wild-type female mice.\n\nA genetic basis for instinctive behavioural traits among non-human species, such as in the above example, is commonly accepted among many biologists; however, attempting to use a genetic basis to explain complex behaviours in human societies has remained extremely controversial.\n\nSteven Pinker argues that critics have been overly swayed by politics and a fear of biological determinism, accusing among others Stephen Jay Gould and Richard Lewontin of being \"radical scientists\", whose stance on human nature is influenced by politics rather than science, while Lewontin, Steven Rose and Leon Kamin who drew a distinction between the politics and history of an idea and its scientific validity argue that sociobiology fails on scientific grounds. Gould grouped sociobiology with eugenics, criticizing both in his book \"The Mismeasure of Man\".\n\nNoam Chomsky has expressed views on sociobiology on several occasions. During a 1976 meeting of the Sociobiology Study Group, as reported by Ullica Segerstråle, Chomsky argued for the importance of a sociobiologically informed notion of human nature. Chomsky argued that human beings are biological organisms and ought to be studied as such, with his criticism of the \"blank slate\" doctrine in the social sciences (which would inspire a great deal of Steven Pinker's and others' work in evolutionary psychology), in his 1975 \"Reflections on Language\". Chomsky further hinted at the possible reconciliation of his anarchist political views and sociobiology in a discussion of Peter Kropotkin's \"\", which focused more on altruism than aggression, suggesting that anarchist societies were feasible because of an innate human tendency to cooperate.\n\nWilson has claimed that he had never meant to imply what \"ought\" to be, only what \"is\" the case. However, some critics have argued that the language of sociobiology readily slips from \"is\" to \"ought\", an instance of the naturalistic fallacy. Pinker has argued that opposition to stances considered anti-social, such as ethnic nepotism, is based on moral assumptions, meaning that such opposition is not falsifiable by scientific advances. The history of this debate, and others related to it, are covered in detail by , , and .\n\n\n\n\n",
    "id": "27919",
    "title": "Sociobiology"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=11461412",
    "text": "I-cell\n\nI-cells also called inclusion cells are abnormal fibroblasts having a large number of dark inclusions in the cytoplasm of the cell (mainly in the central area). The inclusions are of various fats, proteins, carbohydrates, pigments and other insolubles.They are seen in mucolipidosis II.\n",
    "id": "11461412",
    "title": "I-cell"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=48483838",
    "text": "Encapsulin nanocompartment\n\nEncapsulin nanocompartments, or encapsulin protein cages, are spherical bacterial organelles roughly 25-30 nm in diameter that are involved in various aspects of metabolism, in particular protecting bacteria from oxidative stress. Encapsulin nanocompartments are structurally similar to the HK97 bacteriophage and their function depends on the proteins loaded into the nanocompartment. The sphere is formed from 60 (for a 25 nm sphere) or 180 (for a 30 nm sphere) copies of a single protomer, termed encapsulin. Their structure has been studied in great detail using X-ray crystallography and cryo-electron microscopy.\n\nA number of different types of proteins have been identified as being loaded into encapsulin nanocompartments. Peroxidases or proteins similar to ferritins are the two most common types of cargo proteins. While most encapsulin nanocompartments contain only one type of cargo protein, in some species two or three types of cargo proteins are loaded. \n\nEncapsulins purified from \"Rhodococcus jostii\" can be assembled and disassembled with changes in pH. In the assembled state, the compartment enhances the activity of its cargo, a peroxidase enzyme.\n\nRecently, encapsulin nanocompartments have begun to receive considerable interest from bioengineers because of their potential to allow the targeted delivery of drugs, proteins, and mRNAs to specific cells of interest.\n",
    "id": "48483838",
    "title": "Encapsulin nanocompartment"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=19364849",
    "text": "Encapsulin\n\nThe encapsulins are a family of bacterial proteins that serve as the main structural components of encapsulin nanocompartments. There are several different encapsulin proteins, including EncA, which forms the shell, and EncB, EncC, and EncD, which form the core.\n",
    "id": "19364849",
    "title": "Encapsulin"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=48264022",
    "text": "Pursuit predation\n\nPursuit predation is a form of predation in which predators give chase to fleeing prey. The chase can be initiated either by the predator or by the prey, should the prey be alerted to a predator's presence and attempt to flee before the predator gives chase. The chase ends with either the predator capturing and consuming the prey, effectively diminishing the prey's fitness, or with the prey escaping the predator's hunt, thus maintaining the prey's overall fitness, but leaving both prey and predator with metabolic losses. Pursuit predation is typically observed in carnivorous species within the kingdom Animalia, with some iconic examples being cheetahs, lions, and wolves.\n\nPursuit predation is an alternate predation strategy to ambush predation. While pursuit predators use a detection and pursuit phase in order to obtain prey, ambush predators use stealth to capture prey. Strength and speed are important to pursuit predators, whereas ambush predators ignore these in favor of surprise from a typically concealed location. While the two patterns of predation are not mutually exclusive, morphological differences in body plan can create a bias in an organism towards each type of predation.\n\nThere is still uncertainty as to whether predators behave with a general tactic or strategy while preying. However, among pursuit predators, there are several common behaviors. Often, predators will scout potential prey, assessing prey quantity and density prior to engaging in a pursuit. Certain predators choose to pursue prey primarily in a group of conspecifics; such animals are known as pack hunters or group pursuers. Other species choose to hunt alone. These two behaviors are typically due to differences in hunting success, where some groups are very successful in groups and others are more successful alone. Pursuit predators may also choose to either exhaust their metabolic resources rapidly or pace themselves during a chase. This choice can be influenced by prey species, seasonal settings, or temporal settings. Predators that rapidly exhaust their metabolic resources during a chase tend to first stalk their prey, slowly approaching their prey to decrease chase distance and time. When the predator is at a closer distance (one that would lead to easier prey capture), it finally gives chase. Pacing pursuit is more commonly seen in group pursuit, as individual animals do not need to exert as much energy to capture prey. However, this type of pursuit requires group coordination, which may have varying degrees of success. Since groups can engage in longer chases, they often focus on separating a weaker or slower prey item during pursuit. Morphologically speaking, while ambush predation requires stealth, pursuit predation requires speed; pursuit predators are proportionally long-limbed and equipped with cursorial adaptations. Current theories suggest that this proportionally long-limbed approach to body plan was an evolutionary countermeasure to prey adaptation.\n\nGroup pursuers hunt with a collection of conspecifics. Group pursuit is usually seen in species of relatively high sociality; in vertebrates, individuals often seem to have defined roles in pursuit.\n\nAfrican wild dog (\"Lycaon pictus\") packs have been known to split into several smaller groups while in pursuit; one group initiates the chase, while the other travels ahead of the prey's escape path. The group of chase initiators coordinate their chase to lead the prey towards the location of the second group, where the prey's escape path will be effectively cut off. Bottlenose dolphins (\"Tursiops\") have been shown exhibiting similar behaviors of pursuit role specialization. One group within the dolphin pod, known as the drivers, give chase to the fish - forcing the fish into a tight circle formation, while the other group of the pod, the barriers, approach the fish from the opposite direction. This two-pronged attack leaves the fish with only the option of jumping out of the water to escape the dolphins. However, the fish are completely vulnerable in the air; it is at this point when the dolphins leap out and catch the fish. In lion (\"Panthera leo\") pack hunting, each member of the hunting group is assigned a position, from left wing to right wing, in order to better obtain prey. Such specializations in roles within the group are thought to increase sophistication in technique; lion wing members are faster, and will drive prey toward the center where the larger, stronger, killing members of the pride will take down the prey. Many observations of group pursuers note an optimal hunting size in which certain currencies (mass of prey killed or number of prey killed) are maximized with respect to costs (kilometers covered or injuries sustained). Groups size is often dependent on aspects of the environment: number of prey, prey density, number of competitors, seasonal changes, etc.\n\nWhile birds are generally believed to be individual hunters, there are a few examples of birds that cooperate during pursuits. Harris's hawks (\"Parabuteo unicinctus\") have two cooperative strategies for hunting. One strategy involves a group of hawks surrounding prey hidden under some form of cover, while another hawk attempts to penetrate the prey's cover. The penetration attempt flushes the prey out from its cover where it is swiftly killed by one of the surrounding hawks. The second strategy is less commonly used. It involves a \"relay attack\" in which a group of hawks, led by a \"lead\" hawk, engage in a long chase for prey. The \"lead\" hawk will dive in order to kill the prey. If the dive is unsuccessful, the role of the \"lead\" shifts to another hawk who will then dive in another attempt to kill the prey. During one observed relay attack, 20 dives and hence 20 lead switches were exhibited.\n\n\nAs in vertebrates, there are many species of invertebrates which actively pursue prey in groups and exhibit task specialization, but while the vertebrates change their behavior based on their role in hunting, invertebrate task delegation is usually based on actual morphological differences. The vast majority of eusocial insects have castes within a population which tend to differ in size and have specialized structures for different tasks. This differentiation is taken to the extreme in the groups isoptera and hymenoptera, or termites and ants, bees, and wasps respectively.\n\nTermites are detritivores and therefore do not exhibit pursuit behaviors, however ants and wasps can be highly predatory and are known to pursue prey in groups. For example, termite-hunting ants of the genus \"Pachycondyla\", also known as Matabele ants, form raiding parties consisting of ants of different castes, such as soldier ants and worker ants. Soldier ants are much larger than worker ants, with more powerful mandibles and more robust exoskeletons, and so they make up the front lines of raiding parties and are responsible for killing prey. Workers usually butcher and carry off the killed prey, while supporting the soldiers. The raiding parties are highly mobile and move aggressively into the colonies of termites, often breaking through their outer defenses and entering their mounds. The ants do not completely empty the mound of termites, instead they only take a few, allowing the termites to recover their numbers so that the ants have a steady stream of prey.\n\nAsian giant hornets, \"Vespa mandarinia\", form similar raiding parties to hunt their prey, which usually consists of honeybees. The giant hornets group together and as a team can decimate an entire honeybee colony, especially those of non-native European honeybees. Alone, the hornets are subject to attack by the smaller bees, who swarm the hornet and vibrate their abdomens to generate heat, collectively cooking the hornet until it dies. By hunting in groups, the hornets avoid this problem.\n\nWhile most big cat species are individual, ambush predators, Cheetahs (\"Acinonyx jubatus\") are pursuit predators. Widely known as the fastest terrestrial animal, with speeds reaching 61–64 miles per hour, cheetahs take advantage of their speed during chases. However, their speed and acceleration also have disadvantages, as both can only be sustained for short periods of time. Studies show that cheetahs can maintain maximum speed for a distance of approximately 500 yards. Due to these limitations, cheetahs are often observed running at moderate speeds during chases. There are claims that the key to cheetahs' pursuit success may not be just their speed. Cheetahs are extremely agile, able to maneuver and change directions at very high speeds in very short amounts of time. This extensive maneuverability can make up for unsustainable high speed pursuit, as it allows cheetahs to quickly close the distance between prey without decreasing their speed when prey change direction.\n\nThe Painted redstart (\"Myioborus pictus\") is one of the most well documented flush pursuers. When flies, prey for redstarts, are alerted of the presence of predators, they respond by fleeing. Redstarts take advantage of this anti-predator response by spreading and orienting their easily noticeable wings and tails, alerting the flies, but only when they are in a position where the flies' escape path intersects with the redstart's central field of vision. When prey's path are in this field of vision, the redstart's prey capture rate is at it maximum. Once the flies begin to flee, the redstart begins to chase. It has been proposed that redstarts exploit two aspects of the visual sensitivity of their prey: sensitivity to the location of the stimulus in the prey's visual field and sensitivity to the direction of stimulus environment. The effectiveness of this pursuit can also be explained by \"rare enemy effect\", an evolutionary consequence of multi-species predator-prey interactions.\n\nDragonflies are skilled aerial pursuers; they have a 97% success rate for prey capture. This success rate is a consequence of the \"decision\" of which prey to pursue based on initial conditions. Observations of several species of perching dragonflies show more pursuit initiations at larger starting distances for larger size prey species than for much smaller prey. Further evidence points to a potential bias towards larger prey, due to more substantial metabolic rewards. This bias is in spite of the fact that larger prey are typically faster and choosing them results in less successful pursuits. Dragonflies high success rate for prey capture may also be due to their \"interception\" foraging method. Unlike tracking foraging methods, in which predators focus on closing in on the current position of their prey, the interception method has the dragonfly seeking the position directly ahead of their prey as a way of surmising a prey's future location. Perching dragonflies (Libellulidae family), the largest family of dragonflies, have been observed \"staking out\" high density prey spots prior to pursuit. There are no noticeable distinctions in prey capture efficiency between male and female dragonflies. Further, percher dragonflies are bound by their visual range. They are more likely to engage in pursuit when prey come within a subtended angle of around 1-2 degrees. Angles greater than this are outside of a dragonflies visual range.\n\nCurrent theory on the evolution of pursuit predation suggests that the behavior is an evolutionary countermeasure to prey adaptation. Prey animals vary in their likelihood to avoid predation, and it is predation failure that drives evolution of both prey and predator. Predation failure rates vary wildly across the animal kingdom; raptorial birds can fail anywhere from 20% to 80% of the time in predation, while predatory mammals usually fail more than half the time. Prey adaptation drives these low rates in three phases: the detection phase, the pursuit phase, and the resistance phase. The pursuit phase drove the evolution of distinct behaviors for pursuit predation. \nAs selective pressure on prey is higher than on predators adaptation usually occurs in prey long before the reciprocal adaptations in predators. Evidence in the fossil record supports this, with no evidence of modern pursuit predators until the late Tertiary period. Certain adaptations, like long limbs in ungulates, that were thought to be adaptive for speed against predatory behavior have been found to predate predatory animals by over 20 million years. Because of this, modern pursuit predation is an adaptation that may have evolved separately and much later as a need for more energy in colder and more arid climates. Longer limbs in predators, the key morphological adaptation required for lengthy pursuit of prey, is tied in the fossil record to the late Tertiary. It is now believed that modern pursuit predators like the wolf and lion evolved this behavior around this time period as a response to ungulates increasing feeding range. As ungulate prey moved into a wider feeding range to discover food in response to changing climate, predators evolved the longer limbs and behavior necessary to pursue prey across larger ranges. In this respect, pursuit predation is not co-evolutionary with prey adaptation, but a direct response to prey. Prey's adaptation to climate is the key formative reason for evolving the behavior and morphological necessities of pursuit predation.\n\nIn addition to serving as a countermeasure to prey adaptation, pursuit predation has evolved in some species as an alternative, facultative mechanism for foraging. For example, polar bears typically act as specialized predators of seal pups and operate in a manner closely predicted by the optimal foraging theory. However, they have been seen to occasionally employ more energy-inefficient pursuit predation tactics on flightless geese. This alternative predatory strategy may serve as a back-up resource when optimal foraging is circumstantially impossible, or may even be a function of filling dietary needs.\n\nPursuit predation revolves around a distinct movement interaction between predator and prey; as prey move to find new foraging areas, predators should move with them. Predators congregate in areas of high prey density, and prey should therefore avoid these areas. However, dilution factor may be a reason to stay in areas of high density due to a decreased risk of predation. Given the movements of predators over ranges in pursuit predation, though, dilution factor seems a less important cause for predation avoidance. Because of these interactions, spatial patterns of predators and prey are important in preserving population size. Attempts by prey to avoid predation and find food are coupled with predator attempts to hunt and compete with other predators. These interactions act to preserve populations. Models of spatial patterns and synchrony of predator-prey relationships can be used as support for the evolution of pursuit predation as one mechanism to preserve these population mechanics. By pursuing prey over long distances, predators actually improve longterm survival of both their own population and prey population through population synchrony. Pursuit predation acts to even out population fluctuations by moving predatory animals from areas of high predator density to low predator density, and low prey density to high prey density. This keeps migratory populations in synchrony, which increases metapopulation persistence. Pursuit predation’s effect on population persistence is more marked over larger travel ranges. Predator and prey levels are usually more synchronous in predation over larger ranges, as population densities have more ability to even out. Pursuit predation can then be supported as an adaptive mechanism for not just individual feeding success but also metapopulation persistence.\n\nJust as the evolutionary arms race has led to the development of pursuit behavior of predators, so too has it led to the anti-predator adaptations of prey. Alarm displays such as eastern swamphen's tail flicking, white-tailed deer's tail flagging, and Thomson's gazelles' stotting have been observed deterring pursuit. These tactic are believed to signal that a predator's presence is known and, therefore, pursuit will be much more difficult. These displays are more frequent when predators are at an intermediate distance away. Alarm displays are used more often when prey believe predators are more prone to change their decision to pursue. For instance, cheetahs, common predators of Thomson's gazelles, are less likely to change their choice to pursue. As such, gazelles stott less when cheetahs are present than when other predators are present. In addition to behavioral adaptations, there are also morphological anti-predator adaptations to pursuit predators. For example, many birds have evolved rump feathers that fall off with much less force than the feathers of their other body parts. This allows for easier escape from predator birds, as avian predators often approach prey from their rump.\n\nIn many species that fall prey to pursuit predation, gregariousness on a massive scale has evolved as a protective behavior and can be conspecific or heterospecific within a given group. This is primarily due to the Confusion Effect, which states that if prey animals congregate in large groups, predators will have more difficulty identifying and tracking specific individuals. This effect has greater influence when individuals are visually similar and less distinguishable. In groups where individuals are visually similar, there is a negative correlation between group size and predator success rates. This may mean that the overall number of attacks decreases with larger group size or that the number of attacks per kill increases with larger group size. This is especially true in open habitats, such as grasslands or open ocean ecosystems, where view of the prey group is unobstructed, in contrast to a forest or reef. Prey species in these open environments tend to be especially gregarious, with notable examples being starlings and sardines. When individuals of the herd are visually dissimilar, however, the success rate of predators increases dramatically. In one study, wildebeest on the African Savannah were selected at random and had their horns painted white. This introduced a distinction, or oddity, into the population, and researchers found that wildebeest who had white horns were preyed upon at substantially higher rates. By standing out, individuals are not as easily lost in the crowd, and so predators are able to track and pursue them with higher fidelity. This has been proposed as the reason why many schooling fish show little to no sexual dimorphism, and why many species in heterospecific schools bear a close resemblance to other species in their school.\n",
    "id": "48264022",
    "title": "Pursuit predation"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=48444540",
    "text": "Biconcave disc\n\nA biconcave disc is a geometric shape resembling an oblate spheroid with two concavities on the top and on the bottom.\n\nErythrocytes are in the shape of a biconcave disc. It has been suggested that this shape is optimal for surface area to volume ratio, but this hypothesis has not been widely accepted. In addition, it has been suggested that the biconcave shape is needed to optimise the blood's flow properties.\n",
    "id": "48444540",
    "title": "Biconcave disc"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=48756720",
    "text": "Tachyaerobic\n\nTachyaerobic is a term used in biology to describe the muscles of large animals and birds that are able to maintain high levels or physical activity because their hearts make up at least 0.5-0.6 percent of their body mass and maintain high blood pressures. A reptile displaying equal size to a tachyaerobic mammal does not have the same capabilities. Tachyaerobic animal's hearts beat more quickly, produce more oxygen, and distribute blood at a quicker rate than reptiles.\n\nThe use of tachyaerobic muscles is important to animals such as giraffes that need blood circulated through a large body size quickly.\n\n",
    "id": "48756720",
    "title": "Tachyaerobic"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=27492898",
    "text": "Betaine transporter\n\nProteins of the Betaine/Carnitine/Choline Transporter (BCCT) family (TC# 2.A.15) are found in Gram-negative and Gram-positive bacteria and archaea. The BCCT family is a member a large group of secondary transporters, the APC superfamily. Their common functional feature is that they all transport molecules with a quaternary ammonium group [R-N (CH)]. The BCCT family proteins vary in length between 481 and 706 amino acyl residues and possess 12 putative transmembrane α-helical spanners (TMSs). The x-ray structures reveal two 5 TMS repeats with the total number of TMSs being 10. These porters catalyze bidirectional uniport or are energized by pmf-driven or smf-driven proton or sodium ion symport, respectively, or else by substrate:substrate antiport. Some of these permeases exhibit osmosensory and osmoregulatory properties inherent to their polypeptide chains.\n\nSchulze et al. (2010) reported the structures of the sodium-independent carnitine/butyrobetaine antiporter CaiT from \"Proteus mirabilis\" (PmCaiT) at 2.3 Å (, ) and from \"E. coli\" (EcCaiT) at 3.5 Å resolution (, ).\n\nMost members of the BCCT family are Na- or H-dependent, whereas EcCaiT is a Na- and H-independent substrate:product antiporter. The three-dimensional architecture of CaiT resembles that of the Na-dependent transporters LeuT and BetP, but in CaiT, a methionine sulphur takes the place of the Na to coordinate the substrate in the central transport site, accounting for Na independence. Both CaiT structures (, ) show the fully open, inward-facing conformation, and thus complete the set of functional states that describe the alternating access mechanism. EcCaiT (, ) contains two bound butyrobetaine substrate molecules, one in the central transport site, the other in an extracellular binding pocket. In the structure of PmCaiT, a tryptophan side chain occupies the transport site, and access to the extracellular site is blocked. Binding of both substrates to CaiT reconstituted into proteoliposomes is cooperative, with Hill coefficients of up to 1.7, indicating that the extracellular site is regulatory. Schulze et al. (2010) proposed a mechanism whereby the occupied regulatory site increases the binding affinity of the transport site and initiates substrate translocation. Glycine betaine transporters have been found to contain a conserved region with four tryptophans in their central region.\n\nMost secondary-active transporters transport their substrates using an electrochemical ion gradient, but the carnitine transporter (CaiT) is an ion-independent, L-carnitine/gamma-butyrobetaine antiporter. Crystal structures of CaiT from \"E. coli\" and \"Proteus mirabilis\" revealed the inverted five-transmembrane-helix repeat similar to that in the amino acid/Na symporter, LeuT. Kalayil et al. (2013) showed that mutations of arginine 262 (R262) made CaiT Na-dependent with increased transport activity in the presence of a membrane potential, in agreement with substrate/Na cotransport. R262 also plays a role in substrate binding by stabilizing the partly unwound TM1' helix.\n\nModeling CaiT from \"P. mirabilis\" in the outward-open and closed states on the corresponding structures of the related symporter BetP revealed alternating orientations of the buried R262 side chain, which mimic sodium binding and unbinding in the Na-coupled substrate symporters. A similar mechanism may be operative in other Na/H-independent transporters, in which a positively charged amino acid replaces the cotransported cation. The oscillation of the R262 side chain in CaiT indicates how a positive charge triggers the change between outward-open and inward-open conformations.\n\nThe generalized transport reactions catalyzed by members of the BCCT family are:\n\nThe mammalian betaine transporter (BGT1; SLC6A12) is predominantly expressed in the liver (hepatocytes). It is also expressed in the kidney where it is regulated by NFAT5 during a response to osmotic stress. Further, BGT1 is also present in the leptomeninges surrounding the brain. Deletion of the BGT1 gene in mice did not appear to have any impact on the tendency to develop epilepsy. This is to be expected considering that BGT1 is expressed at far lower levels than GAT1 and also has lower affinity for GABA. This implies that it is not likely to contribute significantly to the inactivation of the inhibitory neurotransmitter GABA.\n\n",
    "id": "27492898",
    "title": "Betaine transporter"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=49159269",
    "text": "APC Family\n\nThe Amino Acid-Polyamine-Organocation (APC) Family (TC# 2.A.3) of transport proteins includes members that function as solute:cation symporters and solute:solute antiporters. They occur in bacteria, archaea, fungi, unicellular eukaryotic protists, slime molds, plants and animals. They vary in length, being as small as 350 residues and as large as 850 residues. The smaller proteins are generally of prokaryotic origin while the larger ones are of eukaryotic origin. Most of them possess twelve transmembrane α-helical spanners but have a re-entrant loop involving TMSs 2 and 3. The APC Superfamily was established to encompass a wider range of homologues. \n\nMembers of one subfamily within the APC family (SGP; TC# 2.A.3.9) are amino acid receptors rather than transporters and are truncated at their C-termini, relative to the transporters, having 10 TMSs.\n\nThe eukaryotic members of another subfamily (CAT; TC# 2.A.3.3) and the members of a prokaryotic subfamily (AGT; TC #2.A.3.11) have 14 TMSs.\n\nThe larger eukaryotic and archaeal proteins possess N- and C-terminal hydrophilic extensions. Some animal proteins, for example, those in the LAT subfamily (TC# 2.A.3.8) including ASUR4 (gbY12716) and SPRM1 (gbL25068) associate with a type 1 transmembrane glycoprotein that is essential for insertion or activity of the permease and forms a disulfide bridge with it. These glycoproteins include the CD98 heavy chain protein of \"Mus musculus (gbU25708)\" and the orthologous 4F2 cell surface antigen heavy chain of \"Homo sapiens (spP08195).\" The latter protein is required for the activity of the cystine/glutamate antiporter (2.A.3.8.5), which maintains cellular redox balance and cysteine/glutathione levels. They are members of the rBAT family of mammalian proteins (TC #8.A.9).\n\nTwo APC family members, LAT1 and LAT2 (TC #2.A.3.8.7), transport a neurotoxicant, the methylmercury-L-cysteine complex, by molecular mimicry.\n\nHip1 of \"S. cerevisiae\" (TC #2.A.3.1.5) has been implicated in heavy metal transport.\n\nSubfamilies of the APC family, and the proteins in these families, can be found in the Transporter Classification Database:\n\n\nBased on 3-D structures of APC superfamily members, Rudnick (2011) has proposed the pathway for transport and suggested a \"\"rocking bundle\"\" mechanism.\n\nTransport reactions generally catalyzed by APC Superfamily members include:\n\n",
    "id": "49159269",
    "title": "APC Family"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=49158055",
    "text": "Sulfide intrusion\n\nIn biology, sulfide intrusion refers to an excess of sulfide molecules in the soil interfering with plant growth, often seagrass.\n\nSeagrass bed sediment (soil) is typically anoxic, containing a reduced form of sulfur: hydrogen sulfide (H2S). H2S is a phytotoxin that results from anaerobic digestion, the decomposition of organic matter in the absence of oxygen. However, seagrass can persist in this environment because of physiological adaptations, as well as functional adaptations of other organisms in the ecosystem. For example, bivalves (clams) in the family Lucinidae host symbiotic bacteria that oxidize sulfides. Lucinid bivalves' gills house the bacteria, and the siphon supplies the bacteria and surrounding pore water with oxygenated water from above the sediment. Bacterial oxidation of the sulfides results in sulfates, reducing toxicity.\n\n",
    "id": "49158055",
    "title": "Sulfide intrusion"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=49033936",
    "text": "TSUP family\n\nThe 4-Toluene Sulfonate Uptake Permease (TSUP) family (TC# 2.A.102) is also referred to as the TauE/SafE/YfcA/DUF81 Family. Although its members have not been rigorously characterized, evidence is available that at least some members function in the transport of sulfur containing organic compounds. These include 4-toluene sulfonate which may be transported by the TsaS of \"Cupriavidus necator\" (TC# 2.A.102.1.1), sulfolactate which may be exported by the TauE protein of \"Cupriavidus necator\" (TC# 2.A.102.2.1) and sulfoacetate which may be exported by the SafE1 protein of \"Neptuniibacter caesariensis\" (TC# 2.A.102.2.2). Another member of the TSUP family, TsaS of \"Comamonas testosteroni\", has been reported to function in the uptake of 4-toluene sulfonate. None of these functional assignments can be considered to be certain.\n\n",
    "id": "49033936",
    "title": "TSUP family"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=49162176",
    "text": "AGCS family\n\nMembers of the Alanine or Glycine:Cation Symporter (AGCS) Family (TC# 2.A.25) transport alanine and/or glycine in symport with Na and or H.\n\nKnown proteins in the AGCS family are between 445 and 550 amino acyl residues in length and possess 8 to 12 putative transmembrane α-helical spanners. Members may possess 11 transmembrane segments (TMSs), as seems to be true for DagA (TC# 2.A.25.1.1) and AgcS (TC# 2.A.25.1.3), although Acp (TC# 2.A.25.1.2) has only 8 TMSs, perhaps the result of truncation. As of early 2016, there does not appear to be any 3D crystal structure data available for these proteins. Members of the AGCS family have been found in bacteria and archaea, such as extremophile halotolerant cyanobacterium \"Aphanothece halophytica,\" and thermophilic bacteria, \"Bacillus\" \"PS3\". As of 2015, only three members of the family have been functionally characterized. These proteins show limited sequence similarity in the APC family (TC# 2.A.3). As of early 2016, there do not appear to be any crystal structures available for members of the AGCS family.\n\nThe generalized transport reaction catalyzed by the AGCS family is:\n\nalanine or glycine (out) + Na or H (out) → alanine or glycine (in) + Na or H (in).\n\nThere are currently 10 proteins belonging to the AGCS family. These proteins and their descriptions can be found in the Transporter Classification Database.\n",
    "id": "49162176",
    "title": "AGCS family"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=35039952",
    "text": "Nucleobase cation symporter-2\n\nThe Nucleobase:Cation Symporter-2 (NCS2) Family, also called the Nucleobase/Ascorbate Transporter (NAT) Family, consists of over 1000 sequenced proteins derived from gram-negative and gram-positive bacteria, archaea, fungi, plants and animals. The NCS2/NAT family is a member of the APC Superfamily of secondary carriers. Of the five known families of transporters that act on nucleobases, NCS2/NAT is the only one that is most widespread. Many functionally characterized members are specific for nucleobases including both purines and pyrimidines, but others are purine-specific. However, two closely related rat/human members of the family, SVCT1 and SVCT2, localized to different tissues of the body, co-transport L-ascorbate (vitamin C) and Na with a high degree of specificity and high affinity for the vitamin. Clustering of NCS2/NAT family members on the phylogenetic tree is complex, with bacterial proteins and eukaryotic proteins each falling into at least three distinct clusters. The plant and animal proteins cluster loosely together, but the fungal proteins branch from one of the three bacterial clusters forming a tighter grouping. \"E. coli\" possesses four distantly related paralogous members of the NCS2 family.\n\nProteins of the NCS2 family are 414–650 amino acyl residues in length and probably possess 14 TMSs. Lu et al. (2011) have concluded from x-ray crystallography that UraA (2.A.40.1.1) has 14 TMSs with two 7 TMS inverted repeats. Uracil is located at the interface between the two domains.\n\nUracil permease, UraA UraA with bound uracil at 2.8Å resolution .\n\nThe generalized transport reactions catalyzed by proteins of the NAT/NCS2 family are:\n\nSeveral proteins make up the NCS2/NAT family. A full list of these proteins can be found in the Transporter Classification Database. \nA few types of proteins that make up the NCS2/NAT family include:\n\n",
    "id": "35039952",
    "title": "Nucleobase cation symporter-2"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=49219918",
    "text": "HAAAP family\n\nThe Hydroxy/Aromatic Amino Acid Permease (HAAAP) Family (TC# 2.A.42) is a member of the large Amino Acid-Polyamine-OrganoCation (APC) Superfamily of secondary carriers. Members of the HAAAP family all function in amino acid uptake. Homologues are present in a large number of Gram-negative and Gram-positive bacteria, with at least one member classified from archaea (TC# 2.A.42.1.7, \"Thermococcus barophilus\")\".\"\n\nProteins of the HAAAP family possess 403-443 amino acyl residues and exhibit eleven putative or established transmembrane α-helical spanners (TMSs). These proteins exhibit topological features common to the eukaryotic amino acid/auxin permease (AAAP) family (TC# 2.A.18). These proteins also exhibit limited sequence similarity with some of the AAAP family members. A phylogenetic relationship has been proposed between members of the HAAAP family and APC family since they exhibit limited sequence similarity with one another.\n\nAs of early 2016, there is no crystal structural data available for members of the HAAAP family in RCSB.\n\nThe HAAAP family includes three well-characterized aromatic amino acid:H symport permeases of \"E. coli\": a high affinity tryptophan-specific permease, Mtr (TC# 2.A.42.1.2), a low affinity tryptophan permease, TnaB (TC# 2.A.42.1.3), and a tyrosine-specific permease, TyrP (TC# 2.A.42.1.1), as well as two well-characterized hydroxy amino acid permeases, the serine permease, SdaC (TC# 2.A.42.2.1), of \"E. coli\", and the threonine permease, TdcC (TC# 2.A.42.2.2), of \"E. coli\". \n\nSdaC of \"E. coli\" (TC# 2.A.42.2.1) is also called DcrA, and together with a periplasmic protein DcrB (P37620), has been reported to play a role in phage DNA uptake in conjunction with an outer membrane receptor of the OMR family (TC# 1.B.14). Thus, FhuA (TC #1.B.14.1.4) transports phage T5 DNA while BtuB (TC# 1.B.14.3.1) transports phage C1 DNA (Samsonov et al., 2002). DcuB is a putative lipoprotein found only in enteric bacteria. \n\nAll members of the HAAAP family can be found in the Transporter Classification Database.\n\nThe generalized transport reaction catalyzed by proteins of the HAAAP family is:\n\nAmino acid (out) + nH (out) → Amino acid (in) + nH (in).\n",
    "id": "49219918",
    "title": "HAAAP family"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=27493016",
    "text": "Nucleobase cation symporter-1\n\nThe Nucleobase:Cation Symporter-1 (NCS1) Family (TC# 2.A.39) consists of over 1000 currently sequenced proteins derived from Gram-negative and Gram-positive bacteria, archaea, fungi and plants. These proteins function as transporters for nucleobases including purines and pyrimidines. Members of this family possess twelve transmembrane α-helical spanners (TMSs). At least some of them have been shown to function in uptake by substrate:H symport mechanism.\n\nThe bacterial and yeast proteins are widely divergent and do not cluster closely on the NCS1 family phylogenetic tree. \"B. subtilis\" possesses two paralogues of the NCS1 family, and \"S. cerevisiae\" has several. Two of the yeast proteins (Dal4 (TC# 2.A.39.3.1) and Fur4 (TC# 2.A.39.3.2)) cluster tightly together. Three other \"S. cerevisiae\" proteins, one of which is the thiamin permease, Thi10 (TC# 2.A.39.4.1), and another of which is the nicotinamide riboside transporter, Nrt1 (TC# 2.A.39.4.2), also cluster tightly together. The latter three proteins are likely to be closely related thiamin permease isoforms. The yeast cytosine-purine and vitamin B6 transporters cluster loosely together (24% identity; e-50). The bacterial proteins are derived from several Gram-negative and Gram-positive species. These proteins exhibit limited sequence similarity with the xanthine permease, PbuX (TC# 2.A.39.4.1), of \"Bacillus subtilis\" which is a member of the NCS2 family.\n\nProteins of the NCS1 family are 419-635 amino acyl residues long and possess twelve putative transmembrane α-helical spanners (TMSs). At least some of them have been shown to function in uptake by substrate:H symport. In these respects, and with respect to substrate specificity, these proteins resemble the symporters of the NCS2 family, providing further evidence that the two families represent distant constituents of a single superfamily, the APC Superfamily. The two families probably arose by an early gene duplication event that occurred long before divergence of the three major kingdoms of life. It is possible that they are distant constituents of the MFS (2.A.1).\n\nThe nucleobase-cation-symport-1 (NCS1) transporters are essential components of salvage pathways for nucleobases and related metabolites. Weyand et al. (2008) reported the 2.85-angstrom resolution structure of the NCS1 benzyl-hydantoin transporter, Mhp1 (TC# 2.A.39.3.6), from \"Microbacterium liquefaciens\". This structure (and related structures) are available through RCSB (, , , , , ). Mhp1 contains 12 transmembrane helices, 10 of which are arranged in two inverted repeats of five helices. The structures of the outward-facing open and substrate-bound occluded conformations were solved, showing how the outward-facing cavity closes upon binding of substrate. Comparisons with the leucine transporter LeuT(Aa) and the galactose transporter vSGLT reveal that the outward- and inward-facing cavities are symmetrically arranged on opposite sides of the membrane. The reciprocal opening and closing of these cavities is synchronized by the inverted repeat helices 3 and 8, providing the structural basis of the alternating access model for membrane transport.\n\nNCS1 proteins are H/Na symporters specific for the uptake of purines, pyrimidines and related metabolites. Krypotou et al. 2015 studied the origin, diversification and substrate specificities of fungal NCS1 transporters, suggesting that the two fungal NCS1 subfamilies, Fur and Fcy, and plant homologues, originated through independent horizontal transfers from prokaryotes. Expansion by gene duplication led to functional diversification of fungal NCS1 porters. They characterized all Fur proteins in Aspergillus nidulans. Homology modelling, substrate docking, molecular dynamics and systematic mutational analysis in three Fur transporters with distinct specificities identified residues critical for function and specificity, located within a major substrate binding site, in transmembrane segments TMS1, TMS3, TMS6 and TMS8. They predicted and confirmed that residues determining substrate specificity are located not only in the major substrate binding site, but also in a putative outward-facing selectivity gate. Their evolutionary and structure-function analyses led to the concept that selective channel-like gates may contribute to substrate specificity.\n\nThe generalized transport reaction catalyzed by NCS1 family permeases is:\n\n",
    "id": "27493016",
    "title": "Nucleobase cation symporter-1"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=48050134",
    "text": "Microbial rhodopsin\n\nMicrobial rhodopsin, also known as type-I rhodopsin, is a photochemically active membrane protein composed of seven transmembrane alpha-helices with a retinal chromophore. It includes light-driven proton pumps, ion pumps and ion channels, as well as light sensors. Microbial rhodopsins are found in Archaea, Bacteria, and Eukaryota, as well as in viruses; although it is rare in complex multicellular organisms.\n\nRhodopsin was originally a synonym for \"visual purple\", a visual pigment (light-sensitive molecule) found in the retinas of frogs and other vertebrates, used for dim-light vision, and usually found in rod cells. This is still the meaning of rhodopsin in the narrow sense, any protein evolutionarily homologous to this protein. In a broad non-genetic sense, rhodopsin refers to any molecule, whether related by genetic descent or not (mostly not), consisting of an opsin and a chromophore (generally a variant of retinal). All animal rhodopsins arose (by gene duplication and divergence) late in the history of the large GPCR [G-Protein Coupled Receptor] gene family, which itself arose after the divergence of plants, fungi, choanflagellates and sponges from the earliest animals. The retinal chromophore is found solely in the opsin branch of this large gene family, meaning its occurrence elsewhere represents convergent evolution, not homology.\n\nThe term bacterial rhodopsin originally referred to the first microbial rhodopsin discovered, known today as bacteriorhodopsin. The first bacteriorhodopsin turned out to be of archaeal origin, from \"Halobacterium salinarum\". Since then, other microbial rhodopsins have been discovered, rendering the term \"bacterial rhodopsin\" ambiguous.\n\nBelow is a list of some of the more well-known microbial rhodopsins and some of their properties.\n\nThe Ion-translocating Microbial Rhodopsin (MR) Family (TC# 3.E.1) is a member of the TOG Superfamily of secondary carriers. Members of the MR family catalyze light-driven ion translocation across microbial cytoplasmic membranes or serve as light receptors. Most proteins of the MR family are all of about the same size (250-350 amino acyl residues) and possess seven transmembrane helical spanners with their N-termini on the outside and their C-termini on the inside. There are 9 subfamilies in the MR family:\n\n(1) Bacteriorhodopsins pump protons out of the cell;\n\n(2) Halorhodopsins pump chloride (and other anions such as bromide, iodide and nitrate) into the cell;\n\n(3) Sensory rhodopsins, which normally function as receptors for phototactic behavior, are capable of pumping protons out of the cell if dissociated from their transducer proteins;\n\n(4) the Fungal Chaparones are stress-induced proteins of ill-defined biochemical function, but this subfamily also includes a H-pumping rhodopsin;\n\n(5) the bacterial rhodopsin, called Proteorhodopsin, is a light-driven proton pump that functions as does bacteriorhodopsins;\n\n(6) the \"Neurospora crassa\" retinal-containing receptor serves as a photoreceptor (Neurospora ospin I);\n\n(7) the green algal light-gated proton channel, Channelrhodpsin-1;\n\n(8) Sensory rhodopsins from cyanobacteria.\n\n(9) Light-activated rhodopsin/guanylyl cyclase\n\nA phylogenetic analysis of microbial rhodopsins and a detailed analysis of potential examples of horizontal gene transfer have been published.\n\nAmong the high resolution structures for members of the MR Family are the archaeal proteins, bacteriorhodopsin , sensory rhodopsin II, halorhodopsin as well as an \"Anabaena\" cyanobacterial sensory rhodopsin(TC# 3.E.1.1.6). and others.\n\nThe association of sensory rhodopsins with their transducer proteins appears to determine whether they function as transporters or receptors. Association of a sensory rhodopsin receptor with its transducer occurs via the transmembrane helical domains of the two interacting proteins. There are two sensory rhodopsins in any one halophilic archaeon, one (SRI) that responds positively to orange light but negatively to blue light, the other (SRII) that responds only negatively to blue light. Each transducer is specific for its cognate receptor. An x-ray structure of SRII complexed with its transducer (HtrII) at 1.94 Å resolution is available (). Molecular and evolutionary aspects of the light-signal transduction by microbial sensory receptors have been reviewed.\n\nHomologues include putative fungal chaperone proteins, a retinal-containing rhodopsin from \"Neurospora crassa\", a H-pumping rhodopsin from \"Leptosphaeria maculans\", retinal-containing proton pumps isolated from marine bacteria, a green light-activated photoreceptor in cyanobacteria that does not pump ions and interacts with a small (14 kDa) soluble transducer protein and light-gated H channels from the green alga, \"Chlamydomonas reinhardtii\". The \"N. crassa\" NOP-1 protein exhibits a photocycle and conserved H translocation residues that suggest that this putative photoreceptor is a slow H pump.\n\nMost of the MR family homologues in yeast and fungi are of about the same size and topology as the archaeal proteins (283-344 amino acyl residues; 7 putative transmembrane α-helical segments), but they are heat shock- and toxic solvent-induced proteins of unknown biochemical function. They have been suggested to function as pmf-driven chaperones that fold extracellular proteins, but only indirect evidence supports this postulate. The MR family is distantly related to the 7 TMS LCT family (TC# 2.A.43). Representative members of MR family can be found in the Transporter Classification Database.\n\nBacterio- and halorhodopsins pump 1 H and 1 Cl per photon absorbed, respectively. Specific transport mechanisms and pathways have been proposed. The mechanism involves:\n\n(1) photo-isomerization of the retinal and its initial configurational changes,\n\n(2) deprotonation of the retinal Schiff base and the coupled release of a proton to the extracellular membrane surface,\n\n(3) the switch event that allows reprotonation of the Schiff base from the cytoplasmic side.\n\nSix structural models describe the transformations of the retinal and its interaction with water 402, Asp85, and Asp212 in atomic detail, as well as the displacements of functional residues farther from the Schiff base. The changes provide rationales for how relaxation of the distorted retinal causes movements of water and protein atoms that result in vectorial proton transfers to and from the Schiff base. Helix deformation is coupled to vectorial proton transport in the photocycle of bacteriorhodopsin.\n\nMost residues participating in the trimerization are not conserved in bacteriorhodopsin, a homologous protein capable of forming a trimeric structure in the absence of bacterioruberin. Despite a large alteration in the amino acid sequence, the shape of the intratrimer hydrophobic space filled by lipids is highly conserved between archaerhodopsin-2 and bacteriorhodopsin. Since a transmembrane helix facing this space undergoes a large conformational change during the proton pumping cycle, it is feasible that trimerization is an important strategy to capture special lipid components that are relevant to the protein activity.\n\nA marine bacterial rhodopsin has been reported to function as a proton pump. However, it also resembles sensory rhodopsin II of archaea as well as an Orf from the fungus \"Leptosphaeria maculans\" (AF290180). These proteins exhibit 20-30% identity with each other.\n\nChannelrhodopsin-1 (ChR1) or channelopsin-1 (Chop1; Cop3; CSOA) of \"C. reinhardtii\" is closely related to the archaeal sensory rhodopsins. It has 712 aas with a signal peptide, followed by a short amphipathic region, and then a hydrophobic N-terminal domain with seven probable TMSs (residues 76-309) followed by a long hydrophilic C-terminal domain of about 400 residues. Part of the C-terminal hydrophilic domain is homologous to intersectin (EH and SH3 domain protein 1A) of animals (AAD30271).\n\nChop1 serves as a light-gated proton channel and mediates phototaxis and photophobic responses in green algae. Based on this phenotype, Chop1 could be assigned to TC category #1.A, but because it belongs to a family in which well-characterized homologues catalyze active ion transport, it is assigned to the MR family. Expression of the \"chop1\" gene, or a truncated form of that gene encoding only the hydrophobic core (residues 1-346 or 1-517) in frog oocytes in the presence of all-trans retinal produces a light-gated conductance that shows characteristics of a channel passively but selectively permeable to protons. This channel activity probably generates bioelectric currents.\n\nA homologue of ChR1 in \"C. reinhardtii\" is channelrhodopsin-2 (ChR2; Chop2; Cop4; CSOB). This protein is 57% identical, 10% similar to ChR1. It forms a cation-selective ion channel activated by light absorption. It transports both monovalent and divalent cations. It desensitizes to a small conductance in continuous light. Recovery from desensitization is accelerated by extracellular H and a negative membrane potential. It may be a photoreceptor for dark adapted cells. A transient increase in hydration of transmembrane α-helices with a t(1/2) = 60 μs tallies with the onset of cation permeation. Aspartate 253 accepts the proton released by the Schiff base (t(1/2) = 10 μs), with the latter being reprotonated by aspartic acid 156 (t(1/2) = 2 ms). The internal proton acceptor and donor groups, corresponding to D212 and D115 in bacteriorhodopsin, are clearly different from other microbial rhodopsins, indicating that their spatial positions in the protein were relocated during evolution. E90 deprotonates exclusively in the nonconductive state. The observed proton transfer reactions and the protein conformational changes relate to the gating of the cation channel.\n\nArchaerhodopsin-2 (aR2) was found in the claret membrane of \"Halorubrum sp\". It is a light-driven proton pump. Trigonal and hexagonal crystals revealed that trimers are arranged on a honeycomb lattice. In these crystals, the carotenoid bacterioruberin binds to crevices between the subunits of the trimer. Its polyene chain is inclined from the membrane normal by an angle of about 20 degrees and, on the cytoplasmic side, it is surrounded by helices AB and DE of neighboring subunits. This peculiar binding mode suggests that bacterioruberin plays a structural role for the trimerization of aR2. When compared with the aR2 structure in another crystal form containing no bacterioruberin, the proton release channel takes a more closed conformation in the P321 or P6(3) crystal; i.e., the native conformation of protein is stabilized in the trimeric protein-bacterioruberin complex.\n\nThe generalized transport reaction for bacterio- and sensory rhodopsins is:\n\nThat for halorhodopsin is:\n\n",
    "id": "48050134",
    "title": "Microbial rhodopsin"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=49292515",
    "text": "Nicotinamide ribonucleoside uptake transporters\n\nThe Nicotinamide Ribonucleoside (NR) Uptake Permease (PnuC) Family (TC# 4.B.1) is a family of transmembrane transporters that is part of the TOG superfamily. Close PnuC homologues are found in a wide range of Gram-negative and Gram-positive bacteria, archaea and eukaryotes.\n\nPnuC of \"Salmonella typhimurium\" and \"Haemophilus influenzae\" are believed to function cooperatively with NadR homologues, multifunctional proteins that together with PnuC, participate in NR phosphorylation, transport and transcriptional regulation. NadR, a cytoplasmic protein that is partly membrane associated, contains one well conserved and one poorly conserved mononucleotide-binding consensus sequence (G-X4 GKS). It drives transport and may render transport responsive to internal pyridine nucleotide levels. While its N-terminal half functions as a repressor, its C-terminal half functions as an NR kinase in a putative group translocation process.\n\nThe \"H. influenzae\" homologue has been shown to transport NR from the periplasm into the cytoplasm. Phosphorylation of NR by NadR is required for NR uptake. The ribonucleoside kinase (RNK) domain has both Walker A and Walker B motifs, responsible for ATP binding and phosphoryl transfer. In addition, a proposed LID domain was identified in RNK. LID domains have been found in other kinases, and these domains are regions which are able to move after substrate binding. They are responsible for coordination of three distinct conformations, an open state in the absence of substrate, a partially closed state after substrate binding, and a fully closed state when both substrates are present.\n\nIn \"H. influenzae\", NR enters the NAD resynthesis pathway after phosphorylation to NMN, and subsequently, NAD is synthesized from NMN and ATP via an NMN adenylyl transferase activity. Summarizing these features, NadR represents an amazing multifunctional regulator/enzyme complex able to integrate several features, such as enzymatic catalysis, transport, and transcriptional regulatory activities.\n\nThe components of the \"H. influenzae\" pathway necessary for NAD, NMN, and NR uptake have been determined. Merdanovic et al. characterized two enzymes, an outer membrane nucleotide phosphatase, and an NAD nucleotidase (NadN) located in the periplasm. They showed that NAD and NMN cross the outer membrane mainly via the OmpP2 porin.\n\nOnly NR can be utilized by the PnuC transport system located in the inner membrane. The \"pnuC\" gene product is the protein that is responsible for the main flow of the NR substrate into the cytoplasm. The study of Merdanovic et al. suggests that the RNK activity of NadR determines NR transport and is negatively regulated by cytoplasmic NAD feedback inhibition. Therefore, NR uptake is under NadR feedback control.\n\nATP, not the proton motive force, appears to be required for NR uptake. Thus, the driving force for NR uptake via PnuC is NR phosphorylation by NadR. A concerted group translocation mechanism can be considered whereby NadR facilitates the dissociation of NR from PnuC by phosphorylating it to NMN, thus preventing efflux of NR.\n\nThe proposed transport reaction catalyzed by PnuC and NadR is:\n\nNR (out) + ATP (in) → NMN (in) + ADP (in).\n\nPnuC of \"Salmonella typhimurium\" and \"Haemophilus influenzae\" are integral membrane proteins, 239 and 226 amino acyl residues (aas) in length, respectively, with 7 putative transmembrane α-helical segments.\n\nThe structure of NadR has been determined. Mutations in the \"nadR\" gene which interfere with NR uptake occur in the C-terminal part of NadR. A helix-turn-helix DNA binding domain present in NadR of \"S. enterica serovar Typhimurium\" could not be found in the NadR homologue of \"H. influenzae\". Therefore, it was proposed that in \"H. influenzae\" NadR has no regulatory function at the transcriptional level. The structures of the human NR kinase 1 (2QL6_P) with nucleotide and nucleoside substrates bound have been solved. It is structurally similar to Rossmann fold metabolite kinases.\n\nPnuC has been shown to resemble SWEET porters in overall fold, supporting the conclusion that these two families are members of the TOG superfamily.\n\nNadR Protein from \"H. influenzae\" \n\nNR transporter PnuC \n",
    "id": "49292515",
    "title": "Nicotinamide ribonucleoside uptake transporters"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=49293529",
    "text": "Natural Resistance-Associated Macrophage Proteins\n\nNatural Resistance-Associated Macrophage Proteins (NRAMPs) are members of the Metal Ion (Mn-iron) Transporter (NRAMP) Family (TC# 2.A.55). The NRAMP family is a member of the large APC Superfamily of secondary carriers. Homologues of this family are found in various yeasts, plants, animals, archaea, and Gram-negative and Gram-positive bacteria termed \"natural resistance-associated\" macrophage proteins because one of the animal homologues plays a role in resistance to intracellular bacterial pathogens such as \"Salmonella enterica serovar Typhimurium\", \"Leishmania donovani\" and \"Mycobacterium bovis\". The natural history of \"SLC11\" genes in vertebrates has been discussed by Neves et al. (2011). Proposed to be a distant member of the APC Superfamily, several human pathologies may result from defects in NRAMP-dependent Fe or Mn transport, including iron overload, neurodegenerative diseases and innate susceptibility to infectious diseases.\n\nHumans and rodents possess two distinct NRAMPs. The broad specificity NRAMP2 (DMT1), which transports a range of divalent metal cations, transports Fe and H with a 1:1 stoichiometry and apparent affinities of 6 μm and about 1 μm, respectively. Variable H:Fe stoichiometry has also been reported. The order of substrate preference for NRAMP2 is:\n\nFe> Zn> Mn> Co> Ca> Cu> Ni> Pb\n\nMany of these ions can inhibit iron absorption. Mutation of NRAMP2 in rodents leads to defective endosomal iron export within the ferritin cycle, impaired intestinal iron absorption and microcytic anemia. Symptoms of Mn deficiency are also seen. It is found in apical membranes of intestinal epithelial cells but also in late endosomes and lysosomes.\n\nIn contrast to the widely expressed NRAMP2, NRAMP1 is expressed primarily in macrophages and monocytes and appears to have a preference for Mn rather than Fe. NRAMP1 (TC# 2.A.55.2.3) has been reported to function by metal:H antiport. It is hypothesized that a deficiency for Mn or some other metal prevents the generation of reactive oxygenic and nitrogenic compounds that are used by macrophage to combat pathogens. This hypothesis is supported by studies on the bacterial NRAMP homologues which exhibit extremely high selectivity for Mn over Fe, Zn and other divalent cations. Regulation of these transporters in bacteria can occur through Fur, OxyR, and most commonly a DtxR homolog, MntR.\n\nThe Smf1 protein of \"Saccharomyces cerevisiae\" appears to catalyze high-affinity (\"K\" = 0.3 μm) Mn uptake while the closely related Smf2 protein may catalyze low affinity (\"K\" = 60 μm) Mn uptake in the same organism. Both proteins also mediate H-dependent Fe uptake. These proteins are of 575 and 549 amino acyl residues in length and are predicted to have 8-12 transmembrane α-helical spanners. The \"E. coli\" homologue of 412 aas exhibits 11 putative and confirmed TMSs with the N-terminus in and the C-terminus out. The yeast proteins may be localized to the vacuole and/or the plasma membrane of the yeast cell. Indirect and some direct experiments suggest that they may be able to transport several heavy metals including Mn, Cu, Cd and Co. A third yeast protein, Smf3p, appears to be exclusively intracellular, possibly in the Golgi. NRAMP2 (\"Slc11A2\") of Homo sapiens (TC# 2.A.55.2.1) has a 12 TMS topology with intracellular N- and C-termini. Two-fold structural symmetry in the arrangement of membrane helices for TM1-5 and TM6-10 (conserved Slc2 hydrophobic core) is suggested.\n\nThe generalized transport reaction catalyzed by NRAMP family proteins is:\n\nMe (out) + H (out) ⇌ Me (in) + H (in).\n\n",
    "id": "49293529",
    "title": "Natural Resistance-Associated Macrophage Proteins"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=35038842",
    "text": "Formate-nitrite transporter\n\nThe Formate-Nitrite Transporter (FNT) Family belongs to the Major Intrinsic Protein (MIP) Superfamily. FNT family members have been sequenced from Gram-negative and Gram-positive bacteria, archaea, yeast, plants and lower eukaryotes. The prokaryotic proteins of the FNT family probably function in the transport of the structurally related compounds, formate and nitrite.\n\nWith the exception of the yeast protein (627 amino acyl residues), all characterized members of the family are of 256-285 residues in length and exhibit 6-8 putative transmembrane α-helical spanners (TMSs). In one case, that of the \"E. coli\" FocA (TC# 1.A.16.1.1) protein, a 6 TMS topology has been established. The yeast protein has a similar apparent topology but has a large C-terminal hydrophilic extension of about 400 residues.\n\nFocA of \"E. coli\" is a symmetriv pentamer, with each subunit consisting of six TMSs.\n\nThe phylogenetic tree shows clustering according to function and organismal phylogeny. The putative formate efflux transporters (FocA; TC#s 1.A.16.1.1 and 1.A.16.1.3) of bacteria associated with pyruvate-formate lyase (pfl) comprise cluster I; the putative formate uptake permeases (FdhC; TC#s 1.A.16.2.1 and 1.A.16.2.3) of bacteria and archaea associated with formate dehydrogenase comprise cluster II; the nitrite uptake permeases (NirC, TC#s 1.A.16.2.5, 1.A.16.3.1, and 1.A.16.3.4) of bacteria comprise cluster III, and a yeast protein comprises cluster IV.\n\nThe energy coupling mechanisms for proteins of the FNT family have not been extensively characterized. HCO and NO uptakes may be coupled to H symport. HCO efflux may be driven by the membrane potential by a uniport mechanism or by H antiport. FocA of \"E. coli\" catalyzes bidirectional formate transport and may function by a channel-type mechanism.\n\nFocA, transports short-chain acids. FocA may be able to switch its mode of operation from a passive export channel at high external pH to a secondary active formate/H+ importer at low pH. The crystal structure of \"Salmonella typhimurium\" FocA at pH 4.0 shows that this switch involves a major rearrangement of the amino termini of individual protomers in the pentameric channel. The amino-terminal helices open or block transport in a concerted, cooperative action that indicates how FocA is gated in a pH-dependent way. Electrophysiological studies show that the protein acts as a specific formate channel at pH 7.0 and that it closes upon a shift of pH to 5.1.\n\nThe probable transport reactions catalyzed by different members of the FNT family are:\n\n(1) RCO or NO (out) ⇌ RCO or NO (in),\n\n(2) HCO (in) ⇌ HCO (out),\n\n(3) HS (out) ⇌ HS (in).\n\nA representative list of the currently classified members belonging to the FNT family can be found in the Transporter Classification Database. Some characterized members include:\n\n",
    "id": "35038842",
    "title": "Formate-nitrite transporter"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=35049854",
    "text": "Cation diffusion facilitator\n\nCation diffusion facilitators (CDFs) are transmembrane proteins that provide tolerance of cells to divalent metal ions, such as cadmium, zinc, and cobalt. These proteins are considered to be efflux pumps that remove these divalent metal ions from cells. However, some members of the CDF superfamily are implicated in ion uptake. All members of the CDF family possess six putative transmembrane spanners with strongest conservation in the four N-terminal spanners. The Cation Diffusion Facilitator (CDF) Superfamily includes the following families: \n\nThe CDF family (TC# 2.A.4) is a ubiquitous family, members of which are found in bacteria, archaea and eukaryotes. They transport heavy metal ions, such as cadmium, zinc, cobalt, nickel, copper and mercuric ions. There are 9 mammalian paralogues, ZnT1 - 8 and 10. Most proteins from the family have six transmembrane helices, but MSC2 of \"S. cerevisiae\") and Znt5 and hZTL1 of \"H. sapiens\" have 15 and 12 predicted TMSs, respectively. These proteins exhibit an unusual degree of sequence divergence and size variation (300-750 residues). Eukaryotic proteins exhibit differences in cell localization. Some catalyze heavy metal uptake from the cytoplasm into various intracellular eukaryotic organelles (ZnT2-7) while others (ZnT1) catalyze efflux from the cytoplasm across the plasma membrane into the extracellular medium. Thus, some are found in plasma membranes while others are in organellar membranes such as vacuoles of plants and yeast and the golgi of animals. They catalyze cation:proton antiport, have a single essential zinc-binding site within the transmembrane domains of each monomer within the dimer, and have a binuclear zinc-sensing and binding site in the cytoplamsic C-terminal region. A representative list of proteins belonging to the CDF family can be found in the Transporter Classification Database.\n\nProkaryotic and eukaryotic proteins cluster separately but may function with the same polarity by similar mechanisms. These proteins are secondary carriers which utilize the proton motive force (pmf) and function by H antiport (for metal efflux). One member, CzcD of \"Bacillus subtilis\" (TC# 2.A.4.1.3) \",\" has been shown to exchange the divalent cation (Zn or Cd ) for two monovalent cations (K and H ) in an electroneutral process energized by the transmembrane pH gradient. Another, ZitB of \"E. coli\" (TC #2.A.4.1.4), has been reconstituted in proteoliposomes and studied kinetically. It appears to function by simple Me:H antiport with a 1:1 stoichiometry.\n\nMontanini et al. (2007) have conducted phylogenetic analysis of CDF family members. Their analysis revealed three major and two minor phylogenetic groups. They suggest that the three major groups segregated according to metal ion specificity: \n\nX-ray structure of YiiP of \"E. coli\" represents a homodimer.\n\nCoudray et al. (2013) used cryoelectron microscopy to determine a 13 Å resolution structure of a YiiP homolog from \"Shewanella oneidensis\" within a lipid bilayer in the absence of Zn. Starting from the x-ray structure in the presence of Zn, they used molecular dynamic flexible fitting to build a model. Comparison of the structures suggested a conformational change that involves pivoting of a transmembrane, four-helix bundle (M1, M2, M4, and M5) relative to the M3-M6 helix pair. Although accessibility of transport sites in the x-ray model indicates that it represents an outward-facing state, their model was consistent with an inward-facing state, suggesting that the conformational change is relevant to the alternating access mechanism for transport. They speculated that the dimer may coordinate rearrangement of the transmembrane helices.\n\nInvolved in metal tolerance/resistance by efflux, most CDF proteins share a two-modular architecture consisting of a transmembrane domain (TMD) and a C-terminal domain (CTD) that protrudes into the cytoplasm. A Zn and Cd CDF transporter from the marine bacterium, \"Maricaulis maris,\" that does not possess the CTD is a member of a new, CTD-lacking subfamily of CDFs.\n\nThe generalized transport reaction for CDF family members is:\n\nMe (in) H (out) ± K (out) → Me (out) H (in) ± K (in).\n\n",
    "id": "35049854",
    "title": "Cation diffusion facilitator"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=8553751",
    "text": "Biological organisation\n\nBiological organization is the hierarchy of complex biological structures and systems that define life using a reductionistic approach. The traditional hierarchy, as detailed below, extends from atoms to biospheres. The higher levels of this scheme are often referred to as an ecological organisation concept, or as the field, hierarchical ecology.\n\nEach level in the hierarchy represents an increase in organisational complexity, with each \"object\" being primarily composed of the previous level's basic unit. The basic principle behind the organisation is the concept of \"emergence\"—the properties and functions found at a hierarchical level are not present and irrelevant at the lower levels.\n\nThe biological organisation of life is a fundamental premise for numerous areas of scientific research, particularly in the medical sciences. Without this necessary degree of organisation, it would be much more difficult—and likely impossible—to apply the study of the effects of various physical and chemical phenomena to diseases and physiology (body function). For example, fields such as cognitive and behavioral neuroscience could not exist if the brain was not composed of specific types of cells, and the basic concepts of pharmacology could not exist if it was not known that a change at the cellular level can affect an entire organism. These applications extend into the ecological levels as well. For example, DDT's direct inseciticidal effect occurs at the subcellular level, but affects higher levels up to and including multiple ecosystems. Theoretically, a change in one atom could change the entire biosphere.\n\nThe simple standard biological organisation scheme, from the lowest level to the highest level, is as follows:\n\nMore complex schemes incorporate many more levels. For example, a molecule can be viewed as a grouping of elements, and an atom can be further divided into subatomic particles (these levels are outside the scope of biological organisation). Each level can also be broken down into its own hierarchy, and specific types of these biological objects can have their own hierarchical scheme. For example, genomes can be further subdivided into a hierarchy of genes.\n\nEach level in the hierarchy can be described by its lower levels. For example, the organism may be described at any of its component levels, including the atomic, molecular, cellular, histological (tissue), organ and organ system levels. Furthermore, at every level of the hierarchy, new functions necessary for the control of life appear. These new roles are not functions that the lower level components are capable of and are thus referred to as \"emergent properties\".\n\nEvery organism is organised, though not necessarily to the same degree. An organism can not be organised at the histological (tissue) level if it is not composed of tissues in the first place.\n\nEmpirically, a large proportion of the (complex) biological systems we observe in nature exhibit hierarchic structure. On theoretical grounds we could expect complex systems to be hierarchies in a world in which complexity had to evolve from simplicity. System hierarchies analysis performed in the 1950s, laid the empirical foundations for a field that would be, from 1980's, hierarchical ecology.\n\nThe theoretical foundations are summarized by Thermodynamics.\nWhen biological systems are modeled as physical systems, in its most general abstraction, they are thermodynamic open systems that exhibit self-organised behavior, and the set/subset relations between dissipative structures can be characterized in an hierarchy.\n\nA simpler and more direct way to explain the fundamentals of the \"hierarchical organisation of life\", was introduced in Ecology by Odum and others as the \"Simon's hierarchical principle\"; Simon emphasized that hierarchy \"\"emerges almost inevitably through a wide variety of evolutionary processes, for the simple reason that hierarchical structures are stable\"\".\n\nTo motivate this deep idea, he offered his \"parable\" about imaginary watchmakers.\n\n\n",
    "id": "8553751",
    "title": "Biological organisation"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=2537522",
    "text": "Biologist\n\nA biologist, is a scientist who has specialized knowledge in the field of biology, the scientific study of life. Biologists involved in fundamental research attempt to explore and further explain the underlying mechanisms that govern the functioning of living matter. Biologists involved in applied research attempt to develop or improve more specific processes and understanding, in fields such as medicine, industry and agriculture.\n\nWhile \"biologist\" can apply to any scientist studying biology, most biologists research and specialise in specific fields. In this way, biologists investigate large-scale organism interactions (ecology), whole multicellular organisms, organs, tissues, cells, and small-scale cellular and molecular processes. Other biologists study less direct aspects of life, such as phylogeny and evolution.\n\nBiologists conduct research based on the scientific method, to test the validity of a theory, with hypothesis formation, experimentation and documentation of protocols and datas.\n\nThere are many types of biologists. Some work on microorganisms, while others study multicellular organisms. There is much overlap between different fields of biology such as botany, zoology, microbiology, genetics and evolutionary biology, and it is often difficult to classify a biologist as only one of them. Many jobs in biology as a field require an academic degree. A doctorate or its equivalent is generally required to direct independent research, and involves a specialization in a specific area of biology. Many biological scientists work in research and development. Some conduct fundamental research to advance our knowledge of living organisms, including bacteria and other pathogens. This research enhances understanding and adds to the scientific database of literature. Furthermore, it often aids the development of solutions to problems in areas such as human health and the natural environment. These biological scientists mostly work in government, university, and private industry laboratories. Many expand on specialized research that they started in post-graduate qualifications, such as a doctorate.\n\nBiological scientists who work in applied research or product development often use knowledge gained by basic research to further knowledge in particular fields or applications. For example, this applied research may be used to develop new pharmaceutical drugs, treatments, and medical diagnostic tests; increase crop yields; and develop biofuels. They usually have less freedom than basic researchers to choose the emphasis of their research, and they spend more time working on marketable treatments to meet the business goals of their employers. Biological scientists conducting applied research and product development in private industry may be required to describe their research plans or results to non-scientists who are in a position to veto or approve their ideas. These scientists must consider the business effects of their work.\n\nSome biologists conduct laboratory experiments involving animals, plants or microorganisms. However, some biological research also occurs outside the laboratory and may involve natural observation rather than experimentation. For example, a botanist may investigate the plant species present in a particular environment, while an ecologist might study how a forest area recovers after a fire.\n\nSwift advances in knowledge of genetics and organic molecules spurred growth in the field of biotechnology, transforming the industries in which biological scientists work. Biological scientists can now manipulate the genetic material of animals and plants, attempting to make organisms more productive or resistant to disease. Basic and applied research on biotechnological processes, such as recombining DNA, has led to the production of important substances, including human insulin and growth hormone. Many other substances not previously available in large quantities are now produced by biotechnological means. Some of these substances are useful in treating diseases.\n\nThose working on various genome (chromosomes with their associated genes) projects isolate genes and determine their function. This work continues to lead to the discovery of genes associated with specific diseases and inherited health risks, such as sickle cell anemia. Advances in biotechnology have created research opportunities in almost all areas of biology, with commercial applications in areas such as medicine, agriculture, and environmental remediation.\n\nMost biological scientists specialize in the study of a certain type of organism or in a specific activity, although recent advances have blurred some traditional classifications.\n\nBiological scientists are not usually exposed to unsafe or unhealthy conditions. Those who work with dangerous organisms or toxic substances in the laboratory must follow strict safety procedures to avoid contamination. Many biological scientists, such as botanists, ecologists, and zoologists, conduct field studies that involve strenuous physical activity and primitive living conditions. Biological scientists in the field may work in warm or cold climates, in all kinds of weather.\n\nMarine biologists encounter a variety of working conditions. Some work in laboratories; others work on research ships, and those who work underwater must practice safe diving while working around sharp coral reefs and hazardous marine life. Although some marine biologists obtain their specimens from the sea, many still spend a good deal of their time in laboratories and offices, conducting tests, running experiments, recording results, and compiling data.\n\nMany biological scientists depend on grant money to fund their research. They may be under pressure to meet deadlines and to conform to rigid grant-writing specifications when preparing proposals to seek new or extended funding.\n\nBiological scientists typically work regular hours. While the 40-hour workweek is common, longer hours are not uncommon. Researchers may be required to work odd hours in laboratories or other locations (especially while in the field), depending on the nature of their research.\n\nThe highest honor awarded to biologists is the Nobel Prize in Physiology or Medicine, awarded since 1901, by the Royal Swedish Academy of Sciences. Another significant award is the Crafoord Prize in Biosciences; established in 1980.\n\n\n",
    "id": "2537522",
    "title": "Biologist"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=44707607",
    "text": "Bibliography of encyclopedias: biology\n\nThis is a list of encyclopedias as well as encyclopedic and biographical dictionaries published on the subject of biology in any language.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nref name=\"K371\"/> \n\n",
    "id": "44707607",
    "title": "Bibliography of encyclopedias: biology"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=12936761",
    "text": "MACPF\n\nThe Membrane Attack Complex/Perforin (MACPF) superfamily, sometimes referred to as the MACPF/CDC superfamily, is named after a domain that is common to the membrane attack complex (MAC) proteins of the complement system (C6, C7, C8α, C8β and C9) and perforin (PF). Members of this protein family are pore-forming toxins (PFTs). In eukaryotes, MACPF proteins play a role in immunity and development.\n\nArchetypal members of the family are complement C9 and perforin, both of which function in human immunity. C9 functions by punching holes in the membranes of Gram-negative bacteria. Perforin is released by cytotoxic T cells and lyses virally infected and transformed cells. In addition, perforin permits delivery of cytotoxic proteases called granzymes that cause cell death. Deficiency of either protein can result in human disease. Structural studies reveal that MACPF domains are related to cholesterol-dependent cytolysins (CDCs), a family of pore forming toxins previously thought to only exist in bacteria.\n\nAs of early 2016, there are three families belonging to the MACPF superfamily:\n\nProteins containing MACPF domains play key roles in vertebrate immunity, embryonic development, and neural-cell migration. The ninth component of complement and perforin form oligomeric pores that lyse bacteria and kill virus-infected cells, respectively. The crystal structure of a bacterial MACPF protein, Plu-MACPF from \"Photorhabdus luminescens\" was determined (). The MACPF domain is structurally similar to pore-forming cholesterol-dependent cytolysins from gram-positive bacteria, suggesting that MACPF proteins create pores and disrupt cell membranes similar to cytolysin. A representative list of proteins belonging to the MACPF family can be found in the Transporter Classification Database.\n\nMany proteins belonging to the MACPF superfamily play key roles in plant and animal immunity.\n\nComplement proteins C6-C9 all contain a MACPF domain and assemble into the membrane attack complex. C6, C7 and C8β appear to be non-lytic and function as scaffold proteins within the MAC. In contrast both C8α and C9 are capable of lysing cells. The final stage of MAC formation involves polymerisation of C9 into a large pore that punches a hole in the outer membrane of gram-negative bacteria.\n\nPerforin is stored in granules within cytotoxic T-cells and is responsible for killing virally infected and transformed cells. Perforin functions via two distinct mechanisms. Firstly, like C9, high concentrations of perforin can form pores that lyse cells. Secondly, perforin permits delivery of the cytotoxic granzymes A and B into target cells. Once delivered, granzymes are able to induce apoptosis and cause target cell death.\n\nThe plant protein CAD1 (TC# 1.C.39.11.3) functions in the plant immune response to bacterial infection.\n\nThe sea anemone \"Actineria villosa\" uses a MACPF (AvTX-60A; TC# 1.C.39.10.1)protein as a lethal toxin.\n\nMACPF proteins are also important for the invasion of the Malarial parasite into the mosquito host and the liver.\n\nNot all MACPF proteins function in defence or attack. For example, astrotactin-1 (TC# 9.B.87.3.1) is involved in neural cell migration in mammals and apextrin (TC# 1.C.39.7.4) is involved in sea urchin (\"Heliocidaris erythrogramma\") development. \"Drosophila\" Torso-like protein (TC# 1.C.39.15.1), which controls embryonic patterning, also contains a MACPF domain. Its function is implicated in a receptor tyrosine kinase signaling pathway that specifies differentiation and terminal cell fate.\n\nFunctionally uncharacterised MACPF proteins are sporadically distributed in bacteria. Several species of \"Chlamydia\" contain MACPF proteins. The insect pathogenic bacteria \"Photorhabdus luminescens\" also contains a MACPF protein, however, this molecule appears non-lytic.\n\nThe X-ray crystal structure of Plu-MACPF, a protein from the insect pathogenic enterobacteria \"Photorhabdus luminescens\" has been determined (figure 1). These data reveal that the MACPF domain is homologous to pore forming cholesterol dependent cytolysins (CDC's) from gram-positive pathogenic bacteria such as \"Clostridium perfringens\" (which causes gas gangrene). The amino acid sequence identity between the two families is extremely low, and the relationship is not detectable using conventional sequence based data mining techniques.\n\nIt is suggested that MACPF proteins and CDCs form pores in the same way (figure 1). Specifically it is hypothesised that MACPF proteins oligomerise to form a large circular pore (figure 2). A concerted conformational change within each monomer then results in two α-helical regions unwinding to form four amphipathic β-strands that span the membrane of the target cell. Like CDC's MACPF proteins are thus β-pore forming toxins that act like a molecular hole punch.\n\nOther crystal structures for members of the MACPF superfamily can be found in RCSB: i.e., , , , , \n\nComplement regulatory proteins such as CD59 function as MAC inhibitors and prevent inappropriate activity of complement against self cells (Figure 3). Biochemical studies have revealed the peptide sequences in C8α and C9 that bind to CD59. Analysis of the MACPF domain structures reveals that these sequences map to the second cluster of helices that unfurl to span the membrane. It is therefore suggested that CD59 directly inhibits the MAC by interfering with conformational change in one of the membrane spanning regions.\n\nOther proteins that bind to the MAC include C8γ. This protein belongs to the lipocalin family and interacts with C8α. The binding site on C8α is known, however, the precise role of C8γ in the MAC remains to be understood.\n\nDeficiency of C9, or other components of the MAC results in an increased susceptibility to diseases caused by gram-negative bacteria such as meningococcal meningitis. Overactivity of MACPF proteins can also cause disease. Most notably, deficiency of the MAC inhibitor CD59 results in an overactivity of complement and Paroxysmal nocturnal hemoglobinuria.\n\nPerforin deficiency results in the commonly fatal disorder familial hemophagocytic lymphohistiocytosis (FHL or HLH). This disease is characterised by an overactivation of lymphocytes which results in cytokine mediated organ damage.\n\nThe MACPF protein DBCCR1 may function as a tumor suppressor in bladder cancer.\n\nC6; C7; C8A; C8B; C9; FAM5B; FAM5C; MPEG1;\nPRF1\n",
    "id": "12936761",
    "title": "MACPF"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=39288495",
    "text": "Mitochondrial fusion\n\nMitochondria are dynamic organelles with the ability to fuse (fusion) and divide (fission), forming constantly changing tubular networks in most eukaryotic cells. These mitochondrial dynamics are important for the health of the cell, and defects in dynamics lead to genetic disorders. Through fusion, mitochondria can overcome the dangerous consequences of genetic malfunction. The process of mitochondrial fusion involves a variety of proteins that assist the cell throughout the series of events that form this process. \n\nWhen cells experience metabolic or environmental stresses, mitochondrial fusion and fission work to maintain functional mitochondria. An increase in fusion activity leads to mitochondrial elongation, whereas an increase in fission activity results in mitochondrial fragmentation. The components of this process can influence programmed cell death and lead to neurodegenerative disorders such as Parkinson's disease. Such cell death can be caused by disruptions in the process of either fusion or fission. \n\nThe shapes of mitochondria in cells are continually changing via a combination of fission, fusion, and motility. Specifically, fusion assists in modifying stress by integrating the contents of slightly damaged mitochondria as a form of complementation. By enabling genetic complementation, fusion of the mitochondria allows for two mitochondrial genomes with different defects within the same organelle to individually encode what the other lacks. In doing so, these mitochondrial genomes generate all of the necessary components for a functional mitochondrion.\n\nThe combined effects of continuous fusion and fission give rise to mitochondrial networks. The mechanisms of mitochondrial fusion and fission are regulated by proteolysis and posttranslational modifications. The actions of fission, fusion and motility cause the shapes of these double membrane bound subcellular organelles we know as mitochondria to continually alter their shapes. \n\nThe changes in balance between the rates of mitochondrial fission and fusion directly affect the wide range of mitochondrial lengths that can be observed in different cell types. Rapid fission and fusion of the mitochondria in cultured fibroblasts has been shown to promote the redistribution of mitochondrial green fluorescent protein (GFP) from one mitochondrion to all of the other mitochondria. This process can occur in a cell within a time period as short as an hour.\n\nThe significance of mitochondrial fission and fusion is distinct for nonproliferating neurons, which are unable to survive without mitochondrial fission. Such nonproliferating neurons cause two human diseases known as dominant optic atrophy and Charcot Marie Tooth disease type 2A, which are both caused by fusion defects. Though the importance of these processes is evident, it is still unclear why mitochondrial fission and fusion are necessary for nonproliferating cells.\n\nMany gene products that control mitochondrial fusion have been identified, and can be reduced to three core groups which also control mitochondrial fission. These groups of proteins include mitofusins, OPA1/Mgm1, and Drp1/Dnm1. All of these molecules are GTP hydrolyzing proteins (GTPases) that belong to the dynamin family. Mitochondrial dynamics in different cells are understood by the way in which these proteins regulate and bind to each other. These GTPases in control of mitochondrial fusion are well conserved between mammals, flies, and yeast. Mitochondrial fusion mediators differ between the outer and inner membranes of the mitochondria. Specific membrane-anchored dynamin family members mediate fusion between mitochondrial outer membranes known as Mfn1 and Mfn2. These two proteins are mitofusin contained within humans that can alter the morphology of affected mitochondria in over-expressed conditions. However, a single dynamin family member known as OPA1 in mammals mediates fusion between mitochondrial inner membranes. These regulating proteins of mitochondrial fusion are organism-dependent; therefore, in \"Drosophila\" (fruit flies) and yeasts, the process is controlled by the mitochondrial transmembrane GTPase, Fzo. In \"Drosophila\", Fzo is found in postmeiotic spermatids and the dysfunction of this protein results in male sterility. However, a deletion of Fzo1 in budding yeast results in smaller, spherical mitochondria due to the lack of mitochondrial DNA (mtDNA).\n\nThe balance between mitochondrial fusion and fission in cells is dictated by the up-and-down regulation of mitofusins, OPA1/Mgm1, and Drp1/Dnm1. Apoptosis, or programmed cell death, begins with the breakdown of mitochondria into smaller pieces. This process results from up-regulation of Drp1/Dnm1 and down-regulation of mitofusins. Later in the apoptosis cycle, an alteration of OPA1/Mgm1 activity within the inner mitochondrial membrane occurs. The role of the OPA1 protein is to protect cells against apoptosis by inhibiting the release of cytochrome c. Once this protein is altered, there is a change in the cristae structure, release of cytochrome c, and the activation of the destructive caspase enzymes. These resulting changes indicate that inner mitochondrial membrane structure is linked with regulatory pathways in influencing cell life and death. OPA1 plays both a genetic and molecular role in mitochondrial fusion and in cristae remodeling during apoptosis. OPA1 exists in two forms; the first being soluble and found in the intermembrane space, and the second as an integral inner membrane form, work together to restructure and shape the cristae during and after apoptosis. OPA1 blocks intramitochondrial cytochrome c redistribution which proceeds remodeling of the cristae. OPA1 functions to protect cells with mitochondrial dysfunction due to Mfn deficiencies, doubly for those lacking Mfn1 and Mfn2, but it plays a greater role in cells with only Mfn1 deficiencies as opposed to Mfn2 deficiencies. Therefore, it is supported that OPA1 function is dependent on the amount of Mfn1 present in the cell to promote mitochondrial elongation.\n\nBoth proteins, Mfn1 and Mfn2, can act either together or separately during mitochondrial fusion. Mfn1 and Mfn2 are 81% similar to each other and about 51% similar to the \"Drosophila\" protein Fzo. Results published for a study to determine the impact of fusion on mitochondrial structure revealed that Mfn-deficient cells demonstrated either elongated cells (majority) or small, spherical cells upon observation.\n\nThe Mfn protein has three different methods of action: Mfn1 homotypic oligomers, Mfn2 homotypic oligomers and Mfn1-Mfn2 heterotypic oligomers. It has been suggested that the type of cell determines the method of action but it has yet to be concluded whether or not Mfn1 and Mfn2 perform the same function in the process or if they are separate. Cells lacking this protein are subject to severe cellular defects such as poor cell growth, heterogeneity of mitochondrial membrane potential and decreased cellular respiration.\n\nMitochondrial fusion plays an important role in the process of embroygenesis, as shown through the Mfn1 and Mfn2 proteins. Using Mfn1 and Mfn2 knock-out mice, which die in utero at midgestation due to a placental deficiency, mitochondrial fusion was shown not to be essential for cell survival in vitro, but necessary for embryonic development and cell survival throughout later stages of development. Mfn1 Mfn2 double knock-out mice, which die even earlier in development, were distinguished from the \"single\" knock-out mice. Mouse embryo fibroblasts (MEFs) originated from the double knock-out mice, which do survive in culture even though there is a complete absence of fusion, but parts of their mitochondria show a reduced mitochondrial DNA (mtDNA) copy number and lose membrane potential. This series of events causes problems with adenosine triphosphate (ATP) synthesis.\n\nThe Mitochondrial Inner/Outer Membrane Fusion (MMF) Family (TC# 9.B.25) is a family of proteins that play a role in mitochondrial fusion events. This family belongs to the larger Mitochondrial Carrier (MC) Superfamily. The dynamic nature of mitochondria is critical for function. Chen and Chan (2010) have discussed the molecular basis of mitochondrial fusion, its protective role in neurodegeneration, and its importance in cellular function. The mammalian mitofusins Mfn1 and Mfn2, GTPases localized to the outer membrane, mediate outer-membrane fusion. OPA1, a GTPase associated with the inner membrane, mediates subsequent inner-membrane fusion. Mutations in Mfn2 or OPA1 cause neurodegenerative diseases. Mitochondrial fusion enables content mixing within a mitochondrial population, thereby preventing permanent loss of essential components. Cells with reduced mitochondrial fusion show a subpopulation of mitochondria that lack mtDNA nucleoids. Such mtDNA defects lead to respiration-deficient mitochondria, and their accumulation in neurons leads to impaired outgrowth of cellular processes and consequent neurodegeneration.\n\nA representative list of the proteins belonging to the MMF family is available in the Transporter Classification Database.\n\nMfn1 and Mfn2 (TC# 9.B.25.2.1; Q8IWA4 and O95140, respectively), in mammalian cells are required for mitochondrial fusion, Mfn1 and Mfn2 possess functional distinctions. For instance, the formation of tethered structures \"in vitro\" occurs more readily when mitochondria are isolated from cells overexpressing Mfn1 than Mfn2. In addition, Mfn2 specifically has been shown to associate with Bax and Bak (Bcl-2 family, TC#1.A.21), resulting in altered Mfn2 activity, indicating that the mitofusins possess unique functional characteristics. Lipidic holes may open on opposing bilayers as intermediates, and fusion in cardiac myocytes is coupled with outer mitochondrial membrane destabilization that is opportunistically employed during the mitochondrial permeability transition.\n\nMutations in Mfn2 (but not Mfn1) result in the neurological disorder Charcot-Marie-Tooth syndrome. These mutations can be complemented by the formation of Mfn1–Mfn2 hetero-oligomers but not homo-oligomers of Mfn2–Mfn2 This suggests that within the Mfn1–Mfn2 hetero-oligomeric complex, each molecule is functionally distinct. This suggests that control of the expression levels of each protein likely represents the most basic form of regulation to alter mitochondrial dynamics in mammalian tissues. Indeed, the expression levels of Mfn1 and Mfn2 vary according to cell or tissue type as does the mitochondrial morphology.\n\nIn yeast, three proteins are essential for mitochondrial fusion. Fzo1 (P38297) and Mgm1 (P32266) are conserved guanosine triphosphatases that reside in the outer and inner membranes, respectively. At each membrane, these conserved proteins are required for the distinct steps of membrane tethering and lipid mixing. The third essential component is Ugo1, an outer membrane protein with a region homologous to but distantly related to a region in the Mitochondrial Carrier (MC) family. Hoppins \"et al.\", 2009 showed that Ugo1 is a modified member of this family, containing three transmembrane domains and existing as a dimer, a structure that is critical for the fusion function of Ugo1. Their analyses of Ugo1 indicate that it is required for both outer and inner membrane fusion after membrane tethering, indicating that it operates at the lipid-mixing step of fusion. This role is distinct from the fusion dynamin-related proteins and thus demonstrates that at each membrane, a single fusion protein is not sufficient to drive the lipid-mixing step. Instead, this step requires a more complex assembly of proteins. The formation of a fusion pore has not yet been demonstrated. The Ugo1 protein is a member of the MC superfamily.\n\n",
    "id": "39288495",
    "title": "Mitochondrial fusion"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=49557171",
    "text": "Tellurium ion resistance\n\nThe Tellurium Ion Resistance (TerC) Family (TC# 2.A.109) is part of the Lysine Exporter (LysE) Superfamily. A representative list of proteins belonging to the TerC family can be found in the Transporter Classification Database.\n\nThe TerC family (Pfam 03741) includes the \"E. coli\" TerC protein (TC# 2.A.109.1.1) which has been implicated in tellurium resistance. It is hypothesized to catalyze efflux of tellurium ions. TerC is encoded by plasmid pTE53 from a clinical isolate of \"E. coli.\" It has 346 amino acyl residues (aas) and 9 putative transmembrane segments (TMSs) with a large hydrophilic loop between TMSs 5 and 6.\n\nA homologue in \"Arabidopsis thaliana\" (TC# 9.A.30.2.1) may function in prothylakoid membrane biogenises during early chloroplast development. It has 384 aas and 7-8 putative TMSs. In \"E. coli\", TerC forms a membrane complex with TerB as well as DctA, PspA, HslU, and RplK. The TerB/TerC complex may link different functional modules with biochemical activities of C4-dicarboxylate transport, inner membrane stress response (phage shock protein regulatory complex), ATPase/chaperone activity, and proteosynthesis. It may be part of a metal sensing stress response system. The co-presence of TerC and TerE but not TerF correlates with tellurite resistance when several hundred bacterial strains were assayed.\n\nThe reaction proposed to be catalyzed by TerC is:\n\ntellurium ions (in) → tellurium ions (out).\n\n\n",
    "id": "49557171",
    "title": "Tellurium ion resistance"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=49557494",
    "text": "Peptidoglycolipid addressing protein\n\nThe Peptidoglycolipid Addressing Protein (GAP) Family is a member of the Lysine Exporter (LysE) Superfamily. It is listed as item 2.A.116 in the Transporter Classification Database.The mechanism of its action is not known, but this family has been shown to be a member of the LsyE superfamily. Therefore, these proteins are most likely secondary carriers.\n\nThe proposed generalized reaction catalyzed by members of the GAP family is:\n\nPGL (in) → PGL (outer membrane).\n\n\n",
    "id": "49557494",
    "title": "Peptidoglycolipid addressing protein"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=49601697",
    "text": "NicO transporters\n\nThe Nickel/Cobalt Transporter (NicO) Family (TC# 2.A.113) is a member of the Lysine Exporter (LysE) Superfamily. A representative list of proteins belonging to the NicO family can be found in the Transporter Classification Database.\n\nHomologues of the NicO family have differing predicted topologies: 6, 7 and 8 TMSs. One such homologue, RcnA (YohM; TC# 2.A.113.1.1) of \"E. coli\" (274 aas) has 6 putative transmembrane segments (TMSs) in a 3 + 3 arrangement with a large hydrophilic loop between putativeTMSs 3 and 4. Several homologues of RcnA (e.g., RcnA homologue from \"Ralstonia solanacearum\"; TC# 2.A.113.1.3; CAD17703) have 7 putative TMSs (4 + 3). Still another homologue, UreH of \"Methanocaldococcus janaschii\" (TC# 2.A.113.1.4) has 6 putative TMSs in a more characteristic 3 + 3 TMS arrangement. The NicO family within the LysE superfamily may have a common origin with the TOG superfamily, having lost TMSs 1 and 4 in the 8 TMS TOG superfamily topology.\n\nThis protein is believed to catalyze Co and Ni efflux.\n\nThe overall reaction catalyzed by proteins of the NicO family is probably:\n\n[Ni or Co] (in) → [Ni or Co] (out).\n\n\n",
    "id": "49601697",
    "title": "NicO transporters"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=49625940",
    "text": "Sodium-proton antiporter\n\nSodium/proton antiporters are essential secondary-active transporters for sodium and pH homeostasis. Defects in sodium/proton antiporters may result in heart or kidney failure.\nThere are several families of sodium/proton antiporters that facilitate the exchange of sodium ions with protons across the lipid membrane. Some of them include:\n",
    "id": "49625940",
    "title": "Sodium-proton antiporter"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=49633163",
    "text": "P-Aminobenzoyl-glutamate transporter\n\nThe \"p\"-aminobenzoyl-glutamate transporter (AbgT) family (TC# 2.A.68) is a family of transporter proteins belonging to the ion transporter (IT) superfamily. The AbgT family consists of the AbgT (YdaH; TC# 2.A.68.1.1) protein of \"E. coli\" and the MtrF drug exporter (TC# 2.A.68.1.2) of \"Neisseria gonorrhoeae\". The former protein is apparently cryptic in wild-type cells, but when expressed on a high copy number plasmid, or when expressed at higher levels due to mutation, it appeared to allow uptake (\"K\" = 123 nM; see Michaelis–Menten kinetics) and subsequent utilization of \"p\"-aminobenzoyl-glutamate as a source of \"p\"-aminobenzoate for \"p\"-aminobenzoate auxotrophs. \"p\"-Aminobenzoate is a constituent of and a precursor for the biosynthesis of folic acid. MtrF was annotated as a putative drug efflux pump.\n\nAbgT is 510 amino acyl residues long and has 12-13 putative transmembrane α-helical spanners (TMSs). MtrF is 522 aas long and has 11 or 12 putative TMSs. The 3-d structures of MtrF and a YdaH homologue have been solved, and functional studies show that it is a drug exporter. The 3-d structure shows that it has 9 TMSs with hairpin entry loops.\n\nCrystal Structures:\n\nThe \"abgT\" gene is preceded by two genes, \"abgA\" and \"abgB,\" which code for homologous amino acyl amino hydrolases and hydrolyze \"p\"-aminobenzoyl glutamate to \"p\"-aminobenzoate and glutamate. Because of the structural similarity of \"p\"-aminobenzoyl-glutatmate to peptides, and the enzymatic activities of the \"abgA\" and \"abgB\" gene products, it has been suggested that AbgT is also a peptide transporter. Demonstration of an energy requirement suggested an H-dependent mechanism. Expression of these genes is regulated by AbgR and an unknown effector.\n\nAs noted above, the AbgT family of transporters has been thought to contribute to bacterial folate biosynthesis by importing the catabolite \"p\"-aminobenzoyl-glutamate for producing folate. Approximately 13,000 putative family members were identified in 2015. The X-ray structures of the full-length \"Alcanivorax borkumensis\" YdaH (AbgT) and \"Neisseria gonorrhoeae\" MtrF proteins. The structures revealed that these two transporters assemble as dimers with architectures distinct from all other families of transporters for which 3-d structures were available. Both YdaH and MtrF are bowl-shaped dimers with a solvent-filled basin extending from the cytoplasm halfway across the membrane bilayer. The protomers of YdaH and MtrF contain nine transmembrane helices and two hairpins which suggested a plausible pathway for substrate transport. A combination of the crystal structure, genetic analyses and substrate accumulation assays indicated that both YdaH and MtrF behave as exporters, capable of removing the folate metabolite \"p\"-aminobenzoic acid from bacterial cells. In fact, it was shown that both YdaH and MtrF participate as antibiotic efflux pumps, mediating bacterial resistance to sulfonamide antimetabolite drugs. Possibly, many AbgT-family transporters act as exporters, conferring resistance to sulfonamides.\n\nThe generalized transport reaction initially proposed for AbgT is:\"p\"-aminobenzoyl-glutamate (out) + nH (out) → \"p\"-aminobenzoyl-glutamate (in) + nH (in)but the more recently proposed transport reaction is:Sulfonamide drugs (in) + H (out) → Sulfonamide drugs (out) + H (in)\n\n",
    "id": "49633163",
    "title": "P-Aminobenzoyl-glutamate transporter"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=49733889",
    "text": "CitMHS family\n\nThe Citrate-Mg:H (CitM) / Citrate-Ca:H (CitH) Symporter (CitMHS) Family (TC# 2.A.11) is a family of transport proteins belonging to the Ion transporter superfamily. Members of this family are found in Gram-positive and Gram-negative bacteria, archaea and possibly eukaryotes. These proteins all probably arose by an internal gene duplication event. Lensbouer & Doyle (2010) have reviewed these systems, classifying the porters with three superfamilies, according to ion-preference:\n\n1) Mg-preferring,\n\n2) Ca-preferring, and\n\n3) Fe-preferring.\n\nA representative list of proteins belonging to the CitMHS family can be found in the Transporter Classification Database.\n\nTwo of the characterized members of the CitMHS family, both citrate uptake permeases from \"Bacillus subtilis\", are CitM (TC# 2.A.11.1.1) and CitH (TC# 2.A.11.1.2)\".\"\n\nCitM is believed to transport a citrate-Mgcomplex in symport with one H per Mg-citrate while CitH apparently transports a citrate-Ca complex in symport with protons. The cation specificity of CitM is: Mg, Mn, Ba, Ni, Co, Ca and Zn, in this preferential order. CitM is highly specific for citrate and D-isocitrate and does not transport other di- and tri-carboxylates including succinate, L-isocitrate, cis-aconitate and tricarballylate. For CitH, the cation specificity (in order of preference) is: Ca, Ba and Sr. The two proteins are 60% identical, contain about 400 amino acyl residues and possess twelve putative transmembrane spanners. A CitM homologue in \"S. mutans\" transports citrate conjugated to Fe or Mn but not Ca, Mg or Ni.\n\nThe transport reactions catalyzed by (1) CitM and (2) CitH, respectively, are:\n\n",
    "id": "49733889",
    "title": "CitMHS family"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=49780148",
    "text": "MatC family\n\nThe Malonate Uptake (MatC) family (TC# 2.A.101) is a constituent of the ion transporter (IT) superfamily. It consists of proteins from Gram-negative and Gram-positive bacteria (e.g., Xanthomonas, Rhizobium and Streptomyces species), simple eukaryotes (e.g., \"Chlamydomonas reinhardtii\") and archaea (e.g., \"Methanococcus jannaschii\"). The proteins are of about 450 amino acyl residues in length with 12-14 putative transmembrane segments (TMSs). Closest functionally-characterized homologues are in the DASS (TC #2.A.47) family. One member of this family is a putative malonate transporter (MatC of \"Rhizobium leguminosarum\" bv \"trifolii\", TC# 2.A.101.1.2).\n\n\n",
    "id": "49780148",
    "title": "MatC family"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=49780593",
    "text": "Basic amino acid antiporter family\n\nThe Basic Amino Acid Antiporter (ArcD) family (TC# 2.A.118) is a constituent of the IT superfamily. This family consists of proteins from Gram-negative and Gram-positive bacteria (e.g., \"Streptococcus\", \"Escherichia\", \"Salmonella\", \"Fusobacterium\" and \"Borrelia\" species). The proteins are of about 480 amino acyl residues (aas) in length and have 10-12 putative transmembrane segments (TMSs). Functionally characterized homologues are in the DcuC (TC #2.A.61) and ArsB (TC #2.A.4) families. Some members of the family probably catalyze arginine/ornithine or citruline/ornithine antiport.\n\n",
    "id": "49780593",
    "title": "Basic amino acid antiporter family"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=49781126",
    "text": "NhaA family\n\nNa/H antiporter A (NhaA) family (TC# 2.A.33) contains a number of bacterial sodium-proton antiporter (SPAP) proteins. These are integral membrane proteins that catalyse the exchange of H for Na in a manner that is highly pH dependent. Homologues have been sequenced from a number of bacteria and archaea. Prokaryotes possess multiple paralogues. A representative list of the proteins that belong to the NhaA family can be found in the Transporter Classification Database.\n\nProteins of the NhaA family are of 300-700 amino acyl residues in length. NhaA of \"E. coli\" is a homeodimer, each subunit consisting of a bundle of 12 tilted transmembrane α-helices (TMSs).\n\nMolecular dynamics simulations of NhaA enabled proposal of an atomically detailed model of antiporter function. Three conserved aspartate residues are key to this proposed mechanism: Asp (D164) is the Na-binding site, D163 controls the alternating accessibility of this binding site to the cytoplasm or periplasm, and D133 is crucial for pH regulation.\n\nNa-H antiporters are integral membrane proteins that exchange Na for H across the cytoplasmic membrane and many intracellular membranes. They are essential for Na, pH, and volume homeostasis, which are processes crucial for cell viability. The \"E. coli\" protein probably functions in the regulation of the internal pH when the external pH is alkaline, and the protein effectively functions as a pH sensor. It also uses the H gradient to expel Na from the cell. Its activity is highly pH dependent.\n\nThe generalized transport reaction catalyzed by NhaA is:Na (in) + 2H (out) ⇌ Na (out) + 2H (in).\n\n\n",
    "id": "49781126",
    "title": "NhaA family"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=49781138",
    "text": "NhaB family\n\nThe NhaB family (TC# 2.A.34) belongs to the Ion Transporter (IT) Superfamily. A representative list of proteins belonging to the NhaB family can be found in the Transporter Classification Database.\n\nNhaB homologues are usually about 500 amino acyl residues (aas) in length and possess about 12 transmembrane α-helical spanners (TMSs), although some members differ in their number of TMSs. NhaB homologues also exhibit a region with limited sequence similarity to a 46 kDa membrane protein of unknown function from \"Mycobacterium leprae\" (spP46838) which is also homologous to a member of arsenate resistance pumps of bacteria, archaea and eukaryotes (TC# 3.A.4). Only Gram-negative bacterial proteins have been functionally characterized.\n\nThe \"E. coli\" NhaB is 58% identical to the orthologous \"Vibrio alginolyticus\" Na/H antiporter. Although the latter protein is predicted to exhibit 10 TMSs, construction of \"NhaB-phoA\" fusions led to evidence for a 9 TMS model with the N-terminus in the cytoplasm and the C-terminus in the periplasm. A centrally located aspartyl residue in the 3rd TMS is conserved in all members of the family and important for activity.\n\nThe generalized transport reaction catalyzed by NhaB of \"E. coli\" is:2 Na (in) + 3 H (out) ⇌ 2 Na (out) + 3 H (in).\n\n",
    "id": "49781138",
    "title": "NhaB family"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=49781596",
    "text": "NhaC family\n\nThe NhaC family (TC# 2.A.35) belongs to the Ion Transporter (IT) Superfamily. A representative list of proteins belonging to the NhaC family can be found in the Transporter Classification Database.\n\nTwo members of the NhaC family have been functionally characterized. One is believed to be a Na:H antiporter; the other is a malate·H:lactate·Na antiporter. Several paralogues are found in \"Vibrio cholerae,\" and two paralogues are found encoded in the completely sequenced genomes of both\"Haemophilus influenzae\" and \"Bacillus subtilis.\" \"E. coli\" lacks such a homologue. \"Pyrococcus\" species also have at least one homologue each. Thus, members of the NhaC family are found in both Gram-negative bacteria and Gram-positive bacteria as well as archaea. NhaC of \"B. firmus\" is 462 amino acyl residues long and possesses 12 putative transmembrane α-helical segments. MleN of \"B. subtilis\" (468 aas; TC# 2.A.35.1.2) also exhibits 12 putative TMSs.\n\nThe transport reaction catalyzed by NhaC is probably:Na+ (in) + nH+ (out) ⇌ Na+ (out) + nH+ (in). (n > 1)That catalyzed by MleN is probably:Malate (out) + H (out) + Lactate (in) + Na (in) ⇌ Malate (in) + H (in) + Lactate (out) + Na (out)\n\n\n",
    "id": "49781596",
    "title": "NhaC family"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=49781600",
    "text": "NhaD family\n\nThe NhaD family (TC# 2.A.62) belongs to the Ion Transporter (IT) Superfamily. A representative list of proteins belonging to the NhaD family can be found in the Transporter Classification Database.\n\nThe NhaD Na/H antiporter has been characterized from two \"Vibrio\" species: \"V. parahaemolyticus\" and \"V. cholerae\" and in the haloalkaliphile, \"Alkalimonas amylolytica\". These proteins and their homologues are 400-500 aas long and exhibit 10-13 TMSs. They catalyze Na/H and Li/H antiport. They exhibit activity at basic pH (8-10) with no activity at pH 7.5. The \"Amylolytica\" antiporter has low Na affinity and has optimal activity at 600 mM Na. Homologues are found in proteobacteria of all groups, \"Flavobacteria\" and \"Chlamydia.\" Distant homologues of the IT superfamily are ubiquitous.\n\nThe generalized reaction catalyzed by NhaD is:nH (in) + mNa (out) ⇌ nH (out) + mNa (in).\n\n\n",
    "id": "49781600",
    "title": "NhaD family"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=49781605",
    "text": "NhaE family\n\nThe NhaE family (TC# 2.A.111) belongs to the Ion Transporter (IT) Superfamily. A representative list of proteins belonging to the NhaE family can be found in the Transporter Classification Database.\n\nThe NhaH family consists of proteins from Gram-negative bacteria (e.g., \"Leptospira\", \"Azotobacter\", \"Neisseria\", \"Ralstonia\", \"Chlorobium\" and \"Rhizobial\" species). The proteins are of about 480 aas with 12-14 putative TMSs.\n\nAn open reading frame (ORF) from the genome of \"Neisseria meningitidis\" displaying similarity with the NhaE type of Na/H antiporters was expressed in \"E. coli\" and characterized for sodium transport ability. The \"N. meningitidis\" antiporter (NmNhaE) was able to complement an \"E. coli\" strain devoid of Na/H antiporters (KNabc) with respect to the ability to grow in the presence of high concentrations of NaCl or LiCl. Ion transport assays in everted vesicles prepared from the KNabc strain expressing NmNhaE from a plasmid confirmed its ability to translocate Na and Li.\n\nThe generalized reaction catalyzed by NhaE is,Na or Li (in) + H (out) ⇌ Na or Li (out) + H (in)\n\n",
    "id": "49781605",
    "title": "NhaE family"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=49810360",
    "text": "Monovalent cation:proton antiporter-1\n\nThe Monovalent Cation:Proton Antiporter-1 (CPA1) Family (TC# 2.A.36) is a large family of proteins derived from Gram-positive and Gram-negative bacteria, blue-green bacteria, archaea, yeast, plants and animals. The CPA1 family belongs to the VIC superfamily. Transporters from eukaryotes have been functionally characterized to catalyze Na:H exchange. Their primary physiological functions are thought to be in (1) cytoplasmic pH regulation, extruding the H generated during metabolism, and (2) salt tolerance (in plants), due to Na uptake into vacuoles. Bacterial homologues have also been found to facilitate Na:H antiport, but some also catalyze Li:H antiport or Ca:H antiport under certain conditions.\n\nThe phylogenetic tree for the CPA1 family shows three principal clusters. The first cluster includes proteins derived exclusively from animals, and all of the functionally characterized members of the family belong to this cluster. Of the two remaining clusters, one includes all bacterial homologues while the other includes one from \"Arabidopsis thaliana,\" one from \"Homo sapiens\" and two from yeast (\"S. cerevisiae\" and \"S. pombe\"). Several organisms possess multiple paralogues; for example, seven paralogues are found in \"C. elegans,\" and five are known in humans. Most of these paralogues are very similar in sequence, and they belong to the animal specific cluster.\n\nA representative list of proteins belonging to the CPA1 family can be found in the Transporter Classification Database.\n\nNumerous members of the CPA1 family have been sequenced, and these proteins vary substantially in size. The bacterial proteins have 520-550 amino acyl residues (aas) while eukaryotic proteins are generally larger, varying in size from 540-900 residues. They exhibit 10-12 putative transmembrane α-helical spanners (TMSs). A proposed topological model suggests that in addition to 12 TMSs, a region between TMSs 9 and 10 dips into the membrane to line the pore. However, one homologue, Nhx1 of \"S. cerevisiae\" (TC# 2.A.36.1.12)\",\" has an extracellular glycosylated C-terminus.\n\nUsing the mammalian NHE1 (TC# 2.A.36.1.1), it has been found that TMSs 4 and 9 as well as the extracellular loop between TMSs 3 and 4 are important for drug (amiloride- and benzoyl guanidinium-based derivatives) sensitivities. Mutations in these regions also affect transport activities. M4 and M9 therefore contain critical sites for both drug and cation recognition.\n\nThe generalized transport reaction catalyzed by functionally characterized members of the CPA1 family is:Na (out) + H (in) ⇌ Na (in) + H (out).\n\n\n",
    "id": "49810360",
    "title": "Monovalent cation:proton antiporter-1"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=49813299",
    "text": "Monovalent cation:proton antiporter-3\n\nThe Monovalent Cation (K or Na):Proton Antiporter-3 (CPA3) Family (TC# 2.A.63) is a member of the Na transporting Mrp superfamily. The CPA3 family consists of bacterial multicomponent K:H and Na:H antiporters. The best characterized systems are the PhaABCDEFG system of \"Sinorhizobium meliloti\" (TC# 2.A.63.1.1) that functions in pH adaptation and as a K efflux system, and the MnhABCDEFG system of \"Staphylococcus aureus\" (TC# 2.A.63.1.3) that functions as a Na efflux Na:H antiporter.\n\nA homologous, but only partially sequenced, system was earlier reported to catalyze Na:H antiport in an alkalophilic \"Bacillus\" strain. PhaA and PhaD are respectively homologous to the ND5 and ND4 subunits of the H-pumping (TC #3.D.1). Homologous protein subunits from \"E. coli\" NADH:quinone oxidoreductase can functionally replace MrpA and MrpD in \"Bacillus subtilis.\"\n\nHomologues of PhaA, B, C and D and Nha1, 2, 3 and 4 of an alkalophilic \"Bacillus\" strain are the Yuf(Mrp)T, U, V and D genes of \"Bacillus subtilis\". In this system, YufT is believed to be responsible for Na:H antiporter activity, but it does not have activity in the absence of other constituents of the operon.\n\nThe seven Pha proteins are of the following sizes (in #aas) and exhibit the following putative numbers of transmembrane α-helical spanners (TMSs): \nAll are predicted to be integral membrane proteins.\n\nCorresponding values for the \"S. aureus\" Mnh system are: \nIn view of the complexity of the system, large variation in subunit structure, and the homology with NDH family protein constituents, a complicated energy coupling mechanism, possibly involving a redox reaction, cannot be ruled out.\n\nNa or Li does, but K, Ca, and Mg do not, support significant antiport by the Gram-positive bacterial systems (TC# 2.A.6.3.1.2 and TC# 2.A.6.3.1.3). Na(Li)/H antiporters have alkaline pH optima and apparent K values for Na that are among the lowest reported for bacterial Na/H antiporters. Na/Hantiport consumes the pmf and therefore is probably electrogenic.\n\nYufF (MrpF) appears to catalyze cholate efflux, possibly by a Na symport mechanism. It plays a major role in Na extrusion and is required for initiation of sporulation. Additionally, another component of the operon, MrpF (equivalent to PhaF of \"R. meliloti\") has been implicated in choline and Na efflux. The MrpA-G proteins of \"B. subtilis\" have been shown to be present in a single multicomponent complex. They provide Na/H antiport activity and function in multiple compound resistance and pH homeostasis.\n\nThe generalized reaction believed to be catalyzed by CPA3 family members is:[K or Na] (in) + H (out) ⇌ [K or Na] (out) + H (in).\n\n",
    "id": "49813299",
    "title": "Monovalent cation:proton antiporter-3"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=49861393",
    "text": "Sequence graph\n\nIn comparative genomics, a sequence graph, also called an alignment graph, breakpoint graph, or adjacency graph, is a bidirected graph in which the vertices represent segments of DNA and the edges represent adjacency between segments in a genome. The segments are labeled by the DNA string they represent, and each edge connects the tail end of one segment with the head end of another segment. Each adjacency edge is labelled by a (possibly empty) string of DNA. Traversing a connected component of segments and adjacency edges (called a \"thread\") yields a sequence, which typically represents a genome or a section of a genome. The segments can be thought of as synteny blocks, with the edges dictating how to arrange these blocks in a particular genome, and the labelling of the adjacency edges representing bases that are not contained in synteny blocks. \n\nSequence graphs can be used to represent multiple sequence alignments with the addition of a new kind of edge representing homology between segments. For a set of genomes, one can create an acyclic breakpoint graph with a thread for each genome. For two segments formula_1 and formula_2, where formula_3,formula_4,formula_5, and formula_6 represent the endpoints of the two segments, homology edges can be created from formula_3 to formula_5 and formula_4 to formula_6 or from formula_3 to formula_6 and formula_4 to formula_5 - representing the two possible orientations of the homology. The advantage of representing a multiple sequence alignment this way is that it is possible to include inversions and other structural rearrangements that wouldn't be allowable in a matrix representation.\nIf there are multiple possible paths when traversing a thread in a sequence graph, multiple sequences can be represented by the same thread. This means it is possible to create a sequence graph that represents a population of individuals with slightly different genomes - with each genome corresponding to one path through the graph. These graphs have been proposed as a replacement for the reference human genome. \n",
    "id": "49861393",
    "title": "Sequence graph"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=49921046",
    "text": "H+, Na+-translocating pyrophosphatase family\n\nMembers of the H, Na-translocating Pyrophosphatase (M\"-PPase)\" Family (TC# 3.A.10) are found in the vacuolar (tonoplast) membranes of higher plants, algae, and protozoa, and in both bacteria and archaea. They are therefore ancient enzymes.\n\nFull-length members of the H-PPase family have been sequenced from numerous bacteria, archaea and eukaryotes. These H pumping enzymes, which are probably homodimeric, have been reported to fall into two phylogenetic subfamilies. One subfamily invariably contains a conserved cysteine (Cys) and includes all known K-independent H-PPases, while the other has another conserved cysteine (Cys) but lacks Cys and includes all known K-dependent H-PPases. All H-PPases require Mg, and those from plant vacuoles, acidocalcisomes of protozoa and fermentative bacteria require mM K. Those from respiratory and photosynthetic bacteria as well as archaea are less dependent upon K. However, exceptions may exist. It is not sure whether K is transported.\n\nThe archaeon, \"Methanosarcina\" \"mazei\" Gö1, encodes within its genome two H-translocating pyrophosphatases (PPases), Mvp1 and Mvp2. Mvp1 resembles bacterial PPases while Mvp2 resembles plant PPases. Mvp2 was shown to translocate 1 H per pyrophosphate hydrolyzed.\n\nSome PPases from \"Anaerostipes caccae\", \"Chlorobium limicola\", \"Clostridium tetani\", and \"Desulfuromonas acetoxidans\" have been identified as K-dependent Na transporters. Phylogenetic analysis led to the identification of a monophyletic clade comprising characterized and predicted Na-transporting PPases (Na-PPases) within the K-dependent subfamily. H-transporting PPases (H-PPases) are more heterogeneous and form at least three independent clades in both subfamilies.\n\nThe plant enzymes probably pump one H upon hydrolysis of pyrophosphate, thereby generating a proton motive force, positive and acidic in the tonoplast lumen. They establish a pmf of similar magnitude to that generated by the H-translocating ATPases in the same vacuolar membrane. The bacterial and archaeal proteins may catalyze fully reversible reactions, thus being able to synthesize pyrophosphate when the pmf is sufficient. The enzyme from \"R. rubrum\" contributes to the pmf when light intensity is insufficient to generate a pmf sufficient in magnitude to support rapid ATP synthesis. Both C-termini of the dimeric subunits of V-PPase are on the same side of the membrane, and they are close to each other. Transmembrane domain 6 of vacuolar H-pyrophosphatase appears to mediate both protein targeting and proton transport.\n\nThe generalized transport reaction catalyzed by H-PPases is:pyrophosphate (P) + HO + H (cytoplasm) → inorganic phosphate (2 P) + H (external milieu or vacuolar lumen).\n\nEukaryotic members of the H-PPase family are large proteins of about 770 amino acyl residues (aas) with fifteen or sixteen putative transmembrane α-helical spanners (TMSs). The N-termini are predicted to be in the vacuolar lumen while the C-termini are thought to be in the cytoplasm. These proteins exhibit a region that shows convincing sequence similarity to the regions surrounding the DCCD-sensitive glutamate in the C-terminal regions of the c-subunits of F-type ATPases (TC #3.A.2). The H-pyrophosphatase of \"Streptomyces coelicolor\" has been shown to have a 17 TMS topology with the substrate binding domain exposed to the cytoplasm. The C-terminus is hydrophilic with a single C-terminal TMS. The basic structure is believed to have 16 TMSs with several large cytoplasmic loops containing functional motifs. Several acidic residues in the \"Arabidopsis\" H-PPase have been shown to be important for function. Some plants possess closely related H-PPase isoforms. These enzymes have the enzyme commission number EC 3.6.1.1.\n\nLin et al. (2012) reported the crystal structure of a \"Vigna radiata\" H-PPase (VrH-PPase) in complex with a non-hydrolysable substrate analogue, imidodiphosphate (IDP), at 2.35 Å resolution. Each VrH-PPase subunit consists of an integral membrane domain formed by 16 transmembrane helices. IDP is bound in the cytosolic region of each subunit and trapped by numerous charged residues and five Mg ions. A previously undescribed proton translocation pathway is formed by six core transmembrane helices. Proton pumping can be initialized by PP(i) hydrolysis, and H is then transported into the vacuolar lumen through a pathway consisting of Arg 242, Asp 294, Lys 742 and Glu 301. Lin et al. (2012) proposed a working model of the mechanism for the coupling between proton pumping and PP(i) hydrolysis by H-PPases. Membrane-integral pyrophosphatases (M-PPases) are crucial for the survival of plants, bacteria, and protozoan parasites. They couple pyrophosphate hydrolysis or synthesis to Na or H pumping. The 2.6Å structure of \"Thermotoga maritima\" H-PPase in the resting state revealed a previously unknown solution for ion pumping. The hydrolytic center, 20 angstroms above the membrane, is coupled to the gate formed by the conserved Asp(243), Glu(246), and Lys(707) by an unusual 'coupling funnel' of six α helices. Helix 12 slides down upon substrate binding to open the gate by a simple binding-change mechanism. Below the gate, four helices form the exit channel. Superimposing helices 3 to 6, 9 to 12, and 13 to 16 suggests that M-PPases arose through gene triplication. By comparing the active sites, fluoride inhibition data and the various models for ion transport, Kajander et al. concluded that membrane-integral PPases probably use binding of pyrophosphate to drive pumping.\n\n",
    "id": "49921046",
    "title": "H+, Na+-translocating pyrophosphatase family"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=20097070",
    "text": "Sessility (motility)\n\nIn biology, sessility (in the sense of positional movement or \"motility\") refers to organisms that do not possess a means of self-locomotion and are normally immobile. This is distinct from the second meaning of sessility which refers to an organism or biological structure attached directly by its base without a stalk.\n\nSessile organisms can move through outside sources (such as water currents) but are usually permanently attached to something. Organisms such as corals lay down their own substrate from which they grow. Other organisms grow from a solid such as a rock, dead tree trunk, or a manmade object such as a buoy or ship's hull.\n\nSessile animals typically have a motile phase in their development. Sponges have a motile larval stage, which becomes sessile at maturity. In contrast, many jellyfish develop as sessile polyps early in their life cycle. In the case of the cochineal, it is in the nymph stage (also called the crawler stage) that the cochineal disperses. The juveniles move to a feeding spot and produce long wax filaments. Later they move to the edge of the cactus pad where the wind catches the wax filaments and carries the tiny larval cochineals to a new host.\n\nMany sessile animals, including sponges, corals and hydra, are capable of asexual reproduction \"in situ\" by the process of budding. Sessile organisms such as barnacles and tunicates need some mechanism to move their young into new territory. This is why the most widely accepted theory explaining the evolution of a larval stage is the need for long-distance dispersal ability. Wayne Sousa's 1979 study in intertidal disturbance added support for the theory of nonequilibrium community structure, “suggesting that open space is necessary for the maintenance of diversity in most communities of sessile organisms.”\n\nClumping is a behavior in sessile organisms, in which individuals of a particular species group closely to one another for beneficial purposes, and can be seen in coral reefs and cochineal populations. This allows for faster reproduction and better protection from predators.\n\nThe circalittoral zone of coastal environments and biomes are dominated by sessile organisms such as oysters. Carbonate platforms grow due to the buildup of skeletal remains of sessile organisms, usually microorganisms, which induce carbonate precipitation through their metabolism. \n\nIn anatomy and botany, \"sessility\" refers to an organism or biological structure that has no \"peduncle\" or stalk. See Peduncle (anatomy), Peduncle (botany) and Sessility (botany). A sessile structure has no stalk.\n\n",
    "id": "20097070",
    "title": "Sessility (motility)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=49516442",
    "text": "Phylogenetic inertia\n\nPhylogenetic inertia or phylogenetic constraint refers to the limitations on the future evolutionary pathways that have been imposed by previous adaptations.\n\nCharles Darwin first recognized this phenomenon, though the term was later coined by Huber in 1939. Darwin explained the idea of phylogenetic inertia based on his observations; he spoke about it when explaining the \"Law of Conditions of Existence\". Darwin also suggested that, after speciation, the organisms do not start over from scratch, but have characteristics that are built upon already existing ones that were inherited from their ancestors; and these characteristics likely limit the amount of evolution seen in that new taxa. This is the main concept of phylogenetic inertia.\n\nRichard Dawkins also explained these constraints by likening natural selection to a river in his 1982 book \"The Extended Phenotype\".\n\n\nBirds are the only speciose group of vertebrates that are exclusively oviparous, or egg laying. It has been suggested that birds are phylogenetically constrained, as being derived from reptiles, and likely have not overcome this constraint or diverged far enough away to develop viviparity, or live birth.\n\n\n\nThere have been several studies that have been able to effectively test for phylogenetic inertia when looking into shared traits; predominantly with a comparative methods approach. Some have used comparative methods and found evidence for certain traits attributed to adaptation, and some to phylogeny; there were also numerous traits that could be attributed to both. Another study developed a new method of comparative examination that showed to be a powerful predictor of phylogenetic inertia in a variety of situations. It was called Phylogenetic Eigenvector Regression (PVR), which runs principal component analyses between species on a pairwise phylogenetic distance matrix. In another, different study, the authors described methods for measuring phylogenetic inertia, looked at effectiveness of various comparative methods, and found that different methods can reveal different aspects of drivers. Autoregression and PVR showed good results with morphological traits.\n",
    "id": "49516442",
    "title": "Phylogenetic inertia"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=50349553",
    "text": "FAM63B\n\nFAM63B is a protein which in humans is encoded by the gene FAM63B. This gene is highly expressed in humans. The FAM63B gene is also highly conserved throughout evolutionary history. The discovered function of FAM63B is an interaction with the kinesin-1 light chain and the transportation of vaccinia virus from the nucleus to the cell periphery.\n\nFAM63B is located at 15q21.3-q22.1, spanning 90,707 base pairs on chromosome 15.\n\nThe full name of FAM63B is family with sequence similarity 63, member B. FAM63B is also listed by its alias, KIAA1164, in some publications.\n\nThe FAM63B gene encodes a primary transcript that can be alternatively spliced into 9 protein variants. FAM63B variant a is the most common isoform found in humans.\n\nFAM63B is a member of the Pfam super family, and contains a domain of unknown function (DUF544) that is homologous within the protein family. FAM63B protein variant an also contains a bipartite tryptophan binding motif from W476 to W533. Variant a of the protein also contains a hydrophobic stretch of alanine from 567 to 574 and a mixed charge sequence from residue 598 to 617. FAM63B protein may contain a signal sequence specifying return to the endoplasmic reticulum (KDEL) from residue 607 to 621 in variant a.\n\nThe secondary structure of FAM63B is a combination of coils, some α-helices, and few β-sheets. The Phyre 2 program predicts α-helices in 23% of the protein, β-strands in 9% of the protein, and the remaining 59% of the protein as disordered. The disordered regions coincide with the coiled regions predicted by other programs, and this results in the long stretch of coiled protein beginning at the N-terminus. According to the SOUSI program, there is a 16-amino acid-long span from residues 265 to 280 of FAM63B that could be a transmembrane sequence. However, transmembrane sequences generally need to be at least 20 amino acids long in order to be stable in the membrane, so a transmembrane sequence is unlikely. Therefore, FAM63B is not fixed in the membrane of any organelle and is free to move through the cell and between organelles.\n\nNot much is known about the tertiary structure of FAM63B. A predicted folding is shown.\nPost-translational modifications of the FAM63B protein.\nFAM63B has predicted NES (nuclear export signals) at Val274 and Leu277. Also, a NLS (nuclear localization signal) is predicted for FAM63B at RKRK at residue 599. In agreement, Reinhardt’s method for cytoplasmic/nuclear discrimination predicts FAM63B to be located in the nucleus with a reliability of 76.7%. The presence of both NLS and NES signals and O-GlcNAc post-translational modification of FAM63B supports the protein's location in both the nucleus and cytoplasm and the discovered protein function as a shuttle for vaccinia virus between the nucleus and the cell periphery.\n\nFAM63B has moderately-high to high expression and is constitutively expressed. FAM63B is likely ubiquitously expressed in humans.\n\nExpression of FAM63B is high in the embryonic stem cells and differentiated tissues but low or off in embryoid bodies and other progenitor cells, such as the multipotent mesenchymal stem cells. It is likely that FAM63B is expressed during pluripotency and unipotency but is not important for differentiation, as is occurring in embryoid bodies, mesenchymal stem cells, and other progenitor cells.\n\nThe promoter of FAM63B is GXP_5885, located on the positive strand of chromosome 15 from (58770692, 58771462) and is 711 base pairs long.\n\nFAM63B is shown to interact with one protein, KLC-1. KLC-1, kinesin light chain 1, is a protein which recruits kinesin-1 via its cargo binding light chain and contains a bipartite tryptophan binding motif. This motif is present in a vaccinia virus integral membrane protein, A36, that is required for transport of the virus from the perinuclear space to the cell periphery. In the absence of A36, proteins with a bipartite tryptophan binding motif can interact with the kinesin light chain, recruit KLC-1, and promote virus transport from the nucleus to the cytoplasm.\n\nThe discovered function of FAM63B protein is a transporter of vaccinia virus in the human genome. FAM63B contains a bipartite tryptophan binding motif between W476 and W533. The motif also contains a Q residue at the +2 position, which was found to be a frequent occurrence in proteins that bind KLC-1 or KLC-2. FAM63B is among proteins studied that can rescue virus transport to the cell periphery when expressed in A36-deficient cells, successfully replacing the cytoplasmic domain A36 of vaccinia.\n\nThe specific pathology of FAM63B is unknown.\n\nFAM63B is part of four networks regulated by miRNA, three of which are linked to neuronal differentiation and dopaminergic gene expression. These findings indicate that FAM63B could be used as a biomarker for the detection and treatment of schizophrenia. Furthermore, aberrant methylation of FAM63B may play a role in the development of schizophrenia. FAM63B has also been ranked 13 of 25 on a list of associated genes relevant to arthritis.\n\nFAM63B has one paralog, FAM63A, which is a gene of unknown function. FAM63A gene encodes a protein that is 469 amino acids long and 76% similar to FAM63B.\n\nFAM63B has been found in all multicellular and unicellular eukaryotes, including plants but excluding protists and fungi. The gene has also been found in archaea but not bacteria.\nThe most distant homolog of FAM63B is found in \"Thermoplasmatales archaeon\", an archaea that diverged from the human gene 4.25 billion years ago.\n\nFAM63B is a member of the Pfam super family, and contains a domain of unknown function (DUF544) homologous within the protein family. This region of the protein is highly conserved through FAM63B homologs, as is the bipartite tryptophan binding motif of FAM63B and the C-terminus signal sequence.\n\nThe phylogenetic tree below shows a time calibration for the evolution of FAM63B.\n",
    "id": "50349553",
    "title": "FAM63B"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=50529615",
    "text": "Merodiploid\n\nMerodiploid is a partially diploid bacterium, which has its own chromosome complement and a chromosome fragment introduced by conjugation, transformation or transduction. It can also be defined as an essentially haploid organism that carries a second copy of a part of its genome.The term is derived from the Greek, meros = part, and was originally used to describe both unstable partial diploidy, such as that which occurs briefly in recipients after mating with an Hfr strain (1), and the stable state, exemplified by F-prime strains (see Hfr’S And F-Primes). Over time the usage has tended to confine the term to descriptions of stable genetic states.\n",
    "id": "50529615",
    "title": "Merodiploid"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=50550012",
    "text": "Biological rules\n\nA biological rule or biological law is a generalized law, principle, or rule of thumb formulated to describe patterns observed in living organisms. Biological rules and laws are often developed as succinct, broadly applicable ways to explain complex phenomena or salient observations about the ecology and biogeographical distributions of plant and animal species around the world, though they have been proposed for or extended to all types of organisms. Many of these regularities of ecology and biogeography are named after the biologists who first described them.\n\nFrom the birth of their science, biologists have sought to explain apparent regularities in observational data. In his biology, Aristotle inferred rules governing differences between live-bearing tetrapods (in modern terms, terrestrial placental mammals). Among his rules were that brood size decreases with adult body mass, while lifespan increases with gestation period and with body mass, and fecundity decreases with lifespan. Thus, for example, elephants have smaller and fewer broods than mice, but longer lifespan and gestation. Rules like these concisely organized the sum of knowledge obtained by early scientific measurements of the natural world, and could be used as models to predict future observations.\n\nAmong the earliest biological rules in modern times are those of Karl Ernst von Baer (from 1828 onwards) on embryonic development, and of Constantin Wilhelm Lambert Gloger on animal pigmentation, in 1833.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere is some scepticism among biogeographers about the usefulness of general rules. For example, J.C. Briggs, in his 1987 book \"Biogeography and Plate Tectonics\", comments that while Willi Hennig's rules on cladistics \"have generally been helpful\", his progression rule is \"suspect\".\n",
    "id": "50550012",
    "title": "Biological rules"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=50274456",
    "text": "HUMARA assay\n\nHUMARA Assay is one of the most widely used methods to determine the clonal origin of a tumor. The method is based on X chromosome inactivation and it takes the advantage of having different methylation status of a gene called HUMARA (short for Human Androgen receptor) that is located on X chromosome. Considering the fact that once one X chromosome is inactivated in a cell, all other cells derived from it will have the same X chromosome inactivated, this approach becomes a great tool to differentiate a monoclonal population from a polyclonal one in a female tissue. HUMARA gene, in particular, has three important features that make it highly convenient for the purpose.\n\n1-) The gene is located on X chromosome and it goes through inactivation by methylation in normal embryogenesis of a female infant. The fact that most-but not all-genes on X chromosome undergo inactivation, this feature becomes an important one.\n\n2-) Human Androgen Receptor gene alleles have varying numbers of CAG repeats. Thus, when DNA from a healthy female tissue is amplified by PCR for a specific region of the gene, two separated bands can be seen on the gel.\n\n3-) The region that is amplified by PCR also has certain base orders that make it susceptible to be digested by HpaII (or HhaI) enzyme when it is not methylated. This detail gives the opportunity to researchers to differentiate a methylated allele from the unmethylated one.\n\nThanks to these qualities of HUMARA gene, clonal origin of any tissue from a female mammalian organism can be determined.\n\nThe basic process is performed as the following :\n\n1-) DNA from the tissue is isolated.\n\n2-) The isolated DNA is treated with the suitable enzyme (such as HpaII) in optimal conditions for a suggested amount of time (i.e. overnight).\n\n3-) DNA is cleaned and the certain region of HUMARA gene is amplified by PCR using \"suitable\" primers (as an example, please see:Ref.2)\n\n4-) After running PCR products through a gel, the gel is visualized and the results are analyzed accordingly. If two bands are apparent, the tissue studied is most likely of polyclonal origin. If a single band is observed, the tissue is monoclonal unless two alleles have exactly the same numbers of CAG repeats or different cells with the same inactivated initiated the tumor; so, seemingly monoclonal although it is actually polyclonal.\n\nIn order to make a conclusion about the clonality of a tumor, it is best to use the DNA from a normal tissue of the same person, and a sample without enzyme treatment must also be amplified as a control. If, even in normal tissues without enzyme treatment, a single band is observed, it may be explained as follows; this person has the genetic pattern as XO (this possibility can be excluded to see a band after enzyme treatment because, if indeed XO is the genetic pattern of the sample, then, there will be NO methylation, therefore no band should be visible after digesting with the enzyme. In the case of seeing a band after enzyme treatment, the observation is most likely to mean that the person has two X chromosomes with exact CAG repeats.) When you see two bands for normal tissue (both enzyme treated and untreated), and you see two bands for enzyme treated tumor sample but two bands for untreated tumor DNA, then you are surely looking at a polyclonal tumor. However, if you see the same number of bands only with a single band after enzyme treatment, there is a rather high chance for the tumor of your interest to be monoclonal. In this last case, monoclonality is not for sure because, as stated earlier, there is the possibility of having exact same -CAG- repeats on both alleles.\n",
    "id": "50274456",
    "title": "HUMARA assay"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=50533059",
    "text": "Use of beta-adrenergic agonists livestock\n\nBeta-adrenergic agonists, or β-agonists, are non-hormonal growth promotants that help animals put on muscle instead of fat. Ractopamine (brand names include Optaflexx and Paylean) and zilpaterol (brand name Zilmax) received FDA approval in 1999 and 2003, respectively. They are also approved in Mexico, South Africa, and Canada. 160 countries restrict the importation of beef which has been raised with β-agonists. Temple Grandin was one of the first to describe the potential problems with the supplement. Cattle may arrive at the plant with a stiff gait, acting like they have both stiff muscles and sore feet. The problem could be muscle fatigue. Zilpateral enhances the growth of “fast-twitch” fibers, a type of muscle fiber that fatigues more easily. On August 16, 2013 Merck and Co., the makers of Zilmax, suspended the sale of the product in the US and Canada. But problems may exist with all β-agonists supplementation, and not just for animals.\n\nThe family of β-agonists includes β1-, β2-, and β3-agonists such as clenbuterol, ractopamine, cimaterol, zilpaterol and salbutamol. They possess similar chemical structures with different substituent groups on the phenyl ethanolamines. Due to their significant nutrition redistribution function, β-agonists have been applied in the livestock industry such as pigs and ruminants to reduce carcass fat and increase muscle mass while improving growth rate and feed conversion (Bareille and Faverdin, 1996, Bergen et al., 1989, Byrem et al., 1998, Cardoso and Taveira, 2002, Shook et al., 2009 and Williams et al., 1987). Unfortunately, β-agonists deposited in animal tissues can cause acute poisoning when consumed by humans, particularly in people with symptoms of muscular tremors, cardiac palpitation, nervousness, headache, muscular pain, dizziness, nausea, vomiting, fever and chills (Brambilla et al., 2000). Therefore, the use of β-agonists in animal feeds has been banned in many countries such as the EU and China (Mitchell and Dunnavan, 1998 and Prezelj et al., 2003). There are still illicit usages of β-agonists in animal feeds in many countries.\n\nBroad screening and identification of β-agonists in feed and animal body fluid and tissues using ultra-high performance liquid chromatography-quadrupole-orbitrap high resolution mass spectrometry combined with spectra library search\n• Tingting Lia, 1, \n• Jingjing Caob, 1, \n• Zhen Lib, \n• Xian Wanga, \n• Pingli Hea, \n\nBroad screening and identification of β-agonists in feed, serum, urine, muscle and liver samples was achieved in a quick and highly sensitive manner using ultra high performance liquid chromatography-quadrupole-orbitrap high resolution mass spectrometry (UHPLC-Q-Orbitrap HRMS) combined with a spectra library search. Solid-phase extraction technology was employed for sample purification and enrichment. After extraction and purification, the samples were analyzed using a Q-Orbitrap high-resolution mass spectrometer under full-scan and data-dependent MS/MS mode. The acquired mass spectra were compared with an in-house library (compound library and MS/MS mass spectral library) built with TraceFinder Software which contained the M/Z of the precursor ion, chemical formula, retention time, character fragment ions and the entire MS/MS spectra of 32 β-agonist standards. Screening was achieved by comparing 5 key mass spectral results and positive matches were marked. Using the developed method, the identification results from 10 spiked samples and 238 actual samples indicated that only 2% of acquired mass spectra produced false identities. The method validation results showed that the limit of detection ranged from 0.021–3.854 μg kg−1and 0.015–1.198 ng mL−1 for solid and liquid samples, respectively.\n",
    "id": "50533059",
    "title": "Use of beta-adrenergic agonists livestock"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=50238405",
    "text": "Lorquin Entomological Society\n\nThe Lorquin Entomological Society is a century-old association of professional and amateur entomologists, biologists and naturalists that meet regularly to study and promote entomology and natural history, especially about wildlife in and near Southern California.\n\nThe Lorquin Natural History Club was started in June 1913 by Fordyce Grinnell Jr. A constitution committee was formed in July, and a constitution adopted at the August meeting. It was named after French entomologist Pierre Joseph Michel Lorquin, who collected specimens in California during the California Gold Rush. Early members included not only entomologists like Grinnell, but herpetologists, botanists, an ornithologist, a conchologist, a geologist, and seismologist Charles F. Richter, whose specialty at the time was astronomy. Monthly meetings were held in private homes.\n\nThe group was renamed the Lorquin Entomological Club in 1917 and its meetings moved to the Los Angeles Public Library. In 1919 the club began meeting in the Southwest Museum in Highland Park. Early club activities included field trips into the hills and canyons beyond the termini of the Red Cars of the Pacific Electric and the Yellow Cars of the Los Angeles Railway. It was an era when a butterfly collector could net over 500 in a single day and take over 100 species in the Los Angeles area. The club held their first Butterfly Show February 24–26, 1921, at the Southwest Museum. Among other things it featured a lecture on butterfly hunting, illustrated by stereopticon slides, by museum director John Adams Comstock.\n\nThe Butterfly Show became an annual, month-long event, sponsored by the Museum of History, Science, and Art. In 1926, the Southwest Museum narrowed its scope. Natural history exhibits and specimens were transferred to the Museum of History, Science, and Art. In January 1927, the club moved its meetings there as well, and changed its name to the Lorquin Entomological Society. Comstock, who had been director of the Southwest Museum from 1921 to 1926, published his landmark \"Butterflies of California\" in 1927. Early in 1928 he became Acting Director of the Museum of History, Science, and Art. He served several terms as president of the society. Under his influence, and that of his colleague Charles Montagu Dammers, the society's emphasis evolved from collecting to life history studies. Members in the late 1920s and early 1930s included biologists John Shrader Garth, Jeane Daniel Gunder, Lloyd M. Martin, and Don Meadows.\n\nIn 1929, the society decided that California should name a state insect. They prepared ballots listing three butterfly candidates and sent them to entomologists throughout the state. The nominees were the Lorquin's admiral (\"Limenitis lorquini\"), the California sister (\"Heterochroa californica\"), and the California dog head or Flying pansy (\"Zerene eurydice\"). The California dog head won handily, with 77 votes out of 88 cast. The state legislature took note, and the Bureau of Entomology in the California Department of Agriculture began to use a likeness of the California dog head, labeled \"California State Insect\" on its documents. No further action took place, however, until 1972, when Assemblyman Kenneth L. Maddy introduced Assembly Bill No. 1843 to make what was by then known as the California dogface the official state insect. Governor Ronald Reagan signed the bill into law on July 28, 1972, making California the first in the nation to have a state insect.\n\nIn 1986, Steven R. Kutcher, a member of the society, organized the first annual Insect Fair at the Los Angeles County Arboretum and Botanic Garden. It featured exhibits by the society and others, demonstrations, and lectures. When the fair outgrew the Arboretum, it was moved in 1989 to the Natural History Museum of Los Angeles County (the erstwhile Museum of History, Science, and Art). Eventually the Natural History Museum took over organizing the fair and has done so since. What is now known as Bug Fair, and bills itself as \"the biggest bug festival in North America\", draws 20,000 visitors to its interactive exhibits, vendor tables, and insect menus.\n\nThe society has counted as members many academics, authors, and museum curators in the field of entomology, such as brothers John F. and Thomas C. Emmel, Cristopher Henne, Charles L. Hogue and son James N. 'Jim' Hogue, Noel McFarland, and Rudolf H. T. Mattioni. Members are active in research, environmental protection, habitat restoration, species surveys, entomological outreach, and natural history education. As of 2013, membership stood at just over 100. Regular meetings with speakers are held on the fourth Friday of every month, at BioQuip (a biological supply house in Rancho Dominguez) since remodeling ousted the society from the Natural History Museum in 2009.\n\nThe club published eighteen monthly issues of \"Lorquinia\" between August 1916 and January 1919. They contained botanical and entomological papers and notes. Since about 1955, it has published a newsletter (10 issues per year), most recently called \"NetWork\". In 1999, it published a book by Robert Lee Allen, \"Stalking the wild arthropod: The Lorquin Entomological Society's guide to photographing arthropods\".\n\n\n",
    "id": "50238405",
    "title": "Lorquin Entomological Society"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=42940",
    "text": "Biostasis\n\nBiostasis is the ability of an organism to tolerate environmental changes without having to actively adapt to them. Biostasis is found in organisms that live in habitats that likely encounter unfavorable living conditions, such as drought, freezing temperatures, change in pH levels, pressure, or temperature. Insects undergo a type of dormancy to survive these conditions, called diapause. Diapause may be obligatory for these insects to survive. The insect may also be able to undergo change prior to the arrival of the initiating event.\n\nBiostasis is also used as a synonym for the terms cryostasis or cryonics. Cryonics is a medical procedure concept that is a well established trope in Science Fiction, that may at some point in the future become a possible option for individuals who are terminally ill. The patients would be frozen in what is known as cryonic suspension. The patients are frozen so that when technology advances both to enable them to be unfrozen and brought back to life but also for their illness to be treated, they can be re-animated and their disease can be cured or treated.\n\nMedical biostasis can be put to use in humans to help repair brain damage. \n\nThere is evidence that suggests that medical biostasis procedures could be performed by trauma surgeons by 2026.\n",
    "id": "42940",
    "title": "Biostasis"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=43868891",
    "text": "Social Bonding and Nurture Kinship\n\nSocial Bonding and Nurture Kinship: Compatibility between Cultural and Biological Approaches is a book on human kinship and social behavior by Maximilian Holland, published in 2012. The work synthesizes the perspectives of evolutionary biology, psychology and sociocultural anthropology towards understanding human social bonding and cooperative behavior. It presents a theoretical treatment that many consider to have resolved longstanding questions about the proper place of genetic (or 'blood') connections in human kinship and social relations, and a synthesis that \"\"should inspire more nuanced ventures in applying Darwinian approaches to sociocultural anthropology\"\". The book has been called \"\"A landmark in the field of evolutionary biology\"\" which \"\"gets to the heart of the matter concerning the contentious relationship between kinship categories, genetic relatedness and the prediction of behavior\"\", \"\"places genetic determinism in the correct perspective\"\" and serves as \"\"a shining example of what can be achieved when excellent scholars engage fully across disciplinary boundaries.\"\"\n\nThe aim of the book is to show that \"\"properly interpreted, cultural anthropology approaches (and ethnographic data) and biological approaches are perfectly compatible regarding processes of social bonding in humans.\"\" Holland's position is based on demonstrating that the dominant biological theory of social behavior (inclusive fitness theory) is typically misunderstood to predict that genetic ties are necessary for the \"expression\" of social behaviors, whereas in fact the theory only implicates genetic associations as necessary for the \"evolution\" of social behaviors. Whilst rigorous evolutionary biologists have long understood the distinction between these levels of analysis (see Tinbergen's four questions), past attempts to apply inclusive fitness theory to humans have often overlooked the distinction between \"evolution\" and \"expression\".\n\nBeyond its central argument, the broader philosophical implications of Holland's work are considered by commentators to be that it both \"\"helps to untangle a long-standing disciplinary muddle\"\" and \"\"clarifies the relationship between biological and sociocultural approaches to human kinship.\"\" It is claimed that the book \"\"demonstrates that an alternative non-deterministic interpretation of evolutionary biology is more compatible with actual human social behavior and with the frameworks that sociocultural anthropology employs\"\" and as a consequence, delivers \"\"a convincing, solid and informed blow to the residual genetic determinism that still influences the interpretation of social behaviour.\"\"\n\nThe book's form consists of a cumulative argument (using a wide range of supporting evidence) made over nine chapters, with each chapter ending in a brief retrospective summary, and the final chapter containing a recapitulation and summary of the whole, and drawing some wider conclusions.\n\nHolland begins by tracing transitions in the history of anthropological theories of social behavior and kinship, noting the varying importance with which 'blood ties' have been understood to be a necessary element of human kinship and social relations. He suggests that whilst the mounting ethnographic evidence has led to a move away from the 'blood kinship' concept in recent decades, many sociocultural anthropologists still query the connection between kinship and blood, reproduction or some other apparently biological functions. Meanwhile, many biologists, biological anthropologists and evolutionary psychologists have persisted in viewing human kinship and cooperative behavior as necessarily associated with genetic relationships and 'blood ties'. The current situation has been characterized as \"\"a clash between incommensurate paradigms, holding as they may, completely incompatible ideas about human nature.\"\" Holland argues that a clear resolution to these questions is still outstanding, and would therefore be of value. In closing the introduction, Holland writes; \"The approach is not reductive. The claim is rather that a thorough investigation of the ‘biological facts’ can be useful mainly though allowing a change in focus... away from confusion about the place of genealogy in social ties, and onto a reformulated baseline, built around varied \"processual aspects of social bonding.\"\"\n\nThe book reviews the background and key elements of Hamilton's inclusive fitness theory from the 1960s onwards, setting out its significant conceptual and heuristic value. Holland notes that Hamilton acknowledged that his earliest and most widely known account (1964) contained technical inaccuracies. He also notes Hamilton's early speculations about possible proximate mechanisms of the expression of social behavior (\"supergenes\" as a possible alternative to \"behaviour-evoking-situations\") contained errors that have nevertheless remained very influential in popular accounts. Specifically, the supergenes notion (sometimes called the \"Green-beard effect\") - that organisms may evolve genes that are able to identify identical copies in others and preferentially direct social behaviours towards them - was theoretically clarified and withdrawn by Hamilton in 1987. However, in the intervening years, the notion that supergenes (or more often, simply individual organisms) have evolved \"to identify genetic relatives and preferentially cooperate with them\" took hold, and became the way many biologists came to understand the theory. This persisted, despite Hamilton's 1987 correction. In Holland's view it is the pervasiveness of this longstanding but erroneous perspective, and the suppression of the alternative 'behaviour-evoking-situations' perspective regarding social expression mechanisms, that is largely responsible for the ongoing clash between biological and sociocultural approaches to human kinship.\n\nHolland shows that, in the 1970s and 80s, the first wave of attempts (known as \"human sociobiology\" or \"Darwinian anthropology\") to apply inclusive fitness theory to human social behavior relied on, and further reinforced, this same misinterpretation (above section) about the theory's predictions and the proximate mechanisms of social behavior. Holland also shows that this period of research was burdened with many misplaced assumptions about \"universal\" attributes of the human sexes, sexuality and gender roles, apparently projected from the \"specific cultural values\" of the researchers themselves. Holland also shows that, following the perceived failures of this early wave, and particularly its methodological agnosticism regarding proximate mechanisms of social behavior, the evolutionary psychology school grew up in its place. Although this latter school typically avoided engaging with the ethnographic data on human kinship, Holland argues that in the few cases where it did so, it repeated the misinterpretation of inclusive fitness theory that characterized the first wave. Holland also notes that Kitcher, in his critique of the sociobiological position, suggested that perhaps the expression of social behaviors in humans might quite simply be based on cues of context and familiarity, rather than genetic relatedness \"per se\".\n\nChapters four and five investigate further the theory and evidence surrounding the \"proximate mechanisms\" of social behavior; specifically the question of whether social behaviors are expressed by organisms via \"behaviour-evoking-situations\" or via direct detection of actual genetic relatedness. Related questions have been the domain of kin recognition theory. Holland notes that the name 'kin recognition' itself suggests some expectation that a positive identification of genetic relatedness is a prediction of inclusive fitness theory, and is thus expected. Similar points have been made by others; \"many behavioural ecologists seem to implicitly assume that specialised mechanisms allowing individuals to distinguish their kin from non-kin must have evolved.\" Again, the possibility that \"behaviour-evoking-situations\" might be the more parsimonious mechanism of the expression of social behavior, and fully compatible with inclusive fitness theory, has often been underemphasized. However, Holland's review of the evidence notes that field studies in this area quickly established that \"behaviour-evoking-situations\" do in fact overwhelmingly mediate social behaviours in those species studied, and that, particularly in mammal species, social bonding and familiarity formed in early developmental contexts (e.g. in burrows or nesting sites) are a common mediating mechanism for social behaviors, independently of genetic relatedness \"per se\". On the basis of the preceding theoretical analysis and review of evidence, at the end of chapter five, Holland argues that;\nIt is entirely erroneous, both in reference to theory and in reference to the evidence, to claim or suggest that 'the facts of biology' support the claim that organisms have evolved to cooperate with genetic relatives \"per se\".\n\nHaving argued for the above position on the lack of necessity for genetic relatedness \"per se\" to mediate social bonding and behavior, Holland suggests that \"The further question then is; can we uncover in any greater detail how familiarity and other context-dependent cues operate?\". To discover the extent to which the variety of human kinship behaviors may nevertheless be compatible with this (less deterministic) interpretation of biological theory of social behavior, Holland suggests that a survey of primates' most fundamental social patterns may give clues, especially those of species most closely connected with humans. The variety of primate mating systems, group-membership ('philopatry') patterns, and life-cycle patterns are reviewed. Holland finds that;\n\nLike other mammals, Catarrhini primate demographics are strongly influenced by ecological conditions, particularly density and distribution of food sources... Cohesive social groups and delayed natal dispersal mean that maternally related individuals, including maternal siblings, face a statistically reliable context of interaction in all Catarrhini primates. This reliable context of interaction with maternally related individuals is extended amongst those species with female philopatry (especially Cercopithecinae).\n\nAs with other social mammals, evidence suggest that the reliability of 'behaviour-evoking-situations' this social context provides has shaped the mechanisms of proximate expression of social bonding and behavior;\n\nAdoption of infants by females (and sometimes males) demonstrates that care-giving and bonding to infants is not mediated by positive powers of discrimination. From the infant's perspective, it will bond with any responsive carer. If not necessarily the actual mother, in natural conditions this will often be a maternal relative (particularly an older sibling), but the context is primary, not the actual relatedness. Similarly, social bonding and social behaviours between maternal siblings (and occasionally between other maternal relatives) is context-driven in primates, and mediated via the care-giver.\n\nHolland also notes how Bowlby and colleagues' attachment theory was strongly informed by primate bonding patterns and mechanisms, and that in Bowlby's later writing the then emerging inclusive fitness theory was explicitly linked to. \n[Bowlby's] work demonstrated that social attachments form on the basis of provision of care, and responsiveness to elicitations for care. The social context of living together and the familiarity this brings, provides the circumstance within which social bonds can form...\n\nOn the basis of combining more recent primate research with the findings of attachment theory, Holland proposes that \"In attempting to define more specific forms of \"the giving of care and nurture\" which may mediate social bonding we [find] that provision of food is likely to play a part, as well as the more intangible provision of warmth and comfort, and a safe base for sleeping.\"\n\nHolland claims that, while biological theory of social behavior is not deterministic in respect of genetic relatedness vis-a-vis the formation of social bonds and expression of social behaviors, evidence does point to compatibility between a non-reductive interpretation of the theory and how such bonds and behaviors operate in social mammals, primates and in humans. In the final part of the book, Holland explores the extent to which this perspective is also compatible with sociocultural anthropology's ethnographic accounts of human kinship and social behavior, both occasional accounts from the past, as well as more contemporary accounts that have explicitly eschewed the earlier 'blood ties' assumption. Holland finds that;\n\nMany contemporary accounts focus on social bonds formed in childhood and the importance of the performance of acts of care, including food provision, in mediating these bonds. In all cases it is this performance of care which is considered the overriding factor in\nmediating social bonds, notwithstanding 'blood ties'. In short, there is strong compatibility between the perspectives on social bonding that emerge from a proper account of biological theory and those documented by ethnographers.\n\nHolland's concluding chapter gives a summary of his fundamental position;\n\nA crucial implication of this argument taken as a whole is that \"the expression of the kinds of social behaviours treated by inclusive fitness theory does not require genetic relatedness.\" Sociobiology and evolutionary psychology's claims that biological science predicts that organisms \"will\" direct social behaviour towards relatives are thus both \"theoretically\" and \"empirically\" erroneous. Such claims and their supporting arguments also give a highly misleading and reductive account of basic biological theory. Properly interpreted, \"cultural anthropology approaches (and ethnographic data) and biological approaches are perfectly compatible\" regarding processes of social bonding in humans. Most of all, this requires a focus on the circumstances and processes which lead to social bonding.\n\nThe book notes that, as an outcome of the analysis, Schneider's sociocultural perspective on human kinship is vindicated;\n\nDo the biological facts have some priority or are they but one of the conditions, like ecology, economy, demography, etc., to which kinship systems must adapt? Take note: if the latter is the case, then kinship must be as much rooted in these other conditions as in the biological facts.\n\nThe author supplies several examples of the insight that Schneider's broad approach can provide. The book closes with an example of a clash of cultural perspectives on kinship and family norms, and makes the suggestion that;\n\nConstructing from narrow cultural particulars (Euro-American or otherwise) an essentialised model of 'human nature' does not constitute science; it is closer to cultural colonialism. In any analysis intended to shed light on proposed universals of the human condition, reflexivity is essential, and cultural and biological approaches both surely necessary.\n\nKinship theorist and member of the national academy of science, Robin Fox wrote of the work:\n\nAn excellent and constructive discussion of matters in kinship and its cultural and biological components, handsomely reconciling what have been held to be incompatible positions.Max Holland gets to the heart of the matter concerning the contentious relationship between kinship categories, genetic relatedness and the prediction of behavior. If he had been in the debate in the 1980s then a lot of subsequent confusion could have been avoided\"\n\nIrwin Bernstein, distinguished research professor in the university of Georgia's \"Behavioral and Brain Sciences Program\" made the following comment on Holland's book:\n\nMax Holland has demonstrated extraordinarily thorough scholarship in his exhaustive review of the often contentious discussions of kinship. He has produced a balanced synthesis melding the two approaches exemplified in the biological and sociocultural behavioral positions. His work in reconciling opposing views clearly demonstrates the value of interdisciplinary approaches. This should be the definitive word on the subject.\n\nPhilip Kitcher, John Dewey Professor of Philosophy, and James R. Barker Professorship of Contemporary Civilization at Columbia University, past president of the American Philosophical Association and inaugural winner of the Prometheus Prize, stated of the book:\n\nMax Holland has provided a wide-ranging and deeply-probing analysis of the influence of genetic relatedness and social context on human kinship. He argues that while genetic relatedness may play a role in the evolution of social behavior, it does not determine the forms of such behavior. His discussion is exemplary for its thoroughness, and should inspire more nuanced ventures in applying Darwinian approaches to sociocultural anthropology.\n\nKirk Endicott, professor emeritus of anthropology at the university of Dartmouth, wrote that Holland's book was:\n\nA brilliant discussion of the relationship between kinship and social bonding as understood in evolutionary biology and in sociocultural anthropology. Among other contributions, it debunks the common misconception that biological evolution involves individual organisms actively pursuing the goal of increasing the numbers of their genes in successive generations, the measure of their so-called ‘individual inclusive fitness’. Holland demonstrates that an alternative non-deterministic interpretation of evolutionary biology is more compatible with actual human social behavior and with the frameworks that sociocultural anthropology employs.\n\nJanet Carsten, kinship theorist and professor of anthropology at the university of Edinburgh stated that:\n\nThis book is a scholarly attempt to get beyond the often sterile oppositions between evolutionary and culturalist approaches to kinship. In bringing together two sides of the debate, it constitutes a valuable contribution to kinship studies.\n\nIn a review for the journal Critique of Anthropology, Nicholas Malone concluded that:\n\nLucid and effective... Holland has produced a significant work of scholarship that will be of interest to a wide swath of the anthropological community.\n\nCommenting on the book for the journal Social Analysis, Anni Kajanus found that:\n\nHolland has done an excellent and thorough job in reviewing the disciplinary and interdisciplinary histories of approaches to kinship and social bonds in anthropology, biology, and psychology. Most importantly, he clarifies the different levels of analysis when looking at human behavior in real time and in the evolutionary time frame. This makes the book essential reading for anyone who acknowledges that human relatedness and social bonds are shaped by the\nevolved dispositions of our species, their development through the life-course of an individual, and our specific cultural-historical environments... Holland’s book goes a long way toward clarifying and therefore advancing these theoretical debates\n\nAn in-depth review of the book by primatologist Augusto Vitale, in the journal \"Folia Primatologica\", found that:\n\nStuart Semple, evolutionary anthropologist, reviewing the book in the journal \"Acta Ethologica\" stated that:\n\nAs someone who teaches behavioural ecology to biologists, and primate biology to social and biological anthropologists, I will be strongly recommending this book to all of my advanced undergraduates, masters and PhD students, as well as to my colleagues. Not only does it help to resolve debates that have run for many years, but it is also an outstanding example of what can be achieved by immersing oneself in literature from different fields, while retaining an intellectual openness and exercising incisive analysis. Many of us talk enthusiastically about inter- and multi-disciplinarity, but often this is not much more than lip service. This book is a shining example of what can be achieved when excellent scholars engage fully across disciplinary boundaries. There should be more texts like this.\n\nIn addition to praise for the book's significance, the Folia Primatologica review noted that the book is at times too dense and requires close reading;\nThe argument here and there becomes too detailed and tortuous, but it is absolutely captivating... [Colleagues] who are less used to extremely detailed theoretical reasoning, will find it difficult at the beginning...\n\n",
    "id": "43868891",
    "title": "Social Bonding and Nurture Kinship"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=51066217",
    "text": "Methyl green\n\nMethyl green (CI 42585) is a cationic or positive charged stain, related to Ethyl green, that has been used for staining DNA since the XIX century. It has been used for staining cell nuclei either as a part of the classical Unna-Pappenheim stain, or as a nuclear counterstain ever since.<br>\nIn recent years, its fluorescent properties when bound to DNA have positioned it useful for far-red imaging of live cell nuclei. \nFluorescent DNA staining is routinely used in cancer prognosis.<br>\nMethyl green also emerges as an alternative stain for DNA in agarose gels, fluorometric assays and flow cytometry. It has also been shown that it can be used as an exclusion viability stain for cells.\nIts interaction with DNA has been shown to be non-intercalating, in other words not inserting itself into the DNA, but instead electrostatic with the DNA major groove. It is used in combination with pyronin in the methyl green–pyronin stain which stains and differentiates DNA and RNA.\n",
    "id": "51066217",
    "title": "Methyl green"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=51179574",
    "text": "Tactoid\n\nTactoids are liquid crystal microdomains nucleated in isotropic phases, which can be distinguished as spherical or spindle-shaped birefringent microdroplets under polarized light microscopy. Tactoids are a transition state between isotropic and macroscopic liquid crystalline phases. The first observation of tactoids was made by Zocher in 1925, when he studied the nematic phase formed in vanadium pentoxide sols. After that, tactoids have been found in the phase transition processes in many lyotropic liquid crystalline substances, such as tobacco mosaic virus, polypeptides, and cellulose nanocrystals.\n",
    "id": "51179574",
    "title": "Tactoid"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=51299313",
    "text": "293T\n\n293T (or HEK293T) is a human cell line, derived from the HEK 293 cell line, that expresses a mutant version of the SV40 large T antigen. It is very commonly used in biology for protein expression and production of recombinant retroviruses.\n\n293T was created in Michele Calos's lab at Stanford by stable transfection of the HEK 293 cell line with a plasmid encoding a temperature-sensitive mutant of the SV40 large T antigen; it was originally referred to as 293/\"tsA1609neo\". The first reference to the cell line as \"293T\" may be its use to create the BOSC23 packaging cell line for producing retroviral particles.\n\nThe transfection used to create 293T (involving plasmid pRSV-1609) conferred neomycin/G418 resistance and expression of the tsA1609 allele of SV40 large T antigen; this allele is fully active at 33 °C (its permissive temperature), has substantial function at 37 °C, and is inactive at 40 °C. 293T is very efficiently transfectable with DNA (like its parent HEK 293). Due to the expression of SV40 large T antigen, transfected plasmid DNAs that carry the SV40 origin of replication can replicate in 293T and will transiently maintain a high copy number; this can greatly increase the amount of recombinant protein or retrovirus that can be produced from the cells.\n\nThe full genome sequences of three different isolates of 293T have been determined. They are quite similar to each other but show detectable divergence from the parental HEK 293 cell line.\n\n",
    "id": "51299313",
    "title": "293T"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=38076",
    "text": "Gender\n\nGender is the range of characteristics pertaining to, and differentiating between, masculinity and femininity. Depending on the context, these characteristics may include biological sex (i.e., the state of being male, female, or an intersex variation), sex-based social structures (i.e., gender roles), or gender identity. People who do not identify as having a male or female gender are often grouped under the umbrella terms \"non-binary\" or \"genderqueer\". Some cultures have specific gender roles that are distinct from male and female, such as the hijras of South Asia. These are often referred to as \"third gender\".\n\nSexologist John Money introduced the terminological distinction between biological sex and gender as a role in 1955. Before his work, it was uncommon to use the word \"gender\" to refer to anything but grammatical categories. However, Money's meaning of the word did not become widespread until the 1970s, when feminist theory embraced the concept of a distinction between biological sex and the social construct of gender. Today the distinction is strictly followed in some contexts, especially the social sciences and documents written by the World Health Organization (WHO).\n\nIn other contexts, including some areas of social sciences, \"gender\" includes \"sex\" or replaces it. For instance, in non-human animal research, \"gender\" is commonly used to refer to the biological sex of the animals. This change in the meaning of gender can be traced to the 1980s. In 1993, the US Food and Drug Administration (FDA) started to use \"gender\" instead of \"sex\". Later, in 2011, the FDA reversed its position and began using \"sex \"as the biological classification and \"gender\" as \"a person's self representation as male or female, or how that person is responded to by social institutions based on the individual's gender presentation.\"\n\nThe social sciences have a branch devoted to gender studies. Other sciences, such as sexology and neuroscience, are also interested in the subject. While the social sciences sometimes approach gender as a social construct, and gender studies particularly do, research in the natural sciences investigates whether biological differences in males and females influence the development of gender in humans; both inform debate about how far biological differences influence the formation of gender identity. In the English literature, there is also a trichotomy between biological sex, psychological gender, and social gender role. This framework first appeared in a feminist paper on transsexualism in 1978.\n\nThe modern English word \"gender\" comes from the Middle English \"gender\" (also \"gendere\", \"gendir\" \"gendyr\", \"gendre\"), a loanword from Anglo-Norman and Middle French \"gendre\". This, in turn, came from Latin \"genus\". Both words mean \"kind\", \"type\", or \"sort\". They derive ultimately from a widely attested Proto-Indo-European (PIE) root \"gn-\",\nwhich is also the source of \"kin\", \"kind\", \"king\", and many other English words. It appears in Modern French in the word \"genre\" (type, kind, also \"\") and is related to the Greek root \"gen-\" (to produce), appearing in \"gene\", \"genesis\", and \"oxygen\". The first edition of the Oxford English Dictionary (OED1, Volume 4, 1900) notes the original meaning of \"gender\" as \"kind\" had already become obsolete.\n\nThe word was still widely attested, however, in the specific sense of grammatical gender (the assignment of nouns to categories such as \"masculine\", \"feminine\" and \"neuter\"). According to Aristotle, this concept was introduced by the Greek philosopher Protagoras.\nIn 1926, Henry Watson Fowler stated that the definition of the word pertains to this grammar-related meaning: \n\nThe modern academic sense of the word, in the context of social roles of men and women, dates at least back to 1945, and was popularized and developed by the feminist movement from the 1970s onwards (see § Feminism theory and gender studies below). The theory was that human nature is essentially epicene and social distinctions based on sex are arbitrarily constructed. Matters pertaining to this theoretical process of social construction were labelled matters of \"gender\".\n\nThe popular use of \"gender\" simply as an alternative to \"sex\" (as a biological category) is also widespread, although attempts are still made to preserve the distinction. The American Heritage Dictionary (2000) uses the following two sentences to illustrate the difference, noting that the distinction \"is useful in principle, but it is by no means widely observed, and considerable variation in usage occurs at all levels.\"\n\nIn the last two decades of the 20th century, the use of \"gender\" in academia has increased greatly, outnumbering uses of \"sex\" in the social sciences. While the spread of the word in science publications can be attributed to the influence of feminism, its use as a synonym for sex is attributed to the failure to grasp the distinction made in feminist theory, and the distinction has sometimes become blurred with the theory itself; David Haig stated, \"Among the reasons that working scientists have given me for choosing gender rather than sex in biological contexts are desires to signal sympathy with feminist goals, to use a more academic term, or to avoid the connotation of copulation.\"\n\nIn legal cases alleging discrimination, \"sex\" is usually preferred as the determining factor rather than \"gender\" as it refers to biology rather than socially constructed norms which are more open to interpretation and dispute. Julie Greenberg writes that although gender and sex are separate concepts, they are interlinked in that gender discrimination often results from stereotypes based on what is expected of members of each sex. In \"J.E.B. v. Alabama ex rel. T.B.\", United States Supreme Court Justice Antonin Scalia wrote:\n\n\"Gender identity\" refers to a personal identification with a particular gender and gender role in society. The term \"woman\" has historically been used interchangeably with reference to the female body, though more recently this usage has been viewed as controversial by some feminists.\n\n<nowiki> </nowiki>There are qualitative analyses that explore and present the representations of gender; however, feminists challenge these dominant ideologies concerning gender roles and biological sex. One's biological sex is directly tied to specific social roles and the expectations. Judith Butler considers the concept of being a woman to have more challenges, owing not only to society's viewing women as a social category but also as a felt sense of self, a culturally conditioned or constructed subjective identity. \"Social identity\" refers to the common identification with a collectivity or social category that creates a common culture among participants concerned. According to social identity theory, an important component of the self-concept is derived from memberships in social groups and categories; this is demonstrated by group processes and how inter-group relationships impact significantly on individuals' self perception and behaviors. The groups people belong to therefore provide members with the definition of who they are and how they should behave within their social sphere.\n\nCategorizing males and females into social roles creates a problem, because individuals feel they have to be at one end of a linear spectrum and must identify themselves as man or woman, rather than being allowed to choose a section in between. Globally, communities interpret biological differences between men and women to create a set of social expectations that define the behaviors that are \"appropriate\" for men and women and determine women’s and men’s different access to rights, resources, power in society and health behaviors. Although the specific nature and degree of these differences vary from one society to the next, they still tend to typically favor men, creating an imbalance in power and gender inequalities within most societies. Many cultures have different systems of norms and beliefs based on gender, but there is no universal standard to a masculine or feminine role across all cultures. Social roles of men and women in relation to each other is based on the cultural norms of that society, which lead to the creation of gender systems. The gender system is the basis of social patterns in many societies, which include the separation of sexes, and the primacy of masculine norms.\n\nPhilosopher Michel Foucault said that as sexual subjects, humans are the object of power, which is not an institution or structure, rather it is a signifier or name attributed to \"complex strategical situation\". Because of this, \"power\" is what determines individual attributes, behaviors, etc. and people are a part of an ontologically and epistemologically constructed set of names and labels. Such as, being female characterizes one as a woman, and being a woman signifies one as weak, emotional, and irrational, and is incapable of actions attributed to a \"man\". Butler said that gender and sex are more like verbs than nouns. She reasoned that her actions are limited because she is female. \"I am not permitted to construct my gender and sex willy-nilly,\" she said. \"[This] is so because gender is politically and therefore socially controlled. Rather than 'woman' being something one is, it is something one does.\" More recent criticisms of Judith Butler's theories critique her writing for reinforcing the very conventional dichotomies of gender.\n\nAccording to gender theorist Kate Bornstein, gender can have ambiguity and fluidity. There are two contrasting ideas regarding the definition of gender, and the intersection of both of them is definable as below:\n\nThe World Health Organization defines gender as the result of socially constructed ideas about the behavior, actions, and roles a particular sex performs. The beliefs, values and attitude taken up and exhibited by them is as per the agreeable norms of the society and the personal opinions of the person is not taken into the primary consideration of assignment of gender and imposition of gender roles as per the assigned gender. Intersections and crossing of the prescribed boundaries have no place in the arena of the social construct of the term \"gender\".\n\nThe assignment of gender involves taking into account the physiological and biological attributes assigned by nature followed by the imposition of the socially constructed conduct. The social label of being classified into one or the other sex is necessary for the medical stamp on birth certificates. \"Gender\" is a term used to exemplify the attributes that a society or culture constitutes as \"masculine\" or \"feminine\". Although a person's sex as male or female stands as a biological fact that is identical in any culture, what that specific sex means in reference to a person's gender role as a woman or a man in society varies cross culturally according to what things are considered to be masculine or feminine. These roles are learned from various, intersecting sources such as parental influences, the socialization a child receives in school, and what is portrayed in the local media. It is also important to note that learning gender roles starts from birth and includes seemingly simple things like what color outfits a baby is clothed in or what toys they are given to play with. The cultural traits typically coupled to a particular sex finalize the assignment of gender and the biological differences which play a role in classifying either sex as interchangeable with the definition of gender within the social context.\n\nIn this context, the socially constructed rules are at a cross road with the assignment of a particular gender to a person. Gender ambiguity deals with having the freedom to choose, manipulate and create a personal niche within any defined socially constructed code of conduct while gender fluidity is outlawing all the rules of cultural gender assignment. It does not accept the prevalence of the two rigidly defined genders \"man\" and \"woman\" and believes in freedom to choose any kind of gender with no rules, no defined boundaries and no fulfilling of expectations associated with any particular gender.\n\nBoth these definitions are facing opposite directions with their own defined set of rules and criteria on which the said systems are based.\n\nSexologist John Money coined the term \"gender role\" in 1955. The term \"gender role\" is defined as the actions or responses that may reveal their status as boy, man, girl or woman, respectively. Elements surrounding gender roles include clothing, speech patterns, movement, occupations, and other factors not limited to biological sex. In contrast to taxonomic approaches, some feminist philosophers have argued that gender \"is a vast orchestration of subtle mediations between oneself and others\", rather than a \"private cause behind manifest behaviours\".\n\nBecause social aspects of gender can normally be presumed to be the ones of interest in sociology and closely related disciplines, \"gender role\" is often abbreviated to \"gender\" in their literature.\n\nTraditionally, most societies have only recognized two distinct, broad classes of gender roles, masculine and feminine, that correspond with the biological sexes of male and female. When a baby is born, society allocates the child to one gender or the other, on the basis of what their genitals resemble. However, some societies explicitly incorporate people who adopt the gender role opposite to their biological sex; for example, the two-spirit people of some indigenous American peoples. Other societies include well-developed roles that are explicitly considered more or less distinct from archetypal female and male roles in those societies. In the language of the sociology of gender, they comprise a third gender, more or less distinct from biological sex (sometimes the basis for the role does include intersexuality or incorporates eunuchs).\nOne such gender role is that adopted by the hijras of India and Pakistan. Another example may be the muxe (pronounced ), found in the state of Oaxaca, in southern Mexico. The Bugis people of Sulawesi, Indonesia have a tradition that incorporates all the features above.\n\nIn addition to these traditionally recognized third genders, many cultures now recognize, to differing degrees, various non-binary gender identities. People who are non-binary (or genderqueer) have gender identities that are not exclusively masculine or feminine. They may identify as having an overlap of gender identities, having two or more genders, having no gender, having a fluctuating gender identity, or being third gender or other-gendered. Recognition of non-binary genders is still somewhat new to mainstream Western culture, and non-binary people may face increased risk of assault, harassment, and discrimination.\n\nJoan Roughgarden argues that some non-human animal species also have more than two genders, in that there might be multiple templates for behavior available to individual organisms with a given biological sex.\n\nEarly gender identity research hypothesized a single bipolar dimension of masculinity-femininity, with masculinity and femininity being opposites on one continuum. Assumptions of the unidimensional model were challenged as societal stereotypes changed, which led to the development of a two-dimensional gender identity model. In the model, masculinity and femininity were conceptualized as two separate and orthogonal dimensions, coexisting in varying degrees within an individual. This conceptualization on femininity and masculinity remains the accepted standard today.\n\nTwo instruments incorporating the multidimensional nature of masculinity and femininity have dominated gender identity research: The Bem Sex Role Inventory (BSRI) and the Personal Attributes Questionnaire (PAQ). Both instruments categorize individuals as either being sex typed (males report themselves as identifying primarily with masculine traits, females report themselves as identifying primarily with feminine traits), cross sex-typed (males report themselves as identifying primarily with feminine traits, females report themselves as identifying primarily with masculine traits), androgynous (either males or females who report themselves as high on both masculine and feminine traits) or undifferentiated (either males or females who report themselves as low on both masculine and feminine traits). Twenge (1997) noted that men are generally more masculine than women and women generally more feminine than men, but the association between biological sex and masculinity/femininity is waning.\n\nBiologist and feminist academic Anne Fausto-Sterling rejects the discourse of biological versus social determinism and advocates a deeper analysis of how interactions between the biological being and the social environment influence individuals' capacities. The philosopher and feminist Simone de Beauvoir applied existentialism to women's experience of life: \"One is not born a woman, one becomes one.\" In context, this is a philosophical statement. However, it may be analyzed in terms of biology—a girl must pass puberty to become a woman—and sociology, as a great deal of mature relating in social contexts is learned rather than instinctive.\n\nWithin feminist theory, terminology for gender issues developed over the 1970s. In the 1974 edition of \"Masculine/Feminine or Human\", the author uses \"innate gender\" and \"learned sex roles\", but in the 1978 edition, the use of \"sex\" and \"gender\" is reversed.\nBy 1980, most feminist writings had agreed on using \"gender\" only for socioculturally adapted traits.\n\nIn gender studies the term \"gender\" refers to proposed social and cultural constructions of masculinities and femininities. In this context, \"gender\" explicitly excludes reference to biological differences, to focus on cultural differences. This emerged from a number of different areas: in sociology during the 1950s; from the theories of the psychoanalyst Jacques Lacan; and in the work of French psychoanalysts like Julia Kristeva, Luce Irigaray, and American feminists such as Judith Butler. Those who followed Butler came to regard gender roles as a practice, sometimes referred to as \"performative\".\n\nCharles E. Hurst states that some people think sex will, \"...automatically determine one's gender demeanor and role (social) as well as one's sexual orientation (sexual attractions and behavior). Gender sociologists believe that people have cultural origins and habits for dealing with gender. For example, Michael Schwalbe believes that humans must be taught how to act appropriately in their designated gender to fill the role properly, and that the way people behave as masculine or feminine interacts with social expectations. Schwalbe comments that humans \"are the results of many people embracing and acting on similar ideas\". People do this through everything from clothing and hairstyle to relationship and employment choices. Schwalbe believes that these distinctions are important, because society wants to identify and categorize people as soon as we see them. They need to place people into distinct categories to know how we should feel about them.\n\nHurst comments that in a society where we present our genders so distinctly, there can often be severe consequences for breaking these cultural norms. Many of these consequences are rooted in discrimination based on sexual orientation. Gays and lesbians are often discriminated against in our legal system because of societal prejudices. Hurst describes how this discrimination works against people for breaking gender norms, no matter what their sexual orientation is. He says that \"courts often confuse sex, gender, and sexual orientation, and confuse them in a way that results in denying the rights not only of gays and lesbians, but also of those who do not present themselves or act in a manner traditionally expected of their sex\". This prejudice plays out in our legal system when a person is judged differently because they do not present themselves as the \"correct\" gender.\n\nAndrea Dworkin stated her \"commitment to destroying male dominance and gender itself\" while stating her belief in radical feminism.\n\nPolitical scientist Mary Hawkesworth addresses gender and feminist theory, stating that since the 1970s the concept of gender has transformed and been used in significantly different ways within feminist scholarship. She notes that a transition occurred when several feminist scholars, such as Sandra Harding and Joan Scott, began to conceive of gender \"as an analytic category within which humans think about and organize their social activity\". Feminist scholars in Political Science began employing gender as an analytical category, which highlighted \"social and political relations neglected by mainstream accounts\". However, Hawkesworth states \"feminist political science has not become a dominant paradigm within the discipline\".\n\nAmerican political scientist Karen Beckwith addresses the concept of gender within political science arguing that a \"common language of gender\" exists and that it must be explicitly articulated in order to build upon it within the political science discipline. Beckwith describes two ways in which the political scientist may employ 'gender' when conducting empirical research: \"gender as a category and as a process.\" Employing gender as a category allows for political scientists \"to delineate specific contexts where behaviours, actions, attitudes and preferences considered masculine or feminine result in particular\" political outcomes. It may also demonstrate how gender differences, not necessarily corresponding precisely with sex, may \"constrain or facilitate political\" actors. Gender as a process has two central manifestations in political science research, firstly in determining \"the differential effects of structures and policies upon men and women,\" and secondly, the ways in which masculine and feminine political actors \"actively work to produce favorable gendered outcomes\".\n\nWith regard to gender studies, Jacquetta Newman states that although sex is determined biologically, the ways in which people express gender is not. Gendering is a socially constructed process based on culture, though often cultural expectations around women and men have a direct relationship to their biology. Because of this, Newman argues, many privilege sex as being a cause of oppression and ignore other issues like race, ability, poverty, etc. Current gender studies classes seek to move away from that and examine the intersectionality of these factors in determining people's lives. She also points out that other non-Western cultures do not necessarily have the same views of gender and gender roles. Newman also debates the meaning of equality, which is often considered the goal of feminism; she believes that \"equality\" is a problematic term because it can mean many different things, such as people being treated identically, differently, or fairly based on their gender. Newman believes this is problematic because there is no unified definition as to what equality means or looks like, and that this can be significantly important in areas like public policy.\n\nSociologists generally regard gender as a social construct, and various researchers, including many feminists, consider sex to only be a matter of biology and something that is not about social or cultural construction. For instance, sexologist John Money suggests the distinction between biological sex and gender as a role. Moreover, Ann Oakley, a professor of sociology and social policy, says \"the constancy of sex must be admitted, but so also must the variability of gender.\" The World Health Organization states, \"'[s]ex' refers to the biological and physiological characteristics that define men and women,\" and \"'gender' refers to the socially constructed roles, behaviours, activities, and attributes that a given society considers appropriate for men and women.\" Thus, sex is regarded as a category studied in biology (natural sciences), while gender is studied in humanities and social sciences. Lynda Birke, a feminist biologist, maintains \"'biology' is not seen as something which might change.\" Therefore, it is stated that sex is something that does not change, while gender can change according to social structure.\n\nHowever, there are scholars who argue that sex is also socially constructed. For example, gender theorist Judith Butler states that \"perhaps this construct called 'sex' is as culturally constructed as gender; indeed, perhaps it was always already gender, with the consequence that the distinction between sex and gender turns out to be no distinction at all.\"\nShe continues:It would make no sense, then, to define gender as the cultural interpretation of sex, if sex is itself a gender-centered category. Gender should not be conceived merely as the cultural inscription of meaning based on a given sex (a juridical conception); gender must also designate the very apparatus of production whereby the sexes themselves are established. [...] This production of sex as the pre-discursive should be understood as the effect of the apparatus of cultural construction designated by gender.\n\nButler argues that \"bodies only appear, only endure, only live within the productive constraints of certain highly gendered regulatory schemas,\" and sex is \"no longer as a bodily given on which the construct of gender is artificially imposed, but as a cultural norm which governs the materialization of bodies.\" Marria Lugones states that, among the Yoruba people, there was no concept of gender and no gender system at all before colonialism. She argues that colonial powers used a gender system as a tool for domination and fundamentally changing social relations among the indigenous.\n\nWith regard to history, Linda Nicholson, a professor of history and women's studies, says that the notion of human bodies being separated into two sexes is not historically consistent. She argues that male and female genitals were considered inherently the same in Western society until the 18th century. At that time, female genitals were regarded as incomplete male genitals, and the difference between the two was conceived as a matter of degree. In other words, there was a gradation of physical forms, or a spectrum. Therefore, the current perspective toward sex, which is to consider women and men and their typical genitalia as the only possible natural options, came into existence through historical, not biological roots.\n\nIn addition, drawing from the empirical research of intersex children, Anne Fausto-Sterling, a professor of biology and gender studies, describes how the doctors address the issues of intersexuality. She starts her argument with an example of the birth of an intersexual individual and maintains \"our conceptions of the nature of gender difference shape, even as they reflect, the ways we structure our social system and polity; they also shape and reflect our understanding of our physical bodies.\" Then she adds how gender assumptions affects the scientific study of sex by presenting the research of intersexuals by John Money et al., and she concludes that \"they never questioned the fundamental assumption that there are only two sexes, because their goal in studying intersexuals was to find out more about 'normal' development.\" She also mentions the language the doctors use when they talk with the parents of the intersexuals. After describing how the doctors inform parents about the intersexuality, she asserts that because the doctors believe that the intersexuals are actually male or female, they tell the parents of the intersexuals that it will take a little bit more time for the doctors to determine whether the infant is a boy or a girl. That is to say, the doctors' behavior is formulated by the cultural gender assumption that there are only two sexes. Lastly, she maintains that the differences in the ways in which the medical professionals in different regions treat intersexual people also give us a good example of how sex is socially constructed. In her \"Sexing the body: gender politics and the construction of sexuality\", she introduces the following example: A group of physicians from Saudi Arabia recently reported on several cases of XX intersex children with congenital adrenal hyperplasia (CAH), a genetically inherited malfunction of the enzymes that aid in making steroid hormones. [...] In the United States and Europe, such children, because they have the potential to bear children later in life, are usually raised as girls. Saudi doctors trained in this European tradition recommended such a course of action to the Saudi parents of CAH XX children. A number of parents, however, refused to accept the recommendation that their child, initially identified as a son, be raised instead as a daughter. Nor would they accept feminizing surgery for their child. [...] This was essentially an expression of local community attitudes with [...] the preference for male offspring.\n\nThus it may be said that determining the sex of children is actually a cultural act, and the sex of children is in fact socially constructed. Therefore, it is possible that although sex seems fixed and only related to biology, it may be actually deeply related to historical and social factors as well as biology and other natural sciences.\n\nAnother work of Ann Fausto-Sterling’s in which she discusses gender is \"The Five Sexes: Why Male and Female Are Not Enough.\" In this article, Fausto-Sterling states that Western culture has only two sexes and that even their language restricts the presence of more than two sexes. She argues that instead of having a binomial nomenclature for organizing humans into two distinct sexes (male and female), there are at least five sexes in the broad spectrum of gender. These five sexes include male, female, hermaphrodite, female pseudohermaphrodites (individuals who have ovaries and some male genitalia but lack testes), and male pseudohermaphrodites (individuals who have testes and some female genitalia but lack ovaries). Fausto-Sterling additionally adds that in the category of hermaphrodites, there are additional degrees and levels in which the genitalia are developed; this means that there may be more intersexes that exist in this continuum of gender.\n\nFausto-Sterling argues that sex has been gradually institutionally disciplined into a binary system through medical advances. She brings up multiple instances where gender in history was not split into strictly male or female, Fausto-Sterling mentioned that by the end of the Middle Age, intersex individuals were forced to pick a side in the binary gender code and to adhere by it. She then adds on that \"hermaphrodites have unruly bodies\" and they need to fit into society's definition of gender. Thus, modern-day parents have been urged by medical doctors to decide the sex for their hermaphroditic child immediately after childbirth. She emphasizes that the role of the medical community is that of an institutionalized discipline on society that there can only be two sexes: male and female and only the two listed are considered \"normal.\" Lastly, Fausto-Sterling argues that modern laws require humans to be labelled either as male or female and that \"ironically, a more sophisticated knowledge of the complexity of sexual systems has led to the repression of such intricacy.\" She mentions this quote to inform the prevailing thought that hermaphrodites, without medical intervention, are assumed to live a life full of psychological pain when in fact, there is no evidence in which that is the case. She finishes up her argument asking what would happen if society started accepting intersex individuals.\n\nThe article \"Adolescent Gender-Role Identity and Mental Health: Gender Intensification Revisited\" focuses on the work of Heather A. Priess, Sara M. Lindberg, and Janet Shibley Hyde on whether or not girls and boys diverge in their gender identities during adolescent years. The researchers based their work on ideas previously mentioned by Hill and Lynch in their gender intensification hypothesis in that signals and messages from parents determine and affect their children’s gender role identities. This hypothesis argues that parents affect their children's gender role identities and that different interactions spent with either parents will affect gender intensification. Priess and among other’s study did not support the hypothesis of Hill and Lynch which stated \"that as adolescents experience these and other socializing influences, they will become more stereotypical in their gender-role identities and gendered attitudes and behaviors.\" However, the researchers did state that perhaps the hypothesis Hill and Lynch proposed was true in the past but is not true now due to changes in the population of teens in respect to their gender-role identities.\n\nAuthors of \"Unpacking the Gender System: A Theoretical Perspective on Gender Belief’s and Social Relations\", Cecilia Ridgeway and Shelley Correll, argue that gender is more than an identity or role but is something that is institutionalized through \"social relational contexts.\" Ridgeway and Correll define \"social relational contexts\" as \"any situation in which individuals define themselves in relation to others in order to act.\" They also point out that in addition to social relational contexts, cultural beliefs plays a role in the gender system. The coauthors argue that daily people are forced to acknowledge and interact with others in ways that are related to gender. Every day, individuals are interacting with each other and comply with society's set standard of hegemonic beliefs, which includes gender roles. They state that society's hegemonic cultural beliefs sets the rules which in turn create the setting for which social relational contexts are to take place. Ridgeway and Correll then shift their topic towards sex categorization. The authors define sex categorization as \"the sociocognitive process by which we label another as male or female.\"\n\nGender, as identified through gender normative play, self-identification with a gender, and tendency to engage in aggressive behavior is influenced by prenatal hormone exposure. Studies on other gendered behavior are inconsistent, however some evidence indicates other gendered behavior is influenced by prenatal and early life androgen exposure. Males of most mammals, including humans, exhibit more rough and tumble play behavior, which is influenced by maternal testosterone levels. These levels may also influence sexuality, with non-heterosexual persons exhibiting sex atypical behavior in childhood.\n\nThe biology of gender became the subject of an expanding number of studies over the course of the late 20th century. One of the earliest areas of interest was what became known as \"gender identity disorder\" (GID) and which is now also described as gender dysphoria. Studies in this, and related areas, inform the following summary of the subject by John Money. He stated:\n\nMoney refers to attempts to distinguish a difference between biological sex and social gender as \"scientifically debased\", because of our increased knowledge of a continuum of dimorphic features (Money's word is \"dipolar\") that link biological and behavioral differences. These extend from the exclusively biological \"genetic\" and \"prenatal hormonal\" differences between men and women, to \"postnatal\" features, some of which are social, but others have been shown to result from \"post-pubertal hormonal\" effects.\n\nAlthough causation from the biological—genetic and hormonal—to the behavioral has been broadly demonstrated and accepted, Money is careful to also note that understanding of the causal chains from biology to behavior in sex and gender issues is very far from complete. For example, the existence of a \"gay gene\" has not been proven, but such a gene remains an acknowledged possibility.\n\nThere are studies concerning women who have a condition called congenital adrenal hyperplasia, which leads to the overproduction of the masculine sex hormone, androgen. These women usually have ordinary female appearances (though nearly all girls with congenital adrenal hyperplasia (CAH) have corrective surgery performed on their genitals). However, despite taking hormone-balancing medication given to them at birth, these females are statistically more likely to be interested in activities traditionally linked to males than female activities. Psychology professor and CAH researcher Dr. Sheri Berenbaum attributes these differences to an exposure of higher levels of male sex hormones in utero.\n\nSexual reproduction is a common method of producing a new individual within various species. In sexually reproducing species, individuals produce special kinds of cells (called \"gametes\") whose function is specifically to fuse with one \"unlike\" gamete and thereby to form a new individual. This fusion of two unlike gametes is called fertilization. By convention, where one type of gamete cell is physically larger than the other, it is associated with female sex. Thus an individual that produces exclusively large gametes (ova in humans) is called \"female\", and one that produces exclusively small gametes (spermatozoa in humans) is called \"male\".\n\nAn individual that produces both types of gametes is called \"hermaphrodite\" (a name applicable also to people with one testis and one ovary). In some species hermaphrodites can self-fertilize (see Selfing), in others they can achieve fertilization with females, males or both. Some species, like the Japanese Ash, \"Fraxinus lanuginosa\", only have males and hermaphrodites, a rare reproductive system called \"androdioecy\". Gynodioecy is also found in several species. Human hermaphrodites are typically, but not always, infertile.\n\nWhat is considered defining of sexual reproduction is the \"difference\" between the gametes and the \"binary\" nature of fertilization. Multiplicity of gamete \"types\" within a species would still be considered a form of sexual reproduction. However, of more than 1.5 million living species,\nrecorded up to about the year 2000, \"no third sex cell—and so no third sex—has appeared in multi-cellular animals.\" Why sexual reproduction has an exclusively binary gamete system is not yet known. A few rare species that push the boundaries of the definitions are the subject of active research for light they may shed on the mechanisms of the evolution of sex. For example, the most toxic insect, the harvester ant \"Pogonomyrmex\", has two kinds of female and two kinds of male. One hypothesis is that the species is a hybrid, evolved from two closely related preceding species.\n\nFossil records indicate that sexual reproduction has been occurring for at least one billion years.\nHowever, the reason for the initial evolution of sex, and the reason it has survived to the present are still matters of debate, there are many plausible theories. It appears that the ability to reproduce sexually has evolved independently in various species on many occasions. There are cases where it has also been lost, notably among the Fungi Imperfecti.\nThe blacktip shark (\"Carcharhinus limbatus\"), flatworm (\"Dugesia tigrina\") and some other species can reproduce either sexually or asexually depending on various conditions.\n\nThe following systematic list gender taxonomy illustrates the kinds of diversity that have been studied and reported in medical literature. It is placed in roughly chronological order of biological and social development in the human life cycle. The earlier stages are more purely biological and the latter are more dominantly social. Causation is known to operate from chromosome to gonads, and from gonads to hormones. It is also significant from brain structure to gender identity (see Money quote above). Brain structure and processing (biological) that may explain erotic preference (social), however, is an area of ongoing research. Terminology in some areas changes quite rapidly as knowledge grows.\n\n\nAlthough sexual reproduction is \"defined\" at the cellular level, key features of sexual reproduction operate \"within\" the structures of the gamete cells themselves. Notably, gametes carry very long molecules called DNA that the biological processes of reproduction can \"read\" like a book of instructions. In fact, there are typically many of these \"books\", called \"chromosomes\". Human gametes usually have 23 chromosomes, 22 of which are common to both sexes. The final chromosomes in the two human gametes are called \"sex\" chromosomes because of their role in sex determination. Ova always have the same sex chromosome, labelled \"X\". About half of spermatozoa also have this same X chromosome, the rest have a Y-chromosome. At fertilization the gametes fuse to form a cell, usually with 46 chromosomes, and either XX female or XY male, depending on whether the sperm carried an X or a Y chromosome. Some of the other possibilities are listed above. \n\nGenes which are specific to the X or Y chromosome are called sex-linked genes. For example, the genes which create red and green retinal photoreceptors are located on the X chromosome, which men only have one of. Thus red-green color blindness is an X-linked recessive trait and is much more common in men. However, sex-limited genes on \"any\" chromosome can be expressed and \"say\", for example, \"\"if\" you are in a male body do X, otherwise do not.\"\n\nThe human XY system is not the only sex determination system. Birds typically have a reverse, ZW system—males are ZZ and females ZW. Whether male or female birds influence the sex of offspring is not known for all species. Several species of butterfly are known to have female parent sex determination.\n\nThe platypus has a complex hybrid system, the male has ten sex chromosomes, half X and half Y.\n\n\"It is well established that men have a larger cerebrum than women by about 8–10% (Filipek et al., 1994; Nopoulos et al.,\n2000; Passe et al., 1997a,b; Rabinowicz et al., 1999; Witelson et al., 1995).\"\nHowever, what is functionally relevant are differences in composition and \"wiring\". Richard J. Haier and colleagues at the universities of New Mexico and California (Irvine) found, using brain mapping, that men have more grey matter related to general intelligence than women, and women have more white matter related to intelligence than men – the ratio between grey and white matter is 4% higher for men than women.\n\nGrey matter is used for information processing, while white matter consists of the connections between processing centers. Other differences are measurable but less pronounced.\nMost of these differences are produced by hormonal activity, ultimately derived from the Y chromosome and sexual differentiation. However, differences that arise directly from gene activity have also been observed.\n\nIt has also been demonstrated that brain processing responds to the external environment. Learning, both of ideas and behaviors, appears to be coded in brain processes. It also appears that in several simplified cases this coding operates differently, but in some ways equivalently, in the brains of men and women. For example, both men and women learn and use language; however, bio-chemically, they appear to process it differently. Differences in female and male use of language are likely reflections \"both\" of biological preferences and aptitudes, \"and\" of learned patterns.\n\nGender studies is a field of interdisciplinary study and academic field devoted to gender, gender identity and gendered representation as central categories of analysis. This field includes Women's studies (concerning women, feminity, their gender roles and politics, and feminism), Men's studies (concerning men, masculinity, their gender roles, and politics), and LGBT studies.\nSometimes Gender studies is offered together with Study of Sexuality.\nThese disciplines study gender and sexuality in the fields of literature and language, history, political science, sociology, anthropology, cinema and media studies, human development, law, and medicine.\nIt also analyses race, ethnicity, location, nationality, and disability.\n\nMany of the more complicated human behaviors are influenced by both innate factors and by environmental ones, which include everything from genes, gene expression, and body chemistry, through diet and social pressures. A large area of research in behavioral psychology collates evidence in an effort to discover correlations between behavior and various possible antecedents such as genetics, gene regulation, access to food and vitamins, culture, gender, hormones, physical and social development, and physical and social environments.\n\nA core research area within sociology is the way human behavior operates on \"itself\", in other words, how the behavior of one group or individual influences the behavior of other groups or individuals. Starting in the late 20th century, the feminist movement has contributed extensive study of gender and theories about it, notably within sociology but not restricted to it.\nSocial theorists have sought to determine the specific nature of gender in relation to biological sex and sexuality, with the result being that culturally established gender and sex have become interchangeable identifications that signify the allocation of a specific 'biological' sex within a categorical gender. The second wave feminist view that gender is socially constructed and hegemonic in all societies, remains current in some literary theoretical circles, Kira Hall and Mary Bucholtz publishing new perspectives as recently as 2008.\n\nContemporary socialisation theory proposes the notion that when a child is first born it has a biological sex but no social gender. As the child grows, \"...society provides a string of prescriptions, templates, or models of behaviors appropriate to the one sex or the other,\" which socialises the child into belonging to a culturally specific gender. There is huge incentive for a child to concede to their socialisation with gender shaping the individual’s opportunities for education, work, family, sexuality, reproduction, authority, and to make an impact on the production of culture and knowledge. Adults who do not perform these ascribed roles are perceived from this perspective as deviant and improperly socialized.\n\nSome believe society is constructed in a way that splits gender into a dichotomy via social organisations that constantly invent and reproduce cultural images of gender. Joan Acker believes gendering occurs in at least five different interacting social processes:\n\n\nLooking at gender through a Foucauldian lens, gender is transfigured into a vehicle for the social division of power. Gender difference is merely a construct of society used to enforce the distinctions made between what is assumed to be female and male, and allow for the domination of masculinity over femininity through the attribution of specific gender-related characteristics. \"The idea that men and women are more different from one another than either is from anything else, must come from something other than nature… far from being an expression of natural differences, exclusive gender identity is the suppression of natural similarities.\"\n\nGender conventions play a large role in attributing masculine and feminine characteristics to a fundamental biological sex. Socio-cultural codes and conventions, the rules by which society functions, and which are both a creation of society as well as a constituting element of it, determine the allocation of these specific traits to the sexes. These traits provide the foundations for the creation of hegemonic gender difference. It follows then, that gender can be assumed as the acquisition and internalisation of social norms. Individuals are therefore socialized through their receipt of society’s expectations of 'acceptable' gender attributes that are flaunted within institutions such as the family, the state and the media. Such a notion of 'gender' then becomes naturalized into a person’s sense of self or identity, effectively imposing a gendered social category upon a sexed body.\n\nThe conception that people are gendered rather than sexed also coincides with Judith Butler’s theories of gender performativity. Butler argues that gender is not an expression of what one is, but rather something that one does. It follows then, that if gender is acted out in a repetitive manner it is in fact re-creating and effectively embedding itself within the social consciousness. Contemporary sociological reference to male and female gender roles typically uses \"masculinities\" and \"femininities\" in the plural rather than singular, suggesting diversity both within cultures as well as across them.\n\nThe difference between the sociological and popular definitions of gender involve a different dichotomy and focus. For example, the sociological approach to \"gender\" (social roles: female versus male) focuses on the difference in (economic/power) position between a male CEO (disregarding the fact that he is heterosexual or homosexual) to female workers in his employ (disregarding whether they are straight or gay). However the popular sexual self-conception approach (self-conception: gay versus straight) focuses on the different self-conceptions and social conceptions of those who are gay/straight, in comparison with those who are straight (disregarding what might be vastly differing economic and power positions between female and male groups in each category). There is then, in relation to definition of and approaches to \"gender\", a tension between historic feminist sociology and contemporary homosexual sociology.\n\nA person's sex as male or female has legal significance—sex is indicated on government documents, and laws provide differently for men and women. Many pension systems have different retirement ages for men or women. Marriage is usually only available to opposite-sex couples; in some countries and jurisdictions there are same-sex marriage laws.\n\nThe question then arises as to what legally determines whether someone is female or male. In most cases this can appear obvious, but the matter is complicated for intersex or transgender people. Different jurisdictions have adopted different answers to this question. Almost all countries permit changes of legal gender status in cases of intersexualism, when the gender assignment made at birth is determined upon further investigation to be biologically inaccurate—technically, however, this is not a change of status \"per se\". Rather, it is recognition of a status deemed to exist but unknown from birth. Increasingly, jurisdictions also provide a procedure for changes of legal gender for transgender people.\n\nGender assignment, when there are indications that genital sex might not be decisive in a particular case, is normally not defined by a single definition, but by a combination of conditions, including chromosomes and gonads. Thus, for example, in many jurisdictions a person with XY chromosomes but female gonads could be recognized as female at birth.\n\nThe ability to change legal gender for transgender people in particular has given rise to the phenomena in some jurisdictions of the same person having different genders for the purposes of different areas of the law. For example, in Australia prior to the Re Kevin decisions, transsexual people could be recognized as having the genders they identified with under many areas of the law, including social security law, but not for the law of marriage. Thus, for a period, it was possible for the same person to have two different genders under Australian law.\n\nIt is also possible in federal systems for the same person to have one gender under state law and a different gender under federal law.\n\nFor intersex people, who according to the UN Office of the High Commissioner for Human Rights, \"do not fit typical binary notions of male or female bodies\", access to any form of identification document with a gender marker may be an issue. For other intersex people, there may be issues in securing the same rights as other individuals assigned male or female; other intersex people may seek non-binary gender recognition.\n\nSome countries now legally recognize non-binary or third genders, including Canada, Germany, Australia, and India. In the United States, Oregon, California, and the District of Columbia legally recognize non-binary gender.\n\nNatural languages often make gender distinctions. These may be of various kinds, more or less loosely associated by analogy with various actual or perceived differences between men and women. Some grammatical gender systems go beyond, or ignore, the masculine-feminine distinction.\n\nHistorically, science has been portrayed as a masculine pursuit in which women have faced significant barriers to participate. Even after universities began admitting women in the 19th century, women were still largely relegated to certain scientific fields, such as home science, nursing, and child psychology. Women were also typically given tedious, low-paying jobs and denied opportunities for career advancement. This was often justified by the stereotype that women were naturally more suited to jobs that required concentration, patience, and dexterity, rather than creativity, leadership, or intellect. Although these stereotypes have been dispelled in modern times, women are still underrepresented in prestigious \"hard science\" fields such as physics, and are less likely to hold high-ranking positions.\n\nThis topic includes internal and external religious issues such as gender of God and deities creation myths about human gender, roles and rights (for instance, leadership roles especially ordination of women, sex segregation, gender equality, marriage, abortion, homosexuality)\n\nAccording to Kati Niemelä of the Church Research Institute, women are universally more religious than men. They believe that the difference in religiousity between genders is due to biological differences, for instance usually people seeking security in life are more religious, and as men are considered to be greater risk takers than women, they are less religious. Although religious fanaticism is more often seen in men than women.\nIn Taoism, yin and yang are considered feminine and masculine, respectively. The Taijitu and concept of the Zhou period reach into family and gender relations. Yin is female and yang is male. They fit together as two parts of a whole.\nThe male principle was equated with the sun: active, bright, and shining; the female principle corresponds to the moon: passive, shaded, and reflective. Male toughness was balanced by female gentleness, male action and initiative by female endurance and need for completion, and male leadership by female supportiveness.\n\nIn Judaism, God is traditionally described in the masculine, but in the mystical tradition of the Kabbalah, the Shekhinah represents the feminine aspect of God's essence. However, Judaism traditionally holds that God is completely non-corporeal, and thus neither male nor female. Conceptions of the gender of God notwithstanding, traditional Judaism places a strong emphasis on individuals following Judaism's traditional gender roles, though many modern denominations of Judaism strive for greater egalitarianism. As well, traditional Jewish culture dictates that there are six genders.\n\nIn Christianity, God is traditionally described in masculine terms and the Church has historically been described in feminine terms. On the other hand, Christian theology in many churches distinguishes between the masculine images used of God (Father, King, God the Son) and the reality they signify, which transcends gender, embodies all the virtues of both men and women perfectly, which may be seen through the doctrine of Imago Dei. In the New Testament, Jesus at several times mentions with the masculine pronoun i.e. John 15:26 among other verses. Hence, the Father, the Son and the Holy Spirit (i.e. Trinity) are all mentioned with the masculine pronoun; though the exact meaning of the masculinity of the Christian triune God is contended.\n\nIn Hinduism\n\nIn a number of North American Indigenous cultures, non-man/-woman individuals sometimes carried specific roles within that nation's religious structures. These could be the Lakota wíŋkte, Navajo nadleehí, Anishinaabe niizh manidoowag and hundreds more. Recently, North American Native Americans and First Nations have adopted the term Two-Spirit to refer to the mosaic of different genders cross-culturally.\n\nGender inequality is most common in women dealing with poverty. Many women must shoulder all the responsibility of the household because they must take care of the family. Oftentimes this may include tasks such as tilling land, grinding grain, carrying water and cooking. Also, women are more likely to earn low incomes because of gender discrimination, as men are more likely to receive higher pay, have more opportunities, and have overall more political and social capital then women. Approximately 75% of world's women are unable to obtain bank loans because they have unstable jobs. It shows that there are many women in the world's population but only a few represent world's wealth. In many countries, the financial sector largely neglects women even though they play an important role in the economy, as Nena Stoiljkovic pointed out in \"D+C Development and Cooperation\". In 1978 Diana M. Pearce coined the term feminization of poverty to describe the problem of women having higher rates of poverty. Women are more vulnerable to chronic poverty because of gender inequalities in the distribution of income, property ownership, credit, and control over earned income. Resource allocation is typically gender-biased within households, and continue on a higher level regarding state institutions.\n\nGender and Development (GAD) is a holistic approach to give aid to countries where gender inequality has a great effect of not improving the social and economic development. It is a program focused on the gender development of women to empower them and decrease the level of inequality between men and women.\n\nThe largest discrimination study of the transgender community, conducted in 2013, found that the transgender community is four times more likely to live in extreme poverty (income of less than $10,000 a year) than people who are cisgender (not transgender).\n\nAccording to general strain theory, studies suggest that gender differences between individuals can lead to externalized anger that may result in violent outbursts. These violent actions related to gender inequality can be measured by comparing violent neighborhoods to non-violent neighborhoods. By noticing the independent variables (neighborhood violence) and the dependent variable (individual violence), it's possible to analyze gender roles. The strain in the general strain theory is the removal of a positive stimulus and or the introduction of a negative stimulus, which would create a negative effect (strain) within individual, which is either inner-directed (depression/guilt) or outer-directed (anger/frustration), which depends on whether the individual blames themselves or their environment. Studies reveal that even though males and females are equally likely to react to a strain with anger, the origin of the anger and their means of coping with it can vary drastically. Males are likely to put the blame on others for adversity and therefore externalize feelings of anger. Females typically internalize their angers and tend to blame themselves instead. Female internalized anger is accompanied by feelings of guilt, fear, anxiety and depression. Women view anger as a sign that they've somehow lost control, and thus worry that this anger may lead them to harm others and/or damage relationships. On the other end of the spectrum, men are less concerned with damaging relationships and more focused on using anger as a means of affirming their masculinity. According to the general strain theory, men would more likely engage in aggressive behavior directed towards others due to externalized anger whereas women would direct their anger towards themselves rather than others.\n\nGender, and particularly the role of women is widely recognized as vitally important to international development issues. This often means a focus on gender-equality, ensuring participation, but includes an understanding of the different roles and expectation of the genders within the community.\n\nIn modern times, the study of gender and development has become a broad field that involves politicians, economists, and human rights activists. Gender and Development, unlike previous theories concerning women in development, includes a broader view of the effects of development on gender including economic, political, and social issues. The theory takes a holistic approach to development and its effects on women and recognizes the negative effects gender blind development policies have had on women. Prior to 1970, it was believed that development affected men and women in the same way and no gendered perspective existed for development studies. However, the 1970s saw a transformation in development theory that sought to incorporate women into existing development paradigms.\n\nWhen Ester Boserup published her book, \"Woman’s Role in Economic Development\", there was a realization that development affected men and women differently and there began to be more of a focus on women and development. Boserup argued that women were marginalized in the modernization process and practices of growth, development, and development policy threatened to actually make women worse off.\nBoserup’s work translated into the beginning of a larger discourse termed Women in Development (WID) coined by the Women’s Committee of the Washington DC Chapter of the Society for International Development, a network of female development professionals. The primary goal of WID was to include women into existing development initiatives, since it was argued that women were marginalized and excluded from the benefits of development. In so doing, the WID approach pointed out that the major problem to women’s unequal representation and participation were male biased and patriarchal development policies. In short, the WID approach blamed patriarchy, which did not consider women’s productive and reproductive work. In fact, women were tied to domestic work hence were almost invisible in development programs. The WID approach, however, began to gain criticism as ignoring how women’s economic marginalization was linked to the development model itself.\n\nSome feminists argued that the key concept for women and development should be subordination in the context of new capitalist forms of insecure and hierarchical job structures, rather than marginalization as WID approaches emphasized. The rise of criticism against the WID approach led to the emergence of a new theory, that of Women and Development (WAD).\n\nHowever, just as WID had its critics, so did WAD. Critics of WAD argued that it failed to sufficiently address the differential power relations between women and men, and tended to overemphasize women’s productive as opposed to reproductive roles. Also, rising criticism of the exclusion of men in WID and WAD led to a new theory termed Gender and Development (GAD). Drawing from insights developed in psychology, sociology, and gender studies, GAD theorists shifted from understanding women’s problems as based on their sex (i.e. their biological differences from men) to understanding them as based on gender – the social relations between women and men, their social construction, and how women have been systematically subordinated in this relationship.\n\nAt their most fundamental, GAD perspectives link the social relations of production with the social relations of reproduction – exploring why and how women and men are assigned to different roles and responsibilities in society, how these dynamics are reflected in social, economic, and political theories and institutions, and how these relationships affect development policy effectiveness. According to proponents of GAD, women are cast not as passive recipients of development aid, but rather as active agents of change whose empowerment should be a central goal of development policy. In contemporary times, most literature and institutions that are concerned with women's role in development incorporate a GAD perspective, with the United Nations taking the lead of mainstreaming the GAD approach through its system and development policies.\n\nResearchers at the Overseas Development Institute have highlighted that policy dialogue on the Millennium Development Goals needs to recognize that the gender dynamics of power, poverty, vulnerability and care link all the goals.\nThe various United Nations international women’s conferences in Beijing, Mexico City, Copenhagen, and Nairobi, as well as the development of the Millennium Development Goals in 2000 have taken a GAD approach and holistic view of development. The United Nations Millennium Declaration signed at the United Nations Millennium Summit in 2000 including eight goals that were to be reached by 2015, and although it would be a difficult task to reach them, all of them could be monitored. The eight goals are:\n\nThe MDGs have three goals specifically focused on women: Goal 3, 4 and 5 but women’s issues also cut across all of the goals. These goals overall comprise all aspects of women’s lives including economic, health, and political participation.\n\nGender equality is also strongly linked to education. The Dakar Framework for Action (2000) set out ambitious goals: to eliminate gender disparities in primary and secondary education by 2005, and to achieve gender equality in education by 2015. The focus was on ensuring girls’ full and equal access to and achievement in good quality basic education. The gender objective of the Dakar Framework for Action is somewhat different from the MDG Goal 3 (Target 1): \"Eliminate gender disparity in primary and secondary education, preferably by 2005, and in all levels of education no later than 2015\". MDG Goal 3 does not comprise a reference to learner achievement and good quality basic education, but goes beyond the school level. Studies demonstrate the positive impact of girls’ education on child and maternal health, fertility rates, poverty reduction and economic growth. Educated mothers are more likely to send their children to school.\n\nSome organizations working in developing countries and in the development field have incorporated advocacy and empowerment for women into their work. The Food and Agriculture Organization of the United Nations (FAO) adopted a 10-year strategic framework in November 2009 that includes the strategic objective of gender equity in access to resources, goods, services and decision-making in rural areas, and mainstreams gender equity in all FAO's programs for agriculture and rural development. The Association for Progressive Communications (APC) has developed a Gender Evaluation Methodology for planning and evaluating development projects to ensure they benefit all sectors of society including women.\n\nThe Gender-related Development Index (GDI), developed by the United Nations, aims to show the inequalities between men and women in the following areas: long and healthy life, knowledge, and a decent standard of living. The United Nations Development Programme (UNDP) has introduced indicators designed to add a gendered dimension to the Human Development Index (HDI). Additionally, in 1995, the Gender-related Development Index (GDI) and the Gender Empowerment Measure (GEM) were introduced. More recently, in 2010, UNDP introduced a new indicator, the Gender Inequality Index (GII), which was designed to be a better measurement of gender inequality and to improve the shortcomings of GDI and GEM.\n\nGender is a topic of increasing concern within climate change policy and science. Generally, gender approaches to climate change address gender- differentiated consequences of climate change, as well as unequal adaptation capacities and gendered contribution to climate change. Furthermore, the intersection of climate change and gender raises questions regarding the complex and intersecting power relations arising from it. These differences, however, are mostly not due to biological or physical differences, but are formed by the social, institutional and legal context. Subsequently, vulnerability is less an intrinsic feature of women and girls but rather a product of their marginalization.\nRoehr notes that, while the United Nations officially committed to gender mainstreaming, in practice gender equality is not reached in the context of climate change policies. This is reflected in the fact that discourses of and negotiations over climate change are mostly dominated by men.\nSome feminist scholars hold that the debate on climate change is not only dominated by men but also primarily shaped in ‘masculine’ principles, which limits discussions about climate change to a perspective that focuses on technical solutions. This perception of climate change hides subjectivity and power relations that actually condition climate-change policy and science, leading to a phenomenon that Tuana terms ‘epistemic injustice’.\nSimilarly, MacGregor attests that by framing climate change as an issue of ‘hard’ natural scientific conduct and natural security, it is kept within the traditional domains of hegemonic masculinity.\n\nGender roles and stereotypes have slowly started to change in society within the past few decades. These changes occur mostly in communication, but more specifically during social interactions. The ways people communicate and socialize have also started to change alongside advancement in technology. One of the biggest reasons for this change is due to social media.\n\nOver the past few years, the use of social media globally has started to rise. This rise can be contributed to the abundance of technology available for use among youth. Recent studies suggest that men and women value and use technology differently. Forbes published an article in 2010 that reported 57% of Facebook users are women, which attributed to the fact that women are more active on social media, because on average women have 8% more friends and account for 62% of posts that are shared via Facebook. Another study in 2010 found that in most Western cultures, women spend more time sending text messages compared to men as well as spend more time on social networking sites as a way to communicate with friends and family. Hayat, Lesser and Samuel-Azran (2017) have further shown that while men write more posts in social networking sites, women commented on other people's posts more often. They further showed that women's posts enjoyed higher popularity than men's post\"s.\"\n\nSocial media is more than just the communication of words. With social media increasing in popularity, pictures have become a large role in how many people communicate. Research conducted in 2013 found that over 57% of pictures posted on social networking sites were sexual and were created to gain attention. More shockingly, 58% of women and 45% of men don't look into the camera, which is creating an illusion of withdrawal. Other factors to be considered are the poses in pictures such as women laying down in subordinate positions or even touching themselves in child like ways. In conclusion, research has found that images shared online through social networking sites help establish personal self-reflections that individuals want to share with the world.\n\nAccording to recent research, gender plays a strong role in structuring our social lives, especially since society assigns and creates \"male\" and \"female\" categories. Individuals in society might be able to learn the similarities between gender rather than the differences. Until then, gender will never truly be equal, which is a problem. Social media helps create more equality, because every individual is able to express him or herself however they like. Every individual also has the right to express their opinion, even though some might disagree, but it still gives each gender an equal amount of power to be heard.\n\nYoung adults in the U.S. frequently use social networking sites as a way to connect and communicate with one another, as well as to satisfy their curiosity. Adolescent girls generally use social networking sites as a tool to communicate with peers and reinforce existing relationships; boys on the other hand tend to use social networking sites as a tool to meet new friends and acquaintances. More importantly, social networking sites have allowed individuals to truly express themselves, as they are able to create an identity and socialize with other individuals that can relate. Social networking sites have also given individuals access to create a space where they feel more comfortable about their sexuality. Recent research has indicated that social media is becoming a stronger part of younger individuals media culture, as more intimate stories are being told via social media and are being intertwined with gender, sexuality, and relationships.\n\nTeens are avid internet and social media users in the United States. Research has found that almost all U.S. teens (95%) aged 12 through 17 are online, compared to only 78% of adults. Of these teens, 80% have profiles on social media sites, as compared to only 64% of the online population aged 30 and older. According to a study conducted by the Kaiser Family Foundation, 11-to-18 year olds spend – on average – over one and a half hours a day using a computer and 27 minutes per day visiting social network sites, which accounts for more than one fourth of their daily computer use.\n\nTeen girls and boys differ in what they post in their online profiles. Studies have shown that female users tend to post more \"cute\" pictures, while male participants were more likely to post pictures of them doing action activities. Women in the U.S. also tend to post more pictures of friends, while boys tend to post more about sports and humorous links. The study also found that males would post more alcohol and sexual references. The roles were reversed however, when looking at a teenage dating site. Women referred to sexual references significantly more than males.\n\nBoys share more personal information, like their hometown and phone number. While girls are more conservative about the personal information they allow to go public on these social networking sites. Boys, meanwhile, are more likely to orient towards technology, sports, and humor in the information they post to their profile.\n\nSocial media goes beyond the role of helping individuals express themselves, as it has grown to help individuals create relationships, particularly romantic relationships. A large amount of social media users have found it easier to create relationships in a less direct approach, compared to the traditional approach of awkwardly asking for someone's number.\n\nSocial media plays a big role when it comes to communication between genders. Therefore, it's important to understand how gender stereotypes develop during online interactions. Research in the 1990s suggested that different genders display certain traits such as being active, attractive, dependent, dominant, independent, sentimental, sexy, and submissive when it comes to online interaction. Even though these traits continue to be displayed through gender stereotypes, recent studies show that this isn't necessarily the case anymore.\n\n\n",
    "id": "38076",
    "title": "Gender"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=23420193",
    "text": "Macrobiology\n\nMacrobiology is the branch of biology that studies large living organisms (termed Macro organisms) that can be seen by the naked eye. Macrobiology is the opposite of Microbiology.\n\nMacrocosm and Microcosm\n\nMacrocosm and Microcosm is an important concept in Macrobiology. It refers to a vision of cosmos where the part reflects the whole (macrocosm) and vice versa. It is a feature \"present in all esoteric schools of thinking\", according to scholar Pierre A. Riffard. It underlies practices such as astrology, alchemy and sacred geometry.\n\n",
    "id": "23420193",
    "title": "Macrobiology"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=51529881",
    "text": "Nuclear calcium\n\nThe concentration of calcium in the cell nucleus can increase in response to signals from the environment. Nuclear calcium is an evolutionary conserved potent regulator of gene expression that allows cells to undergo long-lasting adaptive responses. The 'Nuclear Calcium Hypothesis’ by Hilmar Bading describes nuclear calcium in neurons as an important signaling end-point in synapse-to-nucleus communication that activates gene expression programs needed for persistent adaptations. In the nervous system, nuclear calcium is required for long-term memory formation, acquired neuroprotection, and the development of chronic inflammatory pain. In the heart, nuclear calcium is important for the development of cardiac hypertrophy. In the immune system, nuclear calcium is required for human T cell activation. Plants use nuclear calcium to control symbiosis signaling.\n",
    "id": "51529881",
    "title": "Nuclear calcium"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=51529492",
    "text": "Acquired neuroprotection\n\nAcquired neuroprotection is a synaptic-activity-dependent form of adaptation in the nervous system that renders neurons more resistant to harmful conditions. The term was coined by Hilmar Bading. This use-dependent enhancement of cellular survival activity requires changes in gene expression triggered by neuronal activity and nuclear calcium signaling. In rodents, components of the neuroprotective gene program can reduce brain damage caused by seizure-like activity or by a stroke. In acute and chronic neurodegenerative diseases, gene regulatory events important for acquired neuroprotection are antagonized by extrasynaptic NMDA receptor signaling leading to increased vulnerability, loss of structural integrity, and bioenergetics dysfunction.\n",
    "id": "51529492",
    "title": "Acquired neuroprotection"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=52240704",
    "text": "Transboundary breed\n\nA transboundary breed is a breed which is present in several countries. Transboundary species of the five significant livestock types (cattle, sheep, goats, pigs and chickens), have been developed for a hundred years or more in intensive manufacturing systems, which has led to global availability. A relatively small number of worldwide transboundary breeds compose the ever-increasing share of total global animal products. However, only in North America and the Southwest Pacific do the number of transboundary breeds surpass that of local breeds. \n\nThere can be both regional and international types of transboundary breeds. Regional breeds are breeds that are reported to only be found in one \"region\", which may include several countries, and an international transboundary breed is one that is reported to be found in multiple regions. For example, the Holstein Fresian cattle is an international transboundary breed, because it is found in several different continents and regions.\n",
    "id": "52240704",
    "title": "Transboundary breed"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=52335682",
    "text": "Solenocyte\n\nIn biology, Solenocytes are flagellated cells associated with excretion, osmoregulation and ionoregulation in many non-chordates and in some chordates under the sub-phylum Cephalochordata of the sub-group Protochordata.\n\nThese are the cells which form subtypes of Protonephridium along with the other type i.e. Flame Cells. Flame cells can be distinguished from solenocytes as the former is usually ciliated whereas the latter is flagellated.\n\nAn example of organisms in which excretion is performed by Solenocytic protonephridia is Genus \"Branchiostoma.\"\n",
    "id": "52335682",
    "title": "Solenocyte"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=42900237",
    "text": "Genomics data sharing\n\nGenomics research data is the type of data generated from human genomics research. This data may be either the output of academic research or a result of research in industry, for example from clinical trials in the pharmaceutical industry.\n\nSharing of human genomics data has challenges similar to the issues related to sharing of health data and the handling, managing and sharing of the data must be in accordance with patient consent. \n\nStandardisation of annotation vocabularies is important for sharing data in an interoperable fashion, the gene ontology is an open standard enabling linking and access to different data sources via a shared vocabulary. Data annotated with a OWL ontology like the gene ontology and marked up using RDF can be queried in a unified way, effectively providing a standardised, machine readable API.\n\nHere are some examples of data formats used in regards to genomics \n\n\nThere are a number of genomic data resources already in existence and some larger scale projects planned in the next 3 to 5 years. Many of the existing projects are not publicly available but only available through request for academics or researchers. Examples of current genomics projects include People of the British Isles and the International HapMap project. Some larger scale project are being developed by governments prospecting that large genomic databases will be valuable resources for health science and pharmaceuticals. Such large scale projects are planned in the UK (100 thousand genomes project) and The Qatar Genome Project. As these large scale projects progress issues of data sharing will become more poignant.\n\nBiological data\n\nMetadata\n",
    "id": "42900237",
    "title": "Genomics data sharing"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=411782",
    "text": "Staining\n\nStaining is an auxiliary technique used in microscopy to enhance contrast in the microscopic image. Stains and dyes are frequently used in biology and medicine to highlight structures in biological tissues for viewing, often with the aid of different microscopes. Stains may be used to define and examine bulk tissues (highlighting, for example, muscle fibers or connective tissue), cell populations (classifying different blood cells, for instance), or organelles within individual cells.\n\nIn biochemistry it involves adding a class-specific (DNA, proteins, lipids, carbohydrates) dye to a substrate to qualify or quantify the presence of a specific compound. Staining and fluorescent tagging can serve similar purposes. Biological staining is also used to mark cells in flow cytometry, and to flag proteins or nucleic acids in gel electrophoresis.\n\nSimple staining is staining with only one stain/dye. There are various kinds of multiple staining, many of which are examples of counterstaining, differential staining, or both, including double staining and triple staining.\n\nStaining is not limited to biological materials, it can also be used to study the morphology of other materials for example the lamellar structures of semi-crystalline polymers or the domain structures of block copolymers.\n\n\"In vivo\" staining (also called vital staining or intravital staining) is the process of dyeing living tissues—\"in vivo\" means \"in life\" (compare with \"in vitro\" staining). By causing certain cells or structures to take on contrasting colour(s), their form (morphology) or position within a cell or tissue can be readily seen and studied. The usual purpose is to reveal cytological details that might otherwise not be apparent; however, staining can also reveal where certain chemicals or specific chemical reactions are taking place within cells or tissues.\n\n\"In vitro\" staining involves colouring cells or structures that have been removed from their biological context. Certain stains are often combined to reveal more details and features than a single stain alone. Combined with specific protocols for fixation and sample preparation, scientists and physicians can use these standard techniques as consistent, repeatable diagnostic tools. A counterstain is stain that makes cells or structures more visible, when not completely visible with the principal stain.\n\nWhile ex vivo, many cells continue to live and metabolize until they are \"fixed\". Some staining methods are based on this property. Those stains excluded by the living cells but taken up by the already dead cells are called vital stains (e.g. trypan blue or propidium iodide for eukaryotic cells). Those that enter and stain living cells are called supravital stains (e.g. New Methylene Blue and Brilliant Cresyl Blue for reticulocyte staining). However, these stains are eventually toxic to the organism, some more so than others. Partly due to their toxic interaction inside a living cell, when supravital stains enter a living cell, they might produce a characteristic pattern of staining different from the staining of an already fixed cell (e.g. \"reticulocyte\" look versus diffuse \"polychromasia\"). To achieve desired effects, the stains are used in very dilute solutions ranging from to (Howey, 2000). Note that many stains may be used in both living and fixed cells.\n\nThe preparatory steps involved depend on the type of analysis planned; some or all of the following procedures may be required. \n\nFixation–which may itself consist of several steps–aims to preserve the shape of the cells or tissue involved as much as possible. Sometimes heat fixation is used to kill, adhere, and alter the specimen so it accepts stains. Most chemical fixatives (chemicals causing fixation) generate chemical bonds between proteins and other substances within the sample, increasing their rigidity. Common fixatives include formaldehyde, ethanol, methanol, and/or picric acid. Pieces of tissue may be embedded in paraffin wax to increase their mechanical strength and stability and to make them easier to cut into thin slices.\n\nPermeabilization involves treatment of cells with (usually) a mild surfactant. This treatment dissolves cell membranes, and allows larger dye molecules into the cell's interior.\n\nMounting usually involves attaching the samples to a glass microscope slide for observation and analysis. In some cases, cells may be grown directly on a slide. For samples of loose cells (as with a blood smear or a pap smear) the sample can be directly applied to a slide. For larger pieces of tissue, thin sections (slices) are made using a microtome; these slices can then be mounted and inspected.\n\nMost of the dyes commonly used in microscopy are available as BSC-certified stains. This means that samples of the manufacturer's batch have been tested by an independent body, the Biological Stain Commission (BSC), and found to meet or exceed certain standards of purity, dye content and performance in staining techniques. These standards are published in the Commission's journal Biotechnic & Histochemistry. Many dyes are inconsistent in composition from one supplier to another. The use of BSC-certified stains eliminates a source of unexpected results. \n\nSome vendors sell stains \"certified\" by themselves rather than by the Biological Stain Commission. Such products may or may not be suitable for diagnostic and other applications. \n\nA simple staining method for bacteria that is usually successful, even when the \"positive staining\" methods detailed below fail, is to use a negative stain. This can be achieved by smearing the sample onto the slide and then applying nigrosin (a black synthetic dye) or India ink (an aqueous suspension of carbon particles). After drying, the microorganisms may be viewed in bright field microscopy as lighter inclusions well-contrasted against the dark environment surrounding them. Note: negative staining is a mild technique that may not destroy the microorganisms, and is therefore unsuitable for studying pathogens.\n\nGram staining is used to determine gram status to classify bacteria broadly. It is based on the composition of their cell wall. Gram staining uses crystal violet to stain cell walls, iodine as a mordant, and a fuchsin or safranin counterstain to mark all bacteria. Gram status is important in medicine; the presence or absence of a cell wall changes the bacterium's susceptibility to some antibiotics.\n\nGram-positive bacteria stain dark blue or violet. Their cell wall is typically rich with peptidoglycan and lacks the secondary membrane and lipopolysaccharide layer found in Gram-negative bacteria.\n\nOn most Gram-stained preparations, Gram-negative organisms appear red or pink because they are counterstained. Because of presence of higher lipid content, after alcohol-treatment, the porosity of the cell wall increases, hence the CVI complex (crystal violet – iodine) can pass through. Thus, the primary stain is not retained. Also, in contrast to most Gram-positive bacteria, Gram-negative bacteria have only a few layers of peptidoglycan and a secondary cell membrane made primarily of lipopolysaccharide.\n\nEndospore staining is used to identify the presence or absence of endospores, which make bacteria very difficult to kill. This is particularly useful for identifying endospore-forming bacterial pathogens such as \"Clostridium difficile\".\n\nZiehl-Neelsen staining is used to stain species of \"Mycobacterium tuberculosis\" that do not stain with the standard laboratory staining procedures such as Gram staining.\n\nThe stains used are the red coloured Carbol fuchsin that stains the bacteria and a counter stain such as Methylene blue\n\nHaematoxylin and eosin staining protocol is used frequently in histology to examine thin sections of tissue. Haematoxylin stains cell nuclei blue, while eosin stains cytoplasm, connective tissue and other extracellular substances pink or red. Eosin is strongly absorbed by red blood cells, colouring them bright red. In a skilfully made H & E preparation the red blood cells are almost orange, and collagen and cytoplasm (especially muscle) acquire different shades of pink. When the staining is done by a machine, the subtle differences in eosinophilia are often lost. Hematoxylin stains the cell nucleus and other acidic structures (such as RNA-rich portions of the cytoplasm and the matrix of hyaline cartilage) blue. In contrast, eosin stains the cytoplasm and collagen pink.\n\nPapanicolaou staining, or Pap staining, is a frequently used method for examining cell samples from various bodily secretions. It is frequently used to stain Pap smear specimens. It uses a combination of haematoxylin, Orange G, eosin Y, Light Green SF yellowish, and sometimes Bismarck Brown Y.\n\nPeriodic acid-Schiff staining is used to mark carbohydrates (glycogen, glycoprotein, proteoglycans). It is used to distinguish different types of glycogen storage diseases.\n\nMasson's trichrome is (as the name implies) a three-colour staining protocol. The recipe has evolved from Masson's original technique for different specific applications, but all are well-suited to distinguish cells from surrounding connective tissue. Most recipes produce red keratin and muscle fibers, blue or green staining of collagen and bone, light red or pink staining of cytoplasm, and black cell nuclei.\n\nThe Romanowsky stains are all based on a combination of eosinate (chemically reduced eosin) and methylene blue (sometimes with its oxidation products azure A and azure B). Common variants include Wright's stain, Jenner's stain, May-Grunwald stain, Leishman stain and Giemsa stain.\n\nAll are used to examine blood or bone marrow samples. They are preferred over H&E for inspection of blood cells because different types of leukocytes (white blood cells) can be readily distinguished. All are also suited to examination of blood to detect blood-borne parasites such as malaria.\n\nSilver staining is the use of silver to stain histologic sections. This kind of staining is important especially to show proteins (for example type III collagen) and DNA. It is used to show both substances inside and outside cells. Silver staining is also used in temperature gradient gel electrophoresis.\n\nSome cells are \"argentaffin\". These reduce silver solution to metallic silver after formalin fixation. This method was discovered by Italian Camillo Golgi, by using a reaction between silver nitrate and potassium dichromate, thus precipitating silver chromate in some cells (see Golgi's method). Other cells are \"argyrophilic\". These reduce silver solution to metallic silver after being exposed to the stain that contains a reductant, for example hydroquinone or formalin.\n\nSudan staining is the use of Sudan dyes to stain sudanophilic substances, usually lipids. Sudan III, Sudan IV, Oil Red O, Osmium tetroxide, and Sudan Black B are often used. Sudan staining is often used to determine the level of fecal fat to diagnose steatorrhea.\n\nSpecial technique designed for staining true endospores with the use of malachite green dye, once stained, they do not decolourize.\n\nDifferent stains react or concentrate in different parts of a cell or tissue, and these properties are used to advantage to reveal specific parts or areas. Some of the most common biological stains are listed below. Unless otherwise marked, all of these dyes may be used with fixed cells and tissues; vital dyes (suitable for use with living organisms) are noted.\n\nAcridine orange (AO) is a nucleic acid selective fluorescent cationic dye useful for cell cycle determination. It is cell-permeable, and interacts with DNA and RNA by intercalation or electrostatic attractions. When bound to DNA, it is very similar spectrally to fluorescein. Like fluorescein, it is also useful as a non-specific stain for backlighting conventionally stained cells on the surface of a solid sample of tissue (fluorescence backlighted staining).\n\nBismarck brown (also Bismarck brown Y or Manchester brown) imparts a yellow colour to acid mucins.\n\nCarmine is an intensely red dye used to stain glycogen, while Carmine alum is a nuclear stain. Carmine stains require the use of a mordant, usually aluminum.\n\nCoomassie blue (also brilliant blue) nonspecifically stains proteins a strong blue colour. It is often used in gel electrophoresis.\n\nCresyl violet stains the acidic components of the neuronal cytoplasm a violet colour, specifically nissl bodies. Often used in brain research.\n\nCrystal violet, when combined with a suitable mordant, stains cell walls purple. Crystal violet is the stain used in Gram staining.\n\nDAPI is a fluorescent nuclear stain, excited by ultraviolet light and showing strong blue fluorescence when bound to DNA. DAPI binds with A=T rich repeats of chromosomes. DAPI is also not visible with regular transmission microscopy. It may be used in living or fixed cells. DAPI-stained cells are especially appropriate for cell counting.\n\nEosin is most often used as a counterstain to haematoxylin, imparting a pink or red colour to cytoplasmic material, cell membranes, and some extracellular structures. It also imparts a strong red colour to red blood cells. Eosin may also be used as a counterstain in some variants of Gram staining, and in many other protocols. There are actually two very closely related compounds commonly referred to as eosin. Most often used is eosin Y (also known as eosin Y ws or eosin yellowish); it has a very slightly yellowish cast. The other eosin compound is eosin B (eosin bluish or imperial red); it has a very faint bluish cast. The two dyes are interchangeable, and the use of one or the other is more a matter of preference and tradition.\n\nEthidium bromide intercalates and stains DNA, providing a fluorescent red-orange stain. Although it will not stain healthy cells, it can be used to identify cells that are in the final stages of apoptosis – such cells have much more permeable membranes. Consequently, ethidium bromide is often used as a marker for apoptosis in cells populations and to locate bands of DNA in gel electrophoresis. The stain may also be used in conjunction with acridine orange (AO) in viable cell counting. This EB/AO combined stain causes live cells to fluoresce green whilst apoptotic cells retain the distinctive red-orange fluorescence.\n\nAcid fuchsine may be used to stain collagen, smooth muscle, or mitochondria.\nAcid fuchsine is used as the nuclear and cytoplasmic stain in Mallory's trichrome method. Acid fuchsine stains cytoplasm in some variants of Masson's trichrome. In Van Gieson's picro-fuchsine, acid fuchsine imparts its red colour to collagen fibres. Acid fuchsine is also a traditional stain for mitochondria (Altmann's method).\n\nHaematoxylin (hematoxylin in North America) is a nuclear stain. Used with a mordant, haematoxylin stains nuclei blue-violet or brown. It is most often used with eosin in H&E (haematoxylin and eosin) staining—one of the most common procedures in histology.\n\nHoechst is a \"bis\"-benzimidazole derivative compound that binds to the \"minor groove\" of DNA. Often used in fluorescence microscopy for DNA staining, Hoechst stains appear yellow when dissolved in aqueous solutions and emit blue light under UV excitation. There are two major types of Hoechst: \"Hoechst 33258\" and \"Hoechst 33342\". The two compounds are functionally similar, but with a little difference in structure. Hoechst 33258 contains a terminal hydroxyl group and is thus more soluble in aqueous solution, however this characteristics reduces its ability to penetrate the plasma membrane. Hoechst 33342 contains an ethyl substitution on the terminal hydroxyl group (i.e. an ethylether group) making it more hydrophobic for easier plasma membrane passage\n\nIodine is used in chemistry as an indicator for starch. When starch is mixed with iodine in solution, an intensely dark blue colour develops, representing a starch/iodine complex. Starch is a substance common to most plant cells and so a weak iodine solution will stain starch present in the cells. Iodine is one component in the staining technique known as Gram staining, used in microbiology.\nLugol's solution or Lugol's iodine (IKI) is a brown solution that turns black in the presence of starches and can be used as a cell stain, making the cell nuclei more visible. Iodine is also used as a mordant in Gram's staining, it enhances dye to enter through the pore present in the cell wall/membrane.\n\nMalachite green (also known as diamond green B or victoria green B) can be used as a blue-green counterstain to safranin in the Gimenez staining technique for bacteria. It can also be used to directly stain spores.\n\nMethyl green is used commonly with bright-field, as well as fluorescence microscopes to dye the chromatin of cells so that they are more easily viewed.\n\nMethylene blue is used to stain animal cells, such as human cheek cells, to make their nuclei more observable. Also used to stain the blood film and used in cytology.\n\nNeutral red (or toluylene red) stains Nissl substance red. It is usually used as a counterstain in combination with other dyes.\n\nNile blue (or Nile blue A) stains nuclei blue. It may be used with living cells.\n\nNile red (also known as Nile blue oxazone) is formed by boiling Nile blue with sulfuric acid. This produces a mix of Nile red and Nile blue. Nile red is a lipophilic stain; it will accumulate in lipid globules inside cells, staining them red. Nile red can be used with living cells. It fluoresces strongly when partitioned into lipids, but practically not at all in aqueous solution.\n\nOsmium tetraoxide is used in optical microscopy to stain lipids. It dissolves in fats, and is reduced by organic materials to elemental osmium, an easily visible black substance.\n\nRhodamine is a protein specific fluorescent stain commonly used in fluorescence microscopy.\n\nSafranine (or Safranine O) is a red cationic dye. It binds to nuclei (DNA) and other tissue polyanions, including glycosaminoglycans in cartilage and mast cells, and components of lignin and plastids in plant tissues. Safranine should not be confused with saffron, an expensive natural dye that is used in some methods to impart a yellow colour to collagen, to contrast with blue and red colours imparted by other dyes to nuclei and cytoplasm in animal (including human) tissues.\n\nThe incorrect spelling \"safranin\" is in common use. The -ine ending is appropriate \nfor safranine O because this dye is an amine,\n\nTissues which take up stains are called chromatic. Chromosomes were so named because of their ability to absorb a violet stain.\n\nPositive affinity for a specific stain may be designated by the suffix \"-philic\". For example, tissues that stain with an azure stain may be referred to as azurophilic. This may also be used for more generalized staining properties, such as acidophilic for tissues that stain by acidic stains (most notably eosin), basophilic when staining in basic dyes, and \"amphophilic\" when staining with either acid or basic dyes. In contrast, chromophobic tissues do not take up coloured dye readily.\n\nAs in light microscopy, stains can be used to enhance contrast in transmission electron microscopy. Electron-dense compounds of heavy metals are typically used.\n\nPhosphotungstic acid is a common negative stain for viruses, nerves, polysaccharides, and other biological tissue materials.\n\nOsmium tetroxide is used in optical microscopy to stain lipids. It dissolves in fats, and is reduced by organic materials to elemental osmium, an easily visible black substance. Because it is a heavy metal that absorbs electrons, it is perhaps the most common stain used for morphology in biological electron microscopy. It is also used for the staining of various polymers for the study of their morphology by TEM. is very volatile and extremely toxic. It is a strong oxidizing agent as the osmium has an oxidation number of +8. It aggressively oxidizes many materials, leaving behind a deposit of non-volatile osmium in a lower oxidation state.\n\nRuthenium tetroxide is equally volatile and even more aggressive than osmium tetraoxide and able to stain even materials that resist the osmium stain, e.g. polyethylene.\n\nOther chemicals used in electron microscopy staining include:\nammonium molybdate, cadmium iodide, carbohydrazide, ferric chloride, hexamine, indium trichloride, lanthanum nitrate, lead acetate, lead citrate, lead(II) nitrate, periodic acid, phosphomolybdic acid, potassium ferricyanide, potassium ferrocyanide, ruthenium red, silver nitrate, silver proteinate, sodium chloroaurate, thallium nitrate, thiosemicarbazide, uranyl acetate, uranyl nitrate, and vanadyl sulfate.\n\n\n\n",
    "id": "411782",
    "title": "Staining"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=52506202",
    "text": "Sulphide Indole Motility medium\n\nSulphide Indole Motility (SIM) medium is a bacterial growth medium which tests for the ability to reduce sulfates, the ability to produce indoles, and motility. This combination of challenges in one mixture is convenient and commercially available in stab tubes. Inoculated needles are then punctured into the culture and incubated, if the culture becomes cloudy the bacteria were able to infiltrate the media and survive. This method is particularly useful for pathogenic bacteria which are dangerous to handle on wet mount slides\n",
    "id": "52506202",
    "title": "Sulphide Indole Motility medium"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=52399962",
    "text": "Stercomata\n\nStercomata are pellets of waste material accumulated in some species of foraminifera and testate amoebae. The pellets consist largely of clay minerals and are believed to be derived from ingested sediment, indicating that the stercomata-bearing foraminifera are filter-feeders, although it is possible that they may capture suspended particles. In certain species the stercomata are enveloped in sheet-like formations of protoplasm.\n\n",
    "id": "52399962",
    "title": "Stercomata"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=52841665",
    "text": "Thermophyte\n\nThermophyte (Greek \"thérmos\" = warmth, heat + \"phyton\" = plant) is a plant which is tolerant or thriving at high temprtatures. These plants are categorized according to ecological valences at high temperatures, including biological extremely. Such plants included the hot-spring taxa also.\nThe most famous presdstavnici these ecological groups of plants are:\n\n",
    "id": "52841665",
    "title": "Thermophyte"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=8145410",
    "text": "Charles Darwin\n\nCharles Robert Darwin, (; 12 February 1809 – 19 April 1882) was an English naturalist, geologist and biologist, best known for his contributions to the science of evolution. He established that all species of life have descended over time from common ancestors and, in a joint publication with Alfred Russel Wallace, introduced his scientific theory that this branching pattern of evolution resulted from a process that he called natural selection, in which the struggle for existence has a similar effect to the artificial selection involved in selective breeding.\n\nDarwin published his theory of evolution with compelling evidence in his 1859 book \"On the Origin of Species\", overcoming scientific rejection of earlier concepts of transmutation of species. By the 1870s, the scientific community and much of the general public had accepted evolution as a fact. However, many favoured competing explanations and it was not until the emergence of the modern evolutionary synthesis from the 1930s to the 1950s that a broad consensus developed in which natural selection was the basic mechanism of evolution. Darwin's scientific discovery is the unifying theory of the life sciences, explaining the diversity of life.\n\nDarwin's early interest in nature led him to neglect his medical education at the University of Edinburgh; instead, he helped to investigate marine invertebrates. Studies at the University of Cambridge (Christ's College) encouraged his passion for natural science. His five-year voyage on established him as an eminent geologist whose observations and theories supported Charles Lyell's uniformitarian ideas, and publication of his journal of the voyage made him famous as a popular author.\n\nPuzzled by the geographical distribution of wildlife and fossils he collected on the voyage, Darwin began detailed investigations, and in 1838 conceived his theory of natural selection. Although he discussed his ideas with several naturalists, he needed time for extensive research and his geological work had priority. He was writing up his theory in 1858 when Alfred Russel Wallace sent him an essay that described the same idea, prompting immediate joint publication of both of their theories. Darwin's work established evolutionary descent with modification as the dominant scientific explanation of diversification in nature. In 1871 he examined human evolution and sexual selection in \"The Descent of Man, and Selection in Relation to Sex\", followed by \"The Expression of the Emotions in Man and Animals\" (1872). His research on plants was published in a series of books, and in his final book, \"The Formation of Vegetable Mould, through the Actions of Worms\" (1881), he examined earthworms and their effect on soil.\n\nDarwin has been described as one of the most influential figures in human history, and he was honoured by burial in Westminster Abbey.\n\nCharles Robert Darwin was born in Shrewsbury, Shropshire, on 12 February 1809, at his family's home, The Mount. He was the fifth of six children of wealthy society doctor and financier Robert Darwin and Susannah Darwin (\"née\" Wedgwood). He was the grandson of two prominent abolitionists: Erasmus Darwin on his father's side, and Josiah Wedgwood on his mother's side.\nBoth families were largely Unitarian, though the Wedgwoods were adopting Anglicanism. Robert Darwin, himself quietly a freethinker, had baby Charles baptised in November 1809 in the Anglican St Chad's Church, Shrewsbury, but Charles and his siblings attended the Unitarian chapel with their mother. The eight-year-old Charles already had a taste for natural history and collecting when he joined the day school run by its preacher in 1817. That July, his mother died. From September 1818, he joined his older brother Erasmus attending the nearby Anglican Shrewsbury School as a boarder.\n\nDarwin spent the summer of 1825 as an apprentice doctor, helping his father treat the poor of Shropshire, before going to the University of Edinburgh Medical School (at the time the best medical school in the UK) with his brother Erasmus in October 1825. Darwin found lectures dull and surgery distressing, so he neglected his studies. He learned taxidermy in around 40 daily hour-long sessions from John Edmonstone, a freed black slave who had accompanied Charles Waterton in the South American rainforest.\n\nIn Darwin's second year at the university he joined the Plinian Society, a student natural-history group featuring lively debates in which radical democratic students with materialistic views challenged orthodox religious concepts of science. He assisted Robert Edmond Grant's investigations of the anatomy and life cycle of marine invertebrates in the Firth of Forth, and on 27 March 1827 presented at the Plinian his own discovery that black spores found in oyster shells were the eggs of a skate leech. One day, Grant praised Lamarck's evolutionary ideas. Darwin was astonished by Grant's audacity, but had recently read similar ideas in his grandfather Erasmus' journals. Darwin was rather bored by Robert Jameson's natural-history course, which covered geology – including the debate between Neptunism and Plutonism. He learned the classification of plants, and assisted with work on the collections of the University Museum, one of the largest museums in Europe at the time.\n\nDarwin's neglect of medical studies annoyed his father, who shrewdly sent him to Christ's College, Cambridge, to study for a Bachelor of Arts degree as the first step towards becoming an Anglican country parson. As Darwin was unqualified for the \"Tripos\", he joined the \"ordinary\" degree course in January 1828. He preferred riding and shooting to studying. His cousin William Darwin Fox introduced him to the popular craze for beetle collecting; Darwin pursued this zealously, getting some of his finds published in James Francis Stephens' \"Illustrations of British entomology\". He became a close friend and follower of botany professor John Stevens Henslow and met other leading parson-naturalists who saw scientific work as religious natural theology, becoming known to these dons as \"the man who walks with Henslow\". When his own exams drew near, Darwin focused on his studies and was delighted by the language and logic of William Paley's \"Evidences of Christianity\" (1794). In his final examination in January 1831 Darwin did well, coming tenth out of 178 candidates for the \"ordinary\" degree.\n\nDarwin had to stay at Cambridge until June 1831. He studied Paley's \"Natural Theology or Evidences of the Existence and Attributes of the Deity\" (first published in 1802), which made an argument for divine design in nature, explaining adaptation as God acting through laws of nature. He read John Herschel's new book, \"Preliminary Discourse on the Study of Natural Philosophy\" (1831), which described the highest aim of natural philosophy as understanding such laws through inductive reasoning based on observation, and Alexander von Humboldt's \"Personal Narrative\" of scientific travels in 1799–1804. Inspired with \"a burning zeal\" to contribute, Darwin planned to visit Tenerife with some classmates after graduation to study natural history in the tropics. In preparation, he joined Adam Sedgwick's geology course, then travelled with him in the summer for a fortnight, in order to map strata in Wales.\n\nAfter a week with student friends at Barmouth, Darwin returned home on 29 August to find a letter from Henslow proposing him as a suitable (if unfinished) naturalist for a self-funded supernumerary place on with captain Robert FitzRoy, emphasising that this was a position for a gentleman rather than \"a mere collector\". The ship was to leave in four weeks on an expedition to chart the coastline of South America. Robert Darwin objected to his son's planned two-year voyage, regarding it as a waste of time, but was persuaded by his brother-in-law, Josiah Wedgwood II, to agree to (and fund) his son's participation. Darwin took care to remain in a private capacity to retain control over his collection, intending it for a major scientific institution.\n\nAfter delays, the voyage began on 27 December 1831; it lasted almost five years. As FitzRoy had intended, Darwin spent most of that time on land investigating geology and making natural history collections, while HMS \"Beagle\" surveyed and charted coasts. He kept careful notes of his observations and theoretical speculations, and at intervals during the voyage his specimens were sent to Cambridge together with letters including a copy of his journal for his family. He had some expertise in geology, beetle collecting and dissecting marine invertebrates, but in all other areas was a novice and ably collected specimens for expert appraisal. Despite suffering badly from seasickness, Darwin wrote copious notes while on board the ship. Most of his zoology notes are about marine invertebrates, starting with plankton collected in a calm spell.\n\nOn their first stop ashore at St Jago in Cape Verde, Darwin found that a white band high in the volcanic rock cliffs included seashells. FitzRoy had given him the first volume of Charles Lyell's \"Principles of Geology\", which set out uniformitarian concepts of land slowly rising or falling over immense periods, and Darwin saw things Lyell's way, theorising and thinking of writing a book on geology.\nWhen they reached Brazil, Darwin was delighted by the tropical forest, but detested the sight of slavery, and disputed this issue with Fitzroy.\n\nThe survey continued to the south in Patagonia. They stopped at Bahía Blanca, and in cliffs near Punta Alta Darwin made a major find of fossil bones of huge extinct mammals beside modern seashells, indicating recent extinction with no signs of change in climate or catastrophe. He identified the little-known \"Megatherium\" by a tooth and its association with bony armour, which had at first seemed to him to be like a giant version of the armour on local armadillos. The finds brought great interest when they reached England.\n\nOn rides with gauchos into the interior to explore geology and collect more fossils, Darwin gained social, political and anthropological insights into both native and colonial people at a time of revolution, and learnt that two types of rhea had separate but overlapping territories. Further south, he saw stepped plains of shingle and seashells as raised beaches showing a series of elevations. He read Lyell's second volume and accepted its view of \"centres of creation\" of species, but his discoveries and theorising challenged Lyell's ideas of smooth continuity and of extinction of species.\nThree Fuegians on board had been seized during the first \"Beagle\" voyage, then during a year in England were educated as missionaries. Darwin found them friendly and civilised, yet at Tierra del Fuego he met \"miserable, degraded savages\", as different as wild from domesticated animals. He remained convinced that, despite this diversity, all humans were interrelated with a shared origin and potential for improvement towards civilisation. Unlike his scientist friends, he now thought there was no unbridgeable gap between humans and animals. A year on, the mission had been abandoned. The Fuegian they had named Jemmy Button lived like the other natives, had a wife, and had no wish to return to England.\n\nDarwin experienced an earthquake in Chile and saw signs that the land had just been raised, including mussel-beds stranded above high tide. High in the Andes he saw seashells, and several fossil trees that had grown on a sand beach. He theorised that as the land rose, oceanic islands sank, and coral reefs round them grew to form atolls.\n\nOn the geologically new Galápagos Islands, Darwin looked for evidence attaching wildlife to an older \"centre of creation\", and found mockingbirds allied to those in Chile but differing from island to island. He heard that slight variations in the shape of tortoise shells showed which island they came from, but failed to collect them, even after eating tortoises taken on board as food. In Australia, the marsupial rat-kangaroo and the platypus seemed so unusual that Darwin thought it was almost as though two distinct Creators had been at work. He found the Aborigines \"good-humoured & pleasant\", and noted their depletion by European settlement.\n\nFitzRoy investigated how the atolls of the Cocos (Keeling) Islands had formed, and the survey supported Darwin's theorising. FitzRoy began writing the official \"Narrative\" of the \"Beagle\" voyages, and after reading Darwin's diary he proposed incorporating it into the account. Darwin's \"Journal\" was eventually rewritten as a separate third volume, on natural history.\n\nIn Cape Town, Darwin and FitzRoy met John Herschel, who had recently written to Lyell praising his uniformitarianism as opening bold speculation on \"that mystery of mysteries, the replacement of extinct species by others\" as \"a natural in contradistinction to a miraculous process\".\nWhen organising his notes as the ship sailed home, Darwin wrote that, if his growing suspicions about the mockingbirds, the tortoises and the Falkland Islands fox were correct, \"such facts undermine the stability of Species\", then cautiously added \"would\" before \"undermine\". He later wrote that such facts \"seemed to me to throw some light on the origin of species\".\n\nWhen the ship reached Falmouth, Cornwall, on 2 October 1836, Darwin was already a celebrity in scientific circles as in December 1835 Henslow had fostered his former pupil's reputation by giving selected naturalists a pamphlet of Darwin's geological letters. Darwin visited his home in Shrewsbury and saw relatives, then hurried to Cambridge to see Henslow, who advised him on finding naturalists available to catalogue the collections and agreed to take on the botanical specimens. Darwin's father organised investments, enabling his son to be a self-funded gentleman scientist, and an excited Darwin went round the London institutions being fêted and seeking experts to describe the collections. Zoologists had a huge backlog of work, and there was a danger of specimens just being left in storage.\n\nCharles Lyell eagerly met Darwin for the first time on 29 October and soon introduced him to the up-and-coming anatomist Richard Owen, who had the facilities of the Royal College of Surgeons to work on the fossil bones collected by Darwin. Owen's surprising results included other gigantic extinct ground sloths as well as the \"Megatherium\", a near complete skeleton of the unknown \"Scelidotherium\" and a hippopotamus-sized rodent-like skull named \"Toxodon\" resembling a giant capybara. The armour fragments were actually from \"Glyptodon\", a huge armadillo-like creature as Darwin had initially thought. These extinct creatures were related to living species in South America.\n\nIn mid-December, Darwin took lodgings in Cambridge to organise work on his collections and rewrite his \"Journal\". He wrote his first paper, showing that the South American landmass was slowly rising, and with Lyell's enthusiastic backing read it to the Geological Society of London on 4 January 1837. On the same day, he presented his mammal and bird specimens to the Zoological Society. The ornithologist John Gould soon announced that the Galapagos birds that Darwin had thought a mixture of blackbirds, \"gros-beaks\" and finches, were, in fact, twelve separate species of finches. On 17 February, Darwin was elected to the Council of the Geological Society, and Lyell's presidential address presented Owen's findings on Darwin's fossils, stressing geographical continuity of species as supporting his uniformitarian ideas.\n\nEarly in March, Darwin moved to London to be near this work, joining Lyell's social circle of scientists and experts such as Charles Babbage, who described God as a programmer of laws. Darwin stayed with his freethinking brother Erasmus, part of this Whig circle and a close friend of the writer Harriet Martineau, who promoted Malthusianism underlying the controversial Whig Poor Law reforms to stop welfare from causing overpopulation and more poverty. As a Unitarian, she welcomed the radical implications of transmutation of species, promoted by Grant and younger surgeons influenced by Geoffroy. Transmutation was anathema to Anglicans defending social order, but reputable scientists openly discussed the subject and there was wide interest in John Herschel's letter praising Lyell's approach as a way to find a natural cause of the origin of new species.\n\nGould met Darwin and told him that the Galápagos mockingbirds from different islands were separate species, not just varieties, and what Darwin had thought was a \"wren\" was also in the finch group. Darwin had not labelled the finches by island, but from the notes of others on the ship, including FitzRoy, he allocated species to islands. The two rheas were also distinct species, and on 14 March Darwin announced how their distribution changed going southwards.\nBy mid-March, Darwin was speculating in his \"Red Notebook\" on the possibility that \"one species does change into another\" to explain the geographical distribution of living species such as the rheas, and extinct ones such as the strange \"Macrauchenia\", which resembled a giant guanaco. His thoughts on lifespan, asexual reproduction and sexual reproduction developed in his \"B\" notebook around mid-July on to variation in offspring \"to adapt & alter the race to \"changing\" world\" explaining the Galápagos tortoises, mockingbirds and rheas. He sketched branching descent, then a genealogical branching of a single evolutionary tree, in which \"It is absurd to talk of one animal being higher than another\", discarding Lamarck's independent lineages progressing to higher forms.\n\nWhile developing this intensive study of transmutation, Darwin became mired in more work. Still rewriting his \"Journal\", he took on editing and publishing the expert reports on his collections, and with Henslow's help obtained a Treasury grant of £1,000 to sponsor this multi-volume \"Zoology of the Voyage of H.M.S. Beagle\", a sum equivalent to about £ in 2016. He stretched the funding to include his planned books on geology, and agreed to unrealistic dates with the publisher. As the Victorian era began, Darwin pressed on with writing his \"Journal\", and in August 1837 began correcting printer's proofs.\n\nDarwin's health suffered under the pressure. On 20 September he had \"an uncomfortable palpitation of the heart\", so his doctors urged him to \"knock off all work\" and live in the country for a few weeks. After visiting Shrewsbury he joined his Wedgwood relatives at Maer Hall, Staffordshire, but found them too eager for tales of his travels to give him much rest. His charming, intelligent, and cultured cousin Emma Wedgwood, nine months older than Darwin, was nursing his invalid aunt. His uncle Josiah pointed out an area of ground where cinders had disappeared under loam and suggested that this might have been the work of earthworms, inspiring \"a new & important theory\" on their role in soil formation, which Darwin presented at the Geological Society on 1 November.\n\nWilliam Whewell pushed Darwin to take on the duties of Secretary of the Geological Society. After initially declining the work, he accepted the post in March 1838. Despite the grind of writing and editing the \"Beagle\" reports, Darwin made remarkable progress on transmutation, taking every opportunity to question expert naturalists and, unconventionally, people with practical experience such as farmers and pigeon fanciers. Over time, his research drew on information from his relatives and children, the family butler, neighbours, colonists and former shipmates. He included mankind in his speculations from the outset, and on seeing an orangutan in the zoo on 28 March 1838 noted its childlike behaviour.\nThe strain took a toll, and by June he was being laid up for days on end with stomach problems, headaches and heart symptoms. For the rest of his life, he was repeatedly incapacitated with episodes of stomach pains, vomiting, severe boils, palpitations, trembling and other symptoms, particularly during times of stress, such as attending meetings or making social visits. The cause of Darwin's illness remained unknown, and attempts at treatment had little success.\n\nOn 23 June, he took a break and went \"geologising\" in Scotland. He visited Glen Roy in glorious weather to see the parallel \"roads\" cut into the hillsides at three heights. He later published his view that these were marine raised beaches, but then had to accept that they were shorelines of a proglacial lake.\n\nFully recuperated, he returned to Shrewsbury in July. Used to jotting down daily notes on animal breeding, he scrawled rambling thoughts about career and prospects on two scraps of paper, one with columns headed \"\"Marry\"\" and \"\"Not Marry\"\". Advantages included \"constant companion and a friend in old age ... better than a dog anyhow\", against points such as \"less money for books\" and \"terrible loss of time.\" Having decided in favour, he discussed it with his father, then went to visit Emma on 29 July. He did not get around to proposing, but against his father's advice he mentioned his ideas on transmutation.\n\nContinuing his research in London, Darwin's wide reading now included the sixth edition of Malthus's \"An Essay on the Principle of Population\", and on 28 September 1838 he noted its assertion that human \"population, when unchecked, goes on doubling itself every twenty five years, or increases in a geometrical ratio\", a geometric progression so that population soon exceeds food supply in what is known as a Malthusian catastrophe. Darwin was well prepared to compare this to de Candolle's \"warring of the species\" of plants and the struggle for existence among wildlife, explaining how numbers of a species kept roughly stable. As species always breed beyond available resources, favourable variations would make organisms better at surviving and passing the variations on to their offspring, while unfavourable variations would be lost. He wrote that the \"final cause of all this wedging, must be to sort out proper structure, & adapt it to changes\", so that \"One may say there is a force like a hundred thousand wedges trying force into every kind of adapted structure into the gaps of in the economy of nature, or rather forming gaps by thrusting out weaker ones.\" This would result in the formation of new species. As he later wrote in his \"Autobiography\":\n\nBy mid December, Darwin saw a similarity between farmers picking the best stock in selective breeding, and a Malthusian Nature selecting from chance variants so that \"every part of newly acquired structure is fully practical and perfected\", thinking this comparison \"a beautiful part of my theory\". He later called his theory natural selection, an analogy with what he termed the artificial selection of selective breeding.\n\nOn 11 November, he returned to Maer and proposed to Emma, once more telling her his ideas. She accepted, then in exchanges of loving letters she showed how she valued his openness in sharing their differences, also expressing her strong Unitarian beliefs and concerns that his honest doubts might separate them in the afterlife. While he was house-hunting in London, bouts of illness continued and Emma wrote urging him to get some rest, almost prophetically remarking \"So don't be ill any more my dear Charley till I can be with you to nurse you.\" He found what they called \"Macaw Cottage\" (because of its gaudy interiors) in Gower Street, then moved his \"museum\" in over Christmas. On 24 January 1839, Darwin was elected a Fellow of the Royal Society (FRS).\n\nOn 29 January, Darwin and Emma Wedgwood were married at Maer in an Anglican ceremony arranged to suit the Unitarians, then immediately caught the train to London and their new home.\n\nDarwin now had the framework of his theory of natural selection \"by which to work\", as his \"prime hobby\". His research included extensive experimental selective breeding of plants and animals, finding evidence that species were not fixed and investigating many detailed ideas to refine and substantiate his theory. For fifteen years this work was in the background to his main occupation of writing on geology and publishing expert reports on the \"Beagle\" collections.\n\nWhen FitzRoy's \"Narrative\" was published in May 1839, Darwin's \"Journal and Remarks\" was such a success as the third volume that later that year it was published on its own. Early in 1842, Darwin wrote about his ideas to Charles Lyell, who noted that his ally \"denies seeing a beginning to each crop of species\".\n\nDarwin's book \"The Structure and Distribution of Coral Reefs\" on his theory of atoll formation was published in May 1842 after more than three years of work, and he then wrote his first \"pencil sketch\" of his theory of natural selection. To escape the pressures of London, the family moved to rural Down House in September. On 11 January 1844, Darwin mentioned his theorising to the botanist Joseph Dalton Hooker, writing with melodramatic humour \"it is like confessing a murder\". Hooker replied \"There may in my opinion have been a series of productions on different spots, & also a gradual change of species. I shall be delighted to hear how you think that this change may have taken place, as no presently conceived opinions satisfy me on the subject.\"\n\nBy July, Darwin had expanded his \"sketch\" into a 230-page \"Essay\", to be expanded with his research results if he died prematurely. In November, the anonymously published sensational best-seller \"Vestiges of the Natural History of Creation\" brought wide interest in transmutation. Darwin scorned its amateurish geology and zoology, but carefully reviewed his own arguments. Controversy erupted, and it continued to sell well despite contemptuous dismissal by scientists.\n\nDarwin completed his third geological book in 1846. He now renewed a fascination and expertise in marine invertebrates, dating back to his student days with Grant, by dissecting and classifying the barnacles he had collected on the voyage, enjoying observing beautiful structures and thinking about comparisons with allied structures. In 1847, Hooker read the \"Essay\" and sent notes that provided Darwin with the calm critical feedback that he needed, but would not commit himself and questioned Darwin's opposition to continuing acts of creation.\n\nIn an attempt to improve his chronic ill health, Darwin went in 1849 to Dr. James Gully's Malvern spa and was surprised to find some benefit from hydrotherapy. Then, in 1851, his treasured daughter Annie fell ill, reawakening his fears that his illness might be hereditary, and after a long series of crises she died.\n\nIn eight years of work on barnacles (Cirripedia), Darwin's theory helped him to find \"homologies\" showing that slightly changed body parts served different functions to meet new conditions, and in some genera he found minute males parasitic on hermaphrodites, showing an intermediate stage in evolution of distinct sexes. In 1853, it earned him the Royal Society's Royal Medal, and it made his reputation as a biologist. In 1854 he became a Fellow of the Linnean Society of London, gaining postal access to its library. He began a major reassessment of his theory of species, and in November realised that divergence in the character of descendants could be explained by them becoming adapted to \"diversified places in the economy of nature\".\n\nBy the start of 1856, Darwin was investigating whether eggs and seeds could survive travel across seawater to spread species across oceans. Hooker increasingly doubted the traditional view that species were fixed, but their young friend Thomas Henry Huxley was firmly against the transmutation of species. Lyell was intrigued by Darwin's speculations without realising their extent. When he read a paper by Alfred Russel Wallace, \"On the Law which has Regulated the Introduction of New Species\", he saw similarities with Darwin's thoughts and urged him to publish to establish precedence. Though Darwin saw no threat, on 14 May 1856 he began writing a short paper. Finding answers to difficult questions held him up repeatedly, and he expanded his plans to a \"big book on species\" titled \"Natural Selection\", which was to include his \"note on Man\". He continued his researches, obtaining information and specimens from naturalists worldwide including Wallace who was working in Borneo. In mid-1857 he added a section heading; \"Theory applied to Races of Man\", but did not add text on this topic. On 5 September 1857, Darwin sent the American botanist Asa Gray a detailed outline of his ideas, including an abstract of \"Natural Selection\", which omitted human origins and sexual selection. In December, Darwin received a letter from Wallace asking if the book would examine human origins. He responded that he would avoid that subject, \"so surrounded with prejudices\", while encouraging Wallace's theorising and adding that \"I go much further than you.\"\n\nDarwin's book was only partly written when, on 18 June 1858, he received a paper from Wallace describing natural selection. Shocked that he had been \"forestalled\", Darwin sent it on that day to Lyell, as requested by Wallace, and although Wallace had not asked for publication, Darwin suggested he would send it to any journal that Wallace chose. His family was in crisis with children in the village dying of scarlet fever, and he put matters in the hands of his friends. After some discussion, Lyell and Hooker decided on a joint presentation at the Linnean Society on 1 July of \"On the Tendency of Species to form Varieties; and on the Perpetuation of Varieties and Species by Natural Means of Selection\". On the evening of 28 June, Darwin's baby son died of scarlet fever after almost a week of severe illness, and he was too distraught to attend.\n\nThere was little immediate attention to this announcement of the theory; the president of the Linnean Society remarked in May 1859 that the year had not been marked by any revolutionary discoveries. Only one review rankled enough for Darwin to recall it later; Professor Samuel Haughton of Dublin claimed that \"all that was new in them was false, and what was true was old\". Darwin struggled for thirteen months to produce an abstract of his \"big book\", suffering from ill health but getting constant encouragement from his scientific friends. Lyell arranged to have it published by John Murray.\n\n\"On the Origin of Species\" proved unexpectedly popular, with the entire stock of 1,250 copies oversubscribed when it went on sale to booksellers on 22 November 1859. In the book, Darwin set out \"one long argument\" of detailed observations, inferences and consideration of anticipated objections. In making the case for common descent, he included evidence of homologies between humans and other mammals. Having outlined sexual selection, he hinted that it could explain differences between human races. He avoided explicit discussion of human origins, but implied the significance of his work with the sentence; \"Light will be thrown on the origin of man and his history.\" His theory is simply stated in the introduction:\nAt the end of the book he concluded that:\nThe last word was the only variant of \"evolved\" in the first five editions of the book. \"Evolutionism\" at that time was associated with other concepts, most commonly with embryological development, and Darwin first used the word evolution in \"The Descent of Man\" in 1871, before adding it in 1872 to the 6th edition of \"The Origin of Species\".\n\nThe book aroused international interest, with less controversy than had greeted the popular \"Vestiges of the Natural History of Creation\". Though Darwin's illness kept him away from the public debates, he eagerly scrutinised the scientific response, commenting on press cuttings, reviews, articles, satires and caricatures, and corresponded on it with colleagues worldwide. The book did not explicitly discuss human origins, but included a number of hints about the animal ancestry of humans from which the inference could be made. The first review asked, \"If a monkey has become a man–what may not a man become?\" and said it should be left to theologians as it was too dangerous for ordinary readers. Amongst early favourable responses, Huxley's reviews swiped at Richard Owen, leader of the scientific establishment Huxley was trying to overthrow. In April, Owen's review attacked Darwin's friends and condescendingly dismissed his ideas, angering Darwin, but Owen and others began to promote ideas of supernaturally guided evolution. Patrick Matthew drew attention to his 1831 book which had a brief appendix suggesting a concept of natural selection leading to new species, but he had not developed the idea.\n\nThe Church of England's response was mixed. Darwin's old Cambridge tutors Sedgwick and Henslow dismissed the ideas, but liberal clergymen interpreted natural selection as an instrument of God's design, with the cleric Charles Kingsley seeing it as \"just as noble a conception of Deity\". In 1860, the publication of \"Essays and Reviews\" by seven liberal Anglican theologians diverted clerical attention from Darwin, with its ideas including higher criticism attacked by church authorities as heresy. In it, Baden Powell argued that miracles broke God's laws, so belief in them was atheistic, and praised \"Mr Darwin's masterly volume [supporting] the grand principle of the self-evolving powers of nature\". Asa Gray discussed teleology with Darwin, who imported and distributed Gray's pamphlet on theistic evolution, \"Natural Selection is not inconsistent with natural theology\". The most famous confrontation was at the public 1860 Oxford evolution debate during a meeting of the British Association for the Advancement of Science, where the Bishop of Oxford Samuel Wilberforce, though not opposed to transmutation of species, argued against Darwin's explanation and human descent from apes. Joseph Hooker argued strongly for Darwin, and Thomas Huxley's legendary retort, that he would rather be descended from an ape than a man who misused his gifts, came to symbolise a triumph of science over religion.\n\nEven Darwin's close friends Gray, Hooker, Huxley and Lyell still expressed various reservations but gave strong support, as did many others, particularly younger naturalists. Gray and Lyell sought reconciliation with faith, while Huxley portrayed a polarisation between religion and science. He campaigned pugnaciously against the authority of the clergy in education, aiming to overturn the dominance of clergymen and aristocratic amateurs under Owen in favour of a new generation of professional scientists. Owen's claim that brain anatomy proved humans to be a separate biological order from apes was shown to be false by Huxley in a long running dispute parodied by Kingsley as the \"Great Hippocampus Question\", and discredited Owen.\n\nDarwinism became a movement covering a wide range of evolutionary ideas. In 1863 Lyell's \"Geological Evidences of the Antiquity of Man\" popularised prehistory, though his caution on evolution disappointed Darwin. Weeks later Huxley's \"Evidence as to Man's Place in Nature\" showed that anatomically, humans are apes, then \"The Naturalist on the River Amazons\" by Henry Walter Bates provided empirical evidence of natural selection. Lobbying brought Darwin Britain's highest scientific honour, the Royal Society's Copley Medal, awarded on 3 November 1864. That day, Huxley held the first meeting of what became the influential \"X Club\" devoted to \"science, pure and free, untrammelled by religious dogmas\". By the end of the decade most scientists agreed that evolution occurred, but only a minority supported Darwin's view that the chief mechanism was natural selection.\n\nThe \"Origin of Species\" was translated into many languages, becoming a staple scientific text attracting thoughtful attention from all walks of life, including the \"working men\" who flocked to Huxley's lectures. Darwin's theory also resonated with various movements at the time and became a key fixture of popular culture. Cartoonists parodied animal ancestry in an old tradition of showing humans with animal traits, and in Britain these droll images served to popularise Darwin's theory in an unthreatening way. While ill in 1862 Darwin began growing a beard, and when he reappeared in public in 1866 caricatures of him as an ape helped to identify all forms of evolutionism with Darwinism.\n\nDespite repeated bouts of illness during the last twenty-two years of his life, Darwin's work continued. Having published \"On the Origin of Species\" as an abstract of his theory, he pressed on with experiments, research, and writing of his \"big book\". He covered human descent from earlier animals including evolution of society and of mental abilities, as well as explaining decorative beauty in wildlife and diversifying into innovative plant studies.\n\nEnquiries about insect pollination led in 1861 to novel studies of wild orchids, showing adaptation of their flowers to attract specific moths to each species and ensure cross fertilisation. In 1862 \"Fertilisation of Orchids\" gave his first detailed demonstration of the power of natural selection to explain complex ecological relationships, making testable predictions. As his health declined, he lay on his sickbed in a room filled with inventive experiments to trace the movements of climbing plants. Admiring visitors included Ernst Haeckel, a zealous proponent of \"Darwinismus\" incorporating Lamarckism and Goethe's idealism. Wallace remained supportive, though he increasingly turned to Spiritualism.\n\nDarwin's book \"The Variation of Animals and Plants under Domestication\" (1868) was the first part of his planned \"big book\", and included his unsuccessful hypothesis of pangenesis attempting to explain heredity. It sold briskly at first, despite its size, and was translated into many languages. He wrote most of a second part, on natural selection, but it remained unpublished in his lifetime.\nLyell had already popularised human prehistory, and Huxley had shown that anatomically humans are apes. With \"The Descent of Man, and Selection in Relation to Sex\" published in 1871, Darwin set out evidence from numerous sources that humans are animals, showing continuity of physical and mental attributes, and presented sexual selection to explain impractical animal features such as the peacock's plumage as well as human evolution of culture, differences between sexes, and physical and cultural racial characteristics, while emphasising that humans are all one species. His research using images was expanded in his 1872 book \"The Expression of the Emotions in Man and Animals\", one of the first books to feature printed photographs, which discussed the evolution of human psychology and its continuity with the behaviour of animals. Both books proved very popular, and Darwin was impressed by the general assent with which his views had been received, remarking that \"everybody is talking about it without being shocked.\" His conclusion was \"that man with all his noble qualities, with sympathy which feels for the most debased, with benevolence which extends not only to other men but to the humblest living creature, with his god-like intellect which has penetrated into the movements and constitution of the solar system–with all these exalted powers–Man still bears in his bodily frame the indelible stamp of his lowly origin.\"\n\nHis evolution-related experiments and investigations led to books on Orchids, \"Insectivorous Plants, The Effects of Cross and Self Fertilisation in the Vegetable Kingdom\", different forms of flowers on plants of the same species, and \"The Power of Movement in Plants\". His botanical work was interpreted and popularised by various writers including Grant Allen and H. G. Wells, and helped transform plant science in the late C19 and early C20. In his last book he returned to \"The Formation of Vegetable Mould through the Action of Worms\".\n\nIn 1882 he was diagnosed with what was called \"angina pectoris\" which then meant coronary thrombosis and disease of the heart. At the time of his death, the physicians diagnosed \"anginal attacks\", and \"heart-failure\". Today it is speculated that Darwin was suffering from chronic Chagas disease. This speculation is based on a journal entry written by Darwin, describing he was bitten by the \"Kissing Bug\" in Mendoza, Argentina in 1835; and based on the constellation of clinical symptoms he exhibited, including cardiac disease which is a hallmark of chronic Chagas disease. Exhuming Darwin's body is likely necessary to definitively determine his state of infection by detecting DNA of infecting parasite, T. cruzi, that causes Chagas disease . \n\nHe died at Down House on 19 April 1882. His last words were to his family, telling Emma \"I am not the least afraid of death – Remember what a good wife you have been to me – Tell all my children to remember how good they have been to me\", then while she rested, he repeatedly told Henrietta and Francis \"It's almost worth while to be sick to be nursed by you\". He had expected to be buried in St Mary's churchyard at Downe, but at the request of Darwin's colleagues, after public and parliamentary petitioning, William Spottiswoode (President of the Royal Society) arranged for Darwin to be honoured by burial in Westminster Abbey, close to John Herschel and Isaac Newton. The funeral was held on Wednesday 26 April and was attended by thousands of people, including family, friends, scientists, philosophers and dignitaries.\n\nBy the time of his death, Darwin had convinced most scientists that evolution as descent with modification was correct, and he was regarded as a great scientist who had revolutionised ideas. In June 1909, though few at that time agreed with his view that \"natural selection has been the main but not the exclusive means of modification\", he was honoured by more than 400 officials and scientists from across the world who met in Cambridge to commemorate his centenary and the fiftieth anniversary of \"On the Origin of Species\". Around the beginning of the 20th century, a period that has been called \"the eclipse of Darwinism\", scientists proposed various alternative evolutionary mechanisms, which eventually proved untenable. Ronald Fisher, an English statistician, finally united Mendelian genetics with natural selection, in the period between 1918 and his 1930 book \"The Genetical Theory of Natural Selection\". He gave the theory a mathematical footing and brought broad scientific consensus that natural selection was the basic mechanism of evolution, thus founding the basis for population genetics and the modern evolutionary synthesis, with J.B.S. Haldane and Sewall Wright, which set the frame of reference for modern debates and refinements of the theory.\n\nDuring Darwin's lifetime, many geographical features were given his name. An expanse of water adjoining the Beagle Channel was named \"Darwin Sound\" by Robert FitzRoy after Darwin's prompt action, along with two or three of the men, saved them from being marooned on a nearby shore when a collapsing glacier caused a large wave that would have swept away their boats, and the nearby Mount Darwin in the Andes was named in celebration of Darwin's 25th birthday. When the \"Beagle\" was surveying Australia in 1839, Darwin's friend John Lort Stokes sighted a natural harbour which the ship's captain Wickham named \"Port Darwin\": a nearby settlement was renamed Darwin in 1911, and it became the capital city of Australia's Northern Territory.\n\nMore than 120 species and nine genera have been named after Darwin. In one example, the group of tanagers related to those Darwin found in the Galápagos Islands became popularly known as \"Darwin's finches\" in 1947, fostering inaccurate legends about their significance to his work.\n\nDarwin's work has continued to be celebrated by numerous publications and events. The Linnean Society of London has commemorated Darwin's achievements by the award of the Darwin–Wallace Medal since 1908. Darwin Day has become an annual celebration, and in 2009 worldwide events were arranged for the bicentenary of Darwin's birth and the 150th anniversary of the publication of \"On the Origin of Species\".\n\nDarwin has been commemorated in the UK, with his portrait printed on the reverse of £10 banknotes printed along with a hummingbird and HMS \"Beagle\", issued by the Bank of England.\n\nA life-size seated statue of Darwin can be seen in the main hall of the Natural History Museum in London.\nA seated statue of Darwin, unveiled 1897, stands in front of Shrewsbury Library, the building that used to house Shrewsbury School, which Darwin attended as a boy. Another statue of Darwin as a young man is situated in the grounds of Christ's College, Cambridge.\n\nDarwin College, a postgraduate college at Cambridge University, is named after the Darwin family.\n\nThe Darwins had ten children: two died in infancy, and Annie's death at the age of ten had a devastating effect on her parents. Charles was a devoted father and uncommonly attentive to his children. Whenever they fell ill, he feared that they might have inherited weaknesses from inbreeding due to the close family ties he shared with his wife and cousin, Emma Wedgwood. He examined this topic in his writings, contrasting it with the advantages of crossing amongst many organisms. Despite his fears, most of the surviving children and many of their descendants went on to have distinguished careers (see Darwin-Wedgwood family).\n\nOf his surviving children, George, Francis and Horace became Fellows of the Royal Society, distinguished as astronomer, botanist and civil engineer, respectively. All three were knighted. Another son, Leonard, went on to be a soldier, politician, economist, eugenicist and mentor of the statistician and evolutionary biologist Ronald Fisher.\n\nDarwin's family tradition was nonconformist Unitarianism, while his father and grandfather were freethinkers, and his baptism and boarding school were Church of England. When going to Cambridge to become an Anglican clergyman, he did not doubt the literal truth of the Bible. He learned John Herschel's science which, like William Paley's natural theology, sought explanations in laws of nature rather than miracles and saw adaptation of species as evidence of design. On board HMS \"Beagle\", Darwin was quite orthodox and would quote the Bible as an authority on morality. He looked for \"centres of creation\" to explain distribution, and related the antlion found near kangaroos to distinct \"periods of Creation\".\n\nBy his return, he was critical of the Bible as history, and wondered why all religions should not be equally valid. In the next few years, while intensively speculating on geology and the transmutation of species, he gave much thought to religion and openly discussed this with his wife Emma, whose beliefs also came from intensive study and questioning. The theodicy of Paley and Thomas Malthus vindicated evils such as starvation as a result of a benevolent creator's laws, which had an overall good effect. To Darwin, natural selection produced the good of adaptation but removed the need for design, and he could not see the work of an omnipotent deity in all the pain and suffering, such as the ichneumon wasp paralysing caterpillars as live food for its eggs. He still viewed organisms as perfectly adapted, and \"On the Origin of Species\" reflects theological views. Though he thought of religion as a tribal survival strategy, Darwin was reluctant to give up the idea of God as an ultimate lawgiver. He was increasingly troubled by the problem of evil.\n\nDarwin remained close friends with the vicar of Downe, John Brodie Innes, and continued to play a leading part in the parish work of the church, but from around 1849 would go for a walk on Sundays while his family attended church. He considered it \"absurd to doubt that a man might be an ardent theist and an evolutionist\" and, though reticent about his religious views, in 1879 he wrote that \"I have never been an atheist in the sense of denying the existence of a God. – I think that generally ... an agnostic would be the most correct description of my state of mind\".\n\nThe \"Lady Hope Story\", published in 1915, claimed that Darwin had reverted to Christianity on his sickbed. The claims were repudiated by Darwin's children and have been dismissed as false by historians.\n\nDarwin's views on social and political issues reflected his time and social position. He grew up in a family of Whig reformers who, like his uncle Josiah Wedgwood, supported electoral reform and the emancipation of slaves. Darwin was passionately opposed to slavery, while seeing no problem with the working conditions of English factory workers or servants. His taxidermy lessons in 1826 from the freed slave John Edmonstone, who he long recalled as \"a very pleasant and intelligent man\", reinforced his belief that black people shared the same feelings, and could be as intelligent as people of other races. He took the same attitude to native people he met on the \"Beagle\" voyage. These attitudes were not unusual in Britain in the 1820s, much as it shocked visiting Americans. British society became more racist in mid century, but Darwin remained strongly against slavery, against \"ranking the so-called races of man as distinct species\", and against ill-treatment of native people. He valued European civilisation and saw colonisation as spreading its benefits, with the sad but inevitable effect that savage peoples who did not become civilised faced extinction. Darwin's theories presented this as natural, and were cited to promote policies that went against his humanitarian principles.\n\nHe thought men's eminence over women was the outcome of sexual selection, a view disputed by Antoinette Brown Blackwell in \"The Sexes Throughout Nature\".\n\nDarwin was intrigued by his half-cousin Francis Galton's argument, introduced in 1865, that statistical analysis of heredity showed that moral and mental human traits could be inherited, and principles of animal breeding could apply to humans. In \"The Descent of Man\", Darwin noted that aiding the weak to survive and have families could lose the benefits of natural selection, but cautioned that withholding such aid would endanger the instinct of sympathy, \"the noblest part of our nature\", and factors such as education could be more important. When Galton suggested that publishing research could encourage intermarriage within a \"caste\" of \"those who are naturally gifted\", Darwin foresaw practical difficulties, and thought it \"the sole feasible, yet I fear utopian, plan of procedure in improving the human race\", preferring to simply publicise the importance of inheritance and leave decisions to individuals. Francis Galton named this field of study \"eugenics\" in 1883.\n\nDarwin's fame and popularity led to his name being associated with ideas and movements that, at times, had only an indirect relation to his writings, and sometimes went directly against his express comments.\n\nThomas Malthus had argued that population growth beyond resources was ordained by God to get humans to work productively and show restraint in getting families, this was used in the 1830s to justify workhouses and laissez-faire economics. Evolution was by then seen as having social implications, and Herbert Spencer's 1851 book \"Social Statics\" based ideas of human freedom and individual liberties on his Lamarckian evolutionary theory.\n\nSoon after the \"Origin\" was published in 1859, critics derided his description of a struggle for existence as a Malthusian justification for the English industrial capitalism of the time. The term \"Darwinism\" was used for the evolutionary ideas of others, including Spencer's \"survival of the fittest\" as free-market progress, and Ernst Haeckel's polygenistic ideas of human development. Writers used natural selection to argue for various, often contradictory, ideologies such as laissez-faire dog-eat dog capitalism, colonialism and imperialism. However, Darwin's holistic view of nature included \"dependence of one being on another\"; thus pacifists, socialists, liberal social reformers and anarchists such as Peter Kropotkin stressed the value of co-operation over struggle within a species. Darwin himself insisted that social policy should not simply be guided by concepts of struggle and selection in nature.\n\nAfter the 1880s, a eugenics movement developed on ideas of biological inheritance, and for scientific justification of their ideas appealed to some concepts of Darwinism. In Britain, most shared Darwin's cautious views on voluntary improvement and sought to encourage those with good traits in \"positive eugenics\". During the \"Eclipse of Darwinism\", a scientific foundation for eugenics was provided by Mendelian genetics. Negative eugenics to remove the \"feebleminded\" were popular in America, Canada and Australia, and eugenics in the United States introduced compulsory sterilization laws, followed by several other countries. Subsequently, Nazi eugenics brought the field into disrepute.\n\nThe term \"Social Darwinism\" was used infrequently from around the 1890s, but became popular as a derogatory term in the 1940s when used by Richard Hofstadter to attack the laissez-faire conservatism of those like William Graham Sumner who opposed reform and socialism. Since then, it has been used as a term of abuse by those opposed to what they think are the moral consequences of evolution.\n\nDarwin was a prolific writer. Even without publication of his works on evolution, he would have had a considerable reputation as the author of \"The Voyage of the Beagle\", as a geologist who had published extensively on South America and had solved the puzzle of the formation of coral atolls, and as a biologist who had published the definitive work on barnacles. While \"On the Origin of Species\" dominates perceptions of his work, \"The Descent of Man\" and \"The Expression of the Emotions in Man and Animals\" had considerable impact, and his books on plants including \"The Power of Movement in Plants\" were innovative studies of great importance, as was his final work on \"The Formation of Vegetable Mould through the Action of Worms\".\n\n. Darwin was eminent as a naturalist, geologist, biologist, and author. After working as a physician's assistant and two years as a medical student, he was educated as a clergyman; he was also trained in taxidermy.\n\n. Robert FitzRoy was to become known after the voyage for biblical literalism, but at this time he had considerable interest in Lyell's ideas, and they met before the voyage when Lyell asked for observations to be made in South America. FitzRoy's diary during the ascent of the River Santa Cruz in Patagonia recorded his opinion that the plains were raised beaches, but on return, newly married to a very religious lady, he recanted these ideas.\n\n. In the section \"Morphology\" of Chapter XIII of \"On the Origin of Species\", Darwin commented on homologous bone patterns between humans and other mammals, writing: \"What can be more curious than that the hand of a man, formed for grasping, that of a mole for digging, the leg of the horse, the paddle of the porpoise, and the wing of the bat, should all be constructed on the same pattern, and should include the same bones, in the same relative positions?\" and in the concluding chapter: \"The framework of bones being the same in the hand of a man, wing of a bat, fin of the porpoise, and leg of the horse … at once explain themselves on the theory of descent with slow and slight successive modifications.\"\nIn \"On the Origin of Species\" Darwin mentioned human origins in his concluding remark that \"In the distant future I see open fields for far more important researches. Psychology will be based on a new foundation, that of the necessary acquirement of each mental power and capacity by gradation. Light will be thrown on the origin of man and his history.\"\n\nIn \"Chapter VI: Difficulties on Theory\" he referred to sexual selection: \"I might have adduced for this same purpose the differences between the races of man, which are so strongly marked; I may add that some little light can apparently be thrown on the origin of these differences, chiefly through sexual selection of a particular kind, but without here entering on copious details my reasoning would appear frivolous.\"\n\nIn \"The Descent of Man\" of 1871, Darwin discussed the first passage:\n\"During many years I collected notes on the origin or descent of man, without any intention of publishing on the subject, but rather with the determination not to publish, as I thought that I should thus only add to the prejudices against my views. It seemed to me sufficient to indicate, in the first edition of my 'Origin of Species,' that by this work 'light would be thrown on the origin of man and his history;' and this implies that man must be included with other organic beings in any general conclusion respecting his manner of appearance on this earth.\" In a preface to the 1874 second edition, he added a reference to the second point: \"it has been said by several critics, that when I found that many details of structure in man could not be explained through natural selection, I invented sexual selection; I gave, however, a tolerably clear sketch of this principle in the first edition of the 'Origin of Species,' and I there stated that it was applicable to man.\"\n\n. See, for example, WILLA volume 4, \"Charlotte Perkins Gilman and the Feminization of Education\" by Deborah M. De Simone: \"Gilman shared many basic educational ideas with the generation of thinkers who matured during the period of \"intellectual chaos\" caused by Darwin's Origin of the Species. Marked by the belief that individuals can direct human and social evolution, many progressives came to view education as the panacea for advancing social progress and for solving such problems as urbanisation, poverty, or immigration.\"\n\n. See, for example, the song \"A lady fair of lineage high\" from Gilbert and Sullivan's \"Princess Ida\", which describes the descent of man (but not woman!) from apes.\n\n. Darwin's belief that black people had the same essential humanity as Europeans, and had many mental similarities, was reinforced by the lessons he had from John Edmonstone in 1826. Early in the \"Beagle\" voyage, Darwin nearly lost his position on the ship when he criticised FitzRoy's defence and praise of slavery. He wrote home about \"how steadily the general feeling, as shown at elections, has been rising against Slavery. What a proud thing for England if she is the first European nation which utterly abolishes it! I was told before leaving England that after living in slave countries all my opinions would be altered; the only alteration I am aware of is forming a much higher estimate of the negro character.\" Regarding Fuegians, he \"could not have believed how wide was the difference between savage and civilized man: it is greater than between a wild and domesticated animal, inasmuch as in man there is a greater power of improvement\", but he knew and liked civilised Fuegians like Jemmy Button: \"It seems yet wonderful to me, when I think over all his many good qualities, that he should have been of the same race, and doubtless partaken of the same character, with the miserable, degraded savages whom we first met here.\"\n\nIn the \"Descent of Man\", he mentioned the similarity of Fuegians' and Edmonstone's minds to Europeans' when arguing against \"ranking the so-called races of man as distinct species\".\n\nHe rejected the ill-treatment of native people, and for example wrote of massacres of Patagonian men, women, and children, \"Every one here is fully convinced that this is the most just war, because it is against barbarians. Who would believe in this age that such atrocities could be committed in a Christian civilized country?\"\n\n. Geneticists studied human heredity as Mendelian inheritance, while eugenics movements sought to manage society, with a focus on social class in the United Kingdom, and on disability and ethnicity in the United States, leading to geneticists seeing this as impractical pseudoscience. A shift from voluntary arrangements to \"negative\" eugenics included compulsory sterilisation laws in the United States, copied by Nazi Germany as the basis for Nazi eugenics based on virulent racism and \"racial hygiene\".()\n",
    "id": "8145410",
    "title": "Charles Darwin"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=59238",
    "text": "Entomology\n\nEntomology () is the scientific study of insects, a branch of zoology. In the past the term \"insect\" was more vague, and historically the definition of entomology included the study of terrestrial animals in other arthropod groups or other phyla, such as arachnids, myriapods, earthworms, land snails, and slugs. This wider meaning may still be encountered in informal use.\n\nLike several of the other fields that are categorized within zoology, entomology is a taxon-based category; any form of scientific study in which there is a focus on insect-related inquiries is, by definition, entomology. Entomology therefore overlaps with a cross-section of topics as diverse as molecular genetics, behavior, biomechanics, biochemistry, systematics, physiology, developmental biology, ecology, morphology, and paleontology.\n\nAt some 1.3 million described species, insects account for more than two-thirds of all known organisms, date back some 400 million years, and have many kinds of interactions with humans and other forms of life on earth.\n\nEntomology is rooted in nearly all human cultures from prehistoric times, primarily in the context of agriculture (especially biological control and beekeeping), but scientific study began only as recently as the 16th century.\n\nWilliam Kirby is widely considered as the father of Entomology. In collaboration with William Spence, he published a definitive entomological encyclopedia, \"Introduction to Entomology\", regarded as the subject's foundational text. He also helped to found the Royal Entomological Society in London in 1833, one of the earliest such societies in the world; earlier antecedents, such as the Aurelian society date back to the 1740s.\n\nEntomology developed rapidly in the 19th and 20th centuries, and was studied by large numbers of people, including such notable figures as Charles Darwin, Jean-Henri Fabre, Vladimir Nabokov, Karl von Frisch (winner of the 1973 Nobel Prize in Physiology or Medicine), and two-time Pulitzer Prize winner E. O. Wilson.\n\nThere has also been a history of people becoming entomologists through museum curation and research assistance, such as Sophie Lutterlough at the Smithsonian National Museum of Natural History. Insect identification is an increasingly common hobby, with butterflies and dragonflies being the most popular.\nMost insects can easily be recognized to order such as Hymenoptera (bees, wasps, and ants) or Coleoptera (beetles). However, insects other than Lepidoptera (butterflies and moths) are typically identifiable to genus or species only through the use of Identification keys and Monographs. Because the class Insecta contains a very large number of species (over 330,000 species of beetles alone) and the characteristics separating them are unfamiliar, and often subtle (or invisible without a microscope), this is often very difficult even for a specialist. This has led to the development of automated species identification systems targeted on insects, for example, Daisy, ABIS, SPIDA and Draw-wing.\n\nIn 1994 the Entomological Society of America launched a new professional certification program for the pest control industry called The Associate Certified Entomologist (ACE). To qualify as a \"true entomologist\" an individual would normally require an advanced degree, with most entomologists pursuing their PhD. While not true entomologists in the traditional sense, individuals who attain the ACE certification may be referred to as ACEs, Amateur Entomologists, or Associate Entomologists.\n\nMany entomologists specialize in a single order or even a family of insects, and a number of these subspecialties are given their own informal names, typically (but not always) derived from the scientific name of the group:\n\nLike other scientific specialties, entomologists have a number of local, national, and international organizations. There are also many organizations specializing in specific subareas.\n\nHere is a list of selected museums which contain very large insect collections.\n\nZoological survey of India \n\n\n\n\n\nEntomology has been depicted in various films such as:\n\n\n\n",
    "id": "59238",
    "title": "Entomology"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=53316821",
    "text": "Proximal Centriole-Like\n\nThe proximal centriole-like or PCL is an atypical type of centriole found in the sperm cells of insects. The PCL name is due to some similarity to the Proximal centriole found in Vertebrates sperm and the hypothesis that the two structures are homologous. The PCL is an atypical type of centriole because it does not have microtubules, a defining feature of centrioles. However, the PCL is a type of centriole for several reasons. (1) the PCL formation is dependent upon the same genetic pathway that mediates the initiation of centriole formation. (2) The PCL is composed of centriolar proteins. (3) After fertilization, the sperm PCL function like a centriole. The PCL recruits pericentriolar material (PCM) forming a centrosome that acts as a microtubule-organizing center (MTOC). The PCL also serves as a platform to form a typical centriole in the zygote, as expected from a centriole. Also, the PCL is essential to form one of the two spindle poles of the dividing zygote \n\nThe PCL was discovered in flies. However, it is also found in beetles, suggesting it is a common form of atypical centriole in insects.\n",
    "id": "53316821",
    "title": "Proximal Centriole-Like"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=53354629",
    "text": "CRISPR-Display\n\nCRISPR-Display (CRISP-Disp) is a modification of the CRISPR/Cas9 (Clustered regularly interspaced short palindromic repeats) system for genome editing. The CRISPR/Cas9 system uses a short guide RNA (sgRNA) sequence to direct a \"Streptococcus pyogenes\" Cas9 nuclease, acting as a programmable DNA binding protein, to cleave DNA at a site of interest.\n\nCRISPR-Display, in contrast, uses a nuclease deficient Cas9 (dCas9) and an engineered sgRNA with aptameric accessory RNA domains, ranging from 100bp to 5kb, outside of the normal complimentary targeting sequence. The accessory RNA domains can be functional domains, such as long non-coding RNAs (lncRNAs), protein-binding motifs, or epitope tags for immunochemistry. This allows for investigation of the functionality of certain lncRNAs, and targeting of ribonucleoprotein (RNP) complexes to genomic loci.\n\nCRISPR-Display was first published in Nature Methods in July 2015, and developed by David M. Shechner, Ezgi Hacisuleyman, Scott T. Younger and John Rinn at Harvard University and Massachusetts Institute of Technology (MIT), USA.\n\nThe CRISPR/Cas9 system is based on an adaptive immune system of prokaryotic organisms, and its use for genome editing was first proposed and developed in collaboration between Jennifer Doudna (University of California, Berkeley) and Emmanuelle Charpentier (Max Planck Institute for Infection Biology, Germany). The method, and its application in editing human cells, was published in Science on August 17, 2012. In January 2013, the Feng Zhang lab at the Broad Institute at MIT published another method in Science, having further optimized the sgRNA structure and expression for use in mammalian cells. By the beginning of 2014, almost 2500 studies mentioning CRISPR in their title has been published.\n\nNon-coding RNAs (ncRNAs) are RNA transcripts that are not translated into a protein product, but instead exert their function as RNA molecules. They are involved in a range of processes, like post-transcriptional regulation of gene expression, genomic imprinting, and regulating the chromatin state, and thereby the expression, of a given locus. Many ncRNAs have been discovered, but in many cases, their function has yet to be accurately dissected due to technical challenges. ncRNA function is often not affected by introducing point mutations and premature stop codons. ncRNAs are also thought to regulate gene expression, so deletion studies have a hard time distinguishing effects of ncRNA loss from effects of gene misregulation due to the deletion. Studies of ncRNAs have also lacked the throughput necessary for discerning the RNA based functionality. To meet these challenges, the Rinn lab therefore developed a synthetic biology approach, using CRISPR/Cas9 system, with the Cas9 acting as a conduit, to target ncRNA modules to ectopic genomic locations, and investigating the ncRNAs effects on reporter genes and other genomic features at that site.\n\nCRISPR-Disp modifies the CRISPR/Cas9 technology by using a catalytically inactive, i.e. nuclease deficient, Cas9 mutant (dCas9), and altering the RNA used for targeting Cas9 to a genomic location.\n\nSince sgRNAs are usually expressed by RNA polymerase III, which limits the length of the RNA domain that can be inserted, CRISPR-Display incorporates RNA polymerase II to permit expression of longer transcripts (~80-250 nucleotides) to overcome this limitation. CRISPR-Display can therefore add larger RNA domains, like natural and lncRNA domains, without affecting dCas9 localization.\n\nThe sgRNA is engineered with an aptameric accessory RNA domain in the sequence outside of the targeting sequence. In the development of the technique, five model cofactors with different topology constructs were used: TOP1-4 and INT with an accessory domain (P4-P6 domain) at different positions, including the 5’ and 3’ end and internally within the sgRNA. Each domain contained a stem-loop that can be recognized by a PP7 bacteriophage coat protein. The complex was delivered into mammalian cells (HEK293FT cells) by a lentiviral vector.\n\nTo ensure that the attached RNA module both retains targeting functionality as well as the resulting complex drive transcriptional activation at a specific site of interest, transient reporter gene expression of luciferase and fluorescent protein was measured. Two variations of such a transcription activator assay was performed; directly with a dCas9 fused to a transcriptional activator/repressor (VP64, a factor known to enhance gene expression) (Direct activation) or indirectly where the transcriptional activator is fused to an RNA binding protein module on the sgRNA (Bridged activation). Reporter gene activation through direct activation imply the sgRNA variant binds and targets dCas9 efficiently. All the five topologies showed direct activation except TOP3 and TOP4, which showed reduced activity. Bridged activation indicates that the fused RNA accessory domain is intact in mature dCas9 complexes. Bridged activation was observed with TOP1, TOP3 and INT. The results were recapitulated at endogenous loci by targeting minimal sgRNA and selective expanded topologies (TOP1 and INT) to human ASCL1, IL1RN, NTF3 and TTN promoters. Direct and bridged activation were observed by qRT-PCR for each construct proving that CRISP-Disp allows deployment of large RNA domains to genomic loci.\n\nThe effect of internal (stem-loop) insertion size on dCas9 complex was assessed using INT-like constructs with cassettes of PP7 stem loops with a size range from 25 nt to 247 nt. Each construct induced significant activation in the reporter assays signifying that internal insertion size does not influence the dCas9 complex function. Similarly, the effect of internal insert sequence was also determined through a set of unique sgRNA variants displaying cassettes of 25 random nucleotides. Reporter assays and RIP-Seq confirmed that sequence does not govern complex efficacy.\n\nThe utility of CRISP-Disp was explored with an array of functional RNA domains such as natural protein binding motifs, artificial aptamers and small molecules with varying size. While all the complexes were functional and viable, and successfully deployed the RNA domains at endogenous loci, the efficacy changed with length and expression levels. This suggests that optimization of structure and sequence might be important required before designing the construct.\n\nTo determine if artificial lncRNA scaffolds can be used with CRISPR-Display, dCas9 complexes were assembled with artificial RNA with a size comparable to lncRNAs. The constructs were expanded to ~650nt size with an additional P4-P6 domain with hairpin loops that can be recognized by another phage coat protein, MS2. These topology constructs were called double TOP0-2 with the two domains either together at 5’ or 3’end or separately at each end. Transient reporter assays followed by confirmation with RNA Immunoprecipitation sequencing (RIP-qPCR) showed that all the three constructs retained both the domains in the complex.\n\nThis was also tested with natural lncRNA domains by building Pol II-driven TOP1 and INT constructs fused with human lncRNA domains. lncRNAs used had lengths between ~90-4800 nt, and included the NoRC-binding pRNA, three enhancer-transcribed RNAs (eRNAs) FALEC, TRERNA1 and ncRNA-a3), \"Xist\" A-repeat (RepA), and the 4,799-nt transcriptional activator \"HOTTIP\". While all the constructs showed significant direct activation, it decreased with increasing lncRNA-sgRNA length. These lncRNA domains could regulate the reporters independent of dCas9 with pRNA and RepA repressing the GLuc reporter expression (repressors) and TRERNA1, ncRNA-a3 and HOTTIP inducing activation (activators), but were properly targeted to an ectopic location of interest by using the CRISP-Disp system.\n\nThus, CRISP-Disp enables control of gene expression with deployment of both artificial scaffolds as well as natural lncRNA domains.\n\nCRISPR-Display allows for previously unavailable studies of lncRNA functionality, artificial ncRNA functionalization, recruitment of endogenous and engineered proteins to genomic loci, and locus affinity tagging for cell imaging. \nCRISPR-Display allows targeted localization of natural lncRNAs to ectopic sites for investigation of their function. Exposing various ectopic DNA loci to natural lncRNAs can help show the effects of lncRNAs on gene expression and chromatin state, and help dissect the mechanism of such effects. One of the major outstanding questions in the study of lncRNAs is whether effects on chromatin state or gene expression adjacent to a lncRNA locus is due to functional, sequence-specific mechanisms of the lncRNA itself, or due simply to the act of transcribing the lncRNA. Localizing lncRNA to ectopic sites with CRISPR-Display can help separate the function of the RNA itself from the effects of transcribing such RNA species. Before CRISPR-Display, such studies were challenging due to low throughput, and inability to distinguish lncRNA function from other confounding factors like cryptically encoded peptides or functional DNA elements.\n\nCRISPR-Display also allows for targeted use of the wide array of artificial RNAs with specific functionality, such as RNAs for recruitment of endogenous RNA-binding proteins, antibody affinity tagging, and recruitment of tagged fusion proteins.\n\nOne example of artificial ncRNA functionalization is incorporating RNA domains recognized by specific antibodies to the sgRNA. CRISPR-Display can target the sgRNA with a particular epitope sequence to various loci, and fluorescently tagged antibodies can be used to image the locus, showing its localization in the nucleus, and possible interactions with other tagged proteins or genomic loci.\n\nEndogenous proteins known to bind a specific RNA motif can be recruited to ectopic genomic locations by incorporating the RNA motif into the sgRNA. CRISPR-Display can also recruit fusion proteins engineered to bind specific RNA sequences. Recruiting these proteins can allow studies of specific proteins’ and protein complexes’ effects on gene regulation and chromatin states, as well as specific regulation of certain genes for investigation of gene function.\n\nDue to the modularity of the sgRNA, several different sgRNAs with distinct functional modules can be expressed in each cell at once. The different RNA modules can then work simultaneously and independently, allowing for, for example, regulation of one genomic location whilst imaging the effects of the regulation at another location.\n\nThe possible applications of CRISPR-Display will continue to increase with further development and understanding of ncRNA functionalization. It is not unreasonable to think that CRISPR-Display may one day enable complex synthetic biology systems, with distinct temporal expression of sgRNAs, and networks and circuits of gene regulation by targeting of regulatory proteins.\n\n\n",
    "id": "53354629",
    "title": "CRISPR-Display"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=749788",
    "text": "Deformity\n\nA deformity, dysmorphism, or dysmorphic feature is a major abnormality in the shape of a body part or organ compared to the normal shape of that part.\nDeformity may arise from numerous causes:\n\nDeformity can occur in non-humans, as well. Frogs can be mutated due to Ribeiroia (Trematoda) infection.\n\nIn many cases in which a major deformity is present at birth, it is the result of an underlying condition severe enough that the baby does not survive very long. The mortality of severely deformed births may be due to a range of complications including missing or non-functioning vital organs, structural defects that prevent breathing or eating, and high susceptibility to injuries, abnormal facial appearance, or infections that lead to death.\n\nMythological creatures may have been created due to a deformative syndrome also, for instance, descriptions of mermaids may be related to the symptoms of sirenomelia. The Irish Mythology includes the Fomorians, who are almost without exception described as being deformed, possessing only one of what most have two of (eyes, arms, legs, etc.) or having larger than normal limbs.\n\n\n",
    "id": "749788",
    "title": "Deformity"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=53363521",
    "text": "Third-generation sequencing\n\nThird-generation sequencing (also known as long-read sequencing) is a class of DNA sequencing methods currently under active development. Third generation sequencing works by reading the nucleotide sequences at the single molecule level, in contrast to existing methods that require breaking long strands of DNA into small segments then inferring nucleotide sequences by amplification and synthesis. Critical challenges exist in the engineering of the necessary molecular instruments for whole genome sequencing to make the technology commercially available.\n\nSecond-generation sequencing, often referred to as Next-generation sequencing (NGS), has dominated the DNA sequencing space since its development. It has dramatically reduced the cost of DNA sequencing by enabling a massively-paralleled approach capable of producing large numbers of reads at exceptionally high coverages throughout the genome.\n\nSince eukaryotic genomes contain many repetitive regions, a major limitation to this class of sequencing methods is the length of reads it produces. Briefly, second generation sequencing works by first amplifying the DNA molecule and then conducting sequencing by synthesis. The collective fluorescent signal resulting from synthesizing a large number of amplified identical DNA strands allows the inference of nucleotide identity. However, due to random errors, DNA synthesis between the amplified DNA strands would become progressively out-of-sync. Quickly, the signal quality deteriorates as the read-length grows. In order to preserve read quality, long DNA molecules must be broken up into small segments, resulting in a critical limitation of second generation sequencing technologies. Computational efforts aimed to overcome this challenge often rely on approximative heuristics that may not result in accurate assemblies.\n\nBy enabling direct sequencing of single DNA molecules, third generation sequencing technologies have the capability to produce substantially longer reads than second generation sequencing. Such advantage has critical implications for both genome science and the study of biology in general. However, due to various technical challenges, third generation sequencing has error rates at almost unrepairable levels, rendering the technologies impractical for certain applications such as de novo genome assembly. These technologies are undergoing active development and therefore it is expected that there will be further improvements to the high error rates. For applications that are more tolerant to error rates such as metagenomics or larger structural variant calling, third generation sequencing has been found to outperform existing methods.\n\nSequencing technologies with a different approach than second-generation platforms were first described as \"third-generation\" in 2008-2009.\n\nThere are two companies currently at the heart of third generation sequencing technology development: Pacific Biosciences and Oxford Nanopore Technology. These companies are taking fundamentally different approaches to sequencing single DNA molecules.\n\nPacBio developed the sequencing platform of single molecule real time sequencing (SMRT), based on the properties of zero-mode waveguides. Signals are in the form of light signals.\n\nOxford Nanopore’s technology involves passing a DNA molecule through a pore structure and then measuring changes in electrical field surrounding the pore.\n\nIn comparison to the current generation of sequencing technologies, third generation sequencing has the obvious advantage of producing much longer reads. It is expected that these longer read lengths will alleviate numerous computational challenges surrounding genome assembly, transcript reconstruction, and metagenomics among other important areas of modern biology and medicine.\n\nIt is well known that eukaryotic genomes including primates and humans are complex and have large numbers of long repeated regions. Short reads from second generation sequencing must resort to approximative strategies in order to infer sequences over long ranges for assembly and genetic variant calling. Pair end reads have been leveraged by second generation sequencing to combat these limitations. However, exact fragment lengths of pair ends are often unknown and must also be approximated as well. By making long reads lengths possible, third generation sequencing technologies have clear advantages.\n\nEpigenetic markers are stable and potentially heritable modifications to the DNA molecule that are not in its sequence. An example is DNA methylation at CpG sites, which has been found to influence gene expression. Histone modifications are another example. The current generation of sequencing technologies rely on laboratory techniques such as ChIP-sequencing for the detection of epigenetic markers. These techniques involve tagging the DNA strand, breaking and filtering fragments that contain markers, followed by sequencing. Third generation sequencing may enable direct detection of these markers due to their distinctive signal from the other four nucleotide bases.\n\nOther important advantages of third generation sequencing technologies include portability and sequencing speed. Since minimal sample preprocessing is required in comparison to second generation sequencing, smaller equipments could be designed. Oxford Nanopore Technology has recently commercialized the MinION sequencer. This sequencing machine is roughly the size of a regular USB flash drive and can be used readily by connecting to a laptop. In addition, since the sequencing process is not parallelized across regions of the genome, data could be collected and analyzed in real time. These advantages of third generation sequencing may be well-suited in hospital settings where quick and on-site data collection and analysis is demanded.\n\nThird generation sequencing, as it currently stands, faces important challenges mainly surrounding accurate identification of nucleotide bases; error rates are still much higher compared to second generation sequencing. This is generally due to instability of the molecular machinery involved. For example, in PacBio’s single molecular and real time sequencing technology, the DNA polymerase molecule becomes increasingly damaged as the sequencing process occurs. Additionally, since the process happens quickly, the signals given off by individual bases may be blurred by signals from neighbouring bases. This poses a new computational challenge for deciphering the signals and consequently inferring the sequence. Methods such as Hidden Markov Models, for example, have been leveraged for this purpose with some success.\n\nOn average, different individuals of the human population share about 99.9% of their genes. In other words, approximately only one out of every thousand bases would differ between any two person. The high error rates involved with third generation sequencing are inevitably problematic for the purpose of characterizing individual differences that exist between members of the same species.\n\nGenome assembly is the reconstruction of whole genome DNA sequences. This is generally done with two fundamentally different approaches.\n\nWhen a reference genome is available, as one is in the case of human, newly sequenced reads could simply be aligned to the reference genome in order to characterize its properties. Such reference based assembly is quick and easy but has the disadvantage of “hiding\" novel sequences and large copy number variants. In addition, reference genomes do not yet exist for most organisms.\n\n\"De novo\" assembly is the alternative genome assembly approach to reference alignment. It refers to the reconstruction of whole genome sequences entirely from raw sequence reads. This method would be chosen when there is no reference genome, when the species of the given organism is unknown as in metagenomics, or when there exist genetic variants of interest that may not be detected by reference genome alignment.\n\nGiven the short reads produced by the current generation of sequencing technologies, de novo assembly is a major computational problem. It is normally approached by an iterative process of finding and connecting sequence reads with sensible overlaps. Various computational and statistical techniques, such as de bruijn graphs and overlap layout consensus graphs, have been leveraged to solve this problem. Nonetheless, due to the highly repetitive nature of eukaryotic genomes, accurate and complete reconstruction of genome sequences in de novo assembly remains challenging. Pair end reads have been posed as a possible solution, though exact fragment lengths are often unknown and must be approximated.\nLong read lengths offered by third generation sequencing may alleviate many of the challenges currently faced by de novo genome assemblies. For example, if an entire repetitive region can be sequenced unambiguously in a single read, no computation inference would be required. Computational methods have been proposed to alleviate the issue of high error rates. For example, in one study, it was demonstrated that de novo assembly of a microbial genome using PacBio sequencing alone performed superior to that of second generation sequencing.\n\nThird generation sequencing may also be used in conjunction with second generation sequencing. This approach is often referred to as hybrid sequencing. For example, long reads from third generation sequencing may be used to resolve ambiguities that exist in genomes previously assembled using second generation sequencing. On the other hand, short second generation reads have been used to correct errors in that exist in the long third generation reads. In general, this hybrid approach has been shown to improve de novo genome assemblies significantly.\n\nDNA methylation (DNAm) – the covalent modification of DNA at CpG sites resulting in attached methyl groups – is the best understood component of epigenetic machinery. DNA modifications and resulting gene expression can vary across cell types, temporal development, with genetic ancestry, can change due to environmental stimuli and are heritable. After the discovery of DNAm, researchers have also found its correlation to diseases like cancer and autism. In this disease etiology context DNAm is an important avenue of further research.\n\nThe current most common methods for examining methylation state require an assay that fragments DNA before standard second generation sequencing on the Illumina platform. As a result of short read length, information regarding the longer patterns of methylation are lost. Third generation sequencing technologies offer the capability for single molecule real-time sequencing of longer reads, and detection of DNA modification without the aforementioned assay.\n\nOxford Nanopore Technologies’ MinION has been used to detect DNAm. As each DNA strand passes through a pore, it produces electrical signals which have been found to be sensitive to epigenetic changes in the nucleotides, and a hidden Markov model (HMM) was used to analyze MinION data to detect 5-methylcytosine (5mC) DNA modification. The model was trained using synthetically methylated \"E. coli\" DNA and the resulting signals measured by the nanopore technology. Then the trained model was used to detect 5mC in MinION genomic reads from a human cell line which already had a reference methylome. The classifier has 82% accuracy in randomly sampled singleton sites, which increases to 95% when more stringent thresholds are applied.\n\nOther methods address different types of DNA modifications using the MinION platform. Stoiber et al. examined 4-methylcytosine (4mC) and 6-methyladenine (6mA), along with 5mC, and also created a software to directly visualize the raw MinION data in human-friendly way. Here they found that in \"E. coli\", which has a known methylome, event windows of 5 base pairs long can be used to divide and statistically analyze the raw MinION electrical signals. A straightforward Mann-Whitney U test can detect modified portions of the \"E. coli\" sequence, as well as further split the modifications into 4mC, 6mA or 5mC regions.\n\nIt seems likely that in the future, MinION raw data will be used to detect many different epigenetic marks in DNA.\n\nPacBio sequencing has also been used to detect DNA methylation. In this platform the pulse width - the width of a fluorescent light pulse - corresponds to a specific base. In 2010 it was shown that the interpulse distance in control and methylated samples are different, and there is a \"signature\" pulse width for each methylation type. In 2012 using the PacBio platform the binding sites of DNA methyltransferases were characterized. The detection of N6-methylation in C Elegans was shown in 2015. DNA methylation on \"N\"-adenine using the PacBio platform in mouse embryonic stem cells was shown in 2016.\n\nOther forms of DNA modifications – from heavy metals, oxidation, or UV damage – are also possible avenues of research using Oxford Nanopore and PacBio third generation sequencing.\n\nProcessing of the raw data – such as normalization to the median signal – was needed on MinION raw data, reducing real-time capability of the technology. Consistency of the electrical signals is still an issue, making it difficult to accurately call a nucleotide. MinION has low throughput; since multiple overlapping reads are hard to obtain, this further leads to accuracy problems of downstream DNA modification detection. Both the hidden Markov model and statistical methods used with MinION raw data require repeated observations of DNA modifications for detection, meaning that individual modified nucleotides need to be consistently present in multiple copies of the genome, e.g. in multiple cells or plasmids in the sample.\n\nFor the PacBio platform, too, depending on what methylation you expect to find, coverage needs can vary. As of March 2017, other epigenetic factors like histone modifications have not been discoverable using third-generation technologies. Longer patterns of methylation are often lost because smaller contigs still need to be assembled.\n\nTranscriptomics is the study of the transcriptome, usually by characterizing the relative abundances of messenger RNA molecules the tissue under study. According to the central dogma of molecular biology, genetic information flows from double stranded DNA molecules to single stranded mRNA molecules where they can be readily translated into function protein molecules. By studying the transcriptome, one can gain valuable insight into the regulation of gene expressions.\n\nWhile expression levels as the gene level can be more or less accurately depicted by second generation sequencing, transcript level information is still an important challenge. As a consequence, the role of alternative splicing in molecular biology remains largely elusive. Third generation sequencing technologies hold promising prospects in resolving this issue by enabling sequencing of mRNA molecules at their full lengths.\n\nAlternative splicing (AS) is the process by which a single gene may give rise to multiple distinct mRNA transcripts and consequently different protein translations. Some evidence suggests that AS is a ubiquitous phenomenon and may play a key role in determining the phenotypes of organisms, especially in complex eukaryotes; all eukaryotes contain genes consisting of introns that may undergo AS. In particular, it has been estimated that AS occurs in 95% of all human multi-exon genes. AS has undeniable potential to influence myriad biological processes. Advancing knowledge in this area has critical implications for the study of biology in general.\n\nThe current generation of sequencing technologies produce only short reads, putting tremendous limitation on the ability to detect distinct transcripts; short reads must be reverse engineered into original transcripts that could have given rise to the resulting read observations. This task is further complicated by the highly variable expression levels across transcripts, and consequently variable read coverages across the sequence of the gene. In addition, exons may be shared among individual transcripts, rendering unambiguous inferences essentially impossible. Existing computational methods make inferences based on the accumulation of short reads at various sequence locations often by making simplifying assumptions. Cufflinks takes a parsimonious approach, seeking to explain all the reads with the fewest possible number of transcripts. On the other hand, StringTie attempts to simultaneously estimate transcript abundances while assembling the reads. These methods, while reasonable, may not always identify real transcripts.\n\nA study published in 2008 surveyed 25 different existing transcript reconstruction protocols. Its evidence suggested that existing methods are generally weak in assembling transcripts, though the ability to detect individual exons are relatively intact. According to the estimates, average sensitivity to detect exons across the 25 protocols is 80% for Caenorhabditis elegans genes. In comparison, transcript identification sensitivity decreases to 65%. For human, the study reported an exon detection sensitivity averaging to 69% and transcript detection sensitivity had an average of mere 33%. In other words, for human, existing methods are able to identify less than half of all existing transcript.\n\nThird generation sequencing technologies have demonstrated promising prospects in solving the problem of transcript detection as well as mRNA abundance estimation at the level of transcripts. While error rates remain high, third generation sequencing technologies have the capability to produce much longer read lengths. Pacific Bioscience has introduced the iso-seq platform, proposing to sequence mRNA molecules at their full lengths. It is anticipated that Oxford Nanopore will put forth similar technologies. The trouble with higher error rates may be alleviated by supplementary high quality short reads. This approach has been previously tested and reported to reduce the error rate by more than 3 folds.\n\nMetagenomics is the analysis of genetic material recovered directly from environmental samples.\n\nThe main advantage for third-generation sequencing technologies in metagenomics is their speed of sequencing in comparison to second generation techniques. Speed of sequencing is important for example in the clinical setting (i.e. pathogen identification), to allow for efficient diagnosis and timely clinical actions.\n\nOxford Nanopore's MinION was used in 2015 for real-time metagenomic detection of pathogens in complex, high-background clinical samples. The first Ebola virus (EBV) read was sequenced 44 seconds after data acquisition. There was uniform mapping of reads to genome; at least one read mapped to >88% of the genome. The relatively long reads allowed for sequencing of a near-complete viral genome to high accuracy (97–99% identity) directly from a primary clinical sample.\n\nA common phylogenetic marker for microbial community diversity studies is the 16S ribosomal RNA gene. Both MinION and PacBio's SMRT platform have been used to sequence this gene. In this context the PacBio error rate was comparable to that of shorter reads from 454 and Illumina's MiSeq sequencing platforms.\n\nMinION's high error rate (~10-40%) prevented identification of antimicrobial resistance markers, for which single nucleotide resolution is necessary. For the same reason eukaryotic pathogens were not identified. Ease of carryover contamination when re-using same the flow cell (standard wash protocols don’t work) is also a concern. Unique barcodes may allow for more multiplexing. Furthermore, performing accurate species identification for bacteria, fungi and parasites is very difficult, as they share a larger portion of the genome, and some only differ by <5%.\n\nThe per base sequencing cost is still significantly more than that of MiSeq. However, the prospect of supplementing reference databases with full-length sequences from organisms below the limit of detection from the Sanger approach; this could possibly greatly help the identification of organisms in metagenomics.\n\nBefore third generation sequencing can be used reliably in the clinical context, there is a need for standardization of lab protocols. These protocols are not yet as optimized as PCR methods.\n",
    "id": "53363521",
    "title": "Third-generation sequencing"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=52974301",
    "text": "Trans-Golgi network vesicle protein 23 A\n\nTrans-Golgi network vesicle protein 23 A (TVP23A) is a protein coded for the TVP23A gene, formally known as FAM18A. TVP23A is located on chromosome 16. It is known to have human paralogs, TVP23B and TVP23C, as well as orthologs in many different species, notably yeast, mice, and chickens. The general consensus on the TVP23A protein indicate that it has some function in the late Golgi apparatus and is involved in retrograde transport from endosomes back into the Golgi apparatus. The nature of this transport is still unknown.\n\nTVP23A is located at cytogenic band 16p13.13, on the negative strand of Chromosome 16.\nTVP23A stands for Trans-Golgi network Vesicle Protein 23A TVP23A, is the current name for the protein. Aliases of TVP23A include FAM18A, and rarely YDR084C.\n\nThere are two known isoforms of TVP23A, variant one and variant two, with variant one being the more common variant in humans.\n\nTVP23A is a member of the pfam superfamily containing the domain of unknown function 846 (DUF846). TVP23A has a predicted molecular weight of 24.1 kilodaltons, an isoelectric point of 6.5, and relatively high amounts of tryptophan and phenylalanine. The secondary structure of TVP23A consists primarily of alpha helices composing 4 transmembrane domains. There is not much information on the tertiary structure of TVP23A or its homologs. iTASSER was used to generate a prediction for the folding pattern of TVP23A, which supports the presence of multiple helix structures.\nTVP23A is ubiquitously expressed in all human tissues. There is evidence of higher expression in the brain tissue of mice. The promoter for TVP23A is GXP_91266, spanning 1403 base pairs located on the negative strand of chromosome 16.\n\nThe hypothesized function of TVP23A is a transmembrane protein involved in retrograde transport of vesicles from early endosomes into the late Golgi apparatus. TVP23A interactions with SNARE TVI1 were found to be required for retrograde transport.\nTVP23A has been found to interact with four different proteins via Yeast two hybrid arrays. Two of these proteins, YIPF1 and YIPF2, are believed to be Golgi transport proteins.\n\nTVP23A is a DUF846 containing protein, which is homologous throughout TVP-type proteins. This domain contains the 4 transmembrane domains of TVP23A\n\nTVP23A has two paralogs, TVP23B and TVP23C. TVP23B and TVP23C are 96% similar to each other, and both are located on chromosome 17. Due to the locations of these three genes, and their identities to each other, it is probable that ancestral TVP23 underwent duplication and translocation, giving rise to TVP23A on chromosome 16 and TVP23B/C on chromosome 17, which then underwent a second duplication to form TVP23B and TVP23C.\nTVP23A has been found in all multicellular eukaryotes, including fungi. This gene has not been found in bacteria.\n",
    "id": "52974301",
    "title": "Trans-Golgi network vesicle protein 23 A"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=53970843",
    "text": "Machine learning in bioinformatics\n\nMachine learning, a subfield of computer science involving the development of algorithms that learn how to make predictions based on data, has a number of emerging applications in the field of bioinformatics. Bioinformatics deals with computational and mathematical approaches for understanding and processing biological data. \n\nPrior to the emergence of machine learning algorithms, bioinformatics algorithms had to be explicitly programmed by hand which, for problems such as protein structure prediction, proves extremely difficult. Machine learning techniques such as deep learning enable the algorithm to make use of automatic feature learning which means that based on the dataset alone, the algorithm can learn how to combine multiple features of the input data into a more abstract set of features from which to conduct further learning. This multi-layered approach to learning patterns in the input data allows such systems to make quite complex predictions when trained on large datasets. In recent years, the size and number of available biological datasets have skyrocketed, enabling bioinformatics researchers to make use of these machine learning systems. Machine learning has been applied to six main subfields of bioinformatics: genomics, proteomics, microarrays, systems biology, evolution, and text mining.\n\nGenomics involves the study of the genome, the complete DNA sequence, of organisms. While genomic sequence data has historically been sparse due to the technical difficulty in sequencing a piece of DNA, the number of available sequences is growing exponentially. However, while raw data is becoming increasingly available and accessible, the biological interpretation of this data is occurring at a much slower pace. Therefore, there is an increasing need for the development of machine learning systems that can automatically determine the location of protein-encoding genes within a given DNA sequence. This is a problem in computational biology known as gene prediction.\n\nGene prediction is commonly performed through a combination of what are known as extrinsic and intrinsic searches. For the extrinsic search, the input DNA sequence is run through a large database of sequences whose genes have been previously discovered and their locations annotated. A number of the sequence's genes can be identified by determining which strings of bases within the sequence are homologous to known gene sequences. However, given the limitation in size of the database of known and annotated gene sequences, not all the genes in a given input sequence can be identified through homology alone. Therefore, an intrinsic search is needed where a gene prediction program attempts to identify the remaining genes from the DNA sequence alone.\n\nMachine learning is also been used for the problem of multiple sequence alignment which involves aligning many DNA or amino acid sequences in order to determine regions of similarity that could indicate a shared evolutionary history.\nIt can also be used to detect and visualize genome rearrangements.\n\nProteins, strings of amino acids, gain much of their function from protein folding in which they conform into a three-dimensional structure. This structure is composed of a number of layers of folding, including the primary structure (i.e. the flat string of amino acids), the secondary structure (alpha helices and beta sheets), the tertiary structure, and the quartenary structure.\n\nProtein secondary structure prediction is a main focus of this subfield as the further protein foldings (tertiary and quartenary structures) are determined based on the secondary structure. Solving the true structure of a protein is an incredibly expensive and time-intensive process, furthering the need for systems that can accurately predict the structure of a protein by analyzing the amino acid sequence directly. Prior to machine learning, researchers needed to conduct this prediction manually. This trend began in 1951 when Pauling and Corey released their work on predicting the hydrogen bond configurations of a protein from a polypeptide chain. Today, through the use of automatic feature learning, the best machine learning techniques are able to achieve an accuracy of 82-84%. The current state-of-the-art in secondary structure prediction uses a system called DeepCNF (deep convolutional neural fields) which relies on the machine learning model of artificial neural networks to achieve an accuracy of approximately 84% when tasked to classify the amino acids of a protein sequence into one of three structural classes (helix, sheet, or coil). The theoretical limit for three-state protein secondary structure is 88–90%.\n\nMachine learning has also been applied to proteomics problems such as protein side-chain prediction, protein loop modeling, and protein contact map prediction.\n\nMicroarrays, a type of lab-on-a-chip, are used for automatically collecting data about large amounts of biological material. Machine learning can aid in the analysis of this data, and it has been applied to expression pattern identification, classification, and genetic network induction.\n\nThis technology is especially useful for monitoring the expression of genes within a genome, aiding in diagnosing different types of cancer based on which genes are expressed. One of the main problems in this field is identifying which genes are expressed based on the collected data. In addition, due to the huge number of genes on which data is collected by the microarray, there is a large amount of irrelevant data to the task of expressed gene identification, further complicating this problem. Machine learning presents a potential solution to this problem as various classification methods can be used to perform this identification. The most commonly used methods are radial basis function networks, deep learning, Bayesian classification, decision trees, and random forest.\n\nSystems biology focuses on the study of the emergent behaviors from complex interactions of simple biological components in a system. Such components can include molecules such as DNA, RNA, proteins, and metabolites.\n\nMachine learning has been used to aid in the modelling of these complex interactions in biological systems in domains such as genetic networks, signal transduction networks, and metabolic pathways. Probabilistic graphical models, a machine learning technique for determining the structure between different variables, are one of the most commonly used methods for modeling genetic networks. In addition, machine learning has been applied to systems biology problems such as identifying transcription factor binding sites using a technique known as Markov chain optimization. Genetic algorithms, machine learning techniques which are based on the natural process of evolution, have been used to model genetic networks and regulatory structures.\n\nOther systems biology applications of machine learning include the task of enzyme function prediction, high throughput microarray data analysis, analysis of genome-wide association studies to better understand markers of Multiple Sclerosis, protein function prediction, and identification of NCR-sensitivity of genes in yeast.\n\nThe increase in available biological publications led to the issue of the increase in difficulty in searching through and compiling all the relevant available information on a given topic across all sources. This task is known as knowledge extraction. This is necessary for biological data collection which can then in turn be fed into machine learning algorithms to generate new biological knowledge. Machine learning can be used for this knowledge extraction task using techniques such as natural language processing to extract the useful information from human-generated reports in a database. Text Nailing, an alternative approach to machine learning, capable of extracting features from clinical narrative notes was introduced in 2017.\n\nThis technique has been applied to the search for novel drug targets, as this task requires the examination of information stored in biological databases and journals. Annotations of proteins in protein databases often do not reflect the complete known set of knowledge of each protein, so additional information must be extracted from biomedical literature. Machine learning has been applied to automatic annotation of the function of genes and proteins, determination of the subcellular localization of a protein, analysis of DNA-expression arrays, large-scale protein interaction analysis, and molecule interaction analysis.\n\nOther application is the detection and visualization of regions that share high degree of similarity or are new according to a reference.\n",
    "id": "53970843",
    "title": "Machine learning in bioinformatics"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=53452258",
    "text": "Compartmentalized ciliogenesis\n\nCompartmentalized ciliogenesis is the most common type of ciliogenesis where the cilium axoneme is formed separated from the cytoplasm by the ciliary membrane and a ciliary gate known as the transition zone.\n",
    "id": "53452258",
    "title": "Compartmentalized ciliogenesis"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=2730533",
    "text": "Fenestra\n\nA fenestra (plural fenestrae) in anatomy, zoology and biology, is any small opening or pore. It is Latin for window and is used in various fields to describe a window or opening in a structure.\n\nIn gross anatomy in zoology fenestrae are often found in bones particularly in the skull. In anatomy the round window and oval window are also known as the \"fenestra rotunda\" and the \"fenestra ovalis\".\nIn microanatomy they are found in endothelium and fenestrated capillaries and allow for the rapid exchange of molecules between the capillaries, sinusoid blood vessels and surrounding tissue. These pores may enlarge and contract in response to various stimuli such as the hormone noradrenaline. The elastic layer of the tunica intima is a fenestrated membrane.\n\nIn surgery a fenestra is a new opening made in a part of the body to enable drainage or access.\n\nIn biology the perforations in a perforate leaf are also described as fenestrae and the leaf also called a fenestrate leaf. The leaf window is also known as a fenestra, and is a translucent structure that transmits light, as in \"Fenestraria\". \n\nIn zoology the trilobite \"Fenestraspis\" possessed extensive fenestrae in the posterior part of the body and apparently the thorax. In the paleognathae there is an ilio–ischiatic fenestra.\n\n",
    "id": "2730533",
    "title": "Fenestra"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=54069375",
    "text": "Diploidization\n\nDiploidization is the process of converting a polyploid genome back into a diploid one. Polyploidy is a product of whole genome duplication (WGD) and is followed by diploidization as a result of genome shock (1, 2, 4, 5). The plant kingdom has undergone multiple events of polyploidization followed by diploidization in both ancient and recent lineages (1). It has also been hypothesized that vertebrate genomes have gone through two rounds of paleopolyploidy (5). The mechanisms of diploidization are poorly understood but patterns of chromosomal loss and evolution of novel genes are observed in the process.\n\nElimination of Duplicated Genes\n\nUpon the formation of new polyploids, large sections of DNA are rapidly lost from one genome (1, 2, 4). The loss of DNA effectively achieves two purposes. First, the eliminated copy restores the normal gene dosage in the diploid organism (1). Second, the changes in chromosomal genetic structure increase the divergence of the homoeologous chromosomes (similar chromosomes from inter-species hybrid) and promotes homologous chromosome pairing (2). Both are important in terms of adjusting to the induced genome shock.\n\nEvolution of Genes to Ensure Correct Chromosome Pairing\n\nThere have been rare events in which genes that ensure proper chromosome pairing have evolved shortly after polyploidization. One such gene, Ph1, exists in hexaploid wheat (6). These genes keep the two sets of genomes separately by either spatially separating them or giving them a unique chromatin identity to facilitate recognition from its homologous pair. This prevents the need of rapid gene loss to speed up homeologous chromosome diversification.\n\n1. Coordinate inter-genomic gene expression:\n\nDuplicated genes often result in increased dosage of gene products. Doubled dosages are sometimes lethal to the organism thus the two genome copies must coordinate in a structured fashion to maintain normal nuclear activity (1). Many mechanisms of diploidization promote this coordination.\n\n2. Maintain intra-genomic chromosome pairing at meiosis:\n\nChromosome pairing during meiosis is a significant challenge for polyploids. Homoeologous chromosomes with similar genetic content may pair with each other resulting in trivalent or tetravalent interactions (4). The resolution of these structures results in chromosome breakage, rearrangement, and gamete infertility. Diploidization is often required to restore the cell’s ability to stably go through meiosis (2).\n\n3. Reduce costs of maintaining large, duplicated genomes:\n\nLarge genomes are costly to synthesize during replication and hard to maintain (2). The loss of duplicated genes during diploidization effectively reduces the overall size of the genome.\n\nOnce a polyploid is made, either synthetically or naturally, the genome goes through a period of “genome shock”. Genome shock can be defined as a stage in which the genome experiences massive reorganization and structural changes to deal with the external stress (X-ray damage, chromosome duplication, etc.) imposed upon the genome (3). Such changes are termed revolutionary changes and occur early in the process of diploidization (2). Revolutionary changes ensure that the organism has a stable genome that can be passed to its progeny.\n\nAt the end of this process, certain duplicated genes may be preserved thus allowing evolution to shape them into novel functions. This is commonly termed as neofunctionalization. The mechanism of retaining duplicated genes is poorly understood. It has been hypothesized that dosage balance may play a key role in shaping the evolutionary fates of duplicated genes (1). Evolutionary changes refer to the long process of converting duplicated genes into diverse, functional gene derivatives (2).\n\nThere are many ways in which a polyploid organism can convert back to a diploid status. This is usually achieved by elimination of duplicated genes. The main goals of diploidization are: 1. To ensure proper gene dosage 2. To maintain stable cellular division processes. It should be noted that this process does not need to occur rapidly for all chromosomes in one or few steps. In recent polyploid events, segments of the genome may still remain in a tetraploid status. In other words, diploidization is a long ongoing process that is shaped by both intrinsic and evolutionary drives (5).\n\nNormally, homologous chromosomes pair up in bivalents during meiosis and separate into different daughter cells. However, when multiple copies of similar chromosomes are present in the nucleus, homeologous chromosomes can also pair with homologous chromosomes resulting in the formation of trivalents or multivalents (4). The formation of multivalents results in unequal division of the chromosomes and lead to daughter cells lacking one or few chromosomes.\n\nWhen homeologous chromosomes pair up though bivalents or multivalents, illegitimate genetic crossovers may occur (4). Since the chromosomes may differ in genetic structure and content, segments of the chromosome may be shuffled around resulting in massive gene loss. Additionally, illegitimate recombinations may also result in dicentric chromosomes lead to chromosome breakage during anaphase (4). This further contributes to gene loss on duplicated chromosomes.\n\nThe duplicated copies of a gene are commonly non-essential to the plant’s ability to maintain normal growth and development. Therefore, one copy is generally free to mutate/ be lost from the genome (2, 5). This contributes to gene loss through the massive chromosome reorganization events during genome shock.\n\nAs mentioned earlier, duplicated genes are under relaxed selective pressure. Thus it may also be subject to neofunctionalization, the process in which a duplicated gene obtains a novel function.\n\nPolyploidy\n\nPaleopolyploidy\n\nNeofunctionalization\n\n1. Conant, G.C., J. A. Birchler, and J. C. Pires Dosage, duplication, and diploidization: clarifying the interplay of multiple models for duplicate gene evolution over time. \"Current Opinion in Plant Biology\" 2014, 19: 91–98\n\n2. Feldman, Moshe and Avraham A. Levy Genome evolution in allopolyploid wheat—a revolutionary reprogramming followed by gradual changes. \"J. Genet. Genomics\" 2009, 36: 511–518\n\n3. McClintock, Barbara The significance of responses of the genome to challenge \"Science\" 1984, 226: 792–801\n\n4. Hufton, A. L. and G. Panopoulou Polyploidy and genome restructuring: a variety of outcomes \"Current Opinion in Genetics & Development\" 2009, 19: 600–606\n\n5. Wolfe, Kenneth H. Yesterday’s Polyploids and the Mystery of Diploidization \"Nat Rev Genet.\" 2001 May;2(5): 333-41.\n\n6. Martinez-Perez, E, P. Shaw and G. Moore The Ph1 locus is needed to ensure specific somatic and meiotic centromere association \"Nature\" 411: 204-207\n",
    "id": "54069375",
    "title": "Diploidization"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=54424157",
    "text": "Biotron\n\nThe Biotron is a research facility located at the University of Wisconsin-Madison that \"provides controlled environments and climate-controlled greenhouses to support plant, animal, and materials research for university, non-profit, and commercial clients.\"\n\nAn evolution of the phytotron, the development of the facility had its roots in the late 1950s as a campaign established by the Botanical Society of America in search of a national phytotron. With additional funding and support by the National Science Foundation the Biotron was eventually envisioned as a combination facility that would allow both plant and animal tests to be conducted.\n\nPlant physiologist Folke Skoog would be instrumental in bringing the Biotron to the University of Wisconsin-Madison. A colleague of Frits Went at Caltech, Skoog oversaw the two proposals the university submitted to the Botanical Society in 1959. The interdisciplinary nature and scope of the project quickly led it to becoming Madison's most expensive facility at around 4.2 million dollars. It was up to Harold Senn, appointed director of the Biotron to concentrate on assembling funding, which he was able to accomplish by January 1963, with the Ford Foundation and National Institute of Health contributing to the project.\n\nThe Biotron was officially dedicated on September 18, 1970 with many experiments under precise controlled environmental conditions already under study, such as a lizard in the Palm Springs desert or black-eyed peas growing in hot and humid Nigeria. When finally completed in 1971, the Biotron contained over fifty rooms with many able to variate temperature from as low as -25C to as high as 50C with humidity adjustable anywhere from 1%-100%. Data from the various tests and sensors would then be fed and logged into a PDP-8/E-AA computer. Senn supposedly spent an additional $3,000 of the Biotron budget on increasing the computer's memory an extra 4 kilobytes.\nIn 1977, the International Crane Foundation brought endangered Siberian crane eggs from the U.S.S.R to the Biotron for incubation and initial feeding. A hyperbaric chamber was added and used for experiments on the effects of diving on pregnancy. A hypobaric chamber was used for high altitude tests of devices administering doses of vaccines and drugs.\n\nAlthough the Biotron went through a period of decline in the 1980s, unlike other facilities it never closed. In 1986, the first experiments using LEDs to grow plants were developed with NASA and tests were performed for the Galileo probe before its 1989 launch. Much of its work in the 1990s partnered with NASA in the Controlled Ecological Life Support System (CELSS) and its goal in researching the viability of certain vegetables for space travel, in particular the potato. Study of animal hibernation were done by ESA for human space exploration.\n\nThe Biotron has 45 rooms that are isolated from sound and vibration with separate air handling and provide control over the temperature, humidity, and lighting. Some rooms specialize in controlling wind, magnetic fields, electromagnetic radiation, or pressure.\n\n\n",
    "id": "54424157",
    "title": "Biotron"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=54568999",
    "text": "Biological Hermeneutics\n\nBiological Hermeneutics is the transdisciplinary study of written and printed media using artistic and scientific methods to trace the biological history of the text. For more on transdisciplinary study see transdisciplinarity.\n\nBiological Hermeneutics came into being after the development of the microscope during the 17th century. The most celebrated practitioner Robert Hooke devoted two of his 'Schema' of his ground breaking book \"Micrographia\" to the study of the microbiome of the book.\n\nSchema 12 was drawn from studying the red covers of a ‘small book’ which he judged to be made of ‘Sheeps skin’, he found:… a small white spot of hairy mould, multitudes of which I found to bespeck & whiten [the book]. These spots appear’d, through a good Microscope, to be a very pretty shap’d Vegetative body, which, from almost the same part of the Leather, shot out multitudes of small long cylindrical and transparent stalks …\n\nSchema 33 is dedicated to \"'the study of the small silver coloured\" book-worm'.\n\nThe development of the discipline stalled however with the ascendance of Sir Isaac Newton to the presidency of the Royal Society where he 'did much to obscure Hooke'.\n\nA collection of books maintaining the investigation of the transdiscipline can be found at Chetham's Library where the practice was developed from Hooke's initial investigations through the collecting policy of successive librarians who 'set out to acquire a major collection of books and manuscripts that would cover the whole range of available knowledge and would rival the college libraries of Oxford and Cambridge'\n\nIn order to collect biological material for later study books were sent out into the community as parish libraries. Gorton library is the last surviving example and has yet to be investigated using Biological Hermeneutic techniques.\n\nIn 1831 the foundation of the British Association for the Advancement of Science led to the popularisation of science and enabled a wider group to undertake their own investigations outside of the Royal Society creating a space for the further development of the practice.\n",
    "id": "54568999",
    "title": "Biological Hermeneutics"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=3630374",
    "text": "Neural computation\n\nNeural computation is information processing performed by networks of neurons. \n\nNeural computation can be studied for example by building models of neural computation. \n\nThere is a scientific journal dedicated to this subject, \"Neural Computation\". \n\nArtificial neural networks (ANN) is subfield of the research area machine learning. Work on ANNs has been somewhat inspired by knowledge of neural computation. \n",
    "id": "3630374",
    "title": "Neural computation"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=54976874",
    "text": "Infectious tolerance\n\nInfectious tolerance is a term referring to a phenomenon where a tolerance-inducing state is transferred from one cell population to another. It can be induced in many ways; although it is often artificially induced, it is a natural \"in vivo\" process. A number of research deal with the development of a strategy utilizing this phenomenon in transplantation immunology. The goal is to achieve long-term tolerance of the transplant through short-term therapy.\n\nThe term \"infectious tolerance\" was originally used by Gershon and Kondo in 1970 for suppression of naive lymphocyte populations by cells with regulatory function and for the ability to transfer a state of unresponsiveness from one animal to another. Gershon and Kondo discovered that T cells can not only amplify but also diminish immune responses. The T cell population causing this down-regulation was called suppressor T cells and was intensively studied for the following years (nowadays they are called regulatory T cells and are again a very attractive for research). These and other research in the 1970s showed greater complexity of immune regulation, unfortunately these experiments were largely disregarded, as methodological difficulties prevented clear evidence. Later developed new tolerogenic strategies have provided strong evidence to re-evaluate the phenomenon of T cell mediated suppression, in particular the use of non-depleting anti-CD4 monoclonal antibodies, demonstrating that neither thymus nor clonal deletion is necessary to induce tolerance. In 1989 was successfully induced classical transplantation tolerance to skin grafts in adult mice using antibodies blocking T cell coreceptors in CD4+ populations. Later was shown that the effect of monoclonal antibodies is formation of regulatory T lymphocytes. It has been shown that transfer of tolerance to other recipients can be made without further manipulation and that this tolerance transfer depends only on CD4+ T-lymphocytes. Because second-generation tolerance arises in the absence of any monoclonal antibodies to CD4 or CD8, it probably represents a natural response of the immune system, which, once initiated, becomes self-sustaining. This ensures the long duration of once induced tolerance, for as long as the donor antigens are present.\n\nDuring a tolerant state potential effector cells remain but are tightly regulated by induced antigen-specific CD4+ regulatory T cells (iTregs). Many subsets of iTregs play a part in this proces, but CD4CD25FoxP3 Tregs play a key role, because they have the ability to convert conventional T cells into iTregs directly by secretion of the suppressive cytokines TGF-β, IL-10 or IL-35, or indirectly via dendritic cells (DCs). Production of IL-10 induces the formation of another population of regulatory T cells called Tr1. Tr1 cells are dependent on IL-10 and TGF-β as well as Tregs, but differ from them by lacking expression of Foxp3. High IL-10 production is characteristic for Tr1 cells themselves and they also produce TGF-β. In the presence of IL-10 can be also induced tolerogenic DCs from monocytes, whose production of IL-10 is also important for Tr1 formation. These interactions lead to the production of enzymes such as IDO (indolamine 2,3-dioxygenase) that catabolize essential amino acids. This microenvironment with a lack of essential amino acids together with other signals results in mTOR (mammalian target of rapamycin) inhibition which, particularly in synergy with TGF-β, direct the induction of new FoxP3 (forkhead box protein 3) expressing Tregs.\n\n",
    "id": "54976874",
    "title": "Infectious tolerance"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=168986",
    "text": "Glycogen\n\nGlycogen is a multibranched polysaccharide of glucose that serves as a form of energy storage in humans, animals, fungi, and bacteria. The polysaccharide structure represents the main storage form of glucose in the body.\n\nGlycogen functions as one of two forms of long-term energy reserves, with the other form being triglyceride stores in adipose tissue (i.e., body fat). In humans, glycogen is made and stored primarily in the cells of the liver and skeletal muscle. In the liver, glycogen can make up from 5–6% of the organ's fresh weight and the liver of an adult weighing 70 kg can store roughly 100–120 grams of glycogen. In skeletal muscle, glycogen is found in a low concentration (1–2% of the muscle mass) and the skeletal muscle of an adult weighing 70 kg can store roughly 400 grams of glycogen. The amount of glycogen stored in the body—particularly within the muscles and liver—mostly depends on physical training, basal metabolic rate, and eating habits. Small amounts of glycogen are also found in other tissues and cells, including the kidneys, red blood cells, white blood cells, and glial cells in the brain. The uterus also stores glycogen during pregnancy to nourish the embryo.\n\nApproximately 4 grams of glucose are present in the blood of humans at all times; in fasted individuals, blood glucose is maintained constant at this level at the expense of glycogen stores in the liver and skeletal muscle. Glycogen stores in skeletal muscle serve as a form of energy storage for the muscle itself; however, the breakdown of muscle glycogen impedes muscle glucose uptake, thereby increasing the amount of blood glucose available for use in other tissues. Liver glycogen stores serve as a store of glucose for use throughout the body, particularly the central nervous system. The human brain consumes approximately 60% of blood glucose in fasted, sedentary individuals.\n\nGlycogen is the analogue of starch, a glucose polymer that functions as energy storage in plants. It has a structure similar to amylopectin (a component of starch), but is more extensively branched and compact than starch. Both are white powders in their dry state. Glycogen is found in the form of granules in the cytosol/cytoplasm in many cell types, and plays an important role in the glucose cycle. Glycogen forms an energy reserve that can be quickly mobilized to meet a sudden need for glucose, but one that is less compact than the energy reserves of triglycerides (lipids).\n\nGlycogen is a branched biopolymer consisting of linear chains of glucose residues with an average chain length of approximately 8–12 glucose units. Glucose units are linked together linearly by α(1→4) glycosidic bonds from one glucose to the next. Branches are linked to the chains from which they are branching off by α(1→6) glycosidic bonds between the first glucose of the new branch and a glucose on the stem chain.\n\nDue to the way glycogen is synthesised, every glycogen granule has at its core a glycogenin protein.\n\nGlycogen in muscle, liver, and fat cells is stored in a hydrated form, composed of three or four parts of water per part of glycogen associated with 0.45 millimoles (18 mg) of potassium per gram of glycogen.\n\nAs a meal containing carbohydrates or protein is eaten and digested, blood glucose levels rise, and the pancreas secretes insulin. Blood glucose from the portal vein enters liver cells (hepatocytes). Insulin acts on the hepatocytes to stimulate the action of several enzymes, including glycogen synthase. Glucose molecules are added to the chains of glycogen as long as both insulin and glucose remain plentiful. In this postprandial or \"fed\" state, the liver takes in more glucose from the blood than it releases.\n\nAfter a meal has been digested and glucose levels begin to fall, insulin secretion is reduced, and glycogen synthesis stops. When it is needed for energy, glycogen is broken down and converted again to glucose. Glycogen phosphorylase is the primary enzyme of glycogen breakdown. For the next 8–12 hours, glucose derived from liver glycogen is the primary source of blood glucose used by the rest of the body for fuel.\n\nGlucagon, another hormone produced by the pancreas, in many respects serves as a countersignal to insulin. In response to insulin levels being below normal (when blood levels of glucose begin to fall below the normal range), glucagon is secreted in increasing amounts and stimulates both glycogenolysis (the breakdown of glycogen) and gluconeogenesis (the production of glucose from other sources).\n\nMuscle cell glycogen appears to function as an immediate reserve source of available glucose for muscle cells. Other cells that contain small amounts use it locally, as well. As muscle cells lack glucose-6-phosphatase, which is required to pass glucose into the blood, the glycogen they store is available solely for internal use and is not shared with other cells. This is in contrast to liver cells, which, on demand, readily do break down their stored glycogen into glucose and send it through the blood stream as fuel for other organs.\n\nGlycogen was discovered by Claude Bernard. His experiments showed that the liver contained a substance that could give rise to reducing sugar by the action of a \"ferment\" in the liver. By 1857, he described the isolation of a substance he called \"\"la matière glycogène\"\", or \"sugar-forming substance\". Soon after the discovery of glycogen in the liver, A. Sanson found that muscular tissue also contains glycogen. The empirical formula for glycogen of () was established by Kekule in 1858.\n\nGlycogen synthesis is, unlike its breakdown, endergonic—it requires the input of energy. Energy for glycogen synthesis comes from uridine triphosphate (UTP), which reacts with glucose-1-phosphate, forming UDP-glucose, in a reaction catalysed by UTP—glucose-1-phosphate uridylyltransferase. Glycogen is synthesized from monomers of UDP-glucose initially by the protein glycogenin, which has two tyrosine anchors for the reducing end of glycogen, since glycogenin is a homodimer. After about eight glucose molecules have been added to a tyrosine residue, the enzyme glycogen synthase progressively lengthens the glycogen chain using UDP-glucose, adding α(1→4)-bonded glucose. The glycogen branching enzyme catalyzes the transfer of a terminal fragment of six or seven glucose residues from a nonreducing end to the C-6 hydroxyl group of a glucose residue deeper into the interior of the glycogen molecule. The branching enzyme can act upon only a branch having at least 11 residues, and the enzyme may transfer to the same glucose chain or adjacent glucose chains.\n\nGlycogen is cleaved from the nonreducing ends of the chain by the enzyme glycogen phosphorylase to produce monomers of glucose-1-phosphate:\n\nIn vivo, phosphorolysis proceeds in the direction of glycogen breakdown because the ratio of phosphate and glucose-1-phosphate is usually greater than 100. Glucose-1-phosphate is then converted to glucose 6-phosphate (G6P) by phosphoglucomutase. A special debranching enzyme is needed to remove the α(1-6) branches in branched glycogen and reshape the chain into a linear polymer. The G6P monomers produced have three possible fates:\n\nThe most common disease in which glycogen metabolism becomes abnormal is diabetes, in which, because of abnormal amounts of insulin, liver glycogen can be abnormally accumulated or depleted. Restoration of normal glucose metabolism usually normalizes glycogen metabolism, as well.\n\nIn hypoglycemia caused by excessive insulin, liver glycogen levels are high, but the high insulin levels prevent the glycogenolysis necessary to maintain normal blood sugar levels. Glucagon is a common treatment for this type of hypoglycemia.\n\nVarious inborn errors of metabolism are caused by deficiencies of enzymes necessary for glycogen synthesis or breakdown. These are collectively referred to as glycogen storage diseases.\n\nLong-distance athletes, such as marathon runners, cross-country skiers, and cyclists, often experience glycogen depletion, where almost all of the athlete's glycogen stores are depleted after long periods of exertion without sufficient carbohydrate consumption. This phenomenon is referred to as \"hitting the wall\".\n\nGlycogen depletion can be forestalled in three possible ways. First, during exercise, carbohydrates with the highest possible rate of conversion to blood glucose (high glycemic index) are ingested continuously. The best possible outcome of this strategy replaces about 35% of glucose consumed at heart rates above about 80% of maximum. Second, through endurance training adaptations and specialized regimens (e.g. fasting low-intensity endurance training), the body can condition type I muscle fibers to improve both fuel use efficiency and workload capacity to increase the percentage of fatty acids used as fuel, sparing carbohydrate use from all sources. Third, by consuming large quantities of carbohydrates after depleting glycogen stores as a result of exercise or diet, the body can increase storage capacity of intramuscular glycogen stores. This process is known as carbohydrate loading. In general, glycemic index of carbohydrate source does not matter since muscular insulin sensitivity is increased as a result of temporary glycogen depletion.\n\nWhen experiencing glycogen debt, athletes often experience extreme fatigue to the point that it is difficult to move. As a reference, the very best professional cyclists in the world will usually finish a 4- to 5-hr stage race right at the limit of glycogen depletion using the first three strategies.\n\nWhen athletes ingest both carbohydrate and caffeine following exhaustive exercise, their glycogen stores tend to be replenished more rapidly; however, the minimum dose of caffeine at which there is a clinically significant effect on glycogen repletion has not been established.\n\n\n",
    "id": "168986",
    "title": "Glycogen"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=55931886",
    "text": "Bioactive terrarium\n\nA bioactive terrarium (or vivarium) is a terrarium for housing one or more terrestrial animals which also includes populations of small invertebrates and microorganisms to consume and break down the waste products of the primary species. This population of decomposers is colloquially referred to as the \"cleanup crew\" (abbr. \"CUC\"), and can include terrestrial isopods, springtails, earthworms, millipedes, and various beetles as determined by compatibility with the habitat of the primary species (e.g. tropical rainforest vs. desert). In a fully functional bioactive terrarium, all of the waste products will be broken down by the cleanup crew, eliminating the need for cage cleaning and preventing odors. While most bioactive terrariums are also naturalistic (with live plants, soil, lighting, and naturalistic decor), a terrarium may be bioactive without being naturalistic, and naturalistic without being bioactive).\n\nAlthough any enclosure can be made bioactive by addition of a \"cleanup crew\" and appropriate substrate, such enclosures are almost always naturalistic terraria constructed of PVC, wood, glass and/or acrylic; bioactive enclosures in \"rack\" style caging is uncommon. \n\nBioactive enclosures require some form of particulate substrate to provide habitat for the cleanup crew. The choice of substrate is typically determined by the habitat of the primary species (e.g. jungle vs desert), and created by mixing a variety of components such as organic topsoil (free of pesticides and non-biological fertilizers), peat, coco fiber, sand, long-fiber sphagnum moss, cypress mulch, and orchid bark in varying proportions. In wet habitats, there is typically a layer of coarse rocks beneath the substrate to allow drainage, separated from the overlying substrate with a fine plastic mesh. Additionally, some bioactive terraria include leaflitter, which can serve as food and microhabitat for the cleanup crew.\n\nWaste products of the primary species are consumed by a variety of smaller invertebrates, termed a \"cleanup crew\". These can include terrestrial isopods, springtails, earthworms, millipedes, and various beetles, with different species being preferred in different habitats - the cleanup crew for a tropical rainforest bioactive terrarium may rely primarily on springtails, isopods, and earthworms, while a desert habitat would use mostly beetles. If the primary species is insectivorous, they may consume insects in the cleanup crew, and thus the cleanup crew must have sufficient retreats to avoid be completely depopulated.\n\nAdditionally, bioactive terraria typically have a flourishing population of bacteria and other microorganisms which break down the wastes of the cleanup crew and primary species. Fungi may occur, and, if non-toxic, may provide food for the cleanup crew and primary species.\n\n",
    "id": "55931886",
    "title": "Bioactive terrarium"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=56237498",
    "text": "Industries et Agro-Ressources\n\nIndustries et Agro-Ressources (also called Pôle IAR) is a French business cluster created in 2005 and located in Hauts-de-France and Grand Est. Its main focus is Bioeconomy.\n\nSince December 2017, its C.E.O is Yvon Le Hénaff.\n\n",
    "id": "56237498",
    "title": "Industries et Agro-Ressources"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=816785",
    "text": "List of popular science books on evolution\n\nThis is a list of popular science books concerning evolution, sorted by surname of the author.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "id": "816785",
    "title": "List of popular science books on evolution"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1653007",
    "text": "Cellularization\n\nThe theory of cellularization, also known as Ciliate-acoel theory, is one of the theories explaining the origin of the metazoans. It was first based on Ernst Haeckel's assumption that these beasts are formed from the jaws of fish and that the earliest animals derived from ciliate protozoans. Haeckel later abandoned this idea which is revived by Hadzi in 1953. According to the theory, a multinucleated unicellular ciliate ancestor would give rise to organisms similar to modern turbellarian flatworms by cellularization of the external layer. Recent molecular and morphologic data add increasing evidence against this view, and the alternative colonial theory, also proposed by Haeckel in the 1870s is gaining widespread acceptance.\n\n",
    "id": "1653007",
    "title": "Cellularization"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=7195477",
    "text": "Nylon-eating bacteria\n\nNylon-eating bacteria are a strain of \"Flavobacterium\" that are capable of digesting certain by-products of nylon 6 manufacture. This strain of \"Flavobacterium\" sp. K172, became popularly known as nylon-eating bacteria, and the enzymes used to digest the man-made molecules became popularly known as nylonase.\n\nIn 1975, a team of Japanese scientists discovered a strain of \"Flavobacterium\", living in ponds containing waste water from a nylon factory, that was capable of digesting certain byproducts of nylon 6 manufacture, such as the linear dimer of 6-aminohexanoate. These substances are not known to have existed before the invention of nylon in 1935.\n\nFurther study revealed that the three enzymes the bacteria were using to digest the byproducts were significantly different from any other enzymes produced by other \"Flavobacterium\" strains (or, for that matter, any other bacteria), and not effective on any material other than the manmade nylon byproducts.\n\nThis discovery led geneticist Susumu Ohno in a paper published in April 1984 to speculate that the gene for one of the enzymes, 6-aminohexanoic acid hydrolase, had come about from the combination of a gene duplication event with a frameshift mutation. Ohno suggested that many unique new genes have evolved this way.\n\nA 2007 paper that described a series of studies by a team led by Seiji Negoro of the University of Hyogo, Japan, suggested that in fact no frameshift mutation was involved in the evolution of the 6-aminohexanoic acid hydrolase. However, many other genes have been discovered which did evolve by gene duplication followed by a frameshift mutation affecting at least part of the gene.\n\nA 1995 paper showed that scientists have also been able to induce another species of bacterium, \"Pseudomonas aeruginosa\", to evolve the capability to break down the same nylon byproducts in a laboratory by forcing them to live in an environment with no other source of nutrients. The \"P. aeruginosa\" strain did not seem to use the same enzymes that had been utilized by the original \"Flavobacterium\" strain.\n\nAs described in a 1983 publication, other scientists were able to get the ability to generate the enzymes to transfer from the \"Flavobacterium\" strain to a strain of \"E. coli\" bacteria via a plasmid transfer.\n\nThere is scientific consensus that the capacity to synthesize nylonase most probably developed as a single-step mutation that survived because it improved the fitness of the bacteria possessing the mutation. More importantly: The enzyme involved has been produced by a mutation completely randomizing the original gene. Despite this, the new gene still had a novel, albeit weak, catalytic capacity. This is seen as a good example of how mutations easily can provide the raw material for evolution by natural selection.\n\n\n",
    "id": "7195477",
    "title": "Nylon-eating bacteria"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=19163660",
    "text": "Evolve (TV series)\n\nEvolve is a 2008 documentary television series on History. The series premiere, \"Eyes\", was nominated for an Emmy for Outstanding Science, Technology and Nature Programming.\n\nEach episode attempts to explain the evolutionary origins of a particular trait of living creatures: for example, Tyrannosaurus Rex's 13-inch teeth, the gecko's \"Velcro-like\" toe pads, and the bald eagle's \"telescopic\" vision capable of spotting a hare a mile away.\n\nTo date, there are 11 episodes, which are available to buy in a compilation box set. The box incorrectly lists 13 episodes but does list the correct 11 episode running time total. Topics are of the episode as named.\n\nThe dates of the episodes vary from the different sources available on the internet. The following dates have been compiled from different TV listing websites. Only the airdate of the initial episode is consistent among 4 sources:\n\n\n",
    "id": "19163660",
    "title": "Evolve (TV series)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=957529",
    "text": "The Ancestor's Tale\n\nThe Ancestor's Tale: A Pilgrimage to the Dawn of Life is a 2004 popular science book by Richard Dawkins, with contributions from Dawkins' research assistant Yan Wong. It follows the path of humans backwards through evolutionary history, meeting humanity's cousins as they converge on common ancestors. Dawkins' longest book to date, it was nominated for the 2005 Aventis Prize for Science Books.\n\nRichard Dawkins starts the tale by talking about history. He claims that evolution rhymes and patterns recur. He talks about our universe that has its own remarkable set of laws and constants which are capable of generating us and other organisms living on this planet. Not only it is capable of generating organisms, it is capable of evolving them too. \nHe claims that biological evolution has no privilege line of descent and no designated end. Evolution has arrived at many millions of interim ends and organisms are still evolving. He believes that evolution is directional, progressive and even predictable.\nHe also talks about how homo sapiens tend to think that they are more evolved than other, but that's not true, all the other species have gone through evolution too. They just have inherited different traits that helped them survive through natural selection. Dawkins claims that all species are equal.\nHe uses backward chronology, instead of forward chronology because in backward chronology, no matter where you start, you end up celebrating the unity of life. While going forward just extols diversity. In a backward chronology, the ancestors of any set of species must eventually meet at a particular geological moment. The last common ancestor is the one that they all share which he calls \"Concestor\". The oldest concestor is the grand ancestor of all surviving life forms on this planet.\nThere is a single concestor of all surviving life forms and its evidence is that all that have ever been examined share the same genetic code and the genetic code is too complex to have been invented twice. There is no sign of other independent origins of life and if new ones arise, they would probably be eaten by bacteria. \nThis book is a pilgrimage to discover human ancestors and as it progresses, it meets other pilgrims (organisms) who join humans in order as the book reaches the common ancestor that human share with them. Human only passes 40 rendezvous before hitting the origin of life itself. In each rendezvous, we find one particular ancestor, the concestor which has the same labeling number as the rendezvous. All the creatures in this tale are alive except for 2 classes. The two exceptions are Dodo and Handyman. \nDawkin’s book’s structure is inspired by Geoffrey Chaucer's The Canterbury Tales.\nAt each rendezvous point, Dawkins recounts interesting tales concerning the cousin animals which are about to join the band of pilgrims. Every newly recruited species, genus or family has its own peculiar features, often ones that are relevant to human anatomy or otherwise interesting for humans. For instance, Dawkins discusses why the axolotl never needs to grow up, how new species come about, how hard it is to classify animals, and why our fish-like ancestors moved to the land. These peculiar features are studied and analysed using a newly introduced tool or method from evolutionary biology, carefully woven into a tale to illustrate how the Darwinian theory of evolution explains all diversity in nature.\n\nEven though the book is best read sequentially, every chapter can also be read independently as a self-contained tale with an emphasis on a particular aspect of modern biology. As a whole, the book elaborates on all major topics in evolution.\n\nDawkins also tells personal stories about his childhood and time at university. He talks with fondness about a tiny bushbaby he kept as a child in Malawi (Nyasaland). He described his surprise when he learned that the closest living relatives to the hippos are the whales.\n\nThe book was produced in two hardback versions: a British one with extensive colour illustrations (by Weidenfeld & Nicolson), and an American one with a reduced number of black-and-white illustrations (by Houghton Mifflin). Paperback versions and an abridged audio version (narrated by Dawkins and his wife Lalla Ward) have also been published.\n\nThe book is dedicated to Dawkins' friend and mentor, population geneticist John Maynard Smith, who died shortly before the book went to press.\n\nDawkins uses the term \"concestor\"—coined by Nicky Warren—for the most recent common ancestor at each rendezvous point. At each rendezvous point, we meet the concestor of ourselves and the listed species or collection of species. This does not mean that the concestor was much like those creatures; after the \"rendezvous\", our fellow \"pilgrims\" have had as much time to evolve and change as we have. Only creatures alive at the time of the book's writing join us at each rendezvous point. Except for a few special cases, numerous extinct species and families such as the non-avian dinosaurs are excluded from the pilgrimage.\n\nNote: From the lancelets onward, Dawkins only provides dates under duress stating that, \"dating becomes so difficult and controversial that my courage fails me\".\n\nIn what Dawkins calls the \"Great Historic Rendezvous\", he describes the significantly important event of endosymbiosis, which results in the beginnings of eukaryotic cells. In his estimates, this occurred in two or three steps, roughly two billion years ago. Firstly, bacteria, perhaps related to \"Rickettsia\", entered proto-protozoan cells. For one reason or another, the bacteria were not digested and did not kill the cell. The cell offered protection to the bacteria, and the bacteria provided energy to the cell, resulting in a mutualistic symbiotic relationship. This is the speculated origin of mitochondria. Subsequently, photosynthetic bacteria (thought to be related to cyanobacteria) entered some, but not all, of these mitochondria-containing cells. These ancient bacteria evolved to become chloroplasts, and the cells became the Plant and Algal lineages. Meanwhile, the cells which this second endosymbiotic relationship did not occur in went on to form the Kingdoms Fungi and Animalia, as well as various Protozoa.\n\nChloroplasts and mitochondria have their own genomes, and they replicate independent of the cell in which they live. Dawkins acknowledges how the endosymbiotic theory proposed by Lynn Margulis is now virtually universally accepted.\n\n\n\n\n",
    "id": "957529",
    "title": "The Ancestor's Tale"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=7731295",
    "text": "Epic of evolution\n\nIn social, cultural and religious studies, the phrase \"epic of evolution\" has come to refer to a narrative that blends religious and scientific views of cosmic, biological and sociocultural evolution in a mythological manner. According to \"Taylor's Encyclopedia of Religion and Nature\", an \"epic of evolution\" encompasses\n\n\"Epic of evolution\" seems to have originated from the sociobiologist Edward O. Wilson's use of the phrase \"evolutionary epic\" in 1978. Wilson was not the first to use the term but his prominence prompted its usage as the morphed phrase 'epic of evolution'. In later years, he also used the latter term.\n\nNaturalistic and liberal religious writers have picked up on Wilson's term and have used it in a number of texts. These authors however have at times used other terms to refer to the idea: \"Universe Story\" (Brian Swimme, John F. Haught), \"Great Story\" (Connie Barlow, Michael Dowd), \"Everybody's Story\" (Loyal Rue), \"New Story\" (Thomas Berry, Al Gore, Brian Swimme) and \"Cosmic Evolution\" (Eric Chaisson).\n\nEvolution generally refers to biological evolution, but here it means a process in which the whole universe is a progression of interrelated phenomena, a gradual process in which something changes into a different and usually more complex form (emergence). It should not be 'biologized' as it includes many areas of science. In addition, outside of the scientific community, the term evolution is frequently used differently from scientists' usage. Even respected dictionaries may present questionable definitions that are not scientific or only address genetic changes. Unfortunately it is common for the general public to enter into a discussion about evolution with a wrong definition in mind. This often leads to misunderstanding since scientists are viewing evolution from a different perspective. The same applies to the use of the term theory as used in the theory of evolution (see references for Evolution as theory and fact).\n\nThis epic is not a long narrative poem but a series of events that form the proper subject for a laudable kind of tale. It is mythic in that it is a story of ostensibly historical events that serves to unfold part of the worldview of a people and explains a natural phenomenon. It is a form of myth that has an approach to investigation that is empirical or scientific. According to Joseph Campbell myths serve two purposes—provide meaning for a maturing individual (an individuate) and how to be part of a community. This Epic does both.\n\nDr. William Grassie of Temple University writes that the word \"myth\" in common usage is usually misunderstood. In academia it defines \"a story that serves to define the fundamental world view of a culture by explaining aspects of the natural world and delineating the psychological and social practices and ideals of a society\". He suggests that the Greek term \"mythos\" would be a better term to apply to the Epic as it is more all-encompassing. He concluded that there is not yet an interpretive tradition within science and society about this epic of evolution. If anything, there is an anti-interpretation tradition. Consequently, this is dangerous as it is a powerful revelation at this time. Grassie says the Epic is complex and multifaceted, not simple or easy to understand. It takes a romantic vision, philosophical rigor, and artistic interpretations. It requires a consilience of modern disciplines and acceptance of social diversity. The ancient wisdoms of the world's spiritual traditions must be adapted to make the framework to weave the Epic.\n\nE. O. Wilson explained that humans had a need for the epic of evolution because they must have a mythical story or a sublime account of how the world was created and how humanity became part of it. Religious epics fulfill a primal need in this respect as they verify that humans are part of something greater than themselves. The best empirical knowledge that science and history can provide is necessary in order to provide a comparable epic tale that will reliably unite a separated human spirituality. He believes the evolutionary epic can be as inherently noble as any religious epic when it is expressed in a poetic way. In a similar vein, biologist Ursula Goodenough sees the tale of natural emergence as far more magical than traditional religious miracles. It is a story that people can work with in a religious way if they elect to do so.\n\nPhilip Hefner uses the analogy of weaving to describe the Epic. The warp anchors the story and the weft creates the pattern and the tapestry. The Epic, as scientists see it, is the warp and the weft forms the pattern as each of us views it (we are all weavers) but the patterns all have the warp in common. Hefner writes that stories about the evolutionary epic are redolent with ultimacy. It is not science; it is scientifically informed myth, a myth driven by the refusal to give up on the insistence that the natural world and our lives in the world have meaning and purpose. It is a mythical tale of irony and hope that fills a large space in the domain of religion-and-science. Biologist Ursula Goodenough also makes use of Hefner's weaving metaphor.\n\nConnie Barlow and Michael Dowd's Great Story divides the epic into 8 phases eons or eras: the Great Radiance, the Galactic, Hadean, Archean, Proterozoic, Paleozoic, Mesozoic, and Cenozoic. Dowd uses the term 'epic of evolution' to help construct his viewpoint of evolution theology (a form of theistic evolution). His position is that science and religious faith are not mutually exclusive. He preaches that the epic of cosmic, biological, and human evolution, revealed by science, is a basis for an inspiring and meaningful view of our place in the universe. Evolution is viewed as a spiritual process that it is not meaningless blind chance.\n\nLoyal Rue states that there is nothing in the core of everybody's story to rule out belief in a personal deity. However belief in God is not an indispensable part of this narrative and here will be both theistic and non-theistic versions of it. He says it is the fundamental story of matter, created from energy, the organization of that matter into complex conditions, and then via self-organization into diverse life forms. Humans are as other living things—we are by nature star-born, earth-formed, fitness-maximized, biochemical systems. An aspect of the Epic is the evolution of behavior by that biochemical system.\n\nBrian Swimme sees the Epic as a way to gently maneuver a person into the magnificence of the Universe and as an antidote to the unhealthy consciousness of consumerism. It is the way into the future and enabled him to comprehend the cultural significance of this new story of science moving away from a materialistic worldview. It may move science away from its traditional abstractness to the uniqueness found in natural history. To him evolution and creativity are equivalent so it could be the Epic of Creativity (similar to Gordon Kaufman's thinking). Although the Epic is scientific, it is 'definitely mythic'—it has the fundamental nature of being mythic. \"You take hydrogen gas, and you leave it alone, and it turns into rosebuds, giraffes and humans.\"\n\nGordon D. Kaufman sees the Epic as a serendipitous creative process. He states that it is a notion that can interpret the enormous expansion and complexification of the physical universe (from the Big Bang outward), as well as the evolution of life here on earth and the gradual emergence of human historical existence. The whole vast process manifests (in varying degrees) serendipitous creativity, an everflowing coming into being of new modes of reality. In his book, \"In the beginning—creativity\", he says this creative process is God. Creativity, as metaphor, and as defined in the concept of evolution, has possibilities for constructing a new concept of God. The most foundational kind of creativity is found in that of cosmos/biological evolution—a paradigm that is now the organizing principle of all the sciences. It would seem as though he was equating God to the evolutionary story. This is similar to Dowd who sees the facts of Nature as God's native tongue.\n\nEric Chaisson orients his view of the epic around an \"arrow of time\", which he divides into 'Seven Ages of the Cosmos': particulate, galactic, stellar, planetary, chemical, biological, and cultural. However, such a thermodynamic arrow is not intended to be directional, a common misunderstanding; he sees no purpose, plan, or design in evolution, which he regards as unceasing, uncaring, and unpredictable. Chaisson’s strictly scientific analysis of evolution in its broadest sense—cosmic evolution)—implies an unidirectional, meandering process extending from the big bang to humankind on Earth, and continuing while likely producing increasing complexity, perhaps forever, to ends unknown. As a physicist, he is perhaps best known for attempting to quantify the epic of evolution, using the currently known laws of science and the scientific method (with its insistence on experimental tests of all ideas, principally those of energy rate density) in order to empirically discriminate between objective sense and subjective nonsense. What emerges is a grand and inspiring \"scientific\" narrative of who we are and whence we came—the most recent and up-to-date version of this work having been summarized in a long peer-review article.\n\nBron Taylor's \"Encyclopedia of Religion and Nature\" gives four primary categories: cosmic, planetary, life, cultural.\n\nThe creation-evolution controversy goes beyond the field of evolutionary biology and includes many fields of science (cosmology, geology, paleontology, thermodynamics, physics and sociobiology).\n\nAlthough in the scientific community there is essentially universal agreement that the evidence of evolution is overwhelming, and the consensus supporting the neo-Darwinian evolutionary synthesis is nearly absolute, creationists have asserted that there is a significant scientific controversy and disagreement over the validity of the evolution epic.\n\nThe debate is sometimes portrayed as being between science and religion. However, as the National Academy of Sciences states:\n\nDr.John Haught, Roman Catholic theologian, in his \"Science and Religion: from Conflict to Conversation\" suggests a theistic acceptance of the Epic. He says contemporary theology is being changed by evolutionary science. There are many versions undergoing constant revision. He considers evolution to be, at least provisionally, a most appropriate and fruitful scientific framework within which to think about God today and deplores that contemporary theology gets hung up in the creationism controversy. There are liberal congregations these days that may see the epic of evolution as a history about life and the universe that is both scientific and sacred. The profoundly sacred elements of the story warm up the cold technical facts with awe and reverence, giving Nature an inspiring beauty.\n\nEric Chaisson, in his book, \"Epic of Evolution\", concludes that the coherent story of cosmic evolution—a powerful and noble effort—may perhaps be the way to ethical evolution in the new millennium. Although his is a decidedly materialistic view of the evolutionary epic, he recognizes more than most scientists that humanity is part of this grand story and that we have a responsibility to survive as the only sentient, intelligent beings known in the universe.\n\nNot all of the Epic's advocates are distinguished scientists. Some are Christians who consider it a 'narrative of mythic proportions' that contain religious aspects. They see it as a multifaceted concept that that has been in Christian theology implicitly for hundreds of years and is congenial to perspectives that include ultimacy, transcendence, purpose and morality. However, there are both humanists and creationists who dispute this stance, making unclear the many varied theological stories of our world.\n\nIn 1996 the Institute on Religion in an Age of Science held a conference on the epic of evolution. In 1997 the American Association for the Advancement of Science organized a conference on the epic of evolution as part of their program on Dialogue on Science, Ethics, and Religion. In July 1999, The Forum on Religion and Ecology with special support from the Center for Respect of Life and Environment sponsored a conference titled The Epic of Evolution and World Religions. It consisted of a small invitational gathering of scholars of the world's religious traditions as well as a number of scientists and educators. It explored how the creation stories of the world's religions intersect with or react to the epic of evolution. An Evolutionary Epic conference was held in Hawaii in January 2008. It was attended by scientists, artists, educators, and spiritual and religious leaders. \n\nWashington University in St. Louis offers a course on the epic of evolution. The Epic has also been taught at Northern Arizona University. The course engaged the task of formulating a new epic myth that is based on the physical, natural, social, and cultural sciences for which there are as yet few textbooks. The course was presented in three segments: the cosmos before humans appeared, the human phenomenon, and scenarios for the future of evolution. An annual undergraduate course on \"cosmic evolution\" has been taught at Harvard University for most of the past 35 years.\n\nEvolution Sunday, a Christian church event (1,044 Congregations observed it in 2009) arose from the Clergy Letter Project signed in 2004 by 10,500 American clergy. It is spreading internationally and across other faiths. It supports the story of evolution in a manner similar to the Epic (science and religion compatibility) promoting serious discussion and reflection on the relationship between religion and science. \"For far too long, strident voices, in the name of Christianity, have been claiming that people must choose between religion and modern science,\" says Michael Zimmerman, founder of Evolution Sunday and dean of the College of Liberal Arts and Sciences at Butler University in Indianapolis. \"We're saying you can have your faith, and you can also have science.\"\n\nMichael Dowd and his wife Connie Barlow have traveled the US since 2002 by van as nomads teaching his \"Gospel of Evolution\".\n\n\nReading lists – Books of the Epic of Evolution,\nCosmic Evolution\n\n",
    "id": "7731295",
    "title": "Epic of evolution"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=6947256",
    "text": "Evolution (TV series)\n\nEvolution is a 2001 documentary series by the American broadcaster Public Broadcasting Service (PBS) and WGBH on evolutionary biology, from the producers of \"NOVA\".\nThe spokespeople for the series were Jane Goodall (overall spokesperson), Kenneth R. Miller and Stephen Jay Gould (science spokespeople), Eugenie C. Scott (education spokesperson), Arthur Peacocke and Arnold Thomas (religious spokespeople). The series was narrated by the Irish actor Liam Neeson.\n\nThe series was accompanied by a book by the popular science writer Carl Zimmer \"\". An extensive website provides teaching resources for each episode's material, including \"The Mating Game\", further looks at Charles Darwin, and an interactive history of speciation in the invented \"pollencreeper\" birds.\n\nThe episode \"What about God?\" features discussion of the issues of evolution and creationism at Wheaton College, an Evangelical Protestant college that teaches evolution but has in the past restricted professors from taking a stance on the literal versus the allegorical interpretations of Adam and Eve in the Genesis account of creation.\n\n\nTV critic Julie Salamon, writing in the \"New York Times\", said that \"[a] powerful sense of drama, discovery and intellectual enthusiasm runs through this rich eight-hour series ... The series covers an enormous amount of ground but doesn't leave you feeling swamped.\"\n\nBeing made and broadcast in the country where creation-evolution controversy is strongest, the last episode \"What About God?\" focused on religion, and \"through personal stories of students and teachers, it offers the view that they are compatible\". Phina Borgeson, Faith Network Director of the National Center for Science Education, provided a Congregational Study Guide for Evolution. Conversely, the Discovery Institute's Center for the Renewal of Science and Culture produced a website to refute the documentary and started a petition it called A Scientific Dissent From Darwinism to show that there were \"scientists that dispute the claims\".\n\n",
    "id": "6947256",
    "title": "Evolution (TV series)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=15551070",
    "text": "Evolutionary ideas of the Renaissance and Enlightenment\n\nEvolutionary ideas during the periods of the Renaissance and the Enlightenment developed over a time when natural history became more sophisticated during the 17th and 18th centuries, and as the scientific revolution and the rise of mechanical philosophy encouraged viewing the natural world as a machine with workings capable of analysis. But the evolutionary ideas of the early 18th century were of a religious and spiritural nature. In the second half of the 18th century more materialistic and explicit ideas about biological evolution began to emerge, adding further strands in the history of evolutionary thought.\n\nThe word \"evolution\" (from the Latin \"evolutio\", meaning \"to unroll like a scroll\") appeared in English in the 17th century, referring to an orderly sequence of events, particularly one in which the outcome was somehow contained within it from the start. Notably, in 1677 Sir Matthew Hale, attacking the atheistic atomism of Democritus and Epicurus, used the term \"evolution\" to describe his opponent's ideas that vibrations and collisions of atoms in the void — without divine intervention — had formed \"Primordial Seeds\" (semina) which were the \"immediate, primitive, productive Principles of Men, Animals, Birds and Fishes.\" For Hale, this mechanism was \"absurd\", because \"it must have potentially at least the whole Systeme of Humane Nature, or at least that Ideal Principle or Configuration thereof, in the evolution whereof the complement and formation of the Humane Nature must consist ... and all this drawn from a fortuitous coalition of senseless and dead Atoms.\"\n\nWhile Hale (ironically) first used the term evolution in arguing \"against\" the exact mechanistic view the word would come to symbolize, he also demonstrates that at least some evolutionist theories explored between 1650 and 1800 postulated that the universe, including life on earth, had developed mechanically, entirely without divine guidance. Around this time, the mechanical philosophy of Descartes, reinforced by the physics of Galileo and Newton, began to encourage the machine-like view of the universe which would come to characterise the scientific revolution. However, most contemporary theories of evolution, including those developed by the German idealist philosophers Schelling and Hegel (and mocked by Schopenhauer), held that evolution was a fundamentally \"spiritual\" process, with the entire course of natural and human evolution being \"a self-disclosing revelation of the Absolute\".\n\nTypical of these theorists, Gottfried Leibniz postulated in 1714 that \"monads\" inside objects caused motion by internal forces, and maintained that \"the 'germs' of all things have always existed ... [and] contain within themselves an internal principle of development which drives them on through a vast series of metamorphoses\" to become the geological formations, lifeforms, psychologies, and civilizations of the present. Leibniz clearly felt that evolution proceeded on divine principles — in his \"De rerum originatione radicali\" (1697), he wrote: \"A cumulative increase of the beauty and universal perfection of the works of God, a perpetual and unrestricted progress of the universe as a whole must be recognized, such that it advances to a higher state of cultivation.\" Others, such as J. G. von Herder, expressed similar ideas.\n\nBetween 1603 and 1613 Sir Walter Raleigh was a prisoner in the Tower of London awaiting execution; in this period he wrote a history of the world in five volumes where he described his American experiences and adventures, in it he wondered whether all the new species discovered in the new continent could have found their place on Noah's Ark. A very serious question at the time, he postulates that only animals from the old continent found place on the Ark; eventually, after the Flood, some of these animals would migrate to the new continent and, under environmental pressure, change their appearances to create new species. Fifty years later, Matthew Hale went even further, and said that only the prototypes of all animal species were welcomed on the Ark; these would eventually differentiate after their release. Many clergymen were happy with Raleigh and Hale ideas since they appeared to solve the problem of the Ark's tonnage.\n\nIn his \"Venus Physique\" in 1745, and \"System of Nature\" in 1751, Pierre Louis Maupertuis veered toward more materialist ground. He wrote of natural modifications occurring during reproduction and accumulating over the course of many generations, producing races and even new species. He also anticipated in general terms the idea of natural selection.\n\nVague and general ideas of evolution continued to proliferate among the mid-eighteenth century Enlightenment philosophers. G. L. L. Buffon suggested that what most people referred to as species were really just well-marked varieties. He thought that the members of what was then called a genus (which in terms of modern scientific classification would be considered a family) are all descended from a single, common ancestor. The ancestor of each family had arisen through spontaneous generation; environmental effects then caused them to diverge into different species. He speculated that the 200 or so species of mammals then known might have descended from as few as 38 original forms. Buffon's concept of evolution was strictly limited. He believed there were \"internal molds\" that shaped the spontaneous generation of each family and that the families themselves were entirely and eternally distinct. Thus, lions, tigers, leopards, pumas and house cats could all share a common ancestor, but dogs and house cats could not. Although Darwin's foreword to his 6th edition of \"Origin\" credited Aristotle with foreshadowing the concept of natural selection, he also wrote that \"the first author who in modern times has treated it in a scientific spirit was Buffon\".\n\nSome 18th-century writers speculated about human evolution. John Mitchell, a physician and cartographer, wrote a book in 1744 called \"An Essay upon the Causes of the Different Colours of People in Different Climates\" in which he claimed that the first race on earth had been a brown and reddish colour he said \"that an intermediate tawny colour found amongst Asiatics and Native Amerindians\" had been the “original complexion of mankind” and that others races came about by the original race spending generations in different climates. Between 1767 and 1792 James Burnett, Lord Monboddo included in his writings not only the concept that man had descended from other primates, but also that, in response to their environment, creatures had found methods of transforming their characteristics over long time intervals. He also produced research on the evolution of linguistics, which was cited by Erasmus Darwin in his poem \"Temple of Nature\". Jan-Andrew Henderson states that Monboddo was the first to articulate the theory of natural selection.\n\nIn 1792, the philosopher Immanuel Kant presented, in his \"Critique of Judgement\", what he referred to as “a daring venture of reason”, in which “one organic being [is] derived from another organic being, although from one which is specifically different; e.g., certain water-animals transform themselves gradually into marsh-animals and from these, after some generations, into land-animals.” Some 20th-century philosophers, such as Eric Voegelin, credit Kant with foreshadowing modern evolutionary theory.\nIn 1796, Erasmus Darwin published his \"Zoönomia\", which suggested \"that all warm-blooded animals have arisen from one living filament ... with the power of acquiring new parts\" in response to stimuli, with each round of improvements being inherited by successive generations. In his 1802 poem \"Temple of Nature\", he described the rise of life from minute organisms living in the mud to its modern diversity:\n\nFirst forms minute, unseen by spheric glass,<br>\nMove on the mud, or pierce the watery mass;<br>\nThese, as successive generations bloom,<br>\nNew powers acquire and larger limbs assume;<br>\nWhence countless groups of vegetation spring,<br>\nAnd breathing realms of fin and feet and wing.\n\n",
    "id": "15551070",
    "title": "Evolutionary ideas of the Renaissance and Enlightenment"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=11039478",
    "text": "Henry Farnham Perkins\n\nHenry Farnham Perkins (1877–1956) was an American zoologist and eugenicist.\n\nHe was born at 205 South Prospect Street in Burlington, Chittenden County, Vermont in the house where he spent his entire life in the affluent \"Burlington Hill\" neighborhood next to the University of Vermont on May 10, 1877. He was born into a family with Midwestern roots that trace back to \"Mayflower\" passengers, Love Brewster, a founder of the town of Bridgewater, Massachusetts; Elder William Brewster, the Pilgrim colonist leader and spiritual elder of the Plymouth Colony; and William Bradford, Governor of the Plymouth Colony and the second signer and primary architect of the Mayflower Compact in Provincetown Harbor. He was also a descendant of Martha Wadsworth Brewster, a notable 18th-century American poet and writer.\n\nHe was the only son and the second child of George Henry Perkins and the grandson of Frederick Perkins and Harriet Olmstead. Henry's father was a noted American educator, naturalist and Professor of Geology and kindred sciences at the University of Vermont. His father served as Dean of Arts and Sciences, vice president and was appointed interim president of the University of Vermont during World War I. He was also the state geologist of Vermont from 1898 to 1933. He graduated Phi Beta Kappa from Yale University, class of 1867; received the degree of Doctor of Philosophy from Yale in 1869.\n\nHis mother was Mary Judd Farnham, an 1863 graduate of Knox College, and a daughter of Eli Farnham and Jerusha Brewster Loomis. She attended the debates between Abraham Lincoln and Stephen A. Douglas at Knox College on October 7, 1858. It was said of her that she was a woman of superior mental endowments. She was the president of the Vermont Chapter of the Woman's Christian Temperance Union, and was active in her church and philanthropic work. Her first cousin was Dr. George Trumbull Ladd an American philosopher, educator and psychologist.\n\nMary's parents had emigrated to Illinois in 1836 and were the among the founders and pioneers of Galesburg, Illinois. They built a temporary cabin in Log City near current Lake Storey, just north of Galesburg, the settlers having decided that no log cabins were to be built inside the town limits. They were also instrumental in the founding of Knox College. Eli Farnham served as secretary of the Board of Trustees for nearly forty years and also the first school teacher in Galesburg.\n\nHe graduated Phi Beta Kappa from the University of Vermont, Burlington in 1898, received his M.Sc in 1899 and was awarded his PhD in Zoology at Johns Hopkins University in 1902. The title of his doctoral thesis was \"The Development of Gonionema Murbachii\", which was on the development and life cycle of Gonionema murbachii (a type of jellyfish). After receiving his doctorate, he joined the faculty of UVM where he remained until his retirement.\n\nHe married on June 11, 1903 at Baltimore, Baltimore County, Maryland, Mary Edmunds, born at Baltimore, Maryland on October 18, 1874 the daughter of James Richard Edmunds and Anna Smith Keyser. She was the sister of Dr. Charles Keyser Edmunds, one of his fellow graduate students at Johns Hopkins. He was president of Canton Christian College in Canton, Kwangtung Province, China and the fifth president of Pomona College in Claremont, California. Another brother was James Richard Edmunds, Jr., a graduate of the University of Pennsylvania, and a notable Architect in Baltimore, Maryland.\n\nShe was a descendant of John Howland, (c. 1599–1673) who was one of the Pilgrims who travelled from England to North America on the Mayflower, signed the Mayflower Compact, and helped found Plymouth Colony. She was also the great great granddaughter of Thomas Hurst Hughes, the founder and owner of the Congress Hall Hotel in Cape May, New Jersey, and a Republican member of the United States House of Representatives from New Jersey.\n\nHenry and Mary were the parents of two children, Anna Keyser Perkins-Middlebrook, who married as her second husband, Stanwood Wollaston and Harriet Perkins.\n\nIn 1903, he was appointed associate professor of Zoology at University of Vermont, Burlington. He taught biology, entomology, anatomy and physiology, and embryology during the first half of his career. In 1911 was promoted to full professor and served as chairman of the Zoology Department. His sporadic research projects involved field studies in the rapidly fading naturalist tradition: studies of birds, game fish, and marine invertebrates. He retired in 1945 and remained active in the UVM Alumni Association until his death in 1956.\n\nHis interest in eugenics began shortly after the end of World War I. It was after World War I that he learned of a study by the U.S. Army which was used as part of the draft process. The results from the Army study showed that men from Vermont had an inordinately high rate of \"defects\" (such as diabetes, epilepsy, \"deformities\" and \"mental deficiency\"). Perkins saw this as a problem that needed to be fixed. He went about trying to \"fix\" this through investigation and social reform. This reform, as denounced by historian Nancy Gallagher in her research titled \"Breeding better Vermonters\", also targeted French Canadians and American natives in Vermont state, considered \"insane invasion\" to eliminate.\n\nAround the same time, he revamped his Zoology curriculum and began teaching courses specifically on Heredity and Evolution. His heredity class provided the first known venue for eugenics education at UVM and the inspiration for a \"Eugenics Survey\"—a field station to study Vermonters.\n\nHe died on November 24, 1956 in Burlington, Vermont. He is buried in Greenmount Cemetery, Burlington, Vermont.\n\n\n\n",
    "id": "11039478",
    "title": "Henry Farnham Perkins"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=3645417",
    "text": "Society for the Study of Evolution\n\nThe Society for the Study of Evolution is a professional organization of evolutionary biologists. It was formed in the United States in 1946 to promote evolution and the integration of various fields of science concerned with evolution and to organize the publication of a scientific journal to report on new research on evolution across a variety of fields.\n\nThe Society was established at a meeting in St. Louis on March 30, 1946. Fifty-seven scientists attended the meeting, which was chaired by Alfred E. Emerson. George Gaylord Simpson was elected as the Society's first President, with E. B. Babcock, Emerson, and J. T. Patterson as his Vice-presidents and Ernst Mayr as secretary. This society grew as an extension of the US National Research Council's Committee on Common Problems of Genetics and Paleontology (later renamed the Committee on Common Problems of Genetics, Paleontology and Systematics).\n\nThe first annual meeting of the society was held in Boston, December 28–31, 1946. A grant from the American Philosophical Society led to the publication of the journal \"Evolution\".\n\nCommonly known as 'evolution meeting,' the annual conference is often held together with the Society of Systematic Biologists and the American Society of Naturalists.\n\nThe society has an official journal \"Evolution\". It was started in 1947 and is published by John Wiley & Sons.\n\n\n",
    "id": "3645417",
    "title": "Society for the Study of Evolution"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=11080148",
    "text": "Growing Up in the Universe\n\nGrowing Up in the Universe was a series of lectures given by Richard Dawkins as part of the Royal Institution Christmas Lectures, in which he discussed the evolution of life in the universe.\n\nThe lectures were first broadcast in 1991, in the form of five one-hour episodes, on the BBC in the United Kingdom. The Richard Dawkins Foundation for Reason and Science was granted the rights to the televised lectures, and a DVD version was released by the foundation on 20 April 2007.\n\nDawkins' book \"Climbing Mount Improbable\" developed from the ideas presented in the lectures, and the title itself is taken from the third lecture in the series.\n\nTo start off part one, Dawkins discusses the amazing capabilities of the human body and contrasts these with the limited capabilities of computers and other man-made machines. He uses a small totem pole (which is used in ancestor worship) to illustrate the importance of studying our ancestors to understand how we've evolved. To contrast ease of reproduction with the difficulty of becoming an ancestor, Dawkins uses the example of paper folding to explain exponential growth. Dawkins then tells the audience that exponential growth does not generally happen in the real world - natural factors come into play which control the population numbers, meaning that only an elite group of organisms will actually become distant ancestors. To be in this elite group, the organism must \"have what it takes\" to survive and pass on their genes to offspring.\n\nThe long chain of successful ancestors means that the probability of our existence is very small, and we are lucky to be alive. By turning down the lights and shining a small spotlight on a large ruler in front of him, Dawkins illustrates the darkness of the distant past and of the unknown future.\n\nAfter expounding on how lucky we are to be alive, and urging us not to waste the precious time that we have, Dawkins brings up the usefulness of science in aiding our understanding of the universe. He mentions the reply that Michael Faraday gave to Sir Robert Peel when asked about the use of science. Faraday's response was \"What is the use of a baby?\" Dawkins explains that Faraday was either referring to the vast potential of a baby, or to the idea that there must be something more to life than growing up, working, getting old, and dying. There must be a point to it all; Perhaps science can uncover the answers to our biggest questions.\n\nTo shake off the \"anesthetic of familiarity,\" Dawkins shows the audience a number of strange terrestrial organisms which he humorously nicknames \"By-Jovians,\" playing off a term we might use to refer to living organisms from another planet, for instance Jupiter. He uses a scanning electron microscope to look at small organisms including mites, mosquitoes, and a bee being parasitized by a strepsiptera. Using a model of a eukaryotic cell, he discusses the mitochondria and presents the audience with a complicated diagram of the metabolic pathways.\n\nDawkins suggests that we can also shake off the familiarity by stepping backwards in time. By using a single pace to represent going back 1000 years, he starts at year zero and takes four steps in front of his desk, going back to 4000 BCE. Pointing to a portrait of Homo habilis, he states that to go back to the time of habilis, he would have to walk about two kilometers. He has audience members hold up portraits of other human ancestors, telling them how far he would have to walk to get back to the time of each one.\n\nBy imagining what an advanced alien species would think of humans if they were to arrive on Earth, Dawkins suggests that their science would be similar to ours. They would know about pi, the Pythagorean theorem, and the theory of relativity. However, Dawkins explains that the alien anthropologists would most likely scoff at our local, parochial religious beliefs. He then contrasts evidence-based beliefs with revealed, tradition-based, and authority-based beliefs.\n\nTo explain the problem with beliefs in the supernatural, Dawkins conducts a small experiment with the audience to \"find the psychic.\" Using a coin, he assigns half the audience to will it to land on heads, and assigns the other half to will it to land on tails. After each flip, the section of the audience that was wrong is eliminated from the experiment, and he repeats the experiment using the remainder. After eight coin flips, only one boy in the audience remains. Dawkins then asks the question \"Is he psychic?\" Obviously, because of how the experiment was set up, one person was bound to have been correct about the result of each coin flip. Dawkins argues that this is exactly how seemingly supernatural events occur in the real world, especially when the \"audience\" is the entire population of the planet.\n\nTo conclude the lecture, Dawkins claims that there is nothing wrong with having faith in a proper scientific prediction. To illustrate this, he takes a cannonball which has been suspended from the ceiling with a rope, pulls it aside and touches it to his forehead. He announces that he is going to release the cannonball, letting it swing away from him, and that when it comes back to him, he is going to ignore his natural instinct to run because he has faith in his scientific prediction of what will happen - the cannonball should stop about an inch short of his forehead. He releases the cannonball, and his prediction is proved correct.\n\nDawkins' second lecture of the series examines the problem of design. He presents the audience with a number of simple objects, such as rocks and crystals, and notes that these objects have been formed by simple laws of physics and are therefore not designed. He then examines some designed objects - including a microscope, an electronic calculator, a pocket watch,\nand a clay pot - and notes that none of these objects could have possibly come about by sheer luck. Dawkins then discusses what he calls \"designoid objects\", which are complex objects that are neither simple, nor designed. Not only are they complex on the outside, they are also complex on the inside - perhaps billions of times more complex than a designed object such as a microscope.\n\nDawkins then shows the audience a number of designed and designoid objects, including the pitcher plant, megalithic mounds built by the compass termite, and pots made by trapdoor spiders, potter wasps, and mason bees. He examines some designoid objects that use camouflage, such as a grasshopper that looks like a stone, a sea horse that looks like sea weed, a leaf insect, a green snake, a stick insect, and a collection of butterflies that look like dead leaves when their wings are closed. Dawkins notes that many animals share similar types of camouflage or protection because of a process called convergent evolution. Examples of such designoid objects include the hedgehog and the spiny anteater (both of which evolved pointed spines along their back) and the marsupial wolf (which looks like a dog but is actually a marsupial). He illustrates the reason why convergent evolution occurs by using two small models of commercial aircraft. The reason they look similar isn't due to industrial espionage, it is due to the fact that they are both built to fly, so they must make use of similar design principles.\n\nUsing a camera and a model eye, Dawkins then compares the designed camera with the designoid eye. Both are involved in similar processes - using a lens to direct light onto a film or a retina. Both the camera and the eye also have an iris, which is used to control the amount of light which is allowed in. Using a volunteer from the audience, Dawkins demonstrates the contraction of the human\niris by shining a light into her right eye.\n\nThe lecture then moves into an explanation of natural selection, which brings forth designoid objects. To explain natural selection, Dawkins first explains artificial selection by discussing the evolution of wild cabbage into broccoli, cauliflower, cabbage, red cabbage, kohlrabi, and Brussels sprouts. He continues the discussion of artificial selection by explaining the evolution of the ancestral wolf into the many varieties of modern dog. Starting with the ancestral wolf, Dawkins imagines that everyone on one side of the room is breeding for small wolves, while everyone on the other side is breeding for big wolves. By selectively breeding the smallest or largest of each litter for a number of years, you may eventually end up with something like the Chihuahua on one side of the room, and something like a Great Dane on the other side of the room.\n\nDawkins then introduces an Arthromorphs computer program (similar to the Biomorphs program), explaining how it works while a volunteer uses the computer to selectively breed more and more generations.\n\nAt this point, Dawkins switches from explaining artificial selection to explaining natural selection. To demonstrate natural selection in a computer program, Dawkins uses a program written by Peter Fuchs to simulate the evolution of the spiderweb. The program builds \"genetic\" variations of a parent web, as if the web was actually being built by a child spider. For each generation, a simulation is run which randomly generates flies - some of which will hit the web, and others that will miss it. The child web that is able to capture the highest number of flies is selected as the parent for the next generation of webs. Dawkins shows the audience the \"fossil record\" that the program recorded after simulating a large number of generations overnight. The web starts off very simple and inefficient, but by the end it has evolved into a web that is highly efficient and highly complex. This is the same process that has led to the existence of all designoid objects.\n\nDawkins now discusses the most popular alternative to natural selection, which is known as creationism. He explains that creationists mistakenly believe designoid objects to be designed objects created by a divine being. Quoting from William Paley's \"Natural Theology\", Dawkins discusses the argument from design using the example of the watch and the watchmaker. Even though designoid objects appear to be designed, Darwin proved that this is not the case. Although Darwin's theory was discovered well after Paley developed his watchmaker argument, Dawkins explains that the argument of a divine watchmaker was still a bad argument, even in Paley's day. Paraphrasing David Hume, Dawkins explains that anything capable of creating humans must itself be highly complicated. Thus, the argument from design actually explains nothing - \"shooting itself in the foot.\" While it is true that designoid objects cannot come about by chance, evolution provides a non-random method of creation - namely, natural selection.\n\nAfter developing the argument against a divine creator, Dawkins examines a number of designoid objects that contain imperfections, which is something you would not expect to find in an object that is supposedly created by a divine being. Showing the audience a halibut flatfish, he explains how they evolved from an upright swimming ancestor with one eye on each side of the head into a bottom-hugging flatfish with a distorted set of eyes on one side of the body. Dawkins claims that this is poorly designed, as any proper engineer would design an organism more like a skate, which flattened out on its belly instead of on its side. This is an example of something you would expect from an evolved/designoid object, but not something you would expect from a created/designed object.\n\nUsing labeled building blocks, Dawkins shows the audience how designed objects came to be. He starts off by placing the simple block on the bottom, and explaining that you don't have to start with a complex being, but can start with a very simple foundation. If you have a simple foundation, you can place the next block on top - the designoid block. From this block, you can get complex organisms. Only after complex designoid objects come to be can you get the final building block of design (microscopes, clay pots, etc.).\n\nDawkins starts the lecture coming in with a stick insect on his hand. He describes with how many details such a being imitates its environment, its almost like a key that fits a lock. He then shows another insect, namely a Leaf Insect, which basically looks exactly like a dead leaf. He gives some more examples for this amazing imitation of the surrounding, e.g. a Potoo, which looks like a branch of tree and a thorn bug, which gains protection by looking like a rose thorn.\n\nHe, once again, makes the point that you can compare these beings with a key, which they represent themselves, whereas nature is the lock. Professor Richard Dawkins then explains that a key has to fit a lock exactly, and demonstrates this with a model of a lock. He mentions that a key is something very improbable. However it is hard to measure the probability of such a key, therefore Dawkins takes a bicycle lock for illustration, where you can calculate how likely it is to open the lock, because there is a fixed number of dials with a fixed number of positions. In Dawkin's case we have 3 dials, with 6 positions each, so the probability that you open the lock by sheer luck is one in 216.\n\nDawkins then shows the mechanism of the lock with a big model: Each dial has to be in the correct position in order to open up the lock. The model is then adapted to demonstrate a staged or gradualist solution to finding the right combination to open the lock. The probability of unlocking the combination in three separate phases falls to one in eighteen.\n\nIn this illustration, Dawkins identifies the role of sub-stages in Darwinian evolution. It is to increase the efficiency of mutation without affecting the probability of evolutionary success. The single stage requires 216, while the series of sub-stages requires only 18 non-random mutations at 100% probability of evolutionary success. That is an efficiency factor of 12 for mutations due to sub-staging without any change in probability. Similar efficiencies are achieved with random mutation without any change in probability.\n\nAfter addressing the claim by Fred Hoyle that probability alone could not produce the complexity of a typed text by Shakespeare, Dawkins introduces the notion of inherited improvements over a number of generations. Nature proceeds through small evolutionary steps,rather than large leaps. This idea is illustrated by a model of the ascent of Mount Improbable, which provides the title for this lecture.\n\nDawkins then illustrates the difference between the reproduction of inanimate phenomena, such as fires spread through sparks, with the inter-generational transmission of DNA in living structures. The gradual evolutionary adaption of these organisms is demonstrated through the examples of the eye, varieties of wings and protective camouflage.\n\nThe example of the gradual emergence of the eye is first shown: starting with a simple light sensitive flat surface and demonstrating the evolutionary benefits of a cone shaped proto-eye for detecting shadows and shapes. Dawkins then relates this model to the simple pinhole eye structure of a nautilus mollusc.\n\nThe benefit of wing structures is illustrated by way of body flattening behaviour in tree snakes, the web like skin of flying squirrels and similar adaptions to be found on flying lizards.\n\nDawkins begins by relating the story of asking a little girl \"what she thought flowers were 'for'.\" Her response is anthropocentric, that flowers are there for our benefit. Dawkins points out that many people throughout history have thought that the natural world existed for our benefit, with examples from Genesis and other literature. Author Douglas Adams, who is sitting in the audience, is called to read a relevant passage from his novel \"The Restaurant at the End of the Universe\".\n\nDawkins then asks his audience to put off the idea that the natural world exists for our benefit. He considers the question of flowers seen through the eyes of bees and other pollinators, and performs a series of demonstrations which use ultraviolet light to excite fluorescence in various substances.\n\nDawkins opens by talking how organisms “grow up” to understand the universe around them, which requires certain apparatus, such as a brain. But before brains can become large enough to model the universe they must develop from intermediate forms. Dawkins then discusses the digger wasp and the set of experiments conducted by Nikolaas Tinbergen of how the digger wasp models the local geography around its nest. He then talks about the limitations of the digger wasps’ brain and concludes that only the human brain is sufficiently developed to model large-scale phenomena about the world. He then shows a MRI scan of a human brain (later revealed to be his own brain) and describes how an image develops from the eye onto the visual cortex.\n\nDawkins discusses how the image on the retina is upside-down and in two dimensions but the overlapping images from each of the eyes are composited to form a three-dimensional model in the brain. He shows this by asking the audience to focus on him while holding their hand at eye level which causes them to see two images of their hand; one from each eye. He then describes how using his finger to wriggle his eyeball that the outside world appears to move because he is moving the image on his retina. However this does not happen when he voluntary rolls his eyes from side to side. This is due to the brain using the internal model to compensate for the relative change in position of images on the retina. Dawkins gets someone to wear a virtual reality headset and move around in a 3-D computer generated world and draws an analogy between the model of the universe developed in one’s head with the virtual reality universe developed in the computer.\n\nHe then goes on the show that the brain uses models to describe the universe by looking at how the brain interprets various optical illusions, such as the hollow-face illusion using a rotating hollow mask of Charlie Chaplin, the \"impossible\" geometry of a Penrose triangle, the shifting interpretations of the Necker cube and the ability of humans to find faces in random shapes.\n\nDawkins then begins to discuss the evolution of the human brain. He shows an animation of the increasing skull size from \"Australopithecus\" to \"Homo habilis\" to \"Homo erectus\" and then finally to modern day humans.\n\nThe ability of a brain to run complex simulations is a powerful evolutionary advantage. Dawkins talks about how this ability to model future events by showing a painting suggesting a hypothetical situation in which a female \"Homo erectus\" uses a mental model of a tree fallen across a gorge as a possible solution to crossing the gorge. The group then burns a tree so that it would create a bridge over the gap. He goes on to describe how the complex modelling ability of the brain may have developed due to this imaginative simulation of various possible scenarios or by the development of language, which would allow ideas to be passed from generation to generation, or by technology, which is an extension of human hands and eyes; or, indeed, if it is a combination of all three.\n\nDawkins concludes that purpose has arisen in the Universe due to human brains. The simulations developed in our brain allow us to develop intent and purpose; and over time our collective understanding of the Universe will improve as we continue to study and exchange ideas.\n\n",
    "id": "11080148",
    "title": "Growing Up in the Universe"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=3890352",
    "text": "A Darwinian Left\n\nA Darwinian Left: Politics, Evolution and Cooperation is a 1999 book by Peter Singer, in which the author argues that the view of human nature provided by evolution (e.g., evolutionary psychology) is compatible with and should be incorporated into the ideological framework of the Left.\n\nSinger believes that the Left will be better able to achieve its social and economic goals if it incorporates the more accurate view of human nature provided by evolutionary science: \"To be blind to the facts about human nature is to risk disaster\". For example, Singer argues that the Left's view of human nature as highly malleable, which he identifies with Marxism and the standard social science model, is incorrect.\n\nSinger argues that evolutionary psychology suggests that humans naturally tend to be self-interested. He further argues that the evidence that selfish tendencies are natural must not be taken as evidence that selfishness is right. He concludes that game theory (the mathematical study of strategy) and experiments in psychology offer hope that self-interested people will make short-term sacrifices for the good of others, if society provides the right conditions. Essentially Singer claims that although humans possess selfish, competitive tendencies naturally, they have a substantial capacity for cooperation that has also been selected for during human evolution.\n\nThe philosopher Philip Kitcher has criticised the book's handling of sociobiology, saying that it contains \"credulous retailing of sociobiological speculations\" while noting that \"much of this book is admirable in its clarity, directness, and grasp of central points\".\n\n\n",
    "id": "3890352",
    "title": "A Darwinian Left"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=28080823",
    "text": "RedToL\n\nRedToL, or Red Algal Tree of Life, is part of the collaborative National Science Foundation Assembling the Tree of Life activity (AToL), funded through the Division of Environmental Biology, Directorate for Biological Sciences. The overall goal of AToL is to resolve evolutionary relationships for large groups of organisms throughout the history of life, with the research often involving large teams working across institutions and disciplines. Investigators are typically supported for projects in data acquisition, analysis, algorithm development and dissemination in computational phylogenetics and phyloinformatics.\n\nThe Phylogenetic and Genomic Approaches to Reconstructing the Red Algal Tree of Life focus on the Rhodophyta (red algae), one of the most ancient eukaryotic phyla with fossil evidence for Bangiales stretching back 1.2 billion years. Red algae are not only key members of marine and freshwater aquatic environments but they are sources for important human foods such as dulse and sushi wrap, and have a multitude of pharmaceutical and industrial uses (e.g., agarose and carrageenans). Perhaps most important is the role red algae played in symbiogenesis. A red alga was the ancient (>1 billion years ago) donor of the plastid in chlorophyll c-containing algae (heterokonta or stramenophiles) that rose to prominence in marine ecosystems after the end of Permian with groups such as diatoms currently providing ca. 20% of global fixed carbon.\n\nThe aims of RedToL are: 1) reconstruct a robust phylogeny of 471 red algal species using a concatenated dataset of 2 nuclear, 4 plastid, and 2 mitochondrial encoded gene markers; 2) sequence plastid genomes and generate transcriptome databases for 16 key taxa that represent the phylogenetic (e.g., class- and order-level) breadth of the red algae, 3) make freely available red algal multi-gene and genome data via release to GenBank and a project-specific web site. The outreach and student mentoring program is an important component of the RedToL initiative.\n\nThe 471 chosen species from 294 genera represent the diversity of ca. 6,000 red algal species (i.e., ca. 35% of all red algal genera will be included, which represent all extant orders). The robust phylogenetic framework resulting from this study is the basis for a comprehensive taxonomic revision of the red algae and provides the basis for interpreting key innovations during red algal evolution. Because the chosen marker genes are shared across different Assembling the Tree of Life (AToL) projects, it will provide a common framework for a future comprehensive eukaryotic tree. Complete plastid genome and transcriptome data from 16 major red algal taxa will provide a genome inventory to facilitate understanding of red algal evolution, as well as provide the basis for phylogenetic analyses using a rich set (i.e., 100s) of vertically inherited genes. The genome data will specifically test the relationships among the major classes and orders of red algae. The RedToL team members come from eight institutions including two foreign collaborators and fifteen advisory board members and represent the most prominent active national and international red algal specialists from different generations who use different methods (from taxonomy to phylogenomics) to enhance understanding of red algal biology.\n\n",
    "id": "28080823",
    "title": "RedToL"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=522768",
    "text": "Aquatic adaptation\n\nSeveral animal groups have undergone aquatic adaptation, going from being purely terrestrial animals to living at least part of the time in water. The adaptations in early speciation tend to develop as the animal ventures into water in order to find available food. As successive generations spend more time in the water, natural selection causes the acquisition of more adaptations. Animals of later generations may spend the majority of their life in the water, coming ashore for mating. Finally, fully adapted animals may take to mating and birthing in water.\n\n\"Archelon\" is a type of giant sea turtle dating from the Cretaceous Period, now long extinct. Its smaller cousins survive as the sea turtles of today.\n\n\"Mesosaurus\" (and other mesosaurids) were another group of anapsid reptiles to secondarily return to the sea, eschewing shells, and are also long extinct.\n\nLiving at the same time as, but not closely related to, dinosaurs, the mosasaurs resembled crocodiles but were more strongly adapted to marine life. They became extinct 66 million years ago, at the same time as the dinosaurs. \n\nModern diapsids which have made their own adaptions to allow them to spend significant time in the water include marine iguanas and marine crocodiles. Sea snakes are extensively adapted to the marine environment, giving birth to live offspring in the same way as the Euryapsida (see below) and are largely incapable of terrestrial activity. The arc of their adaptation is evident by observing the primitive Laticauda genus, which must return to land to lay eggs.\n\nThese marine reptiles had ancestors who moved back into the oceans. In the case of ichthyosaurs adapting as fully as the dolphins they superficially resemble, even giving birth to live offspring instead of laying eggs. Euryapsida is now no longer considered a valid taxonomic group (Motani, 2009).\n\nDuring the Paleocene Epoch (about 66 - 55 million years ago), a group of wolf-like artiodactyls related to \"Pakicetus\" began pursuing an amphibious lifestyle in rivers or shallow seas. They were the ancestors of modern whales, dolphins, and porpoises. The cetacea are extensively adapted to marine life and cannot survive on land at all. Their adaptation can be seen in many unique physiognomic characteristics such as the dorsal blowhole, baleen teeth, and the cranial 'melon' organ used for aquatic echolocation. The closest extant terrestrial relative to the whale is the hippopotamus, which spends much of its time in the water and whose name literally means \"horse of the river\".\n\nThe ancestors of the dugong and manatees first appeared in the fossil record about 45 to 50 million years ago in the ocean.\n\nThe fossil records show that phocids existed 12 to 15 million years ago, and odobenids about 14 million years ago. Their common ancestor must have existed even earlier than that.\n\nAlthough polar bears spend most of their time on the ice rather than in the water, polar bears show the beginnings of aquatic adaptation to swimming (high levels of body fat and nostrils that are able to close), diving, and thermoregulation. Distinctly polar bear fossils can be dated to about 100,000 years ago. The polar bear has thick fur and layers of fat on its body to protect it from the cold.\n\nProponents of the aquatic ape hypothesis believe that part of human evolution includes some aquatic adaptation, which has been said to explain human hairlessness, bipedalism, increased subcutaneous fat, descended larynx, vernix caseosa, a hooded nose and various other physiological and anatomical changes. The idea is not accepted by most scholars who study human evolution.\n",
    "id": "522768",
    "title": "Aquatic adaptation"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=7815174",
    "text": "Entropy and life\n\nResearch concerning the relationship between the thermodynamic quantity entropy and the evolution of life began around the turn of the 20th century. In 1910, American historian Henry Adams printed and distributed to university libraries and history professors the small volume \"A Letter to American Teachers of History\" proposing a theory of history based on the second law of thermodynamics and on the principle of entropy. The 1944 book \"What is Life?\" by Nobel-laureate physicist Erwin Schrödinger stimulated research in the field. In his book, Schrödinger originally stated that life feeds on negative entropy, or negentropy as it is sometimes called, but in a later edition corrected himself in response to complaints and stated the true source is free energy. More recent work has restricted the discussion to Gibbs free energy because biological processes on Earth normally occur at a constant temperature and pressure, such as in the atmosphere or at the bottom of an ocean, but not across both over short periods of time for individual organisms.\n\nIn 1863, Rudolf Clausius published his noted memoir \"On the Concentration of Rays of Heat and Light, and on the Limits of its Action\" wherein he outlined a preliminary relationship, as based on his own work and that of William Thomson (Lord Kelvin), between his newly developed concept of entropy and life. Building on this, one of the first to speculate on a possible thermodynamic perspective of evolution was the Austrian physicist Ludwig Boltzmann. In 1875, building on the works of Clausius and Kelvin, Boltzmann reasoned:\n\nIn 1876, American civil engineer Richard Sears McCulloh, in his \"Treatise on the Mechanical Theory of Heat and its Application to the Steam-Engine\", which was an early thermodynamics textbook, states, after speaking about the laws of the physical world, that \"there are none that are established on a firmer basis than the two general propositions of Joule and Carnot; which constitute the fundamental laws of our subject.\" McCulloch then goes on to show that these two laws may be combined in a single expression as follows:\n\nwhere\n\nMcCulloch then declares that the applications of these two laws, i.e. what are currently known as the first law of thermodynamics and the second law of thermodynamics, are innumerable. He then states:\n\nMcCulloch then gives a few of what he calls the “more interesting examples” of the application of these laws in extent and utility. The first example he gives is physiology, wherein he states that “the body of an animal, not less than a steamer, or a locomotive, is truly a heat engine, and the consumption of food in the one is precisely analogous to the burning of fuel in the other; in both, the chemical process is the same: that called combustion.” He then incorporates a discussion of Lavoisier’s theory of respiration with cycles of digestion and excretion, perspiration, but then contradicts Lavoisier with recent findings, such as internal heat generated by friction, according to the new theory of heat, which, according to McCulloch, states that the “heat of the body generally and uniformly is diffused instead of being concentrated in the chest”. McCulloch then gives an example of the second law, where he states that friction, especially in the smaller blooded-vessels, must develop heat. Without doubt, animal heat is thus in part produced. He then asks: “but whence the expenditure of energy causing that friction, and which must be itself accounted for?\"\n\nTo answer this question he turns to the mechanical theory of heat and goes on to loosely outline how the heart is what he calls a “force-pump”, which receives blood and sends it to every part of the body, as discovered by William Harvey, that “acts like the piston of an engine and is dependent upon and consequently due to the cycle of nutrition and excretion which sustains physical or organic life.” It is likely, here, that McCulloch was modeling parts of this argument on that of the famous Carnot cycle. In conclusion, he summarizes his first and second law argument as such:\n\nLater, building on this premise, in the famous 1944 book \"What is Life?\", Nobel-laureate physicist Erwin Schrödinger theorizes that life, contrary to the general tendency dictated by the Second law of thermodynamics, decreases or maintains its entropy by feeding on negative entropy. In his note to Chapter 6 of \"What is Life?\", however, Schrödinger remarks on his usage of the term negative entropy:\n\nThis is what is argued to differentiate life from other forms of matter organization. In this direction, although life's dynamics may be argued to go against the tendency of second law, which states that the entropy of an isolated system tends to increase, it does not in any way conflict or invalidate this law, because the principle that entropy can only increase or remain constant applies only to a closed system which is adiabatically isolated, meaning no heat can enter or leave. Whenever a system can exchange either heat or matter with its environment, an entropy decrease of that system is entirely compatible with the second law. The problem of organization in living systems increasing despite the second law is known as the Schrödinger paradox.\n\nIn 1964, James Lovelock was among a group of scientists who were requested by NASA to make a theoretical life detection system to look for life on Mars during the upcoming space mission. When thinking about this problem, Lovelock wondered “how can we be sure that Martian life, if any, will reveal itself to tests based on Earth’s lifestyle?” To Lovelock, the basic question was “What is life, and how should it be recognized?” When speaking about this issue with some of his colleagues at the Jet Propulsion Laboratory, he was asked what he would do to look for life on Mars. To this, Lovelock replied \"I’d look for an entropy reduction, since this must be a general characteristic of life.\"\n\nIn recent years, the thermodynamic interpretation of evolution in relation to entropy has begun to utilize the concept of the Gibbs free energy, rather than entropy. This is because biological processes on earth take place at roughly constant temperature and pressure, a situation in which the Gibbs free energy is an especially useful way to express the second law of thermodynamics. The Gibbs free energy is given by:\n\nThe minimization of the Gibbs free energy is a form of the principle of minimum energy, which follows from the entropy maximization principle for closed systems. Moreover, the Gibbs free energy equation, in modified form, can be utilized for open systems when chemical potential terms are included in the energy balance equation. In a popular 1982 textbook, \"Principles of Biochemistry\", noted American biochemist Albert Lehninger argues that the order produced within cells as they grow and divide is more than compensated for by the disorder they create in their surroundings in the course of growth and division. In short, according to Lehninger, \"living organisms preserve their internal order by taking from their surroundings free energy, in the form of nutrients or sunlight, and returning to their surroundings an equal amount of energy as heat and entropy.\"\n\nSimilarly, according to the chemist John Avery, from his 2003 book \"Information Theory and Evolution\", we find a presentation in which the phenomenon of life, including its origin and evolution, as well as human cultural evolution, has its basis in the background of thermodynamics, statistical mechanics, and information theory. The (apparent) paradox between the second law of thermodynamics and the high degree of order and complexity produced by living systems, according to Avery, has its resolution \"in the information content of the Gibbs free energy that enters the biosphere from outside sources.\" The process of natural selection responsible for such local increase in order may be mathematically derived directly from the expression of the second law equation for connected non-equilibrium open systems.\n\nThe second law of thermodynamics applied on the origin of life is a far more complicated issue than the further development of life, since there is no \"standard model\" of how the first biological lifeforms emerged; only a number of competing hypotheses. The problem is discussed within the area of abiogenesis, implying gradual pre-Darwinian \"chemical evolution\". In 1924, Alexander Oparin suggested that sufficient energy was provided in a \"primordial soup\". The Belgian scientist Ilya Prigogine was awarded with a Nobel prize in 1977 for an analysis in this area. A related topic is the probability that life would emerge, which has been discussed in several studies, for example by Russell Doolittle.\n\nIn 2013 Azua-Bustos and Vega argued that disregarding the type of lifeform that could be envisioned both on Earth and elsewhere in the Universe, all should share in common the attribute of being entities that decrease their internal entropy at the expense of free energy obtained from its surroundings. As entropy allows the quantification of the degree of disorder in a system, any envisioned lifeform must have a higher degree of order than its supporting environment. These authors showed that by using fractal mathematics analysis alone, they could readily quantify the degree of structural complexity difference (and thus entropy) of living processes as distinct entities separate from their similar abiotic surroundings. This approach may allow the future detection of unknown forms of life both in the Solar System and on recently discovered exoplanets based on nothing more than entropy differentials of complementary datasets (morphology, coloration, temperature, pH, isotopic composition, etc.). \nDetecting ‘life as we don't know it’ by fractal analysis\n\nFor nearly a century and a half, beginning with Clausius' 1863 memoir \"On the Concentration of Rays of Heat and Light, and on the Limits of its Action\", much writing and research has been devoted to the relationship between thermodynamic entropy and the evolution of life. The argument that life feeds on negative entropy or negentropy was asserted by physicist Erwin Schrödinger in a 1944 book \"What is Life?\". He posed, \"How does the living organism avoid decay?\" The obvious answer is: \"By eating, drinking, breathing and (in the case of plants) assimilating.\" Recent writings have used the concept of Gibbs free energy to elaborate on this issue.\nWhile energy from nutrients is necessary to sustain an organism's order, there is also the Schrödinger prescience: \"An organism's astonishing gift of concentrating a stream of order on itself and thus escaping the decay into atomic chaos – of drinking orderliness from a suitable environment – seems to be connected with the presence of the aperiodic solids...\" We now know that the 'aperiodic' crystal is DNA and that the irregular arrangement is a form of information. \"The DNA in the cell nucleus contains the master copy of the software, in duplicate. This software seems to control by \"specifying an algorithm, or set of instructions, for creating and maintaining the entire organism containing the cell.\" DNA and other macromolecules determine an organism's life cycle: birth, growth, maturity, decline, and death. Nutrition is necessary but not sufficient to account for growth in size as genetics is the governing factor. At some point, organisms normally decline and die even while remaining in environments that contain sufficient nutrients to sustain life. The controlling factor must be internal and not nutrients or sunlight acting as causal exogenous variables. Organisms inherit the ability to create unique and complex biological structures; it is unlikely for those capabilities to be reinvented or be taught each generation. Therefore, DNA must be operative as the prime cause in this characteristic as well. Applying Boltzmann's perspective of the second law, the change of state from a more probable, less ordered and high entropy arrangement to one of less probability, more order, and lower entropy seen in biological ordering calls for a function like that known of DNA. DNA's apparent information processing function provides a resolution of the paradox posed by life and the entropy requirement of the second law.\n\nIn 1982, American biochemist Albert Lehninger argued that the \"order\" produced within cells as they grow and divide is more than compensated for by the \"disorder\" they create in their surroundings in the course of growth and division. \"Living organisms preserve their internal order by taking from their surroundings free energy, in the form of nutrients or sunlight, and returning to their surroundings an equal amount of energy as heat and entropy.\"\n\nEvolution-related concepts:\n\nIn a study titled \"Natural selection for least action\" published in the \"Proceedings of the Royal Society A.\", Ville Kaila and Arto Annila of the University of Helsinki describe how the second law of thermodynamics can be written as an equation of motion to describe evolution, showing how natural selection and the principle of least action can be connected by expressing natural selection in terms of chemical thermodynamics. In this view, evolution explores possible paths to level differences in energy densities and so increase entropy most rapidly. Thus, an organism serves as an energy transfer mechanism, and beneficial mutations allow successive organisms to transfer more energy within their environment.\n\nEntropy is defined for equilibrium systems, so objections to the extension of the second law and entropy to biological systems, especially as it pertains to its use to support or discredit the theory of evolution, have been stated. Live systems and indeed much of the systems and processes in the universe operate far from equilibrium, whereas the second law succinctly states that isolated systems evolve toward thermodynamic equilibrium — the state of maximum entropy.\n\nHowever, entropy is well defined much more broadly based on the probabilities of a system's states, whether or not the system is a dynamical one (for which equilibrium could be relevant). Even in those physical systems where equilibrium could be relevant, (1) live systems cannot persist in isolation and (2) the second principle of thermodynamics does not require that free energy be transformed into entropy along the shortest path: live organisms absorb energy from sunlight or from energy-rich chemical compounds and finally return part of such energy to the environment as entropy (heat and low free-energy compounds such as water and CO).\n\n\n\n",
    "id": "7815174",
    "title": "Entropy and life"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=5730634",
    "text": "Junkyard tornado\n\nThe junkyard tornado is an argument used to deride the probability of abiogenesis as comparable to \"the chance that a tornado sweeping through a junkyard might assemble a Boeing 747.\" It was used originally by Fred Hoyle, in which he applied statistical analysis to the origin of life, but similar observations predate Hoyle and have been found all the way back to Darwin's time, and indeed to Cicero in classical times. While Hoyle himself was an atheist, the argument has since become a mainstay of creationist and intelligent design criticisms of evolution.\n\nThis argument is rejected by the vast majority of biologists. From the modern evolutionary standpoint, while the odds of the sudden construction of higher lifeforms are indeed improbably remote, evolution proceeds in many smaller stages, each driven by natural selection rather than by chance, over a long period of time. The transition as a whole is plausible, as each step improves survivability; the Boeing 747 was not designed in a single unlikely burst of creativity, and modern lifeforms were not constructed in one single unlikely event, as the junkyard tornado posits.\n\nAccording to Fred Hoyle's analysis, the probability of cellular life's arising from non-living matter (abiogenesis) was about one-in-10. He commented:\n\nThis echoes his stance, reported elsewhere:\n\nHoyle used this to argue in favor of panspermia, that the origin of life on earth was from preexisting life in space.\n\nThe junkyard tornado derives from arguments most popular in the 1920s, prior to the modern evolutionary synthesis, which are rejected by evolutionary biologists. A preliminary step is to establish that the phase space containing some biological entity (such as humans, working cells, or the eye) is enormous, something not contentious. The argument is then to infer from the huge size of the phase space that the probability that evolution yielded the entity is exceedingly low.\n\nSometimes, arguments invoking the junkyard tornado analogy also invoke Borel's Law, which claims that highly improbable events do not occur. The usual argument against Borel's \"Law\" is that if \"all\" possible outcomes of a natural process are highly improbable when taken individually, then a highly improbable outcome is certain. The true law being referenced is actually the Strong Law of large numbers, but creationists have taken a simple statement made by Borel in books written late in his life concerning probability theory and called this statement Borel's Law.\n\nThis \"Borel's Law\" is actually the universal probability bound, which when applied to evolution is axiomatically incorrect. The universal probability bound assumes that the event one is trying to measure is completely random, and some use this argument to prove that evolution could not possibly occur, since its probability would be much less than that of the universal probability bound. This, however, is fallacious, given that evolution is not a completely random effect (genetic drift), but rather proceeds with the aid of natural selection.\n\nThe junkyard tornado is also applied to cellular biochemistry. It is comparable to the older infinite monkey theorem but instead of the works of William Shakespeare, the claim is that the probability that a protein molecule could achieve a functional sequence of amino acids is too low to be realised by chance alone. The argument conflates the difference between the complexity that arises from living organisms that are able to reproduce themselves (and as such may change to become more complex over time) with the complexity of inanimate objects, unable to pass on any reproductive changes (such as the multitude of parts manufactured in Boeing 747). The comparison breaks down because of this important distinction.\n\nAccording to Ian Musgrave in \"Lies, Damned Lies, Statistics, and Probability of Abiogenesis Calculations\":\nThe junkyard tornado argument is rejected by evolutionary biologists, since, as the late John Maynard Smith pointed out, \"no biologist imagines that complex structures arise in a single step.\" Evolutionary biology explains how complex cellular structures evolved by analysing the intermediate steps required for precellular life. It is these intermediate steps that are omitted in creationist arguments, which is the cause of their overestimating of the improbability of the entire process.\n\nHoyle's argument is a mainstay of creationist, intelligent design, orthogenetic and other criticisms of evolution. It has been labeled a fallacy by Richard Dawkins in his two books \"The Blind Watchmaker\" and \"Climbing Mount Improbable\". Dawkins argues that the existence of God, who under theistic uses of Hoyle's argument is implicitly responsible for the origin of life, defies probability far more than does the spontaneous origin of life even given Hoyle's assumptions, with Dawkins detailing his counter-argument in \"The God Delusion\", describing God as the Ultimate Boeing 747 gambit.\n\n\n",
    "id": "5730634",
    "title": "Junkyard tornado"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=27558974",
    "text": "Underwater camouflage\n\nUnderwater camouflage is the set of methods of achieving crypsis—avoidance of observation—that allows otherwise visible aquatic organisms to remain unnoticed by other organisms such as predators or prey.\n\nCamouflage in large bodies of water differs markedly from camouflage on land. The environment is essentially the same on all sides. Light always falls from above, and there is generally no variable background to compare with trees and bushes. Three main camouflage methods predominate in water: transparency, reflection, and counter-illumination. Transparency and reflectivity are most important in the top 100 metres of the ocean; counter-illumination is the main method from 100 metres down to 1000 metres; while camouflage becomes less important in the dark waters below 1000 metres.\n\nCamouflage in relatively shallow waters is more like terrestrial camouflage, where additional methods are used by many animals. For example, self-decoration is employed by decorator crabs; mimesis by animals such as the leafy sea dragon; countershading by many fish including sharks; distraction with eyespots by many fish; active camouflage through ability to change colour rapidly in fish such as the flounder, and cephalopods including octopus, cuttlefish, and squid.\n\nThe ability to camouflage oneself provides a survival advantage in the constant struggle between predators and prey. Natural selection has produced a wide variety of methods of survival in the oceans.\n\nIn Ancient Greece, Aristotle commented on the color-changing abilities, both for camouflage and for signalling, of cephalopods including the octopus, in his \"Historia animalium\":\n\nThree main camouflage methods predominate in the oceans: transparency, reflection, and counterillumination. Transparency and reflectivity are most important in the top 100 metres of the ocean; counterillumination is the main method from 100 metres down to 1000 metres; while camouflage becomes less important in the dark waters below 1000 metres. Most animals of the open sea use at least one of these methods to camouflage themselves. Camouflage in relatively shallow waters is more like terrestrial camouflage, where additional methods are used by animals in many different groups. These methods of camouflage are described in turn below.\n\nTransparency is common, even dominant, in animals of the open sea, especially those that live in relatively shallow waters. It is found in plankton of many species, as well as larger animals such as jellyfish, salps (floating tunicates), and comb jellies.\nMany marine animals that float near the surface are highly transparent, giving them almost perfect camouflage. However, transparency is difficult for bodies made of materials that have different refractive indices from seawater. Some marine animals such as jellyfish have gelatinous bodies, composed mainly of water; their thick mesogloea is acellular and highly transparent. This conveniently makes them buoyant, but it also makes them large for their muscle mass, so they cannot swim fast. Gelatinous planktonic animals are between 50 and 90 per cent transparent. A transparency of 50 per cent is enough to make an animal invisible to a predator such as cod at a depth of ; better transparency is required for invisibility in shallower water, where the light is brighter and predators can see better. For example, a cod can see prey that are 98 per cent transparent in optimal lighting in shallow water. Therefore, transparency is most effective in deeper waters.\n\nSome tissues such as muscles can be made transparent, provided either they are very thin or organised as regular layers or fibrils that are small compared to the wavelength of visible light. Familiar examples of transparent body parts are the lens and cornea of the vertebrate eye. The lens is made of the protein crystallin; the cornea is made of the protein collagen. Other structures cannot be made transparent, notably the retinas or equivalent light-absorbing structures of eyes — they must absorb light to be able to function. The camera-type eye of vertebrates and cephalopods must be completely opaque. Finally, some structures are visible for a reason, such as to lure prey. For example, the nematocysts (stinging cells) of the transparent siphonophore \"Agalma okenii\" resemble small copepods. Examples of transparent marine animals include a wide variety of larvae, including coelenterates, siphonophores, salps, gastropod molluscs, polychaete worms, many shrimplike crustaceans, and fish; whereas the adults of most of these are opaque and pigmented, resembling the seabed or shores where they live. Adult comb jellies and jellyfish are mainly transparent, like their watery background. The small Amazon river fish \"Microphilypnus amazonicus\" and the shrimps it associates with, \"Pseudopalaemon gouldingi\", are so transparent as to be \"almost invisible\"; further, these species appear to select whether to be transparent or more conventionally mottled (disruptively patterned) according to the local background in the environment.\n\nMany fish are covered with highly reflective scales, giving the appearance of silvered mirror glass. Reflection through silvering is widespread or dominant in fish of the open sea, especially those that live in the top 100 metres. Where transparency cannot be achieved, it can be imitated effectively by silvering to make an animal's body highly reflective. At medium depths at sea, light comes from above, so a mirror oriented vertically makes animals such as fish invisible from the side. Most fish in the upper ocean such as sardine and herring are camouflaged by silvering.\n\nThe marine hatchetfish is extremely flattened laterally (side to side), leaving the body just millimetres thick, and the body is so silvery as to resemble aluminium foil. The mirrors consist of microscopic structures similar to those used to provide structural coloration: stacks of between 5 and 10 crystals of guanine spaced about ¼ of a wavelength apart to interfere constructively and achieve nearly 100 per cent reflection. In the deep waters that the hatchetfish lives in, only blue light with a wavelength of 500 nanometres percolates down and needs to be reflected, so mirrors 125 nanometres apart provide good camouflage.\n\nIn fish such as the herring which live in shallower water, the mirrors must reflect a mixture of wavelengths, and the fish accordingly has crystal stacks with a range of different spacings. A further complication for fish with bodies that are rounded in cross-section is that the mirrors would be ineffective if laid flat on the skin, as they would fail to reflect horizontally. The overall mirror effect is achieved with many small reflectors, all oriented vertically. Silvering is found in other marine animals as well as fish. The cephalopods, including squid, octopus and cuttlefish, have multi-layer mirrors made of protein rather than guanine.\n\nCounter-illumination through bioluminescence on the underside (ventral region) of the body is found in many species that live in the open ocean down to about 1000 metres. The generated light increases an animal's brightness when seen from below to match the brightness of the ocean surface; it is an effective form of active camouflage. It is notably used by some species of squid, such as the midwater squid, \"Abralia veranyi\". These have light-producing organs (photophores) scattered all over their undersides, creating a sparkling glow that prevents the animal from appearing as a dark shape when seen from below. Counter-illumination camouflage is the likely function of the bioluminescence of many marine organisms, though light is also produced to attract or to detect prey and for signalling.\n\nTop/bottom countershading is common in fish including sharks, marlin, and mackerel, and animals in other groups such as dolphins, turtles and penguins. These animals have dark upper sides to match the ocean depths, and light undersides to avoid appearing dark against the bright sea surface.\n\nMimesis is practised by animals such as the leafy sea dragon, \"Phycodurus eques\", and the leaf scorpionfish, \"Taenianotus triacanthus\", which resemble parts of plants, and gently rock their bodies as if swayed by a current. In the fish species \"Novaculichthys taeniourus\", the rockmover or dragon wrasse, there is a striking difference in appearance between the adults and the juveniles. A juvenile Rockmover resembles a loose piece of sea weed. It swims in a vertical position with its head pointing downwards, and behaves in a way that perfectly resembles the movement of a piece of seaweed: moving back and forth in the surge, as if it was inanimate.\n\nSelf-decoration is employed by animals in different groups, including decorator crabs, which attach materials from their environment, as well as living organisms, to camouflage themselves. For example, the Japanese hermit crab, \"Eupagurus constans\", has the hydroid \"Hydractinia sodalis\" growing all over the shell that it lives in. Another hermit crab, \"Eupagurus cuanensis\", has the aposematic orange sponge \"Suberites domuncula\" which is bitter-tasting and not eaten by fish.\n\nSimilarly, sea urchins use their tube feet to pick up debris from the bottom and attach it to their upper surfaces. They use shells, rocks, algae and sometimes sea anemones.\n\nMany fish have eyespots near their tails, a form of automimicry, to distract attacks away from the vulnerable head and eye. For example, \"Chaetodon capistratus\" has both a (disruptive) eyestripe to conceal the eye, and a large eyespot near its tail, giving the impression that the head is at the tail end of the body.\n\nFish such as \"Dascyllus aruanus\" have bold disruptive patterns on their sides, breaking up their outlines with strong contrasts. Fish like \"Heniochus macrolepidotus\" have similar bands of colour that extend into fins projecting far from the body, distracting attention from the true shape of the fish.\n\nSome fish which mimic seaweeds such as the frogfishes \"Antennarius marmoratus\" and \"Pterophryne tumida\" have elaborate projections and spines which are combined with complex disruptive coloration. These have the effect of destroying the signature \"fish\" outline of these animals, as well as helping them to appear as pieces of algae.\n\nA variety of marine animals possess active camouflage through their ability to change colour rapidly. Several bottom-living fish such as the flounder can hide themselves effectively against a variety of backgrounds. Many cephalopods including octopus, cuttlefish, and squid similarly use colour change, in their case both for camouflage and signalling. For example, the big blue octopus, \"Octopus cyanea\", hunts during the day, and can match itself to the colours and textures of its surroundings, both to avoid predators and to enable it to approach prey. It can perfectly resemble a rock or a coral it is hiding beside. When necessary, in order to scare away a potential predator, it can display markings which resemble eyes.\n\nLike all flounders, Peacock flounders, \"Bothus mancus\", have excellent adaptive camouflage. They use cryptic coloration to avoid being detected by both prey and predators. Whenever possible rather than swim, they crawl on their fins along the bottom while constantly changing colours and patterns to match their background. In a study, some flounders demonstrated the ability to change pattern in eight seconds. They were able to match the pattern of checkerboards that they were placed on. Changing pattern is an extremely complex process involving the flounder's vision and hormones. If one of the fish's eyes is damaged, or covered by the sand, the flounder has difficulties in matching its pattern to its surroundings. Whenever the fish is hunting or hiding from predators, it buries itself into the sand, leaving only the eyes protruding.\n\n\n",
    "id": "27558974",
    "title": "Underwater camouflage"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=23003393",
    "text": "Negligible senescence\n\nNegligible senescence is the lack of symptoms of aging in some organisms. Negligibly senescent organisms do not have measurable reductions in their reproductive capability with age, or measurable functional decline with age. Death rates in negligibly senescent organisms do not increase with age as they do in senescent organisms.\n\nThere are many examples of species for whose organisms scientists have not detected an increase in mortality rate after maturity. This may mean that the lifespan of the organism is so long that researchers' subjects have not yet lived up to the time when a measure of the species' longevity can be made. Turtles, for example, were once thought to lack senescence, but more extensive observations have found evidence of decreasing fitness with age.\n\nStudy of negligibly senescent animals may provide clues that lead to better understanding of the aging process and influence theories of aging. The phenomenon of negligible senescence in some animals is a traditional argument for attempting to achieve similar negligible senescence in humans by technological means.\n\nThere are also organisms that exhibit negative senescence, whereby mortality chronologically decreases as the organism ages, for all or part of the life cycle, in disagreement with the Gompertz–Makeham law of mortality (see also Late-life mortality deceleration). Furthermore, there are species that have been observed to regress to a larval state and regrow into adults multiple times, such as \"Turritopsis dohrnii\".\n\nRecent studies have indicated a connection between phenomena related to negligible senescence and the general stability of an organism's genome, specifically transcription processes, over its lifetime.\n\nSome fish, such as some varieties of sturgeon and rougheye rockfish, and some tortoises and turtles are thought to be negligibly senescent, although recent research on turtles has uncovered evidence of senescence in the wild. The age of a captured fish specimen can be measured by examining growth patterns similar to tree rings on the otoliths (parts of motion-sensing organs).\n\nIn plants, aspen trees are one example of biological immortality. Each individual tree can live for 40–150 years above ground, but the root system of the clonal colony is long-lived. In some cases, this is for thousands of years, sending up new trunks as the older trunks die off above ground. One such colony in Utah, given the nickname of \"Pando\", is estimated to be 80,000 years old, making it possibly the oldest living colony of aspens.\n\nThe world's oldest known living non-clonal organism was the Methuselah tree of the species \"Pinus longaeva\" growing high in the White Mountains of Inyo County in eastern California, aged years.\nThis record was superseded in 2012 by another Great Basin bristlecone pine located in the same region as Methuselah, and was estimated to be 5062 years old. The tree was sampled by Edmund Schulman and dated by Tom Harlan.\n\nAmong bacteria, individual organisms are vulnerable and can easily die, but on the level of the colony, bacteria can live indefinitely. The two daughter bacteria resulting from cell division of a parent bacterium can be regarded as unique individuals or as members of a biologically “immortal” colony. The two daughter cells can be regarded as “rejuvenated” copies of the parent cell because damaged macromolecules have been split between the two cells and diluted. See asexual reproduction.\n\nSome examples of maximum observed life span of animals thought to be negligibly senescent are:\nSome rare organisms, such as tardigrades, usually have short lifespans, but are able to survive for thousands of years — and, perhaps, indefinitely — if they enter into the state of cryptobiosis, whereby their metabolism is reversibly suspended. It is hypothesized by advocates of cryonics that the human central nervous system can be similarly put into a state of suspended animation shortly before brain death to be revived at a future point in the technological development of humankind when such operation would be possible.\n\n",
    "id": "23003393",
    "title": "Negligible senescence"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=7373582",
    "text": "Transgressive segregation\n\nIn genetics, transgressive segregation is the formation of extreme phenotypes, or transgressive phenotypes, observed in segregated hybrid populations compared to phenotypes observed in the parental lines. The appearance of these trangressive (extreme) phenotypes can be either positive or negative in terms of fitness. If both parents' favorable alleles come together, it will result in a hybrid having a higher fitness than the two parents. The hybrid species will show more genetic variation and variation in gene expression than their parents. As a result, the hybrid species will have some traits that are transgressive (extreme) in nature. Transgressive segregation can allow a hybrid species to populate different environments/niches in which the parent species do not reside, or compete in the existing environment with the parental species.\n\nGenetic\n\nThere are many causes for transgressive segregation in hybrids. One cause can be due to recombination of additive alleles. Recombination results in new pairs of alleles at two or more loci. These different pairs of alleles can give rise to new phenotypes since gene expression has been changed at these loci. Another cause can be elevated mutation rate. When mutation rates are high, it is more probable that a mutation will occur and cause an extreme phenotypic change. Reduced developmental stability is another cause for transgressive segregation. Developmental stability refers to the capability of a genotype to go through a constant development of a phenotype in a certain environmental setting. If there is a disturbance due to genetic or environmental factors, the genotype will be more sensitive to phenotypic changes. Another cause arises from the interaction between two alleles of two different genes, also known as the epistatic effect. Epistasis is the event when one allele at a locus prevents an allele at another locus to express its product as if it is masking its effect. Therefore, epistasis can be related to gene over dominance caused by heterozygosity at specific loci.[2] What this means is that the heterozygote (hybrid) when compared to the homozygote (parent) is better adapted and therefore shows more transgressive, extreme phenotypes. All of these causes lead to the appearance of these extreme phenotypes and creates a hybrid species that will deviate away from the parent species niche and eventually create an individual \"hybrid\" species.\n\nEnvironmental\nOther than the genetic factors solely causing transgressive segregation, environmental factors can cause genetic factors to take place. Environmental factors that cause transgressive segregation can be influenced by human activity and climate change. Both human activity and climate change have the capability to force species of a specific genome to interact with other species with different genomes.\n\nFor example, if a bridge is built that connects two isolated areas to one another, a gene flow door would open. This open door will increase the interactions between different species with different genomes can create hybrid species that can potentially show transgressive phenotypes. Human activity can open the gene flow door by pursuing harmful actions such as cutting down forests and pollution. Climate change on the other hand can open the gene flow door by breaking climate and environmental barriers that were present before. This convergence between species can give rise to a hybrid species that will have more phenotypic variation when compared to the parent species. This increase in phenotypic variation has the potential for transgressive segregation to occur.\n\nIn Kenya, there is an organism called septoria tritici blotch (STB) that diminishes yield in wheat crop. The parent species of wheat had little resistance toward STB, but the hybrid species due to transgressive segregation showed a higher resistance toward STB and therefore a higher fitness. You can create a higher resistance to STB by crossing genes together that are efficient. In result, out of 36 crosses there were 31 that showed a higher mean fitness than the control, parent value. These 31 crosses indicate a higher resistance to STB. The crosses used were from other commercial wheat's that were high yielding which is advantageous because there is a lower chance of deleterious (unwanted traits) appearing and therefore an increase in beneficial traits. Transgressive segregation has been found to be useful to create a resistance toward this organism in order to increase the yield of wheat crop.\n\nRieseberg used sunflowers to show the transgressive segregation of parental traits. \"Helianthus annuus\" and \"Helianthus petiolaris\" are the two parent groups for the hybrids. Ultimately there were three hybrid sunflower species. When compared to the fitness of the parents, the hybrids showed a higher tolerance in areas which the parent species would not be able to survive i.e. salt marsh, sand dunes, and deserts. Transgressive segregation allowed these hybrids to survive in areas that the parent would not be able to. Therefore, the hybrids were populated in areas where the parent species were not. This is due to hybrid species showing more gene expression (phenotypes) than their parents and also having some genes that are transgressive (extreme) in nature.\n\nThere are many ways to test if transgressive segregation occurred within a population. One common way to test for transgressive segregation is to use a Dunnett's test. This test looks at whether the hybrid species' performance was different from the control group by looking whether or not the mean of the control group (parent species) differs significantly from mean of the other groups. If there is a difference, that is an indication of transgressive segregation. Another commonly used test is the use of quantitative trait loci (QTL) to assess transgressive segregation. Alleles with QTL that were opposed (either by overdomiance or underdominance) of the parental parent QTL indicate that transgressive segregation occurred. Alleles with QTL that was the same as the predicted parent QTL showed that there was no transgressive segregation.\n\nTransgressive segregation creates an opportunity for new hybrid species to arise that are more fit than their ancestors. As seen with the STB in Kenya and Rieseberg's sunflowers, transgressive segregation can be used to create a species that is more adaptable and resistant in areas where there is environmental stress. Transgressive segregation can be seen as genetic engineering in the way that the goal for each of these events is to create an organism that is more fit than the last.\n",
    "id": "7373582",
    "title": "Transgressive segregation"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=10225184",
    "text": "Oxygen evolution\n\nOxygen evolution is the process of generating molecular oxygen through chemical reaction. Mechanisms of oxygen evolution include the oxidation of water during oxygenic photosynthesis, electrolysis of water into oxygen and hydrogen, and electrocatalytic oxygen evolution from oxides and oxoacids.\n\nPhotosynthetic oxygen evolution is the fundamental process by which breathable oxygen is generated in earth's biosphere. The reaction is part of the light-dependent reactions of photosynthesis in cyanobacteria and the chloroplasts of green algae and plants. It utilizes the energy of light to split a water molecule into its protons and electrons for photosynthesis. Free oxygen is generated as a by-product of this reaction, and is released into the atmosphere.\n\nPhotosynthetic oxygen evolution occurs via the light-dependent oxidation of water to molecular oxygen and can be written as the following simplified chemical reaction:\n\n2HO formula_1 4e + 4H + O\n\nThe reaction requires the energy of four photons. The electrons from the oxidized water molecules replace electrons in the P component of photosystem II that have been removed into an electron transport chain via light-dependent excitation and resonance energy transfer onto plastoquinone. Photosytem II, therefore, has also been referred to as water-plastoquinone oxido-reductase. The protons are released into the thylakoid lumen, thus contributing to the generation of a proton gradient across the thylakoid membrane. This proton gradient is the driving force for ATP synthesis via photophosphorylation and coupling the absorption of light energy and oxidation of water to the creation of chemical energy during photosynthesis.\n\nWater oxidation is catalyzed by a manganese-containing cofactor contained in photosystem II known as the oxygen-evolving complex (OEC) or water-splitting complex. Manganese is an important cofactor, and calcium and chloride are also required for the reaction to occur.\n\nX-ray crystallographic data have been used to propose a structure and mechanism of action for the oxygen-evolving complex and its manganese cluster. Based on structural and spectroscopic experiments, oxygen evolution involves a core three-plus-one cluster of three manganese ions and one calcium ion, with one additional manganese, which are oxidized via intermediate states called \"S-states\". The O-O bond of molecular oxygen is formed between manganese-ligated oxygen atoms at the most oxidized, or S4, state. The procedure of reactions in S-states is almost completely understood by Dolai's S-state diagrams. Ref. Dolai, U (2017). \"Chemical Scheme of Water -Splitting Process during Photosynthesis by the way of Experimental Analysis \". 12(6): 65-67 doi: 10.9790/3008-1206026567. ISSN 2319-7676.\n\nIt was not until the end of the 18th century that Joseph Priestley discovered by accident the ability of plants to \"restore\" air that had been \"injured\" by the burning of a candle. He followed up on the experiment by showing that air \"restored\" by vegetation was \"\"not at all inconvenient to a mouse.\"\" He was later awarded a medal for his discoveries that: \"\"...no vegetable grows in vain... but cleanses and purifies our atmosphere.\"\" Priestley's experiments were followed up by Jan Ingenhousz, a Dutch physician, who showed that \"restoration\" of air only worked in the presence of light and green plant parts.\n\nIngenhousz suggested in 1796 that CO (carbon dioxide) is split during photosynthesis to release oxygen, while the carbon combined with water to form carbohydrates. While this hypothesis was attractive and reasonable and thus widely accepted for a long time, it was later proven incorrect. Graduate student C.B. Van Niel at Stanford University found that purple sulfur bacteria reduce carbon to carbohydrates, but accumulate sulfur instead of releasing oxygen. He boldly proposed that, in analogy to the sulfur bacteria's forming elemental sulfur from HS (hydrogen sulfide), plants would form oxygen from HO (water). In 1937, this hypothesis was corroborated by the discovery that plants are capable of producing oxygen in the absence of CO. This discovery was made by Robin Hill, and subsequently the light-driven release of oxygen in the absence of CO was called the \"Hill reaction\". Our current knowledge of the mechanism of oxygen evolution during photosynthesis was further established in experiments tracing isotopes of oxygen from water to oxygen gas.\n\nOxygen evolution occurs as a byproduct of hydrogen production via electrolysis of water. While oxygen production is not the main focus of industrial applications of water electrolysis, it becomes essential for life support systems in situations that require the generation of oxygen for air revitalization. Human exploration of regions that lack breathable oxygen, such as the deep sea or outer space, requires means of reliably generating oxygen apart from earth's atmosphere. Submarines and spacecraft utilize either an electrolytic mechanism (water or solid oxide electrolysis) or chemical oxygen generators as part of their life support equipment.\n\n\n",
    "id": "10225184",
    "title": "Oxygen evolution"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=19445663",
    "text": "Oceanic dispersal\n\nOceanic dispersal is a type of biological dispersal that occurs when terrestrial organisms transfer from one land mass to another by way of a sea crossing. Often this occurs via large rafts of floating vegetation such as are sometimes seen floating down major rivers in the tropics and washing out to sea, occasionally with animals trapped on them. Dispersal via such a raft is sometimes referred to as a \"rafting event\".\n\nColonization of land masses by plants can also occur via long-distance oceanic dispersal of floating seeds.\n\nRafting has played an important role in the colonization of isolated land masses by mammals. Prominent examples include Madagascar, which has been isolated for ~120 million years (Ma), and South America, which was isolated for much of the Cenozoic. Both land masses, for example, appear to have received their primates by this mechanism. According to genetic evidence, the common ancestor of the lemurs of Madagascar appears to have crossed the Mozambique Channel by rafting between 50 and 60 Ma ago. Likewise, the New World monkeys are thought to have originated in Africa and rafted to South America by the Oligocene, when the continents were much closer than they are today. Madagascar also appears to have received its tenrecs (25–42 Ma ago), nesomyid rodents (20–24 Ma ago) and euplerid carnivorans (19–26 Ma ago) by this route and South America its caviomorph rodents (over 30 Ma ago). Simian primates (ancestral to monkeys) and hystricognath rodents (ancestral to caviomorphs) are believed to have previously rafted from Asia to Africa about 40 Ma ago.\n\nAmong reptiles, several iguanid species in the South Pacific have been hypothesized to be descended from iguanas that rafted from Central or South America (an alternative theory involves dispersal of a putative now-extinct iguana lineage from Australia or Asia). Similarly, a number of clades of American geckos seem to have rafted over from Africa during both the Paleogene and Neogene. Skinks of the related genera \"Mabuya\" and \"Trachylepis\" also apparently both floated across the Atlantic from Africa to South America and Fernando de Noronha, respectively, during the last 9 Ma. Skinks from the same group have also rafted from Africa to Cape Verde, Madagascar, the Seychelles, the Comoros and Socotra. (Among lizards, skinks and geckos seem especially capable of surviving long transoceanic journeys.) Surprisingly, even burrowing amphisbaenians and blind snakes appear to have rafted from Africa to South America.\n\nAn example of a bird that is thought to have reached its present location by rafting is the weak-flying South American hoatzin, whose ancestors apparently floated over from Africa.\n\nColonization of groups of islands can occur by an iterative rafting process sometimes called island hopping. Such a process appears to have played a role, for example, in the colonization of the Caribbean by mammals of South American origin (including caviomorphs and monkeys).\n\nA remarkable example of iterative rafting has been proposed for spiders of the genus \"Amaurobioides\". Members of this genus inhabit coastal sites and build silken cells which they seal at high tide; however, they do not balloon. DNA sequence analysis suggests that ancestors of the genus dispersed from southern South America to South Africa about 10 million years (Ma) ago, where the most basal clade is found; subsequent rafting events then took the genus eastward with the Antarctic Circumpolar Current to Australia, then to New Zealand and finally to Chile by about 2 Ma ago. Another example among spiders is the species \"Moggridgea rainbowi\", the only Australian member of a genus otherwise endemic to Africa, with a divergence date of 2 to 16 Ma ago.\n\nHowever, oceanic dispersal of terrestrial species may not always take the form of rafting; in some cases, swimming or simply floating may suffice. Tortoises of the genus \"Chelonoidis\" arrived in South America from Africa in the Oligocene; they were probably aided by their ability to float with their heads up, and to survive up to six months without food or fresh water. South American tortoises then went on to colonize the West Indies and Galápagos Islands. The dispersal of anthracotheres from Asia to Africa about 40 Ma ago, and the much more recent dispersal of hippos (relatives and probably descendants of anthracotheres) from Africa to Madagascar may have occurred by floating or swimming.\n\nThe first documented example of colonization of a land mass by rafting occurred in the aftermath of hurricanes Luis and Marilyn in the Caribbean in 1995. A raft of uprooted trees carrying fifteen or more green iguanas was observed by fishermen landing on the east side of Anguilla – an island where they had never before been recorded. The iguanas had apparently been caught on the trees and rafted two hundred miles across the ocean from Guadeloupe, where they are indigenous. Examination of the weather patterns and ocean currents indicated that they had probably spent three weeks at sea before landfall. This colony began breeding on the new island within two years of its arrival.\n\nThe advent of human civilization has created opportunities for organisms to raft on floating artifacts, which may be more durable than natural floating objects. This phenomenon was noted following the 2011 Tōhoku tsunami in Japan, with about 300 species found to have been carried on debris by the North Pacific Current to the west coast of North America (although no colonizations have been detected thus far).\n\n\n",
    "id": "19445663",
    "title": "Oceanic dispersal"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=817175",
    "text": "Biological dispersal\n\nBiological dispersal refers to both the movement of individuals (animals, plants, fungi, bacteria, etc.) from their birth site to their breeding site ('natal dispersal'), as well as the movement from one breeding site to another ('breeding dispersal').\nDispersal is also used to describe the movement of propagules such as seeds and spores.\nTechnically, dispersal is defined as any movement that has the potential to lead to gene flow. \nThe act of dispersal involves three phases: departure, transfer, settlement and there are different fitness costs and benefits associated with each of these phases.\nThrough simply moving from one habitat patch to another, the dispersal of an individual has consequences not only for individual fitness, but also for population dynamics, population genetics, and species distribution. Understanding dispersal and the consequences both for evolutionary strategies at a species level, and for processes at an ecosystem level, requires understanding on the type of dispersal, the dispersal range of a given species, and the dispersal mechanisms involved.\n\nBiological dispersal may be contrasted with geodispersal, which is the mixing of previously isolated populations (or whole biotas) following the erosion of geographic barriers to dispersal or gene flow (Lieberman, 2005; Albert and Reis, 2011).\n\nDispersal can be distinguished from animal migration (typically round-trip seasonal movement), although within the population genetics literature, the terms 'migration' and 'dispersal' are often used interchangeably.\n\nSome organisms are motile throughout their lives, but others are adapted to move or be moved at precise, limited phases of their life cycles. This is commonly called the dispersive phase of the life cycle. The strategies of organisms' entire life cycles often are predicated on the nature and circumstances of their dispersive phases.\n\nIn general there are two basic types of dispersal:\nDue to population density, dispersal may relieve pressure for resources in an ecosystem, and competition for these resources may be a selection factor for dispersal mechanisms.\n\nDispersal of organisms is a critical process for understanding both geographic isolation in evolution through gene flow and the broad patterns of current geographic distributions (biogeography).\n\nA distinction is often made between natal dispersal where an individual (often a juvenile) moves away from the place it was born, and breeding dispersal where an individual (often an adult) moves away from one breeding location to breed elsewhere.\n\nIn the broadest sense, dispersal occurs when the fitness benefits of moving outweigh the costs.\n\nThere are a number of benefits to dispersal such as locating new resources, escaping unfavorable conditions, avoiding competing with siblings, and avoiding breeding with closely related individuals which could lead to inbreeding depression.\n\nThere are also a number of costs associated with dispersal, which can be thought of in terms of four main currencies: energy, risk, time and opportunity.\nEnergetic costs include the extra energy required to move as well as energetic investment in movement machinery (e.g. wings). Risks include increased injury and mortality during dispersal and the possibility of settling in an unfavorable environment.\nTime spent dispersing is time that often cannot be spent on other activities such as growth and reproduction.\nFinally dispersal can also lead to outbreeding depression if an individual is better adapted to its natal environment than the one it ends up in. In social animals (such as many birds and mammals) a dispersing individual must find and join a new group, which can lead to loss of social rank.\n\n\"Dispersal range\" refers to the distance a species can move from an existing population or the parent organism. An ecosystem depends critically on the ability of individuals and populations to disperse from one habitat patch to another. Therefore, biological dispersal is critical to the stability of ecosystems.\n\nFew species are ever evenly or randomly distributed within or across landscapes. In general, species significantly vary across the landscape in association with environmental features that influence their reproductive success and population persistence. Spatial patterns in environmental features (e.g. resources) permit individuals to escape unfavorable conditions and seek out new locations. This allows the organism to \"test\" new environments for their suitability, provided they are within animal's geographic range. In addition, the ability of a species to disperse over a gradually changing environment could enable a population to survive extreme conditions. (i.e. climate change).\n\nAs the climate changes, prey and predators have to adapt to survive. This poses a problem for many animals, for example the Southern Rockhopper Penguins. These penguins are able to live and thrive in a variety of climates due to the penguins' phenotypic plasticity. However, they are predicted to respond by dispersal, not adaptation this time. This is explained due to their long life spans and slow microevolution. Penguins in the subantarctic have very different foraging behavior than the subtropical waters, it would be very hard to survive and keep up with the fast changing climate because these behaviors took years to shape.\n\nA dispersal barrier may mean that the dispersal range of a species is much smaller than the species distribution. An artificial example is habitat fragmentation due to human land use. Natural barriers to dispersal that limit species distribution include mountain ranges and rivers. An example is the separation of the ranges of the two species of chimpanzee by the Congo River.\n\nOn the other hand, human activities may also expand the dispersal range of a species by providing new dispersal methods (e.g., ships). Many of them become invasive, like rats and stinkbugs, but some species also have a slightly positive effect to human settlers like honeybees and earthworms.\n\nMost animals are capable of locomotion and the basic mechanism of dispersal is movement from one place to another. Locomotion allows the organism to \"test\" new environments for their suitability, provided they are within the animal's range. Movements are usually guided by inherited behaviors.\n\nThe formation of barriers to dispersal or gene flow between adjacent areas can isolate populations on either side of the emerging divide. The geographic separation and subsequent genetic isolation of portions of an ancestral population can result in speciation.\n\nSeed dispersal is the movement or transport of seeds away from the parent plant. Plants have limited mobility and consequently rely upon a variety of dispersal vectors to transport their propagules, including both abiotic and biotic vectors. Seeds can be dispersed away from the parent plant individually or collectively, as well as dispersed in both space and time. The patterns of seed dispersal are determined in large part by the dispersal mechanism and this has important implications for the demographic and genetic structure of plant populations, as well as migration patterns and species interactions. There are five main modes of seed dispersal: gravity, wind, ballistic, water and by animals.\n\nThere are numerous animal forms that are non—motile, such as sponges, bryozoans, tunicates, sea anemones, corals, and oysters. In common, they are all either marine or aquatic. It may seem curious that plants have been so successful at stationary life on land, while animals have not, but the answer lies in the food supply. Plants produce their own food from sunlight and carbon dioxide—both generally more abundant on land than in water. Animals fixed in place must rely on the surrounding medium to bring food at least close enough to grab, and this occurs in the three-dimensional water environment, but with much less abundance in the atmosphere.\n\nAll of the marine and aquatic invertebrates whose lives are spent fixed to the bottom (more or less; anemones are capable of getting up and moving to a new location if conditions warrant) produce dispersal units. These may be specialized \"buds\", or motile sexual reproduction products, or even a sort of alteration of generations as in certain cnidaria.\n\nCorals provide a good example of how sedentary species achieve dispersion. Corals reproduce by releasing sperm and eggs directly into the water. These release events are coordinated by lunar phase in certain warm months, such that all corals of one or many species on a given reef will release on the same single or several consecutive nights. The released eggs are fertilized, and the resulting zygote develops quickly into a multicellular \"planula\". This motile stage then attempts to find a suitable substratum for settlement. Most are unsuccessful and die or are fed upon by zooplankton and bottom dwelling predators such as anemones and other corals. However, untold millions are produced, and a few do succeed in locating spots of bare limestone, where they settle and transform by growth into a polyp. All things being favorable, the single polyp grows into a coral head by budding off new polyps to form a colony.\n\nThe majority of all animals are motile. Although motile animals can, in theory, disperse themselves by their spontaneous and independent locomotive powers, a great many species utilize the existing kinetic energies in the environment, resulting in passive movement. Dispersal by water currents is especially associated with the physically small inhabitants of marine waters known as zooplankton. The term plankton comes from the Greek, πλαγκτον, meaning \"wanderer\" or \"drifter\".\n\nMany animal species, especially freshwater invertebrates, are able to disperse by wind or by transfer with an aid of larger animals (birds, mammals or fishes) as dormant eggs, dormant embryos or, in some cases, dormant adult stages. Tardigrades, some rotifers and some copepods are able to withstand desiccation as adult dormant stages. Many other taxa (Cladocera, Bryozoa, Hydra, Copepoda and so on) can disperse as dormant eggs or embryos. Freshwater sponges usually have special dormant propagules called gemmulae for such a dispersal. Many kinds of dispersal dormant stages are able to withstand not only desiccation and low and high temperature, but also action of digestive enzymes during their transfer through digestive tracts of birds and other animals, high concentration of salts and many kinds of toxicants. Such dormant-resistant stages made possible the long-distance dispersal from one water body to another and broad distribution ranges of many freshwater animals.\n\nDispersal is most commonly quantified either in terms of rate or distance.\n\nDispersal rate (also called migration rate in the population genetics literature) or probability describes the probability that any individual leaves an area or, equivalently, the expected proportion of individual to leave an area.\n\nThe dispersal distance is usually described by a dispersal kernel which gives the probability distribution of the distance traveled by any individual. A number of different functions are used for dispersal kernels in theoretical models of dispersal including the negative exponential distribution, extended negative exponential distribution, normal distribution, exponential power distribution, inverse power distribution, and the two-sided power distribution. The inverse power distribution and distributions with 'fat tails' representing long-distance dispersal events (called leptokurtic distributions) are though to best match empirical dispersal data.\n\nDispersal not only has costs and benefits to the dispersing individual (as mentioned above), but it also has consequences at the level of the population and species as well.\n\nMost populations have a patchy spatial distribution. Dispersal, by moving individuals between different sub-populations, can increase the overall connectivity of the population, helping to minimize the risk of stochastic extinction, since if a sub-population goes extinct by chance, it is likely to be recolonized if the dispersal rate is high.\nIncreased connectivity can also decrease the degree of local adaptation.\n\n\n",
    "id": "817175",
    "title": "Biological dispersal"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=34920670",
    "text": "Hologenome theory of evolution\n\nThe hologenome theory of evolution, also known as the hologenome concept of evolution, recasts the individual animal or plant (and other multicellular organisms) as a community or a \"holobiont\" – the host plus all of its symbiotic microbes. Consequently, the collective genomes of the holobiont form a \"hologenome\". Holobionts and hologenomes are structural entities that replace misnomers in the context of host-microbiota symbioses such as superorganism (i.e., an integrated social unit composed of conspecifics), organ, and metagenome. Variation in the hologenome may encode phenotypic plasticity of the holobiont and can be subject to evolutionary changes caused by selection and drift, if portions of the hologenome are transmitted between generations with reasonable fidelity. One of the important outcomes of recasting the individual as a holobiont subject to evolutionary forces is that genetic variation in the hologenome can be brought about by changes in the host genome and also by changes in the microbiome, including new acquisitions of microbes, horizontal gene transfers, and changes in microbial abundance within hosts. Although there is a rich literature on binary host–microbe symbioses, the hologenome concept distinguishes itself by including the vast symbiotic complexity inherent in many multicellular hosts. For recent literature on holobionts and hologenomes published in an open access platform, see the following reference.\n\nIn 1991, Lynn Margulis coined the term holobiont in a book chapter entitled Symbiogenesis and Symbionticism in \"Symbiosis as a Source of Evolutionary Innovation: Specation and Morphogenesis\" (MIT Press). The term holobiont is derived from the Ancient Greek ὅλος (hólos, “whole”), and the word biont for a unit of living matter.\n\nIn September 1994, Richard Jefferson first introduced the term hologenome at a presentation at Cold Spring Harbor Laboratory during a Symposium \"A Decade of PCR\", published by Cold Spring Harbor Laboratory Press as a video series. At the CSH Symposium and earlier, the unsettling number and diversity of microbes that were being discovered through the powerful tool of PCR-amplification of 16S ribosomal RNA genes was exciting, but confusing interpretations in diverse studies. A number of speakers referred to microbial contributions to mammalian or plant DNA samples as 'contamination'. In his lecture, Jefferson argued that these were likely not contamination, but rather essential components of the samples that reflected the actual genetic composition of the organism being studied, and that the logic of the organism's performance and capabilities would be embedded only in the hologenome. Observations on the ubiquity of microbes in plant and soil samples as well as laboratory work on molecular genetics of vertebrate-associated microbial enzymes informed this assertion (citations needed).\n\nIn 2008, Eugene Rosenberg and Ilana Zilber-Rosenberg independently derived the term hologenome and developed the hologenome theory of evolution. This theory was originally based on the pair’s observations of Vibrio shiloi-mediated bleaching of the coral Oculina patagonica; and since its first introduction, the theory has been promoted as a fusion of Lamarckism and Darwinism and expanded to all of evolution, not just that of corals. The history of the development of the hologenome theory and the logic undergirding its development was the focus of a cover article by Carrie Arnold in New Scientist in January, 2013. The most comprehensive treatment of the theory, including updates by the Rosenbergs on neutrality, pathogenesis and multi-level selection, can be found in their 2013 book.\n\nIn 2013, Robert Brucker and Seth Bordenstein re-invigorated the hologenome concept by showing that gut microbiomes, for the first time, are distinguishable between very closely related Nasonia wasp species and contribute to hybrid death. This publication has formed the basis of the recent resurgence and discussion in holobionts and hologenomes, namely by equating interactions between hosts and microbes as part of a conceptual continuum to interactions between genes in the same genome. In 2015, Seth R. Bordenstein and Kevin R. Theis outlined additional history and summarized a conceptual framework that aligns with pre-existing theories in biology. They discuss questions and critiques, show how neutral and selective processes are applicable to the intellectual framework, and discuss future research questions.\n\nMulticellular life is made possible by the coordination of physically and temporally distinct processes, most prominently through hormones. Hormones mediate critical activities in vertebrates, including ontogeny, somatic and reproductive physiology, sexual development, performance and behaviour. \nMany of these hormones – including most steroids and thyroxines – are secreted, typically as inactivated hormone conjugates to which a glucuronic acid or sulfate has been attached, through the endocrine and apocrine systems into epithelial corridors in which microbiota are widespread and diverse, including gut, urinary tract, lung and skin. There, the inactivated hormones can be re-activated by cleavage of the glucuronide or sulfate residue, allowing the newly re-activated hormone to be reabsorbed. Thus the concentration and bioavailability of many of the hormones is impacted by microbial cleavage of conjugated intermediaries, itself determined by a highly complex and diverse population with redundant enzymatic capabilities. Aspects of this phenomenon of enterohepatic circulation have been known for decades, and represented in a very large and complex literature, but had previously been viewed as an ancillary effect of detoxification and excretion of metabolites and xenobiotics, including effects on lifetimes of pharmaceuticals, including birth control formulations.\n\nThe basic premise is that through the microbial population structure and activity and the diversity of enzyme specificities and kinetic parameters produced by these populations, a spectrum of hormones can be re-activated and resorbed from epithelia, potentially modulating effective time and dose relationships of many vertebrate hormones. The ability to alter and modulate, amplify and suppress, disseminate and recruit new capabilities as microbially-encoded 'traits' means that sampling, sensing and responding to the environment become intrinsic features and emergent capabilities of the holobiont, with mechanisms that can provide rapid, sensitive, nuanced and persistent performance changes. \nStudies by Froebe et al. in 1990 indicating that essential mating pheromones, including androstenols, required activation by skin-associated microbial glucuronidases and sulfatases. In the absence of microbial populations in the skin, no detectable aromatic pheromone was released, as the pro-pheromone remained water-soluble and non-volatile. This effectively meant that the microbes in the skin were essential to produce a mating signal.\n\nSubsequent re-articulation describing the hologenome theory by Rosenberg and Zilber-Rosenberg, published 13 years after Jefferson's definition of the theory, was based on their observations of corals, and the coral probiotic hypothesis.\n\nCoral reefs are the largest structures created by living organisms, and contain abundant and highly complex microbial communities. A coral \"head\" is a colony of genetically identical polyps, which secrete an exoskeleton near the base. Depending on the species, the exoskeleton may be hard, based on calcium carbonate, or soft and proteinaceous. Over many generations, the colony creates a large skeleton that is characteristic of the species. Diverse forms of life take up residence in a coral colony, including photosynthetic algae such as \"Symbiodinium\", as well as a wide range of bacteria including nitrogen fixers, and chitin decomposers, all of which form an important part of coral nutrition. The association between coral and its microbiota is species dependent, and different bacterial populations are found in mucus, skeleton and tissue from the same coral fragment.\n\nOver the past several decades, major declines in coral populations have occurred. Climate change, water pollution and overfishing are three stress factors that have been described as leading to disease susceptibility. Over twenty different coral diseases have been described, but of these, only a handful have had their causative agents isolated and characterized.\n\nCoral bleaching is the most serious of these diseases. In the Mediterranean Sea, the bleaching of \"Oculina patagonica\" was first described in 1994 and, through a rigorous application of Koch's Postulates, determined to be due to infection by \"Vibrio shiloi\". From 1994 to 2002, bacterial bleaching of \"O. patagonica\" occurred every summer in the eastern Mediterranean. Surprisingly, however, after 2003, \"O. patagonica\" in the eastern Mediterranean has been resistant to \"V. shiloi\" infection, although other diseases still cause bleaching.\n\nThe surprise stems from the knowledge that corals are long lived, with lifespans on the order of decades, and do not have adaptive immune systems. Their innate immune systems do not produce antibodies, and they should seemingly not be able to respond to new challenges except over evolutionary time scales. Yet multiple researchers have documented variations in bleaching susceptibility that may be termed 'experience-mediated tolerance'. The puzzle of how corals managed to acquire resistance to a specific pathogen led Eugene Rosenberg and Ilana Zilber-Rosenberg to propose the Coral Probiotic Hypothesis. This hypothesis proposes that a dynamic relationship exists between corals and their symbiotic microbial communities. Beneficial mutations can arise and spread among the symbiotic microbes much faster than in the host corals. By altering its microbial composition, the \"holobiont\" can adapt to changing environmental conditions far more rapidly than by genetic mutation and selection in the host species alone.\n\nExtrapolating the coral probiotic hypothesis to other organisms, including higher plants and animals, led to the Rosenberg's support for and publications around the hologenome theory of evolution.\n\nThe framework of the hologenome theory of evolution is as follows (condensed from Rosenberg \"et al.\", 2007):\n\nSome authors supplement the above principles with an additional one. If a given holobiont is to be considered a unit of natural selection: \n\nTen principles of holobionts and hologenomes were presented in PLOS Biology to clarify and append what these terms and concepts are and are not, to explain how they both support and extend existing theory in the life sciences, and to discuss their potential ramifications for the multifaceted approaches of zoology and botany\n\n\nMany case studies clearly demonstrate the importance of an organism's associated microbiota to its existence. (For example, see the numerous case studies in the Microbiome article.) However, horizontal \"versus\" vertical transmission of endosymbionts must be distinguished. Endosymbionts whose transmission is predominantly vertical may be considered as contributing to the heritable genetic variation present in a host species.\n\nIn the case of colonial organisms such as corals, the microbial associations of the colony persist even though individual members of the colony, reproducing asexually, live and die. Corals also have sexual mode of reproduction, resulting in planktonic larva; it is less clear whether microbial associations persist through this stage of growth. Also, the bacterial community of a colony may change with the seasons.\n\nMany insects maintain heritable obligate symbiosis relationships with bacterial partners. For example, normal development of female wasps of the species \"Asobara tabida\" is dependent on \"Wolbachia\" infection. If \"cured\" of the infection, their ovaries degenerate. Transmission of the infection is vertical through the egg cytoplasm.\n\nIn contrast, many obligate symbiosis relationships have been described in the literature where transmission of the symbionts is via horizontal transfer. A well-studied example is the nocturnally feeding squid \"Euprymna scolopes\", which camouflages its outline against the moonlit ocean surface by emitting light from its underside with the aid of the symbiotic bacterium \"Vibrio fischeri\". The Rosenbergs cite this example within the context of the hologenome theory of evolution. Squid and bacterium maintain a highly co-evolved relationship. The newly hatched squid collects its bacteria from the sea water, and lateral transfer of symbionts between hosts permits faster transfer of beneficial mutations within a host species than are possible with mutations within the host genome.\n\nAnother traditional distinction between endosymbionts has been between primary and secondary symbionts. Primary endosymbionts reside in specialized host cells that may be organized into larger, organ-like structures (in insects, the bacteriome). Associations between hosts and primary endosymbionts are usually ancient, with an estimated age of tens to hundreds of millions of years. According to endosymbiotic theory, extreme cases of primary endosymbionts include mitochondria, plastids (including chloroplasts), and possibly other organelles of eukaryotic cells. Primary endosymbionts are usually transmitted exclusively vertically, and the relationship is always mutualistic and generally obligate for both partners. Primary endosymbiosis is surprisingly common. An estimated 15% of insect species, for example, harbor this type of endosymbiont. In contrast, secondary endosymbiosis is often facultative, at least from the host point of view, and the associations are less ancient. Secondary endosymbionts do not reside in specialized host tissues, but may dwell in the body cavity dispersed in fat, muscle, or nervous tissue, or may grow within the gut. Transmission may be via vertical, horizontal, or both vertical and horizontal transfer. The relationship between host and secondary endosymbiont is not necessarily beneficial to the host; indeed, the relationship may be parasitic.\n\nThe distinction between vertical and horizontal transfer, and between primary and secondary endosymbiosis is not absolute, but follows a continuum, and may be subject to environmental influences. For example, in the stink bug \"Nezara viridula\", the vertical transmission rate of symbionts, which females provide to offspring by smearing the eggs with gastric caeca, was 100% at 20 °C, but decreased to 8% at 30 °C. Likewise, in aphids, the vertical transmission of bacteriocytes containing the primary endosymbiont \"Buchnera\" is drastically reduced at high temperature. In like manner, the distinction between commensal, mutualistic, and parasitic relationships is also not absolute. An example is the relationship between legumes and rhizobial species: N uptake is energetically more costly than the uptake of fixed nitrogen from the soil, so soil N is preferred if not limiting. During the early stages of nodule formation, the plant-rhizobial relationship actually resembles a pathogenesis more than it does a mutualistic association.\n\nLamarckism, the concept that an organism can pass on characteristics that it acquired during its lifetime to its offspring (also known as inheritance of acquired characteristics or soft inheritance) incorporated two common ideas of its time:\n\nAlthough Lamarckian theory was rejected by the neo-Darwinism of the modern evolutionary synthesis in which evolution occurs through random variations being subject to natural selection, the hologenome theory has aspects that harken back to Lamarckian concepts. In addition to the traditionally recognized modes of variation (\"i.e.\" sexual recombination, chromosomal rearrangement, mutation), the holobiont allows for two additional mechanisms of variation that are specific to the hologenome theory: (1) changes in the relative population of existing microorganisms (\"i.e.\" amplification and reduction) and (2) acquisition of novel strains from the environment, which may be passed on to offspring.\n\nChanges in the relative population of existing microorganisms corresponds to Lamarckian \"use and disuse\", while the ability to acquire novel strains from the environment, which may be passed on to offspring, corresponds to Lamarckian \"inheritance of acquired traits\". The hologenome theory, therefore, is said by its proponents to incorporate Lamarckian aspects within a Darwinian framework.\n\nThe pea aphid \"Acyrthosiphon pisum\" maintains an obligate symbiotic relationship with the bacterium \"Buchnera aphidicola\", which is transmitted maternally to the embryos that develop within the mother's ovarioles. Pea aphids live on sap, which is rich in sugars but deficient in amino acids. They rely on their \"Buchnera\" endosymbiotic population for essential amino acids, supplying in exchange nutrients as well as a protected intracellular environment that allows \"Buchnera\" to grow and reproduce. The relationship is actually more complicated than mutual nutrition; some strains of \"Buchnera\" increases host thermotolerance, while other strains do not. Both strains are present in field populations, suggesting that under some conditions, increased heat tolerance is advantageous to the host, while under other conditions, decreased heat tolerance but increased cold tolerance may be advantageous. One can consider the variant \"Buchnera\" genomes as alleles for the larger hologenome. The association between \"Buchnera\" and aphids began about 200 million years ago, with host and symbiont co-evolving since that time; in particular, it has been discovered that genome size in various \"Buchnera\" species has become extremely reduced, in some cases down to 450 kb, which is far smaller even than the 580 kb genome of \"Mycoplasma genitalium\".\n\nDevelopment of mating preferences, \"i.e.\" sexual selection, is considered to be an early event in speciation. In 1989, Dodd reported mating preferences in \"Drosophila\" that were induced by diet. It has recently been demonstrated that when otherwise identical populations of \"Drosophila\" were switched in diet between molasses medium and starch medium, that the \"molasses flies\" preferred to mate with other molasses flies, while the \"starch flies\" preferred to mate with other starch flies. This mating preference appeared after only one generation and was maintained for at least 37 generations. The origin of these differences were changes in the flies' populations of a particular bacterial symbiont, \"Lactobacillus plantarum\". Antibiotic treatment abolished the induced mating preferences. It appears that the symbiotic bacteria changed the levels of cuticular hydrocarbon sex pheromones.\n\nZilber-Rosenberg and Rosenberg (2008) have tabulated many of the ways in which symbionts are transmitted and their contributions to the fitness of the holobiont, beginning with mitochondria found in all eukaryotes, chloroplast in plants, and then various associations described in specific systems. The microbial contributions to host fitness included provision of specific amino acids, growth at high temperatures, provision of nutritional needs from cellulose, nitrogen metabolism, recognition signals, more efficient food utilization, protection of eggs and embryos against metabolism, camouflage against predators, photosynthesis, breakdown of complex polymers, stimulation of the immune system, angiogenesis, vitamin synthesis, fiber breakdown, fat storage, supply of minerals from the soil, supply of organics, acceleration of mineralization, carbon cycling, and salt tolerance.\n\nThe hologenome theory is debated. A major criticism by Ainsworth \"et al.\" has been their claim that \"V. shiloi\" was misidentified as the causative agent of coral bleaching, and that its presence in bleached \"O. patagonica\" was simply that of opportunistic colonization.\n\nIf this is true, the original observation that led to Rosenberg's later articulation of the theory would be invalid. On the other hand, Ainsworth \"et al.\" performed their samplings in 2005, two years after the Rosenberg group discovered \"O. patagonica\" no longer to be susceptible to \"V. shiloi\" infection; therefore their finding that bacteria are not the primary cause of present-day bleaching in Mediterranean coral \"O. patagonica\" should not be considered surprising. The rigorous satisfaction of Koch's postulates, as employed in Kushmaro \"et al.\" (1997), is generally accepted as providing a definitive identification of infectious disease agents.\n\nBaird \"et al.\" (2009) have questioned basic assumptions made by Reshef \"et al.\" (2006) in presuming that (1) coral generation times are too slow to adjust to novel stresses over the observed time scales, and that (2) the scale of dispersal of coral larvae is too large to allow for adaptation to local environments. They may simply have underestimated the potential rapidity of conventional means of natural selection. In cases of severe stress, multiple cases have been documented of ecologically significant evolutionary change occurring over a handful of generations. Novel adaptive mechanisms such as switching symbionts might not be necessary for corals to adjust to rapid climate change or novel stressors.\nOrganisms in symbiotic relationships evolve to accommodate each other, and the symbiotic relationship increases the overall fitness of the participant species. Although the hologenome theory is still being debated, it has gained a significant degree of popularity within the scientific community as a way of explaining rapid adaptive changes that are difficult to accommodate within a traditional Darwinian framework.\n\nDefinitions and uses of the words holobiont and hologenome also differ between proponents and skeptics, and the misuse of the terms has led to confusions over what comprises evidence related to the hologenome. Ongoing discourse is attempting to clear this confusion. Theis et al. clarify that \"critiquing the hologenome concept is not synonymous with critiquing coevolution, and arguing that an entity is not a primary unit of selection dismisses the fact that the hologenome concept has always embraced multilevel selection.\"\n\nFor instance, Chandler and Turelli (2014) criticize the conclusions of Brucker and Bordenstein (2013), noting that their observations are also consistent with an alternative explanation. Brucker and Bordenstein (2014) responded to these criticisms, claiming they were unfounded because of factual inaccuracies and altered arguments and definitions that were not advanced by Brucker and Bordenstein (2013).\n\nRecently, Forest L Rohwer and colleagues developed a novel statistical test to examine the potential for the hologenome theory of evolution in coral species. They found that coral species do not inherit microbial communities, and are instead colonized by a core group of microbes that associate with a diversity of species. The authors conclude: \"Identification of these two symbiont communities supports the holobiont model and calls into question the hologenome theory of evolution.\" However, other studies in coral adhere to the original and pluralistic definitions of holobionts and hologenomes. David Bourne, Kathleen Morrow and Nicole Webster clarify that \"The combined genomes of this coral holobiont form a coral hologenome, and genomic interactions within the hologenome ultimately define the coral phenotype.\"\n",
    "id": "34920670",
    "title": "Hologenome theory of evolution"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=38503554",
    "text": "Robosquirrel\n\nRobosquirrel refers to several versions of robotic squirrels developed by researchers at the University of California, Davis and San Diego State University. Robosquirrel is currently in use and development in an interdisciplinary research project that uses biorobotics to investigate how communication between prey (e.g., squirrels) and predators (e.g., rattlesnakes) evolve in response to each other. It has received extensive science and popular media coverage. \nIt stirred controversy when Senator Tom Coburn listed it in his \"Wastebook 2012\" as a scientific research project that wastes United States federal tax dollars. \nSenator Coburn's release of \"Wastebook 2012\" was quickly picked up by the popular media and the robosquirrel project was the headline of many media stories.\n\nThe robosquirrel research project, in which robosquirrel is used and developed, has four aims: (1) to establish collaborations between ecologists and engineers to develop next generation robotic technology for studying predator-prey communication behavior, (2) to increase minority participation in science, (3) to develop public outreach, and (4) to support undergraduate and graduate education in biology and engineering. Currently, three versions of robosquirrel have been developed. \nIt is currently funded by the National Science Foundation (NSF) \nbased on peer review and by meeting criteria of intellectual merit and broader impacts required by NSF.\n\nThe controversy focuses on the amount of money spent ($325,000) on robosquirrel. The researchers have responded that robosquirrel only cost a few hundred dollars \nand that approximately 70% of the funds are currently spent on training future biologists and engineers.\n\nDonald Owings observed an interesting phenomenon during research in the field: when a California ground squirrel comes across a rattlesnake, it often raises its tail and waves it from side to side.\n\nIn 2007, Owings (Department of Psychology and his graduate student at the time, Aaron Rundus, began collaborating with Sanjay Joshi (Department of Mechanical/Aeronautical Engineering) at University of California, Davis. They used an infrared camera to videotape how snakes and squirrels behaviorally interact. They found that a squirrel shunts blood to its tail and thereby heats it up when encountering a rattlesnake but not for non-poisonous snakes.\n\nRattlesnakes were known to use infrared radiation to detect prey, but what they discovered suggests that the squirrels evolved the ability to communicate with rattlesnakes by heating up their tails and thereby signalling with infrared radiation.\n\nTo test this hypothesis, they built a robotic squirrel (robosquirrel) that could behaviorally respond to rattlesnakes when they moved. They found that when robosquirrel waved a heated tail in response to the movement of a rattlesnake, the snake became defensive (i.e., it moved away from the robotic squirrel) rather than predatory. If robosquirrel waved an unheated tail, then the rattlesnake continued its predatory behavior.\n\nThe next step was to determine whether squirrel behavior when rattlesnakes had not been encountered reduced the predatory behavior of rattlesnakes waiting in ambush. They video-recorded predatory behavior of rattlesnakes when ground squirrels flagged their tails. The researchers found that tail-flagging decreased the likelihood that rattlesnakes would strike and increased the likelihood that they would move away from that area.\n\nThey found that California ground squirrels are actually performing two types of tail flagging: \"vigilance advertising\" and \"perception advertising\". Vigilance advertising is hypothesized to occur when a squirrel enters an area that may contain rattlesnakes. By tail flagging, a squirrel may be signaling to a snake that it is vigilant and prepared to dodge any attacks. Perception advertising is hypothesized to occur when a squirrel encounters a rattlesnake and its tail flagging functions to signal to the snake that it is prepared to respond to attack and to warn other squirrels of the presence of rattlesnakes.\n\nTo test these hypotheses about the function of tail flagging, they built robosquirrel to record encounters with rattlesnakes. Robosquirrel was programmed to wag its tail in the presence of a snake and its tail temperature can increase above its body temperature when it encounters snakes. The robosquirrel behaviors presented to rattlesnakes in naturalistic contexts can be precisely controlled. This control of behavior is impossible with living squirrels and so robosquirrel allows researchers to investigate how rattlesnakes respond to specific behaviors.\n\nThe first robosquirrel was built using a taxidermically mounted adult female California ground squirrel. The tail was moved by a servo, which allowed robosquirrel to flag it tail at specified intervals. MATLAB software via a mini synchronous serial channel microcontroller, controlled the tail-flagging of robosquirrel.\n\nTo control the temperature of the interior of robosquirrel’s body, a coiled Nichrome 80 resistance wire was inserted and a cartridge heater was inserted into its tail. The temperature of the Nichrome wire and the cartridge heater were controlled by thermostats wired with thermocouples that were placed inside its body and tail. Power to the two heaters was regulated via the thermostats by using a proportional-integral differential (PID) control loop to regulate the duty cycle of the power relays.\n\nAn overhead camera videotaped the arena in which robosquirrel was mounted on a wooden stage beneath which were the theromostats and heater relays. A Rattlesnake was marked by attaching a small red light-emitting diode (LED) it. When a snake entered the arena, the images from the arena were analyzed by a program written in MATLAB that determined the distance between the squirrel and the LED. The program then calculated the rate of tail flagging based on the distance between the LED and robosquirrel. As the distance between the squirrel and LED on the rattlesnake decreased, robosquirrel increased its rate of tail flagging. The rates of tail flagging were based on previous field observations.\n\nThe first version of the field robosquirrel was a modification of the laboratory robosquirrel. The heating system of laboratory robosquirrel was repackaged in a toolbox for easy transportation and a Duracell Power-pack inverter provided power for the heaters and thermostats. Instead of using a desktop computer to process images and control tail flagging, an 8-b radio controller allowed a remote control to be used instead.\n\nTesting of robosquirrel was conducted at the Santa Margarita Ecological Reserve (SMER) near Fallbrook, California, operated by San Diego State University. Six red diamond rattlesnakes (\"Crotalus ruber\") were radio tagged for the study. For test trials, one observer positioned robosquirrel and controlled its tail flagging, while another observer monitored the rattlesnake’s response to robosquirrel.\n\nThe researchers, however, ran into problems. First, the outdoor robosquirrel was heavy and difficult to transport, requiring two or three people to carry it. Robosquirrel had to be moved across rough terrain and, at the same time, the researchers had to watch out for venomous snakes, tarantulas, and mountain lions. Second, rattlesnakes located themselves in a variety of ambush sites. Each site presented different challenges in setting up robosquirrel. The researchers concluded that they had to redesign robosquirrel to be less complicated to set up for each new habitat.\n\nTo redesign a more field-usable robosquirrel, custom electronics were developed to reduce bulk and weight. In addition to previous features, new features were added such as onboard cameras and data logging. To reduce weight, all power sources were placed in a small box underneath robosquirrel.\n\nThe new robosquirrel design used an ATmega128 microprocessor with 4 kB of data memory and 128 kB of program memory. The microprocessor was mounted on a custom designed, two-layer printed circuit board with a DB9 serial port for RS-232 communication and an SD card slot for data storage. The circuit board also had a Joint Test Action Group port for programming and radio control hardware. The main program ran in a loop that repeatedly checked for input from the remote control.\n\nNichrome heating elements were controlled by pulse-width modulation (PWM) of transistors. Tail flagging was controlled by a 16-b counter in fast-PWM compare-match mode connected to the same Hitec servo. For this version of robosquirrel, the tail servo was mounted inside robosquirrel, allowing more realistic tail-flagging. A premade receiver–transmitter kit Radio allowed user control of robosquirrel. A camera allowed for positioning of robosquirrel relative to a rattlesnake and for monitoring of a snake.\n\nRobosquirrel was deployed at Camp Ohlone in the Sunol Regional Park in Sunol, California. This site was selected because it has large populations of ground squirrels and rattlesnakes. Seventeen snakes were captured and radio tagged so that they could be tracked. Initial results with the new robosquirrel indicated that rattlesnakes abandon their ambush sites more quickly when robosquirrel tail-flagged than when it did not as predicted.\n\nIn 2008, Senator Tom Coburn began releasing reports of what he considers wasteful federal spending. The first report was released on December 11, 2008. On December 12, 2010, Senator Coburn released the first report entitled \"Wastebook 2010: A Guide to Some of the Most Wasteful Government Spending of 2010.” On October 16, 2012, \"Wastebook 2012\" was released listing 100 instances in which over spending is claimed (note the table of contents only lists 10). The Robosquirrel project is seventh on the list.\n\nSenator Coburn's \"Wastebook\" series has been lauded as important tool for detecting wasteful federal spending and as guide for how to cut the U.S. deficit. Critics have argued that the projects listed in the \"Wastebook\" series were not reviewed by experts before inclusion, that there are many mistakes in \"Wastebook\", and that the \"Wastebook\" series is especially critical of scientific research.\n\nSenator Tom Colburn's \"Wastebook 2012\" listed robosquirrel as an example of wasteful government spending:\n\nIn this corner with a triangular-shaped head, a rattle on its tail, with a venomous bite is the rattlesnake.\n\nAnd in the other corner, whistling and chirping, covered in fur with a bushy tail, is the squirrel.\n\nThese two critters are long-time adversaries. Squirrels are frequently preyed upon by the rattlesnakes, but the snakes rarely attack squirrels who are wagging their tales. When they do, they usually miss the fast moving squirrel.\n\nBut what happens when a snake is confronted by a robot squirrel, built to look, act, and even smell like the real thing?\n\nResearchers at San Diego State University and the University of California (Davis) spent a portion of a $325,000 National Science Foundation (NSF) grant to construct a robotic squirrel named “RoboSquirrel” to answer that question….\n\nDuring these difficult fiscal times of massive deficits, paying $325,000 for a robot squirrel seems a bit squirrelly. (pp. 20–21)\n\nThe robosquirrel project was one of 10 projects listed in his October 15, 2012 press release, but it was singled out by many major news outlets as illustrated by the titles of news articles (e.g., \"Robo-squirrel, video game on government pork list\" in USA Today).\n\nThe main response to Senator Coburn's criticism that \"During these difficult fiscal times of massive deficits, paying $325,000 for a robot squirrel seems a bit squirrelly\" is that the robosquirrel (i.e., second field version) in question cost only a few hundred dollars. Most of the funding has gone toward the education of future scientists and engineers. The science and engineering in the project furthers the advancement of biorobotics and the study of animal behavior and its evolution by building on previous research using biorobotics in animal behavior research.\n\nOn July 1, 2010 (the estimated end date is June 30, 2014), the National Science Foundation began funding a research proposal by Rulon Clark (San Diego State University) and Sanjay Joshi (University of California, Davis) entitled \"Understanding predator-prey signaling interactions: the dynamics of antisnake displays in ground squirrels and kangaroo rats.\" They proposed to investigate the function and evolution of animal communication between predators and prey using robots that can interact with predators. They also stated in their proposal that their research project would support and provide training for future scientists and engineers.\n\nThe researchers reported that they were awarded $390,000 for a 4-year study in 2010 by NSF. Their universities take about one third of the funds in indirect costs, which go for overhead, for example, to building maintenance, energy costs, and scholarships for students. This leaves approximately $260,000 over four years for the research project itself. Most of these funds go towards supporting biology and engineering students who help conduct the proposed research. They estimate that of the direct and indirect budget categories in their NSF grant, nearly $273,000 or 70% of the total funds go to training and support for students. They report that 34 students have received training so far in this research project. They also report that of the funds awarded by NSF, $13,700 has or will be spent on biorobotics over the four years of this study, which is 3.5% of the total funds.\n\n\n",
    "id": "38503554",
    "title": "Robosquirrel"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=15357916",
    "text": "The Phenomenon of Man\n\nThe Phenomenon of Man (\"Le phénomène humain\") is a 1955 book written by the French philosopher, paleontologist and Jesuit priest Pierre Teilhard de Chardin. In this work, Teilhard describes evolution as a process that leads to increasing complexity, culminating in the unification of consciousness.\n\nThe book was finished in the 1930s, but was published posthumously in 1955, and translated into English in 1959. The Roman Catholic Church initially prohibited the publication of some of Teilhard's writings on the grounds that they contradicted orthodoxy.\n\nThe foreword to the book was written by one of the key advocates for natural selection and evolution of the 20th century, and a co-developer of the modern synthesis in biology, Julian Huxley.\n\nTeilhard views evolution as a process that leads to increasing complexity. From the cell to the thinking animal, a process of psychical concentration leads to greater consciousness. The emergence of \"Homo sapiens\" marks the beginning of a new age, as the power acquired by consciousness to turn in upon itself raises humankind to a new sphere. Borrowing Huxley’s expression, Teilhard describes humankind as evolution becoming conscious of itself.\nIn Teilhard's conception of the evolution of the species, a collective identity begins to develop as trade and the transmission of ideas increases. Knowledge accumulates and is transmitted in increasing levels of depth and complexity. This leads to a further augmentation of consciousness and the emergence of a thinking layer that envelops the earth. Teilhard calls the new membrane the “noosphere” (from the Greek “\"nous\"”, meaning mind), a term first coined by Vladimir Vernadsky. The noosphere is the collective consciousness of humanity, the networks of thought and emotion in which all are immersed. \nThe development of science and technology causes an expansion of the human sphere of influence, allowing a person to be simultaneously present in every corner of the world. Teilhard argues that humanity has thus become cosmopolitan, stretching a single organized membrane over the Earth. Teilhard describes the process by which this happens as a \"gigantic psychobiological operation, a sort of mega-synthesis, the “super-arrangement” to which all the thinking elements of the earth find themselves today individually and collectively subject\". The rapid expansion of the noosphere requires a new domain of psychical expansion, which \"is staring us in the face if we would only raise our heads to look at it\".\n\nIn Teilhard’s view, evolution will culminate in the Omega Point, a sort of supreme consciousness. Layers of consciousness will converge in Omega, fusing and consuming them in itself. The concentration of a conscious universe will reassemble in itself all consciousnesses as well as all that we are conscious of. Teilhard emphasizes that each individual facet of consciousness will remain conscious of itself at the end of the process.\n\nIn 1961, the Nobel Prize-winner Peter Medawar, a British immunologist, wrote a scornful review of the book for the journal \"Mind\", calling it \"a bag of tricks\" and saying that the author had shown \"an active willingness to be deceived\": \"the greater part of it, I shall show, is nonsense, tricked out with a variety of metaphysical conceits, and its author can be excused of dishonesty only on the grounds that before deceiving others he has taken great pains to deceive himself\".\n\nThe evolutionary biologist Richard Dawkins called Medawar's review \"devastating\", and \"The Phenomenon of Man\" \"the quintessence of bad poetic science\".\n\nIn the June 1995 issue of \"Wired\", Jennifer Cobb Kreisberg said, \"Teilhard saw the Net coming more than half a century before it arrived\":\nTeilhard imagined a stage of evolution characterized by a complex membrane of information enveloping the globe and fueled by human consciousness. It sounds a little off-the-wall, until you think about the Net, that vast electronic web encircling the Earth, running point to point through a nerve-like constellation of wires.\n\nIn July 2009, during a vespers service in Aosta Cathedral in northern Italy, Pope Benedict XVI, reflecting on the Epistle to the Romans in which \"St. Paul writes that the world itself will one day become a form of living worship\", commented on Teilhard:\nIt's the great vision that later Teilhard de Chardin also had: At the end we will have a true cosmic liturgy, where the cosmos becomes a living host. Let's pray to the Lord that he help us be priests in this sense, to help in the transformation of the world in adoration of God, beginning with ourselves.\n",
    "id": "15357916",
    "title": "The Phenomenon of Man"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=19852895",
    "text": "Introduction to evolution\n\nEvolution is the process of change in all forms of life over generations, and evolutionary biology is the study of how evolution occurs. Biological populations evolve through genetic changes that correspond to changes in the organisms' observable traits. Genetic changes include mutations, which are caused by damage or replication errors in organisms' DNA. As the genetic variation of a population drifts randomly over generations, natural selection gradually leads traits to become more or less common based on the relative reproductive success of organisms with those traits.\n\nThe age of the Earth is about 4.54 billion years. The earliest undisputed evidence of life on Earth dates at least from 3.5 billion years ago, during the Eoarchean Era after geological crust started to solidify, following the earlier molten Hadean Eon. Microbial mat fossils in 3.48 billion-year-old sandstone have been found in Western Australia. Other early physical evidence of life includes graphite, a biogenic substance, in 3.7 billion-year-old metasedimentary rocks in western Greenland and, in 2015, \"remains of biotic life\" found in 4.1 billion-year-old rocks in Western Australia. According to one of the researchers, \"If life arose relatively quickly on Earth ... then it could be common in the universe.\" It is estimated that more than 99 percent of all species, amounting to over five billion species, that ever lived on Earth are extinct. Estimates on the number of Earth's current species range from 10 million to 14 million, of which about 1.2 million have been documented and over 86 percent have not yet been described. More recently, in May 2016, scientists reported that 1 trillion species are estimated to be on Earth currently with only one-thousandth of one percent described.\n\nEvolution does not attempt to explain the origin of life (covered instead by abiogenesis), but it does explain how early lifeforms evolved into the complex ecosystem that we see today. Based on the similarities between all present-day organisms, all life on Earth is assumed to have originated through common descent from a last universal ancestor from which all known species have diverged through the process of evolution. All individuals have hereditary material in the form of genes received from their parents, which they pass on to any offspring. Among offspring there are variations of genes due to the introduction of new genes via random changes called mutations or via reshuffling of existing genes during sexual reproduction. The offspring differs from the parent in minor random ways. If those differences are helpful, the offspring is more likely to survive and reproduce. This means that more offspring in the next generation will have that helpful difference and individuals will not have equal chances of reproductive success. In this way, traits that result in organisms being better adapted to their living conditions become more common in descendant populations. These differences accumulate resulting in changes within the population. This process is responsible for the many diverse life forms in the world.\n\nThe forces of evolution are most evident when populations become isolated, either through geographic distance or by other mechanisms that prevent genetic exchange. Over time, isolated populations can branch off into new species.\n\nThe majority of genetic mutations neither assist, change the appearance of, nor bring harm to individuals. Through the process of genetic drift, these mutated genes are neutrally sorted among populations and survive across generations by chance alone. In contrast to genetic drift, natural selection is not a random process because it acts on traits that are necessary for survival and reproduction. Natural selection and random genetic drift are constant and dynamic parts of life and over time this has shaped the branching structure in the tree of life.\n\nThe modern understanding of evolution began with the 1859 publication of Charles Darwin's \"On the Origin of Species\". In addition, Gregor Mendel's work with plants helped to explain the hereditary patterns of genetics. Fossil discoveries in paleontology, advances in population genetics and a global network of scientific research have provided further details into the mechanisms of evolution. Scientists now have a good understanding of the origin of new species (speciation) and have observed the speciation process in the laboratory and in the wild. Evolution is the principal scientific theory that biologists use to understand life and is used in many disciplines, including medicine, psychology, conservation biology, anthropology, forensics, agriculture and other social-cultural applications.\n\nThe main ideas of evolution may be summarized as follows:\n\n\nIn the 19th century, natural history collections and museums were popular. The European expansion and naval expeditions employed naturalists, while curators of grand museums showcased preserved and live specimens of the varieties of life. Charles Darwin was an English graduate educated and trained in the disciplines of natural history. Such natural historians would collect, catalogue, describe and study the vast collections of specimens stored and managed by curators at these museums. Darwin served as a ship's naturalist on board HMS \"Beagle\", assigned to a five-year research expedition around the world. During his voyage, he observed and collected an abundance of organisms, being very interested in the diverse forms of life along the coasts of South America and the neighboring Galápagos Islands.\n\nDarwin gained extensive experience as he collected and studied the natural history of life forms from distant places. Through his studies, he formulated the idea that each species had developed from ancestors with similar features. In 1838, he described how a process he called natural selection would make this happen.\n\nThe size of a population depends on how much and how many resources are able to support it. For the population to remain the same size year after year, there must be an equilibrium, or balance between the population size and available resources. Since organisms produce more offspring than their environment can support, not all individuals can survive out of each generation. There must be a competitive struggle for resources that aid in survival. As a result, Darwin realised that it was not chance alone that determined survival. Instead, survival of an organism depends on the differences of each individual organism, or \"traits,\" that aid or hinder survival and reproduction. Well-adapted individuals are likely to leave more offspring than their less well-adapted competitors. Traits that hinder survival and reproduction would \"disappear\" over generations. Traits that help an organism survive and reproduce would \"accumulate\" over generations. Darwin realised that the unequal ability of individuals to survive and reproduce could cause gradual changes in the population and used the term \"natural selection\" to describe this process.\n\nObservations of variations in animals and plants formed the basis of the theory of natural selection. For example, Darwin observed that orchids and insects have a close relationship that allows the pollination of the plants. He noted that orchids have a variety of structures that attract insects, so that pollen from the flowers gets stuck to the insects' bodies. In this way, insects transport the pollen from a male to a female orchid. In spite of the elaborate appearance of orchids, these specialised parts are made from the same basic structures that make up other flowers. In his book, \"Fertilisation of Orchids\" (1862), Darwin proposed that the orchid flowers were adapted from pre-existing parts, through natural selection.\n\nDarwin was still researching and experimenting with his ideas on natural selection when he received a letter from Alfred Russel Wallace describing a theory very similar to his own. This led to an immediate joint publication of both theories. Both Wallace and Darwin saw the history of life like a family tree, with each fork in the tree’s limbs being a common ancestor. The tips of the limbs represented modern species and the branches represented the common ancestors that are shared amongst many different species. To explain these relationships, Darwin said that all living things were related, and this meant that all life must be descended from a few forms, or even from a single common ancestor. He called this process \"descent with modification\".\n\nDarwin published his theory of evolution by natural selection in \"On the Origin of Species\" in 1859. His theory means that all life, including humanity, is a product of continuing natural processes. The implication that all life on Earth has a common ancestor has met with objections from some religious groups. Their objections are in contrast to the level of support for the theory by more than 99 percent of those within the scientific community today.\n\nNatural selection is commonly equated with \"survival of the fittest\", but this expression originated in Herbert Spencer's \"Principles of Biology\" in 1864, five years after Charles Darwin published his original works. \"Survival of the fittest\" describes the process of natural selection incorrectly, because natural selection is not only about survival and it is not always the fittest that survives.\n\nDarwins theory of natural selection laid the groundwork for modern evolutionary theory, and his experiments and observations showed that the organisms in populations varied from each other, that some of these variations were inherited, and that these differences could be acted on by natural selection. However, he could not explain the source of these variations. Like many of his predecessors, Darwin mistakenly thought that heritable traits were a product of use and disuse, and that features acquired during an organism's lifetime could be passed on to its offspring. He looked for examples, such as large ground feeding birds getting stronger legs through exercise, and weaker wings from not flying until, like the ostrich, they could not fly at all. This misunderstanding was called the inheritance of acquired characters and was part of the theory of transmutation of species put forward in 1809 by Jean-Baptiste Lamarck. In the late 19th century this theory became known as Lamarckism. Darwin produced an unsuccessful theory he called pangenesis to try to explain how acquired characteristics could be inherited. In the 1880s August Weismann's experiments indicated that changes from use and disuse could not be inherited, and Lamarckism gradually fell from favor.\n\nThe missing information needed to help explain how new features could pass from a parent to its offspring was provided by the pioneering genetics work of Gregor Mendel. Mendel's experiments with several generations of pea plants demonstrated that inheritance works by separating and reshuffling hereditary information during the formation of sex cells and recombining that information during fertilisation. This is like mixing different hands of playing cards, with an organism getting a random mix of half of the cards from one parent, and half of the cards from the other. Mendel called the information \"factors\"; however, they later became known as genes. Genes are the basic units of heredity in living organisms. They contain the information that directs the physical development and behavior of organisms.\n\nGenes are made of DNA. DNA is a long molecule made up of individual molecules called nucleotides. Genetic information is encoded in the sequence of nucleotides, that make up the DNA, just as the sequence of the letters in words carries information on a page. The genes are like short instructions built up of the \"letters\" of the DNA alphabet. Put together, the entire set of these genes gives enough information to serve as an \"instruction manual\" of how to build and run an organism. The instructions spelled out by this DNA alphabet can be changed, however, by mutations, and this may alter the instructions carried within the genes. Within the cell, the genes are carried in chromosomes, which are packages for carrying the DNA. It is the reshuffling of the chromosomes that results in unique combinations of genes in offspring. Since genes interact with one another during the development of an organism, novel combinations of genes produced by sexual reproduction can increase the genetic variability of the population even without new mutations. The genetic variability of a population can also increase when members of that population interbreed with individuals from a different population causing gene flow between the populations. This can introduce genes into a population that were not present before.\n\nEvolution is not a random process. Although mutations in DNA are random, natural selection is not a process of chance: the environment determines the probability of reproductive success. Evolution is an inevitable result of imperfectly copying, self-replicating organisms reproducing over billions of years under the selective pressure of the environment. The outcome of evolution is not a perfectly designed organism. The end products of natural selection are organisms that are adapted to their present environments. Natural selection does not involve progress towards an ultimate goal. Evolution does not strive for more advanced, more intelligent, or more sophisticated life forms. For example, fleas (wingless parasites) are descended from a winged, ancestral scorpionfly, and snakes are lizards that no longer require limbs—although pythons still grow tiny structures that are the remains of their ancestor's hind legs. Organisms are merely the outcome of variations that succeed or fail, dependent upon the environmental conditions at the time.\n\nRapid environmental changes typically cause extinctions. Of all species that have existed on Earth, 99.9 percent are now extinct. Since life began on Earth, five major mass extinctions have led to large and sudden drops in the variety of species. The most recent, the Cretaceous–Paleogene extinction event, occurred 66 million years ago.\n\nGenetic drift is a cause of allelic frequency change within populations of a species. Alleles are different variations of specific genes. They determine things like hair color, skin tone, eye color and blood type; in other words, all the genetic traits that vary between individuals. Genetic drift does not introduce new alleles to a population, but it can reduce variation within a population by removing an allele from the gene pool. Genetic drift is caused by random sampling of alleles. A truly random sample is a sample in which no outside forces affect what is selected. It is like pulling marbles of the same size and weight but of different colors from a brown paper bag. In any offspring, the alleles present are samples of the previous generations alleles, and chance plays a role in whether an individual survives to reproduce and to pass a sample of their generation onward to the next. The allelic frequency of a population is the ratio of the copies of one specific allele that share the same form compared to the number of all forms of the allele present in the population.\n\nGenetic drift affects smaller populations more than it affects larger populations.\n\nThe Hardy–Weinberg principle states that a large population in Hardy–Weinberg equilibrium will have no change in the frequency of alleles as generations pass. It is impossible for a population of any considerable size to reach this equilibrium because of the five requirements that must be met. A population must be infinite in size. There must be a zero percent mutation rate between generations, because mutations can alter existing alleles or create new ones. There can be no immigration or emigration in the population, because individuals arriving and leaving directly change allelic frequencies. There can be no selective pressures of any kind on the population, meaning that no individual is more likely than any other to survive and reproduce. Finally, mating must be totally random, with all males (or females in some cases) being equally desirable mates. This ensures a true random mixing of alleles.\n\nA population that is in Hardy–Weinberg equilibrium is analogous to a deck of cards; no matter how many times the deck is shuffled, no new cards are added and no old ones are taken away. Cards in the deck represent alleles in a population’s gene pool.\n\nA population bottleneck occurs when the population of a species is reduced drastically over a short period of time due to external forces. In a true population bottleneck, the reduction does not favor any combination of alleles; it is totally random chance which individuals survive. A bottleneck can reduce or eliminate genetic variation from a population. Further drift events after the bottleneck event can also reduce the population's genetic diversity. The lack of diversity created can make the population at risk to other selective pressures.\n\nA common example of a population bottleneck is the Northern elephant seal. Due to excessive hunting throughout the 19th century, the population of the northern elephant seal was reduced to 30 individuals or less. They have made a full recovery, with the total number of individuals at around 100,000 and growing. The effects of the bottleneck are visible, however. The seals are more likely to have serious problems with disease or genetic disorders, because there is almost no diversity in the population.\n\nThe founder effect occurs when a small group from one population splits off and forms a new population, often through geographic isolation. This new population's allelic frequency is probably different from the original population's, and will change how common certain alleles are in the populations. The founders of the population will determine the genetic makeup, and potentially the survival, of the new population for generations.\n\nOne example of the founder effect is found in the Amish migration to Pennsylvania in 1744. Two of the founders of the colony in Pennsylvania carried the recessive allele for Ellis–van Creveld syndrome. Because the Amish tend to be religious isolates, they interbreed, and through generations of this practice the frequency of Ellis–van Creveld syndrome in the Amish people is much higher than the frequency in the general population.\n\nThe modern evolutionary synthesis is based on the concept that populations of organisms have significant genetic variation caused by mutation and by the recombination of genes during sexual reproduction. It defines evolution as the change in allelic frequencies within a population caused by genetic drift, gene flow between sub populations, and natural selection. Natural selection is emphasised as the most important mechanism of evolution; large changes are the result of the gradual accumulation of small changes over long periods of time.\n\nThe modern evolutionary synthesis is the outcome of a merger of several different scientific fields to produce a more cohesive understanding of evolutionary theory. In the 1920s, Ronald Fisher, J.B.S. Haldane and Sewall Wright combined Darwin's theory of natural selection with statistical models of Mendelian genetics, founding the discipline of population genetics. In the 1930s and 1940s, efforts were made to merge population genetics, the observations of field naturalists on the distribution of species and sub species, and analysis of the fossil record into a unified explanatory model. The application of the principles of genetics to naturally occurring populations, by scientists such as Theodosius Dobzhansky and Ernst Mayr, advanced the understanding of the processes of evolution. Dobzhansky's 1937 work \"Genetics and the Origin of Species\" helped bridge the gap between genetics and field biology by presenting the mathematical work of the population geneticists in a form more useful to field biologists, and by showing that wild populations had much more genetic variability with geographically isolated subspecies and reservoirs of genetic diversity in recessive genes than the models of the early population geneticists had allowed for. Mayr, on the basis of an understanding of genes and direct observations of evolutionary processes from field research, introduced the biological species concept, which defined a species as a group of interbreeding or potentially interbreeding populations that are reproductively isolated from all other populations. Both Dobzhansky and Mayr emphasised the importance of subspecies reproductively isolated by geographical barriers in the emergence of new species. The paleontologist George Gaylord Simpson helped to incorporate paleontology with a statistical analysis of the fossil record that showed a pattern consistent with the branching and non-directional pathway of evolution of organisms predicted by the modern synthesis. \n\nScientific evidence for evolution comes from many aspects of biology and includes fossils, homologous structures, and molecular similarities between species' DNA.\n\nResearch in the field of paleontology, the study of fossils, supports the idea that all living organisms are related. Fossils provide evidence that accumulated changes in organisms over long periods of time have led to the diverse forms of life we see today. A fossil itself reveals the organism's structure and the relationships between present and extinct species, allowing paleontologists to construct a family tree for all of the life forms on Earth.\n\nModern paleontology began with the work of Georges Cuvier. Cuvier noted that, in sedimentary rock, each layer contained a specific group of fossils. The deeper layers, which he proposed to be older, contained simpler life forms. He noted that many forms of life from the past are no longer present today. One of Cuvier’s successful contributions to the understanding of the fossil record was establishing extinction as a fact. In an attempt to explain extinction, Cuvier proposed the idea of \"revolutions\" or catastrophism in which he speculated that geological catastrophes had occurred throughout the Earth’s history, wiping out large numbers of species. Cuvier's theory of revolutions was later replaced by uniformitarian theories, notably those of James Hutton and Charles Lyell who proposed that the Earth’s geological changes were gradual and consistent. However, current evidence in the fossil record supports the concept of mass extinctions. As a result, the general idea of catastrophism has re-emerged as a valid hypothesis for at least some of the rapid changes in life forms that appear in the fossil records.\n\nA very large number of fossils have now been discovered and identified. These fossils serve as a chronological record of evolution. The fossil record provides examples of transitional species that demonstrate ancestral links between past and present life forms. One such transitional fossil is \"Archaeopteryx\", an ancient organism that had the distinct characteristics of a reptile (such as a long, bony tail and conical teeth) yet also had characteristics of birds (such as feathers and a wishbone). The implication from such a find is that modern reptiles and birds arose from a common ancestor.\n\nThe comparison of similarities between organisms of their form or appearance of parts, called their morphology, has long been a way to classify life into closely related groups. This can be done by comparing the structure of adult organisms in different species or by comparing the patterns of how cells grow, divide and even migrate during an organism's development.\n\nTaxonomy is the branch of biology that names and classifies all living things. Scientists use morphological and genetic similarities to assist them in categorising life forms based on ancestral relationships. For example, orangutans, gorillas, chimpanzees, and humans all belong to the same taxonomic grouping referred to as a family—in this case the family called Hominidae. These animals are grouped together because of similarities in morphology that come from common ancestry (called \"homology\").\nStrong evidence for evolution comes from the analysis of homologous structures: structures in different species that no longer perform the same task but which share a similar structure. Such is the case of the forelimbs of mammals. The forelimbs of a human, cat, whale, and bat all have strikingly similar bone structures. However, each of these four species' forelimbs performs a different task. The same bones that construct a bat's wings, which are used for flight, also construct a whale's flippers, which are used for swimming. Such a \"design\" makes little sense if they are unrelated and uniquely constructed for their particular tasks. The theory of evolution explains these homologous structures: all four animals shared a common ancestor, and each has undergone change over many generations. These changes in structure have produced forelimbs adapted for different tasks.\nHowever, anatomical comparisons can be misleading, as not all anatomical similarities indicate a close relationship. Organisms that share similar environments will often develop similar physical features, a process known as \"convergent evolution\". Both sharks and dolphins have similar body forms, yet are only distantly related—sharks are fish and dolphins are mammals. Such similarities are a result of both populations being exposed to the same selective pressures. Within both groups, changes that aid swimming have been favored. Thus, over time, they developed similar appearances (morphology), even though they are not closely related.\n\nIn some cases, anatomical comparison of structures in the embryos of two or more species provides evidence for a shared ancestor that may not be obvious in the adult forms. As the embryo develops, these homologies can be lost to view, and the structures can take on different functions. Part of the basis of classifying the vertebrate group (which includes humans), is the presence of a tail (extending beyond the anus) and pharyngeal slits. Both structures appear during some stage of embryonic development but are not always obvious in the adult form.\n\nBecause of the morphological similarities present in embryos of different species during development, it was once assumed that organisms re-enact their evolutionary history as an embryo. It was thought that human embryos passed through an amphibian then a reptilian stage before completing their development as mammals. Such a reenactment, often called \"recapitulation theory\", is not supported by scientific evidence. What does occur, however, is that the first stages of development are similar in broad groups of organisms. At very early stages, for instance, all vertebrates appear extremely similar, but do not exactly resemble any ancestral species. As development continues, specific features emerge from this basic pattern.\n\nHomology includes a unique group of shared structures referred to as \"vestigial structures\". \"Vestigial\" refers to anatomical parts that are of minimal, if any, value to the organism that possesses them. These apparently illogical structures are remnants of organs that played an important role in ancestral forms. Such is the case in whales, which have small vestigial bones that appear to be remnants of the leg bones of their ancestors which walked on land. Humans also have vestigial structures, including the ear muscles, the wisdom teeth, the appendix, the tail bone, body hair (including goose bumps), and the semilunar fold in the corner of the eye.\n\nBiogeography is the study of the geographical distribution of species. Evidence from biogeography, especially from the biogeography of oceanic islands, played a key role in convincing both Darwin and Alfred Russel Wallace that species evolved with a branching pattern of common descent. Islands often contain endemic species, species not found anywhere else, but those species are often related to species found on the nearest continent. Furthermore, islands often contain clusters of closely related species that have very different ecological niches, that is have different ways of making a living in the environment. Such clusters form through a process of adaptive radiation where a single ancestral species colonises an island that has a variety of open ecological niches and then diversifies by evolving into different species adapted to fill those empty niches. Well-studied examples include Darwin's finches, a group of 13 finch species endemic to the Galápagos Islands, and the Hawaiian honeycreepers, a group of birds that once, before extinctions caused by humans, numbered 60 species filling diverse ecological roles, all descended from a single finch like ancestor that arrived on the Hawaiian Islands some 4 million years ago. Another example is the Silversword alliance, a group of perennial plant species, also endemic to the Hawaiian Islands, that inhabit a variety of habitats and come in a variety of shapes and sizes that include trees, shrubs, and ground hugging mats, but which can be hybridised with one another and with certain tarweed species found on the west coast of North America; it appears that one of those tarweeds colonised Hawaii in the past, and gave rise to the entire Silversword alliance.\nEvery living organism (with the possible exception of RNA viruses) contains molecules of DNA, which carries genetic information. Genes are the pieces of DNA that carry this information, and they influence the properties of an organism. Genes determine an individual's general appearance and to some extent their behavior. If two organisms are closely related, their DNA will be very similar. On the other hand, the more distantly related two organisms are, the more differences they will have. For example, brothers are closely related and have very similar DNA, while cousins share a more distant relationship and have far more differences in their DNA. Similarities in DNA are used to determine the relationships between species in much the same manner as they are used to show relationships between individuals. For example, comparing chimpanzees with gorillas and humans shows that there is as much as a 96 percent similarity between the DNA of humans and chimps. Comparisons of DNA indicate that humans and chimpanzees are more closely related to each other than either species is to gorillas.\n\nThe field of molecular systematics focuses on measuring the similarities in these molecules and using this information to work out how different types of organisms are related through evolution. These comparisons have allowed biologists to build a \"relationship tree\" of the evolution of life on Earth. They have even allowed scientists to unravel the relationships between organisms whose common ancestors lived such a long time ago that no real similarities remain in the appearance of the organisms.\n\n\"Artificial selection\" is the controlled breeding of domestic plants and animals. Humans determine which animal or plant will reproduce and which of the offspring will survive; thus, they determine which genes will be passed on to future generations. The process of artificial selection has had a significant impact on the evolution of domestic animals. For example, people have produced different types of dogs by controlled breeding. The differences in size between the Chihuahua and the Great Dane are the result of artificial selection. Despite their dramatically different physical appearance, they and all other dogs evolved from a few wolves domesticated by humans in what is now China less than 15,000 years ago.\n\nArtificial selection has produced a wide variety of plants. In the case of maize (corn), recent genetic evidence suggests that domestication occurred 10,000 years ago in central Mexico. Prior to domestication, the edible portion of the wild form was small and difficult to collect. Today \"The Maize Genetics Cooperation • Stock Center\" maintains a collection of more than 10,000 genetic variations of maize that have arisen by random mutations and chromosomal variations from the original wild type.\n\nIn artificial selection the new breed or variety that emerges is the one with random mutations attractive to humans, while in natural selection the surviving species is the one with random mutations useful to it in its non-human environment. In both natural and artificial selection the variations are a result of random mutations, and the underlying genetic processes are essentially the same. Darwin carefully observed the outcomes of artificial selection in animals and plants to form many of his arguments in support of natural selection. Much of his book \"On the Origin of Species\" was based on these observations of the many varieties of domestic pigeons arising from artificial selection. Darwin proposed that if humans could achieve dramatic changes in domestic animals in short periods, then natural selection, given millions of years, could produce the differences seen in living things today.\n\nCoevolution is a process in which two or more species influence the evolution of each other. All organisms are influenced by life around them; however, in coevolution there is evidence that genetically determined traits in each species directly resulted from the interaction between the two organisms.\n\nAn extensively documented case of coevolution is the relationship between \"Pseudomyrmex\", a type of ant, and the \"acacia\", a plant that the ant uses for food and shelter. The relationship between the two is so intimate that it has led to the evolution of special structures and behaviors in both organisms. The ant defends the acacia against herbivores and clears the forest floor of the seeds from competing plants. In response, the plant has evolved swollen thorns that the ants use as shelter and special flower parts that the ants eat.\nSuch coevolution does not imply that the ants and the tree choose to behave in an altruistic manner. Rather, across a population small genetic changes in both ant and tree benefited each. The benefit gave a slightly higher chance of the characteristic being passed on to the next generation. Over time, successive mutations created the relationship we observe today.\n\nGiven the right circumstances, and enough time, evolution leads to the emergence of new species. Scientists have struggled to find a precise and all-inclusive definition of \"species\". Ernst Mayr defined a species as a population or group of populations whose members have the potential to interbreed naturally with one another to produce viable, fertile offspring. (The members of a species cannot produce viable, fertile offspring with members of \"other\" species). Mayr's definition has gained wide acceptance among biologists, but does not apply to organisms such as bacteria, which reproduce asexually.\n\nSpeciation is the lineage-splitting event that results in two separate species forming from a single common ancestral population. A widely accepted method of speciation is called \"allopatric speciation\". Allopatric speciation begins when a population becomes geographically separated. Geological processes, such as the emergence of mountain ranges, the formation of canyons, or the flooding of land bridges by changes in sea level may result in separate populations. For speciation to occur, separation must be substantial, so that genetic exchange between the two populations is completely disrupted. In their separate environments, the genetically isolated groups follow their own unique evolutionary pathways. Each group will accumulate different mutations as well as be subjected to different selective pressures. The accumulated genetic changes may result in separated populations that can no longer interbreed if they are reunited. Barriers that prevent interbreeding are either \"prezygotic\" (prevent mating or fertilisation) or \"postzygotic\" (barriers that occur after fertilisation). If interbreeding is no longer possible, then they will be considered different species. The result of four billion years of evolution is the diversity of life around us, with an estimated 1.75 million different species in existence today.\n\nUsually the process of speciation is slow, occurring over very long time spans; thus direct observations within human life-spans are rare. However speciation has been observed in present-day organisms, and past speciation events are recorded in fossils. Scientists have documented the formation of five new species of cichlid fishes from a single common ancestor that was isolated fewer than 5,000 years ago from the parent stock in Lake Nagubago. The evidence for speciation in this case was morphology (physical appearance) and lack of natural interbreeding. These fish have complex mating rituals and a variety of colorations; the slight modifications introduced in the new species have changed the mate selection process and the five forms that arose could not be convinced to interbreed.\n\nThe theory of evolution is widely accepted among the scientific community, serving to link the diverse specialty areas of biology. Evolution provides the field of biology with a solid scientific base. The significance of evolutionary theory is summarised by Theodosius Dobzhansky as \"nothing in biology makes sense except in the light of evolution.\" Nevertheless, the theory of evolution is not static. There is much discussion within the scientific community concerning the mechanisms behind the evolutionary process. For example, the rate at which evolution occurs is still under discussion. In addition, there are conflicting opinions as to which is the primary unit of evolutionary change—the organism or the gene.\n\nDarwin and his contemporaries viewed evolution as a slow and gradual process. Evolutionary trees are based on the idea that profound differences in species are the result of many small changes that accumulate over long periods.\n\nGradualism had its basis in the works of the geologists James Hutton and Charles Lyell. Hutton's view suggests that profound geological change was the cumulative product of a relatively slow continuing operation of processes which can still be seen in operation today, as opposed to catastrophism which promoted the idea that sudden changes had causes which can no longer be seen at work. A uniformitarian perspective was adopted for biological changes. Such a view can seem to contradict the fossil record, which often shows evidence of new species appearing suddenly, then persisting in that form for long periods. In the 1970s paleontologists Niles Eldredge and Stephen Jay Gould developed a theoretical model that suggests that evolution, although a slow process in human terms, undergoes periods of relatively rapid change (ranging between 50,000 and 100,000 years) alternating with long periods of relative stability. Their theory is called \"punctuated equilibrium\" and explains the fossil record without contradicting Darwin's ideas.\n\nA common unit of selection in evolution is the organism. Natural selection occurs when the reproductive success of an individual is improved or reduced by an inherited characteristic, and reproductive success is measured by the number of an individual's surviving offspring. The organism view has been challenged by a variety of biologists as well as philosophers. Richard Dawkins proposes that much insight can be gained if we look at evolution from the gene's point of view; that is, that natural selection operates as an evolutionary mechanism on genes as well as organisms. In his 1976 book, \"The Selfish Gene\", he explains:\nOthers view selection working on many levels, not just at a single level of organism or gene; for example, Stephen Jay Gould called for a hierarchical perspective on selection.\n\n\n\n",
    "id": "19852895",
    "title": "Introduction to evolution"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=9670",
    "text": "Evolutionism\n\nEvolutionism was a common 19th century belief that organisms inherently improve themselves through progressive inherited change over time (orthogenesis), and increase in complexity through evolution. The belief went on to include cultural evolution and social evolution. In the 1970s the term Neo-Evolutionism was used to describe the idea \"that human beings sought to preserve a familiar style of life unless change was forced on them by factors that were beyond their control\".\n\nThe term is sometimes also colloquially used to refer to the modern synthesis, a scientific theory that describes how biological evolution occurs through natural selection and Mendelian genetics. In addition, the term is used in a broader sense to cover a world-view on a wide variety of topics, including chemical evolution as an alternative term for abiogenesis or for nucleosynthesis of chemical elements, galaxy formation and evolution, stellar evolution, spiritual evolution, technological evolution and universal evolution, which seeks to explain every aspect of the world in which we live.\n\nSince the overwhelming majority of scientists accept evolution as the best explanation of current data, the term is seldom used in the scientific community; to say someone is a biologist implies acceptance of evolutionary views, unless specifically noted otherwise. In the creationevolution controversy, creationists often call those who accept the validity of the modern evolutionary synthesis \"evolutionists\" and the theory itself \"evolutionism.\" Some creationists and creationist organizations, such as the Institute of Creation Research, use these terms in an effort to make it appear that evolutionary biology is a form of secular religion.\n\nEvolution originally was used to refer to an orderly sequence of events with the outcome somehow contained at the start. Darwin did not use the term in \"Origin of Species\" until its sixth edition in 1872, (though earlier editions did use the word \"evolved\") by which time Herbert Spencer had given it scientific currency with a broad definition of progression in complexity (orthogenesis) in 1862. Edward B. Tylor and Lewis H Morgan brought the term \"evolution\" to anthropology though they tended toward the older pre-Spencerian definition helping to form the concept of unilineal (social) evolution used during the later part of what Trigger calls the Antiquarianism-Imperial Synthesis period (c1770-c1900).\n\nIn modern times, the term \"evolution\" is widely used, but the terms \"evolutionism\" and \"evolutionist\" are seldom used in the scientific community to refer to the biological discipline as the term is considered both redundant and anachronistic, though it has been used by creationists in discussing the creation-evolution controversy.\n\nThe Institute for Creation Research, in order to imply placement of evolution in the category of religions, including atheism, fascism, humanism and occultism, commonly uses the words \"evolutionism\" and \"evolutionist\" to describe the consensus of mainstream science and the scientists subscribing to it, thus implying through language that the issue is a matter of religious belief.\n\nThe BioLogos Foundation, an organization that promotes the idea of theistic evolution, uses the term \"evolutionism\" to describe \"the atheistic worldview that so often accompanies the acceptance of biological evolution in public discourse.\" It views this as a subset of scientism.\n\n\n",
    "id": "9670",
    "title": "Evolutionism"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=8787159",
    "text": "Objections to evolution\n\nObjections to evolution have been raised since evolutionary ideas came to prominence in the 19th century. When Charles Darwin published his 1859 book \"On the Origin of Species\", his theory of evolution (the idea that species arose through descent with modification from a single common ancestor in a process driven by natural selection) initially met opposition from scientists with different theories, but eventually came to receive overwhelming acceptance in the scientific community. The observation of evolutionary processes occurring (as well as the modern evolutionary synthesis explaining that evidence) has been uncontroversial among mainstream biologists since the 1940s.\n\nSince then, most criticisms and denials of evolution have come from religious groups, rather than from the scientific community. Although many religious groups have found reconciliation of their beliefs with evolution, such as through theistic evolution, other religious groups continue to reject evolutionary explanations in favor of creationism, the belief that the universe and life were created by supernatural forces. The U.S.-centered creation–evolution controversy has become a focal point of perceived conflict between religion and science.\n\nSeveral branches of creationism, including creation science, neo-creationism, and intelligent design, argue that the idea of life being directly designed by a god or intelligence is at least as scientific as evolutionary theory, and should therefore be taught in public education. Such arguments against evolution have become widespread and include objections to evolution's evidence, methodology, plausibility, morality, and scientific acceptance. The scientific community does not recognize such objections as valid, pointing to detractors' misinterpretations of such things as the scientific method, evidence, and basic physical laws.\n\nEvolutionary ideas came to prominence in the early 19th century with the theory of the transmutation of species put forward by Jean-Baptiste Lamarck. Evolution was at first opposed among the scientific community, notably by Georges Cuvier, and religious grounds. The idea that laws control nature and society gained vast popular audiences with George Combe's \"The Constitution of Man\" of 1828 and the anonymous \"Vestiges of the Natural History of Creation\" of 1844. When Charles Darwin published his 1859 book \"On the Origin of Species\", he convinced most of the scientific community that new species arise through descent through modification in a branching pattern of divergence from common ancestors, but while most scientists accepted that natural selection is a valid and empirically testable hypothesis, Darwin's view that it is the primary mechanism of evolution was rejected by some.\n\nDarwin's contemporaries eventually came to accept the transmutation of species based upon fossil evidence, and the X Club was formed to defend evolution against the church and wealthy amateurs. At that time the specific evolutionary mechanism which Darwin provided of natural selection was actively disputed by scientists in favour of alternative theories such as Lamarckism and orthogenesis. Darwin's gradualistic account was also opposed by saltationism and catastrophism. Lord Kelvin led scientific opposition to gradualism on the basis of his thermodynamic calculations that the Earth was between 24 and 400 million years old, and own views favoured a version of theistic evolution accelerated by divine guidance. This age of the earth was disputed by geological estimates, which gained strength in 1907 when radioactive dating of rocks showed that the Earth was billions of years old. The specific hereditary mechanism Darwin hypothesized of pangenesis that supported gradualism also lacked any supporting evidence and was disputed by the empirical tests of Francis Galton. Although evolution was unchallenged, uncertainties about the mechanism in the eclipse of Darwinism persisted from the 1880s until the 1930s inclusion of Mendelian inheritance and the rise of the modern evolutionary synthesis. The modern synthesis rose to universal acceptance among biologists with the help of new evidence, such as genetics, which confirmed Darwin's predictions and refuted the competing theories.\n\nProtestantism, especially in America, broke out in \"acrid polemics\" and argument about evolution from 1860 to the 1870s—with the turning point possibly marked by the death of Louis Agassiz in 1873—and by 1880 a form of \"Christian evolution\" was becoming the consensus. In Britain, while publication of \"The Descent of Man\" by Darwin in 1871 reinvigorated debate from the previous decade, Sir Henry Chadwick notes a steady acceptance of evolution \"among more educated Christians\" between 1860 and 1885. As a result, evolutionary theory was \"both permissible and respectable\" by 1876. Frederick Temple's lectures on \"The Relations between Religion and Science\" (1884) on how evolution was not \"antagonistic\" to religion highlighted this trend. Temple's appointment as Archbishop of Canterbury in 1896 demonstrated the broad acceptance of evolution within the church hierarchy.\n\nFor decades the Roman Catholic Church avoided official refutation of evolution. However, it would rein in Catholics who proposed that evolution could be reconciled with the Bible, as this conflicted with the First Vatican Council's (1869–70) finding that everything was created out of nothing by God, and to deny that finding could lead to excommunication. In 1950, the encyclical \"Humani generis\" of Pope Pius XII first mentioned evolution directly and officially. It allowed one to enquire into the concept of humans coming from pre-existing living matter, but not to question Adam and Eve or the creation of the soul. In 1996, Pope John Paul II said that evolution is \"more than a hypothesis\" and acknowledged the large body of work accumulated in its support, but reiterated that any attempt to give a material explanation of the human soul is \"incompatible with the truth about man.\" Pope Benedict XVI has reiterated the conviction that human beings \"are not some casual and meaningless product of evolution. Each of us is the result of a thought of God. Each of us is willed, each of us is loved, each of us is necessary.\" At the same time, he has promoted the study of the relationship between the concepts of creation and evolution, based on the conviction that there cannot be a contradiction between faith and reason. Along these lines, the research project Thomistic Evolution, run by a team of Dominican scholars, endeavours to reconcile the scientific evidence on evolution with the teaching of Thomas Aquinas.\n\nMuslim reaction ranged from those believing in literal creation from the Quran to many educated Muslims who subscribed to a version of theistic or guided evolution in which the Quran reinforced rather than contradicted mainstream science. This occurred relatively early, as medieval madrasahs taught the ideas of Al-Jahiz, a Muslim scholar from the 9th century, who proposed concepts similar to natural selection. However, acceptance of evolution remains low in the Muslim world, as prominent figures reject evolution's underpinning philosophy of materialism as unsound to human origins and a denial of Allah. Further objections by Muslim authors and writers largely reflect those put forward in the Western world.\n\nRegardless of acceptance from major religious hierarchies, early religious objections to Darwin's theory are still used in opposition to evolution. The ideas that species change over time through natural processes and that different species share common ancestors seemed to contradict the Genesis account of Creation. Believers in Biblical infallibility attacked Darwinism as heretical. The natural theology of the early 19th century was typified by William Paley's watchmaker analogy, an argument from design still used by the creationist movement. Natural theology included a range of ideas and arguments from the outset, and when Darwin's theory was published, ideas of theistic evolution were presented in which evolution is accepted as a secondary cause open to scientific investigation, while still holding belief in God as a first cause with a non-specified role in guiding evolution and creating humans. This position has been adopted by denominations of Christianity and Judaism in line with modernist theology which views the Bible and Torah as allegorical, thus removing the conflict between evolution and religion.\n\nHowever, in the 1920s Christian fundamentalists in the United States developed their literalist arguments against modernist theology into opposition to the teaching of evolution, with fears that Darwinism had led to German militarism and was a threat to religion and morality. This opposition developed into the creation–evolution controversy involving Christian literalists in the United States objecting to the teaching of evolution in public schools. Although early objectors dismissed evolution as contradicting their interpretation of the Bible, this argument was legally invalidated when the Supreme Court ruled in \"Epperson v. Arkansas\" in 1968 that forbidding the teaching of evolution on religious grounds violated the Establishment Clause.\n\nSince then creationists have developed more nuanced objections to evolution, alleging variously that it is unscientific, infringes on creationists' religious freedoms, or that the acceptance of evolution is a religious stance. Creationists have appealed to democratic principles of fairness, arguing that evolution is controversial and that science classrooms should therefore \"Teach the Controversy.\" These objections to evolution culminated in the intelligent design movement in the 1990s and early 2000s that unsuccessfully attempted to present itself as a scientific alternative to evolution.\n\nOne of the main sources of confusion and ambiguity in the creation–evolution debate is the definition of \"evolution\" itself. In the context of biology, evolution is genetic changes in populations of organisms over successive generations. The word also has a number of different meanings in different fields, from evolutionary computation to molecular evolution to sociocultural evolution to stellar and galactic evolution. When biological evolution is conflated with other evolutionary processes, this can cause errors such as the claim that modern evolutionary theory says anything about abiogenesis or the Big Bang.\n\n\"Evolution\" in colloquial contexts can refer to any sort of progressive development or gradual improvement, and a process that results in greater quality or complexity. When misapplied to biological evolution this common meaning leads to frequent misunderstandings. For example, the idea of devolution (\"backwards\" evolution) is a result of erroneously assuming that evolution is directional or has a specific goal in mind (cf. orthogenesis). In reality, the evolution of an organism has no \"objective\" and is only showing increasing ability of successive generations to survive and reproduce in its environment; and increased suitability is only defined in relation to this environment. Biologists do not consider any one species, such as humans, to be more \"highly evolved\" or \"advanced\" than another. Certain sources have been criticized for indicating otherwise due to a tendency to evaluate nonhuman organisms according to anthropocentric standards rather than more objective ones.\n\nEvolution also does not require that organisms become more complex. Although the history of life shows an apparent trend towards the evolution of biological complexity; there is a question if this appearance of increased complexity is real, or if it comes from neglecting the fact that the majority of life on Earth has always consisted of prokaryotes. In this view, complexity is not a necessary consequence of evolution, but that specific circumstances of evolution on Earth frequently made greater complexity advantageous and thus naturally selected for. Depending on the situation, organisms' complexity can either increase, decrease, or stay the same, and all three of these trends have been observed in evolution.\n\nCreationist sources frequently define evolution according to a colloquial, rather than scientific, meaning. As a result, many attempts to rebut evolution do not address the findings of evolutionary biology (see straw man argument). This also means that advocates of creationism and evolutionary biologists often simply speak past each other.\n\nCritics of evolution assert that evolution is \"just a theory,\" which emphasizes that scientific theories are never absolute, or misleadingly presents it as a matter of opinion rather than of fact or evidence. This reflects a difference of the meaning of \"theory\" in a scientific context: whereas in colloquial speech a \"theory\" is a conjecture or guess, in science a theory is an explanation whose predictions have been verified by experiments or other evidence. \"Evolutionary theory\" refers to an explanation for the diversity of species and their ancestry which has met extremely high standards of scientific evidence. An example of evolution as theory is the modern synthesis of Darwinian natural selection and Mendelian inheritance. As with any scientific theory, the modern synthesis is constantly debated, tested, and refined by scientists, but there is an overwhelming consensus in the scientific community that it remains the only robust model that accounts for the known facts concerning evolution.\n\nCritics also state that evolution is not a fact. In science a fact is a verified empirical observation while in colloquial contexts a fact can simply refer to anything for which there is overwhelming evidence. For example, in common usage theories such as \"the Earth revolves around the Sun\" and \"objects fall due to gravity\" may be referred to as \"facts,\" even though they are purely theoretical. From a scientific standpoint, therefore, evolution may be called a \"fact\" for the same reason that gravity can: under the scientific definition, evolution is an observable process that occurs whenever a population of organisms genetically changes over time. Under the colloquial definition, the theory of evolution can also be called a fact, referring to this theory's well-established nature. Thus, evolution is widely considered both a theory and a fact by scientists.\n\nSimilar confusion is involved in objections that evolution is \"unproven,\" since no theory in science is known to be absolutely true, only verified by empirical evidence. This distinction is an important one in philosophy of science, as it relates to the lack of absolute certainty in all empirical claims, not just evolution. Strict proof is possible only in formal sciences such as logic and mathematics, not natural sciences (where terms such as \"validated\" or \"corroborated\" are more appropriate). Thus, to say that evolution is not proven is trivially true, but no more an indictment of evolution than calling it a \"theory.\" The confusion arises in that the colloquial meaning of \"proof\" is simply \"compelling evidence,\" in which case scientists would indeed consider evolution \"proven.\"\n\nAn objection is often made in the teaching of evolution that evolution is controversial or contentious. Unlike past creationist arguments which sought to abolish the teaching of evolution altogether, this argument makes the weaker claim that evolution should be presented alongside alternative views since it is controversial, and students should be allowed to evaluate and choose between the options on their own.\n\nThis objection forms the basis of the \"Teach the Controversy\" campaign by the Discovery Institute, a think tank based in Seattle, Washington, to promote the teaching of intelligent design in U.S. public schools. This goal followed the Institute's \"wedge strategy,\" an attempt to gradually undermine evolution and ultimately to \"reverse the stifling dominance of the materialist worldview, and to replace it with a science consonant with Christian and theistic convictions.\" Several other attempts were made to insert intelligent design or creationism into the U.S. public school curriculum, including the failed Santorum Amendment in 2001.\n\nScientists and U.S. courts have rejected this objection on the grounds that science is not based on appeals to popularity, but on evidence. The scientific consensus of biologists determines what is considered acceptable science, not popular opinion or fairness, and although evolution is controversial in the public arena, it is entirely uncontroversial among experts in the field.\n\nIn response, creationists have disputed the level of scientific support for evolution. The Discovery Institute has gathered over 761 scientists as of August 2008 to sign \"A Scientific Dissent From Darwinism\" in order to show that there are a number of scientists who dispute what they refer to as \"Darwinian evolution.\" This statement did not profess outright disbelief in evolution, but expressed skepticism as to the ability of \"random mutation and natural selection to account for the complexity of life.\" Several counter-petitions have been launched in turn, including \"A Scientific Support for Darwinism\", which gathered over 7,000 signatures in four days, and Project Steve, a tongue-in-cheek petition that has gathered the signatures of 1,393 (as of May 24, 2016) evolution-supporting scientists named \"Steve\" (or any similar variation thereof—Stephen, Stephanie, Esteban, etc.).\n\nCreationists have argued for over a century that evolution is a \"theory in crisis\" that will soon be overturned, based on objections that it lacks reliable evidence or violates natural laws. These objections have been rejected by most scientists, as have claims that intelligent design, or any other creationist explanation, meets the basic scientific standards that would be required to make them scientific alternatives to evolution. It is also argued that even if evidence against evolution exists, it is a false dilemma to characterize this as evidence \"for\" intelligent design.\n\nA similar objection to evolution is that certain scientific authorities—mainly pre-modern ones—have doubted or rejected evolution. Most commonly, it is argued that Darwin \"recanted\" on his deathbed, a false anecdote originating from Lady Hope's story. These objections are generally rejected as appeals to authority.\n\nA common neo-creationist objection to evolution is that evolution does not adhere to normal scientific standards—that it is not genuinely scientific. It is argued that evolutionary biology does not follow the scientific method and therefore should not be taught in science classes, or at least should be taught alongside other views (i.e., creationism). These objections often deal with the very nature of evolutionary theory, the scientific method, and philosophy of science.\n\nCreationists commonly argue that \"evolution is a religion; it is not a science.\" The purpose of this criticism is to reframe the debate from one between science (evolution) and religion (creationism) to between two religious beliefs—or even to argue that evolution is religious while intelligent design is not. Those that oppose evolution frequently refer to supporters of evolution as \"evolutionists\" or \"Darwinists.\"\n\nThe arguments for evolution being a religion generally amount to arguments by analogy: it is argued that evolution and religion have one or more things in common, and that therefore evolution is a religion. Examples of claims made in such arguments are statements that evolution is based on faith, and that supporters of evolution dogmatically reject alternative suggestions out-of-hand. These claims have become more popular in recent years as the neo-creationist movement has sought to distance itself from religion, thus giving it more reason to make use of a seemingly anti-religious analogy.\n\nSupporters of evolution have argued in response that no scientist's claims are treated as sacrosanct, as shown by the aspects of Darwin's theory that have been rejected or revised by scientists over the years to form first neo-Darwinism and later the modern evolutionary synthesis. The claim that evolution relies on faith is likewise rejected on the grounds that evolution has strong supporting evidence, and therefore does not require faith.\n\nThe argument that evolution is religious has been rejected in general on the grounds that \"religion\" is not defined by how dogmatic or zealous its adherents are, but by its spiritual or supernatural beliefs. Evolutionary supporters point out evolution is neither dogmatic nor based on faith, and they accuse creationists of equivocating between the strict definition of \"religion\" and its colloquial usage to refer to anything that is enthusiastically or dogmatically engaged in. United States courts have also rejected this objection:\n\nAssuming for the purposes of argument, however, that evolution is a religion or religious tenet, the remedy is to stop the teaching of evolution, not establish another religion in opposition to it. Yet it is clearly established in the case law, and perhaps also in common sense, that evolution is not a religion and that teaching evolution does not violate the Establishment Clause, Epperson v. Arkansas, supra, Willoughby v. Stever, No. 15574-75 (D.D.C. May 18, 1973); aff'd. 504 F.2d 271 (D.C. Cir. 1974), cert. denied , 420 U.S. 924 (1975); Wright v. Houston Indep. School Dist., 366 F. Supp. 1208 (S.D. Tex 1978), aff.d. 486 F.2d 137 (5th Cir. 1973), cert. denied 417 U.S. 969 (1974).\n\nA related claim is that evolution is atheistic (see the Atheism section below); creationists sometimes merge the two claims and describe evolution as an \"atheistic religion\" (cf. humanism). This argument against evolution is also frequently generalized into a criticism of all science; it is argued that \"science is an atheistic religion,\" on the grounds that its methodological naturalism is as unproven, and thus as \"faith-based,\" as the supernatural and theistic beliefs of creationism.\n\nA statement is considered falsifiable if there is an observation or a test that could be made that would demonstrate that the statement is false. Statements that are not falsifiable cannot be examined by scientific investigation since they permit no tests that evaluate their accuracy. Creationists such as Henry M. Morris have claimed that any observation can be fitted into the evolutionary framework, so it is impossible to demonstrate that evolution is wrong and therefore evolution is non-scientific.\n\nSupporters of evolution argue that evolution is shown falsifiable by many conceivable lines of evidence that could falsify evolution, such as the fossil record showing no change over time, confirmation that mutations are prevented from accumulating, or observations showing organisms being created supernaturally or spontaneously. Many of Darwin's ideas and assertions of fact have been falsified as evolutionary science has developed and has continued to confirm his central concepts. In contrast, creationist explanations involving the direct intervention of the supernatural in the physical world are not falsifiable, because any result of an experiment or investigation could be the unpredictable action of an omnipotent deity.\n\nIn 1976, the philosopher Karl Popper said that \"Darwinism is not a testable scientific theory but a metaphysical research programme.\" He later changed his mind and argued that Darwin's \"theory of natural selection is difficult to test\" with respect to other areas of science.\n\nThe most direct evidence that evolutionary theory is falsifiable may be the original words of Charles Darwin in chapter 6 of \"On the Origin of Species\": \"If it could be demonstrated that any complex organ existed, which could not possibly have been formed by numerous, successive, slight modifications, my theory would absolutely break down.\" If empirical evidence supported this instance, it would be affirmation of the creationist argument in favor of irreducible complexity.\n\nIn response to the unfalsifiability criticism of evolutionary theory, numerous examples of potential ways to falsify evolution have been proposed. J. B. S. Haldane, when asked what hypothetical evidence could disprove evolution, replied \"fossil rabbits in the Precambrian era.\" Numerous other potential ways to falsify evolution have also been proposed. For example, the fact that humans have one fewer pair of chromosomes than the great apes offered a testable hypothesis involving the fusion or splitting of chromosomes from a common ancestor. The fusion hypothesis was confirmed in 2005 by discovery that human chromosome 2 is homologous with a fusion of two chromosomes that remain separate in other primates. Extra, inactive telomeres and centromeres remain on human chromosome 2 as a result of the fusion. The assertion of common descent could also have been disproven with the invention of DNA sequencing methods. If true, human DNA should be far more similar to chimpanzees and other great apes, than to other mammals. If not, then common descent is falsified. DNA analysis has shown that humans and chimpanzees share a large percentage of their DNA (between 95% to 99.4% depending on the measure). Also, the evolution of chimpanzees and humans from a common ancestor predicts a (geologically) recent common ancestor. Numerous transitional fossils have since been found. Hence, human evolution has passed several falsifiable tests.\n\nA related claim is that natural selection is tautological. Specifically, it is often argued that the phrase \"survival of the fittest\" is a tautology, in that fitness is defined as ability to survive and reproduce. This phrase was first used by Herbert Spencer in 1864 but is rarely used by biologists. Additionally, fitness is more accurately defined as the state of possessing traits that make survival more likely; this definition, unlike simple \"survivability,\" avoids being trivially true.\n\nSimilarly, it is argued that evolutionary theory is circular reasoning, in that evidence is interpreted as supporting evolution, but evolution is required to interpret the evidence. An example of this is the claim that geological strata are dated through the fossils they hold, but that fossils are in turn dated by the strata they are in. However, in most cases strata are not dated by their fossils, but by their position relative to other strata and by radiometric dating, and most strata were dated before the theory of evolution was formulated.\n\nIn his 1982 book, \"Abusing Science: The Case Against Creationism\", philosopher of science Philip Kitcher specifically addresses the \"falsifiability\" question by taking into account notable philosophical critiques of Popper by Carl Gustav Hempel and Willard Van Orman Quine and provides a definition of theory other than as a set of falsifiable statements. As Kitcher points out, if one took a strictly Popperian view of \"theory,\" observations of Uranus when it was first discovered in 1781 would have \"falsified\" Isaac Newton's celestial mechanics. Rather, people suggested that another planet influenced Uranus' orbit—and this prediction was indeed eventually confirmed. Kitcher agrees with Popper that \"there is surely something right in the idea that a science can succeed only if it can fail.\" But he insists that we view scientific theories as consisting of an \"elaborate collection of statements,\" some of which are not falsifiable, and others—what he calls \"auxiliary hypotheses,\" which are.\n\nObjections to the evidence that evolution occurs tend to be more concrete and specific, often involving direct analysis of evolutionary biology's methods and claims.\n\nA common claim of creationists is that evolution has never been observed. Challenges to such objections often come down to debates over how evolution is defined (see the Defining evolution section above). Under the conventional biological definition of \"evolution\", it is a simple matter to observe evolution occurring. Evolutionary processes, in the form of populations changing their genetic composition from generation to generation, have been observed in different scientific contexts, including the evolution of fruit flies, mice, and bacteria in the laboratory, and of tilapia in the field. Such studies on experimental evolution, particularly those using microorganisms, are now providing important insights into how evolution occurs, especially in the case of antibiotic resistance.\n\nIn response to such examples, creationists specify that they are objecting only to macroevolution, not microevolution: most creationist organizations do not dispute the occurrence of short-term, relatively minor evolutionary changes, such as that observed even in dog breeding. Rather, they dispute the occurrence of major evolutionary changes over long periods of time, which by definition cannot be directly observed, only inferred from microevolutionary processes and the traces of macroevolutionary ones.\n\nAs biologists define \"macroevolution\", both microevolution and macroevolution have been observed. Speciations, for example, have been directly observed many times, despite popular misconceptions to the contrary. Additionally, the modern evolutionary synthesis draws no distinction in the processes described by the theory of evolution when considering macroevolution and microevolution as the former is simply at the species level or above and the latter is below the species level. An example of this is ring species.\n\nAdditionally, past macroevolution can be inferred from historical traces. Transitional fossils, for example, provide plausible links between several different groups of organisms, such as \"Archaeopteryx\" linking birds and dinosaurs, or the \"Tiktaalik\" linking fish and limbed amphibians. Creationists dispute such examples, from asserting that such fossils are hoaxes or that they belong exclusively to one group or the other, to asserting that there should be far more evidence of obvious transitional species. Darwin himself found the paucity of transitional species to be one of the greatest weaknesses of his theory: Why then is not every geological formation and every stratum full of such intermediate links? Geology assuredly does not reveal any such finely graduated organic chain; and this, perhaps, is the most obvious and gravest objection which can be urged against my theory. The explanation lies, as I believe, in the extreme imperfection of the geological record. Darwin appealed to the limited collections then available, the extreme lengths of time involved, and different rates of change with some living species differing very little from fossils of the Silurian period. In later editions he added \"that the periods during which species have been undergoing modification, though very long as measured by years, have probably been short in comparison with the periods during which these same species remained without undergoing any change.\" The number of clear transitional fossils has increased enormously since Darwin's day, and this problem has been largely resolved with the advent of the theory of punctuated equilibrium, which predicts a primarily stable fossil record broken up by occasional major speciations.\n\nAs more and more compelling direct evidence for inter-species and species-to-species evolution has been gathered, creationists have redefined their understanding of what amounts to \"created kinds,\" and have continued to insist that more dramatic demonstrations of evolution be experimentally produced. One version of this objection is \"Were you there?,\" popularized by young Earth creationist Ken Ham. It argues that because no one except God could directly observe events in the distant past, scientific claims are just speculation or \"story-telling.\" DNA sequences of the genomes of organisms allow an independent test of their predicted relationships, since species which diverged more recently will be more closely related genetically than species which are more distantly related; such phylogenetic trees show a hierarchical organization within the tree of life, as predicted by common descent.\n\nIn fields such as astrophysics or meteorology, where direct observation or laboratory experiments are difficult or impossible, the scientific method instead relies on observation and logical inference. In such fields, the test of falsifiability is satisfied when a theory is used to predict the results of new observations. When such observations contradict a theory's predictions, it may be revised or discarded if an alternative better explains the observed facts. For example, Newton's theory of gravitation was replaced by Albert Einstein's theory of general relativity when the latter was observed to more precisely predict the orbit of Mercury.\n\nA related objection is that evolution is based on unreliable evidence, claiming that evolution is not even well-evidenced. Typically, this is either based on the argument that evolution's evidence is full of frauds and hoaxes, that current evidence for evolution is likely to be overturned as some past evidence has been, or that certain types of evidence are inconsistent and dubious.\n\nArguments against evolution's reliability are thus often based on analyzing the history of evolutionary thought or the history of science in general. Creationists point out that in the past, major scientific revolutions have overturned theories that were at the time considered near-certain. They thus claim that current evolutionary theory is likely to undergo such a revolution in the future, on the basis that it is a \"theory in crisis\" for one reason or another.\n\nCritics of evolution commonly appeal to past scientific hoaxes such as the Piltdown Man forgery. It is argued that because scientists have been mistaken and deceived in the past about evidence for various aspects of evolution, the current evidence for evolution is likely to also be based on fraud and error. Much of the evidence for evolution has been accused of being fraudulent at various times, including \"Archaeopteryx\", peppered moth melanism, and Darwin's finches; these claims have been subsequently refuted.\n\nIt has also been claimed that certain former pieces of evidence for evolution which are now considered out-of-date and erroneous, such as Ernst Haeckel's 19th-century comparative drawings of embryos, used to illustrate his recapitulation theory (\"\"ontogeny recapitulates phylogeny\"\"), were not merely errors but frauds. Molecular biologist Jonathan Wells criticizes biology textbooks by alleging that they continue to reproduce such evidence after it has been debunked. In response, the National Center for Science Education notes that none of the textbooks reviewed by Wells makes the claimed error, as Haeckel's drawings are shown in a historical context with discussion about why they are wrong, and the accurate modern drawings and photos used in the textbooks are misrepresented by Wells.\n\nCreationists claim that evolution relies on certain types of evidence that do not give reliable information about the past. For example, it is argued that radiometric dating technique of evaluating a material's age based on the radioactive decay rates of certain isotopes generates inconsistent and thus unreliable results. Radiocarbon dating based on the carbon-14 isotope has been particularly criticized. It is argued that radiometric decay relies on a number of unwarranted assumptions such as the principle of uniformitarianism, consistent decay rates, or rocks acting as closed systems. Such arguments have been dismissed by scientists on the grounds that independent methods have confirmed the reliability of radiometric dating as a whole; additionally, different radiometric dating methods and techniques have independently confirmed each other's results.\n\nAnother form of this objection is that fossil evidence is not reliable. This is based on a much wider range of claims. These include that there are too many \"gaps\" in the fossil record, that fossil-dating is circular (see the Unfalsifiability section above), or that certain fossils, such as polystrate fossils, are seemingly \"out of place.\" Examination by geologists have found polystrate fossils to be consistent with \"in situ\" formation. It is argued that certain features of evolution support creationism's catastrophism (cf. Great Flood), rather than evolution's gradualistic punctuated equilibrium, which some assert is an \"ad hoc\" theory to explain the fossil gaps.\n\nSome of the oldest and most common objections to evolution dispute whether evolution can truly account for all the apparent complexity and order in the natural world. It is argued that evolution is too unlikely or otherwise lacking to account for various aspects of life, and therefore that an intelligence, such as God of the Abrahamic religions, must at the very least be appealed to for those specific features.\n\nA common objection to evolution is that it is simply too unlikely for life, in its complexity and apparent \"design,\" to have arisen \"by chance.\" It is argued that the odds of life having arisen without a deliberate intelligence guiding it are so astronomically low that it is unreasonable \"not\" to infer an intelligent designer from the natural world, and specifically from the diversity of life. A more extreme version of this argument is that evolution cannot create complex structures (see the Creation of complex structures section below). The idea that it is simply too implausible for life to have evolved is often wrongly encapsulated with a quotation that the \"probability of life originating on Earth is no greater than the chance that a hurricane, sweeping through a scrapyard, would have the luck to assemble a Boeing 747\"—a claim attributed to astrophysicist Fred Hoyle and known as Hoyle's fallacy. Hoyle was a Darwinist, atheist and anti-theist, but advocated the theory of panspermia, in which abiogenesis begins in outer space and primitive life on Earth is held to have arrived via natural dispersion.\n\nViews superficially similar, but unrelated to Hoyle's, are thus invariably justified with arguments from analogy. The basic idea of this argument for a designer is the teleological argument, an argument for the existence of God based on the perceived order or purposefulness of the universe. A common way of using this as an objection to evolution is by appealing to the 18th-century philosopher William Paley's watchmaker analogy, which argues that certain natural phenomena are analogical to a watch (in that they are ordered, or complex, or purposeful), which means that, like a watch, they must have been designed by a \"watchmaker\"—an intelligent agent. This argument forms the core of intelligent design, a neo-creationist movement seeking to establish certain variants of the design argument as legitimate science, rather than as philosophy or theology, and have them be taught alongside evolution.\n\nThis objection is fundamentally an argument by lack of imagination, or argument from incredulity: a certain explanation is seen as being counterintuitive, and therefore an alternate, more intuitive explanation is appealed to instead. Supporters of evolution generally respond by arguing that evolution is not based on \"chance,\" but on predictable chemical interactions: natural processes, rather than supernatural beings, are the \"designer.\" Although the process involves some random elements, it is the non-random selection of survival-enhancing genes that drives evolution along an ordered trajectory. The fact that the results are ordered and seem \"designed\" is no more evidence for a supernatural intelligence than the appearance of complex natural phenomena (e.g. snowflakes). It is also argued that there is insufficient evidence to make statements about the plausibility or implausibility of abiogenesis, that certain structures demonstrate poor design, and that the implausibility of life evolving exactly as it did is no more evidence for an intelligence than the implausibility of a deck of cards being shuffled and dealt in a certain random order.\n\nIt has also been noted that arguments against some form of life arising \"by chance\" are really objections to nontheistic abiogenesis, not to evolution. Indeed, arguments against \"evolution\" are based on the misconception that abiogenesis is a component of, or necessary precursor to, evolution. Similar objections sometimes conflate the Big Bang with evolution.\n\nChristian apologist and philosopher Alvin Plantinga, a supporter of intelligent design, has formalized and revised the improbability argument as the evolutionary argument against naturalism, which asserts that it is irrational to reject a supernatural, intelligent creator because the apparent probability of certain faculties evolving is so low. Specifically, Plantinga claims that evolution cannot account for the rise of reliable reasoning faculties. Plantinga argues that whereas a God would be expected to create beings with reliable reasoning faculties, evolution would be just as likely to lead to unreliable ones, meaning that if evolution is true, it is irrational to trust whatever reasoning one relies on to conclude that it is true. This novel epistemological argument has been criticized similarly to other probabilistic design arguments. It has also been argued that rationality, if conducive to survival, is more likely to be selected for than irrationality, making the natural development of reliable cognitive faculties more likely than unreliable ones.\n\nA related argument against evolution is that most mutations are harmful. However, the vast majority of mutations are neutral, and the minority of mutations which are beneficial or harmful are often situational; a mutation that is harmful in one environment may be helpful in another.\n\nIn addition to complex structures and systems, among the phenomena that critics variously claim evolution cannot explain are consciousness, hominid intelligence, instincts, emotions, metamorphosis, photosynthesis, homosexuality, music, language, religion, morality, and altruism (see altruism in animals). Most of these, such as hominid intelligence, instinct, emotion, photosynthesis, language, and altruism, have been well-explained by evolution, while others remain mysterious, or only have preliminary explanations. Supporters of evolution further contend that no alternative explanation has been able to adequately explain the biological origin of these phenomena either.\n\nCreationists argue against evolution on the grounds that it cannot explain certain non-evolutionary processes, such as abiogenesis, the Big Bang, or the meaning of life. In such instances, \"evolution\" is being redefined to refer to the entire history of the universe, and it is argued that if one aspect of the universe is seemingly inexplicable, the entire body of scientific theories must be baseless. At this point, objections leave the arena of evolutionary biology and become general scientific or philosophical disputes.\n\nAstronomers Fred Hoyle and Chandra Wickramasinghe have argued in favor of cosmic ancestry, and against abiogenesis and evolution.\n\nThis class of objections is more radical than the above, claiming that a major aspect of evolution is not merely unscientific or implausible, but rather impossible, because it contradicts some other law of nature or is constrained in such a way that it cannot produce the biological diversity of the world.\n\nModern evolutionary theory posits that all biological systems must have evolved incrementally, through a combination of natural selection and genetic drift. Both Darwin and his early detractors recognized the potential problems that could arise for his theory of natural selection if the lineage of organs and other biological features could not be accounted for by gradual, step-by-step changes over successive generations; if all the intermediary stages between an initial organ and the organ it will become are not all improvements upon the original, it will be impossible for the later organ to develop by the process of natural selection alone. Complex organs such as the eye had been presented by William Paley as exemplifying the need for design by God, and anticipating early criticisms that the evolution of the eye and other complex organs seemed impossible, Darwin noted that:\n\n[R]eason tells me, that if numerous gradations from a perfect and complex eye to one very imperfect and simple, each grade being useful to its possessor, can be shown to exist; if further, the eye does vary ever so slightly, and the variations be inherited, which is certainly the case; and if any variation or modification in the organ be ever useful to an animal under changing conditions of life, then the difficulty of believing that a perfect and complex eye could be formed by natural selection, though insuperable by our imagination, can hardly be considered real.\n\nSimilarly, ethologist and evolutionary biologist Richard Dawkins said on the topic of the evolution of the feather in an interview for the television program \"The Atheism Tapes\":\nThere's got to be a series of advantages all the way in the feather. If you can't think of one, then that's your problem not natural selection's problem... It's perfectly possible feathers began as fluffy extensions of reptilian scales to act as insulators... The earliest feathers might have been a different approach to hairiness among reptiles keeping warm.\nCreationist arguments have been made such as \"What use is half an eye?\" and \"What use is half a wing?\". Research has confirmed that the natural evolution of the eye and other intricate organs is entirely feasible. Creationist claims have persisted that such complexity evolving without a designer is inconceivable and this objection to evolution has been refined in recent years as the more sophisticated irreducible complexity argument of the intelligent design movement, formulated by Michael Behe. Biochemist Michael Behe has argued that current evolutionary theory cannot account for certain complex structures, particularly in microbiology. On this basis, Behe argues that such structures were \"purposely arranged by an intelligent agent.\"\n\nIrreducible complexity is the idea that certain biological systems cannot be broken down into their constituent parts and remain functional, and therefore that they could not have evolved naturally from less complex or complete systems. Whereas past arguments of this nature generally relied on macroscopic organs, Behe's primary examples of irreducible complexity have been cellular and biochemical in nature. He has argued that the components of systems such as the blood clotting cascade, the immune system, and the bacterial flagellum are so complex and interdependent that they could not have evolved from simpler systems.\n\nIn the years since Behe proposed irreducible complexity, new developments and advances in biology, such as an improved understanding of the evolution of flagella, have already undermined these arguments. The idea that seemingly irreducibly complex systems cannot evolve has been refuted through evolutionary mechanisms, such as exaptation (the adaptation of organs for entirely new functions) and the use of \"scaffolding,\" which are initially necessary features of a system that later degenerate when they are no longer required. Potential evolutionary pathways have been provided for all of the systems Behe used as examples of irreducible complexity.\n\nThe Cambrian explosion was the relatively rapid appearance around of most major animal phyla as demonstrated in the fossil record, and many more phyla now extinct. This was accompanied by major diversification of other organisms. Prior to the Cambrian explosion most organisms were simple, composed of individual cells occasionally organized into colonies. Over the following 70 or 80 million years, the rate of diversification accelerated by an order of magnitude and the diversity of life began to resemble that of today, although they did not resemble the species of today.\n\nThe basic problem with this is that natural selection calls for the slow accumulation of changes, where a new phyla would take longer than a new class which would take longer than a new order, which would take longer than a new family, which would take longer than a new genus would take longer than emergence of a new species but the apparent occurrence of high-level taxa without precedents is perhaps implying unusual evolutionary mechanisms.\n\nThere is general consensus that many factors helped trigger the Cambrian explosion, but there is no generally accepted consensus about the combination and the Cambrian explosion continues to be an area of controversy and research over why so rapid, why at the phylum level, why so many phyla then and none since, and even if the apparent fossil record is accurate.\n\nAn example of opinions involving the commonly cited rise in oxygen Great Oxidation Event from biologist PZ Myers summarizes: \"What it was was environmental changes, in particular the bioturbation revolution caused by the evolution of worms that released buried nutrients, and the steadily increasing oxygen content of the atmosphere that allowed those nutrients to fuel growth; ecological competition, or a kind of arms race, that gave a distinct selective advantage to novelties that allowed species to occupy new niches; and the evolution of developmental mechanisms that enabled multicellular organisms to generate new morphotypes readily.\" The increase in molecular oxygen (O) also may have allowed the formation of the protective ozone layer (O) that helps shield Earth from lethal UV radiation from the Sun.\n\nA recent objection of creationists to evolution is that evolutionary mechanisms such as mutation cannot generate new information. Creationists such as William A. Dembski, Werner Gitt, and Lee Spetner have attempted to use information theory to dispute evolution. Dembski has argued that life demonstrates specified complexity, and proposed a law of conservation of information that extremely improbable \"complex specified information\" could be conveyed by natural means but never originated without an intelligent agent. Gitt asserted that information is an intrinsic characteristic of life and that an analysis demonstrates the mind and will of their Creator.\n\nThese claims have been widely rejected by the scientific community which asserts that new information is regularly generated in evolution whenever a novel mutation or gene duplication arises. Dramatic examples of entirely new and unique traits arising through mutation have been observed in recent years, such as the evolution of nylon-eating bacteria which developed new enzymes to efficiently digest a material that never existed before the modern era. There is no need to account for the creation of information when an organism is considered together with the environment it evolved in. The information in the genome forms a record of how it was possible to survive in a particular environment. The information is gathered from the environment through trial and error as mutating organisms either reproduce or fail.\n\nAnother objection is that evolution violates the second law of thermodynamics. The law states that \"the entropy of an isolated system not in equilibrium will tend to increase over time, approaching a maximum value at equilibrium\". In other words, an isolated system's entropy (a measure of the dispersal of energy in a physical system so that it is not available to do mechanical work) will tend to increase or stay the same, not decrease. Creationists argue that evolution violates this physical law by requiring a decrease in entropy, or disorder, over time.\n\nThe claims have been criticized for ignoring that the second law only applies to isolated systems. Organisms are open systems as they constantly exchange energy and matter with their environment: for example animals eat food and excrete waste, and radiate and absorb heat. It is argued that the Sun-Earth-space system does not violate the second law because the enormous increase in entropy due to the Sun and Earth radiating into space dwarfs the local decrease in entropy caused by the existence and evolution of self-organizing life.\n\nSince the second law of thermodynamics has a precise mathematical definition, this argument can be analyzed quantitatively. This was done by physicist Daniel F. Styer, who concluded: \"Quantitative estimates of the entropy involved in biological evolution demonstrate that there is no conflict between evolution and the second law of thermodynamics.\"\n\nIn a published letter to the editor of \"The Mathematical Intelligencer\" titled \"How anti-evolutionists abuse mathematics,\" mathematician Jason Rosenhouse stated:\nThe fact is that natural forces routinely lead to local decreases in entropy. Water freezes into ice and fertilised eggs turn into babies. Plants use sunlight to convert carbon dioxide and water into sugar and oxygen, but [we do] not invoke divine intervention to explain the process [...] thermodynamics offers nothing to dampen our confidence in Darwinism.\n\nOther common objections to evolution allege that evolution leads to objectionable results, including bad beliefs, behaviors, and events. It is argued that the teaching of evolution degrades values, undermines morals, and fosters irreligion or atheism. These may be considered appeals to consequences (a form of logical fallacy), as the potential ramifications of belief in evolutionary theory have nothing to do with its objective empirical reality.\n\nIn biological classification humans are animals, a basic point which has been known for more than 2,000 years. The creationist J. Rendle-Short asserted in \"Creation\" magazine that if people are taught evolution they can be expected to behave like animals: since animals behave in all sorts of different ways, this is meaningless. In evolutionary terms, humans are able to acquire knowledge and change their behaviour to meet social standards, so humans behave in the manner of other humans.\n\nIn 1917, Vernon Kellogg published \"Headquarters Nights: A Record of Conversations and Experiences at the Headquarters of the German Army in France and Belgium\", which asserted that German intellectuals were totally committed to might-makes-right due to \"whole-hearted acceptance of the worst of Neo-Darwinism, the \"Allmacht\" of natural selection applied rigorously to human life and society and \"Kultur\".\" This strongly influenced the politician William Jennings Bryan, who saw Darwinism as a moral threat to America and campaigned against evolutionary theory; his campaign culminated in the Scopes Trial, which effectively prevented teaching of evolution in most public schools until the 1960s.\n\nR. Albert Mohler, Jr., president of the Southern Baptist Theological Seminary in Louisville, Kentucky, wrote August 8, 2005, in NPR's \"Taking Issue\" essay series, that \"Debates over education, abortion, environmentalism, homosexuality and a host of other issues are really debates about the origin — and thus the meaning — of human life. ...evolutionary theory stands at the base of moral relativism and the rejection of traditional morality.\"\n\nHenry M. Morris, engineering professor and founder of the Creation Research Society and the Institute of Creation Research, claims that evolution was part of a pagan religion that emerged after the Tower of Babel, was part of Plato's and Aristotle's philosophies, and was responsible for everything from war to pornography to the breakup of the nuclear family. He has also claimed that perceived social ills like crime, teenage pregnancies, homosexuality, abortion, immorality, wars, and genocide are caused by a belief in evolution.\n\nRev. D. James Kennedy of The Center for Reclaiming America for Christ and Coral Ridge Ministries claims that Darwin was responsible for Adolf Hitler's atrocities. In Kennedy's documentary and the accompanying pamphlet with the same title, \"Darwin's Deadly Legacy\", Kennedy states that \"To put it simply, no Darwin, no Hitler.\" In his efforts to expose the \"harmful effects that evolution is still having on our nation, our children, and our world,\" Kennedy also states that, \"We have had 150 years of the theory of Darwinian evolution, and what has it brought us? Whether Darwin intended it or not, millions of deaths, the destruction of those deemed inferior, the devaluing of human life, increasing hopelessness.\" The Discovery Institute's Center for Science and Culture fellow Richard Weikart has made similar claims, as have other creationists. The claim was central to the documentary film \"\" (2008) promoting intelligent design creationism. The Anti-Defamation League describes such claims as outrageous misuse of the Holocaust and its imagery, and as trivializing the \"...many complex factors that led to the mass extermination of European Jewry. Hitler did not need Darwin or evolution to devise his heinous plan to exterminate the Jewish people, and Darwin and evolutionary theory cannot explain Hitler's genocidal madness. Moreover, anti-Semitism existed long before Darwin ever wrote a word.\"\n\nYoung Earth creationist Kent Hovind blames communism, socialism, World War I, World War II, racism, the Holocaust, Stalin's war crimes, the Vietnam War, and Pol Pot's Killing Fields on evolution, as well as the increase in crime, unwed mothers, and other social ills. Hovind's son Eric Hovind claims that evolution is responsible for tattoos, body piercing, premarital sex, unwed births, sexually transmitted diseases (STDs), divorce, and child abuse.\n\nSupporters of evolution dismiss such criticisms as counterfactual, and some argue that the opposite seems to be the case. A study published by the author and illustrator Gregory S. Paul found that religious beliefs, including belief in creationism and disbelief in evolution, are positively correlated with social ills like crime. The Barna Group surveys find that Christians and non-Christians in the U.S. have similar divorce rates, and the highest divorce rates in the U.S. are among Baptists and Pentecostals, both sects which reject evolution and embrace creationism.\n\nMichael Shermer argued in \"Scientific American\" in October 2006 that evolution supports concepts like family values, avoiding lies, fidelity, moral codes and the rule of law. He goes on to suggest that evolution gives more support to the notion of an omnipotent creator, rather than a tinkerer with limitations based on a human model, the more common image subscribed to by creationists. Careful analysis of the creationist charges that evolution has led to moral relativism and the Holocaust yields the conclusion that these charges appear to be highly suspect. Such analyses conclude that the origins of the Holocaust are more likely to be found in historical Christian anti-Semitism than in evolution.\n\nEvolution has been used to justify Social Darwinism, the exploitation of so-called \"lesser breeds without the law\" by \"superior races,\" particularly in the nineteenth century. Typically strong European nations that had successfully expanded their empires could be said to have \"survived\" in the struggle for dominance. With this attitude, Europeans except for Christian missionaries rarely adopted any customs and languages of local people under their empires.\n\nAnother charge leveled at evolutionary theory by creationists is that belief in evolution is either tantamount to atheism, or conducive to atheism. It is commonly claimed that all proponents of evolutionary theory are \"materialistic atheists.\" On the other hand, Davis A. Young argues that creation science \"itself\" is harmful to Christianity because its bad science will turn more away than it recruits. Young asks, \"Can we seriously expect non-Christians to develop a respect for Christianity if we insist on teaching the brand of science that creationism brings with it?\" However, evolution neither requires nor rules out the existence of a supernatural being. Philosopher Robert T. Pennock makes the comparison that evolution is no more atheistic than plumbing. H. Allen Orr, professor of biology at University of Rochester, notes that:\n\nIn addition, a wide range of religions have reconciled a belief in a supernatural being with evolution. Molleen Matsumura of the National Center for Science Education found that \"of Americans in the twelve largest Christian denominations, 89.6% belong to churches that support evolution education.\" These churches include the \"United Methodist Church, National Baptist Convention USA, Evangelical Lutheran Church in America, Presbyterian Church (USA), National Baptist Convention of America, African Methodist Episcopal Church, the Roman Catholic Church, the Episcopal Church, and others.\" A poll in 2000 done for People for the American Way found that 70% of the American public felt that evolution was compatible with a belief in God. Only 48% of the people polled could choose the correct definition of evolution from a list, however.\n\nOne poll reported in the journal \"Nature\" showed that among American scientists (across various disciplines), about 40 percent believe in both evolution and an active deity (theistic evolution). This is similar to the results reported for surveys of the general American public. Also, about 40 percent of the scientists polled believe in a God that answers prayers, and believe in immortality. While about 55% of scientists surveyed were atheists, agnostics, or nonreligious theists, atheism is far from universal among scientists who support evolution, or among the general public that supports evolution. Very similar results were reported from a 1997 Gallup Poll of the American public and scientists.\n\nTraditionalists still object to the idea that diversity in life, including human beings, arose through natural processes without a need for supernatural intervention, and they argue against evolution on the basis that it contradicts their literal interpretation of creation myths about separate \"created kinds.\" However, many religions, such as Catholicism, have reconciled their beliefs with evolution through theistic evolution.\n\n\n\n",
    "id": "8787159",
    "title": "Objections to evolution"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=9499590",
    "text": "Postnaturalism\n\nPostnaturalism is the theory of the postnatural, a term coined to describe organisms that have been intentionally and heritably altered by humans. Postnaturalism is a cultural process whereby organisms are bred to satisfy a specific cultural purpose. It can be used to read these organisms, which serve as insights into our culture by reflecting desires and beliefs prevalent at the time of breeding. This has direct implications for the evolutionary path of these organisms, whittling down undesirable traits to leave only those culturally sought out. Postnaturalism argues that in so doing, humans have and continue to actively alter the evolutionary path of a postnatural organism to suit our cultural desires. The agricultural practice of monoculture, for instance, is just one example of postnatural organisms who have been bred to such an extent that the modern-day species look nothing like their pre-neolithic counterparts. The breeding of these species for this purpose can be seen to be reflected in notable diet changes during this period, which proliferated during ensuing sedentism and urbanisation.\n\nPostnaturalism is a highly selective process. For every organism that has become used in our society, there are countless more that have remained non-postnatural for whatever reason ranging from a perceived lack of future use from them or traits that make them too difficult to farm. One such example is the golden orb-weaver spider which produces a strong, light and useful silk, however they are known to be cannibalistic and thus impossible to farm on a large scale.\n\nPostnatural history is defined as the \"study of the origins, habitats, and evolution of organisms that have been \"intentionally\" and \"heritably\" altered by humans\", which serves as a \"record of the influence of human culture on evolution\". So termed to differentiate it from traditional natural history studies, it is the subject of a storefront in Pittsburgh, United States called the Center for PostNatural History, which builds on this concept to produce an array of displays of organisms which are all postnatural.\n\nThe commencement of the postnatural can be considered to date back to prehistoric civilisation's early interaction with wild species. Here, domestication occurred as a way of adapting the environment around prehistoric people to suit their needs and desires through a gradual process of refinement. Domestication succeeds through the heritable continuation of an organism's phenotype and genotype allowing each generation to continue on from their previous generation. In its simplest terms, domestication alters a species through survival to a change in habitat, food source, or other significant change. These changes can be sought for a variety of different reasons including physical attributes, behavioural characteristics, lifespan, and adaptability to a change in environment.\n\nPostnatural practices include selective breeding, a process by which humans purposefully breed certain organisms for particular biological traits. The practice was known to the Romans, and has been commonly used continuing to this day. Michael Pollan argues that Charles Darwin saw this process and considered it artificial, rather than natural, selection, but in terms of evolutionary progress this distinction becomes irrelevant to the species; the change is irreversible all the same. This is simply because evolution is understood to be unable to 'undo' previous changes, but, particularly in proteins, continues along in a progression of its biological structure depending on what traits are required for survival.\n\nInduced mutation in the context of postnaturalism is the process whereby a specific genetic mutation - usually a rare occurrence - is selectively isolated by people and encouraged to reproduce in future offspring. This differs from the general understanding of a mutation that was induced by treatment from a particular chemical agent in a living species.\n\nA good example of this is the albino rat which possesses, in the wild a notoriously fatal genetic make-up making it much easier to identify to predators, a coat which drew interest in breeders so as to distinguish them from their more unhygienic-looking sewer counterparts prevalent in major cities in the 1800s. So numerous were rats in industrial cities, they became the subject of a sport based on their extermination, rat-baiting. The albino rat thus became distinguished from the regular unsightly sewer rat and even became sold as pets, the owning of one as a child allegedly the basis for Beatrix Potter's book \"Samuel Whiskers\".\n\nGenetic Engineering can be considered to be the purposeful alteration of the genetic makeup of an organism through the introduction of genes from sources not belonging to that organism. The isolation of a particular gene and introduction of another is often achieved through the use of biotechnology more broadly.\nGenetic engineering is a contentious topic and even in searching for a definition, there are several alternatives available highlighting the variation in perceptions around what is considered to be the goal and process of genetic engineering. However, genetic engineering embodies much of what is considered postnatural, but doing so with the next level of technology than used in previous methods such as those mentioned in the above sections. Increased use of advanced technology allows for improved precision and accuracy of methods, and carrying out of transgenics, whilst the use of laboratories as settings is designed to prevent the release of experimented organisms into the wild unless cleared with the relevant protocols and regulations beforehand.\n\nThe current agricultural practice of monoculture is intricately connected with postnaturalism, particularly now much of common contemporary agricultural practice has become mechanised. This mechanisation sometimes requires universality to comply with existing tools, machines and practices, such as the breeding of chickens to be a uniform size to ensure they fit into chicken harvesting machines. On other occasions rather than breed a particular species to a uniform size, some parts of organisms have been so heavily bred and altered that dystocia can regularly occur, such as with the Belgian Blue cattle whose birth canal regularly becomes constricted or even entirely blocked due to the birth canal's reduced size and the increased size of calves. The result is the routine scheduling of Caesarian sections.\n",
    "id": "9499590",
    "title": "Postnaturalism"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=8241706",
    "text": "List of non-fictional lost worlds\n\nThis is a list of non-fictional lost worlds, where the terrain has been isolated from its geological surroundings, and thus gained an independent ecological evolution, often resulting in the birth of species endemic to that area.\n\n\n\n\n\n\n\n",
    "id": "8241706",
    "title": "List of non-fictional lost worlds"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=627964",
    "text": "Survival of the fittest\n\n\"Survival of the fittest\" is a phrase that originated from Darwinian evolutionary theory as a way of describing the mechanism of natural selection. The biological concept of fitness is defined as reproductive success. In Darwinian terms the phrase is best understood as \"Survival of the form that will leave the most copies of itself in successive generations.\"\n\nHerbert Spencer first used the phrase, after reading Charles Darwin's \"On the Origin of Species\", in his \"Principles of Biology\" (1864), in which he drew parallels between his own economic theories and Darwin's biological ones: \"This survival of the fittest, which I have here sought to express in mechanical terms, is that which Mr. Darwin has called 'natural selection', or the preservation of favoured races in the struggle for life.\"\n\nDarwin responded positively to Alfred Russel Wallace's suggestion of using Spencer's new phrase \"survival of the fittest\" as an alternative to \"natural selection\", and adopted the phrase in \"The Variation of Animals and Plants under Domestication\" published in 1868. In \"On the Origin of Species\", he introduced the phrase in the fifth edition published in 1869, intending it to mean \"better designed for an immediate, local environment\".\n\nWhile the phrase \"survival of the fittest” is often used to refer to “natural selection”, it is avoided by modern biologists, because the phrase can be misleading. For example, “survival” is only one aspect of selection, and not always the most important. Another problem is that the word “fit” is frequently confused with a state of physical fitness. In the evolutionary meaning “fitness” is the rate of reproductive output among a class of genetic variants.\n\nThe phrase can also be interpreted to express a theory or hypothesis: that \"fit\" as opposed to \"unfit\" individuals or species, in some sense of \"fit\", will survive some test.\n\nInterpretations of the phrase as expressing a theory are in danger of being tautological, meaning roughly \"those with a propensity to survive have a propensity to survive\"; to have content the theory must use a concept of fitness that is independent of that of survival.\n\nInterpreted as a theory of species survival, the theory that the fittest species survive is undermined by evidence that while direct competition is observed between individuals, populations and species, there is little evidence that competition has been the driving force in the evolution of large groups such as, for example, amphibians, reptiles, and mammals. Instead, these groups have evolved by expanding into empty ecological niches. In the punctuated equilibrium model of environmental and biological change, the factor determining survival is often not superiority over another in competition but ability to survive dramatic changes in environmental conditions, such as after a meteor impact energetic enough to greatly change the environment globally. The main land dwelling animals to survive the K-Pg impact 66 million years ago had the ability to live in underground tunnels, for example.\n\nIn 2010 Sahney et al. argued that there is little evidence that intrinsic, biological factors such as competition have been the driving force in the evolution of large groups. Instead, they cited extrinsic, abiotic factors such as expansion as the driving factor on a large evolutionary scale. The rise of dominant groups such as amphibians, reptiles, mammals and birds occurred by opportunistic expansion into empty ecological niches and the extinction of groups happened due to large shifts in the abiotic environment.\n\nIt has been claimed that \"the survival of the fittest\" theory in biology was interpreted by late 19th century capitalists as \"an ethical precept that sanctioned cut-throat economic competition\" and led to the advent of the theory of \"social Darwinism\" which was used to justify laissez-faire economics, war and racism. However, these ideas predate and commonly contradict Darwin's ideas, and indeed their proponents rarely invoked Darwin in support. The term \"social Darwinism\" referring to capitalist ideologies was introduced as a term of abuse by Richard Hofstadter's \"Social Darwinism in American Thought\" published in 1944.\n\nCritics of theories of evolution have argued that \"survival of the fittest\" provides a justification for behaviour that undermines moral standards by letting the strong set standards of justice to the detriment of the weak. However, any use of evolutionary descriptions to set moral standards would be a naturalistic fallacy (or more specifically the is–ought problem), as prescriptive moral statements cannot be derived from purely descriptive premises. Describing how things are does not imply that things ought to be that way. It is also suggested that \"survival of the fittest\" implies treating the weak badly, even though in some cases of good social behaviour – co-operating with others and treating them well – might improve evolutionary fitness.\n\nRussian anarchist Peter Kropotkin viewed the concept of \"survival of the fittest\" as supporting co-operation rather than competition. In his book \"\" he set out his analysis leading to the conclusion that the fittest was not necessarily the best at competing individually, but often the community made up of those best at working together. He concluded that In the animal world we have seen that the vast majority of species live in societies, and that they find in association the best arms for the struggle for life: understood, of course, in its wide Darwinian sense — not as a struggle for the sheer means of existence, but as a struggle against all natural conditions unfavourable to the species. The animal species, in which individual struggle has been reduced to its narrowest limits, and the practice of mutual aid has attained the greatest development, are invariably the most numerous, the most prosperous, and the most open to further progress.\n\nApplying this concept to human society, Kropotkin presented mutual aid as one of the dominant factors of evolution, the other being self-assertion, and concluded that In the practice of mutual aid, which we can retrace to the earliest beginnings of evolution, we thus find the positive and undoubted origin of our ethical conceptions; and we can affirm that in the ethical progress of man, mutual support not mutual struggle – has had the leading part. In its wide extension, even at the present time, we also see the best guarantee of a still loftier evolution of our race.\n\nHerbert Spencer first used the phrase – after reading Charles Darwin's \"On the Origin of Species\" – in his \"Principles of Biology\" of 1864 in which he drew parallels between his economic theories and Darwin's biological, evolutionary ones, writing, \"This survival of the fittest, which I have here sought to express in mechanical terms, is that which Mr. Darwin has called 'natural selection', or the preservation of favored races in the struggle for life.\"\n\nIn July 1866 Alfred Russel Wallace wrote to Darwin about readers thinking that the phrase \"natural selection\" personified nature as \"selecting\", and said this misconception could be avoided \"by adopting Spencer's term\" \"Survival of the fittest\". Darwin promptly replied that Wallace's letter was \"as clear as daylight. I fully agree with all that you say on the advantages of H. Spencer's excellent expression of 'the survival of the fittest'. This however had not occurred to me till reading your letter. It is, however, a great objection to this term that it cannot be used as a substantive governing a verb\". Had he received the letter two months earlier, he would have worked the phrase into the fourth edition of the \"Origin\" which was then being printed, and he would use it in his \"next book on Domestic Animals etc.\".\n\nDarwin wrote on page 6 of \"The Variation of Animals and Plants under Domestication\" published in 1868, \"This preservation, during the battle for life, of varieties which possess any advantage in structure, constitution, or instinct, I have called Natural Selection; and Mr. Herbert Spencer has well expressed the same idea by the Survival of the Fittest. The term \"natural selection\" is in some respects a bad one, as it seems to imply conscious choice; but this will be disregarded after a little familiarity\". He defended his analogy as similar to language used in chemistry, and to astronomers depicting the \"attraction of gravity as ruling the movements of the planets\", or the way in which \"agriculturists speak of man making domestic races by his power of selection\". He had \"often personified the word Nature; for I have found it difficult to avoid this ambiguity; but I mean by nature only the aggregate action and product of many natural laws,—and by laws only the ascertained sequence of events.\"\n\nIn the first four editions of \"On the Origin of Species\", Darwin had used the phrase \"natural selection\". \nIn Chapter 4 of the 5th edition of \"The Origin\" published in 1869, Darwin implies again the synonym: \"Natural Selection, or the Survival of the Fittest\". By the word \"fittest\" Darwin meant \"better adapted for immediate, local environment\", not the common modern meaning of \"in the best physical shape\" (think of a puzzle piece, not an athlete). In the introduction he gave full credit to Spencer, writing \"I have called this principle, by which each slight variation, if useful, is preserved, by the term Natural Selection, in order to mark its relation to man's power of selection. But the expression often used by Mr. Herbert Spencer of the Survival of the Fittest is more accurate, and is sometimes equally convenient.\"\n\nIn \"The Man Versus The State\", Spencer used the phrase in a postscript to justify a plausible explanation of how his theories would not be adopted by \"societies of militant type\". He uses the term in the context of societies at war, and the form of his reference suggests that he is applying a general principle.\n\nThough Spencer’s conception of organic evolution is commonly interpreted as a form of Lamarckism, Herbert Spencer is sometimes credited with inaugurating Social Darwinism. The phrase \"survival of the fittest\" has become widely used in popular literature as a catchphrase for any topic related or analogous to evolution and natural selection. It has thus been applied to principles of unrestrained competition, and it has been used extensively by both proponents and opponents of Social Darwinism.\n\nEvolutionary biologists criticise the manner in which the term is used by non-scientists and the connotations that have grown around the term in popular culture. The phrase also does not help in conveying the complex nature of natural selection, so modern biologists prefer and almost exclusively use the term natural selection. The biological concept of fitness refers to reproductive success, as opposed to survival, and is not explicit in the specific ways in which organisms can be more \"fit\" (increase reproductive success) as having phenotypic characteristics that enhance survival and reproduction (which was the meaning that Spencer had in mind).\n\n\"Survival of the fittest\" is sometimes claimed to be a tautology. The reasoning is that if one takes the term \"fit\" to mean \"endowed with phenotypic characteristics which improve chances of survival and reproduction\" (which is roughly how Spencer understood it), then \"survival of the fittest\" can simply be rewritten as \"survival of those who are better equipped for surviving\". Furthermore, the expression \"does\" become a tautology if one uses the most widely accepted definition of \"fitness\" in modern biology, namely reproductive success itself (rather than any set of characters conducive to this reproductive success). This reasoning is sometimes used to claim that Darwin's entire theory of evolution by natural selection is fundamentally tautological, and therefore devoid of any explanatory power.\n\nHowever, the expression \"survival of the fittest\" (taken on its own and out of context) gives a very incomplete account of the mechanism of natural selection. The reason is that it does not mention a key requirement for natural selection, namely the requirement of \"heritability\". It is true that the phrase \"survival of the fittest\", in and by itself, is a tautology if fitness is defined by survival and reproduction. Natural selection is the portion of variation in reproductive success that is caused by \"heritable\" characters (see the article on natural selection).\n\nIf certain heritable characters increase or decrease the chances of survival and reproduction of their bearers, then it follows mechanically (by definition of \"heritable\") that those characters that improve survival and reproduction will increase in frequency over generations. This is precisely what is called \"evolution by natural selection.\" On the other hand, if the characters which lead to differential reproductive success are not heritable, then no meaningful evolution will occur, \"survival of the fittest\" or not: if improvement in reproductive success is caused by traits that are not heritable, then there is no reason why these traits should increase in frequency over generations. In other words, natural selection does not simply state that \"survivors survive\" or \"reproducers reproduce\"; rather, it states that \"survivors survive, reproduce and \"therefore\" propagate any \"heritable\" characters which have affected their survival and reproductive success\". This statement is not tautological: it hinges on the testable hypothesis that such fitness-impacting heritable variations actually exist (a hypothesis that has been amply confirmed.)\n\nMomme von Sydow suggested further definitions of 'survival of the fittest' that may yield a testable meaning in biology and also in other areas where Darwinian processes have been influential. However, much care would be needed to disentangle tautological from testable aspects. Moreover, an \"implicit shifting between a testable and an untestable interpretation can be an illicit tactic to immunize natural selection [...] while conveying the impression that one is concerned with testable hypotheses.\"\n\nSkeptic Society founder and \"Skeptic\" magazine publisher Michael Shermer addresses the tautology problem in his 1997 book, \"Why People Believe Weird Things\", in which he points out that although tautologies are sometimes the beginning of science, they are never the end, and that scientific principles like natural selection are testable and falsifiable by virtue of their predictive power. Shermer points out, as an example, that population genetics accurately demonstrate when natural selection will and will not effect change on a population. Shermer hypothesizes that if hominid fossils were found in the same geological strata as trilobites, it would be evidence against natural selection.\n\n\n\n\n",
    "id": "627964",
    "title": "Survival of the fittest"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=12327097",
    "text": "The Complete Works of Charles Darwin Online\n\nThe Complete Work of Charles Darwin Online (or Darwin Online) is a freely-accessible website containing the complete print and manuscript works of Charles Darwin, as well as related supplementary material.\n\nDarwin Online is a research project and website based at the National University of Singapore. It aims to provide all available print and manuscript material except unpublished letters, which are being made available separately by the Darwin Correspondence Project. In addition Darwin Online includes the largest bibliographical list of Darwin's publications and the largest union catalogue of Darwin papers and manuscripts worldwide. The site also provides an extensive collection of related materials such as reviews of Darwin's books, descriptions of his Beagle specimens, obituaries and recollections and works cited or read by Darwin. There is also general history and commentary—some from published sources and some prepared for the project. \n\nThe project began in 2002 and first produced a pilot website, \"The writings of Charles Darwin on the web\", replaced in October 2006 by the new website. The launch was widely reported, and the project has been widely reviewed by professional publications in biology, and librarianship.\n\nThe site contains at least one copy of all known Darwin publications, both as searchable text (98,000 pages) and full colour images (80,000 images). Most of this material was not previously available on the internet – or never even reproduced in any form. New items are still being found and added, along with further editions, translations and transcriptions of Darwin's handwritten papers. \nIn April 2008 Darwin's private papers were launched. The event marked the largest release of new materials by and about Darwin ever published. The collection covers c. 20,000 items across c. 90,000 electronic images. One notable item is the Diary of Emma Darwin (1808–1896), Darwin's wife.\n\nThe site is accessible open access free of charge. It is copyright, with permission for non-commercial use.\n\nThe project was privately funded by the founder and Director, John van Wyhe, from 2002-5. From 2005-8 the project was sponsored by the United Kingdom Arts and Humanities Research Council. The principal grantees John van Wyhe of Christ's College. University of Cambridge is Director of the project; James Secord of the Department of History and Philosophy of Science, University of Cambridge and Janet Browne of the Wellcome Trust Centre for the History of Medicine, University College London were the principal investigators for the grant. The project is now supported by private donation.\n\n",
    "id": "12327097",
    "title": "The Complete Works of Charles Darwin Online"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=19191080",
    "text": "Probabilistic prognosis\n\nProbabilistic prognosis means the anticipation of future events based on a probabilistic structure of past experiences of individual and present situations. Past experiences and present situations help to create hypothesis about the forthcoming future and attributes certain probabilities to them. According to probabilistic prognosis, the preparation of an individual occurs to corresponding actions.\n\nThis ability is the result of biological evolution in a probabilistically organized environment. The prognosis of living creatures optimizes the results of their actions, and therefore they are adequate, exactly to those variable characteristics of environment on which the success of action – satisfaction of needs, achieving goals, depends on.\n\nUnder some pathological conditions, such as schizophrenia, local damage of the brain, the probabilistic prognosis mechanism can be disturbed. The theory of probabilistic prognosis originated as a continuation of Nikolai Bernstein’s ideas stated in his work “Essays of the Physiology of Movement and Physiology of Activity” (1966)\nA new direction of studies of probabilistic prognosis started with works of Professor J.Feigenberg(1969), Professor M.Tsiskaridze (1969), and Preofessor V.Ivannikov(1971), and later were widely explored by other scientists. Studies of probabilistic prognosis remain topical until now and have many followers.\n\n",
    "id": "19191080",
    "title": "Probabilistic prognosis"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=9236",
    "text": "Evolution\n\nEvolution is change in the heritable characteristics of biological populations over successive generations. Evolutionary processes give rise to biodiversity at every level of biological organisation, including the levels of species, individual organisms, and molecules.\n\nRepeated formation of new species (speciation), change within species (anagenesis), and loss of species (extinction) throughout the evolutionary history of life on Earth are demonstrated by shared sets of morphological and biochemical traits, including shared DNA sequences. These shared traits are more similar among species that share a more recent common ancestor, and can be used to reconstruct a biological \"tree of life\" based on evolutionary relationships (phylogenetics), using both existing species and fossils. The fossil record includes a progression from early biogenic graphite, to microbial mat fossils, to fossilised multicellular organisms. Existing patterns of biodiversity have been shaped both by speciation and by extinction.\n\nIn the mid-19th century, Charles Darwin formulated the scientific theory of evolution by natural selection, published in his book \"On the Origin of Species\" (1859). Evolution by natural selection is a process demonstrated by the observation that more offspring are produced than can possibly survive, along with three facts about populations: 1) traits vary among individuals with respect to morphology, physiology, and behaviour (phenotypic variation), 2) different traits confer different rates of survival and reproduction (differential fitness), and 3) traits can be passed from generation to generation (heritability of fitness). Thus, in successive generations members of a population are replaced by progeny of parents better adapted to survive and reproduce in the biophysical environment in which natural selection takes place.\n\nThis teleonomy is the quality whereby the process of natural selection creates and preserves traits that are seemingly fitted for the functional roles they perform. The processes by which the changes occur, from one generation to another, are called evolutionary processes or mechanisms. The four most widely recognised evolutionary processes are natural selection (including sexual selection), genetic drift, mutation and gene migration due to genetic admixture. Natural selection and genetic drift sort variation; mutation and gene migration create variation.\n\nConsequences of selection can include meiotic drive (unequal transmission of certain alleles), nonrandom mating and genetic hitchhiking.\nIn the early 20th century the modern evolutionary synthesis integrated classical genetics with Darwin's theory of evolution by natural selection through the discipline of population genetics. The importance of natural selection as a cause of evolution was accepted into other branches of biology. Moreover, previously held notions about evolution, such as orthogenesis, evolutionism, and other beliefs about innate \"progress\" within the largest-scale trends in evolution, became obsolete. Scientists continue to study various aspects of evolutionary biology by forming and testing hypotheses, constructing mathematical models of theoretical biology and biological theories, using observational data, and performing experiments in both the field and the laboratory.\n\nAll life on Earth shares a common ancestor known as the last universal common ancestor (LUCA), which lived approximately 3.5–3.8 billion years ago. A December 2017 report stated that 3.45 billion year old Australian rocks once contained microorganisms, the earliest direct evidence of life on Earth. Nonetheless, this should not be assumed to be the first living organism on Earth; a study in 2015 found \"remains of biotic life\" from 4.1 billion years ago in ancient rocks in Western Australia. In July 2016, scientists reported identifying a set of 355 genes from the LUCA of all organisms living on Earth. More than 99 percent of all species that ever lived on Earth are estimated to be extinct. Estimates of Earth's current species range from 10 to 14 million, of which about 1.9 million are estimated to have been named and 1.6 million documented in a central database to date. More recently, in May 2016, scientists reported that 1 trillion species are estimated to be on Earth currently with only one-thousandth of one percent described.\n\nIn terms of practical application, an understanding of evolution has been instrumental to developments in numerous scientific and industrial fields, including agriculture, human and veterinary medicine, and the life sciences in general. Discoveries in evolutionary biology have made a significant impact not just in the traditional branches of biology but also in other academic disciplines, including biological anthropology, and evolutionary psychology. Evolutionary computation, a sub-field of artificial intelligence, involves the application of Darwinian principles to problems in computer science.\n\nThe proposal that one type of organism could descend from another type goes back to some of the first pre-Socratic Greek philosophers, such as Anaximander and Empedocles. Such proposals survived into Roman times. The poet and philosopher Lucretius followed Empedocles in his masterwork \"De rerum natura\" (\"On the Nature of Things\").\n\nIn contrast to these materialistic views, Aristotelianism considered all natural things as actualisations of fixed natural possibilities, known as forms. This was part of a medieval teleological understanding of nature in which all things have an intended role to play in a divine cosmic order. Variations of this idea became the standard understanding of the Middle Ages and were integrated into Christian learning, but Aristotle did not demand that real types of organisms always correspond one-for-one with exact metaphysical forms and specifically gave examples of how new types of living things could come to be.\n\nIn the 17th century, the new method of modern science rejected the Aristotelian approach. It sought explanations of natural phenomena in terms of physical laws that were the same for all visible things and that did not require the existence of any fixed natural categories or divine cosmic order. However, this new approach was slow to take root in the biological sciences, the last bastion of the concept of fixed natural types. John Ray applied one of the previously more general terms for fixed natural types, \"species,\" to plant and animal types, but he strictly identified each type of living thing as a species and proposed that each species could be defined by the features that perpetuated themselves generation after generation. The biological classification introduced by Carl Linnaeus in 1735 explicitly recognised the hierarchical nature of species relationships, but still viewed species as fixed according to a divine plan.\n\nOther naturalists of this time speculated on the evolutionary change of species over time according to natural laws. In 1751, Pierre Louis Maupertuis wrote of natural modifications occurring during reproduction and accumulating over many generations to produce new species. Georges-Louis Leclerc, Comte de Buffon suggested that species could degenerate into different organisms, and Erasmus Darwin proposed that all warm-blooded animals could have descended from a single microorganism (or \"filament\"). The first full-fledged evolutionary scheme was Jean-Baptiste Lamarck's \"transmutation\" theory of 1809, which envisaged spontaneous generation continually producing simple forms of life that developed greater complexity in parallel lineages with an inherent progressive tendency, and postulated that on a local level these lineages adapted to the environment by inheriting changes caused by their use or disuse in parents. (The latter process was later called Lamarckism.) These ideas were condemned by established naturalists as speculation lacking empirical support. In particular, Georges Cuvier insisted that species were unrelated and fixed, their similarities reflecting divine design for functional needs. In the meantime, Ray's ideas of benevolent design had been developed by William Paley into the \"Natural Theology or Evidences of the Existence and Attributes of the Deity\" (1802), which proposed complex adaptations as evidence of divine design and which was admired by Charles Darwin.\n\nThe crucial break from the concept of constant typological classes or types in biology came with the theory of evolution through natural selection, which was formulated by Charles Darwin in terms of variable populations. Partly influenced by \"An Essay on the Principle of Population\" (1798) by Thomas Robert Malthus, Darwin noted that population growth would lead to a \"struggle for existence\" in which favorable variations prevailed as others perished. In each generation, many offspring fail to survive to an age of reproduction because of limited resources. This could explain the diversity of plants and animals from a common ancestry through the working of natural laws in the same way for all types of organism. Darwin developed his theory of \"natural selection\" from 1838 onwards and was writing up his \"big book\" on the subject when Alfred Russel Wallace sent him a version of virtually the same theory in 1858. Their separate papers were presented together at an 1858 meeting of the Linnean Society of London. At the end of 1859, Darwin's publication of his \"abstract\" as \"On the Origin of Species\" explained natural selection in detail and in a way that led to an increasingly wide acceptance of Darwin's concepts of evolution at the expense of alternative theories. Thomas Henry Huxley applied Darwin's ideas to humans, using paleontology and comparative anatomy to provide strong evidence that humans and apes shared a common ancestry. Some were disturbed by this since it implied that humans did not have a special place in the universe.\n\nThe mechanisms of reproductive heritability and the origin of new traits remained a mystery. Towards this end, Darwin developed his provisional theory of pangenesis. In 1865, Gregor Mendel reported that traits were inherited in a predictable manner through the independent assortment and segregation of elements (later known as genes). Mendel's laws of inheritance eventually supplanted most of Darwin's pangenesis theory. August Weismann made the important distinction between germ cells that give rise to gametes (such as sperm and egg cells) and the somatic cells of the body, demonstrating that heredity passes through the germ line only. Hugo de Vries connected Darwin's pangenesis theory to Weismann's germ/soma cell distinction and proposed that Darwin's pangenes were concentrated in the cell nucleus and when expressed they could move into the cytoplasm to change the cells structure. De Vries was also one of the researchers who made Mendel's work well-known, believing that Mendelian traits corresponded to the transfer of heritable variations along the germline. To explain how new variants originate, de Vries developed a mutation theory that led to a temporary rift between those who accepted Darwinian evolution and biometricians who allied with de Vries. In the 1930s, pioneers in the field of population genetics, such as Ronald Fisher, Sewall Wright and J. B. S. Haldane set the foundations of evolution onto a robust statistical philosophy. The false contradiction between Darwin's theory, genetic mutations, and Mendelian inheritance was thus reconciled.\n\nIn the 1920s and 1930s the so-called modern synthesis connected natural selection and population genetics, based on Mendelian inheritance, into a unified theory that applied generally to any branch of biology. The modern synthesis explained patterns observed across species in populations, through fossil transitions in palaeontology, and complex cellular mechanisms in developmental biology. The publication of the structure of DNA by James Watson and Francis Crick in 1953 demonstrated a physical mechanism for inheritance. Molecular biology improved our understanding of the relationship between genotype and phenotype. Advancements were also made in phylogenetic systematics, mapping the transition of traits into a comparative and testable framework through the publication and use of evolutionary trees. In 1973, evolutionary biologist Theodosius Dobzhansky penned that \"nothing in biology makes sense except in the light of evolution,\" because it has brought to light the relations of what first seemed disjointed facts in natural history into a coherent explanatory body of knowledge that describes and predicts many observable facts about life on this planet.\n\nSince then, the modern synthesis has been further extended to explain biological phenomena across the full and integrative scale of the biological hierarchy, from genes to species. One extension, known as evolutionary developmental biology and informally called \"evo-devo,\" emphasises how changes between generations (evolution) acts on patterns of change within individual organisms (development). Since the beginning of the 21st century and in light of discoveries made in recent decades, some biologists have argued for an extended evolutionary synthesis, which would account for the effects of non-genetic inheritance modes, such as epigenetics, parental effects, ecological and cultural inheritance, and evolvability.\n\nEvolution in organisms occurs through changes in heritable traits—the inherited characteristics of an organism. In humans, for example, eye colour is an inherited characteristic and an individual might inherit the \"brown-eye trait\" from one of their parents. Inherited traits are controlled by genes and the complete set of genes within an organism's genome (genetic material) is called its genotype.\n\nThe complete set of observable traits that make up the structure and behaviour of an organism is called its phenotype. These traits come from the interaction of its genotype with the environment. As a result, many aspects of an organism's phenotype are not inherited. For example, suntanned skin comes from the interaction between a person's genotype and sunlight; thus, suntans are not passed on to people's children. However, some people tan more easily than others, due to differences in genotypic variation; a striking example are people with the inherited trait of albinism, who do not tan at all and are very sensitive to sunburn.\n\nHeritable traits are passed from one generation to the next via DNA, a molecule that encodes genetic information. DNA is a long biopolymer composed of four types of bases. The sequence of bases along a particular DNA molecule specify the genetic information, in a manner similar to a sequence of letters spelling out a sentence. Before a cell divides, the DNA is copied, so that each of the resulting two cells will inherit the DNA sequence. Portions of a DNA molecule that specify a single functional unit are called genes; different genes have different sequences of bases. Within cells, the long strands of DNA form condensed structures called chromosomes. The specific location of a DNA sequence within a chromosome is known as a locus. If the DNA sequence at a locus varies between individuals, the different forms of this sequence are called alleles. DNA sequences can change through mutations, producing new alleles. If a mutation occurs within a gene, the new allele may affect the trait that the gene controls, altering the phenotype of the organism. However, while this simple correspondence between an allele and a trait works in some cases, most traits are more complex and are controlled by quantitative trait loci (multiple interacting genes).\n\nRecent findings have confirmed important examples of heritable changes that cannot be explained by changes to the sequence of nucleotides in the DNA. These phenomena are classed as epigenetic inheritance systems. DNA methylation marking chromatin, self-sustaining metabolic loops, gene silencing by RNA interference and the three-dimensional conformation of proteins (such as prions) are areas where epigenetic inheritance systems have been discovered at the organismic level. Developmental biologists suggest that complex interactions in genetic networks and communication among cells can lead to heritable variations that may underlay some of the mechanics in developmental plasticity and canalisation. Heritability may also occur at even larger scales. For example, ecological inheritance through the process of niche construction is defined by the regular and repeated activities of organisms in their environment. This generates a legacy of effects that modify and feed back into the selection regime of subsequent generations. Descendants inherit genes plus environmental characteristics generated by the ecological actions of ancestors. Other examples of heritability in evolution that are not under the direct control of genes include the inheritance of cultural traits and symbiogenesis.\n\nAn individual organism's phenotype results from both its genotype and the influence from the environment it has lived in. A substantial part of the phenotypic variation in a population is caused by genotypic variation. The modern evolutionary synthesis defines evolution as the change over time in this genetic variation. The frequency of one particular allele will become more or less prevalent relative to other forms of that gene. Variation disappears when a new allele reaches the point of fixation—when it either disappears from the population or replaces the ancestral allele entirely.\n\nNatural selection will only cause evolution if there is enough genetic variation in a population. Before the discovery of Mendelian genetics, one common hypothesis was blending inheritance. But with blending inheritance, genetic variance would be rapidly lost, making evolution by natural selection implausible. The Hardy–Weinberg principle provides the solution to how variation is maintained in a population with Mendelian inheritance. The frequencies of alleles (variations in a gene) will remain constant in the absence of selection, mutation, migration and genetic drift.\n\nVariation comes from mutations in the genome, reshuffling of genes through sexual reproduction and migration between populations (gene flow). Despite the constant introduction of new variation through mutation and gene flow, most of the genome of a species is identical in all individuals of that species. However, even relatively small differences in genotype can lead to dramatic differences in phenotype: for example, chimpanzees and humans differ in only about 5% of their genomes.\n\nMutations are changes in the DNA sequence of a cell's genome. When mutations occur, they may alter the product of a gene, or prevent the gene from functioning, or have no effect. Based on studies in the fly \"Drosophila melanogaster\", it has been suggested that if a mutation changes a protein produced by a gene, this will probably be harmful, with about 70% of these mutations having damaging effects, and the remainder being either neutral or weakly beneficial.\n\nMutations can involve large sections of a chromosome becoming duplicated (usually by genetic recombination), which can introduce extra copies of a gene into a genome. Extra copies of genes are a major source of the raw material needed for new genes to evolve. This is important because most new genes evolve within gene families from pre-existing genes that share common ancestors. For example, the human eye uses four genes to make structures that sense light: three for colour vision and one for night vision; all four are descended from a single ancestral gene.\n\nNew genes can be generated from an ancestral gene when a duplicate copy mutates and acquires a new function. This process is easier once a gene has been duplicated because it increases the redundancy of the system; one gene in the pair can acquire a new function while the other copy continues to perform its original function. Other types of mutations can even generate entirely new genes from previously noncoding DNA.\n\nThe generation of new genes can also involve small parts of several genes being duplicated, with these fragments then recombining to form new combinations with new functions. When new genes are assembled from shuffling pre-existing parts, domains act as modules with simple independent functions, which can be mixed together to produce new combinations with new and complex functions. For example, polyketide synthases are large enzymes that make antibiotics; they contain up to one hundred independent domains that each catalyse one step in the overall process, like a step in an assembly line.\n\nIn asexual organisms, genes are inherited together, or \"linked\", as they cannot mix with genes of other organisms during reproduction. In contrast, the offspring of sexual organisms contain random mixtures of their parents' chromosomes that are produced through independent assortment. In a related process called homologous recombination, sexual organisms exchange DNA between two matching chromosomes. Recombination and reassortment do not alter allele frequencies, but instead change which alleles are associated with each other, producing offspring with new combinations of alleles. Sex usually increases genetic variation and may increase the rate of evolution.\n\nThe two-fold cost of sex was first described by John Maynard Smith. The first cost is that in sexually dimorphic species only one of the two sexes can bear young. (This cost does not apply to hermaphroditic species, like most plants and many invertebrates.) The second cost is that any individual who reproduces sexually can only pass on 50% of its genes to any individual offspring, with even less passed on as each new generation passes. Yet sexual reproduction is the more common means of reproduction among eukaryotes and multicellular organisms. The Red Queen hypothesis has been used to explain the significance of sexual reproduction as a means to enable continual evolution and adaptation in response to coevolution with other species in an ever-changing environment.\n\nGene flow is the exchange of genes between populations and between species. It can therefore be a source of variation that is new to a population or to a species. Gene flow can be caused by the movement of individuals between separate populations of organisms, as might be caused by the movement of mice between inland and coastal populations, or the movement of pollen between heavy metal tolerant and heavy metal sensitive populations of grasses.\n\nGene transfer between species includes the formation of hybrid organisms and horizontal gene transfer. Horizontal gene transfer is the transfer of genetic material from one organism to another organism that is not its offspring; this is most common among bacteria. In medicine, this contributes to the spread of antibiotic resistance, as when one bacteria acquires resistance genes it can rapidly transfer them to other species. Horizontal transfer of genes from bacteria to eukaryotes such as the yeast \"Saccharomyces cerevisiae\" and the adzuki bean weevil \"Callosobruchus chinensis\" has occurred. An example of larger-scale transfers are the eukaryotic bdelloid rotifers, which have received a range of genes from bacteria, fungi and plants. Viruses can also carry DNA between organisms, allowing transfer of genes even across biological domains.\n\nLarge-scale gene transfer has also occurred between the ancestors of eukaryotic cells and bacteria, during the acquisition of chloroplasts and mitochondria. It is possible that eukaryotes themselves originated from horizontal gene transfers between bacteria and archaea.\n\nFrom a Neo-Darwinian perspective, evolution occurs when there are changes in the frequencies of alleles within a population of interbreeding organisms. For example, the allele for black colour in a population of moths becoming more common. Mechanisms that can lead to changes in allele frequencies include natural selection, genetic drift, genetic hitchhiking, mutation and gene flow.\n\nEvolution by means of natural selection is the process by which traits that enhance survival and reproduction become more common in successive generations of a population. It has often been called a \"self-evident\" mechanism because it necessarily follows from three simple facts:\n\nMore offspring are produced than can possibly survive, and these conditions produce competition between organisms for survival and reproduction. Consequently, organisms with traits that give them an advantage over their competitors are more likely to pass on their traits to the next generation than those with traits that do not confer an advantage.\n\nThe central concept of natural selection is the evolutionary fitness of an organism. Fitness is measured by an organism's ability to survive and reproduce, which determines the size of its genetic contribution to the next generation. However, fitness is not the same as the total number of offspring: instead fitness is indicated by the proportion of subsequent generations that carry an organism's genes. For example, if an organism could survive well and reproduce rapidly, but its offspring were all too small and weak to survive, this organism would make little genetic contribution to future generations and would thus have low fitness.\n\nIf an allele increases fitness more than the other alleles of that gene, then with each generation this allele will become more common within the population. These traits are said to be \"selected \"for\".\" Examples of traits that can increase fitness are enhanced survival and increased fecundity. Conversely, the lower fitness caused by having a less beneficial or deleterious allele results in this allele becoming rarer—they are \"selected \"against\".\" Importantly, the fitness of an allele is not a fixed characteristic; if the environment changes, previously neutral or harmful traits may become beneficial and previously beneficial traits become harmful. However, even if the direction of selection does reverse in this way, traits that were lost in the past may not re-evolve in an identical form (see Dollo's law). However, a re-activation of dormant genes, as long as they have not been eliminated from the genome and were only suppressed perhaps for hundreds of generations, can lead to the re-occurrence of traits thought to be lost like hindlegs in dolphins, teeth in chickens, wings in wingless stick insects, tails and additional nipples in humans etc. \"Throwbacks\" such as these are known as atavisms.\n\nNatural selection within a population for a trait that can vary across a range of values, such as height, can be categorised into three different types. The first is directional selection, which is a shift in the average value of a trait over time—for example, organisms slowly getting taller. Secondly, disruptive selection is selection for extreme trait values and often results in two different values becoming most common, with selection against the average value. This would be when either short or tall organisms had an advantage, but not those of medium height. Finally, in stabilising selection there is selection against extreme trait values on both ends, which causes a decrease in variance around the average value and less diversity. This would, for example, cause organisms to eventually have a similar height.\n\nA special case of natural selection is sexual selection, which is selection for any trait that increases mating success by increasing the attractiveness of an organism to potential mates. Traits that evolved through sexual selection are particularly prominent among males of several animal species. Although sexually favoured, traits such as cumbersome antlers, mating calls, large body size and bright colours often attract predation, which compromises the survival of individual males. This survival disadvantage is balanced by higher reproductive success in males that show these hard-to-fake, sexually selected traits.\n\nNatural selection most generally makes nature the measure against which individuals and individual traits, are more or less likely to survive. \"Nature\" in this sense refers to an ecosystem, that is, a system in which organisms interact with every other element, physical as well as biological, in their local environment. Eugene Odum, a founder of ecology, defined an ecosystem as: \"Any unit that includes all of the organisms...in a given area interacting with the physical environment so that a flow of energy leads to clearly defined trophic structure, biotic diversity and material cycles (ie: exchange of materials between living and nonliving parts) within the system.\" Each population within an ecosystem occupies a distinct niche, or position, with distinct relationships to other parts of the system. These relationships involve the life history of the organism, its position in the food chain and its geographic range. This broad understanding of nature enables scientists to delineate specific forces which, together, comprise natural selection.\n\nNatural selection can act at different levels of organisation, such as genes, cells, individual organisms, groups of organisms and species. Selection can act at multiple levels simultaneously. An example of selection occurring below the level of the individual organism are genes called transposons, which can replicate and spread throughout a genome. Selection at a level above the individual, such as group selection, may allow the evolution of cooperation, as discussed below.\n\nIn addition to being a major source of variation, mutation may also function as a mechanism of evolution when there are different probabilities at the molecular level for different mutations to occur, a process known as mutation bias. If two genotypes, for example one with the nucleotide G and another with the nucleotide A in the same position, have the same fitness, but mutation from G to A happens more often than mutation from A to G, then genotypes with A will tend to evolve. Different insertion vs. deletion mutation biases in different taxa can lead to the evolution of different genome sizes. Developmental or mutational biases have also been observed in morphological evolution. For example, according to the phenotype-first theory of evolution, mutations can eventually cause the genetic assimilation of traits that were previously induced by the environment.\n\nMutation bias effects are superimposed on other processes. If selection would favor either one out of two mutations, but there is no extra advantage to having both, then the mutation that occurs the most frequently is the one that is most likely to become fixed in a population. Mutations leading to the loss of function of a gene are much more common than mutations that produce a new, fully functional gene. Most loss of function mutations are selected against. But when selection is weak, mutation bias towards loss of function can affect evolution. For example, pigments are no longer useful when animals live in the darkness of caves, and tend to be lost. This kind of loss of function can occur because of mutation bias, and/or because the function had a cost, and once the benefit of the function disappeared, natural selection leads to the loss. Loss of sporulation ability in \"Bacillus subtilis\" during laboratory evolution appears to have been caused by mutation bias, rather than natural selection against the cost of maintaining sporulation ability. When there is no selection for loss of function, the speed at which loss evolves depends more on the mutation rate than it does on the effective population size, indicating that it is driven more by mutation bias than by genetic drift. In parasitic organisms, mutation bias leads to selection pressures as seen in Ehrlichia. Mutations are biased towards antigenic variants in outer-membrane proteins.\n\nGenetic drift is the change in allele frequency from one generation to the next that occurs because alleles are subject to sampling error. As a result, when selective forces are absent or relatively weak, allele frequencies tend to \"drift\" upward or downward randomly (in a random walk). This drift halts when an allele eventually becomes fixed, either by disappearing from the population, or replacing the other alleles entirely. Genetic drift may therefore eliminate some alleles from a population due to chance alone. Even in the absence of selective forces, genetic drift can cause two separate populations that began with the same genetic structure to drift apart into two divergent populations with different sets of alleles.\n\nIt is usually difficult to measure the relative importance of selection and neutral processes, including drift. The comparative importance of adaptive and non-adaptive forces in driving evolutionary change is an area of current research.\n\nThe neutral theory of molecular evolution proposed that most evolutionary changes are the result of the fixation of neutral mutations by genetic drift. Hence, in this model, most genetic changes in a population are the result of constant mutation pressure and genetic drift. This form of the neutral theory is now largely abandoned, since it does not seem to fit the genetic variation seen in nature. However, a more recent and better-supported version of this model is the nearly neutral theory, where a mutation that would be effectively neutral in a small population is not necessarily neutral in a large population. Other alternative theories propose that genetic drift is dwarfed by other stochastic forces in evolution, such as genetic hitchhiking, also known as genetic draft.\n\nThe time for a neutral allele to become fixed by genetic drift depends on population size, with fixation occurring more rapidly in smaller populations. The number of individuals in a population is not critical, but instead a measure known as the effective population size. The effective population is usually smaller than the total population since it takes into account factors such as the level of inbreeding and the stage of the lifecycle in which the population is the smallest. The effective population size may not be the same for every gene in the same population.\n\nRecombination allows alleles on the same strand of DNA to become separated. However, the rate of recombination is low (approximately two events per chromosome per generation). As a result, genes close together on a chromosome may not always be shuffled away from each other and genes that are close together tend to be inherited together, a phenomenon known as linkage. This tendency is measured by finding how often two alleles occur together on a single chromosome compared to expectations, which is called their linkage disequilibrium. A set of alleles that is usually inherited in a group is called a haplotype. This can be important when one allele in a particular haplotype is strongly beneficial: natural selection can drive a selective sweep that will also cause the other alleles in the haplotype to become more common in the population; this effect is called genetic hitchhiking or genetic draft. Genetic draft caused by the fact that some neutral genes are genetically linked to others that are under selection can be partially captured by an appropriate effective population size.\n\nGene flow involves the exchange of genes between populations and between species. The presence or absence of gene flow fundamentally changes the course of evolution. Due to the complexity of organisms, any two completely isolated populations will eventually evolve genetic incompatibilities through neutral processes, as in the Bateson-Dobzhansky-Muller model, even if both populations remain essentially identical in terms of their adaptation to the environment.\n\nIf genetic differentiation between populations develops, gene flow between populations can introduce traits or alleles which are disadvantageous in the local population and this may lead to organisms within these populations evolving mechanisms that prevent mating with genetically distant populations, eventually resulting in the appearance of new species. Thus, exchange of genetic information between individuals is fundamentally important for the development of the biological species concept.\n\nDuring the development of the modern synthesis, Sewall Wright developed his shifting balance theory, which regarded gene flow between partially isolated populations as an important aspect of adaptive evolution. However, recently there has been substantial criticism of the importance of the shifting balance theory.\n\nEvolution influences every aspect of the form and behaviour of organisms. Most prominent are the specific behavioural and physical adaptations that are the outcome of natural selection. These adaptations increase fitness by aiding activities such as finding food, avoiding predators or attracting mates. Organisms can also respond to selection by cooperating with each other, usually by aiding their relatives or engaging in mutually beneficial symbiosis. In the longer term, evolution produces new species through splitting ancestral populations of organisms into new groups that cannot or will not interbreed.\n\nThese outcomes of evolution are distinguished based on time scale as macroevolution versus microevolution. Macroevolution refers to evolution that occurs at or above the level of species, in particular speciation and extinction; whereas microevolution refers to smaller evolutionary changes within a species or population, in particular shifts in gene frequency and adaptation. In general, macroevolution is regarded as the outcome of long periods of microevolution. Thus, the distinction between micro- and macroevolution is not a fundamental one—the difference is simply the time involved. However, in macroevolution, the traits of the entire species may be important. For instance, a large amount of variation among individuals allows a species to rapidly adapt to new habitats, lessening the chance of it going extinct, while a wide geographic range increases the chance of speciation, by making it more likely that part of the population will become isolated. In this sense, microevolution and macroevolution might involve selection at different levels—with microevolution acting on genes and organisms, versus macroevolutionary processes such as species selection acting on entire species and affecting their rates of speciation and extinction.\n\nA common misconception is that evolution has goals, long-term plans, or an innate tendency for \"progress\", as expressed in beliefs such as orthogenesis and evolutionism; realistically however, evolution has no long-term goal and does not necessarily produce greater complexity. Although complex species have evolved, they occur as a side effect of the overall number of organisms increasing and simple forms of life still remain more common in the biosphere. For example, the overwhelming majority of species are microscopic prokaryotes, which form about half the world's biomass despite their small size, and constitute the vast majority of Earth's biodiversity. Simple organisms have therefore been the dominant form of life on Earth throughout its history and continue to be the main form of life up to the present day, with complex life only appearing more diverse because it is more noticeable. Indeed, the evolution of microorganisms is particularly important to modern evolutionary research, since their rapid reproduction allows the study of experimental evolution and the observation of evolution and adaptation in real time.\n\nAdaptation is the process that makes organisms better suited to their habitat. Also, the term adaptation may refer to a trait that is important for an organism's survival. For example, the adaptation of horses' teeth to the grinding of grass. By using the term \"adaptation\" for the evolutionary process and \"adaptive trait\" for the product (the bodily part or function), the two senses of the word may be distinguished. Adaptations are produced by natural selection. The following definitions are due to Theodosius Dobzhansky:\n\nAdaptation may cause either the gain of a new feature, or the loss of an ancestral feature. An example that shows both types of change is bacterial adaptation to antibiotic selection, with genetic changes causing antibiotic resistance by both modifying the target of the drug, or increasing the activity of transporters that pump the drug out of the cell. Other striking examples are the bacteria \"Escherichia coli\" evolving the ability to use citric acid as a nutrient in a long-term laboratory experiment, \"Flavobacterium\" evolving a novel enzyme that allows these bacteria to grow on the by-products of nylon manufacturing, and the soil bacterium \"Sphingobium\" evolving an entirely new metabolic pathway that degrades the synthetic pesticide pentachlorophenol. An interesting but still controversial idea is that some adaptations might increase the ability of organisms to generate genetic diversity and adapt by natural selection (increasing organisms' evolvability).\n\nAdaptation occurs through the gradual modification of existing structures. Consequently, structures with similar internal organisation may have different functions in related organisms. This is the result of a single ancestral structure being adapted to function in different ways. The bones within bat wings, for example, are very similar to those in mice feet and primate hands, due to the descent of all these structures from a common mammalian ancestor. However, since all living organisms are related to some extent, even organs that appear to have little or no structural similarity, such as arthropod, squid and vertebrate eyes, or the limbs and wings of arthropods and vertebrates, can depend on a common set of homologous genes that control their assembly and function; this is called deep homology.\n\nDuring evolution, some structures may lose their original function and become vestigial structures. Such structures may have little or no function in a current species, yet have a clear function in ancestral species, or other closely related species. Examples include pseudogenes, the non-functional remains of eyes in blind cave-dwelling fish, wings in flightless birds, the presence of hip bones in whales and snakes, and sexual traits in organisms that reproduce via asexual reproduction. Examples of vestigial structures in humans include wisdom teeth, the coccyx, the vermiform appendix, and other behavioural vestiges such as goose bumps and primitive reflexes.\n\nHowever, many traits that appear to be simple adaptations are in fact exaptations: structures originally adapted for one function, but which coincidentally became somewhat useful for some other function in the process. One example is the African lizard \"Holaspis guentheri\", which developed an extremely flat head for hiding in crevices, as can be seen by looking at its near relatives. However, in this species, the head has become so flattened that it assists in gliding from tree to tree—an exaptation. Within cells, molecular machines such as the bacterial flagella and protein sorting machinery evolved by the recruitment of several pre-existing proteins that previously had different functions. Another example is the recruitment of enzymes from glycolysis and xenobiotic metabolism to serve as structural proteins called crystallins within the lenses of organisms' eyes.\n\nAn area of current investigation in evolutionary developmental biology is the developmental basis of adaptations and exaptations. This research addresses the origin and evolution of embryonic development and how modifications of development and developmental processes produce novel features. These studies have shown that evolution can alter development to produce new structures, such as embryonic bone structures that develop into the jaw in other animals instead forming part of the middle ear in mammals. It is also possible for structures that have been lost in evolution to reappear due to changes in developmental genes, such as a mutation in chickens causing embryos to grow teeth similar to those of crocodiles. It is now becoming clear that most alterations in the form of organisms are due to changes in a small set of conserved genes.\n\nInteractions between organisms can produce both conflict and cooperation. When the interaction is between pairs of species, such as a pathogen and a host, or a predator and its prey, these species can develop matched sets of adaptations. Here, the evolution of one species causes adaptations in a second species. These changes in the second species then, in turn, cause new adaptations in the first species. This cycle of selection and response is called coevolution. An example is the production of tetrodotoxin in the rough-skinned newt and the evolution of tetrodotoxin resistance in its predator, the common garter snake. In this predator-prey pair, an evolutionary arms race has produced high levels of toxin in the newt and correspondingly high levels of toxin resistance in the snake.\n\nNot all co-evolved interactions between species involve conflict. Many cases of mutually beneficial interactions have evolved. For instance, an extreme cooperation exists between plants and the mycorrhizal fungi that grow on their roots and aid the plant in absorbing nutrients from the soil. This is a reciprocal relationship as the plants provide the fungi with sugars from photosynthesis. Here, the fungi actually grow inside plant cells, allowing them to exchange nutrients with their hosts, while sending signals that suppress the plant immune system.\n\nCoalitions between organisms of the same species have also evolved. An extreme case is the eusociality found in social insects, such as bees, termites and ants, where sterile insects feed and guard the small number of organisms in a colony that are able to reproduce. On an even smaller scale, the somatic cells that make up the body of an animal limit their reproduction so they can maintain a stable organism, which then supports a small number of the animal's germ cells to produce offspring. Here, somatic cells respond to specific signals that instruct them whether to grow, remain as they are, or die. If cells ignore these signals and multiply inappropriately, their uncontrolled growth causes cancer.\n\nSuch cooperation within species may have evolved through the process of kin selection, which is where one organism acts to help raise a relative's offspring. This activity is selected for because if the \"helping\" individual contains alleles which promote the helping activity, it is likely that its kin will \"also\" contain these alleles and thus those alleles will be passed on. Other processes that may promote cooperation include group selection, where cooperation provides benefits to a group of organisms.\n\nSpeciation is the process where a species diverges into two or more descendant species.\n\nThere are multiple ways to define the concept of \"species.\" The choice of definition is dependent on the particularities of the species concerned. For example, some species concepts apply more readily toward sexually reproducing organisms while others lend themselves better toward asexual organisms. Despite the diversity of various species concepts, these various concepts can be placed into one of three broad philosophical approaches: interbreeding, ecological and phylogenetic. The Biological Species Concept (BSC) is a classic example of the interbreeding approach. Defined by Ernst Mayr in 1942, the BSC states that \"species are groups of actually or potentially interbreeding natural populations, which are reproductively isolated from other such groups.\" Despite its wide and long-term use, the BSC like others is not without controversy, for example because these concepts cannot be applied to prokaryotes, and this is called the species problem. Some researchers have attempted a unifying monistic definition of species, while others adopt a pluralistic approach and suggest that there may be different ways to logically interpret the definition of a species.\n\nBarriers to reproduction between two diverging sexual populations are required for the populations to become new species. Gene flow may slow this process by spreading the new genetic variants also to the other populations. Depending on how far two species have diverged since their most recent common ancestor, it may still be possible for them to produce offspring, as with horses and donkeys mating to produce mules. Such hybrids are generally infertile. In this case, closely related species may regularly interbreed, but hybrids will be selected against and the species will remain distinct. However, viable hybrids are occasionally formed and these new species can either have properties intermediate between their parent species, or possess a totally new phenotype. The importance of hybridisation in producing new species of animals is unclear, although cases have been seen in many types of animals, with the gray tree frog being a particularly well-studied example.\n\nSpeciation has been observed multiple times under both controlled laboratory conditions and in nature. In sexually reproducing organisms, speciation results from reproductive isolation followed by genealogical divergence. There are four primary geographic modes of speciation. The most common in animals is allopatric speciation, which occurs in populations initially isolated geographically, such as by habitat fragmentation or migration. Selection under these conditions can produce very rapid changes in the appearance and behaviour of organisms. As selection and drift act independently on populations isolated from the rest of their species, separation may eventually produce organisms that cannot interbreed.\n\nThe second mode of speciation is peripatric speciation, which occurs when small populations of organisms become isolated in a new environment. This differs from allopatric speciation in that the isolated populations are numerically much smaller than the parental population. Here, the founder effect causes rapid speciation after an increase in inbreeding increases selection on homozygotes, leading to rapid genetic change.\n\nThe third mode is parapatric speciation. This is similar to peripatric speciation in that a small population enters a new habitat, but differs in that there is no physical separation between these two populations. Instead, speciation results from the evolution of mechanisms that reduce gene flow between the two populations. Generally this occurs when there has been a drastic change in the environment within the parental species' habitat. One example is the grass \"Anthoxanthum odoratum\", which can undergo parapatric speciation in response to localised metal pollution from mines. Here, plants evolve that have resistance to high levels of metals in the soil. Selection against interbreeding with the metal-sensitive parental population produced a gradual change in the flowering time of the metal-resistant plants, which eventually produced complete reproductive isolation. Selection against hybrids between the two populations may cause reinforcement, which is the evolution of traits that promote mating within a species, as well as character displacement, which is when two species become more distinct in appearance.\n\nFinally, in sympatric speciation species diverge without geographic isolation or changes in habitat. This form is rare since even a small amount of gene flow may remove genetic differences between parts of a population. Generally, sympatric speciation in animals requires the evolution of both genetic differences and non-random mating, to allow reproductive isolation to evolve.\n\nOne type of sympatric speciation involves crossbreeding of two related species to produce a new hybrid species. This is not common in animals as animal hybrids are usually sterile. This is because during meiosis the homologous chromosomes from each parent are from different species and cannot successfully pair. However, it is more common in plants because plants often double their number of chromosomes, to form polyploids. This allows the chromosomes from each parental species to form matching pairs during meiosis, since each parent's chromosomes are represented by a pair already. An example of such a speciation event is when the plant species \"Arabidopsis thaliana\" and \"Arabidopsis arenosa\" crossbred to give the new species \"Arabidopsis suecica\". This happened about 20,000 years ago, and the speciation process has been repeated in the laboratory, which allows the study of the genetic mechanisms involved in this process. Indeed, chromosome doubling within a species may be a common cause of reproductive isolation, as half the doubled chromosomes will be unmatched when breeding with undoubled organisms.\n\nSpeciation events are important in the theory of punctuated equilibrium, which accounts for the pattern in the fossil record of short \"bursts\" of evolution interspersed with relatively long periods of stasis, where species remain relatively unchanged. In this theory, speciation and rapid evolution are linked, with natural selection and genetic drift acting most strongly on organisms undergoing speciation in novel habitats or small populations. As a result, the periods of stasis in the fossil record correspond to the parental population and the organisms undergoing speciation and rapid evolution are found in small populations or geographically restricted habitats and therefore rarely being preserved as fossils.\n\nExtinction is the disappearance of an entire species. Extinction is not an unusual event, as species regularly appear through speciation and disappear through extinction. Nearly all animal and plant species that have lived on Earth are now extinct, and extinction appears to be the ultimate fate of all species. These extinctions have happened continuously throughout the history of life, although the rate of extinction spikes in occasional mass extinction events. The Cretaceous–Paleogene extinction event, during which the non-avian dinosaurs became extinct, is the most well-known, but the earlier Permian–Triassic extinction event was even more severe, with approximately 96% of all marine species driven to extinction. The Holocene extinction event is an ongoing mass extinction associated with humanity's expansion across the globe over the past few thousand years. Present-day extinction rates are 100–1000 times greater than the background rate and up to 30% of current species may be extinct by the mid 21st century. Human activities are now the primary cause of the ongoing extinction event; global warming may further accelerate it in the future.\n\nThe role of extinction in evolution is not very well understood and may depend on which type of extinction is considered. The causes of the continuous \"low-level\" extinction events, which form the majority of extinctions, may be the result of competition between species for limited resources (the competitive exclusion principle). If one species can out-compete another, this could produce species selection, with the fitter species surviving and the other species being driven to extinction. The intermittent mass extinctions are also important, but instead of acting as a selective force, they drastically reduce diversity in a nonspecific manner and promote bursts of rapid evolution and speciation in survivors.\n\nThe Earth is about 4.54 billion years old. The earliest undisputed evidence of life on Earth dates from at least 3.5 billion years ago, during the Eoarchean Era after a geological crust started to solidify following the earlier molten Hadean Eon. Microbial mat fossils have been found in 3.48 billion-year-old sandstone in Western Australia. Other early physical evidence of a biogenic substance is graphite in 3.7 billion-year-old metasedimentary rocks discovered in Western Greenland as well as \"remains of biotic life\" found in 4.1 billion-year-old rocks in Western Australia. According to one of the researchers, \"If life arose relatively quickly on Earth … then it could be common in the universe.\" \n\nMore than 99 percent of all species, amounting to over five billion species, that ever lived on Earth are estimated to be extinct. Estimates on the number of Earth's current species range from 10 million to 14 million, of which about 1.9 million are estimated to have been named and 1.6 million documented in a central database to date, leaving at least 80 percent not yet described.\n\nHighly energetic chemistry is thought to have produced a self-replicating molecule around 4 billion years ago, and half a billion years later the last common ancestor of all life existed. The current scientific consensus is that the complex biochemistry that makes up life came from simpler chemical reactions. The beginning of life may have included self-replicating molecules such as RNA and the assembly of simple cells.\n\nAll organisms on Earth are descended from a common ancestor or ancestral gene pool. Current species are a stage in the process of evolution, with their diversity the product of a long series of speciation and extinction events. The common descent of organisms was first deduced from four simple facts about organisms: First, they have geographic distributions that cannot be explained by local adaptation. Second, the diversity of life is not a set of completely unique organisms, but organisms that share morphological similarities. Third, vestigial traits with no clear purpose resemble functional ancestral traits and finally, that organisms can be classified using these similarities into a hierarchy of nested groups—similar to a family tree. However, modern research has suggested that, due to horizontal gene transfer, this \"tree of life\" may be more complicated than a simple branching tree since some genes have spread independently between distantly related species.\n\nPast species have also left records of their evolutionary history. Fossils, along with the comparative anatomy of present-day organisms, constitute the morphological, or anatomical, record. By comparing the anatomies of both modern and extinct species, paleontologists can infer the lineages of those species. However, this approach is most successful for organisms that had hard body parts, such as shells, bones or teeth. Further, as prokaryotes such as bacteria and archaea share a limited set of common morphologies, their fossils do not provide information on their ancestry.\n\nMore recently, evidence for common descent has come from the study of biochemical similarities between organisms. For example, all living cells use the same basic set of nucleotides and amino acids. The development of molecular genetics has revealed the record of evolution left in organisms' genomes: dating when species diverged through the molecular clock produced by mutations. For example, these DNA sequence comparisons have revealed that humans and chimpanzees share 98% of their genomes and analysing the few areas where they differ helps shed light on when the common ancestor of these species existed.\n\nProkaryotes inhabited the Earth from approximately 3–4 billion years ago. No obvious changes in morphology or cellular organisation occurred in these organisms over the next few billion years. The eukaryotic cells emerged between 1.6–2.7 billion years ago. The next major change in cell structure came when bacteria were engulfed by eukaryotic cells, in a cooperative association called endosymbiosis. The engulfed bacteria and the host cell then underwent coevolution, with the bacteria evolving into either mitochondria or hydrogenosomes. Another engulfment of cyanobacterial-like organisms led to the formation of chloroplasts in algae and plants.\n\nThe history of life was that of the unicellular eukaryotes, prokaryotes and archaea until about 610 million years ago when multicellular organisms began to appear in the oceans in the Ediacaran period. The evolution of multicellularity occurred in multiple independent events, in organisms as diverse as sponges, brown algae, cyanobacteria, slime moulds and myxobacteria. In January 2016, scientists reported that, about 800 million years ago, a minor genetic change in a single molecule called GK-PID may have allowed organisms to go from a single cell organism to one of many cells.\n\nSoon after the emergence of these first multicellular organisms, a remarkable amount of biological diversity appeared over approximately 10 million years, in an event called the Cambrian explosion. Here, the majority of types of modern animals appeared in the fossil record, as well as unique lineages that subsequently became extinct. Various triggers for the Cambrian explosion have been proposed, including the accumulation of oxygen in the atmosphere from photosynthesis.\n\nAbout 500 million years ago, plants and fungi colonised the land and were soon followed by arthropods and other animals. Insects were particularly successful and even today make up the majority of animal species. Amphibians first appeared around 364 million years ago, followed by early amniotes and birds around 155 million years ago (both from \"reptile\"-like lineages), mammals around 129 million years ago, homininae around 10 million years ago and modern humans around 250,000 years ago. However, despite the evolution of these large animals, smaller organisms similar to the types that evolved early in this process continue to be highly successful and dominate the Earth, with the majority of both biomass and species being prokaryotes.\n\nConcepts and models used in evolutionary biology, such as natural selection, have many applications.\n\nArtificial selection is the intentional selection of traits in a population of organisms. This has been used for thousands of years in the domestication of plants and animals. More recently, such selection has become a vital part of genetic engineering, with selectable markers such as antibiotic resistance genes being used to manipulate DNA. Proteins with valuable properties have evolved by repeated rounds of mutation and selection (for example modified enzymes and new antibodies) in a process called directed evolution.\n\nUnderstanding the changes that have occurred during an organism's evolution can reveal the genes needed to construct parts of the body, genes which may be involved in human genetic disorders. For example, the Mexican tetra is an albino cavefish that lost its eyesight during evolution. Breeding together different populations of this blind fish produced some offspring with functional eyes, since different mutations had occurred in the isolated populations that had evolved in different caves. This helped identify genes required for vision and pigmentation.\n\nMany human diseases are not static phenomena, but capable of evolution. Viruses, bacteria, fungi and cancers evolve to be resistant to host immune defences, as well as pharmaceutical drugs. These same problems occur in agriculture with pesticide and herbicide resistance. It is possible that we are facing the end of the effective life of most of available antibiotics and predicting the evolution and evolvability of our pathogens and devising strategies to slow or circumvent it is requiring deeper knowledge of the complex forces driving evolution at the molecular level.\n\nIn computer science, simulations of evolution using evolutionary algorithms and artificial life started in the 1960s and were extended with simulation of artificial selection. Artificial evolution became a widely recognised optimisation method as a result of the work of Ingo Rechenberg in the 1960s. He used evolution strategies to solve complex engineering problems. Genetic algorithms in particular became popular through the writing of John Henry Holland. Practical applications also include automatic evolution of computer programmes. Evolutionary algorithms are now used to solve multi-dimensional problems more efficiently than software produced by human designers and also to optimise the design of systems.\n\nIn the 19th century, particularly after the publication of \"On the Origin of Species\" in 1859, the idea that life had evolved was an active source of academic debate centred on the philosophical, social and religious implications of evolution. Today, the modern evolutionary synthesis is accepted by a vast majority of scientists. However, evolution remains a contentious concept for some theists.\n\nWhile various religions and denominations have reconciled their beliefs with evolution through concepts such as theistic evolution, there are creationists who believe that evolution is contradicted by the creation myths found in their religions and who raise various objections to evolution. As had been demonstrated by responses to the publication of \"Vestiges of the Natural History of Creation\" in 1844, the most controversial aspect of evolutionary biology is the implication of human evolution that humans share common ancestry with apes and that the mental and moral faculties of humanity have the same types of natural causes as other inherited traits in animals. In some countries, notably the United States, these tensions between science and religion have fuelled the current creation–evolution controversy, a religious conflict focusing on politics and public education. While other scientific fields such as cosmology and Earth science also conflict with literal interpretations of many religious texts, evolutionary biology experiences significantly more opposition from religious literalists.\n\nThe teaching of evolution in American secondary school biology classes was uncommon in most of the first half of the 20th century. The Scopes Trial decision of 1925 caused the subject to become very rare in American secondary biology textbooks for a generation, but it was gradually re-introduced later and became legally protected with the 1968 \"Epperson v. Arkansas\" decision. Since then, the competing religious belief of creationism was legally disallowed in secondary school curricula in various decisions in the 1970s and 1980s, but it returned in pseudoscientific form as intelligent design (ID), to be excluded once again in the 2005 \"Kitzmiller v. Dover Area School District\" case.\n\n\nIntroductory reading\n\nAdvanced reading\n\n\n\n",
    "id": "9236",
    "title": "Evolution"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=53365898",
    "text": "Earliest known life forms\n\nThe earliest known life forms on Earth are putative fossilized microorganisms found in hydrothermal vent precipitates. The earliest time that life forms first appeared on Earth is unknown. They may have lived earlier than 3.77 billion years ago, possibly as early as 4.28 billion years ago, not long after the oceans formed 4.41 billion years ago, and not long after the formation of the Earth 4.54 billion years ago. The earliest \"direct\" evidence of life on Earth are fossils of microorganisms permineralized in 3.465-billion-year-old Australian Apex chert rocks.\n\nA life form, or lifeform, is an entity or being that is living.\nEarth remains the only place in the universe known to harbor life forms.\n\nMore than 99% of all species of life forms, amounting to over five billion species, that ever lived on Earth are estimated to be extinct.\n\nSome estimates on the number of Earth's current species of life forms range from 10 million to 14 million, of which about 1.2 million have been documented and over 86 percent have not yet been described. However, a May 2016 scientific report estimates that 1 trillion species are currently on Earth, with only one-thousandth of one percent described. The total number of DNA base pairs on Earth is estimated at 5.0 x 10 with a weight of 50 billion tonnes. In comparison, the total mass of the biosphere has been estimated to be as much as 4 TtC (trillion tons of carbon). In July 2016, scientists reported identifying a set of 355 genes from the Last Universal Common Ancestor (LUCA) of all organisms living on Earth.\n\nThe Earth's biosphere includes soil, hot springs, rock up to or deeper underground, the deepest parts of the ocean, and at least high into the atmosphere. Under certain test conditions, life forms have been observed to thrive in the near-weightlessness of space and to survive in the vacuum of outer space. Life forms appear to thrive in the Mariana Trench, the deepest spot in the Earth's oceans. Other researchers reported related studies that life forms thrive inside rocks up to below the sea floor under of ocean off the coast of the northwestern United States, as well as beneath the seabed off Japan. In August 2014, scientists confirmed the existence of life forms living below the ice of Antarctica. \n\nAccording to one researcher, \"You can find microbes everywhere — they're extremely adaptable to conditions, and survive wherever they are.\"\n\nFossil evidence informs most studies of the origin of life. The age of the Earth is about 4.54 billion years; the earliest undisputed evidence of life on Earth dates from at least 3.5 billion years ago.\n\nThere is evidence that life began much earlier.\n\nIn 2017, fossilized microorganisms, or microfossils, were announced to have been discovered in hydrothermal vent precipitates in the Nuvvuagittuq Belt of Quebec, Canada that may be as old as 4.28 billion years old, the oldest record of life on Earth, suggesting \"an almost instantaneous emergence of life\" after ocean formation 4.41 billion years ago, and not long after the formation of the Earth 4.54 billion years ago. \n\"Remains of life\" have been found in 4.1 billion-year-old rocks in Western Australia.\n\nEvidence of biogenic graphite, and possibly stromatolites, was discovered in 3.7 billion-year-old metasedimentary rocks in southwestern Greenland.\n\nIn May 2017, evidence of life on land may have been found in 3.48 billion-year-old geyserite which is often found around hot springs and geysers, and other related mineral deposits, uncovered in the Pilbara Craton of Western Australia. This complements the November 2013 publication that microbial mat fossils had been found in 3.48 billion-year-old sandstone in Western Australia.\n\nIn November 2017, a study by the University of Edinburgh suggested that life on Earth may have originated from biological particles carried by streams of space dust.\n\nA December 2017 report stated that 3.465-billion-year-old Australian Apex chert rocks once contained microorganisms, the earliest \"direct\" evidence of life on Earth.\n\nIn January 2018, a study found that 4.5 billion-year-old meteorites found on Earth contained liquid water along with prebiotic complex organic substances that may be ingredients for life.\n\nAccording to biologist Stephen Blair Hedges, \"If life arose relatively quickly on Earth … then it could be common in the universe.\"\n\n",
    "id": "53365898",
    "title": "Earliest known life forms"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=30038",
    "text": "Thomas Henry Huxley\n\nThomas Henry Huxley (; 4 May 1825 – 29 June 1895) was an English biologist specialising in comparative anatomy. He is known as \"Darwin's Bulldog\" for his advocacy of Charles Darwin's theory of evolution.\n\nHuxley's famous debate in 1860 with Samuel Wilberforce was a key moment in the wider acceptance of evolution and in his own career. Huxley had been planning to leave Oxford on the previous day, but, after an encounter with Robert Chambers, the author of \"Vestiges\", he changed his mind and decided to join the debate. Wilberforce was coached by Richard Owen, against whom Huxley also debated about whether humans were closely related to apes.\n\nHuxley was slow to accept some of Darwin's ideas, such as gradualism, and was undecided about natural selection, but despite this he was wholehearted in his public support of Darwin. Instrumental in developing scientific education in Britain, he fought against the more extreme versions of religious tradition.\n\nOriginally coining the term in 1869, Huxley elaborated on \"agnosticism\" in 1889 to frame the nature of claims in terms of what is knowable and what is not. Huxley statesAgnosticism, in fact, is not a creed, but a method, the essence of which lies in the rigorous application of a single principle... the fundamental axiom of modern science... In matters of the intellect, follow your reason as far as it will take you, without regard to any other consideration... In matters of the intellect, do not pretend that conclusions are certain which are not demonstrated or demonstrable. Use of that term has continued to the present day (see Thomas Henry Huxley and agnosticism).\n\nHuxley had little formal schooling and was virtually self-taught. He became perhaps the finest comparative anatomist of the later 19th century. He worked on invertebrates, clarifying relationships between groups previously little understood. Later, he worked on vertebrates, especially on the relationship between apes and humans. After comparing \"Archaeopteryx\" with \"Compsognathus\", he concluded that birds evolved from small carnivorous dinosaurs, a theory widely accepted today.\n\nThe tendency has been for this fine anatomical work to be overshadowed by his energetic and controversial activity in favour of evolution, and by his extensive public work on scientific education, both of which had significant effects on society in Britain and elsewhere.\n\nThomas Henry Huxley was born in Ealing, which was then a village in Middlesex. He was the second youngest of eight children of George Huxley and Rachel Withers. Like some other British scientists of the nineteenth century such as Alfred Russel Wallace, Huxley was brought up in a literate middle-class family which had fallen on hard times. His father was a mathematics teacher at Ealing School until it closed, putting the family into financial difficulties. As a result, Thomas left school at age 10, after only two years of formal schooling.\n\nDespite this unenviable start, Huxley was determined to educate himself. He became one of the great autodidacts of the nineteenth century. At first he read Thomas Carlyle, James Hutton's \"Geology\", and Hamilton's \"Logic\". In his teens he taught himself German, eventually becoming fluent and used by Charles Darwin as a translator of scientific material in German. He learned Latin, and enough Greek to read Aristotle in the original.\n\nLater on, as a young adult, he made himself an expert, first on invertebrates, and later on vertebrates, all self-taught. He was skilled in drawing and did many of the illustrations for his publications on marine invertebrates. In his later debates and writing on science and religion his grasp of theology was better than most of his clerical opponents. Huxley, a boy who left school at ten, became one of the most knowledgeable men in Britain.\n\nHe was apprenticed for short periods to several medical practitioners: at 13 to his brother-in-law John Cooke in Coventry, who passed him on to Thomas Chandler, notable for his experiments using mesmerism for medical purposes. Chandler's practice was in London's Rotherhithe amidst the squalor endured by the Dickensian poor. Here Thomas would have seen poverty, crime and rampant disease at its worst. Next, another brother-in-law took him on: John Salt, his eldest sister's husband. Now 16, Huxley entered Sydenham College (behind University College Hospital), a cut-price anatomy school whose founder, Marshall Hall, discovered the reflex arc. All this time Huxley continued his programme of reading, which more than made up for his lack of formal schooling.\n\nA year later, buoyed by excellent results and a silver medal prize in the Apothecaries' yearly competition, Huxley was admitted to study at Charing Cross Hospital, where he obtained a small scholarship. At Charing Cross, he was taught by Thomas Wharton Jones, Professor of Ophthalmic Medicine and Surgery at University College London. Jones had been Robert Knox's assistant when Knox bought cadavers from Burke and Hare. The young Wharton Jones, who acted as go-between, was exonerated of crime, but thought it best to leave Scotland. He was a fine teacher, up-to-date in physiology and also an ophthalmic surgeon. In 1845, under Wharton Jones' guidance, Huxley published his first scientific paper demonstrating the existence of a hitherto unrecognised layer in the inner sheath of hairs, a layer that has been known since as Huxley's layer. No doubt remembering this, and of course knowing his merit, later in life Huxley organised a pension for his old tutor.\n\nAt twenty he passed his First M.B. examination at the University of London, winning the gold medal for anatomy and physiology. However, he did not present himself for the final (Second M.B.) exams and consequently did not qualify with a university degree. His apprenticeships and exam results formed a sufficient basis for his application to the Royal Navy.\n\nAged 20, Huxley was too young to apply to the Royal College of Surgeons for a licence to practise, yet he was 'deep in debt'. So, at a friend's suggestion, he applied for an appointment in the Royal Navy. He had references on character and certificates showing the time spent on his apprenticeship and on requirements such as dissection and pharmacy. Sir William Burnett, the Physician General of the Navy, interviewed him and arranged for the College of Surgeons to test his competence (by means of a \"viva voce\").\n\nFinally Huxley was made Assistant Surgeon ('surgeon's mate') to HMS \"Rattlesnake\", about to start for a voyage of discovery and surveying to New Guinea and Australia. The \"Rattlesnake\" left England on 3 December 1846 and, once they had arrived in the southern hemisphere, Huxley devoted his time to the study of marine invertebrates. He began to send details of his discoveries back to England, where publication was arranged by Edward Forbes FRS (who had also been a pupil of Knox). Both before and after the voyage Forbes was something of a mentor to Huxley.\n\nHuxley's paper \"On the anatomy and the affinities of the family of Medusae\" was published in 1849 by the Royal Society in its \"Philosophical Transactions\". Huxley united the Hydroid and Sertularian polyps with the Medusae to form a class to which he subsequently gave the name of \"Hydrozoa\". The connection he made was that all the members of the class consisted of two cell layers, enclosing a central cavity or stomach. This is characteristic of the phylum now called the \"Cnidaria\". He compared this feature to the serous and mucous structures of embryos of higher animals. When at last he got a grant from the Royal Society for the printing of plates, Huxley was able to summarise this work in \"The Oceanic Hydrozoa\", published by the Ray Society in 1859.\n\nThe value of Huxley's work was recognised and, on returning to England in 1850, he was elected a Fellow of the Royal Society. In the following year, at the age of twenty-six, he not only received the Royal Society Medal but was also elected to the Council. He met Joseph Dalton Hooker and John Tyndall, who remained his lifelong friends. The Admiralty retained him as a nominal assistant-surgeon, so he might work on the specimens he collected and the observations he made during the voyage of the \"Rattlesnake\". He solved the problem of \"Appendicularia\", whose place in the animal kingdom Johannes Peter Müller had found himself wholly unable to assign. It and the Ascidians are both, as Huxley showed, tunicates, today regarded as a sister group to the vertebrates in the phylum \"Chordata\". Other papers on the morphology of the cephalopods and on brachiopods and rotifers are also noteworthy. The \"Rattlesnake\"'s official naturalist, John MacGillivray, did some work on botany, and proved surprisingly good at notating Australian aboriginal languages. He wrote up the voyage in the standard Victorian two volume format.\n\nHuxley effectively resigned from the navy (by refusing to return to active service) and, in July 1854, he became Professor of Natural History at the Royal School of Mines and naturalist to the British Geological Survey in the following year. In addition, he was Fullerian Professor at the Royal Institution 1855–58 and 1865–67; Hunterian Professor at the Royal College of Surgeons 1863–69; President of the British Association for the Advancement of Science 1869–1870; President of the Quekett Microscopical Club 1878; President of the Royal Society 1883–85; Inspector of Fisheries 1881–85; and President of the Marine Biological Association 1884–1890.\n\nThe thirty-one years during which Huxley occupied the chair of natural history at the Royal School of Mines included work on vertebrate palaeontology and on many projects to advance the place of science in British life. Huxley retired in 1885, after a bout of depressive illness which started in 1884. He resigned the presidency of the Royal Society in mid-term, the Inspectorship of Fisheries, and his chair (as soon as he decently could) and took six months' leave. His pension was a fairly handsome £1500 a year.\n\nIn 1890, he moved from London to Eastbourne where he edited the nine volumes of his \"Collected Essays\". In 1894 he heard of Eugene Dubois' discovery in Java of the remains of \"Pithecanthropus erectus\" (now known as \"Homo erectus\"). Finally, in 1895, he died of a heart attack (after contracting influenza and pneumonia), and was buried in North London at St Marylebone. This small family plot had been purchased upon the death of his beloved youngest son Noel, who died of scarlet fever in 1860; Huxley's wife Henrietta Anne née Heathorn and son Noel are also buried there. No invitations were sent out, but two hundred people turned up for the ceremony; they included Joseph Dalton Hooker, William Henry Flower, Mulford B. Foster, Edwin Lankester, Joseph Lister and, apparently, Henry James.\n\nHuxley and his wife had five daughters and three sons:\n\nNoel Huxley (1856–1860), died aged 4.\nJessie Oriana Huxley (1856–1927), married architect Fred Waller in 1877.\nMarian Huxley (1859–1887), married artist John Collier in 1879.\nLeonard Huxley (1860–1933), married Julia Arnold.\nRachel Huxley (1862–1934), married civil engineer Alfred Eckersley in 1884.\nHenrietta (Nettie) Huxley (1863–1940), married Harold Roller, travelled Europe as a singer.\nHenry Huxley (1865–1946), became a fashionable general practitioner in London.\nEthel Huxley (1866–1941) married artist John Collier (widower of sister) in 1889.\n\nFrom 1870 onwards, Huxley was to some extent drawn away from scientific research by the claims of public duty. He served on eight Royal Commissions, from 1862 to 1884. From 1871 to 1880 he was a Secretary of the Royal Society and from 1883 to 1885 he was president. He was president of the Geological Society from 1868 to 1870. In 1870, he was president of the British Association at Liverpool and, in the same year was elected a member of the newly constituted London School Board. He was president of the Quekett Microscopical Club from 1877 to 1879. He was the leading person amongst those who reformed the Royal Society, persuaded government about science, and established scientific education in British schools and universities. Before him, science was mostly a gentleman's occupation; after him, science was a profession.\n\nHe was awarded the highest honours then open to British men of science. The Royal Society, who had elected him as Fellow when he was 25 (1851), awarded him the Royal Medal the next year (1852), a year before Charles Darwin got the same award. He was the youngest biologist to receive such recognition. Then later in life came the Copley Medal in 1888 and the Darwin Medal in 1894; the Geological Society awarded him the Wollaston Medal in 1876; the Linnean Society awarded him the Linnean Medal in 1890. There were many other elections and appointments to eminent scientific bodies; these and his many academic awards are listed in the \"Life and Letters\". He turned down many other appointments, notably the Linacre chair in zoology at Oxford and the Mastership of University College, Oxford.\n\nIn 1873 the King of Sweden made Huxley, Hooker and Tyndall Knights of the Order of the Polar Star: they could wear the insignia but not use the title in Britain. Huxley collected many honorary memberships of foreign societies, academic awards and honorary doctorates from Britain and Germany. He also became foreign member of the Royal Netherlands Academy of Arts and Sciences in 1892.\n\nAs recognition of his many public services he was given a pension by the state, and was appointed Privy Councillor in 1892.\n\nDespite his many achievements he was given no award by the British state until late in life. In this he did better than Darwin, who got no award of any kind from the state. (Darwin's proposed knighthood was vetoed by ecclesiastical advisers, including Wilberforce) Perhaps Huxley had commented too often on his dislike of honours, or perhaps his many assaults on the traditional beliefs of organised religion made enemies in the establishment—he had vigorous debates in print with Benjamin Disraeli, William Ewart Gladstone and Arthur Balfour, and his relationship with Lord Salisbury was less than tranquil.\n\nHuxley was for about thirty years evolution's most effective advocate, and for some Huxley was \"\"the\" premier advocate of science in the nineteenth century [for] the whole English-speaking world\".\n\nThough he had many admirers and disciples, his retirement and later death left British zoology somewhat bereft of leadership. He had, directly or indirectly, guided the careers and appointments of the next generation, but none were of his stature. The loss of Francis Balfour in 1882, climbing the Alps just after he was appointed to a chair at Cambridge, was a tragedy. Huxley thought he was \"the only man who can carry out my work\": the deaths of Balfour and W. K. Clifford were \"the greatest losses to science in our time\".\n\nThe first half of Huxley's career as a palaeontologist is marked by a rather strange predilection for 'persistent types', in which he seemed to argue that evolutionary advancement (in the sense of major new groups of animals and plants) was rare or absent in the Phanerozoic. In the same vein, he tended to push the origin of major groups such as birds and mammals back into the Palaeozoic era, and to claim that no order of plants had ever gone extinct.\n\nMuch paper has been consumed by historians of science ruminating on this strange and somewhat unclear idea. Huxley was wrong to pitch the loss of orders in the Phanerozoic as low as 7%, and he did not estimate the number of new orders which evolved. Persistent types sat rather uncomfortably next to Darwin's more fluid ideas; despite his intelligence, it took Huxley a surprisingly long time to appreciate some of the implications of evolution. However, gradually Huxley moved away from this conservative style of thinking as his understanding of palaeontology, and the discipline itself, developed.\n\nHuxley's detailed anatomical work was, as always, first-rate and productive. His work on fossil fish shows his distinctive approach: whereas pre-Darwinian naturalists collected, identified and classified, Huxley worked mainly to reveal the evolutionary relationships between groups.\n\nThe lobed-finned fish (such as coelacanths and lung fish) have paired appendages whose internal skeleton is attached to the shoulder or pelvis by a single bone, the humerus or femur. His interest in these fish brought him close to the origin of tetrapods, one of the most important areas of vertebrate palaeontology.\n\nThe study of fossil reptiles led to his demonstrating the fundamental affinity of birds and reptiles, which he united under the title of \"Sauropsida\". His papers on \"Archaeopteryx\" and the origin of birds were of great interest then and still are.\n\nApart from his interest in persuading the world that man was a primate, and had descended from the same stock as the apes, Huxley did little work on mammals, with one exception. On his tour of America Huxley was shown the remarkable series of fossil horses, discovered by O. C. Marsh, in Yale's Peabody Museum. Marsh was part palaeontologist, part robber baron, a man who had hunted buffalo and met Red Cloud (in 1874). Funded by his uncle George Peabody, Marsh had made some remarkable discoveries:\nthe huge Cretaceous aquatic bird \"Hesperornis\", and the dinosaur footprints along the Connecticut River were worth the trip by themselves, but the horse fossils were really special.\n\nThe collection at that time went from the small four-toed forest-dwelling \"Orohippus\" from the Eocene through three-toed species such as \"Miohippus\" to species more like the modern horse. By looking at their teeth he could see that, as the size grew larger and the toes reduced, the teeth changed from those of a browser to those of a grazer. All such changes could be explained by a general alteration in habitat from forest to grassland. And, it is now known, that is what did happen over large areas of North America from the Eocene to the Pleistocene: the ultimate causative agent was global temperature reduction (see Paleocene–Eocene Thermal Maximum). The modern account of the evolution of the horse has many other members, and the overall appearance of the tree of descent is more like a bush than a straight line.\n\nThe horse series also strongly suggested that the process was gradual, and that the origin of the modern horse lay in North America, not in Eurasia. If so, then something must have happened to horses in North America, since none were there when Europeans arrived. The experience was enough for Huxley to give credence to Darwin's gradualism, and to introduce the story of the horse into his lecture series.\n\nHuxley was originally not persuaded of \"development theory\", as evolution was once called. This can be seen in his savage review of Robert Chambers' \"Vestiges of the Natural History of Creation\", a book which contained some quite pertinent arguments in favour of evolution. Huxley had also rejected Lamarck's theory of transmutation, on the basis that there was insufficient evidence to support it. All this scepticism was brought together in a lecture to the Royal Institution, which made Darwin anxious enough to set about an effort to change young Huxley's mind. It was the kind of thing Darwin did with his closest scientific friends, but he must have had some particular intuition about Huxley, who was from all accounts a most impressive person even as a young man.\n\nHuxley was therefore one of the small group who knew about Darwin's ideas before they were published (the group included Joseph Dalton Hooker and Charles Lyell). The first publication by Darwin of his ideas came when Wallace sent Darwin his famous paper on natural selection, which was presented by Lyell and Hooker to the Linnean Society in 1858 alongside excerpts from Darwin's notebook and a Darwin letter to Asa Gray. Huxley's famous response to the idea of natural selection was \"How extremely stupid not to have thought of that!\" However, he never conclusively made up his mind about whether natural selection was the main method for evolution, though he did admit it was a hypothesis which was a good working basis.\n\nLogically speaking, the prior question was whether evolution had taken place at all. It is to this question that much of Darwin's \"On the Origin of Species\" was devoted. Its publication in 1859 completely convinced Huxley of evolution and it was this and no doubt his admiration of Darwin's way of amassing and using evidence that formed the basis of his support for Darwin in the debates that followed the book's publication.\n\nHuxley's support started with his anonymous favourable review of the \"Origin\" in the \"Times\" for 26 December 1859, and continued with articles in several periodicals, and in a lecture at the Royal Institution in February 1860. At the same time, Richard Owen, whilst writing an extremely hostile anonymous review of the \"Origin\" in the \"Edinburgh Review\", also primed Samuel Wilberforce who wrote one in the \"Quarterly Review\", running to 17,000 words. The authorship of this latter review was not known for sure until Wilberforce's son wrote his biography. So it can be said that, just as Darwin groomed Huxley, so Owen groomed Wilberforce; and both the proxies fought public battles on behalf of their principals as much as themselves. Though we do not know the exact words of the Oxford debate, we do know what Huxley thought of the review in the \"Quarterly\":\n\nSince Lord Brougham assailed Dr Young, the world has seen no such specimen of the insolence of a shallow pretender to a Master in Science as this remarkable production, in which one of the most exact of observers, most cautious of reasoners, and most candid of expositors, of this or any other age, is held up to scorn as a \"flighty\" person, who endeavours \"to prop up his utterly rotten fabric of guess and speculation,\" and whose \"mode of dealing with nature\" is reprobated as \"utterly dishonourable to Natural Science.\"\n\nIf I confine my retrospect of the reception of the \"Origin of Species\" to a twelvemonth, or thereabouts, from the time of its publication, I do not recollect anything quite so foolish and unmannerly as the \"Quarterly Review\" article...\n\nHuxley said \"I am Darwin's bulldog\". While the second half of Darwin's life was lived mainly within his family, the younger combative Huxley operated mainly out in the world at large. A letter from Huxley to Ernst Haeckel (2 November 1871) states: \"The dogs have been snapping at [Darwin's] heels too much of late.\" At Oxford and Cambridge Universities, \"Bulldog\" was and still is student slang for a university policeman, whose job was to corral errant students and maintain their moral rectitude.\n\nFamously, Huxley responded to Wilberforce in the debate at the British Association meeting, on Saturday 30 June 1860 at the Oxford University Museum. Huxley's presence there had been encouraged on the previous evening when he met Robert Chambers, the Scottish publisher and author of \"Vestiges\", who was walking the streets of Oxford in a dispirited state, and begged for assistance. The debate followed the presentation of a paper by John William Draper, and was chaired by Darwins's former botany tutor John Stevens Henslow. Darwin's theory was opposed by the Lord Bishop of Oxford, Samuel Wilberforce, and those supporting Darwin included Huxley and their mutual friends Hooker and Lubbock. The platform featured Brodie and Professor Beale, and Robert FitzRoy, who had been captain of HMS \"Beagle\" during Darwin's voyage, spoke against Darwin.\n\nWilberforce had a track record against evolution as far back as the previous Oxford B.A. meeting in 1847 when he attacked Chambers' \"Vestiges\". For the more challenging task of opposing the \"Origin\", and the implication that man descended from apes, he had been assiduously coached by Richard Owen – Owen stayed with him the night before the debate. On the day Wilberforce repeated some of the arguments from his \"Quarterly Review\" article (written but not yet published), then ventured onto slippery ground. His famous jibe at Huxley (as to whether Huxley was descended from an ape on his mother's side or his father's side) was probably unplanned, and certainly unwise. Huxley's reply to the effect that he would rather be descended from an ape than a man who misused his great talents to suppress debate—the exact wording is not certain—was widely recounted in pamphlets and a spoof play.\n\nThe letters of Alfred Newton include one to his brother giving an eye-witness account of the debate, and written less than a month afterwards. Other eyewitnesses, with one or two exceptions (Hooker especially thought \"he\" had made the best points), give similar accounts, at varying dates after the event. The general view was and still is that Huxley got much the better of the exchange though Wilberforce himself thought he had done quite well. In the absence of a verbatim report differing perceptions are difficult to judge fairly; Huxley wrote a detailed account for Darwin, a letter which does not survive; however, a letter to his friend Frederick Daniel Dyster does survive with an account just three months after the event.\n\nOne effect of the debate was to increase hugely Huxley's visibility amongst educated people, through the accounts in newspapers and periodicals. Another consequence was to alert him to the importance of public debate: a lesson he never forgot. A third effect was to serve notice that Darwinian ideas could not be easily dismissed: on the contrary, they would be vigorously defended against orthodox authority. A fourth effect was to promote professionalism in science, with its implied need for scientific education. A fifth consequence was indirect: as Wilberforce had feared, a defence of evolution did undermine literal belief in the Old Testament, especially the Book of Genesis. Many of the liberal clergy at the meeting were quite pleased with the outcome of the debate; they were supporters, perhaps, of the controversial \"Essays and Reviews\". Thus both on the side of science, and on the side of religion, the debate was important, and its outcome significant. (see also below)\n\nThat Huxley and Wilberforce remained on courteous terms after the debate (and able to work together on projects such as the Metropolitan Board of Education) says something about both men, whereas Huxley and Owen were never reconciled.\n\nFor nearly a decade his work was directed mainly to the relationship of man to the apes. This led him directly into a clash with Richard Owen, a man widely disliked for his behaviour whilst also being admired for his capability. The struggle was to culminate in some severe defeats for Owen. Huxley's Croonian Lecture, delivered before the Royal Society in 1858 on \"The Theory of the Vertebrate Skull\" was the start. In this, he rejected Owen's theory that the bones of the skull and the spine were homologous, an opinion previously held by Goethe and Lorenz Oken.\n\nFrom 1860–63 Huxley developed his ideas, presenting them in lectures to working men, students and the general public, followed by publication. Also in 1862 a series of talks to working men was printed lecture by lecture as pamphlets, later bound up as a little green book; the first copies went on sale in December. Other lectures grew into Huxley's most famous work \"Evidence as to Man's place in Nature\" (1863) where he addressed the key issues long before Charles Darwin published his \"Descent of Man\" in 1871.\n\nAlthough Darwin did not publish his \"Descent of Man\" until 1871, the general debate on this topic had started years before (there was even a precursor debate in the 18th century between Monboddo and Buffon). Darwin had dropped a hint when, in the conclusion to the \"Origin\", he wrote: \"In the distant future... light will be thrown on the origin of man and his history\". Not so distant, as it turned out. A key event had already occurred in 1857 when Richard Owen presented (to the Linnean Society) his theory that man was marked off from all other mammals by possessing features of the brain peculiar to the genus \"Homo\". Having reached this opinion, Owen separated man from all other mammals in a subclass of its own. No other biologist held such an extreme view. Darwin reacted \"Man...as distinct from a chimpanzee [as] an ape from a platypus... I cannot swallow that!\" Neither could Huxley, who was able to demonstrate that Owen's idea was completely wrong.\nThe subject was raised at the 1860 BA Oxford meeting, when Huxley flatly contradicted Owen, and promised a later demonstration of the facts. In fact, a number of demonstrations were held in London and the provinces. In 1862 at the Cambridge meeting of the B.A. Huxley's friend William Flower gave a public dissection to show that the same structures (the posterior horn of the lateral ventricle and hippocampus minor) were indeed present in apes. The debate was widely publicised, and parodied as the \"Great Hippocampus Question\". It was seen as one of Owen's greatest blunders, revealing Huxley as not only dangerous in debate, but also a better anatomist.\n\nOwen conceded that there was something that could be called a hippocampus minor in the apes, but stated that it was much less developed and that such a presence did not detract from the overall distinction of simple brain size.\n\nHuxley's ideas on this topic were summed up in January 1861 in the first issue (new series) of his own journal, the \"Natural History Review\": \"the most violent scientific paper he had ever composed\". This paper was reprinted in 1863 as chapter 2 of \"Man's Place in Nature\", with an addendum giving his account of the Owen/Huxley controversy about the ape brain. In his \"Collected Essays\" this addendum was removed.\n\nThe extended argument on the ape brain, partly in debate and partly in print, backed by dissections and demonstrations, was a landmark in Huxley's career. It was highly important in asserting his dominance of comparative anatomy, and in the long run more influential in establishing evolution amongst biologists than was the debate with Wilberforce. It also marked the start of Owen's decline in the esteem of his fellow biologists.\n\nThe following was written by Huxley to Rolleston before the BA meeting in 1861:\n\nDuring those years there was also work on human fossil anatomy and anthropology. In 1862 he examined the Neanderthal skull-cap, which had been discovered in 1857. It was the first pre-\"sapiens\" discovery of a fossil man, and it was immediately clear to him that the brain case was surprisingly large.\n\nPerhaps less productive was his work on physical anthropology, a topic which fascinated the Victorians. Huxley classified the human races into nine categories, and discussed them under four headings as: Australoid, Negroid, Xanthocroic and Mongoloid types. Such classifications depended mainly on appearance and anatomical characteristics.\n\nHuxley was certainly not slavish in his dealings with Darwin. As shown in every biography, they had quite different and rather complementary characters. Important also, Darwin was a field naturalist, but Huxley was an anatomist, so there was a difference in their experience of nature. Lastly, Darwin's views on science were different from Huxley's views. For Darwin, natural selection was the best way to explain evolution because it explained a huge range of natural history facts and observations: it solved problems. Huxley, on the other hand, was an empiricist who trusted what he could see, and some things are not easily seen. With this in mind, one can appreciate the debate between them, Darwin writing his letters, Huxley never going quite so far as to say he thought Darwin was right.\n\nHuxley's reservations on natural selection were of the type \"until selection and breeding can be seen to give rise to varieties which are infertile with each other, natural selection cannot be proved\". Huxley's position on selection was agnostic; yet he gave no credence to any other theory. Despite this concern about evidence, Huxley saw that if evolution came about through variation, reproduction and selection then other things would also be subject to the same pressures. This included ideas because they are invented, imitated and selected by humans: ‘The struggle for existence holds as much in the intellectual as in the physical world. A theory is a species of thinking, and its right to exist is coextensive with its power of resisting extinction by its rivals.’ This is the same idea as meme theory put forward by Richard Dawkins in 1976.\n\nDarwin's part in the discussion came mostly in letters, as was his wont, along the lines: \"The empirical evidence you call for is both impossible in practical terms, and in any event unnecessary. It's the same as asking to see every step in the transformation (or the splitting) of one species into another. My way so many issues are clarified and problems solved; no other theory does nearly so well\".\n\nHuxley's reservation, as Helena Cronin has so aptly remarked, was contagious: \"it spread itself for years among all kinds of doubters of Darwinism\". One reason for this doubt was that comparative anatomy could address the question of descent, \"but not the question of mechanism\".\n\nHuxley was a pallbearer at the funeral of Charles Darwin on 26 April 1882.\n\nIn November 1864, Huxley succeeded in launching a dining club, the X Club, composed of like-minded people working to advance the cause of science; not surprisingly, the club consisted of most of his closest friends. There were nine members, who decided at their first meeting that there should be no more. The members were: Huxley, John Tyndall, J. D. Hooker, John Lubbock (banker, biologist and neighbour of Darwin), Herbert Spencer (social philosopher and sub-editor of the Economist), William Spottiswoode (mathematician and the Queen's Printer), Thomas Hirst (Professor of Physics at University College London), Edward Frankland (the new Professor of Chemistry at the Royal Institution) and George Busk, zoologist and palaeontologist (formerly surgeon for HMS \"Dreadnought\"). All except Spencer were Fellows of the Royal Society. Tyndall was a particularly close friend; for many years they met regularly and discussed issues of the day. On more than one occasion Huxley joined Tyndall in the latter's trips into the Alps and helped with his investigations in glaciology.\n\nThere were also some quite significant X-Club satellites such as William Flower and George Rolleston, (Huxley protegés), and liberal clergyman Arthur Stanley, the Dean of Westminster. Guests such as Charles Darwin and Hermann von Helmholtz were entertained from time to time.\n\nThey would dine early on first Thursdays at a hotel, planning what to do; high on the agenda was to change the way the Royal Society Council did business. It was no coincidence that the Council met later that same evening. First item for the Xs was to get the Copley Medal for Darwin, which they managed after quite a struggle.\n\nThe next step was to acquire a journal to spread their ideas. This was the weekly \"Reader\", which they bought, revamped and redirected. Huxley had already become part-owner of the \"Natural History Review\" bolstered by the support of Lubbock, Rolleston, Busk and Carpenter (X-clubbers and satellites). The journal was switched to pro-Darwinian lines and relaunched in January 1861. After a stream of good articles the \"NHR\" failed after four years; but it had helped at a critical time for the establishment of evolution. The \"Reader\" also failed, despite its broader appeal which included art and literature as well as science. The periodical market was quite crowded at the time, but most probably the critical factor was Huxley's time; he was simply over-committed, and could not afford to hire full-time editors. This occurred often in his life: Huxley took on too many ventures, and was not so astute as Darwin at getting others to do work for him.\n\nHowever, the experience gained with the \"Reader\" was put to good use when the X Club put their weight behind the founding of \"Nature\" in 1869. This time no mistakes were made: above all there was a permanent editor (though not full-time), Norman Lockyer, who served until 1919, a year before his death. In 1925, to celebrate his centenary, \"Nature\" issued a supplement devoted to Huxley.\n\nThe peak of the X Club's influence was from 1873 to 1885 as Hooker, Spottiswoode and Huxley were Presidents of the Royal Society in succession. Spencer resigned in 1889 after a dispute with Huxley over state support for science. After 1892 it was just an excuse for the surviving members to meet. Hooker died in 1911, and Lubbock (now Lord Avebury) was the last surviving member.\n\nHuxley was also an active member of the Metaphysical Society, which ran from 1869 to 1880. It was formed around a nucleus of clergy and expanded to include all kinds of opinions. Tyndall and Huxley later joined The Club (founded by Dr. Johnson) when they could be sure that Owen would not turn up.\n\nWhen Huxley himself was young there were virtually no degrees in British universities in the biological sciences and few courses. Most biologists of his day were either self-taught, or took medical degrees. When he retired there were established chairs in biological disciplines in most universities, and a broad consensus on the curricula to be followed. Huxley was the single most influential person in this transformation.\n\nIn the early 1870s the Royal School of Mines moved to new quarters in South Kensington; ultimately it would become one of the constituent parts of Imperial College London. The move gave Huxley the chance to give more prominence to laboratory work in biology teaching, an idea suggested by practice in German universities. In the main, the method was based on the use of carefully chosen types, and depended on the dissection of anatomy, supplemented by microscopy, museum specimens and some elementary physiology at the hands of Foster.\n\nThe typical day would start with Huxley lecturing at 9am, followed by a program of laboratory work supervised by his demonstrators. Huxley's demonstrators were picked men—all became leaders of biology in Britain in later life, spreading Huxley's ideas as well as their own. Michael Foster became Professor of Physiology at Cambridge; E. Ray Lankester became Jodrell Professor of Zoology at University College London (1875–91), Professor of Comparative Anatomy at Oxford (1891–98) and Director of the Natural History Museum (1898–1907); S.H. Vines became Professor of Botany at Cambridge; W.T. Thiselton-Dyer became Hooker's successor at Kew (he was already Hooker's son-in-law!); T. Jeffery Parker became Professor of Zoology and Comparative Anatomy at University College, Cardiff; and William Rutherford became the Professor of Physiology at Edinburgh. William Flower, Conservator to the Hunterian Museum, and THH's assistant in many dissections, became Sir William Flower, Hunterian Professor of Comparative Anatomy and, later, Director of the Natural History Museum. It's a remarkable list of disciples, especially when contrasted with Owen who, in a longer professional life than Huxley, left no disciples at all. \"No one fact tells so strongly against Owen... as that he has never reared one pupil or follower\".\n\nHuxley's courses for students were so much narrower than the man himself that many were bewildered by the contrast: \"The teaching of zoology by use of selected animal types has come in for much criticism\"; Looking back in 1914 to his time as a student, Sir Arthur Shipley said \"Darwin's later works all dealt with living organisms, yet our obsession was with the dead, with bodies preserved, and cut into the most refined slices\". E.W MacBride said \"Huxley... would persist in looking at animals as material structures and not as living, active beings; in a word... he was a necrologist. To put it simply, Huxley preferred to teach what he had actually seen with his own eyes.\n\nThis largely morphological program of comparative anatomy remained at the core of most biological education for a hundred years until the advent of cell and molecular biology and interest in evolutionary ecology forced a fundamental rethink. It is an interesting fact that the methods of the field naturalists who led the way in developing the theory of evolution (Darwin, Wallace, Fritz Müller, Henry Bates) were scarcely represented at all in Huxley's program. Ecological investigation of life in its environment was virtually non-existent, and theory, evolutionary or otherwise, was at a discount. Michael Ruse finds no mention of evolution or Darwinism in any of the exams set by Huxley, and confirms the lecture content based on two complete sets of lecture notes.\n\nSince Darwin, Wallace and Bates did not hold teaching posts at any stage of their adult careers (and Műller never returned from Brazil) the imbalance in Huxley's program went uncorrected. It is surely strange that Huxley's courses did not contain an account of the evidence collected by those naturalists of life in the tropics; evidence which they had found so convincing, and which caused their views on evolution by natural selection to be so similar. Desmond suggests that \"[biology] had to be simple, synthetic and assimilable [because] it was to train teachers and had no other heuristic function\". That must be part of the reason; indeed it does help to explain the stultifying nature of much school biology. But zoology as taught at all levels became far too much the product of one man.\n\nHuxley was comfortable with comparative anatomy, at which he was the greatest master of the day. He was not an all-round naturalist like Darwin, who had shown clearly enough how to weave together detailed factual information and subtle arguments across the vast web of life. Huxley chose, in his teaching (and to some extent in his research) to take a more straightforward course, concentrating on his personal strengths.\n\nHuxley was also a major influence in the direction taken by British schools: in November 1870 he was voted onto the London School Board. In primary schooling, he advocated a wide range of disciplines, similar to what is taught today: reading, writing, arithmetic, art, science, music, etc. In secondary education he recommended two years of basic liberal studies followed by two years of some upper-division work, focusing on a more specific area of study. A practical example of the latter is his famous 1868 lecture \"On a Piece of Chalk\" which was first published as an essay in \"Macmillan's Magazine\" in London later that year. The piece reconstructs the geological history of Britain from a simple piece of chalk and demonstrates science as \"organized common sense\".\n\nHuxley supported the reading of the Bible in schools. This may seem out of step with his agnostic convictions, but he believed that the Bible's significant moral teachings and superb use of language were relevant to English life. \"I do not advocate burning your ship to get rid of the cockroaches\".\nHowever, what Huxley proposed was to create an \"edited version\" of the Bible, shorn of \"shortcomings and errors... statements to which men of science absolutely and entirely demur... These tender children [should] not be taught that which you do not yourselves believe\". The Board voted against his idea, but it also voted against the idea that public money should be used to support students attending church schools. Vigorous debate took place on such points, and the debates were minuted in detail. Huxley said \"I will never be a party to enabling the State to sweep the children of this country into denominational schools\". The Act of Parliament which founded board schools permitted the reading of the Bible, but did not permit any denominational doctrine to be taught.\n\nIt may be right to see Huxley's life and work as contributing to the secularisation of British society which gradually occurred over the following century. Ernst Mayr said \"It can hardly be doubted that [biology] has helped to undermine traditional beliefs and value systems\"  — and Huxley more than anyone else was responsible for this trend in Britain. Some modern Christian apologists consider Huxley the father of antitheism, though he himself maintained that he was an agnostic, not an atheist. He was, however, a lifelong and determined opponent of almost all organised religion throughout his life, especially the \"Roman Church... carefully calculated for the destruction of all that is highest in the moral nature, in the intellectual freedom, and in the political freedom of mankind\". In the same line of thought, in an article in \"Popular Science\", Huxley used the expression \"the so-called Christianity of Catholicism,\" explaining: \"I say 'so-called' not by way of offense, but as a protest against the monstruous assumption that Catholic Christianity is explicitly or implictly contained in any trust-worthy record of the teaching of Jesus of Nazareth.\"\n\nVladimir Lenin remarked (in \"Materialism and empirio-criticism\") \"In Huxley's case... agnosticism serves as a fig-leaf for materialism\" (see also the Debate with Wilberforce above).\n\nHuxley's interest in education went still further than school and university classrooms; he made a great effort to reach interested adults of all kinds: after all, he himself was largely self-educated. There were his lecture courses for working men, many of which were published afterwards, and there was the use he made of journalism, partly to earn money but mostly to reach out to the literate public. For most of his adult life he wrote for periodicals—the \"Westminster Review\", the \"Saturday Review\", the \"Reader\", the \"Pall Mall Gazette\", \"Macmillan's Magazine\", the \"Contemporary Review\". Germany was still ahead in formal science education, but interested people in Victorian Britain could use their initiative and find out what was going on by reading periodicals and using the lending libraries.\n\nIn 1868 Huxley became Principal of the South London Working Men's College in Blackfriars Road. The moving spirit was a portmanteau worker, Wm. Rossiter, who did most of the work; the funds were put up mainly by F.D. Maurice's Christian Socialists. At sixpence for a course and a penny for a lecture by Huxley, this was some bargain; and so was the free library organised by the college, an idea which was widely copied. Huxley thought, and said, that the men who attended were as good as any country squire.\n\nThe technique of printing his more popular lectures in periodicals which were sold to the general public was extremely effective. A good example was \"The physical basis of life\", a lecture given in Edinburgh on 8 November 1868. Its theme — that vital action is nothing more than \"the result of the molecular forces of the protoplasm which displays it\" — shocked the audience, though that was nothing compared to the uproar when it was published in the \"Fortnightly Review\" for February 1869. John Morley, the editor, said \"No article that had appeared in any periodical for a generation had caused such a sensation\". The issue was reprinted seven times and protoplasm became a household word; \"Punch\" added 'Professor Protoplasm' to his other soubriquets.\n\nThe topic had been stimulated by Huxley seeing the cytoplasmic streaming in plant cells, which is indeed a sensational sight. For these audiences Huxley's claim that this activity should not be explained by words such as vitality, but by the working of its constituent chemicals, was surprising and shocking. Today we would perhaps emphasise the extraordinary structural arrangement of those chemicals as the key to understanding what cells do, but little of that was known in the nineteenth century.\n\nWhen the Archbishop of York thought this 'new philosophy' was based on Auguste Comte's positivism, Huxley corrected him: \"Comte's philosophy [is just] Catholicism minus Christianity\" (Huxley 1893 vol 1 of Collected Essays \"Methods & Results\" 156). A later version was \"[positivism is] sheer Popery with M. Comte in the chair of St Peter, and with the names of the saints changed\". (lecture on \"The scientific aspects of positivism\" Huxley 1870 \"Lay Sermons, Addresses and Reviews\" p149). Huxley's dismissal of positivism damaged it so severely that Comte's ideas withered in Britain.\n\nDuring his life, and especially in the last ten years after retirement, Huxley wrote on many issues relating to the humanities.\n\nPerhaps the best known of these topics is \"Evolution and Ethics\", which deals with the question of whether biology has anything particular to say about moral philosophy. Both Huxley and his grandson Julian Huxley gave Romanes Lectures on this theme. For a start, Huxley dismisses religion as a source of moral authority. Next, he believes the mental characteristics of man are as much a product of evolution as the physical aspects. Thus, our emotions, our intellect, our tendency to prefer living in groups and spend resources on raising our young are part and parcel of our evolution, and therefore inherited.\n\nDespite this, the \"details\" of our values and ethics are not inherited: they are partly determined by our culture, and partly chosen by ourselves. Morality and duty are often at war with natural instincts; ethics cannot be derived from the \"struggle for existence\": \"Of moral purpose I see not a trace in nature. That is an article of exclusively human manufacture.\" It is therefore our responsibility to make ethical choices (see Ethics and Evolutionary ethics). This seems to put Huxley as a compatibilist in the Free Will vs Determinism debate. In this argument Huxley is diametrically opposed to his old friend Herbert Spencer.\n\nHuxley's dissection of Rousseau's views on man and society is another example of his later work. The essay undermines Rousseau's ideas on man as a preliminary to undermining his ideas on the ownership of property. Characteristic is: \"The doctrine that all men are, in any sense, or have been, at any time, free and equal, is an utterly baseless fiction.\"\n\nHuxley's method of argumentation (his strategy and tactics of persuasion in speech and print) is itself much studied. His career included controversial debates with scientists, clerics and politicians; persuasive discussions with Royal Commissions and other public bodies; lectures and articles for the general public, and a mass of detailed letter-writing to friends and other correspondents. A large number of textbooks have excerpted his prose for anthologies.\n\nHuxley worked on ten Royal and other commissions (titles somewhat shortened here). The Royal Commission is the senior investigative forum in the British constitution. A rough analysis shows that five commissions involved science and scientific education; three involved medicine and three involved fisheries. Several involve difficult ethical and legal issues. All deal with possible changes to law and/or administrative practice.\n\n\n\nIn 1855, he married Henrietta Anne Heathorn (1825–1915), an English émigrée whom he had met in Sydney. They kept correspondence until he was able to send for her. They had five daughters and three sons:\n\n\nHuxley's relationships with his relatives and children were genial by the standards of the day—so long as they lived their lives in an honourable manner, which some did not. After his mother, his eldest sister Lizzie was the most important person in his life until his own marriage. He remained on good terms with his children, more than can be said of many Victorian fathers. This excerpt from a letter to Jessie, his eldest daughter is full of affection:\n\nHuxley's descendants include children of Leonard Huxley:\n\n\nOther significant descendants of Huxley, such as Sir Crispin Tickell, are treated in the Huxley family.\n\nBiographers have sometimes noted the occurrence of mental illness in the Huxley family. His father became \"sunk in worse than childish imbecility of mind\", and later died in Barming Asylum; brother George suffered from \"extreme mental anxiety\" and died in 1863 leaving serious debts. Brother James, a well known psychiatrist and Superintendent of Kent County Asylum, was at 55 \"as near mad as any sane man can be\"; and there is more. His favourite daughter, the artistically talented Mady (Marian), who became the first wife of artist John Collier, was troubled by mental illness for years. She died of pneumonia in her mid-twenties.\n\nAbout Huxley himself we have a more complete record. As a young apprentice to a medical practitioner, aged thirteen or fourteen, Huxley was taken to watch a post-mortem dissection. Afterwards he sank into a 'deep lethargy' and though Huxley ascribed this to dissection poisoning, Bibby and others may be right to suspect that emotional shock precipitated the depression. Huxley recuperated on a farm, looking thin and ill.\n\nThe next episode we know of in Huxley's life when he suffered a debilitating depression was on the third voyage of HMS \"Rattlesnake\" in 1848. Huxley had further periods of depression at the end of 1871, and again in 1873. Finally, in 1884 he sank into another depression, and this time it precipitated his decision to retire in 1885, at the age of only 60. This is enough to indicate the way depression (or perhaps a moderate bi-polar disorder) interfered with his life, yet unlike some of the other family members, he was able to function extremely well at other times.\n\nThe problems continued sporadically into the third generation. Two of Leonard's sons suffered serious depression: Trevennen committed suicide in 1914 and Julian suffered a breakdown in 1913, and five more later in life.\n\nDarwin's ideas and Huxley's controversies gave rise to many cartoons and satires. It was the debate about man's place in nature that roused such widespread comment: cartoons are so numerous as to be almost impossible to count; Darwin's head on a monkey's body is one of the visual clichés of the age. The \"Great Hippocampus Question\" attracted particular attention:\n\n\n\n\n\n\n \n",
    "id": "30038",
    "title": "Thomas Henry Huxley"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1574457",
    "text": "BioBlitz\n\nA BioBlitz, also written without capitals as bioblitz, is an intense period of biological surveying in an attempt to record all the living species within a designated area. Groups of scientists, naturalists and volunteers conduct an intensive field study over a continuous time period (e.g., usually 24 hours). There is a public component to many BioBlitzes, with the goal of getting the public interested in biodiversity. To encourage more public participation, these BioBlitzes are often held in urban parks or nature reserves close to cities.\n\nA BioBlitz has different opportunities and benefits than a traditional, scientific field study. Some of these potential benefits include:\n\nThe term \"BioBlitz\" was first coined by U.S. National Park Service naturalist Susan Rudy while assisting with the first BioBlitz. The first BioBlitz was held at Kenilworth Aquatic Gardens, Washington D.C. in 1996. Approximately 1000 species were identified at this first event. This first accounting of biodiversity was organized by Sam Droege (USGS) and Dan Roddy (NPS) with the assistance of other government scientists. The public and especially the news media were invited. Since the success of the first bioblitz, many organisations around the world have repeated this concept.\n\nSince then, most BioBlitz contain a public component so that adults, kids, teens and anyone interested can join experts and scientists in the field. Participating in these hands-on field studies is a fun and exciting way for people to learn about biodiversity and better understand how to protect it.\n\nIn 1998, Harvard biologist E.O. Wilson and Massachusetts wildlife expert Peter Alden developed a program to catalog the organisms around Walden Pond. This led to a statewide program known as Biodiversity Days. This concept is very similar to a BioBlitz and occasionally the two terms are used interchangeably.\n\nA variation on the BioBlitz, the Blogger Blitz began in 2007. Rather than gather volunteers and scientists at one location, participant blogs pledged to conduct individual surveys of biodiveristy. These results were then compiled and mapped. The purpose of this blitz is not to survey down to species level across all taxonomic groups, but rather to raise awareness about biodiversity and provide a general snapshot of diversity.\n\nFrom 2006 through 2016 National Geographic Society and the US National Park Service partnered to put on a Bioblitz in a different National Park each year culminating in a Bioblitz across the National Park Service in 2016 as part of the National Park Service Centennial Celebration. The iNaturalist platform was used as the recording tool for the 2014, 2015, and 2016 Centennial Bioblitzes in this series. \"The National Park Service and National Geographic hosted the first ever nationwide National Parks BioBlitz with scientists, park managers, and the public serving as citizen scientists making more than 60,000 observations on the mobile app iNaturalist. This endeavor has currently documented nearly 7,000 species, including species that are new to parks’ species lists, and engaged an estimated 80,000 public participants.\" Check out the iNaturalist results.\n\nHighlights of the 2016 nationwide BioBlitz include:\n\n\nActive Bioblitz\nInactive and historic BioBlitz\n\n\nBioBlitz Events in Hungary are organized by the Hungarian Biodiversity Research Society http://www.biodiverzitasnap.hu/ since 2006, starting with the eco-village Gyürüfű and its surroundings in Baranya County. Since then the Society organizes BioBlitz Events (called also Biodiversity Days) every year, sometimes even several events a year, during which 60-80 experts and researchers contribute to a profound momentary inventory of a chosen area in Hungary, and from time to time in cross-border areas in joint-projects with neighbour countries. The Hungarian Biodiversity Research Society invites local inhabitants and the interested public to join their events, and focusses in its outreach to young local and regional pupils and their teachers just like students from Hungary and abroad. The BioBlitz Events are taking place in partnership with the local National Park Directories, Municipalities and Civil Organisations. A rather fresh approach is the involvement of high school students during their obligatory community/voluntary work into research and field work in the topics of biodiversity and nature protection based upon long term co-operation contracts with schools and educational centres. The main goals pursued by the Hungarian Biodiversity Research Society are to promote the correct understanding of biodiversity in its true context, based upon data collection, monitoring, research and expertise, passing on knowledge from generation to generation and outreach to the broader public. It also aims to strengthen national and international networks. The results of the BioBlitz Events are published in print and on-line media and serve mainly as fundamentals for maintenance-instructions for protected areas and for appropriate natural-resource management, but also for educational purposes.\n\n\n\n\n\n\n\n\n\n\n\nBristol Natural History Consortium now host the National BioBlitz Network. (www.bnhc.org.uk)\n\n\n\n",
    "id": "1574457",
    "title": "BioBlitz"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=10300128",
    "text": "Systematic Census of Australian Plants\n\nThe Systematic census of Australian plants, with chronologic, literary and geographic annotations, more commonly known as the Systematic Census of Australian Plants, also known by its standard botanic abbreviation Syst. Census Austral. Pl., is a survey of the vascular flora of Australia prepared by Government botanist for the state of Victoria Ferdinand von Mueller and published in 1882.\n\nVon Mueller describes the development of the census in the preface of the volume as an extension of the seven volumes of the \"Flora Australiensis\" written by George Bentham. A new flora was necessary since as more areas of Australia were explored and settled, the flora of the island-continent became better collected and described. The first census increased the number of described species from the 8125 in \"Flora Australiensis\" to 8646. The book records all the known species indigenous to Australia and Norfolk Island; with records of species distribution. \n\nVon Mueller noted that by 1882 it had become difficult to distinguish some introduced species from native ones:\nThe lines of demarkation between truly indigenous and more recently immigrated plants can no longer in all cases be drawn with precision; but whereas \"Alchemilla vulgaris\" and \"Veronica serpyllifolia\" were found along with several European Carices in untrodden parts of the Australian Alps during the author's earliest explorations, \"Alchemilla arvensis\" and \"Veronica peregrina\" were at first only noticed near settlements. The occurrence of \"Arabis glabra\", \"Geum urbanum\", \"Agiimonia eupatoria\", \"Eupatorium cannabinum\", \"Cavpesium cernuum\" and some others may therefore readily be disputed as indigenous, and some questions concerning the nativity of various of our plants will probably remain for ever involved in doubts.\n\nIn 1889 an updated edition of the census was published, the \"Second Systematic Census\" increased the number of described species to 8839. Von Mueller dedicated both works to Joseph Dalton Hooker and A. P. de Candolle.\n\nThe work is of historic significance as the first Australian flora written in Australia. Following its publication, research and writing on the flora of Australia has largely been carried out in Australia. \n\n\n",
    "id": "10300128",
    "title": "Systematic Census of Australian Plants"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=22022351",
    "text": "Census of Coral Reefs\n\nThe Census of Coral Reefs (CReefs) is a field project of the Census of Marine Life that surveys the biodiversity of coral reef ecosystems internationally. The project works to study what species live in coral reef ecosystems, to develop standardized protocols for studying coral reef ecosystems, and to increase access to and exchange of information about coral reefs scattered throughout the globe. \n\nThe CReefs project uses the implementation of autonomous reef-monitoring structures (ARMS) to study the species that inhabit coral reefs. These structures are placed on the sea floor in areas where coral reefs exist, where they are left for one year. At the end of the year, the ARMvS is pulled to the surface, along with the species which have inhabited it, for analysis. \n\nCoral reefs are thought to be the most organically different of all marine ecosystems. Major declines in key reef ecosystems suggest a decline in reef population throughout the world due to environmental stresses. The vulnerability of coral reef ecosystems is expected to increase significantly in response to climate change. The reefs are also being threatened by induced coral bleaching, ocean acidification, sea-level rise, and changing storm tracks. Reef biodiversity could be in danger of being lost before it is even documented, and researchers will be left with a limited and poor understanding of these complex ecosystems.\n\nIn an attempt to enhance global understanding of reef biodiversity, the goals of the CReefs Census of Coral Reef Ecosystems were to conduct a diverse global census of coral reef ecosystems. And increase access to and exchange of coral reef data throughout the world. Because coral reefs are the most diverse and among the most threatened of all marine ecosystems, there is great justification to learn more about them.\n",
    "id": "22022351",
    "title": "Census of Coral Reefs"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=22023966",
    "text": "Global Census of Marine Life on Seamounts\n\nGlobal Census of Marine Life on Seamounts (commonly CenSeam) is a global scientific initiative, launched in 2005, that is designed to expand the knowledge base of marine life at seamounts. Seamounts are underwater mountains, not necessarily volcanic in origin, which often form subsurface archipelagoes and are found throughout the world's ocean basins, with almost half in the Pacific. There are estimated to be as many as 100,000 seamounts at least one kilometer in height, and more if lower rises are included. However, they have not been explored very much—in fact, only about half of one percent have been sampled—and almost every expedition to a seamount discovers new species and new information. There is evidence that seamounts can host concentrations of biologic diversity, each with its own unique local ecosystem; they seem to affect oceanic currents, resulting among other things in local concentration of plankton which in turn attracts species that graze on it, and indeed are probably a significant overall factor in biogeography of the oceans. They also may serve as way stations in the migration of whales and other pelagic species. Despite being poorly studied, they are heavily targeted by commercial fishing, including dredging. In addition they are of interest to potential seabed mining.\n\nThe overall goal of CenSeam is \"to determine the role of seamounts in the biogeography, biodiversity, productivity, and evolution of marine organisms, and to evaluate the effects of human exploitation on seamounts.\" To this effect, the group organizes and contributes to various research efforts about seamount biodiversity. Specifically, the project aims to act as a standardized scaffold for future studies and samplings, citing inefficiency and incompatibility between individual research efforts in the past. To give a scale of their mission, there are an estimated 100,000 seamounts in the ocean, but only 350 of them have been sampled, and only about 100 sampled thoroughly. Although sampling all 100,000 seamounts is infeasible, major seamounts can be sampled in such a way.\n\nCenSeam is a subdivision of the Census of Marine Life program. Organisationally, the components of CenSeam consist of a secretariat (Malcolm Clark, Mireille Consalvey, Ashley Rowden and Karen Stocks) which is hosted by the National Institute of Water and Atmospheric Research in Wellington, New Zealand; an international steering committee; a taxonomic advisory panel; and two working groups, Data Analysis and Standardisation.\n\nIn 2008 CenSeam began collaborating with the International Seabed Authority to study effects of seabed mining on seamount ecosystems.\n\n\n",
    "id": "22023966",
    "title": "Global Census of Marine Life on Seamounts"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=7391667",
    "text": "Census of Marine Life\n\nThe Census of Marine Life was a 10-year scientific initiative, involving a global network of researchers in more than 80 nations, engaged to assess and explain the diversity, distribution, and abundance of life in the oceans. The world's first comprehensive Census of Marine Life — past, present, and future — was released in 2010 in London.\n\nThe Census consists of three major component themes organized around the questions:\n\n\nCensus researchers undertook the task of constructing the history of marine animal populations since human predation became important, roughly the last 500 years. This program component is the History of Marine Animal Populations (HMAP).\n\nThe largest component of the Census involved investigating what now lives in the world's oceans through 14 field projects. Each sampled the biota in one of six realms of the global oceans using a range of technologies. Details of these field projects are provided below.\n\nForecasting what will live in the oceans involves modeling and simulation. This component program was the Future of Marine Animal Populations (FMAP). This group focused on integrating data from different sources and creating statistical and analytical tools to make predictions for marine populations and ecosystems.\n\nThe global initiative required a state-of-the-art data assimilation framework, and this effort, the Ocean Biogeographic Information System (OBIS), forms the fourth component program of the Census. The vision is that users will be able to click on maps of the oceans on their laptop or desktop anywhere in the world and bring up Census data on what is reported to live in the ocean zone of interest. At the end of 2010, OBIS contained more than 30 million records. OBIS is designed to make sharing data easy, helping to improve understanding of the patterns and processes that govern marine life.\n\nCoML was engaged in 14 field projects:\n\n• The Census of Diversity of Abyssal Marine Life documented actual species diversity of abyssal plains as a basis for global change research and for a better understanding of historical causes and actual ecological factors regulating biodiversity. CeDAMar collected reliable data on the large-scale distribution of one of the largest and most inaccessible environments on the planet.\n\nThe Census of Antarctic Marine Life (CAML) investigated the distribution and abundance of Antarctica’s marine biodiversity, how it is affected by climate change, and how change will alter the nature of the ecosystem services currently provided by the Southern Ocean for the benefit of mankind.\n\nThe Arctic Ocean Diversity (ArcOD) Census of Marine Life project documented the present Arctic Ocean biodiversity on a Pan-Arctic scale. The emphasis of this program was on biodiversity because processes are critically impacted by the composition of biota involved in them. The operational approach was to help coordinate, encourage and support research efforts designed to examine the biodiversity in the three major realms: sea ice, water column and seafloor.\n\nCOMARGE was a field project of the Census of Marine Life launched in 2005. The project described biodiversity patterns of benthic and bentho-demersal communities on continental margins, with a focus on multiple habitats and spatial scales, and identifying the contribution of environmental heterogeneities to these patterns. The project is led by Myriam Sibuet and managed by Lenaick Menot, both of France.\n\nThe Pacific Ocean Shelf Tracking (POST) program developed and promoted the application of new electronic tagging technology to study the marine life history of Pacific salmon. A major area of focus of the POST project involved the development of a permanent continental-scale marine telemetry system. POST's array sits on the seabed of the continental shelf and upstream in several major rivers and is used to monitor the movements of not only salmon but many other types of marine animals along the shelf. Tracking data generated from the array can be applied toward the development of fishery management policies aimed at the sustainable harvest of resources, and to the understanding and conservation of other marine and diadromous species.\n\nBy enhancing global understanding of reef biodiversity, the CoML Census of Coral Reef Ecosystems (CReefs) conducted a taxonomically diversified global census of coral reef ecosystems, increased tropical taxonomic knowledge, developed new, universal protocols (e.g. DNA-based technologies and long term sampling devices) and increased access to and exchange of coral reef data dispersed throughout the world.\n\nThe International Census of Marine Microbes (ICoMM) facilitated the inventory of microbial diversity and developed a strategy to catalogue all known diversity of single-cell organisms inclusive of the Bacteria, Archaea, Protista and associated viruses, and explored and discovered unknown microbial diversity, and placed that knowledge into appropriate ecological and evolutionary contexts.\n\nMAR-ECO described the patterns of distribution, abundance, and the trophic relationships among the organisms inhabiting the waters over and around the Mid-Atlantic Ridge. It identified and modeled the ecological processes that cause variability in these patterns. The project focused on fish, crustaceans, cephalopods, and gelatinous plankton and other actively swimming organisms, but there was focus on top predators such as seabirds and cetaceans, which interact with the more surface environment.\n\nThe Natural Geography In Shore Areas (NaGISA) project was a collaborative effort aimed at inventorying and monitoring habitat specific biodiversity in the global nearshore. The international character of the project and its target zone were reflected in the word \"nagisa\", which is Japanese for the narrow coastal zone where the land meets the sea. NaGISA held a unique position in the Census of Marine Life as an ambassador project, linking CoML to local interests around the world. NaGISA's first aim was to drawi up a global baseline of nearshore biodiversity and then use its network to continue monitoring those same shores for the next 50 years.\n\nThe Gulf of Maine was selected as the ecosystem pilot study for CoML. This program gained knowledge to enable ecosystem-based management in a large marine environment. The program advanced knowledge of both biodiversity and ecological processes over a range of habitats and food-chain levels, from plankton to whales.\n\nCenSeam was a global study of seamount ecosystems, to determine their role in the biogeography, biodiversity, productivity, and evolution of marine organisms, and to evaluate the effects of human exploitation. CenSeam commenced in 2005 and the CenSeam science community, with particular input from CenSeam's Data Analysis Working Group (DAWG), defined two overarching priority themes (1) What factors drive community composition and diversity on seamounts, including any differences between seamounts and other habitat types? (2) What are the impacts of human activities on seamount community structure and function?\n\nThe Tagging of Pacific Predators (TOPP) research program was a collaboration among scientists from the U.S., Australia, Canada, Mexico, Japan, France and the UK, that applied new technologies to understanding the environmental basis for movements and behaviors of large pelagic animals in the North Pacific. With new electronic tags, TOPP scientists followed the migrations of marine fishes, turtles, birds, pinnipeds, whales and Humboldt squid as they crisscrossed the Pacific basin. The results answer basic questions about the animals' biology including where they feed and breed, and what migration corridors they use.\n\nChEss improved the knowledge of the biodiversity and biogeography of species from deep-water chemosynthetically-driven ecosystems at a global scale and increased the understanding of the processes that shape these communities. Scientist located under the ChEss umbrella were global in nature. ChEss addressed the main questions of CoML on diversity, abundance and distribution of marine species, within the realm of deep-water reducing environments such as hydrothermal vents, cold seeps, whale falls, sunken wood and areas of low oxygen that intersect with continental margins and seamounts. ChEss scientists combined results from research on all these systems in order to understand the phylogeographic relationships amongst all deep-water chemosynthetic ecosystems.\n\nChEss scientist, Michel Segonzac, was co-finder of the popular media sensation \"Kiwa hirsuta\".\n\nThe Census of Marine Zooplankton (CMarZ) worked toward a taxonomically comprehensive assessment of biodiversity of animal plankton throughout the world oceans. The project produced information on zooplankton species diversity, biomass, biogeographical distribution, genetic diversity, and community structure. The taxonomic focus was the animals that drift with ocean currents throughout their lives (i.e., the holozooplankton). This assemblage currently includes ~7,000 described species in fifteen phyla. The Census encompassed unique marine environments and those likely to be inhabited by endemic and undescribed zooplankton species.\n\nThree non-field projects in which CoML is engaged are:\nThe Ocean Biogeographic Information System, or OBIS, is an international information system focused on marine biodiversity. It provides expert geo-referenced data on marine species and currently contains more than 30 million georeferenced, accurately identified species records from more than 800 databases. OBIS provides spatial query tools for visualizing relationships among species and their environment. This information is readily and freely accessible by the Internet and requires no special software to use.\n\nFMAP attempted to describe and synthesize globally changing patterns of species abundance, distribution, and diversity, and to model the effects of fishing, climate change and other key variables on those patterns. This work was done across ocean realms and with an emphasis on understanding past changes and predicting future scenarios.\n\nThe History of Marine Animal Populations (HMAP) improved the understanding of ecosystem dynamics, specifically with regard to long-term changes in stock abundance, the ecological impact of large-scale harvesting by man, and the role of marine resources in the historical development of human society. Since the earliest historical records, man has harvested a variety of different animals from the oceans. While ecologists have traditionally aimed to identify the current conditions of many of the animal populations affected both directly and indirectly by harvesting, much less focus has been given to the status of affected populations in earlier times. HMAP created a historical reference point of marine populations against which modern populations can be compared to determine how ocean ecosystems are changing with respect to human impact and even climate change.\n\nA Mapping and Visualization Team based at Duke University's Marine Geospatial Ecology Lab developed and shared methods to display the results of the ten-year Census of Marine Life.\n\nOne of the global goals of the Census of Marine Life was strengthening support for marine biodiversity research at the national or regional level that will continue after the CoML has concluded. Following in the footsteps of other successful global research programs, several nations and groups of nations brought together regional CoML stakeholders, such as researchers, government and non-government agencies, and resource managers, to assess the status of knowledge of marine biodiversity in their waters. These assessments led to the organization of National or Regional Implementation Committees (NRICs) to implement more local programs and improve the geographic scope of CoML and its projects. By engaging scientists, funding agencies, policy-makers and the broad user community, these national and regional committees identified their research and data priorities for marine biodiversity and find ways to make them happen by building partnerships, exploring funding opportunities for local science, and promoting CoML to local audiences. The committees worked under the umbrella of the CoML.\n\nAreas of the globe represented by an NRIC include:\n\nCoML was a partner with the Encyclopedia of Life in their effort to create descriptive web-page records for all the known species on Earth. CoML was an integral part on creating pages for marine species.\n\nCoML was a partner with Barcode of Life to help create DNA barcoding for the anticipated 230,000 marine species known thus far. CoML provided scientific data to Barcode of Life to greatly accelerate the process of creating DNA barcodes.\n\nGoogle and Census of Marine Life partnered on Google Earth 5.0. Ocean in Google Earth contains a layer devoted to the Census of Marine Life that allows users to follow scientists from the Census on expeditions and see marine life and features found during the Census.\n\nThe Census of Marine Life housed a Synthesis Group that integrated and synthesized the large amounts of information produced by CoML projects into common themes and overarching messages. This group produced a comprehensive and cohesive suite of products released to the public in 2010. Products include books, webpages, collections, articles, maps, and many others.\n\n\n\n",
    "id": "7391667",
    "title": "Census of Marine Life"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=22034606",
    "text": "Census of Marine Zooplankton\n\nThe Census of Marine Zooplankton is a field project of the Census of Marine Life that has aimed to produce a global assessment of the species diversity, biomass, biogeographic distribution, and genetic diversity of more than 7,000 described species of zooplankton that drift the ocean currents throughout their lives. CMarZ focuses on the deep sea, under-sampled regions, and biodiversity hotspots. \n\nTechnology plays a great role in CMarZ's research, including the use of integrated morphological and molecular sampling through DNA Barcoding. CMarZ makes its datasets available via the CMarZ Database.\n\n",
    "id": "22034606",
    "title": "Census of Marine Zooplankton"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=28105864",
    "text": "Biogeography of Deep-Water Chemosynthetic Ecosystems\n\nThe Biogeography of Deep-Water Chemosynthetic Ecosystems is a field project of the Census of Marine Life programme (CoML). The main aim of ChEss is to determine the biogeography of deep-water chemosynthetic ecosystems at a global scale and to understand the processes driving these ecosystems. ChEss addresses the main questions of CoML on diversity, abundance and distribution of marine species, focusing on deep-water reducing environments such as hydrothermal vents, cold seeps, whale falls, sunken wood and areas of low oxygen that intersect with continental margins and seamounts.\n\nDeep-sea hydrothermal vents and their associated fauna were first discovered along the Galapagos Rift in the eastern Pacific in 1977. Vents are now known to occur along all active mid ocean ridges and back-arc spreading centres, from fast to ultra-slow spreading ridges. The interest in chemosynthetic environments was strengthened by the discovery of chemosynthetic-based fauna at cold seeps along the base of the Florida Escarpment in 1983. Cold seeps occur along active and passive continental margins. More recently, the study of chemosynthtetic fauna has extended to the communities that develop in other reducing habitats such as whale falls, sunken wood and areas of oxygen minima when they intersect with the margin or seamounts.\nSince the first discovery of hydrothermal vents, more than 600 species have been described from vents and seeps. This is equivalent of 1 new description every 2 weeks(!). As biologists, geochemists, and physicists combine research efforts in these systems, new species will certainly be discovered. Moreover, because of the extreme conditions of the vent and seep habitat, certain species may have specific physiological adaptations with interesting results for the biochemical and medical industry.\n\nThese globally distributed, ephemeral and insular habitats that support endemic faunas offer natural laboratories for studies on dispersal, isolation and evolution. Here, hydrographic and topographic controls on biodiversity and biogeography might be much more readily resolved than in systems where climate and human activity obscure their role. In addition, hydrothermal vents have been suggested to be the habitat of the origin of life. These hypotheses are being used by ChEss researchers in collaboration with NASA to develop programmes to search for life in planets or moons of the outer space.\n\nOnly a small fraction of the global ridge system (~65000 km) and of the vast continental margin regions have been explored and their communities described. It is the aim of ChEss to improve the knowledge on the diversity, abundance and distribution of species from vents, seeps and other reducing habitats at a global scale, understanding their the abiotic and biotic processes that shape and maintain these ecosystems and their biogeography.\n\nMain ChEss Science Questions\n\n\"Objective 1. To create a centralised database\"\n\nTo create a centralised database, ChEssBase, of deep-water vent, cold seep, whalefall and OMZ species. ChEssBase is a web-based database that incorporates archived and newly collected biological material. The database is geo- and bio-referenced. ChEssBase is available online and has been integrated with OBIS.\n\n\"Objective 2. To develop a long-term field programme\"\n\nTo develop a long-term field programme to locate potential vent and seep sites and continue research on whalefalls and OMZ sites. The field programme aims to explain the main gaps in our knowledge of the diversity, abundance and distribution of chemosynthetic species globally. A limited number of target areas have been selected where specific scientific questions relevant to biogeographical issues will be answered.\n\nThe target areas have been grouped into two categories. Category I, combined areas: Area A: Equatorial Atlantic Belt region; Area B: the SE Pacific region; Area C: NZ region; Area D: the Arctic and Antarctic regions, within the International Polar Year. Category II, specific areas: 1 – The ice-covered Gakkel Ridge, 2 – the (ultra)-slow ridges of the Norwegian-Greenland Sea, 3 – the northern MAR between the Iceland and Azores hot-spots; 4 – the Brazilian continental margin, 5 – the East Scotia Ridge and Bransfield Strait, 6 – the SW Indian Ridge, 7 – the Central Indian Ridge.\nDuring the field programme, ChEss will promote the development and refinement of deep-towed, remotely operated (ROV) and autonomous underwater (AUV) vehicle technologies to locate, map and sample new chemosynthetic systems. Using optical, chemical and acoustic techniques, ChEss researchers hope to gain a better understanding of not only biogeographical patterns, but to determine the processes driving these ecosystems.\n\nDuring the field programme, ChEss will promote the development and refinement of deep-towed, remotely operated (ROV) and autonomous underwater (AUV) vehicle technologies to locate, map and sample new chemosynthetic systems. Using optical, chemical and acoustic techniques, ChEss researchers hope to gain a better understanding of not only biogeographical patterns, but to determine the processes driving these ecosystems.\n\n\"Objective 3: Outreach and Education\"\n\nChEss has multi-lingual education pages related to vents, seeps and whalefalls. There is a dedicated page for key outreach initiatives such as live cruise diaries, open days, schools activities etc.\n\nChEss has joined forces with the other deep-sea CoML projects and this has resulted in the creation of the DEep-Sea Education and Outreach group (DESEO) that has produced a book \"Deeper than Light\" published in 5 languages.\n\n",
    "id": "28105864",
    "title": "Biogeography of Deep-Water Chemosynthetic Ecosystems"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=42192153",
    "text": "Gradsect\n\nA gradsect or gradient-directed transect is a low-input, high-return sampling method where the aim is to maximise information about the distribution of biota in any area of study. Most living things are rarely distributed at random, their placement being largely determined by a hierarchy of environmental factors. For this reason, standard statistical designs based on purely random sampling or systematic (e.g. grid-based) systems tend to be less efficient in recovering information about the distribution of taxa than sample designs that are purposively directed instead along deterministic environmental gradients.\n\nEcologists have long been aware of the significance of environmental gradient based approaches to better understand community dynamics and this is reflected especially in the work of Robert Whittaker (1967) and others. Although in practice, life-scientists intuitively sample gradients, until the early 1980s there was little formal theoretical or empirical support for such an approach, sample design being driven largely by traditional statistical methods based on probability theory incorporating random sampling.\n\nIntensively sampled landscape-based surveys in Australia provided a reference platform for developing and testing a less logistically demanding and yet statistically acceptable gradient-based survey design that avoided the need for random or purely grid-based sampling. These initial studies and subsequently developed statistical support for purposive, gradient-based survey provided a formalized, practical alternative to more logistically demanding traditional designs. It was here the term gradsect was coined that coupled purposive, transect sampling with a hierarchical framework of environmental gradients considered to be key determinants of species distribution.\n\nIn constructing a gradsect, existing information is initially reviewed in which a hierarchy of environmental gradients is first identified either by visual means (maps, aerial photographs etc..) or through numerical analysis or spatial analysis of institutional or other data sources. A typical regional gradsect for example may be constructed according to a primary climate gradient (temperature, moisture, seasonality) then a secondary gradient (geomorphology, lithology, major and minor drainage systems), a tertiary gradient possibly represented by a local soil catena or local land use farming system or finer scale gradient levels representing local vegetational sequences. Through an inspection of spatial overlays of all gradients, a minimum number of sample locations is then purposively located to reflect, as far as possible, total environmental variation. For logistic and other purposes (such as improving the capacity to locate rare species) the steepest gradients are usually selected. In this way an \"ideal\" gradsect is constructed that may then be modified to accommodate logistic tradeoffs. The selection discipline requires that the fullest possible range of each hierarchical level is sampled. This commonly results in a set of progressively nested clusters of sample sites contained within the overarching primary gradient that may not reflect a linear distribution. At relatively local landscape scale, a primary gradients may be represented by salinity levels or water depth as in tidal wetlands or micro-topographic relief as in forest margins or a riparian zone. For most practical purposes, transects are commonly laid out along contours perpendicular to the main direction of the gradient. Iterative spatial analysis of environmental layers over a digital elevation model can then be used to identify areas requiring additional sampling thereby improving environmental representativeness.\n\nInitial studies in gradsect development revealed considerable logistic and other advantages over more traditional non-gradient-based survey designs concerned primarily with random sampling. This finding is now widely supported especially in biodiversity and other areas of environmental surveying and conservation design (see Applications next). Apart from improved logistic efficiency, the gradsect method seeks to maximise environmental representativeness which has the dual advantage of potentially improving location of rarities and enhancing spatial modelling of species distribution. Because the underlying statistical model is not based on probability theory, gradsect sampling cannot be used to estimate numbers of species or other biological attributes per unit area. For that purpose some measure of random sampling needs to be built into the sample design.\n\nSince the publication of gradsect theory in 1984, subsequent vegetational and landscape studies in regional Australia (Austin and Heyligers 1989; Ludwig and Tongway (1995) were followed by a successful evaluation of the method in faunal surveys in South Africa (Wessels et al.). Since then applications involving gradsects have ranged from habitat suitability studies of fungi (Shearer and Crane 2011 ), termites (Gillison et al. 2003) other macro invertebrates (Lawes et al. 2005 ); birds (Damalas 2005) small and large mammals (Laurance 1994; Ramono et al. 2009). Vegetation studies using gradsects have been widely applied in many countries ranging from tidal wetlands (Parker et al. 2011) and agricultural cropping systems and forested landscape mosaics (Gillison et al. 2004) to infectious diseases (Boone et al. 2000 ). At broader geographic and national scales (Grossman et al., 1998, 2007; USA/NPS 2012) gradsects have been applied to guide field sampling and forest mapping in mountainous terrain (Sandman and Lertzmann 2003) as well as wide-ranging remote sensing applications (Mallinis et al. 2008; Rocchini et al. 2011 ).\n",
    "id": "42192153",
    "title": "Gradsect"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=43518148",
    "text": "Butterfly count\n\nA butterfly count is an event that is organized for the purpose of identifying and counting butterflies in a specific geographical area. Butterfly counts occur regularly in North America and Europe.\n\nThe counts are conducted by interested, mostly non-professional, residents of the area who maintain an interest in determining the numbers and species of butterflies in their locale. A butterfly count usually occurs at a specific time during the year and is sometimes coordinated to occur with other counts which may include a park, county, entire state or country. The results of the counts are usually shared with other interested parties including professional lepidopterists and researchers. The data gathered during a count can indicate population changes and health within a species.\n\nProfessional, universities, clubs, elementary and secondary schools, other educational providers, nature preserves, parks and amateur organizations can organize a count. The participants often receive training to help them identify the butterfly species. The North American Butterfly Association has organized over 400 counts in 2014.\n\nThere are several methods for counting butterflies currently in use, with the notable division being between restricted and open searches. Most counts are designed to count all butterflies observed in a locality.\n\nCounts may be targeted at single species and, in some cases, butterflies are observed and counted as they move from one area to another. A heavily researched example of butterfly migration is the annual migration of monarch butterflies in North America. Some programs will tag butterflies to trace their migration routes, but these are migratory programs and not butterfly counts. Butterfly counts are sometimes done where there is a concentration (a roost) of a species of butterflies in an area. One example of this is the winter count of western monarch butterflies as they roost together at sites in California, northern Mexico and Arizona.\n\nFrequently referred to as \"Pollard Transects\" or \"Pollard Walks\" in North America, a transect is a protocol designed to standardize the recording of butterfly observations, the initial format was outlined by Ernie Pollard in 1977. The transect protocol involves one observer walking a fixed path at a constant pace, multiple times in a season. Butterflies are counted when they are seen within a prescribed distance from the path, often 2.5 meters on either side of the path, and only when the butterflies are seen in front of, or above, the observer (i.e., no backtracking). A second person may work with the observer to identify and/or photograph insects spotted by the observer. Transects should not change from year to year and ideally should sample a variety of habitats.\n\nExamples of long-running restricted searches are Art Shapiro's Butterfly Project in the US (started in 1972), and the UK Butterfly Monitoring Scheme (started in 1976).\n\nOpen searches, also sometimes referred to as \"checklist searches\", are intended to focus on the presence and abundance of butterflies in a given area. They can be single events such as the North American Butterfly Association's July 1 and July 4 counts in Canada and the U.S. respectively, or they can be regular or ad hoc counts conducted by individuals or groups. The lack of formal structure makes them suitable for many citizen science programs.\n\nIn terms of the relative outcomes or the efficacy of open vs. restricted searches, studies have shown that open searches are more likely to find a greater number of species in a given area. Royer, et al. note that one reason for this is that during an open search, the \"observer is free to search out places where butterflies typically would breed or congregate\" rather than follow the fixed path of a transect.\n\nTo promote the broad public participation in butterfly monitoring, researcher from Austria propose to combine a simplified assessment scheme on group level executed by laypeople, with detailed assessments from butterfly experts. To evaluate their approach they compared data collected by pupils with independent assessments of professional butterfly experts. Beside some identification uncertainties data collected by trained and supervised pupils were successfully used to predict the general habitat quality for butterflies. \n\nOpportunistic or incidental sightings are a butterfly sightings that are not part of a formal count. Observers may note signal butterflies or multiple species. An example of an opportunistic sighting is observing a butterfly in garden and reporting it.\n\nDescribed as a \"special type of open search\", atlas projects are generally targeted at a specific geographical area such as a province or state. The goal is to assess the presence or absence of species, usually over a multi-year period. Each atlas program will design its own data requirements but as they are measuring abundance and presence, they tend to accept data from transects, counts and opportunistic sightings to build a database. The longest running atlas program in North America is the Ontario Butterfly Atlas Online, which is supported by the Toronto Entomologists' Association and began collecting data in 1969.\nTransects and open searches are not as comprehensive in tropical locations due to issues such as density of flora and the heigh of the forest canopy. A count system using bait stations with fermenting fruit has been used to assess specific populations.\n\nParticipants are encouraged to employ a number of techniques to quantify large aggregations by making estimates of butterflies:\n\n\nThe number of butterflies can be estimated by the area size they inhabit, for example, in the overwintering population present in Mexico the population expressed in hectares.\nButterflies can be counted in their egg, larvae and instar number.\n\nButterflies are sometimes captured, tagged and recovered. The number of tags recovered in a specific area is used to determine population size and direction of flight.\n\n\n",
    "id": "43518148",
    "title": "Butterfly count"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=21957291",
    "text": "Census of Antarctic Marine Life\n\nThe Census of Antarctic Marine Life (CAML) is a field project of the Census of Marine Life that researches the marine biodiversity of Antarctica, how it is affected by climate change, and how this change is altering the ecosystem of the Southern Ocean.\n\nThe program started in 2005 as a 5-year initiative with the scientific goal being to study the evolution of life in Antarctic waters, to determine how this has influenced the diversity of the present biota, and use these observations to predict how it might respond to future change. However, due to modern and extravagant changes within technology, we are able to witness and influence biodiversity reproduction and development. This enables us to gain further insight toward characteristics that allow such biodiversity to flourish within this barren desert referred to as the Arctic and Antarctic.\n\nCAML has collected its data from 18 Antarctic research vessels during the International Polar Year, which is freely accessible at Scientific Committee on Antarctic Research Marine Biodiversity Information Network (SCAR-MarBIN). The Register of Antarctic Marine Species has 9,350 verified species (16,500 taxa) in 17 phyla, from microbes to whales. For 1500 species the DNA barcode is available.\n\nThe information from CAML is a robust baseline against which future change may be measured.\n\n",
    "id": "21957291",
    "title": "Census of Antarctic Marine Life"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=51485577",
    "text": "Great Elephant Census\n\nThe Great Elephant Census—the largest wildlife survey in history—was an African-wide census designed to provide accurate data about the number and distribution of African elephants by using standardized aerial surveys of hundreds of thousands of square miles or terrain in Africa.\n\nThe census was completed and published in the online journal \"PeerJ\" on 31 August 2016 at a cost of 7 million.\n\nScientists believe that prior to European colonization there were as many as 20 million African elephants. By 1979 only 1.3 million elephants remained on the continent.\n\nA pan-African elephant census has not been conducted since the 1970s. The idea of a modern census was devised by Elephants Without Borders and supported, both financially and logistically, by Paul G. Allen. It was also supported by other organizations and individuals, including African Parks, Frankfurt Zoological Society, Wildlife Conservation Society, The Nature Conservancy, IUCN African Elephant Specialist Group, Howard Frederick, Mike Norton-Griffith, Kevin Dunham, Chris Touless and Curtice Griffin with the report released in September 2016.\n\nMike Chase, the founder of Elephants Without Borders, was the lead scientist of the census. Chase lead a group of 90 scientists and 286 crew in 18 African countries for over two years to collect the data. During this time the team flew a distance of over , equivalent to flying to the moon and a quarter of the way back, in over 10,000 hours of collecting data. The area covered represents 93% of the elephants known range.\n\nForest Elephants which live in central and western Africa were excluded from the survey.\n\nThe final report was released on 31 August 2016 in Honolulu at the IUCN World Conservation Congress. Data collected showed a 30 percent decline in the population of African savanna elephant in 15 of the 18 countries surveyed. The reduction occurred between 2007 and 2014, representing a loss of approximately 144,000 elephants.\n\nThe total population of Africa's savannah elephants is 352,271, far lower than previously estimated.\n\nThree countries with significant elephant population were not surveyed; Namibia which would not release figures and both South Sudan and the Central African Republic where the survey was postponed as a result of armed conflict.\n\nThe rate of decline in the population is also accelerating and reached 8% in 2014. The loss of population is primarily a result of poaching with the elephants being killed for their tusks. The ivory is then illegally sold in China and the United States. It is estimated approximately 100 elephants are killed every day for ivory. Loss of habitat is another reason for the drastic reduction in population.\n\n84% of the population was sighted in legally protected areas, and high numbers of carcasses also being found in the same protected areas.\n",
    "id": "51485577",
    "title": "Great Elephant Census"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=21957226",
    "text": "Census of Diversity of Abyssal Marine Life\n\nThe Census of Diversity of Abyssal Marine Life (CeDAMar) is a field project of the Census of Marine Life that studies the species diversity of one of the largest and most inaccessible environments on the planet, the abyssal plain. CeDAMar uses data to create an estimation of global species diversity and provide a better understanding of the history of deep-sea fauna, including its present diversity and dependence on environmental parameters. CeDAMar initiatives aim to identify centers of high biodiversity useful for planning both commercial and conservation efforts, and are able to be used in future studies on the effects of climate change on the deep sea.\n\nAs of May 2009, participation by upwards of 56 institutions in 17 countries has resulted in the publication of nearly 300 papers. Results of CeDAMar-related research were also published in a 2010 textbook on deep-sea biodiversity by Michael Rex and Ron Etter, members of CeDAMar's Scientific Steering Committee.()\n\nCeDAMar is led by Dr. Pedro Martinez Arbizu of Germany and Dr. Craig Smith, USA.\n\n",
    "id": "21957226",
    "title": "Census of Diversity of Abyssal Marine Life"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=2785135",
    "text": "Imbibition\n\nImbibition is a special type of diffusion when water is absorbed by solids-colloids causing an enormous increase in volume. Examples include the absorption of water by seeds and dry wood. If it were not for the pressure due to imbibition, seedlings would not have been able to emerge out of soil into the open; they probably would not have been able to establish.\n\nImbibition is also diffusion since water surface potential movement is along a concentration gradient; the seeds and other such materials have almost no water hence they absorb water easily. Water potential gradient between the absorbent and the liquid imbibed is essential for imbibition. In addition, for any substance to imbibe any liquid, affinity between the adsorbant and the liquid is also a pre-requesite.\n\nImbibition occurs when a wetting fluid displaces a non-wetting fluid, contrary to drainage where a non-wetting phase displaces the wetting fluid. The two processes are governed by different mechanisms.\n\nOne example of imbibition that is found in nature is the absorption of water by hydrophilic colloids. Matrix potential contributes significantly to water in such substances. Examples of plant material which exhibit imbibition are dry seeds before germination. Imbibition can also entrain the genetic clock that controls circadian rhythms in Arabidopsis thaliana and (probably) other plants. Another example is that of imbibition in the Amott test.\n\nDifferent types of organic substances have different imbibing capacities. Proteins have a very high imbibing capacity then starch less and cellulose least. That is why proteinaceous pea seeds swell more on imbibition than starchy wheat seeds.\n\nImbibition of water increases the volume of the imbibant, which results in imbibitional pressure. This pressure can be of tremendous magnitude. This fact can be demonstrated by the splitting of rocks by inserting dry wooden stalks in the crevices of the rocks and soaking them in water, a technique used by early Egyptians to cleave stone blocks.\n\nSkin grafts (split thickness and full thickness) receive oxygenation and nutrition via imbibition, maintaining cellular viability until the processes of inosculation and revascularisation have re-established a new blood supply within these tissues.\n\n",
    "id": "2785135",
    "title": "Imbibition"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1525710",
    "text": "Assimilation (biology)\n\nBiological assimilation, or bio-assimilation, is the combination of two processes to supply cells with nutrients. The first is the process of absortion of vitamins, minerals, and other chemicals from food within the gastrointestinal tract. In humans, this is done with a chemical breakdown (enzymes and acids) and physical breakdown (oral mastication and stomach churning). The second process of bio-assimilation is the chemical alteration of substances in the bloodstream by the liver or cellular secretions. Although a few similar compounds can be absorbed in digestion bio-assimilation, the bioavailability of many compounds is dictated by this second process since both the liver and cellular secretions can be very specific in their metabolic action (see chirality). This second process is where the absorbed food reaches the cells via the liver.\n\nMost foods are composed of largely indigestible components depending on the enzymes and effectiveness of an animal's digestive tract. The most well-known of these indigestible compounds is cellulose; the basic chemical polymer in the makeup of plant cell walls. Most animals, however, do not produce cellulase; the enzyme needed to digest cellulose. However some animal and species have developed symbiotic relationships with cellulase-producing bacteria (see termites and metamonads.) This allows termites to use the energy-dense cellulose carbohydrate. Other such enzymes are known to significantly improve bio-assimilation of nutrients. Because of the use of bacterial derivatives enzymatic dietary supplements now contain such enzymes as amylase, glucoamylase, protease, invertase, peptidase, lipase, lactase, phytase, and cellulase. These enzymes improve the overall bioassimilation in the digestive tract but are still not proven to increase bloodstream bioavailability. \nBasically the enzymes and other breakdowns make the bigger substances of food smaller so they can go through the rest of their digestion more easily.\n\n\n",
    "id": "1525710",
    "title": "Assimilation (biology)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=10523316",
    "text": "Degranulation\n\nDegranulation is a cellular process that releases antimicrobial cytotoxic or other molecules from secretory vesicles called granules found inside some cells. It is used by several different cells involved in the immune system, including granulocytes (neutrophils, basophils, and eosinophils) and mast cells. It is also used by certain lymphocytes such as natural killer (NK) cells and cytotoxic T cells, whose main purpose is to destroy invading microorganisms.\n\nAntigens interact with IgE molecules already bound to high affinity Fc receptors on the surface of mast cells to induce degranulation, via the activation of tyrosine kinases within the cell. The mast cell releases a mixture of compounds, including histamine, proteoglycans, serotonin, and serine proteases from its cytoplasmic granules.\n\nIn a similar mechanism, activated eosinophils release preformed mediators such as major basic protein, and enzymes such as peroxidase, following interaction between their Fc receptors and IgE molecules that are bound to large parasites like helminths.\n\nFour kinds of granules exist in neutrophils that display differences in content and regulation. Secretory vesicles are the most likely to release their contents by degranulation, followed by gelatinase granules, specific granules, and azurophil granules.\nCytotoxic T cells and NK cells release molecules like perforin and granzymes by\na process of directed exocytosis to kill infected target cells.\n\n",
    "id": "10523316",
    "title": "Degranulation"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=10469455",
    "text": "Immunoglobulin class switching\n\nImmunoglobulin class switching, also known as isotype switching, isotypic commutation or class-switch recombination (CSR), is a biological mechanism that changes a B cell's production of immunoglobulin (antibodies) from one type to another, such as from the isotype IgM to the isotype IgG. During this process, the constant-region portion of the antibody heavy chain is changed, but the variable region of the heavy chain stays the same (the terms \"variable\" and \"constant\" refer to changes or lack thereof between antibodies that target different epitopes). Since the variable region does not change, class switching does not affect antigen specificity. Instead, the antibody retains affinity for the same antigens, but can interact with different effector molecules.\n\nClass switching occurs after activation of a mature B cell via its membrane-bound antibody molecule (or B cell receptor) to generate the different classes of antibody, all with the same variable domains as the original antibody generated in the immature B cell during the process of V(D)J recombination, but possessing distinct constant domains in their heavy chains.\n\nNaïve mature B cells produce both IgM and IgD, which are the first two heavy chain segments in the immunoglobulin locus. After activation by antigen, these B cells proliferate. If these activated B cells encounter specific signaling molecules via their CD40 and cytokine receptors (both modulated by T helper cells), they undergo antibody class switching to produce IgG, IgA or IgE antibodies. During class switching, the constant region of the immunoglobulin heavy chain changes but the variable regions, and therefore antigenic specificity, stay the same. This allows different daughter cells from the same activated B cell to produce antibodies of different isotypes or subtypes (e.g. IgG1, IgG2 etc.).\n\nThe order of the heavy chain exons are as follows:\n\n\nClass switching occurs by a mechanism called class switch recombination (CSR) binding. Class switch recombination is a biological mechanism that allows the class of antibody produced by an activated B cell to change during a process known as isotype or class switching. During CSR, portions of the antibody heavy chain locus are removed from the chromosome, and the gene segments surrounding the deleted portion are rejoined to retain a functional antibody gene that produces antibody of a different isotype. Double-stranded breaks are generated in DNA at conserved nucleotide motifs, called switch (S) regions, which are upstream from gene segments that encode the constant regions of antibody heavy chains; these occur adjacent to all heavy chain constant region genes with the exception of the δ-chain. DNA is nicked and broken at two selected S-regions by the activity of a series of enzymes, including Activation-Induced (Cytidine) Deaminase (AID), uracil DNA glycosylase and apyrimidic/apurinic (AP)-endonucleases. The intervening DNA between the S-regions is subsequently deleted from the chromosome, removing unwanted μ or δ heavy chain constant region exons and allowing substitution of a γ, α or ε constant region gene segment. The free ends of the DNA are rejoined by a process called non-homologous end joining (NHEJ) to link the variable domain exon to the desired downstream constant domain exon of the antibody heavy chain. In the absence of non-homologous end joining, free ends of DNA may be rejoined by an alternative pathway biased toward microhomology joins. With the exception of the μ and δ genes, only one antibody class is expressed by a B cell at any point in time.\nWhile class switch recombination is mostly a deletional process, rearranging a chromosome in \"cis\", it can also occur (in 10 to 20% of cases, depending upon the Ig class) as an inter-chromosomal translocation mixing immunoglobulin heavy chain genes from both alleles.\n\nT cell cytokines modulate class switching in mouse (Table 1) and human (Table 2). These cytokines may have suppressive effect on production of IgM.\n\nIn addition to the highly repetitive structure of the target S regions, the process of class switching needs S regions to be first transcribed and spliced out of the immunoglobulin heavy chain transcripts (where they lie within introns). Chromatin remodeling, accessibility to transcription and to AID and synapsis of broken S regions are under the control of a large super-enhancer, located downstream the more distal Calpha gene, the 3' regulatory region (3'RR). In some occasions, the 3'RR super-enhancer can itself be targeted by AID and undergo DNA breaks and junction with Sµ, which then deletes the Ig heavy chain locus and defines locus suicide recombination (LSR).\n\n",
    "id": "10469455",
    "title": "Immunoglobulin class switching"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=909363",
    "text": "Biotinylation\n\nIn biochemistry, biotinylation is the process of covalently attaching biotin to a protein, nucleic acid or other molecule. Biotinylation is rapid, specific and is unlikely to perturb the natural function of the molecule due to the small size of biotin (MW = 244.31 g/mol). Biotin binds to streptavidin and avidin with an extremely high affinity, fast on-rate, and high specificity, and these interactions are exploited in many areas of biotechnology to isolate biotinylated molecules of interest. Biotin-binding to streptavidin and avidin is resistant to extremes of heat, pH and proteolysis, making capture of biotinylated molecules possible in a wide variety of environments. Also, multiple biotin molecules can be conjugated to a protein of interest, which allows binding of multiple streptavidin, avidin or Neutravidin protein molecules and increases the sensitivity of detection of the protein of interest. There is a large number of biotinylation reagents available that exploit the wide range of possible labelling methods. Due to the strong affinity between biotin and streptavidin, the purification of biotinylated proteins has been a widely used approach to identify protein-protein interactions and post-translational events such as ubiquitylation in molecular biology.\n\nProteins can be biotinylated chemically or enzymatically. Chemical biotinylation utilises various conjugation chemistries to yield nonspecific biotinylation of amines, carboxylates, sulfhydryls and carbohydrates (e.g., NHS-coupling gives biotinylation of any primary amines in the protein). Enzymatic biotinylation results in biotinylation of a specific lysine within a certain sequence by a bacterial biotin ligase. Most chemical biotinylation reagents consist of a reactive group attached via a linker to the valeric acid side chain of biotin. As the biotin binding pocket in avidin / streptavidin is buried beneath the protein surface, biotinylation reagents possessing a longer linker are desirable, as they enable the biotin molecule to be more accessible to binding avidin/streptavidin/Neutravidin protein. This linker can also mediate the solubility of biotinylation reagents; linkers that incorporate poly(ethylene) glycol (PEG) can make water-insoluble reagents soluble or increase the solubility of biotinylation reagents that are already soluble to some extent.\n\nIn contrast to chemical biotinylation methods, enzymatic biotinylation allows biotin to be linked at exactly one residue present in the protein. This biotinylation reaction can also go to completion, meaning that the product is generated with high uniformity and can be linked to streptavidin in a defined orientation e.g. for MHC multimers. Enzymatic biotinylation is most often carried out by fusing the protein of interest at its N-terminus, C-terminus or at an internal loop to a 15 amino acid peptide, termed AviTag or Acceptor Peptide (AP). This tag serves as a recognition site for an \"E. coli\" biotin holoenzyme synthetase, also known as biotin ligase (BirA). Once tagged, the protein is then incubated with BirA allowing biotinylation to take place in the presence of biotin and ATP. Enzymatic biotinylation can be carried out \"in vitro\" but BirA also reacts specifically with its target peptide inside mammalian and bacterial cells and at the cell surface, while other cellular proteins are not modified. Enzymatic biotinylation can also take place \"in vivo\" typically through the co-expression of an Avitag tagged protein and BirA. Another way to biotinylate proteins is by fusing the target protein to the biotin carboxyl carrier protein (BCCP). A protein fused by BCCP can be recognized by biotin molecules \"in vivo\" and attach to it.\n\nThe most common targets for modifying protein molecules are primary amine groups that are present as lysine side chain epsilon-amines and N-terminal α-amines. Amine-reactive biotinylation reagents can be divided into two groups based on water solubility.\n\nN-hydroxysuccinimide (NHS) esters have poor solubility in aqueous solutions. For reactions in aqueous solution, they must first be dissolved in an organic solvent, then diluted into the aqueous reaction mixture. The most commonly used organic solvents for this purpose are dimethyl sulfoxide (DMSO) and dimethyl formamide (DMF), which are compatible with most proteins at low concentrations. Because of the hydrophobicity of NHS-esters, NHS biotinylation reagents can also diffuse through the cell membrane, meaning that they will biotinylate both internal and external components of a cell.\n\nSulfo-NHS esters are more soluble in water and should be dissolved in water just before use because they hydrolyze easily. The water solubility of sulfo-NHS-esters stems from their sulfonate group on the N-hydroxysuccinimide ring and eliminates the need to dissolve the reagent in an organic solvent. Sulfo-NHS-esters of biotin also can be used as cell surface biotinylation reagents, because they do not penetrate the cell membrane.\n\nThe chemical reactions of NHS- and sulfo-NHS esters are essentially identical, in that they both react spontaneously with amines to form an amide bond. Because the target for the ester is a deprotonated primary amine, the reaction is favored under basic conditions (above pH 7). Hydrolysis of the NHS ester is a major competing reaction, and the rate of hydrolysis increases with increasing pH. NHS- and sulfo-NHS-esters have a half-life of several hours at pH 7 but only a few minutes at pH 9.\n\nThere is some flexibility in the conditions for conjugating NHS-esters to primary amines. Incubation temperatures can range from 4-37 °C, pH values in the reaction range from 7-9, and incubation times range from a few minutes to 12 hours. Buffers containing amines (such as Tris or glycine) must be avoided, because they compete with the reaction.\n\nAn alternative to primary amine biotinylation is to label sulfhydryl groups with biotin. Because free sulfhydryl groups are less prevalent on most proteins compared to primary amines, sulfhydryl biotinylation is useful when primary amines are located in the regulatory domain(s) of the target protein or when a reduced level of biotinylation is required. Sulfhydryl-reactive groups such as maleimides, haloacetyls and pyridyl disulfides, require free sulfhydryl groups for conjugation; disulfide bonds must first be reduced to free up the sulfhydryl groups for biotinylation. If no free sulfhydryl groups are available, lysines can be modified with various thiolation reagents (Traut's reagent, SAT(PEG4), SATA and SATP), resulting in the addition of a free sulfhydryl. Sulfhydryl biotinylation is performed at a slightly lower pH (6.5-7.5) than labeling with NHS esters. \n\nBesides whole proteins, biotinylated peptides can be synthesized by introducing a cysteine (Cys) residue during synthesis at the terminus of the amino acid chain to get a site specific and oriented biotinylation. Nucleotides can also be biotinylated by incorporation of biotinylated nucleotides.\n\nCarboxyl groups are found on the C-terminal ends of proteins and on glutamate and aspartate amino acid side chains. Biotinylation reagents that target carboxyl groups do not have a carboxyl-reactive moiety per se but instead rely on a carbodiimide crosslinker such as EDC to bind the primary amine on the biotinylation reagents to the carboxyl group on the target protein.\n\nBiotinylation at carboxyl groups occur at pH 4.5–5.5. To prevent crossreactivity of the crosslinker with buffer constituents, buffers should not contain primary amines (e.g., Tris, glycine) or carboxyls (e.g., acetate, citrate); MES buffer is an ideal choice.\n\nGlycoproteins can be biotinylated by modifying the carbohydrate residues to aldehydes, which then react with hydrazine- or alkoxyamine-based biotinylation reagents. Sodium periodate oxidizes the sialic acids on glycoproteins to aldehydes to form these stable linkages at pH 4–6.\n\nPolyclonal antibodies are heavily glycosylated, and because glycosylation does not interfere with the antibody activity, biotinylating the glycosyl groups is an ideal strategy to generate biotinylated antibodies.\n\nOligonucleotides are readily biotinylated in the course of oligonucleotide synthesis by the phosphoramidite method using commercial biotin phosphoramidite. Upon the standard deprotection, the conjugates obtained can be purified using reverse-phase or anion-exchange HPLC\n\nPhotoactivatable biotinylation reagents are ideal when primary amines, sulfhydryls, carboxyls and carbohydrates are not available for labeling. These reagents rely on aryl azides, which become activated by ultraviolet light (UV; >350 nm), which then react at C-H and N-H bonds. Because these types of bonds occur independent of the type of amino acid, this type of biotinylation is termed \"non-specific\".\n\nPhotoactivatable biotinylation reagents can also be used to activate biotinylation at specific times in an experiment or during certain reaction conditions, by simply exposing the reaction to UV light at the specific time or condition.\n\nThe biotin tag can be used in affinity chromatography together with a column that has avidin (or streptavidin or Neutravidin) bound to it, which is the natural ligand for biotin. However, harsh conditions (e.g., 6M GuHCl at pH 1.5) are needed to break the avidin/streptavidin - biotin interaction, which will most likely denature the protein carrying the biotin tag. If isolation of the tagged protein is needed, it is better to tag the protein with iminobiotin. This biotin analogue gives strong binding to avidin/streptavidin at alkaline pH, but the affinity is reduced upon lowering the pH. Therefore, an iminobiotin-tagged functional protein can be released from an avidin/streptavidin column by decreasing the pH (to around pH 4).\n\nThis tag can also be used in detection of the protein via anti-biotin antibodies or avidin/streptavidin-tagged detection strategies such as enzyme reporters (e.g., horseradish peroxidase, alkaline phosphatase) or fluorescent probes. This can be useful in localization by fluorescent or electron microscopy, ELISA assays, ELISPOT assays, western blots and other immunoanalytical methods. Detection with monovalent streptavidin can avoid clustering or aggregation of the biotinylated target.\n\nThe non-covalent bond formed between biotin and avidin or streptavidin has a binding affinity that is higher than most antigen and antibody bonds and approaches the strength of a covalent bond. This very tight binding makes labeling proteins with biotin a useful tool for applications such as affinity chromatography using immobilized avidin or streptavidin to separate the biotinylated protein from a mixture of other proteins and biochemicals. Biotinylated protein such as biotinylated bovine serum albumin (BSA) is used in solid-phase assays as a coating on the well surface in multiwell assay plates. Biotinylation of red blood cells has been used as a means of determining total blood volume without the use of radiolabels such as chromium 51, allowing volume determinations in low birth weight infants and pregnant women who could not otherwise be exposed to the required doses of radioactivity. Furthermore, biotinylation of MHC molecules to create MHC multimers has become a useful tool for identifying and isolating antigen-specific T-cell populations. More recently, \"in vivo\" protein biotinylation was developed to study protein-protein interactions and proximity in living cells\n\nReaction conditions for biotinylation are chosen so that the target molecule (e.g., an antibody) is labeled with sufficient biotin molecules to purify or detect the molecule, but not so much that the biotin interferes with the function of the molecule.\n\nThe HABA (2-(4-hydroxyazobenzene) benzoic acid) assay can be used to determine the extent of biotinylation. HABA dye is bound to avidin or streptavidin and yields a characteristic absorbance. When biotinylated proteins or other molecules are introduced, the biotin displaces the dye, resulting in a change in absorbance at 500 nm. This change is directly proportional to the level of biotin in the sample. The disadvantage of the HABA assay is that it uses large amounts of sample.\n\nExtent of biotinylation can also be measured by streptavidin gel-shift, since streptavidin remains bound to biotin during agarose gel electrophoresis or polyacrylamide gel electrophoresis. The proportion of target biotinylated can be measured via the change in band intensity of the target with or without excess streptavidin, seen quickly and quantitatively for biotinylated proteins by Coomassie Brilliant Blue staining.\n\n",
    "id": "909363",
    "title": "Biotinylation"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1872854",
    "text": "Biochemical cascade\n\nA biochemical cascade, also known as a signaling cascade or signaling pathway, is a series of chemical reactions which are initiated by a stimulus (first messenger) acting on a receptor that is transduced to the cell interior through second messengers (which amplify the initial signal) and ultimately to effector molecules, resulting in a cell response to the initial stimulus. At each step of the signaling cascade, various controlling factors are involved to regulate cellular actions, responding effectively to cues about their changing internal and external environments.\n\nCells require a full and functional cellular machinery to live. When they belong to complex multicellular organisms, they need to communicate among themselves and work for symbiosis in order to give life to the organism. These communications between cells triggers intracellular signaling cascades, termed signal transduction pathways, that regulate specific cellular functions. Each signal transduction occurs with a primary extracellular messenger that binds to a transmembrane or nuclear receptor, initiating intracellular signals. The complex formed produces or releases second messengers that integrate and adapt the signal, amplifying it, by activating molecular targets, which in turn trigger effectors that will lead to the desired cellular response.\n\nSignal transduction is realized by activation of specific receptors and consequent production/delivery of second messengers, as Ca or cAMP. These molecules operate as signal transducers, triggering intracellular cascades and in turn amplifying the initial signal.\nTwo main signal transduction mechanisms have been identified, via nuclear receptors, or via transmembrane receptors. In the first one, first messenger cross through the cell membrane, binding and activating intracellular receptors localized at nucleus or cytosol, which then act as transcriptional factors regulating directly gene expression. This is possible due to the lipophilic nature of those ligands, mainly hormones. In the signal transduction via transmembrane receptors, first messenger bind to the extracellular domain of transmembrane receptor activating it. This receptors may have intrinsic catalytic activity or may be coupled to effector enzymes, or may also be associated to ionic channels. Therefore, there are four main transmembrane receptor types: G protein coupled receptors (GPCRs), tyrosine kinase receptors (RTKs), serine/threonine kinase receptors (RSTKs), and ligand-gated ion channels (LGICs).\nSecond messengers can be classified into three classes: \n\nThe cellular response in signal transduction cascades involves alteration of the expression of effector genes or activation/inhibition of targeted proteins. Regulation of protein activity mainly involves phosphorylation/dephosphorylation events, leading to its activation or inhibition. It is the case for the vast majority of responses as a consequence of the binding of the primary messengers to membrane receptors. This response is quick, as it involves regulation of molecules that are already present in the cell. On the other hand, the induction or repression of the expression of genes requires the binding of transcriptional factors to the regulatory sequences of these genes. The transcriptional factors are activated by the primary messengers, in most cases, due to their function as nuclear receptors for these messengers. The secondary messengers like DAG or Ca could also induce or repress gene expression, via transcriptional factors. This response is slower than the first because it involves more steps, like transcription of genes and then the effect of newly formed proteins in a specific target. The target could be a protein or another gene.\n\nIn biochemistry, several important enzymatic cascades and signal transduction cascades participate in metabolic pathways or signaling networks, in which enzymes are usually involved to catalyze the reactions. For example, the tissue factor pathway in the coagulation cascade of secondary hemostasis is the primary pathway leading to fibrin formation, and thus, the initiation of blood coagulation. The pathways are a series of reactions, in which a zymogen (inactive enzyme precursor) of a serine protease and its glycoprotein co-factors are activated to become active components that then catalyze the next reaction in the cascade, ultimately resulting in cross-linked fibrin.\n\nAnother example, sonic hedgehog signaling pathway, is one of the key regulators of embryonic development and is present in all bilaterians. Different parts of the embryo have different concentrations of hedgehog signaling proteins, which give cells information to make the embryo develop properly and correctly into a head or a tail. When the pathway malfunctions, it can result in diseases like basal cell carcinoma. Recent studies point to the role of hedgehog signaling in regulating adult stem cells involved in maintenance and regeneration of adult tissues. The pathway has also been implicated in the development of some cancers. Drugs that specifically target hedgehog signaling to fight diseases are being actively developed by a number of pharmaceutical companies. Most biochemical cascades are series of events, in which one event triggers the next, in a linear fashion. Negative cascades, however, include events that are in a circular fashion, or can cause or be caused by multiple events.\n\nBiochemical cascades include: \nNegative cascades include:\n\nAdhesion is an essential process to epithelial cells so that epithelium can be formed and cells can be in permanent contact with extracellular matrix and other cells. Several pathways exist to accomplish this communication and adhesion with environment. But the main signalling pathways are the cadherin and integrin pathways.\nThe cadherin pathway is present in adhesion junctions or in desmosomes and it is responsible for epithelial adhesion and communication with adjacent cells. Cadherin is a transmembrane glycoprotein receptor that establishes contact with another cadherin present in the surface of a neighbour cell forming an adhesion complex. This adhesion complex is formed by β-catenin and α-catenin, and p120 is essential for its stabilization and regulation. This complex then binds to actin, leading to polymerization. For actin polymerization through the cadherin pathway, proteins of the Rho GTPases family are also involved. This complex is regulated by phosphorylation, which leads to downregulation of adhesion. Several factors can induce the phosphorylation, like EGF, HGF or v-Src. The cadherin pathway also has an important function in survival and proliferation because it regulates the concentration of cytoplamatic β-catenin. When β-catenin is free in the cytoplasm, normally it is degraded, however if the Wnt signalling is activated, β-catenin degradation is inhibited and it is translocated to the nucleus where it forms a complex with transcription factors. This leads to activation of genes responsible for cell proliferation and survival. So the cadherin-catenin complex is essential for cell fate regulation. \nIntegrins are heterodimeric glycoprotein receptors that recognize proteins present in the extracellular matrix, like fibronectin and laminin. In order to function, integrins have to form complexes with ILK and Fak proteins. For adhesion to the extracellular matrix, ILK activate the Rac and Cdc42 proteins and leading to actin polymerization. ERK also leads to actin polymerization trough activation of cPLA2. Recruitment of FAK by integrin leads to Akt activation and this inhibits pro-apoptotic factors like BAD and Bax. When adhesion through integrins do not occur the pro-apoptotic factors are not inhibited and resulting in apoptosis.\n\nThe hepatocyte is a complex and multifunctional differentiated cell whose cell response will be influenced by the zone in hepatic lobule, because concentrations of oxygen and toxic substances present in the hepatic sinusoids change from periportal zone to centrilobular zone10. The hepatocytes of the intermediate zone have the appropriate morphological and functional features since they have the environment with average concentrations of oxygen and other substances. \nThis specialized cell is capable of:\nThe hepatocyte also regulates other functions for constitutive synthesis of proteins (albumin, ALT and AST) that influences the synthesis or activation of other molecules (synthesis of urea and essential amino acids), activate vitamin D, utilization of vitamin K, transporter expression of vitamin A and conversion of thyroxine.\n\nPurinergic signalling has an essential role at interactions between neurons and glia cells, allowing these to detect action potentials and modulate neuronal activity, contributing for intra and extracellular homeostasis regulation. Besides purinergic neurotransmitter, ATP acts as a trophic factor at cellular development and growth, being involved on microglia activation and migration, and also on axonal myelination by oligodendrocytes. There are two main types of purinergic receptors, P1 binding to adenosine, and P2 binding to ATP or ADP, presenting different signalling cascades.\nThe Nrf2/ARE signalling pathway has a fundamental role at fighting against oxidative stress, to which neurons are especially vulnerable due to its high oxygen consumption and high lipid content. This neuroprotective pathway involves control of neuronal activity by perisynaptic astrocytes and neuronal glutamate release, with the establishment of tripartite synapses. The Nrf2/ARE activation leads to a higher expression of enzymes involved in glutathione syntheses and metabolism, that have a key role in antioxidant response.\nThe LKB1/NUAK1 signalling pathway regulates terminal axon branching at cortical neurons, via local immobilized mitochondria capture. Besides NUAK1, LKB1 kinase acts under other effectors enzymes as SAD-A/B and MARK, therefore regulating neuronal polarization and axonal growth, respectively. These kinase cascades implicates also Tau and others MAP.\nAn extended knowledge of these and others neuronal pathways could provide new potential therapeutic targets for several neurodegenerative chronic diseases as Alzheimer's, Parkinson's and Huntington's disease, and also amyotrophic lateral sclerosis.\n\nThe blood cells (erythrocytes, leukocytes and platelets) are produced by hematopoiesis.\nThe erythrocytes have as main function the O delivery to the tissues, and this transfer occurs by diffusion and is determined by the O tension (PO). The erythrocyte is able to feel the tissue need for O and cause a change in vascular caliber, through the pathway of ATP release, which requires an increase in cAMP, and are regulated by the phosphodiesterase (PDE). This pathway can be triggered via two mechanisms: physiological stimulus (like reduced O2 tension) and activation of the prostacyclin receptor (IPR). This pathway includes heterotrimeric G proteins, adenylyl cyclase (AC), protein kinase A (PKA), cystic fibrosis transmembrane conductance regulator (CFTR), and a final conduit that transport ATP to vascular lumen (pannexin 1 or voltage-dependent anion channel (VDAC)). The released ATP acts on purinergic receptors on endothelial cells, triggering the synthesis and release of several vasodilators, like nitric oxide (NO) and prostacyclin (PGI). \nThe current model of leukocyte adhesion cascade includes many steps mentioned in Table 1. The integrin-mediated adhesion of leukocytes to endothelial cells is related with morphological changes in both leukocytes and endothelial cells, which together support leukocyte migration through the venular walls. Rho and Ras small GTPases are involved in the principal leukocyte signaling pathways underlying chemokine-stimulated integrin-dependent adhesion, and have important roles in regulating cell shape, adhesion and motility.\nAfter a vascular injury occurs, platelets are activated by locally exposed collagen (glycoprotein (GP) VI receptor), locally generated thrombin (PAR1 and PAR4 receptors), platelet-derived thromboxane A2 (TxA2) (TP receptor) and ADP (P2Y1 and P2Y12 receptors) that is either released from damaged cells or secreted from platelet dense granules. The von Willebrand factor (VWF) serves as an essential accessory molecule. In general terms, platelet activation initiated by agonist takes to a signaling cascade that leads to an increase of the cytosolic calcium concentration. Consequently, the integrin αβ is activated and the binding to fibrinogen allows the aggregation of platelets to each other. The increase of cytosolic calcium also leads to shape change and TxA2 synthesis, leading to signal amplification.\n\nThe main goal of biochemical cascades in lymphocytes is the secretion of molecules that can suppress altered cells or eliminate pathogenic agents, through proliferation, differentiation and activation of these cells. Therefore, the antigenic receptors play a central role in signal transduction in lymphocytes, because when antigens interact with them lead to a cascade of signal events. These receptors, that recognize the antigen soluble (B cells) or linked to a molecule on Antigen Presenting Cells (T cells), don't have long cytoplasm tails, so they are anchored to signal proteins, which contain a long cytoplasmic tails with a motif that can be phosphorylated (ITAM – immunoreceptor tyrosine-based activation motif) and resulting in different signal pathways. The antigen receptor and signal protein form a stable complex, named BCR or TCR, in B or T cells, respectively. The family Src is essential for signal transduction in these cells, because it's responsible for phosphorylation of ITAMs. Therefore, Lyn and Lck, in lymphocytes B and T, respectively, phosphorylate ITAMs after the antigen recognition and the conformational change of the receptor, which leads to the binding of Syk/Zap-70 kinases to ITAM and its activation. Syk kinase is specific of lymphocytes B and Zap-70 is present in T cells. After activation of these enzymes, some adaptor proteins are phosphorylated, like BLNK (B cells) and LAT (T cells). These proteins after phosphorylation become activated and allow binding of others enzymes that continue the biochemical cascade. One example of a protein that binds to adaptor proteins and become activated is PLC that is very important in the lymphocyte signal pathways. PLC is responsible for PKC activation, via DAG and Ca, which leads to phosphorylation of CARMA1 molecule, and formation of CBM complex. This complex activates Iκκ kinase, which phosphorylates I-κB, and then allows the translocation of NF-κB to the nucleus and transcription of genes encoding cytokines, for example. Others transcriptional factors like NFAT and AP1 complex are also important for transcription of cytokines. The differentiation of B cells to plasma cells is also an example of a signal mechanism in lymphocytes, induced by a cytokine receptor. In this case, some interleukins bind to a specific receptor, which leads to activation of MAPK/ERK pathway. Consequently, the BLIMP1 protein is translated and inhibits PAX5, allowing immunoglobulin genes transcription and activation of XBP1 (important for the secretory apparatus formation and enhancing of protein synthesis). Also, the coreceptors (CD28/CD19) play an important role because they can improve the antigen/receptor binding and initiate parallel cascade events, like activation o PI3 Kinase. PIP3 then is responsible for activation of several proteins, like vav (leads to activation of JNK pathway, which consequently leads to activation of c-Jun) and btk (can also activate PLC).\n\nThe Wnt signaling pathway can be divided in canonical and non-canonical. The canonical signaling involves binding of Wnt to Frizzled and LRP5 co-receptor, leading to GSK3 fosforilation and inhibition of β-catenin degradation, resulting in its accumulation and translocation to the nucleus, where it acts as a transcription factor. The non-canonical Wnt signaling can be divided in planar cell polarity (PCP) pathway and Wnt/calcium pathway. It is characterized by binding of Wnt to Frizzled and activation of G proteins and to an increase of intracellular levels of calcium through mechanisms involving PKC 50. The Wnt signaling pathway plays a significative role in osteoblastogenesis and bone formation, inducing the differentiation of mesenquimal pluripotent cells in osteoblasts and inhibiting the RANKL/RANK pathway and osteoclastogenesis.\n\nRANKL is a member of the TNF superfamily of ligands. Through binding to the RANK receptor it activates various molecules, like NF-kappa B, MAPK, NFAT and PI3K52. The RANKL/RANK signaling pathway regulates osteoclastogenesis, as well as, the survival and activation of osteoclasts.\n\nAdenosine is very relevant in bone metabolism, as it plays a role in formation and activation of both osteoclasts and osteoblasts. Adenosine acts by binding to purinergic receptors and influencing adenilyl cyclase activity and the formation of cAMP and PKA 54. Adenosine may have opposite effects on bone metabolism, because while certain purinergic receptors stimulate adenilyl cyclase activity, others have the opposite effect. Under certain circumstances adenosine stimulates bone destruction and in other situations it promotes bone formation, depending on the purinergic receptor that is being activated.\n\nSelf-renewal and differentiation abilities are exceptional properties of stem cells. These cells can be classified by their differentiation capacity, which progressively decrease with development, in totipotents, pluripotents, multipotents and unipotents.\n\nSelf-renewal process is highly regulated from cell cycle and genetic transcription control. There are some signaling pathways, such as LIF/JAK/STAT3 (Leukemia inhibitory factor/Janus kinase/Signal transducer and activator of transcription 3) and BMP/SMADs/Id (Bone morphogenetic proteins/ Mothers against decapentaplegic/ Inhibitor of differentiation), mediated by transcription factors, epigenetic regulators and others components, and they are responsible for self-renewal genes expression and inhibition of differentiation genes expression, respectively.\n\nAt cell cycle level there is an increase of complexity of the mechanisms in somatic stem cells. However, it is observed a decrease of self-renewal potential with age. These mechanisms are regulated by p16-CDK4/6-Rb and p19-p53-P21 signaling pathways. Embryonic stem cells have constitutive cyclin E-CDK2 activity, which hyperphosphorylates and inactivates Rb. This leads to a short G1 phase of the cell cycle with rapid G1-S transition and little dependence on mitogenic signals or D cyclins for S phase entry. In fetal stem cells, mitogens promote a relatively rapid G1-S transition through cooperative action of cyclin D-CDK4/6 and cyclin E-CDK2 to inactivate Rb family proteins. p16 and p19 expression are inhibited by Hmga2-dependent chromatin regulation. Many young adult stem cells are quiescent most of the time. In the absence of mitogenic signals, cyclin-CDKs and the G1-S transition are suppressed by cell cycle inhibitors including Ink4 and Cip/Kip family proteins. As a result, Rb is hypophosphorylated and inhibits E2F, promoting quiescence in G0-phase of the cell cycle. Mitogen stimulation mobilizes these cells into cycle by activating cyclin D expression. In old adult stem cells, let-7 microRNA expression increases, reducing Hmga2 levels and increasing p16 and p19 levels. This reduces the sensitivity of stem cells to mitogenic signals by inhibiting cyclin-CDK complexes. As a result, either stem cells cannot enter the cell cycle, or cell division slows in many tissues.\n\nExtrinsic regulation is made by signals from the niche, where stem cells are found, which is able to promote quiescent state and cell cycle activation in somatic stem cells. Asymmetric division is characteristic of somatic stem cells, maintaining the reservoir of stem cells in the tissue and production of specialized cells of the same.\n\nStem cells show an elevated therapeutic potential, mainly in hemato-oncologic pathologies, such as leukemia and lymphomas. Little groups of stem cells were found into tumours, calling cancer stem cells. There are evidences that these cells promote tumor growth and metastasis.\n\nThe oocyte is the female cell involved in reproduction. There is a close relationship between the oocyte and the surrounding follicular cells which is crucial to the development of both. GDF9 and BMP15 produced by the oocyte bind to BMPR2 receptors on follicular cells activating SMADs 2/3, ensuring follicular development. Concomitantly, oocyte growth is initiated by binding of KITL to its receptor KIT in the oocyte, leading to the activation of PI3K/Akt pathway, allowing oocyte survival and development. During embryogenesis, oocytes initiate meiosis and stop in prophase I. This arrest is maintained by elevated levels of cAMP within the oocyte. It was recently suggested that cGMP cooperates with cAMP to maintain the cell cycle arrest. During meitotic maturation, the LH peak that precedes ovulation activates MAPK pathway leading to gap junction disruption and breakdown of communication between the oocyte and the follicular cells. PDE3A is activated and degrades cAMP, leading to cell cycle progression and oocyte maturation. The LH surge also leads to the production of progesterone and prostaglandins that induce the expression of ADAMTS1 and other proteases, as well as their inhibitors. This will lead to degradation of the follicular wall, but limiting the damage and ensuring that the rupture occurs in the appropriate location, releasing the oocyte into the Fallopian tubes. Oocyte activation depends on fertilization by sperm. It is initiated with sperm's attraction induced by prostaglandins produced by the oocyte, which will create a gradient that will influence the sperm's direction and velocity. After fusion with the oocyte, PLC ζ of the spermatozoa is released into the oocyte leading to an increase in Ca2+ levels that will activate CaMKII which will degrade MPF, leading to the resumption of meiosis. The increased Ca levels will induce the exocytosis of cortical granules that degrade ZP receptors, used by sperm to penetrate the oocyte, blocking polyspermy. Deregulation of these pathways will lead to several diseases like, oocyte maturation failure syndrome which results in infertility. Increasing our molecular knowledge of oocyte development mechanisms could improve the outcome of assisted reproduction procedures, facilitating conception.\n\nSpermatozoon is the male gamete. After ejaculation this cell is not mature, so it can't fertilize the oocyte. To have the ability to fertilize the female gamete, this cell suffers capacitation and acrosome reaction in female reproductive tract. The signaling pathways best described for spermatozoon involve these processes. The cAMP/PKA signaling pathway leads to sperm cells capacitation; however, adenilyl cyclase in sperm cells is different from the somatic cells. Adenilyl cyclase in spermatozoon doesn't recognize G proteins, so it is stimulated by bicarbonate and Ca ions. Then, it converts ATP into cAMP, which activates PKA. PKA leads to protein tyrosine phosphorylation.\nPhospholipase C (PLC) is involved in acrosome reaction. ZP3 is a glycoprotein present in zona pelucida and it interacts with receptors in spermatozoon. So, ZP3 can activate G protein coupled receptors and tyrosine kinase receptors, that leads to production of PLC. PLC cleaves the phospholipid phosphatidylinositol 4,5-bisphosphate (PIP2) into diacyl glycerol (DAG) and inositol 1,4,5-trisphosphate (IP3). IP3 is released as a soluble structure into the cytosol and DAG remains bound to the membrane. IP3 binds to IP3 receptors, present in acrosome membrane. In addition, calcium and DAG together work to activate protein kinase C, which goes on to phosphorylate other molecules, leading to altered cellular activity. These actions cause an increase in cytosolic concentration of Ca that leads to dispersion of actin and consequently promotes plasmatic membrane and outer acrosome membrane fusion.\nProgesterone is a steroid hormone produced in cumulus oophorus. In somatic cells it binds to receptors in nucleus; however, in spermatozoon its receptors are present in plasmatic membrane. This hormone activates AKT that leads to activation of other protein kinases, involved in capacitation and acrosome reaction.\nWhen ROS (reactive oxygen species) are present in high concentration, they can affect the physiology of cells, but when they are present in moderated concentration they are important for acrosome reaction and capacitation. ROS can interact with cAMP/PKA and progesterone pathway, stimulating them. ROS also interacts with ERK pathway that leads to activation of Ras, MEK and MEK-like proteins. These proteins activate protein tyrosine kinase (PTK) that phosphorylates various proteins important for capacitation and acrosome reaction.\n\nVarious signalling pathways, as FGF, WNT and TGF-β pathways, regulate the processes involved in embryogenesis.\n\nFGF (Fibroblast Growth Factor) ligands bind to receptors tyrosine kinase, FGFR (Fibroblast Growth Factor Receptors), and form a stable complex with co-receptors HSPG (Heparan Sulphate Proteoglycans) that will promote autophosphorylation of the intracellular domain of FGFR and consequent activation of four main pathways: MAPK/ERK, PI3K, PLCγ and JAK/STAT.\nThe WNT pathway allows β-catenin function in gene transcription, once the interaction between WNT ligand and G protein-coupled receptor Frizzled inhibits GSK-3 (Glycogen Synthase Kinase-3) and thus formation of β-catenin destruction complex. Although there is some controversy about the effects of this pathway in embryogenesis, it is thought that WNT signalling induces primitive streak, mesoderm and endoderm formation.\nIn TGF-β (Transforming Growth Factor β) pathway, BMP (Bone Morphogenic Protein), Activin and Nodal ligands bind to their receptors and activate Smads that bind to DNA and promote gene transcription. Activin is necessary for mesoderm and specially endoderm differentiation, and Nodal and BMP are involved in embryo patterning. BMP is also responsible for formation of extra-embryonic tissues before and during gastrulation, and for early mesoderm differentiation, when Activin and FGF pathways are activated.\n\nPathway building has been performed by individual groups studying a network of interest (e.g., immune signaling pathway) as well as by large bioinformatics consortia (e.g., the Reactome Project) and commercial entities (e.g., Ingenuity Systems). Pathway building is the process of identifying and integrating the entities, interactions, and associated annotations, and populating the knowledge base. Pathway construction can have either a data-driven objective (DDO) or a knowledge-driven objective (KDO). Data-driven pathway construction is used to generate relationship information of genes or proteins identified in a specific experiment such as a microarray study. Knowledge-driven pathway construction entails development of a detailed pathway knowledge　base for particular domains of interest, such as a cell type, disease, or system. The curation process of a biological pathway entails identifying and structuring content, mining information manually and/or computationally, and assembling a knowledgebase using appropriate software tools. A schematic illustrating the major steps involved in the data-driven and knowledge-driven construction processes.\n\nFor either DDO or KDO pathway construction, the first step is to mine pertinent information from relevant information sources about the entities and interactions. The information retrieved is assembled using appropriate formats, information standards, and pathway building tools to obtain a pathway prototype. The pathway is further refined to include context-specific annotations such as species, cell/tissue type, or disease type. The pathway can then be verified by the domain experts and updated by the curators based on appropriate feedback. Recent attempts to improve knowledge integration have led to refined classifications of cellular entities, such as GO, and to the assembly of structured knowledge repositories. Data repositories, which contain information regarding sequence data, metabolism, signaling, reactions, and interactions are a major source of information for pathway building. A few useful databases are described in the following table.\nLegend: Y – Yes, N – No; BIND – Biomolecular Interaction Network Database, DIP – Database of Interacting Proteins, GNPV – Genome Network Platform Viewer, HPRD = Human Protein Reference Database, MINT – Molecular Interaction database, MIPS – Munich Information center for Protein Sequences, UNIHI – Unified Human Interactome, OPHID – Online Predicted Human Interaction Database, EcoCyc – Encyclopaedia of E. Coli Genes and Metabolism, MetaCyc – aMetabolic Pathway database, KEGG – Kyoto Encyclopedia of Genes and Genomes, PANTHER – Protein Analysis Through Evolutionary Relationship database, STKE – Signal Transduction Knowledge Environment, PID – The Pathway Interaction Database, BioPP – Biological Pathway Publisher. A comprehensive list of resources can be found at http://www.pathguide.org.\n\nThe increasing amount of genomic and molecular information is the basis for understanding higher-order biological systems, such as the cell and the organism, and their interactions with the environment, as well as for medical, industrial and other practical applications. The KEGG resource (http://www.genome.jp/kegg/) provides a reference knowledge base for linking genomes to biological systems, categorized as building blocks in the genomic space (KEGG GENES), the chemical space (KEGG LIGAND), wiring diagrams of interaction networks and reaction networks (KEGG PATHWAY), and ontologies for pathway reconstruction (BRITE database).\nThe KEGG PATHWAY database is a collection of manually drawn pathway maps for metabolism, genetic information processing, environmental information processing such as signal transduction, ligand–receptor interaction and cell communication, various other cellular processes and human diseases, all based on extensive survey of published literature.\n\nGene Map Annotator and Pathway Profiler (GenMAPP) (http://www.genmapp.org/) a free, open-source, stand-alone computer program is designed for organizing, analyzing, and sharing genome scale data in the context of biological pathways. GenMAPP database support multiple gene annotations and species as well as custom species database creation for a potentially unlimited number of species. Pathway resources are expanded by utilizing homology information to translate pathway content between species and extending existing pathways with data derived from conserved protein interactions and coexpression. A new mode of data visualization including time-course, single nucleotide polymorphism (SNP), and splicing, has been implemented with GenMAPP database to support analysis of complex data. GenMAPP also offers innovative ways to display and share data by incorporating HTML export of analyses for entire sets of pathways as organized web pages (http://www.genmapp.org/tutorials/Converting-MAPPs-between-species.pdf). In short, GenMAPP provides a means to rapidly interrogate complex experimental data for pathway-level changes in a diverse range of organisms.\n\nGiven the genetic makeup of an organism, the complete set of possible reactions constitutes its reactome. Reactome, located at http://www.reactome.org is a curated, peer-reviewed resource of human biological processes/pathway data. The basic unit of the Reactome database is a reaction; reactions are then grouped into causal chains to form pathways The Reactome data model allows us to represent many diverse processes in the human system, including the pathways of intermediary metabolism, regulatory pathways, and signal transduction, and high-level processes, such as the cell cycle. Reactome provides a qualitative framework, on which quantitative data can be superimposed. Tools have been developed to facilitate custom data entry and annotation by expert biologists, and to allow visualization and exploration of the finished dataset as an interactive process map. Although the primary curational domain is pathways from Homo sapiens, electronic projections of human pathways onto other organisms are regularly created via putative orthologs, thus making Reactome relevant to model organism research communities. The database is publicly available under open source terms, which allows both its content and its software infrastructure to be freely used and redistributed. Studying whole transcriptional profiles and cataloging protein–protein interactions has yielded much valuable biological information, from the genome or proteome to the physiology of an organism, an organ, a tissue or even a single cell. The Reactome database containing a framework of possible reactions which, when combined with expression and enzyme kinetic data, provides the infrastructure for quantitative models, therefore, an integrated view of biological processes, which links such gene products and can be systematically mined by using bioinformatics applications. Reactome data available in a variety of standard formats, including BioPAX, SBML and PSI-MI, and also enable data exchange with other pathway databases, such as the Cycs, KEGG and amaze, and molecular interaction databases, such as BIND and HPRD. The next data release will cover apoptosis, including the death receptor signaling pathways, and the Bcl2 pathways, as well as pathways involved in hemostasis. Other topics currently under development include several signaling pathways, mitosis, visual phototransduction and hematopoeisis. In summary, Reactome provides high-quality curated summaries of fundamental biological processes in humans in a form of biologist-friendly visualization of pathways data, and is an open-source project.\n\nIn the post-genomic age, high-throughput sequencing and gene/protein profiling techniques have transformed biological research by enabling comprehensive monitoring of a biological system, yielding a list of differentially expressed genes or proteins, which is useful in identifying genes that may have roles in a given phenomenon or phenotype. With DNA microarrays and genome-wide gene engineering, it is possible to screen global gene expression profiles to contribute a wealth of genomic data to the public domain. With RNA interference, it is possible to distill the inferences contained in the experimental literature and primary databases into knowledge bases that consist of annotated representations of biological pathways. In this case, individual genes and proteins are known to be involved in biological processes, components, or structures, as well as how and where gene products interact with each other. Pathway-oriented approaches for analyzing microarray data, by grouping long lists of individual genes, proteins, and/or other biological molecules according to the pathways they are involved in into smaller sets of related genes or proteins, which reduces the complexity, have proven useful for connecting genomic data to specific biological processes and systems. Identifying active pathways that differ between two conditions can have more explanatory power than a simple list of different genes or proteins. In addition, a large number of pathway analytic methods exploit pathway knowledge in public repositories such as Gene Ontology (GO) or Kyoto Encyclopedia of Genes and Genomes (KEGG), rather than inferring pathways from molecular measurements. Furthermore, different research focuses have given the word \"pathway\" different meanings. For example, 'pathway' can denote a metabolic pathway involving a sequence of enzyme-catalyzed reactions of small molecules, or a signaling pathway involving a set of protein phosphorylation reactions and gene regulation events. Therefore, the term \"pathway analysis\" has a very broad application. For instance, it can refer to the analysis physical interaction networks (e.g., protein–protein interactions), kinetic simulation of pathways, and steady-state pathway analysis (e.g., flux-balance analysis), as well as its usage in the inference of pathways from expression and sequence data. Several functional enrichment analysis tools and algorithms have been developed to enhance data interpretation. The existing knowledge base–driven pathway analysis methods in each generation have been summarized in recent literature.\n\nA program package MatchMiner was used to scan HUGO names for cloned genes of interest are scanned, then are input into GoMiner, which leveraged the GO to identify the biological processes, functions and components represented in the gene profile. Also, Database for Annotation, Visualization, and Integrated Discovery (DAVID) and KEGG database can be used for the analysis of microarray expression data and the analysis of each GO biological process (P), cellular component (C), and molecular function (F) ontology. In addition, DAVID tools can be used to analyze the roles of genes in metabolic pathways and show the biological relationships between genes or gene-products and may represent metabolic pathways. These two databases also provide bioinformatics tools online to combine specific biochemical information on a certain organism and facilitate the interpretation of biological meanings for experimental data. By using a combined approach of Microarray-Bioinformatic technologies, a potential metabolic mechanism contributing to colorectal cancer (CRC) has been demonstrated Several environmental factors may be involved in a series of points along the genetic pathway to CRC. These include genes associated with bile acid metabolism, glycolysis metabolism and fatty acid metabolism pathways, supporting a hypothesis that some metabolic alternations observed in colon carcinoma may occur in the development of CRC.\n\nCellular models are instrumental in dissecting a complex pathological process into simpler molecular events. Parkinson's disease (PD) is multifactorial and clinically heterogeneous; the aetiology of the sporadic (and most common) form is still unclear and only a few molecular mechanisms have been clarified so far in the neurodegenerative cascade. In such a multifaceted picture, it is particularly important to identify experimental models that simplify the study of the different networks of proteins and genes involved. Cellular models that reproduce some of the features of the neurons that degenerate in PD have contributed to many advances in our comprehension of the pathogenic flow of the disease. In particular, the pivotal biochemical pathways (i.e. apoptosis and oxidative stress, mitochondrial impairment and dysfunctional mitophagy, unfolded protein stress and improper removal of misfolded proteins) have been widely explored in cell lines, challenged with toxic insults or genetically modified. The central role of a-synuclein has generated many models aiming to elucidate its contribution to the dysregulation of various cellular processes. Classical cellular models appear to be the correct choice for preliminary studies on the molecular action of new drugs or potential toxins and for understanding the role of single genetic factors. Moreover, the availability of novel cellular systems, such as cybrids or induced pluripotent stem cells, offers the chance to exploit the advantages of an in vitro investigation, although mirroring more closely the cell population being affected.\n\nSynaptic degeneration and death of nerve cells are defining features of Alzheimer's disease (AD), the most prevalent age-related neurodegenerative disorders. In AD, neurons in the hippocampus and basal forebrain (brain regions that subserve learning and memory functions) are selectively vulnerable. Studies of postmortem brain tissue from AD people have provided evidence for increased levels of oxidative stress, mitochondrial dysfunction and impaired glucose uptake in vulnerable neuronal populations. Studies of animal and cell culture models of AD suggest that increased levels of oxidative stress (membrane lipid peroxidation, in particular) may disrupt neuronal energy metabolism and ion homeostasis, by impairing the function of membrane ion-motive ATPases, glucose and glutamate transporters. Such oxidative and metabolic compromise may thereby render neurons vulnerable to excitotoxicity and apoptosis. Recent studies suggest that AD can manifest systemic alterations in energy metabolism (e.g., increased insulin resistance and dysregulation of glucose metabolism). Emerging evidence that dietary restriction can forestall the development of AD is consistent with a major \"metabolic\" component to these disorders, and provides optimism that these devastating brain disorders of aging may be largely preventable.\n\n",
    "id": "1872854",
    "title": "Biochemical cascade"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1740374",
    "text": "Thrifty phenotype\n\nThe thrifty phenotype hypothesis says that reduced fetal growth is strongly associated with a number of chronic conditions later in life. This increased susceptibility results from adaptations made by the fetus in an environment limited in its supply of nutrients. The thrifty phenotype is a component of the Fetal Origins Hypothesis. These chronic conditions include coronary heart disease, stroke, diabetes, and hypertension.\n\nProponents of this idea say that in poor nutritional conditions, a pregnant woman can modify the development of her unborn child such that it will be prepared for survival in an environment in which resources are likely to be short, resulting in a thrifty phenotype (Hales & Barker, 1992). It is sometimes called Barker's hypothesis, after Professor David J. P. Barker, researching at the University of Southampton who published the theory in 1990.\n\nThe thrifty phenotype hypothesis suggests that early-life metabolic adaptations help in survival of the organism by selecting an appropriate trajectory of growth in response to environmental cues. Recently, some scientists have proposed that the thrifty phenotype prepares the organism for its likely adult environment in long term.\n\nHowever, environmental changes during early development may result in the selected trajectory becoming inappropriate, resulting in adverse effects on health. This paradox generates doubts about whether the thrifty phenotype is adaptive for human offspring. Thus, the thrifty phenotype should be considered as the capacity of all offspring to respond to environmental cues during early ontogenetic development. It has been suggested that the thrifty phenotype is the consequence of three unlike adaptive processes: maternal effects, niche construction and developmental plasticity, which all are influenced by the brain. While developmental plasticity demonstrates an adaptation by the offspring, niche construction and parental effects are result of parental selections rather than offspring fitness. Therefore, the thrifty phenotype can be described as a manipulation of offspring phenotype for the benefit of maternal fitness. The information that enters offspring phenotype during early development mirror the mother’s own developmental experience and the quality of the environment during her own maturation rather than predicting the possible future environment of the offspring\n\nMany human diseases in adulthood are related to growth patterns during early life, determining early-life nutrition as the underlying mechanism. Individuals with a thrifty phenotype will have \"a smaller body size, a lowered metabolic rate and a reduced level of behavioural activity… adaptations to an environment that is chronically short of food\" (Bateson & Martin, 1999). Those with a thrifty phenotype who actually develop in an affluent environment may be more prone to metabolic disorders, such as obesity and type II diabetes, whereas those who have received a positive maternal forecast will be adapted to good conditions and therefore better able to cope with rich diets. This idea (Barker, 1992) is now widely (if not universally) accepted and is a source of concern for societies undergoing a transition from sparse to better nutrition (Robinson, 2001).\n\nRisk factors of thrifty phenotype include advanced maternal age and placental insufficiency.\n\nThe ability to conserve, acquire and expend energy is believed to be an innate, ancient trait that is imbedded in the genome in a way that is quite protected against mutations. These changes are also believed to possibly be inherited across generations. Leptin has been identified as a possible gene for the acquisition of these thrifty traits.\n\nOn a larger anatomic scale, the molecular mechanisms are broadly caused by a suboptimal environment in the reproductive tract or maternal physiological adaptations to pregnancy.\n\n\n",
    "id": "1740374",
    "title": "Thrifty phenotype"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=19916613",
    "text": "Radical (chemistry)\n\nIn chemistry, a radical (more precisely, a free radical) is an atom, molecule, or ion that has an unpaired valence electron.\nWith some exceptions, these unpaired electrons make free radicals highly chemically reactive towards other substances, or even towards themselves: their molecules will often spontaneously dimerize or polymerize if they come in contact with each other. Most radicals are reasonably stable only at very low concentrations in inert media or in a vacuum.\n\nA notable example of a free radical is the hydroxyl radical (HO•), a molecule that has one unpaired electron on the oxygen atom. Two other examples are triplet oxygen and triplet carbene (:) which have two unpaired electrons. In contrast, the hydroxyl anion () is not a radical, since the unpaired electron is resolved by the addition of an electron; singlet oxygen and singlet carbene are not radicals as the two electrons are paired.\n\nFree radicals may be created in a number of ways, including synthesis with very dilute or rarefied reagents, reactions at very low temperatures, or breakup of larger molecules. The latter can be affected by any process that puts enough energy into the parent molecule, such as ionizing radiation, heat, electrical discharges, electrolysis, and chemical reactions. Radicals are intermediate stages in many chemical reactions.\n\nFree radicals play an important role in combustion, atmospheric chemistry, polymerization, plasma chemistry, biochemistry, and many other chemical processes. In living organisms, the free radicals superoxide and nitric oxide and their reaction products regulate many processes, such as control of vascular tone and thus blood pressure. They also play a key role in the intermediary metabolism of various biological compounds. Such radicals can even be messengers in a process dubbed redox signaling. A radical may be trapped within a \"solvent cage\" or be otherwise bound.\n\nUntil late in the 20th century the word \"radical\" was used in chemistry to indicate any connected group of atoms, such as a methyl group or a carboxyl, whether it was part of a larger molecule or a molecule on its own. The qualifier \"free\" was then needed to specify the unbound case. Following recent nomenclature revisions, a part of a larger molecule is now called a functional group or substituent, and \"radical\" now implies \"free\". However, the old nomenclature may still appear in some books.\n\nThe term radical was already in use when the now obsolete radical theory was developed. Louis-Bernard Guyton de Morveau introduced the phrase \"radical\" in 1785 and the phrase was employed by Antoine Lavoisier in 1789 in his Traité Élémentaire de Chimie. A radical was then identified as the root base of certain acids (the Latin word \"radix\" meaning \"root\"). Historically, the term \"radical\" in radical theory was also used for bound parts of the molecule, especially when they remain unchanged in reactions. These are now called functional groups. For example, methyl alcohol was described as consisting of a methyl \"radical\" and a hydroxyl \"radical\". Neither are radicals in the modern chemical sense, as they are permanently bound to each other, and have no unpaired, reactive electrons; however, they can be observed as radicals in mass spectrometry when broken apart by irradiation with energetic electrons.\n\nIn a modern context the first organic (carbon–containing) free radical identified was triphenylmethyl radical, (CH)C•. This species was discovered by Moses Gomberg in 1900 at the University of Michigan USA. In 1933 Morris Kharash and Frank Mayo proposed that free radicals were responsible for anti-Markovnikov addition of hydrogen bromide to allyl bromide.\n\nIn chemical equations, free radicals are frequently denoted by a dot placed immediately to the right of the atomic symbol or molecular formula as follows:\n\nRadical reaction mechanisms use single-headed arrows to depict the movement of single electrons:\n\nThe homolytic cleavage of the breaking bond is drawn with a 'fish-hook' arrow to distinguish from the usual movement of two electrons depicted by a standard curly arrow. The second electron of the breaking bond also moves to pair up with the attacking radical electron; this is not explicitly indicated in this case.\n\nFree radicals also take part in radical addition and radical substitution as reactive intermediates. Chain reactions involving free radicals can usually be divided into three distinct processes. These are \"initiation\", \"propagation\", and \"termination\".\n\nThe formation of radicals may involve the breaking of covalent bonds by homolysis, a process that requires significant amounts of energy. Such energies are known as homolytic bond dissociation energies, usually abbreviated as \"Δ\"H\"°\". Splitting H into 2H•, for example, requires a Δ\"H\"° of +435 kJ·mol, while splitting Cl into 2Cl• requires a Δ\"H\"° of +243 kJ·mol.\n\nThe energy needed to break a specific bond (generally covalent) between two atoms known as bond energy is a result of all the relative attractions and repulsions between the atoms of the molecule, however the most relevant are the bond's atoms and the immediate neighbors. As an approximation the most important parameters that influence the bonding between two atoms in a molecule are the mutual energy match and overlap of covalent orbitals and the repulsion between nonbonding orbitals. Likewise, radicals requiring more energy to form are less stable than those requiring less energy. An additional barrier can be the selection rule. Propagation, however, is very exothermic.\n\nRadical formation through homolytic bond cleavage most often happens between two atoms of similar electronegativity; in organic chemistry, this is often between the O–O bond in peroxide species or between O–N bonds. Radicals may also be formed by single-electron oxidation or reduction of an atom or molecule: an example is the production of superoxide by the electron transport chain. Early studies in organometallic chemistry – especially F. A. Paneth and K. Hahnfeld's studies of tetra-alkyl lead species during the 1930s – supported the heterolytic fission of bonds and a radical-based mechanism. Although radical ions do exist, most species are electrically neutral.\n\nAlthough radicals are generally short-lived due to their reactivity, there are long-lived radicals. These are categorized as follows:\n\nThe prime example of a stable radical is molecular dioxygen (O). Another common example is nitric oxide (NO). Organic radicals can be long lived if they occur in a conjugated π system, such as the radical derived from α-tocopherol (vitamin E). There are also hundreds of examples of thiazyl radicals, which show low reactivity and remarkable thermodynamic stability with only a very limited extent of π resonance stabilization.\n\nPersistent radical compounds are those whose longevity is due to steric crowding around the radical center, which makes it physically difficult for the radical to react with another molecule. Examples of these include Gomberg's triphenylmethyl radical, Fremy's salt (Potassium nitrosodisulfonate, (KSO)NO·), aminoxyls, (general formula RNO·) such as TEMPO, TEMPOL, nitronyl nitroxides, and azephenylenyls and radicals derived from PTM (perchlorophenylmethyl radical) and TTM (tris(2,4,6-trichlorophenyl)methyl radical). Persistent radicals are generated in great quantity during combustion, and \"may be responsible for the oxidative stress resulting in cardiopulmonary disease and probably cancer that has been attributed to exposure to airborne fine particles.\"\n\nDiradicals are molecules containing two radical centers. Multiple radical centers can exist in a molecule. Atmospheric oxygen naturally exists as a diradical in its ground state as triplet oxygen. The low reactivity of atmospheric oxygen is due to its diradical state. Non-radical states of dioxygen are actually less stable than the diradical. The relative stability of the oxygen diradical is primarily due to the spin-forbidden nature of the triplet-singlet transition required for it to grab electrons, i.e., \"oxidize\". The diradical state of oxygen also results in its paramagnetic character, which is demonstrated by its attraction to an external magnet. Diradicals can also occur in metal-oxo complexes, lending themselves for studies of spin forbidden reactions in transition metal chemistry.\n\nRadical alkyl intermediates are stabilized by similar physical processes to carbocations: as a general rule, the more substituted the radical center is, the more stable it is. This directs their reactions. Thus, formation of a tertiary radical (RC·) is favored over secondary (RHC·), which is favored over primary (RHC·). Likewise, radicals next to functional groups such as carbonyl, nitrile, and ether are more stable than tertiary alkyl radicals.\n\nRadicals attack double bonds. However, unlike similar ions, such radical reactions are not as much directed by electrostatic interactions. For example, the reactivity of nucleophilic ions with α,β-unsaturated compounds (C=C–C=O) is directed by the electron-withdrawing effect of the oxygen, resulting in a partial positive charge on the carbonyl carbon. There are two reactions that are observed in the ionic case: the carbonyl is attacked in a direct addition to carbonyl, or the vinyl is attacked in conjugate addition, and in either case, the charge on the nucleophile is taken by the oxygen. Radicals add rapidly to the double bond, and the resulting α-radical carbonyl is relatively stable; it can couple with another molecule or be oxidized. Nonetheless, the electrophilic/neutrophilic character of radicals has been shown in a variety of instances. One example is the alternating tendency of the copolymerization of maleic anhydride (electrophilic) and styrene (slightly nucleophilic).\n\nIn intramolecular reactions, precise control can be achieved despite the extreme reactivity of radicals. In general, radicals attack the closest reactive site the most readily. Therefore, when there is a choice, a preference for five-membered rings is observed: four-membered rings are too strained, and collisions with carbons six or more atoms away in the chain are infrequent.\n\nTriplet carbenes and nitrenes, which are diradicals, have distinctive chemistry.\n\nA familiar free-radical reaction is combustion. The oxygen molecule is a stable diradical, best represented by ·O-O·. Because spins of the electrons are parallel, this molecule is stable. While the ground state of oxygen is this unreactive spin-unpaired (triplet) diradical, an extremely reactive spin-paired (singlet) state is available. For combustion to occur, the energy barrier between these must be overcome. This barrier can be overcome by heat, requiring high temperatures. The triplet-singlet transition is also \"forbidden\". This presents an additional barrier to the reaction. It also means molecular oxygen is relatively unreactive at room temperature except in the presence of a catalytic heavy atom such as iron or copper.\n\nCombustion consists of various radical chain reactions that the singlet radical can initiate. The flammability of a given material strongly depends on the concentration of free radicals that must be obtained before initiation and propagation reactions dominate leading to combustion of the material. Once the combustible material has been consumed, termination reactions again dominate and the flame dies out. As indicated, promotion of propagation or termination reactions alters flammability. For example, because lead itself deactivates free radicals in the gasoline-air mixture, tetraethyl lead was once commonly added to gasoline. This prevents the combustion from initiating in an uncontrolled manner or in unburnt residues (engine knocking) or premature ignition (preignition).\n\nWhen a hydrocarbon is burned, a large number of different oxygen radicals are involved. Initially, hydroperoxyl radical (HOO·) are formed. These then react further to give organic hydroperoxides that break up into hydroxyl radicals (HO·).\n\nIn addition to combustion, many polymerization reactions involve free radicals. As a result, many plastics, enamels, and other polymers are formed through radical polymerization. For instance, drying oils and alkyd paints harden due to radical crosslinking by oxygen from the atmosphere.\n\nRecent advances in radical polymerization methods, known as living radical polymerization, include:\nThese methods produce polymers with a much narrower distribution of molecular weights.\n\nThe most common radical in the lower atmosphere is molecular dioxygen. Photodissociation of source molecules produces other free radicals. In the lower atmosphere, the most important examples of free radical production are the photodissociation of nitrogen dioxide to give an oxygen atom and nitric oxide (see below), which plays a key role in smog formation—and the photodissociation of ozone to give the excited oxygen atom O(1D) (see below). The net and return reactions are also shown ( and , respectively).\n\nIn the upper atmosphere, a particularly important source of radicals is the photodissociation of normally unreactive chlorofluorocarbons (CFCs) by solar ultraviolet radiation, or by reactions with other stratospheric constituents (see eq. 1 below). These reactions give off the chlorine radical, Cl•, which reacts with ozone in a catalytic chain reaction ending in Ozone depletion and regeneration of the chlorine radical, allowing it to reparticipate in the reaction (see – below). Such reactions are believed to be the primary cause of depletion of the ozone layer (the net result is shown in below), and this is why the use of chlorofluorocarbons as refrigerants has been restricted.\n\nFree radicals play an important role in a number of biological processes. Many of these are necessary for life, such as the intracellular killing of bacteria by phagocytic cells such as granulocytes and macrophages. Researchers have also implicated free radicals in certain cell signalling processes, known as redox signaling. Some of these signaling molecules involve the free radical-induce peroxidation of tissue stores of polyunsaturated fatty acids such as linoleic acid, arachidonic acid, and docosahexaenoic acid. For example, free radical attack of linoleic acid produces a series of 13-Hydroxyoctadecadienoic acids and 9-Hydroxyoctadecadienoic acids which may act to regulate localized tissue inflammatory and/or healing responses, pain perception, and the proliferation of malignant cells. Free radical attacks on arachidonic acid and docosahexaenoic acid produce a similar but broader array of signaling products.\n\nBy a DPPH method on free radical inhibitor abilities of the methanolic extracts of plant sample of Cannabis sativa, were found to be 52.98%, where those of ascorbic acid is 80.76% and quercetin 72.13%, respectively. Cannabis sativa showed that extracts have the proton donating ability and could serve as free radical inhibitor or scavengers, acting possibly as primary antioxidants.\n\nThe two most important oxygen-centered free radicals are superoxide and hydroxyl radical. They derive from molecular oxygen under reducing conditions. However, because of their reactivity, these same free radicals can participate in unwanted side reactions resulting in cell damage. Excessive amounts of these free radicals can lead to cell injury and death, which may contribute to many diseases such as cancer, stroke, myocardial infarction, diabetes and major disorders. Many forms of cancer are thought to be the result of reactions between free radicals and DNA, potentially resulting in mutations that can adversely affect the cell cycle and potentially lead to malignancy. Some of the symptoms of aging such as atherosclerosis are also attributed to free-radical induced oxidation of cholesterol to 7-ketocholesterol. In addition free radicals contribute to alcohol-induced liver damage, perhaps more than alcohol itself. Free radicals produced by cigarette smoke are implicated in inactivation of alpha 1-antitrypsin in the lung. This process promotes the development of emphysema.\n\nFree radicals may also be involved in Parkinson's disease, senile and drug-induced deafness, schizophrenia, and Alzheimer's. The classic free-radical syndrome, the iron-storage disease hemochromatosis, is typically associated with a constellation of free-radical-related symptoms including movement disorder, psychosis, skin pigmentary melanin abnormalities, deafness, arthritis, and diabetes mellitus. The free-radical theory of aging proposes that free radicals underlie the aging process itself. Similarly, the process of mitohormesis suggests that repeated exposure to free radicals may extend life span.\n\nBecause free radicals are necessary for life, the body has a number of mechanisms to minimize free-radical-induced damage and to repair damage that occurs, such as the enzymes superoxide dismutase, catalase, glutathione peroxidase and glutathione reductase. In addition, antioxidants play a key role in these defense mechanisms. These are often the three vitamins, vitamin A, vitamin C and vitamin E and polyphenol antioxidants. Furthermore, there is good evidence indicating that bilirubin and uric acid can act as antioxidants to help neutralize certain free radicals. Bilirubin comes from the breakdown of red blood cells' contents, while uric acid is a breakdown product of purines. Too much bilirubin, though, can lead to jaundice, which could eventually damage the central nervous system, while too much uric acid causes gout.\n\nReactive oxygen species or ROS are species such as superoxide, hydrogen peroxide, and hydroxyl radical, commonly associated with cell damage. ROS form as a natural by-product of the normal metabolism of oxygen and have important roles in cell signaling.\n\nOxybenzone has been found to form free radicals in sunlight, and therefore may be associated with cell damage as well. This only occurred when it was combined with other ingredients commonly found in sunscreens, like titanium oxide and octyl methoxycinnamate.\n\nROS attack the polyunsaturated fatty acid, linoleic acid, to form a series of 13-Hydroxyoctadecadienoic acid and 9-Hydroxyoctadecadienoic acid products that serve as signaling molecules that may trigger responses that counter the tissue injury which caused their formation. ROS attacks other polyunsaturated fatty acids, e.g. arachidonic acid and docosahexaenoic acid, to produce a similar series of signaling products.\n\nIn most fields of chemistry, the historical definition of radicals contends that the molecules have nonzero spin. However, in fields including spectroscopy, chemical reaction, and astrochemistry, the definition is slightly different. Gerhard Herzberg, who won the Nobel prize for his research into the electron structure and geometry of radicals, suggested a looser definition of free radicals: \"any transient (chemically unstable) species (atom, molecule, or ion)\". The main point of his suggestion is that there are many chemically unstable molecules that have zero spin, such as C, C, CH and so on. This definition is more convenient for discussions of transient chemical processes and astrochemistry; therefore researchers in these fields prefer to use this loose definition.\n\nFree radical diagnostic techniques include:\n\n\n",
    "id": "19916613",
    "title": "Radical (chemistry)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1783345",
    "text": "Bioerosion\n\nBioerosion describes the erosion of hard ocean substrates – and less often terrestrial substrates – by living organisms. Marine bioerosion can be caused by mollusks, polychaete worms, phoronids, sponges, crustaceans, echinoids, and fish; it can occur on coastlines, on coral reefs, and on ships; its mechanisms include biotic boring, drilling, rasping, and scraping. On dry land, bioerosion is typically performed by pioneer plants or plant-like organisms such as lichen, and mostly chemical (e.g. by acidic secretions on limestone) or mechanical (e.g. by roots growing into cracks) in nature.\n\nBioerosion of coral reefs generates the fine and white coral sand characteristic of tropical islands. The coral is converted to sand by internal bioeroders such as algae, fungi, bacteria (microborers) and sponges (Clionaidae), bivalves (including \"Lithophaga\"), sipunculans, polychaetes, acrothoracican barnacles and phoronids, generating extremely fine sediment with diameters of 10 to 100 micrometres. External bioeroders include sea urchins (such as \"Diadema\") and chitons. These forces in concert produce a great deal of erosion. Sea urchin erosion of calcium carbonate has been reported in some reefs at annual rates exceeding 20 kg/m². \nFish also erode coral while eating algae. Parrotfish cause a great deal of bioerosion using well developed jaw muscles, tooth armature, and a pharyngeal mill, to grind ingested material into sand-sized particles. Bioerosion of coral reef aragonite by parrotfish can range from 1017.7±186.3 kg/yr (0.41±0.07 m³/yr) for \"Chlorurus gibbus\" and 23.6±3.4 kg/yr (9.7 10-³±1.3 10-³ m²/yr) for \"Chlorurus sordidus\" (Bellwood, 1995).\n\nBioerosion is also well known in the fossil record on shells and hardgrounds (Bromley, 1970), with traces of this activity stretching back well into the Precambrian (Taylor & Wilson, 2003). Macrobioerosion, which produces borings visible to the naked eye, shows two distinct evolutionary radiations. One was in the Middle Ordovician (the Ordovician Bioerosion Revolution; see Wilson & Palmer, 2006) and the other in the Jurassic (see Taylor & Wilson, 2003; Bromley, 2004; Wilson, 2007). Microbioerosion also has a long fossil record and its own radiations (see Glaub & Vogel, 2004; Glaub et al., 2007).\n\n\n\n\n",
    "id": "1783345",
    "title": "Bioerosion"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=737798",
    "text": "Hydroxyl radical\n\nThe hydroxyl radical, OH, is the neutral form of the hydroxide ion (OH). Hydroxyl radicals are highly reactive (easily becoming hydroxyl groups) and consequently short-lived; however, they form an important part of radical chemistry. Most notably hydroxyl radicals are produced from the decomposition of hydroperoxides (ROOH) or, in atmospheric chemistry, by the reaction of excited atomic oxygen with water. It is also an important radical formed in radiation chemistry, since it leads to the formation of hydrogen peroxide and oxygen, which can enhance corrosion and SCC in coolant systems subjected to radioactive environments. Hydroxyl radicals are also produced during UV-light dissociation of HO (suggested in 1879) and likely in Fenton chemistry, where trace amounts of reduced transition metals catalyze peroxide-mediated oxidations of organic compounds.\n\nIn organic synthesis, hydroxyl radicals are most commonly generated by photolysis of 1-hydroxy-2(1\"H\")-pyridinethione.\n\nThe hydroxyl radical is often referred to as the \"detergent\" of the troposphere because it reacts with many pollutants, decomposing them through \"cracking\", often acting as the first step to their removal. It also has an important role in eliminating some greenhouse gases like methane and ozone. The rate of reaction with the hydroxyl radical often determines how long many pollutants last in the atmosphere, if they do not undergo photolysis or are rained out. For instance methane, which reacts relatively slowly with hydroxyl radical, has an average lifetime of >5 years and many CFCs have lifetimes of 50 years or more. Pollutants, such as larger hydrocarbons, can have very short average lifetimes of less than a few hours.\n\nThe first reaction with many volatile organic compounds (VOCs) is the removal of a hydrogen atom, forming water and an alkyl radical (R).\nThe alkyl radical will typically react rapidly with oxygen forming a peroxy radical.\n\nThe fate of this radical in the troposphere is dependent on factors such as the amount of sunlight, pollution in the atmosphere and the nature of the alkyl radical that formed it.\n\nHydroxyl radicals can occasionally be produced as a byproduct of immune action. Macrophages and microglia most frequently generate this compound when exposed to very specific pathogens, such as certain bacteria. The destructive action of hydroxyl radicals has been implicated in several neurological autoimmune diseases such as HAND when immune cells become over-activated and toxic to neighboring healthy cells.\n\nThe hydroxyl radical can damage virtually all types of macromolecules: carbohydrates, nucleic acids (mutations), lipids (lipid peroxidation), and amino acids (e.g. conversion of Phe to \"m\"-Tyrosine and \"o\"-Tyrosine). The hydroxyl radical has a very short \"in vivo\" half-life of approximately 10 seconds and a high reactivity. This makes it a very dangerous compound to the organism.\n\nUnlike superoxide, which can be detoxified by superoxide dismutase, the hydroxyl radical cannot be eliminated by an enzymatic reaction.\n\nThe hydroxyl OH radical is one of the main chemical species controlling the oxidizing capacity of the global Earth atmosphere. This oxidizing reactive species has a major impact on the concentrations and distribution of greenhouse gases and pollutants in the Earth atmosphere. It is the most widespread oxidizer in the troposphere, the lowest part of the atmosphere. Understanding OH variability is important to evaluating human impacts on the atmosphere and climate. The OH species has a lifetime in the Earth atmosphere of less than one second. Understanding the role of OH in the oxidation process of methane (CH) present in the atmosphere to first carbon monoxide (CO) and then carbon dioxide (CO) is important for assessing the residence time of this greenhouse gas, the overall carbon budget of the troposphere, and its influence on the process of global warming. The lifetime of OH radicals in the Earth atmosphere is very short, therefore OH concentrations in the air are very low and very sensitive techniques are required for its direct detection. Global average hydroxyl radical concentrations have been measured indirectly by analyzing methyl chloroform (CHCCl) present in the air. The results obtained by Montzka \"et al.\" (2011) shows that the interannual variability in OH estimated from CHCCl measurements is small, indicating that global OH is generally well buffered against perturbations. This small variability is consistent with measurements of methane and other trace gases primarily oxidized by OH, as well as global photochemical model calculations.\n\nIn 2014, researchers reported their discovery of a \"hole\" or absence of hydroxyl throughout the entire depth of the troposphere across a large region of the tropical West Pacific. They suggested that this hole is permitting large quantities of ozone-degrading chemicals to reach the stratosphere, and that this may be significantly reinforcing ozone depletion in the polar regions with potential consequences for the climate of the Earth.\n\nThe first experimental evidence for the presence of 18 cm absorption lines of the hydroxyl (OH) radical in the radio absorption spectrum of Cassiopeia A was obtained by Weinreb et al. (Nature, Vol. 200, pp. 829, 1963) based on observations made during the period October 15–29, 1963.\n\nOH is a diatomic molecule. The electronic angular momentum along the molecular axis is +1 or −1, and the electronic spin angular momentum \"S\" = . Because of the orbit-spin coupling, the spin angular momentum can be oriented in parallel or anti arallel directions to the orbital angular momentum, producing the splitting into Π and Π states. The Π ground state of OH is split by lambda doubling interaction (an interaction between the nuclei rotation and the unpaired electron motion around its orbit). Hyperfine interaction with the unpaired spin of the proton further splits the levels.\n\nIn order to study gas phase interstellar chemistry, it is convenient to distinguish two types of interstellar clouds: diffuse clouds, with and , and dense clouds, with and density . Ion chemical routes in both dense and diffuse clouds have been established for some works (Hartquist, \"Molecular Astrophysics\", 1990).\n\nThe OH radical is linked with the production of HO in molecular clouds. Studies of OH distribution in Taurus Molecular Cloud-1 (TMC-1) suggest that in dense gas, OH is mainly formed by dissociative recombination of HO. Dissociative recombination is the reaction in which a molecular ion recombines with an electron and dissociates into neutral fragments. Important formation mechanisms for OH are:\n\nExperimental data on association reactions of H and OH suggest that radiative association involving atomic and diatomic neutral radicals may be considered as an effective mechanism for the production of small neutral molecules in the interstellar clouds. The formation of O occurs in the gas phase via the neutral exchange reaction between O and OH, which is also the main sink for OH in dense regions.\n\nWe can see that atomic oxygen takes part both in the production and destruction of OH, so the abundance of OH depends mainly on the H abundance. Then, important chemical pathways leading from OH radicals are:\n\nRate constants can be derived from the dataset published in a website. Rate constants have the form:\n\nThe following table has the rate constants calculated for a typical temperature in a dense cloud .\nFormation rates \"r\" can be obtained using the rate constants \"k\"(\"T\") and the abundances of the reactants species C and D:\n\nwhere [Y] represents the abundance of the species Y. In this approach, abundances were taken from \"The UMIST database for astrochemistry 2006\", and the values are relatives to the H density. Following table shows the ratio \"r\"/\"r\" in order to get a view of the most important reactions.\n\nThe results suggest that (1a) reaction is the most prominent reaction in dense clouds. It is in concordance with Harju et al. 2000.\n\nThe next table shows the results by doing the same procedure for destruction reaction:\nResults shows that reaction 1A is the main sink for OH in dense clouds.\n\nDiscoveries of the microwave spectra of a considerable number of molecules prove the existence of rather complex molecules in the interstellar clouds, and provides the possibility to study dense clouds, which are obscured by the dust they contain. The OH molecule has been observed in the interstellar medium since 1963 through its 18-cm transitions. In the subsequent years OH was observed by its rotational transitions at far infrared wavelengths, mainly in the Orion region. Because each rotational level of OH is split in by lambda doubling, astronomers can observe a wide variety of energy states from the ground state.\n\nVery high densities are required to thermalize the rotational transitions of OH, so it is difficult to detect far-infrared emission lines from a quiescent molecular cloud. Even at H densities of 10 cm, dust must be optically thick at infrared wavelengths. But the passage of a shock wave through a molecular cloud is precisely the process which can bring the molecular gas out of equilibrium with the dust, making observations of far-infrared emission lines possible. A moderately fast shock may produce a transient raise in the OH abundance relative to hydrogen. So, it is possible that far-infrared emission lines of OH can be a good diagnostic of shock conditions.\n\nDiffuse clouds are of astronomical interest because they play a primary role in the evolution and thermodynamics of ISM. Observation of the abundant atomic hydrogen in 21 cm has shown good signal-to-noise ratio in both emission and absorption. Nevertheless, HI observations have a fundamental difficulty when they are directed at low mass regions of the hydrogen nucleus, as the center part of a diffuse cloud: the thermal width of the hydrogen lines are of the same order as the internal velocities of structures of interest, so cloud components of various temperatures and central velocities are indistinguishable in the spectrum. Molecular line observations in principle do not suffer from this problem. Unlike HI, molecules generally have excitation temperature \"T\" ≪ \"T\", so that emission is very weak even from abundant species. CO and OH are the most easily studied candidate molecules. CO has transitions in a region of the spectrum (wavelength < 3 mm) where there are not strong background continuum sources, but OH has the 18 cm emission, line convenient for absorption observations. Observation studies provide the most sensitive means of detections of molecules with subthermal excitation, and can give the opacity of the spectral line, which is a central issue to model the molecular region.\n\nStudies based in the kinematic comparison of OH and HI absorption lines from diffuse clouds are useful in determining their physical conditions, specially because heavier elements provide higher velocity resolution.\n\nOH masers, a type of astrophysical maser, were the first masers to be discovered in space and have been observed in more environments than any other type of maser.\n\nIn the Milky Way, OH masers are found in stellar masers (evolved stars), interstellar masers (regions of massive star formation), or in the interface between supernova remnants and molecular material. Interstellar OH masers are often observed from molecular material surrounding ultracompact H II regions (UC H II). But there are masers associated with very young stars that have yet to create UC H II regions. This class of OH masers appears to form near the edges of very dense material, place where HO masers form, and where total densities drop rapidly and UV radiation form young stars can dissociate the HO molecules. So, observations of OH masers in these regions, can be an important way to probe the distribution of the important HO molecule in interstellar shocks at high spatial resolutions.\n\nHydroxyl radicals play a key role in the oxidative destruction of organic pollutant using a series of methodologies collectively known as advanced oxidation processes (AOPs). The destruction of pollutants in AOPs is based on the non-selective reaction of hydroxyl radicals on organic compounds. It is highly effective against a series of pollutants including pesticides, pharmaceutical compounds, dyes, etc.\n\n\n",
    "id": "737798",
    "title": "Hydroxyl radical"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=13745014",
    "text": "Xenohormesis\n\nXenohormesis explains how certain molecules such as plant polyphenols, which indicate stress in the plants, can have a longevity-conferring effect in consumers of plant (i.e. mammals). It was first used in the paper \"Small molecules that regulate lifespan: evidence for xenohormesis\". by David Sinclair and colleagues from Harvard Medical School. Further studies then picked up the term.\n\nIf the plants an animal is eating are under stress, their increased polyphenol content may signal forthcoming famine conditions. It could be advantageous for the animal to begin to react—i.e. to hunker down to prepare for the lean times to come. The effects researchers have observed from resveratrol may be just such a response.\n\n",
    "id": "13745014",
    "title": "Xenohormesis"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=31756786",
    "text": "Sclerobiont\n\nSclerobionts are collectively known as organisms living in or on any kind of hard substrate (Taylor and Wilson, 2003). A few examples of sclerobionts include \"Entobia\" borings, \"Gastrochaenolites\" borings, \"Oichnus\" borings, \"Talpina\" borings, serpulids, encrusting oysters, encrusting foraminiferans, \"Stomatopora\" bryozoans, and \"“Berenicea”\" bryozoans. \n\nBioerosion\n",
    "id": "31756786",
    "title": "Sclerobiont"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=24544",
    "text": "Photosynthesis\n\nPhotosynthesis is a process used by plants and other organisms to convert light energy into chemical energy that can later be released to fuel the organisms' activities (energy transformation). This chemical energy is stored in carbohydrate molecules, such as sugars, which are synthesized from carbon dioxide and water – hence the name \"photosynthesis\", from the Greek φῶς, \"phōs\", \"light\", and σύνθεσις, \"synthesis\", \"putting together\". In most cases, oxygen is also released as a waste product. Most plants, most algae, and cyanobacteria perform photosynthesis; such organisms are called photoautotrophs. Photosynthesis is largely responsible for producing and maintaining the oxygen content of the Earth's atmosphere, and supplies all of the organic compounds and most of the energy necessary for life on Earth.\n\nAlthough photosynthesis is performed differently by different species, the process always begins when energy from light is absorbed by proteins called reaction centres that contain green chlorophyll pigments. In plants, these proteins are held inside organelles called chloroplasts, which are most abundant in leaf cells, while in bacteria they are embedded in the plasma membrane. In these light-dependent reactions, some energy is used to strip electrons from suitable substances, such as water, producing oxygen gas. The hydrogen freed by the splitting of water is used in the creation of two further compounds that serve as short-term stores of energy, enabling its transfer to drive other reactions: these compounds are reduced nicotinamide adenine dinucleotide phosphate (NADPH) and adenosine triphosphate (ATP), the \"energy currency\" of cells.\n\nIn plants, algae and cyanobacteria, long-term energy storage in the form of sugars is produced by a subsequent sequence of reactions called the Calvin cycle; some bacteria use different mechanisms, such as the reverse Krebs cycle, to achieve the same end. In the Calvin cycle, atmospheric carbon dioxide is incorporated into already existing organic carbon compounds, such as ribulose bisphosphate (RuBP). Using the ATP and NADPH produced by the light-dependent reactions, the resulting compounds are then reduced and removed to form further carbohydrates, such as glucose.\n\nThe first photosynthetic organisms probably evolved early in the evolutionary history of life and most likely used reducing agents such as hydrogen or hydrogen sulfide, rather than water, as sources of electrons. Cyanobacteria appeared later; the excess oxygen they produced contributed directly to the oxygenation of the Earth, which rendered the evolution of complex life possible. Today, the average rate of energy capture by photosynthesis globally is approximately 130 terawatts, which is about three times the current power consumption of human civilization.\nPhotosynthetic organisms also convert around 100–115 thousand million metric tonnes of carbon into biomass per year.\n\nPhotosynthetic organisms are photoautotrophs, which means that they are able to synthesize food directly from carbon dioxide and water using energy from light. However, not all organisms that use light as a source of energy carry out photosynthesis; photoheterotrophs use organic compounds, rather than carbon dioxide, as a source of carbon. In plants, algae, and cyanobacteria, photosynthesis releases oxygen. This is called \"oxygenic photosynthesis\" and is by far the most common type of photosynthesis used by living organisms. Although there are some differences between oxygenic photosynthesis in plants, algae, and cyanobacteria, the overall process is quite similar in these organisms. There are also many varieties of anoxygenic photosynthesis, used mostly by certain types of bacteria, which consume carbon dioxide but do not release oxygen.\n\nCarbon dioxide is converted into sugars in a process called carbon fixation; photosynthesis captures energy from sunlight to convert carbon dioxide into carbohydrate. Carbon fixation is an endothermic redox reaction. In general outline, photosynthesis is the opposite of cellular respiration; in the latter, glucose and other compounds are oxidized to produce carbon dioxide and water, and to release chemical energy (an exothermic reaction) to drive the organism's metabolism. The two processes, reduction of carbon dioxide to carbohydrate and then later oxidation of the carbohydrate, are distinct: photosynthesis and cellular respiration take place through a different sequence of chemical reactions and in different cellular compartments.\n\nThe general equation for photosynthesis as first proposed by Cornelius van Niel is therefore:\n\nSince water is used as the electron donor in oxygenic photosynthesis, the equation for this process is:\n\nThis equation emphasizes that water is both a reactant in the light-dependent reaction and a product of the light-independent reaction, but canceling \"n\" water molecules from each side gives the net equation:\n\nOther processes substitute other compounds (such as arsenite) for water in the electron-supply role; for example some microbes use sunlight to oxidize arsenite to arsenate: The equation for this reaction is:\n\nPhotosynthesis occurs in two stages. In the first stage, \"light-dependent reactions\" or \"light reactions\" capture the energy of light and use it to make the energy-storage molecules ATP and NADPH. During the second stage, the \"light-independent reactions\" use these products to capture and reduce carbon dioxide.\n\nMost organisms that utilize oxygenic photosynthesis use visible light for the light-dependent reactions, although at least three use shortwave infrared or, more specifically, far-red radiation.\n\nSome organisms employ even more radical variants of photosynthesis. Some archea use a simpler method that employs a pigment similar to those used for vision in animals. The bacteriorhodopsin changes its configuration in response to sunlight, acting as a proton pump. This produces a proton gradient more directly, which is then converted to chemical energy. The process does not involve carbon dioxide fixation and does not release oxygen, and seems to have evolved separately from the more common types of photosynthesis.\n\nIn photosynthetic bacteria, the proteins that gather light for photosynthesis are embedded in cell membranes. In its simplest form, this involves the membrane surrounding the cell itself. However, the membrane may be tightly folded into cylindrical sheets called thylakoids, or bunched up into round vesicles called \"intracytoplasmic membranes\". These structures can fill most of the interior of a cell, giving the membrane a very large surface area and therefore increasing the amount of light that the bacteria can absorb.\n\nIn plants and algae, photosynthesis takes place in organelles called chloroplasts. A typical plant cell contains about 10 to 100 chloroplasts. The chloroplast is enclosed by a membrane. This membrane is composed of a phospholipid inner membrane, a phospholipid outer membrane, and an intermembrane space. Enclosed by the membrane is an aqueous fluid called the stroma. Embedded within the stroma are stacks of thylakoids (grana), which are the site of photosynthesis. The thylakoids appear as flattened disks. The thylakoid itself is enclosed by the thylakoid membrane, and within the enclosed volume is a lumen or thylakoid space. Embedded in the thylakoid membrane are integral and peripheral membrane protein complexes of the photosynthetic system.\n\nPlants absorb light primarily using the pigment chlorophyll. The green part of the light spectrum is not absorbed but is reflected which is the reason that most plants have a green color. Besides chlorophyll, plants also use pigments such as carotenes and xanthophylls. Algae also use chlorophyll, but various other pigments are present, such as phycocyanin, carotenes, and xanthophylls in green algae, phycoerythrin in red algae (rhodophytes) and fucoxanthin in brown algae and diatoms resulting in a wide variety of colors.\n\nThese pigments are embedded in plants and algae in complexes called antenna proteins. In such proteins, the pigments are arranged to work together. Such a combination of proteins is also called a light-harvesting complex.\n\nAlthough all cells in the green parts of a plant have chloroplasts, the majority of those are found in specially adapted structures called leaves. Certain species adapted to conditions of strong sunlight and aridity, such as many Euphorbia and cactus species, have their main photosynthetic organs in their stems. The cells in the interior tissues of a leaf, called the mesophyll, can contain between 450,000 and 800,000 chloroplasts for every square millimeter of leaf. The surface of the leaf is coated with a water-resistant waxy cuticle that protects the leaf from excessive evaporation of water and decreases the absorption of ultraviolet or blue light to reduce heating. The transparent epidermis layer allows light to pass through to the palisade mesophyll cells where most of the photosynthesis takes place.\n\nIn the light-dependent reactions, one molecule of the pigment chlorophyll absorbs one photon and loses one electron. This electron is passed to a modified form of chlorophyll called pheophytin, which passes the electron to a quinone molecule, starting the flow of electrons down an electron transport chain that leads to the ultimate reduction of NADP to NADPH. In addition, this creates a proton gradient (energy gradient) across the chloroplast membrane, which is used by ATP synthase in the synthesis of ATP. The chlorophyll molecule ultimately regains the electron it lost when a water molecule is split in a process called photolysis, which releases a dioxygen (O) molecule as a waste product.\n\nThe overall equation for the light-dependent reactions under the conditions of non-cyclic electron flow in green plants is:\n\nNot all wavelengths of light can support photosynthesis. The photosynthetic action spectrum depends on the type of accessory pigments present. For example, in green plants, the action spectrum resembles the absorption spectrum for chlorophylls and carotenoids with absorption peaks in violet-blue and red light. In red algae, the action spectrum is blue-green light, which allows these algae to use the blue end of the spectrum to grow in the deeper waters that filter out the longer wavelengths (red light) used by above ground green plants. The non-absorbed part of the light spectrum is what gives photosynthetic organisms their color (e.g., green plants, red algae, purple bacteria) and is the least effective for photosynthesis in the respective organisms.\n\nIn plants, light-dependent reactions occur in the thylakoid membranes of the chloroplasts where they drive the synthesis of ATP and NADPH. The light-dependent reactions are of two forms: cyclic and non-cyclic.\n\nIn the non-cyclic reaction, the photons are captured in the light-harvesting antenna complexes of photosystem II by chlorophyll and other accessory pigments (see diagram at right). The absorption of a photon by the antenna complex frees an electron by a process called photoinduced charge separation. The antenna system is at the core of the chlorophyll molecule of the photosystem II reaction center. That freed electron is transferred to the primary electron-acceptor molecule, pheophytin. As the electrons are shuttled through an electron transport chain (the so-called Z-scheme shown in the diagram), it initially functions to generate a chemiosmotic potential by pumping proton cations (H) across the membrane and into the thylakoid space. An ATP synthase enzyme uses that chemiosmotic potential to make ATP during photophosphorylation, whereas NADPH is a product of the terminal redox reaction in the \"Z-scheme\". The electron enters a chlorophyll molecule in Photosystem I. There it is further excited by the light absorbed by that photosystem. The electron is then passed along a chain of electron acceptors to which it transfers some of its energy. The energy delivered to the electron acceptors is used to move hydrogen ions across the thylakoid membrane into the lumen. The electron is eventually used to reduce the co-enzyme NADP with a H to NADPH (which has functions in the light-independent reaction); at that point, the path of that electron ends.\n\nThe cyclic reaction is similar to that of the non-cyclic, but differs in that it generates only ATP, and no reduced NADP (NADPH) is created. The cyclic reaction takes place only at photosystem I. Once the electron is displaced from the photosystem, the electron is passed down the electron acceptor molecules and returns to photosystem I, from where it was emitted, hence the name \"cyclic reaction\".\n\nThe NADPH is the main reducing agent produced by chloroplasts, which then goes on to provide a source of energetic electrons in other cellular reactions. Its production leaves chlorophyll in photosystem I with a deficit of electrons (chlorophyll has been oxidized), which must be balanced by some other reducing agent that will supply the missing electron. The excited electrons lost from chlorophyll from photosystem I are supplied from the electron transport chain by plastocyanin. However, since photosystem II is the first step of the \"Z-scheme\", an external source of electrons is required to reduce its oxidized chlorophyll \"a\" molecules. The source of electrons in green-plant and cyanobacterial photosynthesis is water. Two water molecules are oxidized by four successive charge-separation reactions by photosystem II to yield a molecule of diatomic oxygen and four hydrogen ions; the electrons yielded are transferred to a redox-active tyrosine residue that then reduces the oxidized chlorophyll \"a\" (called P680) that serves as the primary light-driven electron donor in the photosystem II reaction center. That photo receptor is in effect reset and is then able to repeat the absorption of another photon and the release of another photo-dissociated electron. The oxidation of water is catalyzed in photosystem II by a redox-active structure that contains four manganese ions and a calcium ion; this oxygen-evolving complex binds two water molecules and contains the four oxidizing equivalents that are used to drive the water-oxidizing reaction (Dolai's S-state diagrams). Photosystem II is the only known biological enzyme that carries out this oxidation of water. The hydrogen ions released contribute to the transmembrane chemiosmotic potential that leads to ATP synthesis. Oxygen is a waste product of light-dependent reactions, but the majority of organisms on Earth use oxygen for cellular respiration, including photosynthetic organisms.\n\nIn the light-independent (or \"dark\") reactions, the enzyme RuBisCO captures CO from the atmosphere and, in a process called the Calvin-Benson cycle, it uses the newly formed NADPH and releases three-carbon sugars, which are later combined to form sucrose and starch. The overall equation for the light-independent reactions in green plants is\n\nCarbon fixation produces the intermediate three-carbon sugar product, which is then converted to the final carbohydrate products. The simple carbon sugars produced by photosynthesis are then used in the forming of other organic compounds, such as the building material cellulose, the precursors for lipid and amino acid biosynthesis, or as a fuel in cellular respiration. The latter occurs not only in plants but also in animals when the energy from plants is passed through a food chain.\n\nThe fixation or reduction of carbon dioxide is a process in which carbon dioxide combines with a five-carbon sugar, ribulose 1,5-bisphosphate, to yield two molecules of a three-carbon compound, glycerate 3-phosphate, also known as 3-phosphoglycerate. Glycerate 3-phosphate, in the presence of ATP and NADPH produced during the light-dependent stages, is reduced to glyceraldehyde 3-phosphate. This product is also referred to as 3-phosphoglyceraldehyde (PGAL) or, more generically, as triose phosphate. Most (5 out of 6 molecules) of the glyceraldehyde 3-phosphate produced is used to regenerate ribulose 1,5-bisphosphate so the process can continue. The triose phosphates not thus \"recycled\" often condense to form hexose phosphates, which ultimately yield sucrose, starch and cellulose. The sugars produced during carbon metabolism yield carbon skeletons that can be used for other metabolic reactions like the production of amino acids and lipids.\n\nIn hot and dry conditions, plants close their stomata to prevent water loss. Under these conditions, will decrease and oxygen gas, produced by the light reactions of photosynthesis, will increase, causing an increase of photorespiration by the oxygenase activity of ribulose-1,5-bisphosphate carboxylase/oxygenase and decrease in carbon fixation. Some plants have evolved mechanisms to increase the concentration in the leaves under these conditions.\n\nPlants that use the C carbon fixation process chemically fix carbon dioxide in the cells of the mesophyll by adding it to the three-carbon molecule phosphoenolpyruvate (PEP), a reaction catalyzed by an enzyme called PEP carboxylase, creating the four-carbon organic acid oxaloacetic acid. Oxaloacetic acid or malate synthesized by this process is then translocated to specialized bundle sheath cells where the enzyme RuBisCO and other Calvin cycle enzymes are located, and where released by decarboxylation of the four-carbon acids is then fixed by RuBisCO activity to the three-carbon 3-phosphoglyceric acids. The physical separation of RuBisCO from the oxygen-generating light reactions reduces photorespiration and increases fixation and, thus, the photosynthetic capacity of the leaf. plants can produce more sugar than plants in conditions of high light and temperature. Many important crop plants are plants, including maize, sorghum, sugarcane, and millet. Plants that do not use PEP-carboxylase in carbon fixation are called C plants because the primary carboxylation reaction, catalyzed by RuBisCO, produces the three-carbon 3-phosphoglyceric acids directly in the Calvin-Benson cycle. Over 90% of plants use carbon fixation, compared to 3% that use carbon fixation; however, the evolution of in over 60 plant lineages makes it a striking example of convergent evolution.\n\nXerophytes, such as cacti and most succulents, also use PEP carboxylase to capture carbon dioxide in a process called Crassulacean acid metabolism (CAM). In contrast to metabolism, which \"spatially\" separates the fixation to PEP from the Calvin cycle, CAM \"temporally\" separates these two processes. CAM plants have a different leaf anatomy from plants, and fix the at night, when their stomata are open. CAM plants store the mostly in the form of malic acid via carboxylation of phosphoenolpyruvate to oxaloacetate, which is then reduced to malate. Decarboxylation of malate during the day releases inside the leaves, thus allowing carbon fixation to 3-phosphoglycerate by RuBisCO. Sixteen thousand species of plants use CAM.\n\nCyanobacteria possess carboxysomes, which increase the concentration of around RuBisCO to increase the rate of photosynthesis. An enzyme, carbonic anhydrase, located within the carboxysome releases CO from the dissolved hydrocarbonate ions (HCO). Before the CO diffuses out it is quickly sponged up by RuBisCO, which is concentrated within the carboxysomes. HCO ions are made from CO outside the cell by another carbonic anhydrase and are actively pumped into the cell by a membrane protein. They cannot cross the membrane as they are charged, and within the cytosol they turn back into CO very slowly without the help of carbonic anhydrase. This causes the HCO ions to accumulate within the cell from where they diffuse into the carboxysomes. Pyrenoids in algae and hornworts also act to concentrate around rubisco.\n\nThe overall process of photosynthesis takes place in four stages:\n\nPlants usually convert light into chemical energy with a photosynthetic efficiency of 3–6%.\nAbsorbed light that is unconverted is dissipated primarily as heat, with a small fraction (1–2%) re-emitted as chlorophyll fluorescence at longer (redder) wavelengths. This fact allows measurement of the light reaction of photosynthesis by using chlorophyll fluorometers.\n\nActual plants' photosynthetic efficiency varies with the frequency of the light being converted, light intensity, temperature and proportion of carbon dioxide in the atmosphere, and can vary from 0.1% to 8%. By comparison, solar panels convert light into electric energy at an efficiency of approximately 6–20% for mass-produced panels, and above 40% in laboratory devices.\n\nThe efficiency of both light and dark reactions can be measured but the relationship between the two can be complex. For example, the ATP and NADPH energy molecules, created by the light reaction, can be used for carbon fixation or for photorespiration in C plants. Electrons may also flow to other electron sinks. For this reason, it is not uncommon for authors to differentiate between work done under non-photorespiratory conditions and under photorespiratory conditions.\nChlorophyll fluorescence of photosystem II can measure the light reaction, and Infrared gas analyzers can measure the dark reaction. It is also possible to investigate both at the same time using an integrated chlorophyll fluorometer and gas exchange system, or by using two separate systems together. Infrared gas analyzers and some moisture sensors are sensitive enough to measure the photosynthetic assimilation of CO, and of ΔHO using reliable methods CO is commonly measured in μmols/m/s, parts per million or volume per million and H0 is commonly measured in mmol/m/s or in mbars. By measuring CO assimilation, ΔHO, leaf temperature, barometric pressure, leaf area, and photosynthetically active radiation or PAR, it becomes possible to estimate, “A” or carbon assimilation, “E” or transpiration, “gs” or stomatal conductance, and Ci or intracellular CO. However, it is more common to used chlorophyll fluorescence for plant stress measurement, where appropriate, because the most commonly used measuring parameters FV/FM and Y(II) or F/FM’ can be made in a few seconds, allowing the measurement of larger plant populations.\n\nGas exchange systems that offer control of CO levels, above and below ambient, allow the common practice of measurement of A/Ci curves, at different CO levels, to characterize a plant’s photosynthetic response.\nIntegrated chlorophyll fluorometer – gas exchange systems allow a more precise measure of photosynthetic response and mechanisms. While standard gas exchange photosynthesis systems can measure Ci, or substomatal CO levels, the addition of integrated chlorophyll fluorescence measurements allows a more precise measurement of C to replace Ci. The estimation of CO at the site of carboxylation in the chloroplast, or C, becomes possible with the measurement of mesophyll conductance or g using an integrated system.\n\nPhotosynthesis measurement systems are not designed to directly measure the amount of light absorbed by the leaf. But analysis of chlorophyll-fluorescence, P700- and P515-absorbance and gas exchange measurements reveal detailed information about e.g. the photosystems, quantum efficiency and the CO assimilation rates. With some instruments even wavelength-dependency of the photosynthetic efficiency can be analyzed.\n\nA phenomenon known as quantum walk increases the efficiency of the energy transport of light significantly. In the photosynthetic cell of an algae, bacterium, or plant, there are light-sensitive molecules called chromophores arranged in an antenna-shaped structure named a photocomplex. When a photon is absorbed by a chromophore, it is converted into a quasiparticle referred to as an exciton, which jumps from chromophore to chromophore towards the reaction center of the photocomplex, a collection of molecules that traps its energy in a chemical form that makes it accessible for the cell's metabolism. The exciton's wave properties enable it to cover a wider area and try out several possible paths simultaneously, allowing it to instantaneously \"choose\" the most efficient route, where it will have the highest probability of arriving at its destination in the minimum possible time. Because that quantum walking takes place at temperatures far higher than quantum phenomena usually occur, it is only possible over very short distances, due to obstacles in the form of destructive interference that come into play. These obstacles cause the particle to lose its wave properties for an instant before it regains them once again after it is freed from its locked position through a classic \"hop\". The movement of the electron towards the photo center is therefore covered in a series of conventional hops and quantum walks.\n\nEarly photosynthetic systems, such as those in green and purple sulfur and green and purple nonsulfur bacteria, are thought to have been anoxygenic, and used various other molecules as electron donors rather than water. Green and purple sulfur bacteria are thought to have used hydrogen and sulfur as electron donors. Green nonsulfur bacteria used various amino and other organic acids as an electron donor. Purple nonsulfur bacteria used a variety of nonspecific organic molecules. The use of these molecules is consistent with the geological evidence that Earth's early atmosphere was highly reducing at that time.\n\nFossils of what are thought to be filamentous photosynthetic organisms have been dated at 3.4 billion years old.\n\nThe main source of oxygen in the Earth's atmosphere derives from oxygenic photosynthesis, and its first appearance is sometimes referred to as the oxygen catastrophe. Geological evidence suggests that oxygenic photosynthesis, such as that in cyanobacteria, became important during the Paleoproterozoic era around 2 billion years ago. Modern photosynthesis in plants and most photosynthetic prokaryotes is oxygenic. Oxygenic photosynthesis uses water as an electron donor, which is oxidized to molecular oxygen () in the photosynthetic reaction center.\n\nSeveral groups of animals have formed symbiotic relationships with photosynthetic algae. These are most common in corals, sponges and sea anemones. It is presumed that this is due to the particularly simple body plans and large surface areas of these animals compared to their volumes. In addition, a few marine mollusks \"Elysia viridis\" and \"Elysia chlorotica\" also maintain a symbiotic relationship with chloroplasts they capture from the algae in their diet and then store in their bodies. This allows the mollusks to survive solely by photosynthesis for several months at a time. Some of the genes from the plant cell nucleus have even been transferred to the slugs, so that the chloroplasts can be supplied with proteins that they need to survive.\n\nAn even closer form of symbiosis may explain the origin of chloroplasts. Chloroplasts have many similarities with photosynthetic bacteria, including a circular chromosome, prokaryotic-type ribosome, and similar proteins in the photosynthetic reaction center. The endosymbiotic theory suggests that photosynthetic bacteria were acquired (by endocytosis) by early eukaryotic cells to form the first plant cells. Therefore, chloroplasts may be photosynthetic bacteria that adapted to life inside plant cells. Like mitochondria, chloroplasts possess their own DNA, separate from the nuclear DNA of their plant host cells and the genes in this chloroplast DNA resemble those found in cyanobacteria. DNA in chloroplasts codes for redox proteins such as those found in the photosynthetic reaction centers. The CoRR Hypothesis proposes that this Co-location is required for Redox Regulation.\n\nThe biochemical capacity to use water as the source for electrons in photosynthesis evolved once, in a common ancestor of extant cyanobacteria. The geological record indicates that this transforming event took place early in Earth's history, at least 2450–2320 million years ago (Ma), and, it is speculated, much earlier. Because the Earth's atmosphere contained almost no oxygen during the estimated development of photosynthesis, it is believed that the first photosynthetic cyanobacteria did not generate oxygen. Available evidence from geobiological studies of Archean (>2500 Ma) sedimentary rocks indicates that life existed 3500 Ma, but the question of when oxygenic photosynthesis evolved is still unanswered. A clear paleontological window on cyanobacterial evolution opened about 2000 Ma, revealing an already-diverse biota of blue-green algae. Cyanobacteria remained the principal primary producers of oxygen throughout the Proterozoic Eon (2500–543 Ma), in part because the redox structure of the oceans favored photoautotrophs capable of nitrogen fixation. Green algae joined blue-green algae as the major primary producers of oxygen on continental shelves near the end of the Proterozoic, but it was only with the Mesozoic (251–65 Ma) radiations of dinoflagellates, coccolithophorids, and diatoms did the primary production of oxygen in marine shelf waters take modern form. Cyanobacteria remain critical to marine ecosystems as primary producers of oxygen in oceanic gyres, as agents of biological nitrogen fixation, and, in modified form, as the plastids of marine algae.\n\nAlthough some of the steps in photosynthesis are still not completely understood, the overall photosynthetic equation has been known since the 19th century.\n\nJan van Helmont began the research of the process in the mid-17th century when he carefully measured the mass of the soil used by a plant and the mass of the plant as it grew. After noticing that the soil mass changed very little, he hypothesized that the mass of the growing plant must come from the water, the only substance he added to the potted plant. His hypothesis was partially accurate — much of the gained mass also comes from carbon dioxide as well as water. However, this was a signaling point to the idea that the bulk of a plant's biomass comes from the inputs of photosynthesis, not the soil itself.\n\nJoseph Priestley, a chemist and minister, discovered that, when he isolated a volume of air under an inverted jar, and burned a candle in it, the candle would burn out very quickly, much before it ran out of wax. He further discovered that a mouse could similarly \"injure\" air. He then showed that the air that had been \"injured\" by the candle and the mouse could be restored by a plant.\n\nIn 1778, Jan Ingenhousz, repeated Priestley's experiments. He discovered that it was the influence of sunlight on the plant that could cause it to revive a mouse in a matter of hours.\n\nIn 1796, Jean Senebier, a Swiss pastor, botanist, and naturalist, demonstrated that green plants consume carbon dioxide and release oxygen under the influence of light. Soon afterward, Nicolas-Théodore de Saussure showed that the increase in mass of the plant as it grows could not be due only to uptake of CO but also to the incorporation of water. Thus, the basic reaction by which photosynthesis is used to produce food (such as glucose) was outlined.\n\nCornelis Van Niel made key discoveries explaining the chemistry of photosynthesis. By studying purple sulfur bacteria and green bacteria he was the first to demonstrate that photosynthesis is a light-dependent redox reaction, in which hydrogen reduces carbon dioxide.\n\nRobert Emerson discovered two light reactions by testing plant productivity using different wavelengths of light. With the red alone, the light reactions were suppressed. When blue and red were combined, the output was much more substantial. Thus, there were two photosystems, one absorbing up to 600 nm wavelengths, the other up to 700 nm. The former is known as PSII, the latter is PSI. PSI contains only chlorophyll \"a\", PSII contains primarily chlorophyll \"a\" with most of the available chlorophyll \"b\", among other pigment. These include phycobilins, which are the red and blue pigments of red and blue algae respectively, and fucoxanthol for brown algae and diatoms. The process is most productive when the absorption of quanta are equal in both the PSII and PSI, assuring that input energy from the antenna complex is divided between the PSI and PSII system, which in turn powers the photochemistry.\nRobert Hill thought that a complex of reactions consisting of an intermediate to cytochrome b (now a plastoquinone), another is from cytochrome f to a step in the carbohydrate-generating mechanisms. These are linked by plastoquinone, which does require energy to reduce cytochrome f for it is a sufficient reductant. Further experiments to prove that the oxygen developed during the photosynthesis of green plants came from water, were performed by Hill in 1937 and 1939. He showed that isolated chloroplasts give off oxygen in the presence of unnatural reducing agents like iron oxalate, ferricyanide or benzoquinone after exposure to light. The Hill reaction is as follows:\n\nwhere A is the electron acceptor. Therefore, in light, the electron acceptor is reduced and oxygen is evolved.\n\nSamuel Ruben and Martin Kamen used radioactive isotopes to determine that the oxygen liberated in photosynthesis came from the water.\n\nMelvin Calvin and Andrew Benson, along with James Bassham, elucidated the path of carbon assimilation (the photosynthetic carbon reduction cycle) in plants. The carbon reduction cycle is known as the Calvin cycle, which ignores the contribution of Bassham and Benson. Many scientists refer to the cycle as the Calvin-Benson Cycle, Benson-Calvin, and some even call it the Calvin-Benson-Bassham (or CBB) Cycle.\n\nNobel Prize-winning scientist Rudolph A. Marcus was able to discover the function and significance of the electron transport chain.\n\nOtto Heinrich Warburg and Dean Burk discovered the I-quantum photosynthesis reaction that splits the CO, activated by the respiration.\n\nLouis N.M. Duysens and Jan Amesz discovered that chlorophyll a will absorb one light, oxidize cytochrome f, chlorophyll a (and other pigments) will absorb another light, but will reduce this same oxidized cytochrome, stating the two light reactions are in series.\n\nIn 1893, Charles Reid Barnes proposed two terms, \"photosyntax\" and \"photosynthesis\", for the biological process of \"synthesis of complex carbon compounds out of carbonic acid, in the presence of chlorophyll, under the influence of light\". Over time, the term \"photosynthesis\" came into common usage as the term of choice. Later discovery of anoxygenic photosynthetic bacteria and photophosphorylation necessitated redefinition of the term.\n\nAfter WWII at late 1940 at the University of California, Berkeley, the details of photosynthetic carbon metabolism were sorted out by the chemists Melvin Calvin, Andrew Benson, James Bassham and a score of students and researchers utilizing the carbon-14 isotope and paper chromatography techniques. The pathway of CO2 fixation by the algae \"Chlorella\" in a fraction of a second in light resulted in a 3 carbon molecule called phosphoglyceric acid (PGA). For that original and ground-breaking work, a Nobel Prize in Chemistry was awarded to Melvin Calvin in 1961. In parallel, plant physiologists studied leaf gas exchanges using the new method of infrared gas analysis and a leaf chamber where the net photosynthetic rates ranged from 10 to 13 u mole CO2/square metere.sec., with the conclusion that all terrestrial plants having the same photosynthetic capacities that were light saturated at less than 50% of sunlight.\n\nLater in 1958-1963 at Cornell University, field grown maize was reported to have much greater leaf photosynthetic rates of 40 u mol CO2/square meter.sec and was not saturated at near full sunlight. This higher rate in maize was almost double those observed in other species such as wheat and soybean, indicating that large differences in photosynthesis exist among higher plants. At the University of Arizona, detailed gas exchange research on more than 15 species of monocot and dicot uncovered for the first time that differences in leaf anatomy are crucial factors in differentiating photosynthetic capacities among species. In tropical grasses, including maize, sorghum, sugarcane, Bermuda grass and in the dicot amaranthus, leaf photosynthetic rates were around 38−40 u mol CO2/square meter.sec., and the leaves have two types of green cells, i. e. outer layer of mesophyll cells surrounding a tightly packed cholorophyllous vascular bundle sheath cells. This type of anatomy was termed Kranz anatomy in the 19th century by the botanist Gottlieb Haberlandt while studying leaf anatomy of sugarcane. Plant species with the greatest photosynthetic rates and Kranz anatomy showed no apparent photorespiration, very low CO2 compensation point, high optimum temperature, high stomatal resistances and lower mesophyll resistances for gas diffusion and rates never saturated at full sun light. The research at Arizona was designated Citation Classic by the ISI 1986. These species was later termed C4 plants as the first stable compound of CO2 fixation in light has 4 carbon as malate and aspartate. Other species that lack Kranz anatomy were termed C3 type such as cotton and sunflower, as the first stable carbon compound is the 3-carbon PGA acid. At 1000 ppm CO2 in measuring air, both the C3 and C4 plants had similar leaf photosynthetic rates around 60 u mole CO2/square meter.sec. indicating the suppression of photorespiration in C3 plants.\n\nThere are three main factors affecting photosynthesis and several corollary factors. The three main are:\n\nThe process of photosynthesis provides the main input of free energy into the biosphere, and is one of four main ways in which radiation is important for plant life.\n\nThe radiation climate within plant communities is extremely variable, with both time and space.\n\nIn the early 20th century, Frederick Blackman and Gabrielle Matthaei investigated the effects of light intensity (irradiance) and temperature on the rate of carbon assimilation.\nThese two experiments illustrate several important points: First, it is known that, in general, photochemical reactions are not affected by temperature. However, these experiments clearly show that temperature affects the rate of carbon assimilation, so there must be two sets of reactions in the full process of carbon assimilation. These are the light-dependent 'photochemical' temperature-independent stage, and the light-independent, temperature-dependent stage. Second, Blackman's experiments illustrate the concept of limiting factors. Another limiting factor is the wavelength of light. Cyanobacteria, which reside several meters underwater, cannot receive the correct wavelengths required to cause photoinduced charge separation in conventional photosynthetic pigments. To combat this problem, a series of proteins with different pigments surround the reaction center. This unit is called a phycobilisome.\n\nAs carbon dioxide concentrations rise, the rate at which sugars are made by the light-independent reactions increases until limited by other factors. RuBisCO, the enzyme that captures carbon dioxide in the light-independent reactions, has a binding affinity for both carbon dioxide and oxygen. When the concentration of carbon dioxide is high, RuBisCO will fix carbon dioxide. However, if the carbon dioxide concentration is low, RuBisCO will bind oxygen instead of carbon dioxide. This process, called photorespiration, uses energy, but does not produce sugars.\n\nRuBisCO oxygenase activity is disadvantageous to plants for several reasons:\n\nThe salvaging pathway for the products of RuBisCO oxygenase activity is more commonly known as photorespiration, since it is characterized by light-dependent oxygen consumption and the release of carbon dioxide.\n\n",
    "id": "24544",
    "title": "Photosynthesis"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=33548640",
    "text": "PI curve\n\nThe PI (photosynthesis-irradiance) curve is a graphical representation of the empirical relationship between solar irradiance and photosynthesis. A derivation of the Michaelis–Menten curve, it shows the generally positive correlation between light intensity and photosynthetic rate. Plotted along the \"x\"-axis is the independent variable, light intensity (irradiance), while the \"y\"-axis is reserved for the dependent variable, photosynthetic rate.\n\nThe PI curve can be applied to terrestrial and marine reactions but is most commonly used to explain ocean-dwelling phytoplankton’s photosynthetic response to changes in light intensity. Using this tool to approximate biological productivity is important because phytoplankton contribute ~50% of total global carbon fixation and are important suppliers to the marine food web.\n\nWithin the scientific community, the curve can be referred to as the PI, PE or Light Response Curve. While individual researchers may have their own preferences, all are readily acceptable for use in the literature. Regardless of nomenclature, the photosynthetic rate in question can be described in terms of carbon (C) fixed per unit per time. Since individuals vary in size, it is also useful to normalise C concentration to Chlorophyll a (an important photosynthetic pigment) to account for specific biomass.\n\nAs far back as 1905, marine researchers attempted to develop an equation to be used as the standard in establishing the relationship between solar irradiance and photosynthetic production. Several groups had relative success, but in 1976 a comparison study conducted by Alan Jassby and Trevor Platt, researchers at the Bedford Institute of Oceanography in Dartmouth, Nova Scotia, reached a conclusion that solidified the way in which a PI curve is developed. After evaluating the eight most-used equations, Jassby and Platt revealed that an adaptation of the Michaelis-Menten equation, previously used in enzyme kinetics, best served the relationship demonstration. Their findings were so conclusive that the Michaelis–Menten equation remains the standard for PI curve generation.\n\nThere are two simple derivations of the equation that are commonly used to generate the hyperbolic curve. The first assumes photosynthetic rate increases with increasing light intensity until Pmax is reached and continues to photosynthesise at the maximum rate thereafter.\n\nBoth Pmax and the initial slope of the curve, ΔP/ΔI, are species-specific, and are influenced by a variety of factors, such as nutrient concentration, temperature and the physiological capabilities of the individual. Light intensity is influenced by latitudinal position and undergo daily and seasonal fluxes which will also affect the overall photosynthetic capacity of the individual. These three parameters are predictable and can be used to predetermine the general PI curve a population should follow.\n\nAs can be seen in the graph, two species can have different responses to the same incremental changes in light intensity. Population A (in blue) has an initial rate higher than that of Population B (in red) and also exhibits a stronger rate change to increased light intensities at lower irradiance. Therefore, Population A will dominate in an environment with lower light availability. Although Population B has a slower photosynthetic response to increases in light intensity its Pmax is higher than that of Population A. This allows for eventual population dominance at greater light intensities. There are many determining factors influencing population success; using the PI curve to elicit predictions of rate flux to environmental changes is useful for monitoring phytoplankton bloom dynamics and ecosystem stability.\n\nThe second equation accounts for the phenomenon of photoinhibition. In the upper few meters of the ocean, phytoplankton may be subjected to irradiance levels that damage the chlorophyll-a pigment inside the cell, subsequently decreasing photosynthetic rate. The response curve depicts photoinhibition as a decrease in photosynthetic rate at light intensities stronger than those necessary for achievement of Pmax.\n\nTerms not included in the above equation are:\n\nData sets showing interspecific differences and population dynamics.\n\nThe hyperbolic response between photosynthesis and irradiance, depicted by the PI curve, is important for assessing phytoplankton population dynamics, which influence many aspects of the marine environment.\n\n\n",
    "id": "33548640",
    "title": "PI curve"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=33799190",
    "text": "N-philes\n\nN-philes are group of radical molecules which are specifically attracted to the C=N bonds, defying often the selectivity rules of electrophilic attack. N-philes can often masquerade as electrophiles, where acyl radicals are excellent examples which interact with Pi electrons of aryl groups.\n",
    "id": "33799190",
    "title": "N-philes"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=14241428",
    "text": "Mineralization (biology)\n\nIn biology, mineralization refers to a process where an inorganic substance precipitates in an organic matrix. This may be due to normal biological processes that take place during the life of an organism such as the formation of bones, egg shells, teeth, coral, and other exoskeletons. This term may also refer to abnormal processes that result in kidney and gall stones.\n\nMineralization can be subdivided into different categories depending on the following: the organisms or processes that create chemical conditions necessary for mineral formation, the origin of the substrate at the site of mineral precipitation, and the degree of control that the substrate has on crystal morphology, composition, and growth. These subcategories include: biomineralization, organomineralization, and inorganic mineralization, which can be subdivided further. However, usage of these terms vary widely in scientific literature because there are no standardized definitions.The following definitions are based largely on a paper written by Dupraz et al. (2009), which provided a framework for differentiating these terms.\n\nBiomineralization, biologically-controlled mineralization, occurs when crystal morphology, growth, composition, and location is completely controlled by the cellular processes of a specific organism. Examples include the shells of invertebrates, such as molluscs and brachiopods. Additionally, mineralization of collagen provides the crucial compressive strength for the bones, cartilage, and teeth of vertebrates.\n\nThis type of mineralization includes both biologically-induced mineralization and biologically-influenced mineralization.\n\nInorganic mineralization is a completely abiotic process. Chemical conditions necessary for mineral formation develop via environmental processes, such as evaporation or degassing. Furthermore, the substrate for mineral deposition is abiotic (i.e. contains no organic compounds) and there is no control on crystal morphology or composition. Examples of this type of mineralization include cave formations, such as stalagmites and stalactites.\n\nBiological mineralization can also take place as a result of fossilization. \nSee also calcification.\n\nBone mineralization occurs in human body by cells called osteoblasts.\n",
    "id": "14241428",
    "title": "Mineralization (biology)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=11344743",
    "text": "Biomolecular engineering\n\nBiomolecular engineering is the application of engineering principles and practices to the purposeful manipulation of molecules of biological origin. Biomolecular engineers integrate knowledge of biological processes with the core knowledge of chemical engineering in order to focus on molecular level solutions to issues and problems in the life sciences related to the environment, agriculture, energy, industry, food production, biotechnology and medicine. Biomolecular engineers purposefully manipulate carbohydrates, proteins, nucleic acids and lipids within the framework of the relation between their structure (see: nucleic acid structure, carbohydrate chemistry, protein structure,), function (see: protein function) and properties and in relation to applicability to such areas as environmental remediation, crop and livestock production, biofuel cells and biomolecular diagnostics. Fundamental attention is given to the thermodynamics and kinetics of molecular recognition in enzymes, antibodies, DNA hybridization, bio-conjugation/bio-immobilization and bioseparations. Attention is also given to the rudiments of engineered biomolecules in cell signaling, cell growth kinetics, biochemical pathway engineering and bioreactor engineering. Biomolecular engineers are leading the major shift towards understanding and controlling the molecular mechanisms that define life as we know it.\n\nDuring World War II, the need for large quantities of penicillin of acceptable quality brought together chemical engineers and microbiologists to focus on penicillin production. This created the right conditions to start a chain of reactions that lead to the creation of the field of biomolecular engineering. Biomolecular engineering was first defined in 1992 by the National Institutes of Health as research at the interface of chemical engineering and biology with an emphasis at the molecular level\". Although first defined as research, biomolecular engineering has since become an academic discipline and a field of engineering practice. Herceptin, a humanized Mab for breast cancer treatment, became the first drug designed by a biomolecular engineering approach and was approved by the FDA. Also, \"Biomolecular Engineering\" was a former name of the journal \"New Biotechnology\".\n\nBio-inspired technologies of the future can help explain biomolecular engineering. Looking at the Moore's law \"Prediction\", in the future quantum and biology-based processors are \"big\" technologies. With the use of biomolecular engineering, the way our processors work can be manipulated in order to function in the same sense a biological cell work. Biomolecular engineering has the potential to become one of the most important scientific disciplines because of its advancements in the analyses of gene expression patterns as well as the purposeful manipulation of many important biomolecules to improve functionality. Research in this field may lead to new drug discoveries, improved therapies, and advancement in new bioprocess technology. With the increasing knowledge of biomolecules, the rate of finding new high-value molecules including but not limited to antibodies, enzymes, vaccines, and therapeutic peptides will continue to accelerate. Biomolecular engineering will produce new designs for therapeutic drugs and high-value biomolecules for treatment or prevention of cancers, genetic diseases, and other types of metabolic diseases. Also, there is anticipation of industrial enzymes that are engineered to have desirable properties for process improvement as well the manufacturing of high-value biomolecular products at a much lower production cost. Using recombinant technology, new antibiotics that are active against resistant strains will also be produced.\n\nBiomolecular engineering deals with the manipulation of many key biomolecules. These include, but are not limited to, proteins, carbohydrates, nucleic acids, and lipids. These molecules are the basic building blocks of life and by controlling, creating, and manipulating their form and function there are many new avenues and advantages available to society. Since every biomolecule is different, there are a number of techniques used to manipulate each one respectively.\n\nProteins are polymers that are made up of amino acid chains linked with peptide bonds. They have four distinct levels of structure: primary, secondary, tertiary, and quaternary. \nPrimary structure refers to the amino acid backbone sequence. Secondary structure focuses on minor conformations that develop as a result of the hydrogen bonding between the amino acid chain. If most of the protein contains intermolecular hydrogen bonds it is said to be fibrillar, and the majority of its secondary structure will be beta sheets. However, if the majority of the orientation contains intramolecular hydrogen bonds, then the protein is referred to as globular and mostly consists of alpha helices. There are also conformations that consist of a mix of alpha helices and beta sheets as well as a beta helixes with an alpha sheets.\n\nThe tertiary structure of proteins deal with their folding process and how the overall molecule is arranged. Finally, a quaternary structure is a group of tertiary proteins coming together and binding.\nWith all of these levels, proteins have a wide variety of places in which they can be manipulated and adjusted. Techniques are used to affect the amino acid sequence of the protein (site-directed mutagenesis), the folding and conformation of the protein, or the folding of a single tertiary protein within a quaternary protein matrix. \nProteins that are the main focus of manipulation are typically enzymes. These are proteins that act as catalysts for biochemical reactions. By manipulating these catalysts, the reaction rates, products, and effects can be controlled. Enzymes and proteins are important to the biological field and research that there are specific subsets of engineering focusing only on proteins and enzymes.\n\nCarbohydrates are another important biomolecule. These are polymers, called polysaccharides, which are made up of chains of simple sugars connected via glycosidic bonds. These monosaccharides consist of a five to six carbon ring that contains carbon, hydrogen, and oxygen - typically in a 1:2:1 ratio, respectively. Common monosaccharides are glucose, fructose, and ribose. When linked together monosaccharides can form disaccharides, oligosaccharides, and polysaccharides: the nomenclature is dependent on the number of monosaccharides linked together. Common dissacharides, two monosaccharides joined together, are sucrose, maltose, and lactose. Important polysaccharides, links of many monosaccharides, are cellulose, starch, and chitin.\n\nCellulose is a polysaccharide made up of beta 1-4 linkages between repeat glucose monomers. It is the most abundant source of sugar in nature and is a major part of the paper industry. \nStarch is also a polysaccharide made up of glucose monomers; however, they are connected via an alpha 1-4 linkage instead of beta. Starches, particularly amylase, are important in many industries, including the paper, cosmetic, and food. \nChitin is a derivation of cellulose, possessing an acetamide group instead of an –OH on one of its carbons. Acetimide group is deacetylated the polymer chain is then called chitosan. Both of these cellulose derivatives are a major source of research for the biomedical and food industries. They have been shown to assist with blood clotting, have antimicrobial properties, and dietary applications. A lot of engineering and research is focusing on the degree of deacetylation that provides the most effective result for specific applications.\n\nNucleic acids are macromolecules that consist of DNA and RNA which are biopolymers consisting of chains of biomolecules. These two molecules are the genetic code and template that make life possible. Manipulation of these molecules and structures causes major changes in function and expression of other macromolecules. Nucleosides are glycosylamines containing a nucleobase bound to either ribose or deoxyribose sugar via a beta-glycosidic linkage. The sequence of the bases determine the genetic code. Nucleotides are nucleosides that are phosphorylated by specific kinases via a phosphodiester bond. Nucleotides are the repeating structural units of nucleic acids. The nucleotides are made of a nitrogenous base, a pentose (ribose for RNA or deoxyribose for DNA), and three phosphate groups. See, Site-directed mutagenesis, recombinant DNA, and ELISAs.\n\nLipids are biomolecules that are made up of glycerol derivatives bonded with fatty acid chains. Glycerol is a simple polyol that has a formula of C3H5(OH)3. Fatty acids are long carbon chains that have a carboxylic acid group at the end. The carbon chains can be either saturated with hydrogen; every carbon bond is occupied by a hydrogen atom or a single bond to another carbon in the carbon chain, or they can be unsaturated; namely, there are double bonds between the carbon atoms in the chain. Common fatty acids include lauric acid, stearic acid, and oleic acid. The study and engineering of lipids typically focuses on the manipulation of lipid membranes and encapsulation. Cellular membranes and other biological membranes typically consist of a phospholipid bilayer membrane, or a derivative thereof. Along with the study of cellular membranes, lipids are also important molecules for energy storage. By utilizing encapsulation properties and thermodynamic characteristics, lipids become significant assets in structure and energy control when engineering molecules.\n\nRecombinant DNA are DNA biomolecules that contain genetic sequences that are not native to the organism’s genome. Using recombinant techniques, it is possible to insert, delete, or alter a DNA sequence precisely without depending on the location of restriction sites. Recombinant DNA is used for a wide range of applications.\n\nThe traditional method for creating recombinant DNA typically involves the use of plasmids in the host bacteria. The plasmid contains a genetic sequence corresponding to the recognition site of a restriction endonuclease, such as EcoR1. After foreign DNA fragments, which have also been cut with the same restriction endonuclease, have been inserted into host cell, the restriction endonuclease gene is expressed by applying heat, or by introducing a biomolecule, such as arabinose. Upon expression, the enzyme will cleave the plasmid at its corresponding recognition site creating sticky ends on the plasmid. Ligases then joins the sticky ends to the corresponding sticky ends of the foreign DNA fragments creating a recombinant DNA plasmid.\n\nAdvances in genetic engineering have made the modification of genes in microbes quite efficient allowing constructs to be made in about a weeks worth of time. It has also made it possible to modify the organism's genome itself. Specifically, use of the genes from the bacteriophage lambda are used in recombination. This mechanism, known as recombineering, utilizes the three proteins Exo, Beta, and Gam, which are created by the genes exo, bet, and gam respectively. Exo is a double stranded DNA exonuclease with 5’ to 3’ activity. It cuts the double stranded DNA leaving 3’ overhangs. Beta is a protein that binds to single stranded DNA and assists homologous recombination by promoting annealing between the homology regions of the inserted DNA and the chromosomal DNA. Gam functions to protect the DNA insert from being destroyed by native nucleases within the cell.\n\nRecombinant DNA can be engineered for a wide variety of purposes. The techniques utilized allow for specific modification of genes making it possible to modify any biomolecule. It can be engineered for laboratory purposes, where it can be used to analyze genes in a given organism. In the pharmaceutical industry, proteins can be modified using recombination techniques. Some of these proteins include human insulin. Recombinant insulin is synthesized by inserting the human insulin gene into \"E. coli\", which then produces insulin for human use. Other proteins, such as human growth hormone, factor VIII, and hepatitis B vaccine are produced using similar means. Recombinant DNA can also be used for diagnostic methods involving the use of the ELISA method. This makes it possible to engineer antigens, as well as the enzymes attached, to recognize different substrates or be modified for bioimmobilization. Recombinant DNA is also responsible for many products found in the agricultural industry. Genetically modified food, such as golden rice, has been engineered to have increased production of vitamin A for use in societies and cultures where dietary vitamin A is scarce. Other properties that have been engineered into crops include herbicide-resistance and insect-resistance.\n\nSite-directed mutagenesis is a technique that has been around since the 1970s. The early days of research in this field yielded discoveries about the potential of certain chemicals such as bisulfite and aminopurine to change certain bases in a gene. This research continued, and other processes were developed to create certain nucleotide sequences on a gene, such as the use of restriction enzymes to fragment certain viral strands and use them as primers for bacterial plasmids. The modern method, developed by Michael Smith in 1978, uses an oligonucleotide that is complementary to a bacterial plasmid with a single base pair mismatch or a series of mismatches.\n\nSite directed mutagenesis is a valuable technique that allows for the replacement of a single base in an oligonucleotide or gene. The basics of this technique involve the preparation of a primer that will be a complementary strand to a wild type bacterial plasmid. This primer will have a base pair mismatch at the site where the replacement is desired. The primer must also be long enough such that the primer will anneal to the wild type plasmid. After the primer anneals, a DNA polymerase will complete the primer. When the bacterial plasmid is replicated, the mutated strand will be replicated as well. The same technique can be used to create a gene insertion or deletion. Often, an antibiotic resistant gene is inserted along with the modification of interest and the bacteria are cultured on an antibiotic medium. The bacteria that were not successfully mutated will not survive on this medium, and the mutated bacteria can easily be cultured.\n\nSite-directed mutagenesis can be useful for many different reasons. A single base pair replacement, could change a codon, and thus replace an amino acid in a protein. This is useful for studying the way certain proteins behave. It is also useful because enzymes can be purposefully manipulated by changing certain amino acids. If an amino acid is changed that is in close proximity to the active site, the kinetic parameters may change drastically, or the enzyme might behave in a different way. Another application of site directed mutagenesis is exchanging an amino acid residue far from the active site with a lysine residue or cysteine residue. These amino acids make it easier to covalently bond the enzyme to a solid surface, which allows for enzyme re-use and use of enzymes in continuous processes. Sometimes, amino acids with non-natural functional groups (such as ketones and azides) are added to proteins These additions may be for ease of bioconjugation, or to study the effects of amino acid changes on the form and function of the proteins. The coupling of site directed mutagenesis and PCR are being utilized to reduce interleukin-6 activity in cancerous cells. The bacteria \"bacillus subtilis\" is often used in site directed mutagenesis. The bacteria secretes an enzyme called subtilisin through the cell wall. Biomolecular engineers can purposely manipulate this gene to essentially make the cell a factory for producing whatever protein the insertion in the gene codes.\n\nBio-immobilization and bio-conjugation is the purposeful manipulation of a biomolecule’s mobility by chemical or physical means to obtain a desired property. Immobilization of biomolecules allows exploiting characteristics of the molecule under controlled environments. For example\n, the immobilization of glucose oxidase on calcium alginate gel beads can be used in a bioreactor. The resulting product will not need purification to remove the enzyme because it will remain linked to the beads in the column. Examples of types of biomolecules that are immobilized are enzymes, organelles, and complete cells. \nBiomolecules can be immobilized using a range of techniques. The most popular are physical entrapment, adsorption, and covalent modification.\n\nBecause immobilization restricts the biomolecule, care must be given to ensure that functionality is not entirely lost. Variables to consider are pH, temperature, solvent choice, ionic strength, orientation of active sites due to conjugation. For enzymes, the conjugation will lower the kinetic rate due to a change in the 3-dimensional structure, so care must be taken to ensure functionality is not lost. \nBio-immobilization is used in technologies such as diagnostic bioassays, biosensors, ELISA, and bioseparations. Interleukin (IL-6) can also be bioimmobilized on biosensors. The ability to observe these changes in IL-6 levels is important in diagnosing an illness. A cancer patient will have elevated IL-6 level and monitoring those levels will allow the physician to watch the disease progress. A direct immobilization of IL-6 on the surface of a biosensor offers a fast alternative to ELISA.\n\nThe polymerase chain reaction (PCR) is a scientific technique that is used to replicate a piece of a DNA molecule by several orders of magnitude. PCR implements a cycle of repeated heated and cooling known as thermal cycling along with the addition of DNA primers and DNA polymerases to selectively replicate the DNA fragment of interest. The technique was developed by Kary Mullis in 1983 while working for the Cetus Corporation. Mullis would go on to win the Nobel Prize in Chemistry in 1993 as a result of the impact that PCR had in many areas such as DNA cloning, DNA sequencing, and gene analysis.\n\nA number of biomolecular engineering strategies have played a very important role in the development and practice of PCR. For instance a crucial step in ensuring the accurate replication of the desired DNA fragment is the creation of the correct DNA primer. The most common method of primer synthesis is by the phosphoramidite method. This method includes the biomolecular engineering of a number of molecules to attain the desired primer sequence. The most prominent biomolecular engineering technique seen in this primer design method is the initial bioimmobilization of a nucleotide to a solid support. This step is commonly done via the formation of a covalent bond between the 3’-hydroxy group of the first nucleotide of the primer and the solid support material.\n\nFurthermore, as the DNA primer is created certain functional groups of nucleotides to be added to the growing primer require blocking to prevent undesired side reactions. This blocking of functional groups as well as the subsequent de-blocking of the groups, coupling of subsequent nucleotides, and eventual cleaving from the solid support are all methods of manipulation of biomolecules that can be attributed to biomolecular engineering. The increase in interleukin levels is directly proportional to the increased death rate in breast cancer patients. PCR paired with Western blotting and ELISA help define the relationship between cancer cells and IL-6.\n\nEnzyme-linked immunosorbent assay is an assay that utilizes the principle of antibody-antigen recognition to test for the presence of certain substances. The three main types of ELISA tests which are indirect ELISA, sandwich ELISA, and competitive ELISA all rely on the fact that antibodies have an affinity for only one specific antigen. Furthermore, these antigens or antibodies can be attached to enzymes which can react to create a colorimetric result indicating the presence of the antibody or antigen of interest. Enzyme linked immunosorbent assays are used most commonly as diagnostic tests to detect HIV antibodies in blood samples to test for HIV, human chorionic gonadotropin molecules in urine to indicate pregnancy, and Mycobacterium tuberculosis antibodies in blood to test patients for tuberculosis. Furthermore, ELISA is also widely used as a toxicology screen to test people's serum for the presence of illegal drugs.\n\nAlthough there are three different types of solid state enzyme-linked immunosorbent assays, all three types begin with the bioimmobilization of either an antibody or antigen to a surface. This bioimmobilization is the first instance of biomolecular engineering that can be seen in ELISA implementation. This step can be performed in a number of ways including a covalent linkage to a surface which may be coated with protein or another substance. The bioimmobilization can also be performed via hydrophobic interactions between the molecule and the surface. Because there are many different types of ELISAs used for many different purposes the biomolecular engineering that this step requires varies depending on the specific purpose of the ELISA.\n\nAnother biomolecular engineering technique that is used in ELISA development is the bioconjugation of an enzyme to either an antibody or antigen depending on the type of ELISA. There is much to consider in this enzyme bioconjugation such as avoiding interference with the active site of the enzyme as well as the antibody binding site in the case that the antibody is conjugated with enzyme. This bioconjugation is commonly performed by creating crosslinks between the two molecules of interest and can require a wide variety of different reagents depending on the nature of the specific molecules.\n\nInterleukin (IL-6) is a signaling protein that has been known to be present during an immune response. The use of the sandwich type ELISA quantifies the presence of this cytokine within spinal fluid or bone marrow samples.\n\nBiomolecular engineering is an extensive discipline with applications in many different industries and fields. As such, it is difficult to pinpoint a general perspective on the Biomolecular engineering profession. The biotechnology industry, however, provides an adequate representation. The biotechnology industry, or biotech industry, encompasses all firms that use biotechnology to produce goods or services or to perform biotechnology research and development. In this way, it encompasses many of the industrial applications of the biomolecular engineering discipline. By examination of the biotech industry, it can be gathered that the principal leader of the industry is the United States, followed by France and Spain. It is also true that the focus of the biotechnology industry and the application of biomolecular engineering is primarily clinical and medical. People are willing to pay for good health, so most of the money directed towards the biotech industry stays in health-related ventures.\n\nScaling up a process involves using data from an experimental-scale operation (model or pilot plant) for the design of a large (scaled-up) unit, of commercial size. Scaling up is a crucial part of commercializing a process. For example, insulin produced by genetically modified Escherichia coli bacteria was initialized on a lab-scale, but to be made commercially viable had to be scaled up to an industrial level. In order to achieve this scale-up a lot of lab data had to be used to design commercial sized units. For example, one of the steps in insulin production involves the crystallization of high purity glargin insulin. In order to achieve this process on a large scale we want to keep the Power/Volume ratio of both the lab-scale and large-scale crystallizers the same in order to achieve homogeneous mixing. \nWe also assume the lab-scale crystallizer has geometric similarity to the large-scale crystallizer. Therefore,\n\nP/V α Nd\n\nwhere d= crystallizer impeller diameter\n\nN= impeller rotation rate\n\nA broad term encompassing all engineering applied to the life sciences. This field of study utilizes the principles of biology along with engineering principles to create marketable products. Some bioengineering applications include:\n\nBiochemistry is the study of chemical processes in living organisms, including, but not limited to, living matter. Biochemical processes govern all living organisms and living processes and the field of biochemistry seeks to understand and manipulate these processes.\n\n\n\nBioelectrical engineering involves the electrical fields generated by living cells or organisms. Examples include the electric potential developed between muscles or nerves of the body. This discipline requires knowledge in the fields of electricity and biology to understand and utilize these concepts to improve or better current bioprocesses or technology.\n\n\nBiomedical engineering is a sub category of bioengineering that uses many of the same principles but focuses more on the medical applications of the various engineering developments. Some applications of biomedical engineering include:\n\n\nChemical engineering is the processing of raw materials into chemical products. It involves preparation of raw materials to produce reactants, the chemical reaction of these reactants under controlled conditions, the separation of products, the recycle of byproducts, and the disposal of wastes. Each step involves certain basic building blocks called “unit operations,” such as extraction, filtration, and distillation. These unit operations are found in all chemical processes. Biomolecular engineering is a subset of Chemical Engineering that applies these same principles to the processing of chemical substances made by living organisms.\n\nThe discipline of biomolecular engineering has become ever more prevalent with the better understanding and advancement of current sciences and technologies. In previous years, biomolecular engineering was not a well-known career path, but the growth in popularity of this subject has resulted in new programs offered to undergraduate and graduate students.\n\nNewly developed and offered undergraduate programs across the United States, often coupled to the chemical engineering program, allow students to achieve a B.S. degree. According to ABET (Accreditation Board for Engineering and Technology), biomolecular engineering curricula \"must provide thorough grounding in the basic sciences including chemistry, physics, and biology, with some content at an advanced level… [and] engineering application of these basic sciences to design, analysis, and control, of chemical, physical, and/or biological processes.\" Common curricula consist of major engineering courses including transport, thermodynamics, separations, and kinetics, with additions of life sciences courses including biology and biochemistry, and including specialized biomolecular courses focusing on cell biology, nano- and biotechnology, biopolymers, etc.\n\nTo further education in biomolecular engineering studies, the option to get an M.S. or Ph.D. is becoming ever more available in various colleges and universities.\n\n\n\n",
    "id": "11344743",
    "title": "Biomolecular engineering"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=35215882",
    "text": "Isothermal microcalorimetry\n\nIsothermal microcalorimetry (IMC) is a laboratory method for real-time monitoring and dynamic analysis of chemical, physical and biological processes. Over a period of hours or days, IMC determines the onset, rate, extent and energetics of such processes for specimens in small ampoules (e.g. 3–20 ml) at a constant set temperature (c. 15 °C–150 °C).\n\nIMC accomplishes this dynamic analysis by measuring and recording vs. elapsed time the net rate of heat flow (μJ/sec = μW) to or from the specimen ampoule, and the cumulative amount of heat (J) consumed or produced.\n\nIMC is a powerful and versatile analytical tool for four closely related reasons:\n\nThe IMC method of studying rates of processes is thus broadly applicable, provides real-time continuous data, and is sensitive. The measurement is simple to make, takes place unattended and is non-interfering (e.g. no fluorescent or radioactive markers are needed).\n\nHowever, there are two main caveats that must be heeded in use of IMC:\n\nIn general, possible applications of IMC are only limited by the imagination of the person who chooses to employ it as an analytical tool and the physical constraints of the method. Besides the two general limitations (main caveats) described above, these constraints include specimen and ampoule size, and the temperatures at which measurements can be made. IMC is generally best suited to evaluating processes which take place over hours or days. IMC has been used in an extremely wide range of applications, and many examples are discussed in this article, supported by references to published literature. Applications discussed range from measurement of slow oxidative degradation of polymers and instability of hazardous industrial chemicals to detection of bacteria in urine and evaluation of the effects of drugs on parasitic worms. \"The present emphasis in this article is applications of the latter type—biology and medicine.\"\n\nCalorimetry is the science of measuring the heat of chemical reactions or physical changes. Calorimetry is performed with a calorimeter.\n\nIsothermal microcalorimetry (IMC) is a laboratory method for real-time, continuous measurement of the heat flow rate (μJ/sec = μW) and cumulative amount of heat (J) consumed or produced at essentially constant temperature by a specimen placed in an IMC instrument. Such heat is due to chemical or physical changes taking place in the specimen. The heat flow is proportional to the aggregate rate of changes taking place at a given time. The aggregate heat produced during a given time interval is proportional to the cumulative amount of aggregate changes which have taken place.\n\nIMC is thus a means for dynamic, quantitative evaluation of the rates and energetics of a broad range of rate processes, including biological processes. A rate process is defined here as a physical and/or chemical change whose progress over time can be described either empirically or by a mathematical model (Bibliography: Glasstone, et al. 1941 and Johnson, et al. 1974 and Rate equation).\n\nThe simplest use of IMC is detecting that one or more rate processes are taking place in a specimen because heat is being produced or consumed at a rate that is greater than the detection limit of the instrument used. This can be a useful, for example, as a general indicator that a solid or liquid material is not inert but instead is changing at a given temperature. In biological specimens containing a growth medium, appearance over time of a detectable and rising heat flow signal is a simple general indicator of the presence of some type of replicating cells.\nHowever, for most applications it is paramount to know, by some means, what process or processes are being measured by monitoring heat flow. In general this entails first having detailed physical, chemical and biological knowledge of the items placed in an IMC ampoule before it is placed in an IMC instrument for evaluation of heat flow over time. It is also then necessary to analyze the ampoule contents after IMC measurements of heat flow have been made for one or more periods of time. Also, logic-based variations in ampoule contents can be used to identify the specific source or sources of heat flow. When rate process and heat flow relationships have been established, it is then possible to rely directly on the IMC data.\n\nWhat IMC can measure in practice depends in part on specimen dimensions, and they are necessarily constrained by instrument design. A given commercial instrument typically accepts specimens of up to a fixed diameter and height. Instruments accepting specimens with dimensions of up to ca. 1 or 2 cm in diameter x ca. 5 cm in height are typical. In a given instrument larger specimens of a given type usually produce greater heat flow signals, and this can augment detection and precision.\n\nFrequently, specimens are simple 3 to 20 ml cylindrical ampoules (Fig. 1) containing materials whose rate processes are of interest—e.g. solids, liquids, cultured cells—or any combination of these or other items expected to result in production or consumption of heat. Many useful IMC measurements can be carried out using simple sealed ampoules, and glass ampoules are common since glass is not prone to undergoing heat-producing chemical or physical changes. However, metal or polymeric ampoules are sometimes employed. Also, instrument/ampoule systems are available which allow injection or controlled through-flow of gasses or liquids and/or provide specimen mechanical stirring.\n\nCommercial IMC instruments allow heat flow measurements at temperatures ranging from ca. 15 °C – 150 °C. The range for a given instrument may be somewhat different.\n\nIMC is extremely sensitive – e.g. heat from slow chemical reactions in specimens weighing a few grams, taking place at reactant consumption rates of a few percent per year, can be detected and quantified in a matter of days. Examples include gradual oxidation of polymeric implant materials and shelf life studies of solid pharmaceutical drug formulations (Applications (Solid Materials)).\n\nAlso the rate of metabolic heat production of e.g. a few thousand living cells, microorganisms or protozoa in culture in an IMC ampoule can be measured. The amount of such metabolic heat can be correlated (through experimentation) with the number of cells or organisms present. Thus, IMC data can be used to monitor in real time the number of cells or organisms present and the net rate of growth or decline in this number (Applications (Biology and Medicine)).\n\nAlthough some non-biological applications of IMC are discussed (Applications (Solid Materials)) the present emphasis in this article is on the use of IMC in connection with biological processes (Applications (Biology and Medicine)).\n\nA graphic display of a common type of IMC data is shown in Fig. 2. At the top is a plot of recorded heat flow (μ W=μ J/s) vs. time from a specimen in a sealed ampoule, due to an exothermic rate process which begins, accelerates, reaches a peak heat flow and then subsides. Such data are directly useful (e.g. detection of a process and its duration under fixed conditions) but the data are also easily assessed mathematically to determine process parameters. For example, Fig. 2 also shows an integration of the heat flow data, giving accumulated heat (J) vs. time. As shown, parameters such as the maximum growth (heat generation) rate of the process, and the duration time of the lag phase before the process reaches maximum heat can be calculated from the integrated data. Calculations using heat flow rate data stored as computer files are easily automated. Analyzing IMC data in this manner to determine growth parameters has important applications the life sciences (Applications (Biology and Medicine)). Also, heat flow rates obtained at a series of temperatures can be used to obtain the activation energy of the process being evaluated (Hardison et al. 2003).\n\nLavoisier and Laplace are credited with creating and using the first isothermal calorimeter in ca. 1780 (Bibliography: Lavoisier A & Laplace PS 1780). Their instrument employed ice to produce a relatively constant temperature in a confined space. They realized that when they placed a heat-producing specimen on the ice (e.g. a live animal), the mass of liquid water produced by the melting ice was directly proportional to the heat produced by the specimen.\n\nMany modern IMC instrument designs stem from work done in Sweden in the late 1960s and early 1970s (Wadsö 1968, Suurkuusk & Wadsö 1974). This work took advantage of the parallel development of solid state electronic devices—particularly commercial availability of small thermoelectric effect (Peltier-Seebeck) devices for converting heat flow into voltage—and vice versa.\n\nIn the 1980s, multi-channel designs emerged (Suurkuusk 1982), which allow parallel evaluation of multiple specimens. This greatly increased the power and usefulness of IMC and led to efforts to fine-tune the method (Thorén et al. 1989). Much of the further design and development done in the 1990s was also accomplished in Sweden by Wadsö and Suurkuusk and their colleagues. This work took advantage of the parallel development of personal computer technology which greatly augmented the ability to easily store, process and interpret heat flow vs. time data.\n\nInstrument development work since the 1990s has taken further advantage of the continued development of solid-state electronics and personal computer technology. This has created IMC instruments of increasing sensitivity and stability, numbers of parallel channels, and even greater ability to conveniently record, store and rapidly process IMC data. In connection with wider use, substantial attention has been paid to creating standards for describing the performance of IMC instruments (e.g. precision, accuracy, sensitivity) and for methods of calibration (Wadsö and Goldberg 2001).\n\nModern IMC instruments are actually semi-adiabatic—i.e. heat transfer between the specimen and its surroundings is not zero (adiabatic), because IMC measurement of heat flow depends on the existence of a small temperature differential—ca. 0.001 °C. However, because the differential is so low, IMC measurements are essentially isothermal. Fig. 3. shows an overview of an IMC instrument which contains 48 separate heat flow measurement modules. One module is shown. The module's measuring unit is typically a Peltier-Seebeck device. The device produces a voltage proportional to the temperature difference between a specimen which is producing or consuming heat and a thermally inactive reference which is at the temperature of the heat sink. The temperature difference is in turn proportional to the rate at which the specimen is producing or consuming heat (see Calibration below). All the modules in an instrument use the same heat sink and thermostat and thus all produce data at the same set temperature. However, it is generally possible to start and stop measurements in each ampoule independently. In a highly parallel (e.g. 48-channel) instrument like the one shown in Fig. 3, this makes it possible to perform (start and stop) several different experiments whenever it is convenient to do so.\n\nAlternatively, IMC instruments can be equipped with duplex modules which yield signals proportional to the heat flow difference between two ampoules. One of two such duplex ampoules is often a blank or control—i.e. a specimen which does not contain the material producing the rate process of interest, but whose content is otherwise identical to that which is in the specimen ampoule. This provides a means for eliminating minor heat-producing reactions which are not of interest—for example gradual chemical changes over a period of days in a cell culture medium at the measurement temperature. Many useful IMC measurements can be carried out using simple sealed ampoules. However, as mentioned above, instrument/ampoule systems are available which allow or even control flow of gasses or liquids to and/or from the specimens and/or provide specimen mechanical stirring.\n\nHeat flow is usually measured relative to a reference insert, as shown in Fig. 3. This is typically a metal coupon that is chemically and physically stable at any temperature in the instrument's operating range and thus will not produce or consume heat itself. For best performance, the reference should have a heat capacity close to that of the specimen (e.g. IMC ampoule plus contents).\n\nCommercial IMC instruments are often operated as heat conduction (hc) calorimeters in which heat produced by the specimen (i.e. material in an ampoule) flows to the heat sink, typically an aluminum block contained in a thermostat (e.g. constant temperature bath). As mentioned above, an IMC instrument operating in hc mode is not precisely isothermal because small differences between the set temperature and the specimen temperature necessarily exist—so that there is measurable heat flow. However, small variations in specimen temperature do not significantly affect heat sink temperature because the heat capacity of the heat sink is much higher than the specimen—usually ca. 100×.\n\nHeat transfer between the specimen and the heat sink takes place through a Peltier-Seebeck device, allowing dynamic measurement of heat produced or consumed. In research-quality instruments, thermostat/heat sink temperature is typically accurate to <nowiki>«/nowiki> ±0.1 K and maintained within ca. <nowiki>«/nowiki> ±100 μK/24h. The precision with which heat sink temperature is maintained over time is a major determinant of the precision of the heat flow measurements over time. An advantage of hc mode is a large dynamic range. Heat flows of ca. 50,000 μ W can be measured with a precision of ca. ±0.2 μW. Thus measuring a heat flow of ca. <nowiki»</nowiki>0.2 μW above baseline constitutes detection of heat flow, although a more conservative detection of 10× the precision limit is often used.\n\nSome IMC instruments operate (or can also be operated) as power compensation (pc) calorimeters. In this case, in order to maintain the specimen at the set temperature, heat produced is compensated using a Peltier-Seebeck device. Heat consumed is compensated either by an electric heater or by reversing the polarity of the device (van Herwaarden, 2000). If a given instrument is operated in pc mode rather than hc, the precision of heat flow measurement remains the same (e.g. ca. ±0.2 μW). The advantage of compensation mode is a smaller time constant – i.e. the time needed to detect a given heat flow pulse is ca.10X shorter than in conduction mode. The disadvantage is a ca. 10X smaller dynamic range compared to hc mode.\n\nFor operation in either hc or pc mode, routine calibration in commercial instruments is usually accomplished with built-in electric heaters. The performance of the electrical heaters can in turn be validated using specimens of known heat capacity or which produce chemical reactions whose heat production per unit mass is known from thermodynamics (Wadsö and Goldberg 2001). In either hc or pc mode, the resulting signal is a computer-recordable voltage, calibrated to represent specimen μ W-range heat flow vs. time. Specifically, if no significant thermal gradients exist in the specimen, then P = e <nowiki>[</nowiki>U <nowiki>+</nowiki> t (dU/dt)<nowiki>]</nowiki>, where P is heat flow (i.e. μ W), ε is the calibration constant, U the measured potential difference across the thermopile, and t the time constant. Under steady-state conditions—for example during the release of a constant electrical calibration current, this simplifies to P = e U. (Wadsö and Goldberg 2001).\n\nMany highly useful IMC measurements can be conducted in sealed ampoules (Fig. 1) which offer advantages of simplicity, protection from contamination and (where needed) a substantial margin of bio-safety for persons handling or exposed to the ampoules. A closed ampoule can contain any desired combination of solids, liquids, gasses or items of biologic origin. Initial gas composition in the ampoule head space can be controlled by sealing the ampoule in the desired gas environment.\n\nHowever, there are also IMC instrument/ampoule designs which permit controlled flow of gas or liquid through the ampoule during measurement and/or mechanical stirring. Also, with proper accessories, some IMC instruments can be operated as ITC (isothermal titration calorimetry) instruments. The topic of ITC is covered elsewhere (see Isothermal titration calorimetry). In addition some IMC instruments can record heat flow while the temperature is slowly changed (scanned) over time. The scanning rate has to be slow-ca. ± 2 K°/h-in order to keep IMC-scale specimens (e.g. a few grams) sufficiently close to the heat sink temperature (<nowiki>«/nowiki>ca. 0.1 °C). Fast scanning of temperature is the province of Differential Scanning Calorimetry (DSC) instruments which generally use much smaller specimens. Some DSC instruments can be operated in IMC mode, but the small ampoule (and therefore specimen) size needed for scanning limit the utility and sensitivity of DSC instruments used in IMC mode.\n\nHeat flow rate (μJ/sec = μW) measurements are accomplished by first setting an IMC instrument thermostat at a selected temperature and allowing the instrument<nowiki>'</nowiki>s heat sink to stabilize at that temperature. If an IMC instrument operating at one temperature is set to a new temperature, re-stabilization at the new temperature setting may take several hours—even a day. As explained above, achievement and maintenance of a precisely stable temperature is fundamental to achieving precise heat flow measurements in the μW range over extended times (e.g. days).\n\nAfter temperature stabilization, if an externally prepared ampoule (or some solid specimen of ampoule dimensions) is used, it is slowly introduced (e.g. lowered) into an instrument<nowiki>'</nowiki>s measurement module, usually in a staged operation. The purpose is to ensure that by the time the ampoule/specimen is in the measurement position, its temperature is close to (within c. 0.001 °C) of the measurement temperature. This is so that any heat flow then measured is due to specimen rate processes rather than due to a continuing process of bringing the specimen to the set temperature. The time for introduction of a specimen in a 3–20 ml IMC ampoule into measurement position is ca. 40 minutes in many instruments. This means that heat flow from any processes which take place within a specimen during that the introduction period will not be recorded.\n\nIf an in-place ampoule is used, and some agent or specimen is injected, this also produces a period of instability, but it is on the order ca. 1 minute. Fig. 5 provides examples of both the long period needed to stabilize an instrument if an ampoule is introduced directly, and the short period of instability due to injection.\n\nAfter the introduction process, specimen heat flow can be precisely recorded continuously, for as long as it is of interest. The extreme stability of research-grade instruments (<nowiki>«/nowiki> ±100 μK/24h ) means that accurate measurements can be (and often are) made for a period of days. Since the heat flow signal is essentially readable in real time, it serves as a means for deciding whether or not heat flow of interest is still occurring. Also, modern instruments store heat flow vs. time data as computer files, so both real-time and retrospective graphic display and mathematical analysis of data are possible.\n\nAs indicated below, IMC has many advantages as a method for analyzing rate processes, but there are also some caveats that must be heeded.\n\nAny rate process can be studied—if suitable specimens will fit IMC instrument module geometry, and proceed at rates amenable to IMC methodology (see above). As shown under Applications, IMC is in use to quantify an extremely wide range of rate processes in vitro—e.g. from solid-state stability of polymers (Hardison et al. 2003) to efficacy of drug compounds against parasitic worms (Maneck et al. 2011). IMC can also determine the aggregate rate of uncharacterized, complex, or multiple interactions (Lewis & Daniels). This is especially useful for comparative screening—e.g. the effects of different combinations of material composition and/or fabrication processes on overall physico-chemical stability.\n\nIMC heat flow data are obtained as voltage fluctuations vs. time, stored as computer files and can be displayed essentially in real time—as the rate process is occurring. The heat flow-related voltage is continuous over time, but in modern instruments it is normally sampled digitally. The frequency of digital sampling can be controlled as needed—i.e. frequent sampling of rapid heat flow changes for better time resolution or slower sampling of slow changes in order to limit data file size.\n\nIMC is sensitive enough to detect and quantify in short times (hours, days) reactions which consume only a few percent of reactants over long times (months). IMC thus avoids long waits often needed until enough reaction product has accumulated for conventional (e.g. chemical) assays. This applies to both physical and biological specimens (see Applications).\n\nAt each combination of specimen variables and set temperature of interest, IMC provides direct determination of the heat flow kinetics and cumulative heat of rate processes. This avoids any need to assume that a rate process remains the same when temperature or other controlled variables are changed before an IMC measurement.\n\nFor comparisons of the effect of experimental variables (e.g. initial concentrations) on rate processes, IMC does not require development and use of chemical or other assay methods. If absolute data are required (e.g. quantity of product produced by a process), then assays can be conducted in parallel on specimens identical to those used for IMC (and/or on IMC specimens after IMC runs). The resultant assay data is used to calibrate the rate data obtained by IMC.\n\nIMC does not require adding markers (e.g. fluorescent or radioactive substances) to capture rate processes. Unadulterated specimens can be used, and after an IMC run, the specimen is unchanged (except by the processes which have taken place). The post-IMC specimen can be subjected to any kind of physical, chemical, morphological or other evaluation of interest.\n\nAs indicated in the methodology description, when the IMC method of inserting a sealed ampoule is used, it is not possible to capture heat flow during the first ca. 40 minutes while the specimen is slowly being brought to the set temperature. In this mode therefore, IMC is best suited to studying processes which start slowly or occur slowly at a given temperature. This caveat also applies to the time \"before\" insertion—i.e. time elapsed between preparing a specimen (in which a rate process may then start) and starting the IMC insertion process (Charlebois \"et al.\" 2003). This latter effect is usually minimized if the temperature chosen for IMC is substantially higher (e.g. 37 °C) than the temperature at which the specimen is prepared (e.g. 25 °C).\n\nIMC captures the \"aggregate\" heat production or consumption resulting from all processes taking place within a specimen, including for example\n\n\nThus great care must be taken in experimental planning and design to identify all possible processes which may be taking place. It is often necessary to design and conduct preliminary studies intended to systematically determine if multiple processes are taking place and if so, their contributions to aggregate heat flow. One strategy, in order to eliminate extraneous heat flow data, is to compare heat flow for a specimen in which the rate process of interest is taking place with that from a blank specimen which includes everything in the specimen of interest—except the item which will undergo the rate process of interest. This can be directly accomplished with instruments having duplex IMC modules which report the net heat flow difference between two ampoules.\n\nAfter a discussion of some special sources of IMC application information, several specific categories of IMC analysis of rate processes are covered, and recent examples (with literature references) are discussed in each category.\n\nThe Bibliography lists the four extensive volumes of the Handbook of Thermal Analysis and Calorimetry: Vol. 1 Principles and Practice (1998), Vol. 2 Applications to Inorganic and Miscellaneous Materials (2003), Vol. 3 Applications to Polymers and Plastics (2002), and Vol. 4 From Macromolecules to Man (1999). These constitute a prime source of information on (and literature references to) IMC applications and examples published prior to ca. 2000.\n\nSome IMC instrument manufacturers have assembled application notes, and make them available to the public. The notes are often (but not always) adaptations of journal papers. An example is the Microcalorimetry Compendium Vol. I and II offered by TA Instruments, Inc. and listed in the Bibliography.\n\n\"Proteins\" the first section of notes in Vol. I, is not of interest here, as it describes studies employing Isothermal titration calorimetry. The subsequent sections of Vol. I, Life & Biological Sciences and Pharmaceuticals contain application notes for both IMC and Differential scanning calorimetry. Vol. II of the compendium is devoted almost entirely to IMC applications. Its sections are entitled Cement, Energetics, Material and Other. A possible drawback to these two specific compendia is that none of the notes are dated. Although the compendia were published in 2009, some of the notes describe IMC instruments which were in use years ago and are no longer available. Thus, some of the notes, while still relevant and instructive, often describe studies done before 2000.\n\nIn general, possible applications of IMC are only limited by the imagination of the person who chooses to employ IMC as an analytical tool—within the previously described constraints presented by existing IMC instruments and methodology. This is because it is a universal means for monitoring any chemical, physical or biological rate process. Below are some IMC application categories with examples in each. In most categories, there are many more published examples than those mentioned and referenced. The categories are somewhat arbitrary and often overlap. A different set of categories might be just as logical, and more categories could be added.\n\nIMC is widely used for studying the rates of formation of a variety of materials by various processes. It is best suited to study processes which occur slowly—i.e. over hours or days. A prime example is the study of hydration and setting reactions of calcium mineral cement formulations. One paper provides an overview (Gawlicki, et al. 2010) and another describes a simple approach (Evju 2003). Other studies focus on insights into cement hydration provided by IMC combined with IR spectroscopy (Ylmen et al. 2010) and on using IMC to study the influence of compositional variables on cement hydration and setting times (Xu et al. 2011).\n\nIMC can also be conveniently used to study the rate and amount of hydration (in air of known humidity) of calcium minerals or other minerals. To provide air of known humidity for such studies, small containers of saturated salt solutions can be placed in an IMC ampoule along with a non-hydrated mineral specimen. The ampoule is then sealed and introduced into an IMC instrument. The saturated salt solution keeps the air in the ampoule at a known rH, and various common salt solutions provide humidities ranging from e.g. 32-100% rH. Such studies have been performed on μm size range calcium hydroxyapatite particles and calcium-containing bioactive glass \"nano\" particles (Doostmohammadi et al. 2011).\n\nIMC is well suited for rapidly quantifying the rates of slow changes in materials (Willson et al. 1995). Such evaluations are variously described as studies of stability, degradation or shelf life. \nFor example, IMC has been widely used for many years in shelf life studies of solid drug formulations in the pharmaceutical industry (Pikal et al. 1989, Hansen et al. 1990, Konigbauer et al. 1992.) IMC has the ability to detect slow degradation during simulated shelf storage far sooner than conventional analytical methods and without the need to employ chemical assay techniques. IMC is also a rapid, sensitive method for determining the often functionally crucial amorphous content of drugs such as nifedipine (Vivoda et al. 2011).\n\nIMC can be used for rapidly determining the rate of slow changes in industrial polymers. For example, gamma radiation sterilization of a material frequently used for surgical implants—ultra-high-molecular-weight polyethylene (UHMWPE)—is known to produce free radicals in the polymer. The result is slow oxidation and gradual undesirable embrittlement of the polymer on the shelf or in vivo. IMC could detect oxidation-related heat and quantified an oxidation rate of ca. 1% per year in irradiated UHMWPE at room temperature in air (Charlebois et al. 2003). In a related study the activation energy was determined from measurements at a series of temperatures (Hardison et al. 2003).\n\nIMC is also of great utility in evaluating the \"runaway potential\" of materials which are significant fire or explosion hazards. For example, it has been used to determine autocatalytic kinetics of cumene hydroperoxide (CHP), an intermediate which is used in the chemical industry and whose sudden decomposition has caused a number of fires and explosions. Fig. 4 Shows the IMC data documenting thermal decomposition of CHP at 5 different temperatures (Chen et al. 2008).\n\nThe term metabolismics can be used to describe studies of the quantitative measurement of the rate at which heat is produced or consumed vs. time by cells (including microbes) in culture, by tissue specimens, or by small whole organisms. As described subsequently, metabolismics can be useful as a diagnostic tool; especially in either (a) identifying the nature of a specimen from its heat flow vs. time signature under a given set of conditions, or (b) determining the effects of e.g. pharmaceutical compounds on metabolic processes, organic growth or viability. Metabolismics is related to metabolomics. The latter is the systematic study of the unique chemical fingerprints that specific cellular processes leave behind; i.e. the study of their small-molecule metabolite profiles. When IMC is used to determine metabolismics, the products of the metabolic processes studied are subsequently available for metabolomics studies. Since IMC does not employ biochemical or radioactive markers, the post-IMC specimens consist only of metabolic products and remaining culture medium (if any was used). If metabolismics and metabolomics are used together, they can provide a comprehensive record of a metabolic process taking place in vitro: its rate and energetics, and its metabolic products.\n\nTo determine metabolismics using IMC, there must of course be sufficient cells, tissue or organisms initially present (or present later if replication is taking place during IMC measurements) to generate a heat flow signal above a given instrument's detection limit. A landmark 2002 general paper on the topic of metabolism provides an excellent perspective from which to consider IMC metabolismic studies (see Bibliography, West, Woodruff and Brown 2002). It describes how metabolic rates are related and how they scale over the entire range from \"molecules and mitochondria to cells and mammals\". Importantly for IMC, the authors also note that while the metabolic rate of a given type of mammalian cell in vivo declines markedly with increasing animal size (mass), the size of the donor animal has no effect on the metabolic rate of the cell when cultured in vitro.\n\nMammalian cells in culture have a metabolic rate of ca. 30×10 W/cell (Figs. 2 and 3 in Bibliography: West, Woodruff and Brown 2002). By definition, IMC instruments have a sensitivity of at least 1×10 W (i.e. 1 μ W). Therefore, the metabolic heat of ca. 33,000 cells is detectable. Based on this sensitivity, IMC was used to perform a large number of pioneering studies of cultured mammalian cell metabolismics in the 1970s and 1980s in Sweden. One paper (Monti 1990) serves as an extensive guide to work done up until 1990. It includes explanatory text and 42 references to IMC studies of heat flow from cultured human erythrocytes, platelets, lymphocytes, lymphoma cells, granulocytes, adipocytes, skeletal muscle and myocardial tissue. The studies were done to determine how and where IMC might be used as a clinical diagnostic method and/or provide insights into metabolic differences between cells from healthy persons and persons with various diseases or health problems.\n\nDevelopments since ca. 2000 in IMC (e.g. massively parallel instruments, real-time, computer-based storage and analysis of heat flow data) have stimulated further use of IMC in cultured cell biology. For example, IMC has been evaluated for assessing antigen-induced lymphocyte proliferation (Murigande et al. 2009) and revealed aspects of proliferation not seen using a conventional non-continuous radioactive marker assay method. IMC has also been applied to the field of tissue engineering. One study (Santoro et al. 2011) demonstrated that IMC could be used to measure the growth (i.e. proliferation) rate in culture of human chondrocytes harvested for tissue engineering use. It showed that IMC can potentially serve to determine the effectiveness of different growth media formulations and also determine whether cells donated by a given individual can be grown efficiently enough to consider using them to produce engineered tissue.\n\nIMC has also been used to measure the metabolic response of cultured macrophages to surgical implant wear debris. IMC showed that the response was stronger to μm size range particles of polyethylene than to similarly sized Co alloy particles (Charlebois et al. 2002). A related paper covers the general topic of applying IMC in the field of synthetic solid materials used in surgery and medicine (Lewis and Daniels 2003)\".\n\nAt least two studies have suggested IMC can be of substantial use in tumor pathology. In one study (Bäckman 1990), the heat production rate of T-lymphoma cells cultured in suspension was measured. Changes in temperature and pH induced significant variations, but stirring rate and cell concentration did not. A more direct study of possible diagnostic use (Kallerhoff et al. 1996) produced promising results. For the uro-genital tissue biopsy specimens studied, the results showed\n\nAs of this writing (2012) IMC has not become widely used in cultured cell toxicology even though it has been used periodically and successfully since the 1980s. IMC is advantageous in toxicology when it is desirable to observe cultured cell metabolism in real time and to quantify the rate of metabolic decline as a function of the concentration of a possibly toxic agent. One of the earliest reports (Ankerst et al. 1986) of IMC use in toxicology was a study of antibody-dependent cellular toxicity (ADCC) against human melanoma cells of various combinations of antiserum, monoclonal antibodies and also peripheral blood lymphocytes as effector cells. Kinetics of melanoma cell metabolic heat flow vs. time in closed ampoules were measured for 20 hours. The authors concluded that \n\nIMC is also being used in environmental toxicology. In an early study (Thorén 1992) toxicity against monolayers of alveolar macrophages of particles of MnO, TiO and SiO (silica) were evaluated. IMC results were in accord with results obtained by fluorescein ester staining and microscopic image analysis—except that IMC showed toxic effects of quartz not discernable by image analysis. This latter observation—in accord with known alveolar effects—indicated to the authors that IMC was a more sensitive technique.\n\nMuch more recently (Liu et al. 2007), IMC has been shown to provide dynamic metabolic data which assess toxicity against fibroblasts of Cr(VI) from potassium chromate. Fig. 5 shows baseline results determining the metabolic heat flow from cultured fibroblasts prior to assessing the effects of Cr(VI). The authors concluded that \n\nSimple closed ampoule IMC has also been used and advocated for assessing the cultured cell toxicity of candidate surgical implant materials—and thus serve as a biocompatibility screening method. In one study (Xie et al. 2000) porcine renal tubular cells in culture were exposed to both polymers and titanium metal in the form of \"microplates\" having known surface areas of a few cm. The authors concluded that IMC \n\nIn another implant materials study (Doostmohammadi et al. 2011) both a rapidly growing yeast culture and a human chondrocyte culture were exposed to particles (diam.<nowiki>«/nowiki> 50 μm) of calcium hydroxyapatite (HA) and bioactive (calcium-containing) silica glass. The glass particles slowed or curtailed yeast growth as a function of increasing particle concentration. The HA particles had much less effect and never entirely curtailed yeast growth at the same concentrations. The effects of both particle types on chondrocyte growth were minimal at the concentration employed. The authors concluded that \n\"The cytotoxicity of particulate materials such as bioactive glass and hydroxyapatite particles can be evaluated using the microcalorimetry method. This is a modern method for in vitro study of biomaterials biocompatibility and cytotoxicity which can be used alongside the old conventional assays.\"\n\nPublications describing use of IMC in microbiology began in the 1980s (Jesperson 1982). While some IMC microbiology studies have been directed at viruses (Heng et al. 2005) and fungi (Antoci et al. 1997), most have been concerned with bacteria. A recent paper (Braissant et al. 2010) provides a general introduction to IMC metabolismic methods in microbiology and an overview of applications in medical and environmental microbiology. The paper also explains how heat flow vs. time data for bacteria in culture are an exact expression—as they occur over time—of the fluctuations in microorganism metabolic activity and replication rates in a given medium (Fig. 6).\n\nIn general, bacteria are about 1/10 the size of mammalian cells and produce perhaps 1/10 as much metabolic heat-i.e. ca. 3x10 W/cell. Thus, compared to mammalian cells (see above) ca. 10X as many bacteria—ca. 330,000—must be present to produce detectable heat flow—i.e. 1 μW. However, many bacteria replicate orders of magnitude more rapidly in culture than mammalian cells, often doubling their number in a matter of minutes (see Bacterial growth). As a result, a small initial number of bacteria in culture and initially undetectable by IMC rapidly produce a detectable number. For example, 100 bacteria doubling every 20 minutes will in less than 4 hours produce <nowiki»</nowiki>330,000 bacteria and thus an IMC-detectable heat flow. Consequently, IMC can be used for easy, rapid detection of bacteria in the medical field. Examples include detection of bacteria in human blood platelet products (Trampuz et al. 2007) and urine (Bonkat \"et al.\" 2011) and rapid detection of tuberculosis (Braissant et al. 2010, Rodriguez et al. 2011). Fig. 7 shows an example of detection times of Tuberculosis bacteria as a function of the initial amount of bacteria present in a closed IMC ampoule containing a culture medium.\n\nFor microbes in growth media in closed ampoules, IMC heat flow data can also be used to closely estimate basic microbial growth parameters; i.e. maximum growth rate and duration time of the lag phase before maximum growth rate is achieved. This is an important special application of the basic analysis of these parameters explained previously (Overview (Data Obtained)).\n\n\"Unfortunately, the IMC literature contains some published papers in which the relation between heat flow data and microbial growth in closed ampoules has been misunderstood. However, in 2013 an extensive clarification was published, describing (a) details of the relation between IMC heat flow data and microbial growth, (b) selection of mathematical models which describe microbial growth and (c) determination of microbial growth parameters from IMC data using these models (Braissant et al. 2013).\"\n\nIn a logical extension of the ability of IMC to detect and quantify bacterial growth, known concentrations of antbiotics can be added to bacterial culture, and IMC can then be used to quantify their effects on viability and growth. Closed ampoule IMC can easily capture basic pharmacologic information—e.g. minimum inhibitory concentration (MIC) of an antibiotic needed to stop growth of a given organism. In addition it can simultaneously provide dynamic growth parameters—lag time and maximum growth rate (see Fig. 2, Howell et al. 2011, Braissant et al. 2013), which assess mechanisms of action. Bactericidal action (see Bactericide) is indicated by an increased lag time as a function of increasing antibiotic concentration, while bacteriostatic action (see Bacteriostatic agent) is indicated by a decrease in growth rate with concentration. The IMC approach to antibiotic assessment has been demonstrated for a number of a types of bacteria and antibiotics (von Ah et al. 2009). Closed ampoule IMC can also rapidly differentiate between normal and resistant strains of bacteria such as \"Staphylococcus aureus\" (von Ah et al. 2008, Baldoni et al. 2009). IMC has also been used to assess the effects of disinfectants on the viability of mouth bacteria adhered to dental implant materials (Astasov-Frauenhoffer et al. 2011). In a related earlier study, IMC was used to measure the heat of adhesion of dental bacteria to glass (Hauser- Gerspach et al. 2008).\n\nAnalogous successful use of IMC to determine the effects of antitumor drugs on tumor cells in culture within a few hours has been demonstrated (Schön and Wadsö 1988). Rather than the closed-ampoule approach, an IMC setup was used which allowed drug injection into stirred specimens.\n\nAs of this writing (2013), IMC has been used less widely in mammalian cell in vitro pharmacodynamic studies than in microbial studies.\n\nIt is possible to use IMC to perform metabolismic studies of living multicellular organisms—if they are small enough to be placed in IMC ampoules (Lamprecht & Becker 1988). IMC studies have been made of insect pupa metabolism during ventilating movements (Harak et al. 1996) and effects of chemical agents on pupal growth (Kuusik et al. 1995). IMC has also proved effective in assessing the effects of aging on nematode worm metabolism (Braekman et al. 2002).\n\nIMC has also proved highly useful for in vitro assessments of the effects of pharmaceuticals on tropical parasitic worms (Manneck et al. 2011-1, Maneck et al. 2011-2, Kirchhofer et al. 2011). An interesting feature of these studies is the use of a simple manual injection system for introducing the pharmaceuticals into sealed ampoules containing the worms. Also, IMC not only documents the general metabolic decline over time due to the drugs, but also the overall frequency of worm motor activity and its decline in amplitude over time as reflected in fluctuations in the heat flow data.\n\nBecause of its versatility, IMC can be an effective tool in the fields of plant and environmental biology. In an early study (Hansen et al. 1989), the metabolic rate of larch tree clone tissue specimens was measured. The rate was predictive of long-term tree growth rates, was consistent for specimens from a given tree and was found to correlate with known variations in the long-term growth of clones from different trees.\n\nBacterial oxalotrophic metabolism is common in the environment, particularly in soils. Oxalotrophic bacteria are capable of using oxalate as a sole carbon and energy source. Closed-ampoule IMC was used to study metabolism of oxalotrophic soil bacteria exposed to both an optimized medium containing potassium oxalate as the sole carbon source and a model soil (Bravo et al. 2011). Using an optimized medium, growth of six different strains of soil bacteria was easily monitored and reproducibly quantified and differentiated over a period days. IMC measurement of bacterial metabolic heat flow in the model soil was more difficult, but a proof of concept was demonstrated.\n\nMoonmilk is a white, creamy material found in caves. It is a non-hardening, fine crystalline precipitate from limestone and is composed mainly of calcium and/or magnesium carbonates. Microbes may be involved in its formation. It is difficult to infer microbial activities in moonmilk from standard static chemical and microscopic assays of moonmilk composition and structure. Closed ampoule IMC has been used to solve this problem (Braissant, Bindscheidler et al. 2011). It was possible to determine the growth rates of chemoheterotrophic microbial communities on moonmilk after the addition of various carbon sources simulating mixes that would be brought into contact with moonmilk due to snow melt or rainfall. Metabolic activity was high and comparable to that found in some soils.\n\nHarris \"et al.\" (2012), studying differing fertilizer input regimes, found that, when expressed as heat output per unit soil microbial biomass, microbial communities under organic fertilizer regimes produced less waste heat than those under inorganic regimes.\n\nIMC has been shown to have diverse uses in food science and technology. An overview (Wadsö and Galindo 2009) discusses successful applications in assessing vegetable cutting wound respiration, cell death from blanching, milk fermentation, microbiological spoilage prevention, thermal treatment and shelf life. Another publication (Galindo et al. 2005) reviews the successful use of IMC for monitoring and predicting quality changes during storage of minimally processed fruits and vegetables.\n\nIMC has also proven effective in accomplishing enzymatic assays for orotic acid in milk (Anastasi et al. 2000) and malic acid in fruits, wines and other beverages and also cosmetic products (Antonelli et al. 2008). IMC has also been used to assess the efficacy of anti-browning agents on fresh- cut potatoes (Rocculi et al. 2007). IMC has also proven effective in assessing the extent to which low-energy pulsed electric fields (PEFs) affect the heat of germination of barley seeds—important in connection with their use in producing malted beverages (Dymek et al. 2012).\n\n\n\n",
    "id": "35215882",
    "title": "Isothermal microcalorimetry"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=21426616",
    "text": "Proofreading (biology)\n\nThe term proofreading is used in genetics to refer to the error-correcting processes, first proposed by John Hopfield and Jacques Ninio, involved in DNA replication, immune system specificity, enzyme-substrate recognition among many other processes that require enhanced specificity. The proofreading mechanisms of Hopfield and Ninio are non-equilibrium active processes that consume ATP to enhance specificity of various biochemical reactions.\n\nIn bacteria, all three DNA polymerases (I, II and III) have the ability to proofread, using 3’ → 5’ exonuclease activity. When an incorrect base pair is recognized, DNA polymerase reverses its direction by one base pair of DNA and excises the mismatched base. Following base excision, the polymerase can re-insert the correct base and replication can continue.\n\nIn eukaryotes only the polymerases that deal with the elongation (delta and epsilon) have proofreading ability (3’ → 5’ exonuclease activity).\n\nProofreading also occurs in mRNA translation for \"protein\" synthesis. In this case, one mechanism is release of any incorrect aminoacyl-tRNA before peptide bond formation.\n\nThe extent of proofreading in DNA replication determines the mutation rate, and is different in different species.\nFor example, loss of proofreading due to mutations in the DNA polymerase epsilon gene results in a hyper-mutated genotype with >100 mutations per Mbase of DNA in human colorectal cancers.\n\nThe extent of proofreading in other molecular processes can depend on the effective population size of the species and the number of genes affected by the same proofreading mechanism.\n\n",
    "id": "21426616",
    "title": "Proofreading (biology)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=33329683",
    "text": "Spontaneous absolute asymmetric synthesis\n\nSpontaneous absolute asymmetric synthesis is a chemical phenomenon that \"stochastically\" generates chirality based on autocatalysis and small fluctuations in the ratio of enantiomers present in a racemic mixture. In certain reactions which initially do not contain chiral information, stochastically distributed enantiomeric excess can be observed. The phenomenon is different from chiral amplification, where enantiomeric excess is present from the beginning and not stochastically distributed. Hence, when the experiment is repeated many times, the average enantiomeric excess approaches 0%. The phenomenon has important implications concerning the origin of homochirality in nature.\n",
    "id": "33329683",
    "title": "Spontaneous absolute asymmetric synthesis"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=56566",
    "text": "Chronobiology\n\nChronobiology is a field of biology that examines periodic (cyclic) phenomena in living organisms and their adaptation to solar- and lunar-related rhythms. These cycles are known as biological rhythms. Chronobiology comes from the ancient Greek χρόνος (\"chrónos\", meaning \"time\"), and biology, which pertains to the study, or science, of life. The related terms \"chronomics\" and \"chronome\" have been used in some cases to describe either the molecular mechanisms involved in chronobiological phenomena or the more quantitative aspects of chronobiology, particularly where comparison of cycles between organisms is required.\n\nChronobiological studies include but are not limited to comparative anatomy, physiology, genetics, molecular biology and behavior of organisms within biological rhythms mechanics. Other aspects include epigenetics, development, reproduction, ecology and evolution.\n\nThe variations of the timing and duration of biological activity in living organisms occur for many essential biological processes. These occur (a) in animals (eating, sleeping, mating, hibernating, migration, cellular regeneration, etc.), (b) in plants (leaf movements, photosynthetic reactions, etc.), and in microbial organisms such as fungi and protozoa. They have even been found in bacteria, especially among the cyanobacteria (aka blue-green algae, see bacterial circadian rhythms). The most important rhythm in chronobiology is the circadian rhythm, a roughly 24-hour cycle shown by physiological processes in all these organisms. The term \"circadian\" comes from the Latin \"circa\", meaning \"around\" and \"dies\", \"day\", meaning \"approximately a day.\" It is regulated by circadian clocks.\n\nThe circadian rhythm can further be broken down into routine cycles during the 24-hour day:\n\nWhile circadian rhythms are defined as endogenously regulated, other biological cycles may be regulated by exogenous signals. In some cases, multi-trophic systems may exhibit rhythms driven by the circadian clock of one of the members (which may also be influenced or reset by external factors). The endogenous plant cycles may regulate the activity of the bacterium by controlling availability of plant-produced photosynthate.\n\nMany other important cycles are also studied, including:\n\nWithin each cycle, the time period during which the process is more active is called the \"acrophase\". When the process is less active, the cycle is in its \"bathyphase\" or \"trough\" phase. The particular moment of highest activity is the \"peak\" or \"maximum\"; the lowest point is the \"nadir\". How high (or low) the process gets is measured by the \"amplitude\".\n\nA circadian cycle was first observed in the 18th century in the movement of plant leaves by the French scientist Jean-Jacques d'Ortous de Mairan. In 1751 Swedish botanist and naturalist Carl Linnaeus (Carl von Linné) designed a flower clock using certain species of flowering plants. By arranging the selected species in a circular pattern, he designed a clock that indicated the time of day by the flowers that were open at each given hour. For example, among members of the daisy family, he used the hawk's beard plant which opened its flowers at 6:30 am and the hawkbit which did not open its flowers until 7 am.\n\nThe 1960 symposium at Cold Spring Harbor Laboratory laid the groundwork for the field of chronobiology.\n\nIt was also in 1960 that Patricia DeCoursey invented the phase response curve, one of the major tools used in the field since.\n\nFranz Halberg of the University of Minnesota, who coined the word \"circadian\", is widely considered the \"father of American chronobiology.\" However, it was Colin Pittendrigh and not Halberg who was elected to lead the \"Society for Research in Biological Rhythms\" in the 1970s. Halberg wanted more emphasis on the human and medical issues while Pittendrigh had his background more in evolution and ecology. With Pittendrigh as leader, the Society members did basic research on all types of organisms, plants as well as animals. More recently it has been difficult to get funding for such research on any other organisms than mice, rats, humans and fruit flies.\n\nMore recently, light therapy and melatonin administration have been explored by Alfred J. Lewy (OHSU), Josephine Arendt (University of Surrey, UK) and other researchers as a means to reset animal and human circadian rhythms. Additionally, the presence of low-level light at night accelerates circadian re-entrainment of hamsters of all ages by 50%; this is thought to be related to simulation of moonlight.\n\nHumans can be morning people or evening people; these variations are called chronotypes for which there are various assessment tools and biological markers.\n\nIn the second half of 20th century, substantial contributions and formalizations have been made by Europeans such as Jürgen Aschoff and Colin Pittendrigh, who pursued different but complementary views on the phenomenon of entrainment of the circadian system by light (parametric, continuous, tonic, gradual vs. nonparametric, discrete, phasic, instantaneous, respectively).\n\nThere is also a food-entrainable biological clock, which is not confined to the suprachiasmatic nucleus. The location of this clock has been disputed. Working with mice, however, Fuller \"et al.\" concluded that the food-entrainable clock seems to be located in the dorsomedial hypothalamus. During restricted feeding, it takes over control of such functions as activity timing, increasing the chances of the animal successfully locating food resources.\n\nChronobiology is an interdisciplinary field of investigation. It interacts with medical and other research fields such as sleep medicine, endocrinology, geriatrics, sports medicine, space medicine and photoperiodism.\n\nIn spite of the similarity of the name to legitimate biological rhythms, the theory and practice of biorhythms is a classic example of pseudoscience. It attempts to describe a set of cyclic variations in human behavior based on a person's birth date. It is not a part of chronobiology.\n\n\n",
    "id": "56566",
    "title": "Chronobiology"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=13653300",
    "text": "Kinetic proofreading\n\nKinetic proofreading (or kinetic amplification) is a mechanism for error correction in biochemical reactions, proposed independently by John Hopfield (1974) and Jacques Ninio (1975). Kinetic proofreading allows enzymes to discriminate between two possible reaction pathways leading to correct or incorrect products with an accuracy higher than what one would predict based on the difference in the activation energy between these two pathways.\n\nIncreased specificity is obtained by introducing an irreversible step exiting the pathway, with reaction intermediates leading to incorrect products more likely to prematurely exit the pathway than reaction intermediates leading to the correct product. If the exit step is fast relative to the next step in the pathway, the specificity can be increased by a factor of up to the ratio between the two exit rate constants. (If the next step is fast relative to the exit step, specificity will not be increased because there will not be enough time for exit to occur.) This can be repeated more than once to increase specificity further.\n\nIn protein synthesis, the error rate is on the order of 1 in 10,000. This means that when a ribosome is matching anticodons of tRNA to the codons of mRNA, it matches complementary sequences correctly nearly all the time. Hopfield noted that because of how similar the substrates are (the difference between a wrong codon and a right codon can be as small as a difference in a single base), an error rate that small is unachievable with a one-step mechanism. Both wrong and right tRNA can bind to the ribosome, and if the ribosome can only discriminate between them by complementary matching of the anticodon, it must rely on the small free energy difference between binding three matched complementary bases or only two.\n\nA one-shot machine which tests whether the codons match or not by examining whether the codon and anticodon are bound will not be able to tell the difference between wrong and right codon with an error rate less than formula_1 unless the free energy difference is at least 10kT, which is much larger than the free energy difference for single codon binding. This is a thermodynamic bound, so it cannot be evaded by building a different machine. However, this can be overcome by kinetic proofreading, which introduces an irreversible step through the input of energy.\n\nAnother molecular recognition mechanism, which does \"not\" require expenditure of free energy is that of conformational proofreading. The incorrect product may also be formed but hydrolyzed at a greater rate than the correct product, giving the possibility of theoretically infinite specificity the longer you let this reaction run, but at the cost of large amounts of the correct product as well. (Thus there is a tradeoff between product production and its efficiency.) The hydrolytic activity may be on the same enzyme, as in DNA polymerases with editing functions, or on different enzymes.\n\nHopfield suggested a simple way to achieve smaller error rates using a molecular ratchet which takes many irreversible steps, each testing to see if the sequences match. At each step, energy is expended and specificity (the ratio of correct substrate to incorrect substrate at that point in the pathway) increases.\n\nThe requirement for energy in each step of the ratchet is due to the need for the steps to be irreversible; for specificity to increase, entry of substrate and analogue must occur largely through the entry pathway, and exit largely through the exit pathway. If entry were an equilibrium, the earlier steps would form a pre-equilibrium and the specificity benefits of entry into the pathway (less likely for the substrate analogue) would be lost; if the exit step were an equilibrium, then the substrate analogue would be able to re-enter the pathway through the exit step, bypassing the specificity of earlier steps altogether.\n\nAlthough one test will only be able to discriminate between mismatched and matched sequences a fraction formula_2 of the time, two tests will both fail only formula_3 of the time, and N tests will fail formula_4 of the time. In terms of free energy, the discrimination power of N successive tests for two states with a free energy formula_5 is the same as one test between two states with a free energy formula_6.\n\nTo achieve an error rate of formula_1 requires several comparison steps. Hopfield predicted on the basis of this theory that there is a multistage ratchet in the ribosome which tests the match several times before incorporating the next amino acid into the protein.\n\n\nBiochemical processes that use kinetic proofreading to improve specificity implement the delay-inducing multistep ratchet by a variety of distinct biochemical networks. Nonetheless, many such networks result in the times to completion of the molecular assembly and the proofreading steps (also known as the first passage time) that approach a near-universal, exponential shape for high proofreading rates and large network sizes. Since exponential completion times are characteristic of a two-state Markov process, this observation makes kinetic proofreading one of only a few examples of biochemical processes where structural complexity results in a much simpler large-scale, phenomenological dynamics.\n\nThe increase in specificity, or the overall amplification factor of a kinetic proofreading network that may include multiple pathways and especially loops is intimately related to the topology of the network: the specificity grows exponentially with the number of loops in the network. An example is homologous recombination in which the number of loops scales like the square of DNA length. The universal completion time emerges precisely in this regime of large number of loops and high amplification.\n\n",
    "id": "13653300",
    "title": "Kinetic proofreading"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=28734201",
    "text": "Transcellular transport\n\nTranscellular transport involves the transportation of solutes by a cell \"through\" a cell. One classic example is the movement of glucose from the intestinal lumen to extracellular fluid by epithelial cells.\n\nEpithelial cells use primary and secondary active transport, often in conjunction with passive diffusion through ion channels, to produce transcellular transport across epithelial tissues. This transport can either be absorption, transport from lumen (apical membrane surface) to blood, or secretion, transport from blood (basolateral membrane surface) to lumen.\n\nThe transcellular pathway of transport is important in the intestinal absorption of drug molecules, the other being the paracellular pathway. The transcellular pathway of transport include transcellular diffusion, active carrier mediated transportation, and transcytosis. The transcelluar diffusion simply involves the movement of solutes based on a diffusion gradient moving from an area of high concentration to an area of low concentration, however, the cell membrane is a hydrophobic environment and will not allow the passive diffusion of charged, hydrophilic, or zwitterion molecules. Active transport involves the use of energy to transport specific substrates across barriers, even against the concentration gradient. Macromolecules can sometimes be transported through transcytosis.\n\nIn contrast, paracellular transport is the transfer of substances across an epithelium by passing through an intercellular space between the cells.\n",
    "id": "28734201",
    "title": "Transcellular transport"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=8694335",
    "text": "Biocommunication (science)\n\nIn the study of the biological sciences, biocommunication is any specific type of communication within (intraspecific) or between (interspecific) species of plants, animals, fungi, protozoa and microorganisms. Communication basically means sign-mediated interactions following three levels of (syntactic, pragmatic and semantic) rules. Signs in most cases are chemical molecules (semiochemicals), but also tactile, or as in animals also visual and auditive. Biocommunication of animals may include vocalizations (as between competing bird species), or pheromone production (as between various species of insects), chemical signals between plants and animals (as in tannin production used by vascular plants to warn away insects), and chemically mediated communication between plants and within plants. \n\nBiocommunication of fungi demonstrates that mycelia communication integrates interspecific sign-mediated interactions between fungal organisms soil bacteria and plant root cells without which plant nutrition could not be organized. Biocommunication of Archaea represents keylevels of sign-mediated interactions in the evolutionarily oldest akaryotes.\n\nBiocommunication theory may be considered to be a branch of biosemiotics. Whereas Biosemiotics studies the production and interpretation of signs and codes, biocommunication theory investigates concrete interactions mediated by signs. Accordingly, syntactic, semantic, and pragmatic aspects of biocommunication processes are distinguished. Biocommunication specific to animals (animal communication) is considered a branch of zoosemiotics. The semiotic study of molecular genetics, can be considered a study of biocommunication at its most basic level. \n\nGiven the complexity and range of biological organisms and the further complexity within the neural organization of any particular animal organism, there is a variety of biocommunication languages.\n\nA hierarchy of biocommunication languages in animals has been proposed by Subhash Kak: these languages, in order of increasing generality, are associative, re-organizational, and quantum. The three types of formal languages of the Chomsky hierarchy map into the associative language class.\n\n",
    "id": "8694335",
    "title": "Biocommunication (science)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=123418",
    "text": "Transdifferentiation\n\nTransdifferentiation, also known as lineage reprogramming, is a process in which one mature somatic cell transforms into another mature somatic cell without undergoing an intermediate pluripotent state or progenitor cell type. It is a type of metaplasia, which includes all cell fate switches, including the interconversion of stem cells. Current uses of transdifferentiation include disease modeling and drug discovery and in the future may include gene therapy and regenerative medicine. The term 'transdifferentiation' was originally coined by Selman and Kafatos in 1974 to describe a change in cell properties as cuticle producing cells became salt-secreting cells in silk moths undergoing metamorphosis.\n\nDavis et al. 1987 reported the first instance of transdifferentiation where a cell changed from one adult cell type to another. Forcing mouse embryonic fibroblasts to express MyoD was found to be sufficient to turn those cells into myoblasts.\n\nThere are no known instances where adult cells change directly from one lineage to another except \"Turritopsis dohrnii\". Rather, cells dedifferentiate and then redifferentiate into the cell type of interest. In newts when the eye lens is removed, pigmented epithelial cells de-differentiate and then redifferentiate into the lens cells.\nIn the pancreas, it has been demonstrated that alpha cells can spontaneously switch fate and transdifferentiate into beta cells in both healthy and diabetic human and mouse pancreatic islets.\nWhile it was previously believed that oesophageal cells were developed from the transdifferentiation of smooth muscle cells, that has been shown to be false.\n\nThe first example of functional transdifferentiation has been provided by Ferber et al. by induce a shift in the developmental fate of cells in liver and convert them into 'pancreatic beta-cell-like' cells. The cells induced a wide, functional and long-lasting transdifferentiation process that reduced the effects of hyperglycemia in diabetic mice. Moreover, the trans-differentiated beta-like cells were found to be resistant to the autoimmune attack that characterizes type 1 diabetes.\n\nThe second step was to undergo transdifferentiation in human specimens. By transducing liver cells with a single gene, Sapir et al. were able to induce human liver cells to transdifferentiate into human beta cells.\n\nThis approach has been demonstrated in mice, rat, xenopus and human tissues (Al-Hasani et al., 2013).\n\nSchematic model of the hepatocyte-to-beta cell transdifferentiation process. Hepatocytes are obtained by liver biopsy from diabetic patient, cultured and expanded ex vivo, transduced with a PDX1 virus, transdifferentiated into functional insulin-producing beta cells, and transplanted back into the patient.\n\nIn this approach, transcription factors from progenitor cells of the target cell type are transfected into a somatic cell to induce transdifferentiation. There exists two different means of determining which transcription factors to use: by starting with a large pool and narrowing down factors one by one or by starting with one or two and adding more. One theory to explain the exact specifics is that ectopic TFs directs the cell to an earlier progenitor state and then redirects it towards a new cell type. Rearrangement of the chromatin structure via DNA methylation or histone modification may play a role as well. Here is a list of in vitro examples and in vivo examples. In vivo methods of transfecting specific mouse cells utilize the same kinds of vectors as in vitro experiments, except that the vector is injected into a specific organ. Zhou et al. (2008) injected Ngn3, Pdx1 and Mafa into the dorsal splenic lobe (pancreas) of mice to reprogram pancreatic exocrine cells into β-cells in order to ameliorate hyperglycaemia.\n\nSomatic cells are first transfected with pluripotent reprogramming factors temporarily (Oct4, Sox2, Nanog, etc.) before being transfected with the desired inhibitory or activating factors. Here is a list of \nexamples in vitro.\n\nThe DNA methylation inhibitor, 5-azacytidine is also known to promote phenotypic transdifferentiation of cardiac cells to skeletal myoblasts.\n\nThe transcription factors serve as a short term trigger to an irreversible process. The transdifferentiation liver cells observed 8 months after one single injection of pdx1.\n\nThe ectopic transcription factors turn off the host repertoire of gene expression in each of the cells. However, the alternate desired repertoire is being turned on only in a subpopulation of predisposed cells. Despite the massive dedifferentiation – lineage tracing approach indeed demonstrates that transdifferentiation originates in adult cells.\n\nDetermining the unique set of cellular factors that is needed to be manipulated for each cell conversion is a long and costly process that involved much trial and error. As a result, this first step of identifying the key set of cellular factors for cell conversion is the major obstacle researchers face in the field of cell reprogramming. An international team of researchers have developed an algorithm, called Mogrify(1), that can predict the optimal set of cellular factors required to convert one human cell type to another.\nWhen tested, Mogrify was able to accurately predict the set of cellular factors required for previously published cell conversions correctly. To further validate Mogrify's predictive ability, the team conducted two novel cell conversions in the laboratory using human cells, and these were successful in both attempts solely using the predictions of Mogrify. Mogrify has been made available online for other researchers and scientists.\n\nWhen examining transdifferentiated cells, it is important to look for markers of the target cell type and the absence of donor cell markers which can be accomplished using green fluorescent protein or immunodetection. It is also important to examine the cell function, epigenome, transcriptome, and proteome profiles. Cells can also be evaluated based upon their ability to integrate into the corresponding tissue in vivo and functionally replace its natural counterpart. In one study, transdifferentiating tail-tip fibroblasts into hepatocyte-like cells using transcription factors Gata4, Hnf1α and Foxa3, and inactivation of p19(Arf) restored hepatocyte-like liver functions in only half of the mice using survival as a means of evaluation.\n\nGenerally transdifferentiation that occurs in mouse cells does not translate in effectiveness or speediness in human cells. Pang et al. found that while transcription factors Ascl1, Brn2 and Myt1l turned mouse cells into mature neurons, the same set of factors only turned human cells into immature neurons. However, the addition of NeuroD1 was able to increase efficiency and help cells reach maturity.\n\nThe order of expression of transcription factors can direct the fate of the cell. Iwasaki et al. (2006) showed that in hematopoietic lineages, the expression timing of Gata-2 and (C/EBPalpha) can change whether or not a lymphoid-committed progenitors can differentiate into granulocyte/monocyte progenitor, eosinophil, basophil or bipotent basophil/mast cell progenitor lineages.\n\nIt has been found for induced pluripotent stem cells that when injected into mice, the immune system of the synergeic mouse rejected the teratomas forming. Part of this may be because the immune system recognized epigenetic markers of specific sequences of the injected cells. However, when embryonic stem cells were injected, the immune response was much lower. Whether or not this will occur within transdifferentiated cells remains to be researched.\n\nIn order to accomplish transfection, one may use integrating viral vectors such as lentiviruses or retroviruses, non-integrating vectors such as Sendai viruses or adenoviruses, microRNAs and a variety of other methods including using proteins and plasmids; one example is the non-viral delivery of transcription factor-encoding plasmids with a polymeric carrier to elicit neuronal transdifferentiation of fibroblasts. When foreign molecules enter cells, one must take into account the possible drawbacks and potential to cause tumorous growth. Integrating viral vectors have the chance to cause mutations when inserted into the genome. One method of going around this is to excise the viral vector once reprogramming has occurred, an example being Cre-Lox recombination Non-integrating vectors have other issues concerning efficiency of reprogramming and also the removal of the vector. Other methods are relatively new fields and much remains to be discovered.\n\n\n",
    "id": "123418",
    "title": "Transdifferentiation"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=18801936",
    "text": "The Chemical Basis of Morphogenesis\n\n\"The Chemical Basis of Morphogenesis\" is an article written by the English mathematician Alan Turing in 1952 describing the way in which natural patterns such as stripes, spots and spirals may arise naturally out of a homogeneous, uniform state. The theory, which can be called a reaction–diffusion theory of morphogenesis, has served as a basic model in theoretical biology.\n\nReaction–diffusion systems have attracted much interest as a prototype model for pattern formation. Patterns such as fronts, spirals, targets, hexagons, stripes and dissipative solitons are found in various types of reaction-diffusion systems in spite of large discrepancies e.g. in the local reaction terms.\nReaction-diffusion processes form one class of explanation for the embryonic development of animal coats and skin pigmentation. Another reason for the interest in reaction-diffusion systems is that although they represent nonlinear partial differential equations, there are often possibilities for an analytical treatment.\n\n",
    "id": "18801936",
    "title": "The Chemical Basis of Morphogenesis"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=34969847",
    "text": "Field cancerization\n\nField cancerization (also termed field change, field change cancerization, field carcinogenesis, cancer field effect or premalignant field defect) is a biological process in which large areas of cells at a tissue surface or within an organ are affected by a carcinogenic alteration(s). The process arises from exposure to an injurious environment, often over a lengthy period.\n\nThe initial step in field cancerization is associated with various molecular lesions such as acquired genetic mutations and epigenetic changes, occurring over a widespread, multi-focal \"field\". These initial molecular changes may subsequently progress to cytologically recognizable premalignant foci of dysplasia, and eventually to carcinoma in situ (CIS) or cancer. The image of a longitudinally opened colon resection on this page shows an area of a colon resection that likely has a field cancerization or field defect. It has one cancer and four premalignant polyps.\nField cancerization can occur in any tissue. Prominent examples of field cancerization include premalignant field defects in head and neck cancer, lung cancer, colorectal cancer, Barrett's esophagus, skin, breast ducts and bladder. Field cancerization has implications for cancer surveillance and treatment. Despite adequate resection and being histologically normal, the remaining locoregional tissue has an increased risk for developing multiple independent cancers, either synchronously or metachronously.\n\nA common carcinogenic alteration, found in many cancers and in their adjacent field defects from which the cancers likely arose, is reduced expression of one or more DNA repair enzymes. Since reduced DNA repair expression is often present in a field cancerization or a field defect, it is likely to have been an early step in progression to the cancer.\n\nField defects associated with gastrointestinal tract cancers also commonly displayed reduced apoptosis competence, aberrant proliferation and genomic instability. Field defects of the gastrointestinal tract that show those common faults occurred in the oropharynx, esophagus, stomach, bile duct, pancreas, small intestine and colon/rectum.\n\nThe field defect adjacent to a colon cancer consists of the inner surface of the colon (the epithelium) that has about 1 million crypts (indentations in the surface of the epithelium). Each crypt has about 5,000 cells in the shape of a test-tube and all 5,000 cells of the crypt are generated from the few stem cells at the base of the crypt. The stem cells at the base of the crypt can undergo \"crypt conversion\" where a stem cell with a selective advantage takes over the stem cell niche, and all cells of that crypt display consistent expression (high or low) of a protein being evaluated.\n\nThe diagram shows results obtained by Facista et al. A particular colon resection from a colon cancer patient was evaluated for expression of 3 different DNA repair enzymes: KU86 (active in the non-homologous end joining pathway), ERCC1 (active in the nucleotide excision DNA repair pathway) and PMS2 (active in the mismatch DNA repair pathway). The percent of crypts in 6 tissue samples taken within the field defect were evaluated for frequency of high levels of expression of each of the repair proteins. Almost every crypt in all tissue samples from this patient showed high expression of KU86. However, the majority of crypts in all 6 tissue samples were reduced or absent in protein expression of ERCC1 and PMS2. The crypts with reduced or absent expression of ERCC1 or PMS2 usually occurred in large patches of adjacent crypts. Both ERCC1 and PMS2, in these tissue samples, were thought to be deficient due to epigenetic alterations.\n",
    "id": "34969847",
    "title": "Field cancerization"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=26805",
    "text": "Sex\n\nOrganisms of many species are specialized into male and female varieties, each known as a sex, with some falling in between being intersex. Sexual reproduction involves the combining and mixing of genetic traits: specialized cells known as gametes combine to form offspring that inherit traits from each parent. Gametes can be identical in form and function (known as isogamy), but in many cases an asymmetry has evolved such that two sex-specific types of gametes (heterogametes) exist (known as anisogamy).\n\nAmong humans and other mammals, males typically carry XY chromosomes, whereas females typically carry XX chromosomes, which are a part of the XY sex-determination system. Other animals have a sex-determination system as well, such as the ZW sex-determination system in birds, and the X0 sex-determination system in insects.\n\nThe gametes produced by an organism are determined by its sex: males produce male gametes (spermatozoa, or sperm, in animals; pollen in plants) while females produce female gametes (ova, or egg cells); individual organisms which produce both male and female gametes are termed hermaphroditic. Frequently, physical differences are associated with the different sexes of an organism; these sexual dimorphisms can reflect the different reproductive pressures the sexes experience. For instance, mate choice and sexual selection can accelerate the evolution of physical differences between the sexes.\n\nOne of the basic properties of life is reproduction, the capacity to generate new individuals, and sex is an aspect of this process. Life has evolved from simple stages to more complex ones, and so have the reproduction mechanisms. Initially the reproduction was a replicating process that consists in producing new individuals that contain the same genetic information as the original or parent individual. This mode of reproduction is called \"asexual\", and it is still used by many species, particularly unicellular, but it is also very common in multicellular organisms. In sexual reproduction, the genetic material of the offspring comes from two different individuals. As sexual reproduction developed by way of a long process of evolution, intermediates exist. Bacteria, for instance, reproduce asexually, but undergo a process by which a part of the genetic material of an individual (donor) is transferred to an other (recipient).\n\nDisregarding intermediates, the basic distinction between asexual and sexual reproduction is the way in which the genetic material is processed. Typically, prior to an asexual division, a cell duplicates its genetic information content, and then divides. This process of cell division is called mitosis. In sexual reproduction, there are special kinds of cells that divide without prior duplication of its genetic material, in a process named meiosis. The resulting cells are called gametes, and contain only half the genetic material of the parent cells. These gametes are the cells that are prepared for the sexual reproduction of the organism. Sex comprises the arrangements that enable sexual reproduction, and has evolved alongside the reproduction system, starting with similar gametes (isogamy) and progressing to systems that have different gamete types, such as those involving a large female gamete (ovum) and a small male gamete (sperm).\n\nIn complex organisms, the sex organs are the parts that are involved in the production and exchange of gametes in sexual reproduction. Many species, particularly animals, have sexual specialization, and their populations are divided into male and female individuals. Conversely, there are also species in which there is no sexual specialization, and the same individuals both contain masculine and feminine reproductive organs, and they are called hermaphrodites. This is very frequent in plants.\n\nSexual reproduction first probably evolved about a billion years ago within ancestral single-celled eukaryotes. The reason for the evolution of sex, and the reason(s) it has survived to the present, are still matters of debate. Some of the many plausible theories include: that sex creates variation among offspring, sex helps in the spread of advantageous traits, that sex helps in the removal of disadvantageous traits, and that sex facilitates repair of germ-line DNA.\n\nSexual reproduction is a process specific to eukaryotes, organisms whose cells contain a nucleus and mitochondria. In addition to animals, plants, and fungi, other eukaryotes (e.g. the malaria parasite) also engage in sexual reproduction. Some bacteria use conjugation to transfer genetic material between cells; while not the same as sexual reproduction, this also results in the mixture of genetic traits.\n\nThe defining characteristic of sexual reproduction in eukaryotes is the difference between the gametes and the binary nature of fertilization. Multiplicity of gamete types within a species would still be considered a form of sexual reproduction. However, no third gamete is known in multicellular animals.\n\nWhile the evolution of sex dates to the prokaryote or early eukaryote stage, the origin of chromosomal sex determination may have been fairly early in eukaryotes (see Evolution of anisogamy). The ZW sex-determination system is shared by birds, some fish and some crustaceans. XY sex determination is used by most mammals, but also some insects, and plants (\"Silene latifolia\"). X0 sex-determination is found in certain insects.\n\nNo genes are shared between the avian ZW and mammal XY chromosomes, and from a comparison between chicken and human, the Z chromosome appeared similar to the autosomal chromosome 9 in human, rather than X or Y, suggesting that the ZW and XY sex-determination systems do not share an origin, but that the sex chromosomes are derived from autosomal chromosomes of the common ancestor of birds and mammals.\nA paper from 2004 compared the chicken Z chromosome with platypus X chromosomes and suggested that the two systems are related.\n\nSexual reproduction in eukaryotes is a process whereby organisms form offspring that combine genetic traits from both parents. Chromosomes are passed on from one generation to the next in this process. Each cell in the offspring has half the chromosomes of the mother and half of the father.\nGenetic traits are contained within the deoxyribonucleic acid (DNA) of chromosomes—by combining one of each type of chromosomes from each parent, an organism is formed containing a doubled set of chromosomes. This double-chromosome stage is called \"diploid\", while the single-chromosome stage is \"haploid\". Diploid organisms can, in turn, form haploid cells (gametes) that randomly contain one of each of the chromosome pairs, via meiosis. Meiosis also involves a stage of chromosomal crossover, in which regions of DNA are exchanged between matched types of chromosomes, to form a new pair of mixed chromosomes. Crossing over and fertilization (the recombining of single sets of chromosomes to make a new diploid) result in the new organism containing a different set of genetic traits from either parent.\n\nIn many organisms, the haploid stage has been reduced to just gametes specialized to recombine and form a new diploid organism; in others, the gametes are capable of undergoing cell division to produce multicellular haploid organisms. In either case, gametes may be externally similar, particularly in size (isogamy), or may have evolved an asymmetry such that the gametes are different in size and other aspects (anisogamy).\nBy convention, the larger gamete (called an ovum, or egg cell) is considered female, while the smaller gamete (called a spermatozoon, or sperm cell) is considered male. An individual that produces exclusively large gametes is female, and one that produces exclusively small gametes is male. An individual that produces both types of gametes is a hermaphrodite; in some cases hermaphrodites are able to self-fertilize and produce offspring on their own, without a second organism.\n\nMost sexually reproducing animals spend their lives as diploid organisms, with the haploid stage reduced to single cell gametes. The gametes of animals have male and female forms—spermatozoa and egg cells. These gametes combine to form embryos which develop into a new organism.\n\nThe male gamete, a spermatozoon (produced within a testicle), is a small cell containing a single long flagellum which propels it.\nSpermatozoa are extremely reduced cells, lacking many cellular components that would be necessary for embryonic development. They are specialized for motility, seeking out an egg cell and fusing with it in a process called fertilization.\n\nFemale gametes are egg cells (produced within ovaries), large immobile cells that contain the nutrients and cellular components necessary for a developing embryo.\nEgg cells are often associated with other cells which support the development of the embryo, forming an egg. In mammals, the fertilized embryo instead develops within the female, receiving nutrition directly from its mother.\n\nAnimals are usually mobile and seek out a partner of the opposite sex for mating. Animals which live in the water can mate using external fertilization, where the eggs and sperm are released into and combine within the surrounding water. Most animals that live outside of water, however, must transfer sperm from male to female to achieve internal fertilization.\n\nIn most birds, both excretion and reproduction is done through a single posterior opening, called the cloaca—male and female birds touch cloaca to transfer sperm, a process called \"cloacal kissing\". In many other terrestrial animals, males use specialized sex organs to assist the transport of sperm—these male sex organs are called intromittent organs. In humans and other mammals this male organ is the penis, which enters the female reproductive tract (called the vagina) to achieve insemination—a process called sexual intercourse. The penis contains a tube through which semen (a fluid containing sperm) travels. In female mammals the vagina connects with the uterus, an organ which directly supports the development of a fertilized embryo within (a process called gestation).\n\nBecause of their motility, animal sexual behavior can involve coercive sex. Traumatic insemination, for example, is used by some insect species to inseminate females through a wound in the abdominal cavity—a process detrimental to the female's health.\n\nLike animals, plants have developed specialized male and female gametes. Within seed plants, male gametes are contained within hard coats, forming pollen. The female gametes of plants are contained within ovules; once fertilized by pollen these form seeds which, like eggs, contain the nutrients necessary for the development of the embryonic plant.\n\nMany plants have flowers and these are the sexual organs of those plants. Flowers are usually hermaphroditic, producing both male and female gametes. The female parts, in the center of a flower, are the pistils, each unit consisting of a carpel, a style and a stigma. One or more of these reproductive units may be merged to form a single compound pistil. Within the carpels are ovules which develop into seeds after fertilization. The male parts of the flower are the stamens: these consist of long filaments arranged between the pistil and the petals that produce pollen in anthers at their tips. When a pollen grain lands upon the stigma on top of a carpel's style, it germinates to produce a pollen tube that grows down through the tissues of the style into the carpel, where it delivers male gamete nuclei to fertilize an ovule that eventually develops into a seed.\n\nIn pines and other conifers the sex organs are conifer cones and have male and female forms. The more familiar female cones are typically more durable, containing ovules within them. Male cones are smaller and produce pollen which is transported by wind to land in female cones. As with flowers, seeds form within the female cone after pollination.\n\nBecause plants are immobile, they depend upon passive methods for transporting pollen grains to other plants. Many plants, including conifers and grasses, produce lightweight pollen which is carried by wind to neighboring plants. Other plants have heavier, sticky pollen that is specialized for transportation by insects. The plants attract these insects or larger animals such as humming birds and bats with nectar-containing flowers. These animals transport the pollen as they move to other flowers, which also contain female reproductive organs, resulting in pollination.\n\nMost fungi reproduce sexually, having both a haploid and diploid stage in their life cycles. These fungi are typically isogamous, lacking male and female specialization: haploid fungi grow into contact with each other and then fuse their cells. In some of these cases the fusion is asymmetric, and the cell which donates only a nucleus (and not accompanying cellular material) could arguably be considered \"male\".\n\nSome fungi, including baker's yeast, have mating types that create a duality similar to male and female roles. Yeast with the same mating type will not fuse with each other to form diploid cells, only with yeast carrying the other mating type.\n\nFungi produce mushrooms as part of their sexual reproduction. Within the mushroom diploid cells are formed, later dividing into haploid spores—the height of the mushroom aids the dispersal of these sexually produced offspring.\n\nThe most basic sexual system is one in which all organisms are hermaphrodites, producing both male and female gametes— this is true of some animals (e.g. snails) and the majority of flowering plants. In many cases, however, specialization of sex has evolved such that some organisms produce only male or only female gametes. The biological cause for an organism developing into one sex or the other is called sex determination.\n\nIn the majority of species with sex specialization, organisms are either male (producing only male gametes) or female (producing only female gametes). Exceptions are common—for example, the roundworm \"C. elegans\" has an hermaphrodite and a male sex (a system called androdioecy).\n\nSometimes an organism's development is intermediate between male and female, a condition called intersex. Sometimes intersex individuals are called \"hermaphrodite\"; but, unlike biological hermaphrodites, intersex individuals are unusual cases and are not typically fertile in both male and female aspects.\n\nIn genetic sex-determination systems, an organism's sex is determined by the genome it inherits. Genetic sex-determination usually depends on asymmetrically inherited sex chromosomes which carry genetic features that influence development; sex may be determined either by the presence of a sex chromosome or by how many the organism has. Genetic sex-determination, because it is determined by chromosome assortment, usually results in a 1:1 ratio of male and female offspring.\n\nHumans and other mammals have an XY sex-determination system: the Y chromosome carries factors responsible for triggering male development. The \"default sex,\" in the absence of a Y chromosome, is female-like. Thus, XX mammals are female and XY are male. In humans, biological sex is determined by five factors present at birth: the presence or absence of a Y chromosome (which alone determines the individual's \"genetic sex\"), the type of gonads, the sex hormones, the internal reproductive anatomy (such as the uterus in females), and the external genitalia.\n\nXY sex determination is found in other organisms, including the common fruit fly and some plants. In some cases, including in the fruit fly, it is the number of X chromosomes that determines sex rather than the presence of a Y chromosome (see below).\n\nIn birds, which have a ZW sex-determination system, the opposite is true: the W chromosome carries factors responsible for female development, and default development is male. In this case ZZ individuals are male and ZW are female. The majority of butterflies and moths also have a ZW sex-determination system. In both XY and ZW sex determination systems, the sex chromosome carrying the critical factors is often significantly smaller, carrying little more than the genes necessary for triggering the development of a given sex.\n\nMany insects use a sex determination system based on the number of sex chromosomes. This is called X0 sex-determination—the 0 indicates the absence of the sex chromosome. All other chromosomes in these organisms are diploid, but organisms may inherit one or two X chromosomes. In field crickets, for example, insects with a single X chromosome develop as male, while those with two develop as female. In the nematode \"C. elegans\" most worms are self-fertilizing XX hermaphrodites, but occasionally abnormalities in chromosome inheritance regularly give rise to individuals with only one X chromosome—these X0 individuals are fertile males (and half their offspring are male).\n\nOther insects, including honey bees and ants, use a haplodiploid sex-determination system. In this case diploid individuals are generally female, and haploid individuals (which develop from unfertilized eggs) are male. This sex-determination system results in highly biased sex ratios, as the sex of offspring is determined by fertilization rather than the assortment of chromosomes during meiosis.\n\nFor many species, sex is not determined by inherited traits, but instead by environmental factors experienced during development or later in life. Many reptiles have temperature-dependent sex determination: the temperature embryos experience during their development determines the sex of the organism. In some turtles, for example, males are produced at lower incubation temperatures than females; this difference in critical temperatures can be as little as 1–2 °C.\n\nMany fish change sex over the course of their lifespan, a phenomenon called sequential hermaphroditism. In clownfish, smaller fish are male, and the dominant and largest fish in a group becomes female. In many wrasses the opposite is true—most fish are initially female and become male when they reach a certain size. Sequential hermaphrodites may produce both types of gametes over the course of their lifetime, but at any given point they are either female or male.\n\nIn some ferns the default sex is hermaphrodite, but ferns which grow in soil that has previously supported hermaphrodites are influenced by residual hormones to instead develop as male.\n\nMany animals and some plants have differences between the male and female sexes in size and appearance, a phenomenon called sexual dimorphism. Sex differences in humans include, generally, a larger size and more body hair in men; women have breasts, wider hips, and a higher body fat percentage. In other species, the differences may be more extreme, such as differences in coloration or bodyweight.\n\nSexual dimorphisms in animals are often associated with sexual selection – the competition between individuals of one sex to mate with the opposite sex. Antlers in male deer, for example, are used in combat between males to win reproductive access to female deer. In many cases the male of a species is larger than the female. Mammal species with extreme sexual size dimorphism tend to have highly polygynous mating systems—presumably due to selection for success in competition with other males—such as the elephant seals. Other examples demonstrate that it is the preference of females that drive sexual dimorphism, such as in the case of the stalk-eyed fly.\n\nOther animals, including most insects and many fish, have larger females. This may be associated with the cost of producing egg cells, which requires more nutrition than producing sperm—larger females are able to produce more eggs. For example, female southern black widow spiders are typically twice as long as the males. Occasionally this dimorphism is extreme, with males reduced to living as parasites dependent on the female, such as in the anglerfish. Some plant species also exhibit dimorphism in which the females are significantly larger than the males, such as in the moss \"Dicranum\" and the liverwort \"Sphaerocarpos\". There is some evidence that, in these genera, the dimorphism may be tied to a sex chromosome, or to chemical signalling from females.\n\nIn birds, males often have a more colourful appearance and may have features (like the long tail of male peacocks) that would seem to put the organism at a disadvantage (e.g. bright colors would seem to make a bird more visible to predators). One proposed explanation for this is the handicap principle. This hypothesis says that, by demonstrating he can survive with such handicaps, the male is advertising his genetic fitness to females—traits that will benefit daughters as well, who will not be encumbered with such handicaps.\n\n\n",
    "id": "26805",
    "title": "Sex"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=47772789",
    "text": "Biotin hydrazide\n\nBiotin hydrazide is a biotinyl derivative that can be used as a probe for the determination of protein carbonylation. It readily forms Schiff bases with carbonyl groups.\n",
    "id": "47772789",
    "title": "Biotin hydrazide"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=5652450",
    "text": "Biological process\n\nBiological processes are the processes vital for a living organism to live. Biological processes are made up of many chemical reactions or other events that are involved in the persistence and transformation of life forms. Metabolism and homeostasis are examples.\n\nRegulation of biological processes occurs when any process is modulated in its frequency, rate or extent. Biological processes are regulated by many means; examples include the control of gene expression, protein modification or interaction with a protein or substrate molecule.\n\n\nhttp://geneontology.org/page/biological-process-ontology-guidelines\n",
    "id": "5652450",
    "title": "Biological process"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=379303",
    "text": "Gas exchange\n\nGas exchange is the biological process by which gases move passively by diffusion across a surface. Typically, this surface is - or contains - a biological membrane that forms the boundary between an organism and its extracellular environment.\n\nGases are constantly consumed and produced by cellular and metabolic reactions in most living things, so an efficient system for gas exchange between, ultimately, the interior of the cell(s) and the external environment is required. Small, particularly unicellular organisms, such as bacteria and protozoa, have a high surface-area to volume ratio. In these creatures the gas exchange membrane is typically the cell membrane. Some small multicellular organisms, such as flatworms, are also able to perform sufficient gas exchange across the skin or cuticle that surrounds their bodies. However, in most larger organisms, which have a small surface-area to volume ratios, specialised structures with convoluted surfaces such as gills, pulmonary alveoli and spongy mesophyll provide the large area needed for effective gas exchange. These convoluted surfaces may sometimes be internalised into the body of the organism. This is the case with the alveoli, which form the inner surface of the mammalian lung, the spongy mesophyll, which is found inside the leaves of some kinds of plant, or the gills of those molluscs that have them, which are found in the mantle cavity.\n\nIn aerobic organisms, gas exchange is particularly important for respiration, which involves the uptake of oxygen () and release of carbon dioxide (). Conversely, in oxygenic photosynthetic organisms such as most land plants, uptake of carbon dioxide and release of both oxygen and water vapour are the main gas-exchange processes occurring during the day. Other gas-exchange processes are important in less familiar organisms: \"e.g.\" carbon dioxide, methane and hydrogen are exchanged across the cell membrane of methanogenic archaea. In nitrogen fixation by diazotrophic bacteria, and denitrification by heterotrophic bacteria (such as \"Paracoccus denitrificans\" and various pseudomonads), nitrogen gas is exchanged with the environment, being taken up by the former and released into it by the latter, while giant tube worms rely on bacteria to oxidize hydrogen sulfide extracted from their deep sea environment, using dissolved oxygen in the water as an electron acceptor.\n\nThe exchange of gases occurs as a result of diffusion down a concentration gradient. Gas molecules move from a region in which they are at high concentration to one in which they are at low concentration. Diffusion is a passive process, meaning that no energy is required to power the transport, and it follows Fick’s Law: \n\nIn relation to a typical biological system, where two compartments ('inside' and 'outside'), are separated by a membrane barrier, and where a gas is allowed to spontaneously diffuse down its concentration gradient:\n\n\nFig. 1. Fick's Law for gas-exchange surface\n\nGases must first dissolve in a liquid in order to diffuse across a membrane, so all biological gas exchange systems require a moist environment. In general, the higher the concentration gradient across the gas-exchanging surface, the faster the rate of diffusion across it. Conversely, the thinner the gas-exchanging surface (for the same concentration difference), the faster the gases will diffuse across it.\n\nIn the equation above, \"J\" is the flux expressed per unit area, so increasing the area will make no difference to its value. However, an increase in the available surface area, will increase the \"amount\" of gas that can diffuse in a given time. This is because the amount of gas diffusing per unit time (d\"q\"/d\"t\") is the product of \"J\" and the area of the gas-exchanging surface, \"A\":\n\nSingle-celled organisms such as bacteria and amoebae do not have specialised gas exchange surfaces, because they can take advantage of the high surface area they have relative to their volume. The amount of gas an organism produces (or requires) in a given time will be in rough proportion to the volume of its cytoplasm. The volume of a unicellular organism is very small, therefore it produces (and requires) a relatively small amount of gas in a given time. In comparison to this small volume, the surface area of its cell membrane is very large, and adequate for its gas-exchange needs without further modification. However, as an organism increases in size, its surface area and volume do not scale in the same way. Consider an imaginary organism that is a cube of side-length, \"L\". Its volume increases with the cube (\"L\") of its length, but its external surface area increases only with the square (\"L\") of its length. This means the external surface rapidly becomes inadequate for the rapidly increasing gas-exchange needs of a larger volume of cytoplasm. Additionally, the thickness of the surface that gases must cross (d\"x\" in Fick's Law) can also be larger in larger organisms: in the case of a single-celled organism, a typical cell membrane is only 10 nm thick; but in larger organisms such as roundworms (Nematoda) the equivalent exchange surface - the cuticle - is substantially thicker at 0.5 µm.\n\nIn multicellular organisms therefore, specialised respiratory organs such as gills or lungs are often used to provide the additional surface area for the required rate of gas exchange with the external environment. However the distances between the gas exchanger and the deeper tissues are often too great for diffusion to meet gaseous requirements of these tissues. The gas exchangers are therefore frequently coupled to gas-distributing circulatory systems, which transport the gases evenly to all the body tissues regardless of their distance from the gas exchanger.\n\nSome multicellular organisms such as flatworms (Platyhelminthes) are relatively large but very thin, allowing their outer body surface to act as a gas exchange surface without the need for a specialised gas exchange organ. Flatworms therefore lack gills or lungs, and also lack a circulatory system. Other multicellular organisms such as sponges (Porifera) have an inherently high surface area, because they are very porous and/or branched. Sponges do not require a circulatory system or specialised gas exchange organs, because their feeding strategy involves one-way pumping of water through their porous bodies using flagellated collar cells. Each cell of the sponge's body is therefore exposed to a constant flow of fresh oxygenated water. They can therefore rely on diffusion across their cell membranes to carry out the gas exchange needed for respiration.\n\nIn organisms that have circulatory systems associated with their specialized gas-exchange surfaces, a great variety of systems are used for the interaction between the two.\n\nIn a countercurrent flow system, air (or, more usually, the water containing dissolved air) is drawn in the \"opposite\" direction to the flow of blood in the gas exchanger. A countercurrent system such as this maintains a steep concentration gradient along the length of the gas-exchange surface (see lower diagram in Fig. 2). This is the situation seen in the gills of fish and many other aquatic creatures. The gas-containing environmental water is drawn unidirectionally across the gas-exchange surface, with the blood-flow in the gill capillaries beneath flowing in the opposite direction. Although this theoretically allows almost complete transfer of a respiratory gas from one side of the exchanger to the other, in fish less than 80% of the oxygen in the water flowing over the gills is generally transferred to the blood.\n\nAlternative arrangements are cross current systems found in birds. and dead-end air-filled sac systems found in the lungs of mammals. In a cocurrent flow system, the blood and gas (or the fluid containing the gas) move in the same direction through the gas exchanger. This means the magnitude of the gradient is variable along the length of the gas-exchange surface, and the exchange will eventually stop when an equilibrium has been reached (see upper diagram in Fig. 2).\nCocurrent flow gas exchange systems are not known to be used in nature.\n\nThe gas exchanger in mammals is internalized to form lungs, as it is in most of the larger land animals. Gas exchange occurs in microscopic dead-end air-filled sacs called alveoli, where a very thin membrane (called the blood-air barrier) separates the blood in the alveolar capillaries (in the walls of the alveoli) from the alveolar air in the sacs.\n\nThe membrane across which gas exchange takes place in the alveoli (i.e. the blood-air barrier) is extremely thin (in humans, on average, 2.2 μm thick). It consists of the alveolar epithelial cells, their basement membranes and the endothelial cells of the pulmonary capillaries (Fig. 4). The large surface area of the membrane comes from the folding of the membrane into about 300 million alveoli, with diameters of approximately 75-300 µm each. This provides an extremely large surface area (approximately 145 m) across which gas exchange can occur.\n\nAir is brought to the alveoli in small doses (called the tidal volume), by breathing in (inhalation) and out (exhalation) through the respiratory airways, a set of relatively narrow and moderately long tubes which start at the nose or mouth and end in the alveoli of the lungs in the chest. Air moves in and out through the same set of tubes, in which the flow is in one direction during inhalation, and in the opposite direction during exhalation.\n\nDuring each inhalation, at rest, approximately 500 ml of fresh air flows in through the nose. Its is warmed and moistened as it flows through the nose and pharynx. By the time it reaches the trachea the inhaled air's temperature is 37 °C and it is saturated with water vapor. On arrival in the alveoli it is diluted and thoroughly mixed with the approximately 2.5–3.0 liters of air that remained in the alveoli after the last exhalation. This relatively large volume of air that is semi-permanently present in the alveoli throughout the breathing cycle is known as the functional residual capacity (FRC).\n\nAt the beginning of inhalation the airways are filled with unchanged alveolar air, left over from the last exhalation. This is the dead space volume, which is usually about 150 ml. It is the first air to re-enter the alveoli during inhalation. Only after the dead space air has returned to the alveoli does the remainder of the tidal volume (500 ml - 150 ml = 350 ml) enter the alveoli. The entry of such a small volume of fresh air with each inhalation, ensures that the composition of the FRC hardly changes during the breathing cycle (Fig. 5). The alveolar partial pressure of oxygen remains very close to 13–14 kPa (100 mmHg), and the partial pressure of carbon dioxide varies minimally around 5.3 kPa (40 mmHg) throughout the breathing cycle (of inhalation and exhalation). The corresponding partial pressures of oxygen and carbon dioxide in the ambient (dry) air at sea level are 21 kPa (160 mmHg) and 0.04 kPa (0.3 mmHg) respectively.\nThis alveolar air, which constitutes the FRC, completely surrounds the blood in the alveolar capillaries (Fig. 6). Gas exchange in mammals occurs between this alveolar air (which differs significantly from fresh air) and the blood in the alveolar capillaries. The gases on either side of the gas exchange membrane equilibrate by simple diffusion. This ensures that the partial pressures of oxygen and carbon dioxide in the blood leaving the alveolar capillaries, and ultimately circulates throughout the body, are the same as those in the FRC.\n\nThe marked difference between the composition of the alveolar air and that of the ambient air can be maintained because the functional residual capacity is contained in dead-end sacs connected to the outside air by long, narrow, tubes (the airways: nose, pharynx, larynx, trachea, bronchi and their branches and sub-branches down to the bronchioles). This anatomy, and the fact that the lungs are not emptied and re-inflated with each breath, provides mammals with a \"portable atmosphere\", whose composition differs significantly from the present-day ambient air.\n\nThe composition of the air in the FRC is carefully monitored, by measuring the partial pressures of oxygen and carbon dioxide in the arterial blood. If either gas pressure deviates from normal, reflexes are elicited that change the rate and depth of breathing in such a way that normality is restored within seconds or minutes.\n\nAll the blood returning from the body tissues to the right side of the heart flows through the alveolar capillaries before being pumped around the body again. On its passage through the lungs the blood comes into close contact with the alveolar air, separated from it by a very thin diffusion membrane which is only, on average, about 2 μm thick. The gas pressures in the blood will therefore rapidly equilibrate with those in the alveoli, ensuring that the arterial blood that circulates to all the tissues throughout the body has an oxygen tension of 13−14 kPa (100 mmHg), and a carbon dioxide tension of 5.3 kPa (40 mmHg). These arterial partial pressures of oxygen and carbon dioxide are homeostatically controlled. A rise in the arterial formula_3, and, to a lesser extent, a fall in the arterial formula_3, will reflexly cause deeper and faster breathing till the blood gas tensions return to normal. The converse happens when the carbon dioxide tension falls, or, again to a lesser extent, the oxygen tension rises: the rate and depth of breathing are reduced till blood gas normality is restored.\n\nSince the blood arriving in the alveolar capillaries has a formula_3 of, on average, 6 kPa (45 mmHg), while the pressure in the alveolar air is 13 kPa (100 mmHg), there will be a net diffusion of oxygen into the capillary blood, changing the composition of the 3 liters of alveolar air slightly. Similarly, since the blood arriving in the alveolar capillaries has a formula_3 of also about 6 kPa (45 mmHg), whereas that of the alveolar air is 5.3 kPa (40 mmHg), there is a net movement of carbon dioxide out of the capillaries into the alveoli. The changes brought about by these net flows of individual gases into and out of the functional residual capacity necessitate the replacement of about 15% of the alveolar air with ambient air every 5 seconds or so. This is very tightly controlled by the continuous monitoring of the arterial blood gas tensions (which accurately reflect partial pressures of the respiratory gases in the alveolar air) by the aortic bodies, the carotid bodies, and the blood gas and pH sensor on the anterior surface of the medulla oblongata in the brain. There are also oxygen and carbon dioxide sensors in the lungs, but they primarily determine the diameters of the bronchioles and pulmonary capillaries, and are therefore responsible for directing the flow of air and blood to different parts of the lungs.\n\nIt is only as a result of accurately maintaining the composition of the 3 liters alveolar air that with each breath some carbon dioxide is discharged into the atmosphere and some oxygen is taken up from the outside air. If more carbon dioxide than usual has been lost by a short period of hyperventilation, respiration will be slowed down or halted until the alveolar formula_3 has returned to 5.3 kPa (40 mmHg). It is therefore strictly speaking untrue that the primary function of the respiratory system is to rid the body of carbon dioxide “waste”. In fact the total concentration of carbon dioxide in arterial blood is about 26 mM (or 58 ml/100 ml), compared to the concentration of oxygen in saturated arterial blood of about 9 mM (or 20 ml/100 ml blood). This large concentration of carbon dioxide plays a pivotal role in the determination and maintenance of the pH of the extracellular fluids. The carbon dioxide that is breathed out with each breath could probably be more correctly be seen as a byproduct of the body’s extracellular fluid carbon dioxide and pH homeostats\n\nIf these homeostats are compromised, then a respiratory acidosis, or a respiratory alkalosis will occur. In the long run these can be compensated by renal adjustments to the H and HCO concentrations in the plasma; but since this takes time, the hyperventilation syndrome can, for instance, occur when agitation or anxiety cause a person to breathe fast and deeply thus blowing off too much CO from the blood into the outside air, precipitating a set of distressing symptoms which result from an excessively high pH of the extracellular fluids.\n\nOxygen has a very low solubility in water, and is therefore carried in the blood loosely combined with hemoglobin. The oxygen is held on the hemoglobin by four ferrous iron-containing heme groups per hemoglobin molecule. When all the heme groups carry one O molecule each the blood is said to be “saturated” with oxygen, and no further increase in the partial pressure of oxygen will meaningfully increase the oxygen concentration of the blood. Most of the carbon dioxide in the blood is carried as HCO ions in the plasma. However the conversion of dissolved CO into HCO (through the addition of water) is too slow for the rate at which the blood circulates through the tissues on the one hand, and alveolar capillaries on the other. The reaction is therefore catalyzed by carbonic anhydrase, an enzyme inside the red blood cells. The reaction can go in either direction depending on the prevailing partial pressure of carbon dioxide. A small amount of carbon dioxide is carried on the protein portion of the hemoglobin molecules as carbamino groups. The total concentration of carbon dioxide (in the form of bicarbonate ions, dissolved CO, and carbamino groups) in arterial blood (i.e. after it has equilibrated with the alveolar air) is about 26 mM (or 58 ml/100 ml), compared to the concentration of oxygen in saturated arterial blood of about 9 mM (or 20 ml/100 ml blood).\n\nThe dissolved oxygen content in fresh water is approximately 8–10 ml/liter compared to that of air which is 210 ml/liter. Water is 800 times more dense than air and 100 times more viscous. Therefore, oxygen has a diffusion rate in air 10,000 times greater than in water. The use of sac-like lungs to remove oxygen from water would therefore not be efficient enough to sustain life. Rather than using lungs, gaseous exchange takes place across the surface of highly vascularized gills. Gills are specialised organs containing filaments, which further divide into lamellae. The lamellae contain capillaries that provide a large surface area and short diffusion distances, as their walls are extremely thin. Gill rakers are found within the exchange system in order to filter out food, and keep the gills clean.\n\nGills use a countercurrent flow system that increases the efficiency of oxygen-uptake (and waste gas loss). Oxygenated water is drawn in through the mouth and passes over the gills in one direction while blood flows through the lamellae in the opposite direction. This countercurrent maintains steep concentration gradients along the entire length of each capillary (see the diagram in the \"Interaction with circulatory systems\" section above). Oxygen is able to continually diffuse down its gradient into the blood, and the carbon dioxide down its gradient into the water. The deoxygenated water will eventually pass out through the operculum (gill cover). Although countercurrent exchange systems theoretically allow an almost complete transfer of a respiratory gas from one side of the exchanger to the other, in fish less than 80% of the oxygen in the water flowing over the gills is generally transferred to the blood.\n\nAmphibians have three main organs involved in gas exchange: the lungs, the skin, and the gills, which can be used singly or in a variety of different combinations. The relative importance of these structures differs according to the age, the environment and species of the amphibian. The skin of amphibians and their larvae is highly vascularised, leading to relatively efficient gas exchange when the skin is moist. The larvae of amphibians, such as the pre-metamorphosis tadpole stage of frogs, also have external gills. The gills are absorbed into the body during metamorphosis, after which the lungs will then take over. The lungs are usually simpler than in the other land vertebrates, with few internal septa and larger alveoli; however, toads, which spend more time on land, have a larger alveolar surface with more developed lungs. To increase the rate of gas exchange by diffusion, amphibians maintain the concentration gradient across the respiratory surface using a process called buccal pumping. The lower floor of the mouth is moved in a \"pumping\" manner, which can be observed by the naked eye.\n\nAll reptiles breathe using lungs. In squamates (the lizards and snakes) ventilation is driven by the axial musculature, but this musculature is also used during movement, so some squamates rely on buccal pumping to maintain gas exchange efficiency.\n\nDue to the rigidity of turtle and tortoise shells, significant expansion and contraction of the chest is difficult. Turtles and tortoises depend on muscle layers attached to their shells, which wrap around their lungs to fill and empty them. Some aquatic turtles can also pump water into a highly vascularised mouth or cloaca to achieve gas-exchange.\n\nCrocodiles have a structure similar to the mammalian diaphragm - the diaphragmaticus - but this muscle helps create a unidirectional flow of air through the lungs rather than a tidal flow: this is more similar to the air-flow seen in birds than that seen in mammals. During inhalation, the diaphragmaticus pulls the liver back, inflating the lungs into the space this creates. Air flows into the lungs from the bronchus during inhalation, but during exhalation, air flows out of the lungs into the bronchus by a different route: this one-way movement of gas is achieved by aerodynamic valves in the airways.\n\nBirds have lungs but no diaphragm. They rely mostly on air sacs for ventilation. These air sacs do not play a direct role in gas exchange, but help to move air unidirectionally across the gas exchange surfaces in the lungs. During inhalation, fresh air is taken from the trachea down into the posterior air sacs and into the parabronchi which lead from the posterior air sacs into the lung. The air that enters the lungs joins the air which is already in the lungs, and is drawn forward across the gas exchanger into anterior air sacs. During exhalation, the posterior air sacs force air into the same parabronchi of the lungs, flowing in the same direction as during inhalation, allowing continuous gas exchange irrespective of the breathing cycle. Air exiting the lungs during exhalation joins the air being expelled from the anterior air sacs (both consisting of \"spent air\" that has passed through the gas exchanger) entering the trachea to be exhaled (Fig. 10). Selective bronchoconstriction at the various bronchial branch points ensures that the air does not ebb and flow through the bronchi during inhalation and exhalation, as it does in mammals, but follows the paths described above.\n\nThe unidirectional airflow through the parabronchi exchanges respiratory gases with a \"crosscurrent\" blood flow (Fig. 9). The partial pressure of O (formula_3) in the parabronchioles declines along their length as O diffuses into the blood. The capillaries leaving the exchanger near the entrance of airflow take up more O than capillaries leaving near the exit end of the parabronchi. When the contents of all capillaries mix, the final formula_3 of the mixed pulmonary venous blood is higher than that of the exhaled air, but lower than that of the inhaled air.\n\nGas exchange in plants is dominated by the roles of carbon dioxide, oxygen and water vapor. is the only carbon source for autotrophic growth by photosynthesis, and when a plant is actively photosynthesising in the light, it will be taking up carbon dioxide, and losing water vapor and oxygen. At night, plants respire, and gas exchange partly reverses: water vapor is still lost (but to a smaller extent), but oxygen is now taken up and carbon dioxide released.\nPlant gas exchange occurs mostly through the leaves. Gases diffuse into and out of the intercellular spaces within the leaf through pores called stomata, which are typically found on the lower surface of the leaf. Gases enter into the photosynthetic tissue of the leaf through dissolution onto the moist surface of the palisade and spongy mesophyll cells. The spongy mesophyll cells are loosely packed, allowing for an increased surface area, and subsequently an increased rate of gas-exchange. Uptake of carbon dioxide necessarily results in some loss of water vapor, because both molecules enter and leave by the same stomata, so plants experience a gas exchange dilemma: gaining enough without losing too much water. Therefore, water loss from other parts of the leaf is minimised by the waxy cuticle on the leaf's epidermis. The size of a stoma is regulated by the opening and closing of its two guard cells: the turgidity of these cells determines the state of the stomatal opening, and this itself is regulated by water stress. Plants showing crassulacean acid metabolism are drought-tolerant xerophytes and perform almost all their gas-exchange at night, because it is only during the night that these plants open their stomata. By opening the stomata only at night, the water vapor loss associated with carbon dioxide uptake is minimised. However, this comes at the cost of slow growth: the plant has to store the carbon dioxide in the form of malic acid for use during the day, and it cannot store unlimited amounts.\n\nGas exchange measurements are important tools in plant science: this typically involves sealing the plant (or part of a plant) in a chamber and measuring changes in the concentration of carbon dioxide with an infrared gas analyzer. If the environmental conditions (humidity, concentration, light and temperature) are fully controlled, the measurements of uptake and water release reveal important information about the assimilation and transpiration rates. The intercellular concentration reveals important information about the photosynthetic condition of the plants. Simpler methods can be used in specific circumstances: hydrogencarbonate indicator can be used to monitor the consumption of in a solution containing a single plant leaf at different levels of light intensity, and oxygen generation by the pondweed \"Elodea\" can be measured by simply collecting the gas in a submerged test-tube containing a small piece of the plant.\n\nThe mechanism of gas exchange in invertebrates depends their size, feeding strategy, and habitat (aquatic or terrestrial).\n\nThe sponges (Porifera) are sessile creatures, meaning they are unable to move on their own and normally remain attached to their substrate. They obtain nutrients through the flow of water across their cells, and they exchange gases by simple diffusion across their cell membranes. Pores called ostia draw water into the sponge and the water is subsequently circulated through the sponge by cells called choanocytes which have hair-like structures that move the water through the sponge.\n\nThe cnidarians include corals, sea anemones, jellyfish and hydras. These animals are always found in aquatic environments, ranging from fresh water to salt water. They do not have any dedicated respiratory organs; instead, every cell in their body can absorb oxygen from the surrounding water, and release waste gases to it. One key disadvantage of this feature is that cnidarians can die in environments where water is stagnant, as they deplete the water of its oxygen supply. Corals often form symbiosis with other organisms, particularly photosynthetic dinoflagellates. In this symbiosis, the coral provides shelter and the other organism provides nutrients to the coral, including oxygen.\n\nThe roundworms (Nematoda), flatworms (Platyhelminthes), and many other small invertebrate animals living in aquatic or otherwise wet habitats do not have a dedicated gas-exchange surface or circulatory system. They instead rely on diffusion of and directly across their cuticle. The cuticle is the semi-permeable outermost layer of their bodies.\n\nOther aquatic invertebrates such as most molluscs (Mollusca) and larger crustaceans (Crustacea) such as lobsters, have gills analogous to those of fish, which operate in a similar way.\nUnlike the invertebrates groups mentioned so far, insects are usually terrestrial, and exchange gases across a moist surface in direct contact with the atmosphere, rather than in contact with surrounding water. The insect's exoskeleton is impermeable to gases, including water vapor, so they have a more specialised gas exchange system, requiring gases to be directly transported to the tissues via a complex network of tubes. This respiratory system is separated from their circulatory system. Gases enter and leave the body through openings called spiracles, located laterally along the thorax and abdomen. Similar to plants, insects are able to control the opening and closing of these spiracles, but instead of relying on turgor pressure, they rely on muscle contractions. These contractions result in an insect's abdomen being pumped in and out. The spiracles are connected to tubes called tracheae, which branch repeatedly and ramify into the insect's body. These branches terminate in specialised tracheole cells which provides a thin, moist surface for efficient gas exchange, directly with cells.\n\nThe other main group of terrestrial arthropod, the arachnids (spiders, scorpion, mites, and their relatives) typically perform gas exchange with a book lung.\n\n",
    "id": "379303",
    "title": "Gas exchange"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=49223016",
    "text": "Electrolithoautotroph\n\nAn Electrolithoautotroph is an organism which feeds on electricity. These organisms use electricity to convert carbon dioxide to organic matters by using electrons directly taken from solid-inorganic electron donors. Electrolithoautotrophs are microorganisms which are found in the deep crevices of the ocean. The warm, mineral-rich environment provides a rich source of nutrients. The electron source for carbon assimilation from diffusible Fe(2+) ions to an electrode under the condition that electrical current is the only source of energy and electrons. Electrolithoautotrophs form a third metabolic pathway compared to photosynthesis (plants converting light into sugar) and chemosynthesis (animals consuming food)\n",
    "id": "49223016",
    "title": "Electrolithoautotroph"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=40861730",
    "text": "Replicate (biology)\n\nIn the biological sciences, a replicate is an exact copy of a sample that is being analyzed, such as a cell, organism or molecule, on which exactly the same procedure is done. This is often done in order to check for experimental or procedural error. In the absence of error replicates should yield the same result. However, replicates are not independent tests of the hypothesis because they are still the same sample, and so do not test for variation between samples.\n\nReplicates are often created to test the quality and repeatability of a procedure, or for a destructive procedure where preserving the original sample is desirable. They are also sometimes inappropriately used to inflate the apparent number of observations in a sample, creating an illusion of statistical significance.\n\n",
    "id": "40861730",
    "title": "Replicate (biology)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=17018118",
    "text": "Basic rest-activity cycle\n\nThe basic rest-activity cycle (BRAC) is a physiological arousal mechanism in humans proposed by Nathaniel Kleitman, hypothesized to occur during both sleep and wakefulness. \n\nEmpirically, it is a ultradian rhythm of approximately 90 minutes (80–120 minutes) that is characterized by different level of excitement and rest. The cycle is mediated by the human biological clock. It is most readily observed in stages of sleep, for example, rapid eye movement sleep (REM) and the delta activity cycle.\n\nWhen awake, our brainwaves are faster during the first half of the cycle, when we feel alert and focused, and then our brainwaves slow; in the last 20 minutes when we feel dreamy and perhaps a little tired, while our body is being readied for the alert part of the following cycle. \n\nWhen asleep, our brainwaves first slow and then rise, and the BRAC occurs as stages of sleep—first falling into deep sleep, then rising into the REM stage, when dreams occur.\n",
    "id": "17018118",
    "title": "Basic rest-activity cycle"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=53959022",
    "text": "Cyclomorphosis\n\nCyclomorphosis (also known as seasonal polyphenism) is the name given to the occurrence of cyclic or seasonal changes in the phenotype of an organism through successive generations.\n\nIt occurs in small aquatic invertebrates that reproduce by parthenogenesis and give rise to several generations annually. It occurs especially in marine planktonic animals, and is thought to be caused by the epigenetic effect of environmental cues on the organism, thereby altering the course of their development.\n",
    "id": "53959022",
    "title": "Cyclomorphosis"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=344974",
    "text": "Chemosynthesis\n\nIn biochemistry, chemosynthesis is the biological conversion of one or more carbon-containing molecules (usually carbon dioxide or methane) and nutrients into organic matter using the oxidation of inorganic compounds (e.g., hydrogen gas, hydrogen sulfide) or methane as a source of energy, rather than sunlight, as in photosynthesis. Chemoautotrophs, organisms that obtain carbon through chemosynthesis, are phylogenetically diverse, but also groups that include conspicuous or biogeochemically-important taxa include the sulfur-oxidizing gamma and epsilon proteobacteria, the aquificaeles, the methanogenic archaea and the neutrophilic iron-oxidizing bacteria.\n\nMany microorganisms in dark regions of the oceans use chemosynthesis to produce biomass from single carbon molecules. Two categories can be distinguished. In the rare sites at which hydrogen molecules (H) are available, the energy available from the reaction between CO and H (leading to production of methane, CH) can be large enough to drive the production of biomass. Alternatively, in most oceanic environments, energy for chemosynthesis derives from reactions in which substances such as hydrogen sulfide or ammonia are oxidized. This may occur with or without the presence of oxygen.\n\nMany chemosynthetic microorganisms are consumed by other organisms in the ocean, and symbiotic associations between chemosynthesizers and respiring heterotrophs are quite common. Large populations of animals can be supported by chemosynthetic secondary production at hydrothermal vents, methane clathrates, cold seeps, whale falls, and isolated cave water.\n\nIt has been hypothesized that chemosynthesis may support life below the surface of Mars, Jupiter's moon Europa, and other planets. Chemosynthesis may have also been the first type of metabolism that evolved on Earth, leading the way for cellular respiration and photosynthesis to develop later.\n\nGiant tube worms use bacteria in their trophosome to fix carbon dioxide (using hydrogen sulfide as an energy source) and produce sugars and amino acids.\nSome reactions produce sulfur:\nInstead of releasing oxygen gas as in photosynthesis, the process produces solid globules of sulfur. In bacteria capable of chemosynthesis, such as purple sulfur bacteria, yellow globules of sulfur are present and visible in the cytoplasm.\n\nIn 1890, Sergei Nikolaevich Vinogradskii (or Winogradsky) proposed a novel type of life process called \"anorgoxydant\". His discovery suggested that some microbes could live solely on inorganic matter and emerged during his physiological research in the 1880s in Strassburg and Zurich on sulfur, iron, and nitrogen bacteria.\n\nIn 1897, Wilhelm Pfeffer coined the term \"chemosynthesis\" for the energy production by oxidation of inorganic substances, in association with autotrophic carbon dioxide assimilation - what would be named today as chemolithoautotrophy. Later, the term would be expanded to include also chemoorganoautotrophs, which are organisms that use organic energy substrates in order to assimilate carbon dioxide. Thus, chemosynthesis can be seen as a synonym of chemoautotrophy.\n\nThe term \"chemotrophy\", less restrictive, would be introduced in the 1940s by André Lwoff for the production of energy by the oxidation of electron donors, organic or not, associated with auto- or heterotrophy.\n\nThe suggestion of Vinogradskii was confirmed nearly 90 years later, when hydrothermal ocean vents were predicted to exist in the 1970s. The hot springs and strange creatures were discovered by \"Alvin\", the world's first deep-sea submersible, in 1977 at the Galapagos Rift. At about the same time, Harvard graduate student Colleen Cavanaugh proposed chemosynthetic bacteria that oxidize sulfides or elemental sulfur as a mechanism by which tube worms could survive near hydrothermal vents. Cavanaugh later managed to confirm that this was indeed the method by which the worms could thrive, and is generally credited with the discovery of chemosynthesis.\n\nA 2004 television series hosted by Bill Nye named chemosynthesis as one of the 100 greatest scientific discoveries of all time.\n\nIn 2013, researchers reported their discovery of bacteria living in the rock of the oceanic crust below the thick layers of sediment, and apart from the hydrothermal vents that form along the edges of the tectonic plates. Preliminary findings are that these bacteria subsist on the hydrogen produced by chemical reduction of olivine by seawater circulating in the small veins that permeate the basalt that comprises oceanic crust. The bacteria synthesize methane by combining hydrogen and carbon dioxide.\n\n",
    "id": "344974",
    "title": "Chemosynthesis"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=23017175",
    "text": "Eidonomy\n\nEidonomy is the study of the external appearance of an organism. It is thus the opposite of anatomy, which refers to internal morphology.\nWhile predominant early in the history of biology it is little studied in particular anymore as it is ripe with the effects of convergent evolution. It thus yields less new information about organisms than anatomy, and therefore the external appearance of lifeforms is usually studied as part of general investigations in morphology, e.g. in the context of phylogenetic research.\n\n\n",
    "id": "23017175",
    "title": "Eidonomy"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=2355918",
    "text": "Ethnobiology\n\n]\n\nEthnobiology is the scientific study of the way living things are treated or used by different human cultures. It studies the dynamic relationships between people, biota, and environments, from the distant past to the immediate present.\n\n\"People-biota-environment\" interactions around the world are documented and studied through time, across cultures, and across disciplines in a search for valid, reliable answers to two 'defining' questions: \"How and in what ways do human societies use nature, and how and in what ways do human societies view nature?\"\n\nNaturalists have been interested in local biological knowledge since the time Europeans started colonising the world, from the 15th century onwards. Paul Sillitoe wrote that:\n\nLocal biological knowledge, collected and sampled over these early centuries significantly informed the early development of modern biology:\n\nEthnobiology itself, as a distinctive practice, only emerged during the 20th century as part of the records then being made about other peoples, and other cultures. As a practice, it was nearly always ancillary to other pursuits when documenting others' languages, folklore, and natural resource use. Roy Ellen commented that:\n\nThis 'first phase' in the development of ethnobiology as a practice has been described as still having an essentially utilitarian purpose, often focusing on identifying those 'native' plants, animals and technologies of some potential use and value within increasingly dominant western economic systems\n\nArising out of practices in Phase I (above) came a 'second phase' in the development of 'ethnobiology', with researchers now striving to better document and better understand how other peoples' themselves \"conceptualise and categorise\" the natural world around them. In Sillitoe's words:\n\nThis 'second' phase is marked:\n\nBy the turn of the 21st century ethnobiological practices, research, and findings have had a significant impact and influence across a number of fields of biological inquiry including ecology, conservation biology, development studies, and political ecology.\n\nThe Society of Ethnobiology advises on its web page:\nEthnobiology is a rapidly growing field of research, gaining professional, student, and public interest .. internationally\nEthnobiology has come out from its place as an ancillary practice in the shadows of other core pursuits, to arise as a whole field of inquiry and research in its own right: taught within many tertiary institutions and educational programmes around the world; with its own methods manuals, its own readers, and its own textbooks\n\nAll societies make use of the biological world in which they are situated, but there are wide differences in use, informed by perceived need, available technology, and the culture's sense of morality and sustainability. Ethnobiologists investigate what lifeforms are used for what purposes, the particular techniques of use, the reasons for these choices, and symbolic and spiritual implications of them.\n\nDifferent societies divide the living world up in different ways. Ethnobiologists attempt to record the words used in particular cultures for living things, from the most specific terms (analogous to species names in Linnean biology) to more general terms (such as 'tree' and even more generally 'plant'). They also try to understand the overall structure or hierarchy of the classification system (if there is one; there is ongoing debate as to whether there must always be an implied hierarchy.\n\nSocieties invest themselves and their world with meaning partly through their answers to questions like \"how did the world happen?\", \"how and why did people come to be?\", \"what are proper practices, and why?\", and \"what realities exist beyond or behind our physical experience?\" Understanding these elements of a societies' perspective is important to cultural research in general, and ethnobiologists investigate how a societies' view of the natural world informs and is informed by them.\n\nIn order to live effectively in a given place, a people needs to understand the particulars of their environment, and many traditional societies have complex and subtle understandings of the places in which they live. Ethnobiologists seek to share in these understandings, subject to ethical concerns regarding intellectual property and cultural appropriation.\n\nIn cross cultural ethnobiology research, two or more communities participate simultaneously. This enables the researcher to compare how a bio-resource is used by different communities.\n\nStudies and writings within ethnobiology draw upon research from fields including archaeology, geography, linguistics, systematics, population biology, ecology, cultural anthropology, ethnography, pharmacology, nutrition, conservation, and sustainable development.\n\nThrough much of the history of ethnobiology, its practitioners were primarily from dominant cultures, and the benefit of their work often accrued to the dominant culture, with little control or benefit invested in the indigenous peoples whose practice and knowledge they recorded.\n\nJust as many of those indigenous societies work to assert legitimate control over physical resources such as traditional lands or artistic and ritual objects, many work to assert legitimate control over their intellectual property.\n\nIn an age when the potential exists for large profits from the discovery of, for example, new food crops or medicinal plants, modern ethnobiologists must consider intellectual property rights, the need for informed consent, the potential for harm to informants, and their \"debt to the societies in which they work\".\n\nFurthermore, these questions must be considered not only in light of western industrialized nations' common understanding of ethics and law, but also in light of the ethical and legal standards of the societies from which the ethnobiologist draws information.\n\nParticipatory ethnobotanical lab and field research station to encourage the involvement of local youth in detailed nutrition methodological propagation and restoration of local threatened species. Will provide a unique opportunity to further its goal of protecting our natural heritage.\n\n",
    "id": "2355918",
    "title": "Ethnobiology"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=2356015",
    "text": "Folk biology\n\nFolk biology or folkbiology is the cognitive study of how people classify and reason about the organic world. Humans everywhere classify animals and plants into obvious species-like groups. The relationship between a folk taxonomy and a scientific classification can assist in understanding how evolutionary theory deals with the apparent constancy of \"common species\" and the organic processes centering on them. From the vantage of evolutionary psychology, such natural systems are arguably routine \"habits of mind\", a sort of heuristic used to make sense of the natural world.\n\n",
    "id": "2356015",
    "title": "Folk biology"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=578784",
    "text": "Photobiology\n\nPhotobiology is the scientific study of the interactions of light (technically, non-ionizing radiation) and living organisms. The field includes the study of photophysics, photochemistry, photosynthesis, photomorphogenesis, visual processing, circadian rhythms, photomovement, bioluminescence, and ultraviolet radiation effects.\n\nThe division between ionizing radiation and nonionizing radiation is typically considered to be 10 eV, the energy required to ionize an oxygen atom.\n\n\n",
    "id": "578784",
    "title": "Photobiology"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=20324690",
    "text": "Archaeobiology\n\nArchaeobiology, the study of the biology of ancient times through archaeological materials, is a subspecialty of archaeology. It can be seen as a blanket term for paleobotany, animal osteology, zooarchaeology, microbiology, and many other sub-disciplines. Specifically, plant and animal remains are also called ecofacts. Sometimes these ecofacts can be left by humans and sometimes they can be naturally occurring. Archaeobiology tends to focus on more recent finds, so the difference between archaeobiology and palaeontology is mainly one of date: archaeobiologists typically work with more recent, non-fossilised material found at archaeological sites. Only very rarely are archaeobiological excavations performed at sites with no sign of human presence.\n\nThe prime interest of paleobotany is to reconstruct the vegetation that people in the past would have encountered in a particular place and time. Plant studies have always been overshadowed by faunal studies because bones are more conspicuous than plant remains when excavating. Collection of plant remains could everything including pollen, soil, diatoms, wood, plant remains and phytoliths. Phytoliths are sediments and diatoms are water deposits. Each plant remain can tell the archaeologist different things about the environment during a certain time period. Animal remains were the first evidence used by 19th century archaeologists. Today, archaeologists use faunal remains as a guide to the environment. It helps archaeologists understand whether the fauna were present naturally or through activities of carnivores or people. Archaeologists deal with macrofauna and microfauna. Microfauna are better indicators of climate and environmental change than larger species. These can be as small as a bug or as big as a fish or bird. Macrofauna helps archaeologists build a picture of past human diet.\n\nBacteria and Protists form two separate kingdoms, but both are fairly similar when compared. Bacteria are prokaryotic microorganisms, while protists are a group of eukaryotic organisms. Because both are microorganisms, both fall under the study of microbiology and special techniques are required for archaeologists to even see them.\nArchaeologists, in order to find these microorganisms in a site, have to first take samples from the site and bring them in for lab analysis. Once in the lab, they can use equipment such as optical microscopes, in order to actually see evidence of micro organic remains. Archaeologists that look at these microorganisms do not actually find the living bacteria or protist, but instead find indentations left behind in material from where they had been. Depending on where the indentations were in the strata, archaeologists can determine the age of the microorganisms.\n\nPaleomycology is the study of fungi in the fossil record. The study of past fungi can lead to the evolutionary past. Much of fungi are made up of parasites of animals, plants or insects. Most of the contemporary fungi resemble its ancestors, dating back over a million years ago. For example, “In the Dominican amber, a mosquito was found with several types of parasitic fungi growing on its outside cuticle. What is interesting is that the fungi resemble modern day fungi in class Trichomycetes, which are common gut-inhabiting zygomycetes of insects, but they differ from Trichomycetes in that the fungi are on the outside of the insect rather than the inside.” The study of ancient fungi can be used to track the evolution of fungi through millions of years.\n\nThe study of osteology is a study of bones and can be a subdiscipline in archeology. Osteologists in archeology reconstruct bones of humans or animals from the past to find more about the past civilizations. . Osteology is used in archaeology to determine the age, gender, and ethnicity of the remains. It is also helpful to rebuild past societies’ cultural background. Using the remains from the past can help modern archaeologist uncover the past from what they ate to their daily activities. This can help uncover the mysteries of past histories.\n\n",
    "id": "20324690",
    "title": "Archaeobiology"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=2146241",
    "text": "Hydrobiology\n\nHydrobiology is the science of life and life processes in water. Much of modern hydrobiology can be viewed as a sub-discipline of ecology but the sphere of hydrobiology includes taxonomy, economic biology, industrial biology, morphology, physiology etc. The one distinguishing aspect is that all relate to aquatic organisms. Much work is closely related to limnology and can be divided into lotic system ecology (flowing waters) and lentic system ecology (still waters).\n\nOne of the significant areas of current research is eutrophication. Special attention is paid to biotic interactions in plankton assemblage including the microbial loop, the mechanism of influencing water blooms, phosphorus load and lake turnover. Another subject of research is the acidification of mountain lakes. Long-term studies are carried out on changes in the ionic composition of the water of rivers, lakes and reservoirs in connection with acid rain and fertilisation. One goal of current research is elucidation of the basic environmental functions of the ecosystem in reservoirs, which are important for water quality management and water supply.\n\nMuch of the early work of hydrobiologists concentrated on the biological processes utilised in sewage treatment and water purification especially slow sand filters. Other historically important work sought to provide biotic indices for classifying waters according to the biotic communities that they supported. This work continues to this day in Europe in the development of classification tools for assessing water bodies for the EU water framework directive.\n\nThe following are the research interests of hydrobiologists:\n\n\n\n",
    "id": "2146241",
    "title": "Hydrobiology"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=536188",
    "text": "Phenology\n\nPhenology is the study of periodic plant and animal life cycle events and how these are influenced by seasonal and interannual variations in climate, as well as habitat factors (such as elevation). The word, coined by the Belgian botanist Charles Morren around 1849, is derived from the Greek φαίνω (\"phainō\"), \"to show, to bring to light, make to appear\" + λόγος (\"logos\"), amongst others \"study, discourse, reasoning\" and indicates that phenology has been principally concerned with the dates of first occurrence of biological events in their annual cycle. Examples include the date of emergence of leaves and flowers, the first flight of butterflies and the first appearance of migratory birds, the date of leaf colouring and fall in deciduous trees, the dates of egg-laying of birds and amphibia, or the timing of the developmental cycles of temperate-zone honey bee colonies. In the scientific literature on ecology, the term is used more generally to indicate the time frame for any seasonal biological phenomena, including the dates of last appearance (e.g., the seasonal phenology of a species may be from April through September).\n\nBecause many such phenomena are very sensitive to small variations in climate, especially to temperature, phenological records can be a useful proxy for temperature in historical climatology, especially in the study of climate change and global warming. For example, viticultural records of grape harvests in Europe have been used to reconstruct a record of summer growing season temperatures going back more than 500 years.\nIn addition to providing a longer historical baseline than instrumental measurements, phenological observations provide high temporal resolution of ongoing changes related to global warming.\n\nObservations of phenological events have provided indications of the progress of the natural calendar since ancient agricultural times. Many cultures have traditional phenological proverbs and sayings which indicate a time for action: \"When the sloe tree is white as a sheet, sow your barley whether it be dry or wet\" or attempt to forecast future climate: \"If oak's before ash, you're in for a splash. If ash before oak, you're in for a soak\". But the indications can be pretty unreliable, as an alternative version of the rhyme shows: \"If the oak is out before the ash, 'Twill be a summer of wet and splash; If the ash is out before the oak,'Twill be a summer of fire and smoke.\" Theoretically, though, these are not mutually exclusive, as one forecasts immediate conditions and one forecasts future conditions.\n\nThe North American Bird Phenology Program at USGS Patuxent Wildlife Research Center (PWRC) is in possession of a collection of millions of bird arrival and departure date records for over 870 species across North America, dating between 1880 and 1970. This program, originally started by Wells W. Cooke, involved over 3,000 observers including many notable naturalists of the time. The program ran for 90 years and came to a close in 1970 when other programs starting up at PWRC took precedence. The program was again started in 2009 to digitize the collection of records and now with the help of citizens worldwide, each record is being transcribed into a database which will be publicly accessible for use.\n\nThe English naturalists Gilbert White and William Markwick reported the seasonal events of more than 400 plant and animal species, Gilbert White in Selborne, Hampshire and William Markwick in Battle, Sussex over a 25-year period between 1768 and 1793. The data, reported in White's \"Natural History and Antiquities of Selborne\" are reported as the earliest and latest dates for each event over 25 years; so annual changes cannot therefore be determined.\n\nIn Japan and China the time of blossoming of cherry and peach trees is associated with ancient festivals and some of these dates can be traced back to the eighth century. Such historical records may, in principle, be capable of providing estimates of climate at dates before instrumental records became available. For example, records of the harvest dates of the pinot noir grape in Burgundy have been used in an attempt to reconstruct spring–summer temperatures from 1370 to 2003; the reconstructed values during 1787–2000 have a correlation with Paris instrumental data of about 0.75.\n\nRobert Marsham is the founding father of modern phenological recording. Marsham was a wealthy landowner who kept systematic records of \"Indications of spring\" on his estate at Stratton Strawless, Norfolk, from 1736. These were in the form of dates of the first occurrence of events such as flowering, bud burst, emergence or flight of an insect. Consistent records of the same events or \"phenophases\" were maintained by generations of the same family over unprecedentedly long periods of time, eventually ending with the death of Mary Marsham in 1958, so that trends can be observed and related to long-term climate records. The data show significant variation in dates which broadly correspond with warm and cold years. Between 1850 and 1950 a long-term trend of gradual climate warming is observable, and during this same period the Marsham record of oak leafing dates tended to become earlier.\n\nAfter 1960 the rate of warming accelerated, and this is mirrored by increasing earliness of oak leafing, recorded in the data collected by Jean Combes in Surrey. Over the past 250 years, the first leafing date of oak appears to have advanced by about 8 days, corresponding to overall warming on the order of 1.5 °C in the same period.\n\nTowards the end of the 19th century the recording of the appearance and development of plants and animals became a national pastime, and between 1891 and 1948 a programme of phenological recording was organised across the British Isles by the Royal Meteorological Society (RMS). Up to 600 observers submitted returns in some years, with numbers averaging a few hundred. During this period 11 main plant phenophases were consistently recorded over the 58 years from 1891–1948, and a further 14 phenophases were recorded for the 20 years between 1929 and 1948. The returns were summarised each year in the Quarterly Journal of the RMS as \"The Phenological Reports\". The 58-year data have been summarised by Jeffree (1960), and show that flowering dates could be as many as 21 days early and as many as 34 days late, with extreme earliness greatest in summer flowering species, and extreme lateness in spring flowering species. In all 25 species, the timings of all phenological events are significantly related to temperature, indicating that phenological events are likely to get earlier as climate warms.\n\n\"The Phenological Reports\" ended suddenly in 1948 after 58 years, and Britain was without a national recording scheme for almost 50 years, just at a time when climate change was becoming evident. During this period, important contributions were made by individual dedicated observers. The naturalist and author Richard Fitter recorded the First Flowering Date (FFD) of 557 species of British flowering plants in Oxfordshire between about 1954 and 1990. Writing in Science in 2002, Richard Fitter and his son Alistair Fitter found that \"the average FFD of 385 British plant species has advanced by 4.5 days during the past decade compared with the previous four decades.\" They note that FFD is sensitive to temperature, as is generally agreed, that \"150 to 200 species may be flowering on average 15 days earlier in Britain now than in the very recent past\" and that these earlier FFDs will have \"profound ecosystem and evolutionary consequences\".\n\nIn the last decade, national recording in Britain has been resumed by the UK Phenology network , run by Woodland Trust and the Centre for Ecology and Hydrology and the BBC Springwatch survey. There is a USA National Phenology Network in which both professional scientists and lay recorders participate, a European Phenology Network that has monitoring, research and educational remits and many other countries such as Canada (Alberta Plantwatch and Saskatchewan PlantWatch), China and Australia have phenological programs.\n\nIn eastern North America, almanacs are traditionally used for information on action phenology (in agriculture), taking into account the astronomical positions at the time.\nWilliam Felker has studied phenology in Ohio, US, since 1973 and now publishes \"Poor Will's Almanack\", a phenological almanac for farmers (not to be confused with a late 18th-century almanac by the same name).\n\nRecent technological advances in studying the earth from space have resulted in a new field of phenological research that is concerned with observing the phenology of whole ecosystems and stands of vegetation on a global scale using proxy approaches. These methods complement the traditional phenological methods which recorded the first occurrences of individual species and phenophases.\n\nThe most successful of these approaches is based on tracking the temporal change of a Vegetation Index (like Normalized Difference Vegetation Index(NDVI)). NDVI makes use of the vegetation's typical low reflection in the red (red energy is mostly absorbed by growing plants for Photosynthesis) and strong reflection in the Near Infrared (Infrared energy is mostly reflected by plants due to their cellular structure). Due to its robustness and simplicity, NDVI has become one of the most popular remote sensing based products. Typically, a vegetation index is constructed in such a way that the attenuated reflected sunlight energy (1% to 30% of incident sunlight) is amplified by ratio-ing red and NIR following this equation:\n\nThe evolution of the vegetation index through time, depicted by the graph above, exhibits a strong correlation with the typical green vegetation growth stages (emergence, vigor/growth, maturity, and harvest/senescence). These temporal curves are analyzed to extract useful parameters about the vegetation growing season (start of season, end of season, length of growing season, etc.). Other growing season parameters could potentially be extracted, and global maps of any of these growing season parameters could then be constructed and used in all sorts of climatic change studies.\n\nA noteworthy example of the use of remote sensing based phenology is the work of Ranga Myneni from Boston University. This work showed an apparent increase in vegetation productivity that most likely resulted from the increase in temperature and lengthening of the growing season in the boreal forest. Another example based on the MODIS enhanced vegetation index (EVI) reported by Alfredo Huete at the University of Arizona and colleagues showed that the Amazon Rainforest, as opposed to the long held view of a monotonous growing season or growth only during the wet rainy season, does in fact exhibit growth spurts during the dry season.\n\nHowever, these phenological parameters are only an approximation of the true biological growth stages. This is mainly due to the limitation of current space-based remote sensing, especially the spatial resolution, and the nature of vegetation index. A pixel in an image does not contain a pure target (like a tree, a shrub, etc.) but contains a mixture of whatever intersected the sensor's field of view.\n\n\n",
    "id": "536188",
    "title": "Phenology"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=11791198",
    "text": "Bioclimatology\n\nBioclimatology is the interdisciplinary field of science that studies the interactions between the biosphere and the Earth's atmosphere on time scales of the order of seasons or longer (by opposition to biometeorology).\n\nClimate processes largely control the distribution, size, shape and properties of living organisms on Earth. For instance, the general circulation of the atmosphere on a planetary scale broadly determines the location of large deserts or the regions subject to frequent precipitation, which, in turn, greatly determine which organisms can naturally survive in these environments. Furthermore, changes in climates, whether due to natural processes or to human interferences, may progressively modify these habitats and cause overpopulation or extinction of indigenous species.\n\nThe biosphere, for its part, and in particular continental vegetation, which constitutes over 99% of the total biomass, has played a critical role in establishing and maintaining the chemical composition of the Earth's atmosphere, especially during the early evolution of the planet (See History of Earth for more details on this topic). Currently, the terrestrial vegetation exchanges some 60 billion tons of carbon with the atmosphere on an annual basis (through processes of carbon fixation and carbon respiration), thereby playing a critical role in the carbon cycle. On a global and annual basis, small imbalances between these two major fluxes, as do occur through changes in land cover and land use, contribute to the current increase in atmospheric carbon dioxide.\n\n",
    "id": "11791198",
    "title": "Bioclimatology"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=3929318",
    "text": "Biocybernetics\n\nBiocybernetics is the application of cybernetics to biological science, composed of biological disciplines that benefit from the application of cybernetics including neurology and multicellular systems. Biocybernetics plays a major role in systems biology, seeking to integrate different levels of information to understand how biological systems function.\n\nBiocybernetics is an abstract science and is a fundamental part of theoretical biology, based upon the principles of systemics.\n\nBiocybernetics is a conjoined word from bio (Greek: βίο / life) and cybernetics (Greek: κυβερνητική / controlling-governing). Although the extended form of the word is biological cybernetics, the field is most commonly referred to as biocybernetics in scientific papers. Bioinformatics may also be properly referred to as bio informatics. \n\nEarly proponents of biocybernetics include Ross Ashby, Hans Drischel, and Norbert Wiener among others. Popular papers published by each scientist are listed below.\n\nRoss Ashby, \"Introduction to Cybernetics\", 1956\n\nNorbert Wiener, \"Cybernetics or Control and Communication in the Animal and the Machine\", 1948\n\nPapers and research that delve into topics involving biocybernetics may be found under a multitude of similar names, including molecular cybernetics, neurocybernetics, and cellular cybernetics. Such fields involve disciplines that specify certain aspects of the study of the living organism (for example, neurocybernetics focuses on the study neurological models in organisms). \n\n\n\n",
    "id": "3929318",
    "title": "Biocybernetics"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1686272",
    "text": "Chemical biology\n\nChemical biology is a scientific discipline spanning the fields of chemistry and biology. The discipline involves the application of chemical techniques, analysis, and often small molecules produced through synthetic chemistry, to the study and manipulation of biological systems. In contrast to biochemistry, which involves to study of the chemistry of biomolecules and regulation of biochemical pathways within and between cells, chemical biology deals with chemistry \"applied to\" biology.\n\nSome forms of chemical biology attempt to answer biological questions by directly probing living systems at the chemical level. In contrast to research using biochemistry, genetics, or molecular biology, where mutagenesis can provide a new version of the organism, cell, or biomolecule of interest, chemical biology probes systems \"in vitro\" and \"in vivo\" with small molecules that have been designed for a specific purpose or identified on the basis of biochemical or cell-based screening (see chemical genetics).\n\nChemical biology is one of several interdisciplinary sciences that tend to differ from older, reductionist fields and whose goals are to achieve a description of scientific holism. Chemical biology has scientific, historical and philosophical roots in medicinal chemistry, supramolecular chemistry, bioorganic chemistry, pharmacology, genetics, biochemistry, and metabolic engineering. \n\nProteomics investigates the proteome, the set of expressed proteins at a given time under defined conditions. As a discipline, proteomics has moved past rapid protein identification and has developed into a biological assay for quantitative analysis of complex protein samples by comparing protein changes in differently perturbed systems. Current goals in proteomics include determining protein sequences, abundance and any post-translational modifications. Also of interest are protein–protein interactions, cellular distribution of proteins and understanding protein activity. Another important aspect of proteomics is the advancement of technology to achieve these goals.\n\nProtein levels, modifications, locations, and interactions are complex and dynamic properties. With this complexity in mind, experiments need to be carefully designed to answer specific questions especially in the face of the massive amounts of data that are generated by these analyses. The most valuable information comes from proteins that are expressed differently in a system being studied. These proteins can be compared relative to each other using quantitative proteomics, which allows a protein to be labeled with a mass tag. Proteomic technologies must be sensitive and robust, it is for these reasons, the mass spectrometer has been the workhorse of protein analysis. The high precision of mass spectrometry can distinguish between closely related species and species of interest can be isolated and fragmented within the instrument. Its applications to protein analysis was only possible in the late 1980s with the development of protein and peptide ionization with minimal fragmentation. These breakthroughs were ESI and MALDI. Mass spectrometry technologies are modular and can be chosen or optimized to the system of interest.\n\nChemical biologists are poised to impact proteomics through the development of techniques, probes and assays with synthetic chemistry for the characterization of protein samples of high complexity. These approaches include the development of enrichment strategies, chemical affinity tags and probes.\n\nSamples for Proteomics contain a myriad of peptide sequences, the sequence of interest may be highly represented or of low abundance. However, for successful MS analysis the peptide should be enriched within the sample. Reduction of sample complexity is achieved through selective enrichment using affinity chromatography techniques. This involves targeting a peptide with a distinguishing feature like a biotin label or a post translational modification. Interesting methods have been developed that include the use of antibodies, lectins to capture glycoproteins, immobilized metal ions to capture phosphorylated peptides and suicide enzyme substrates to capture specific enzymes. Here, chemical biologists can develop reagents to interact with substrates, specifically and tightly, to profile a targeted functional group on a proteome scale. Development of new enrichment strategies is needed in areas like non-ser/thr/tyr phosphorylation sites and other post translational modifications. Other methods of decomplexing samples relies on upstream chromatographic separations.\n\nChemical synthesis of affinity tags has been crucial to the maturation of quantitative proteomics. iTRAQ, Tandem mass tags (TMT) and Isotope-coded affinity tag (ICAT) are protein mass-tags that consist of a covalently attaching group, a mass (isobaric or isotopic) encoded linker and a handle for isolation. Varying mass-tags bind to different proteins as a sort of footprint such that when analyzing cells of differing perturbations, the levels of each protein can be compared relatively after enrichment by the introduced handle. Other methods include SILAC and heavy isotope labeling. These methods have been adapted to identify complexing proteins by labeling a bait protein, pulling it down and analyzing the proteins it has complexed. \nAnother method creates an internal tag by introducing novel amino acids that are genetically encoded in prokaryotic and eukaryotic organisms. These modifications create a new level of control and can facilitate photocrosslinking to probe protein–protein interactions. In addition, keto, acetylene, azide, thioester, boronate, and dehydroalanine- containing amino acids can be used to selectively introduce tags, and novel chemical functional groups into proteins.\n\nTo investigate enzymatic activity as opposed to total protein, activity-based reagents have been developed to label the enzymatically active form of proteins (see Activity-based proteomics). For example, serine hydrolase- and cysteine protease-inhibitors have been converted to suicide inhibitors. This strategy enhances the ability to selectively analyze low abundance constituents through direct targeting. Structures that mimic these inhibitors could be introduced with modifications that will aid proteomic analysis- like an identification handle or mass tag. Enzyme activity can also be monitored through converted substrate. This strategy relies on using synthetic substrate conjugates that contain moieties that are acted upon by specific enzymes. The product conjugates are then captured by an affinity reagent and analyzed. The measured concentration of product conjugate allow the determination of the enzyme velocity. Identification of enzyme substrates (of which there may be hundreds or thousands, many of which unknown) is a problem of significant difficulty in proteomics and is vital to the understanding of signal transduction pathways in cells; techniques for labelling cellular substrates of enzymes is an area chemical biologists can address. A method that has been developed uses \"analog-sensitive\" kinases to label substrates using an unnatural ATP analog, facilitating visualization and identification through a unique handle.\n\nWhile DNA, RNA and proteins are all encoded at the genetic level, there exists a separate system of trafficked molecules in the cell that are not encoded directly at any direct level: sugars. Thus, glycobiology is an area of dense research for chemical biologists. For instance, live cells can be supplied with synthetic variants of natural sugars in order to probe the function of the sugars in vivo. Carolyn Bertozzi, previously at University of California, Berkeley, has developed a method for site-specifically reacting molecules the surface of cells that have been labeled with synthetic sugars.\n\nChemical biologists used automated synthesis of many diverse compounds in order to experiment with effects of small molecules on biological processes. More specifically, they observe changes in the behaviors of proteins when small molecules bind to them. Such experiments may supposedly lead to discovery of small molecules with antibiotic or chemotherapeutic properties. These approaches are identical to those employed in the discipline of pharmacology.\n\nChemical biologists are also interested in developing new small-molecule and biomolecule-based tools to study biological processes, often by molecular imaging techniques. The field of molecular sensing was popularized by Roger Tsien's work developing calcium-sensing fluorescent compounds as well as pioneering the use of GFP, for which he was awarded the 2008 Nobel Prize in Chemistry. Today, researchers continue to utilize basic chemical principles to develop new compounds for the study of biological metabolites and processes.\n\nMany research programs are also focused on employing natural biomolecules to perform a task or act as support for a new chemical method or material. In this regard, researchers have shown that DNA can serve as a template for synthetic chemistry, self-assembling proteins can serve as a structural scaffold for new materials, and RNA can be evolved \"in vitro\" to produce new catalytic function.\n\nA common form of aggregation is long, ordered spindles called amyloid fibrils that are implicated in Alzheimer’s disease and that have been shown to consist of cross-linked beta sheet regions perpendicular to the backbone of the polypeptide. Another form of aggregation occurs with prion proteins, the glycoproteins found with Creutzfeldt–Jakob disease and bovine spongiform encephalopathy. In both structures, aggregation occurs through hydrophobic interactions and water must be excluded from the binding surface before aggregation can occur. A movie of this process can be seen in \"Chemical and Engineering News\". The diseases associated with misfolded proteins are life-threatening and extremely debilitating, which makes them an important target for chemical biology research.\n\nThrough the transcription and translation process, DNA encodes for specific sequences of amino acids. The resulting polypeptides fold into more complex secondary, tertiary, and quaternary structures to form proteins. Based on both the sequence and the structure, a particular protein is conferred its cellular function. However, sometimes the folding process fails due to mutations in the genetic code and thus the amino acid sequence or due to changes in the cell environment (e.g. pH, temperature, reduction potential, etc.). Misfolding occurs more often in aged individuals or in cells exposed to a high degree of oxidative stress, but a fraction of all proteins misfold at some point even in the healthiest of cells.\n\nNormally when a protein does not fold correctly, molecular chaperones in the cell can encourage refolding back into its active form. When refolding is not an option, the cell can also target the protein for degradation back into its component amino acids via proteolytic, lysosomal, or autophagic mechanisms. However, under certain conditions or with certain mutations, the cells can no longer cope with the misfolded protein(s) and a disease state results. Either the protein has a loss-of-function, such as in cystic fibrosis, in which it loses activity or cannot reach its target, or the protein has a gain-of-function, such as with Alzheimer's disease, in which the protein begins to aggregate causing it to become insoluble and non-functional.\n\nProtein misfolding has previously been studied using both computational approaches as well as \"in vivo\" biological assays in model organisms such as \"Drosophila melanogaster\" and \"C. elegans\". Computational models use a \"de novo\" process to calculate possible protein structures based on input parameters such as amino acid sequence, solvent effects, and mutations. This method has the shortcoming that the cell environment has been drastically simplified, which limits the factors that influence folding and stability. On the other hand, biological assays can be quite complicated to perform \"in vivo\" with high-throughput like efficiency and there always remains the question of how well lower organism systems approximate human systems.\nDobson et al. propose combining these two approaches such that computational models based on the organism studies can begin to predict what factors will lead to protein misfolding. Several experiments have already been performed based on this strategy. In experiments on \"Drosophila\", different mutations of beta amyloid peptides were evaluated based on the survival rates of the flies as well as their motile ability. The findings from the study show that the more a protein aggregates, the more detrimental the neurological dysfunction. Further studies using transthyretin, a component of cerebrospinal fluid that binds to beta amyloid peptide deterring aggregation but can itself aggregate especially when mutated, indicate that aggregation prone proteins may not aggregate where they are secreted and rather are deposited in specific organs or tissues based on each mutation. Kelly et al. have shown that the more stable, both kinetically and thermodynamically, a misfolded protein is the more likely the cell is to secrete it from the endoplasmic reticulum rather than targeting the protein for degradation. In addition, the more stress that a cell feels from misfolded proteins the more probable new proteins will misfold. These experiments as well as others having begun to elucidate both the intrinsic and extrinsic causes of misfolding as well as how the cell recognizes if proteins have folded correctly.\n\nAs more information is obtained on how the cell copes with misfolded proteins, new therapeutic strategies begin to emerge. An obvious path would be prevention of misfolding. However, if protein misfolding cannot be avoided, perhaps the cell's natural mechanisms for degradation can be bolstered to better deal with the proteins before they begin to aggregate. Before these ideas can be realized, many more experiments need to be done to understand the folding and degradation machinery as well as what factors lead to misfolding. More information about protein misfolding and how it relates to disease can be found in the recently published book by Dobson, Kelly, and Rameriz-Alvarado entitled Protein Misfolding Diseases Current and Emerging Principles and Therapies.\n\nIn contrast to the traditional biotechnological practice of obtaining peptides or proteins by isolation from cellular hosts through cellular protein production, advances in chemical techniques for the synthesis and ligation of peptides has allowed for the total synthesis of some peptides and proteins. Chemical synthesis of proteins is a valuable tool in chemical biology as it allows for the introduction of non-natural amino acids as well as residue specific incorporation of \"posttranslational modifications\" such as phosphorylation, glycosylation, acetylation, and even ubiquitination. These capabilities are valuable for chemical biologists as non-natural amino acids can be used to probe and alter the functionality of proteins, while post translational modifications are widely known to regulate the structure and activity of proteins. Although strictly biological techniques have been developed to achieve these ends, the chemical synthesis of peptides often has a lower technical and practical barrier to obtaining small amounts of the desired protein. Given the widely recognized importance of proteins as cellular catalysts and recognition elements, the ability to precisely control the composition and connectivity of polypeptides is a valued tool in the chemical biology community and is an area of active research.\nWhile chemists have been making peptides for over 100 years, the ability to efficiently and quickly synthesize short peptides came of age with the development of Bruce Merrifield's solid phase peptide synthesis (SPPS). Prior to the development of SPPS, the concept of step-by-step polymer synthesis on an insoluble support was without chemical precedent. The use of a covalently bound insoluble polymeric support greatly simplified the process of peptide synthesis by reducing purification to a simple \"filtration and wash\" procedure and facilitated a boom in the field of peptide chemistry. The development and \"optimization\" of SPPS took peptide synthesis from the hands of the specialized peptide synthesis community and put it into the hands of the broader chemistry, biochemistry, and now chemical biology community. SPPS is still the method of choice for linear synthesis of polypeptides up to 50 residues in length and has been implemented in commercially available automated peptide synthesizers. One inherent shortcoming in any procedure that calls for repeated coupling reactions is the buildup of side products resulting from incomplete couplings and side reactions. This places the upper bound for the synthesis of linear polypeptide lengths at around 50 amino acids, while the \"average\" protein consists of 250 amino acids. Clearly, there was a need for development of \"non-linear\" methods to allow synthetic access to the average protein.\n\nAlthough the shortcomings of linear SPPS were recognized not long after its inception, it took until the early 1990s for effective methodology to be developed to ligate small peptide fragments made by SPPS, into protein sized polypeptide chains (for recent review of peptide ligation strategies, see review by Dawson \"et al.\"). The oldest and best developed of these methods is termed native chemical ligation. Native chemical ligation was unveiled in a 1994 paper from the laboratory of Stephen B. H. Kent. Native chemical ligation involves the coupling of a C-terminal thioester and an N-terminal cysteine residue, ultimately resulting in formation of a \"native\" amide bond. Further refinements in native chemical ligation have allowed for kinetically controlled coupling of multiple peptide fragments, allowing access to moderately sized peptides such as an HIV-protease dimer and human lysozyme. Even with the successes and attractive features of native chemical ligation, there are still some drawbacks in the utilization of this technique. Some of these drawbacks include the installation and preservation of a reactive C-terminal thioester, the requirement of an N-terminal cysteine residue (which is the second-least-common amino acid in proteins), and the requirement for a sterically unincumbering C-terminal residue.\n\nOther strategies that have been used for the ligation of peptide fragments using the acyl transfer chemistry first introduced with native chemical ligation include expressed protein ligation, sulfurization/desulfurization techniques, and use of removable thiol auxiliaries.\n\nExpressed protein ligation allows for the biotechnological installation of a C-terminal thioester using intein biochemistry, thereby allowing the appendage of a synthetic N-terminal peptide to the recombinantly produced C-terminal portion. This technique allows for access to much larger proteins, as only the N-terminal portion of the resulting protein has to be chemically synthesized. Both sulfurization/desulfurization techniques and the use of removable thiol auxiliaries involve the installation of a synthetic thiol moiety to carry out the standard native chemical ligation chemistry, followed by removal of the auxiliary/thiol. These techniques help to overcome the requirement of an N-terminal cysteine needed for standard native chemical ligation, although the steric requirements for the C-terminal residue are still limiting.\nA final category of peptide ligation strategies include those methods not based on native chemical ligation type chemistry. Methods that fall in this category include the traceless Staudinger ligation, azide-alkyne dipolar cycloadditions, and imine ligations.\n\nMajor contributors in this field today include Stephen B. H. Kent, Philip E. Dawson, and Tom W. Muir, as well as many others involved in methodology development and applications of these strategies to biological problems.\n\nOne of the primary goals of protein engineering is the design of novel peptides or proteins with a desired structure and chemical activity. Because our knowledge of the relationship between primary sequence, structure, and function of proteins is limited, rational design of new proteins with enzymatic activity is extremely challenging. Directed evolution, repeated cycles of genetic diversification followed by a screening or selection process, can be used to mimic Darwinian evolution in the laboratory to design new proteins with a desired activity.\n\nSeveral methods exist for creating large libraries of sequence variants. Among the most widely used are subjecting DNA to UV radiation or chemical mutagens, error-prone PCR, degenerate codons, or recombination. Once a large library of variants is created, selection or screening techniques are used to find mutants with a desired attribute. Common selection/screening techniques include fluorescence-activated cell sorting (FACS), mRNA display, phage display, or \"in vitro\" compartmentalization. Once useful variants are found, their DNA sequence is amplified and subjected to further rounds of diversification and selection. Since only proteins with the desired activity are selected, multiple rounds of directed evolution lead to proteins with an accumulation beneficial traits.\n\nThere are two general strategies for choosing the starting sequence for a directed evolution experiment: \"de novo\" design and redesign. In a protein design experiment, an initial sequence is chosen at random and subjected to multiple rounds of directed evolution. For example, this has been employed successfully to create a family of ATP-binding proteins with a new folding pattern not found in nature. Random sequences can also be biased towards specific folds by specifying the characteristics (such as polar vs. nonpolar) but not the specific identity of each amino acid in a sequence. Among other things, this strategy has been used to successfully design four-helix bundle proteins. Because it is often thought that a well-defined structure is required for activity, biasing a designed protein towards adopting a specific folded structure is likely to increase the frequency of desirable variants in constructed libraries.\n\nIn a protein redesign experiment, an existing sequence serves as the starting point for directed evolution. In this way, old proteins can be redesigned for increased activity or new functions. Protein redesign has been used for protein simplification, creation of new quaternary structures, and topological redesign of a chorismate mutase. To develop enzymes with new activities, one can take advantage of promiscuous enzymes or enzymes with significant side reactions. In this regard, directed evolution has been used on γ-humulene synthase, an enzyme that creates over 50 different sesquiterpenes, to create enzymes that selectively synthesize individual products. Similarly, completely new functions can be selected for from existing protein scaffolds. In one example of this, an RNA ligase was created from a zinc finger scaffold after 17 rounds of directed evolution. This new enzyme catalyzes a chemical reaction not known to be catalyzed by any natural enzyme.\n\nComputational methods, when combined with experimental approaches, can significantly assist both the design and redesign of new proteins through directed evolution. Computation has been used to design proteins with unnatural folds, such as a right-handed coiled coil. These computational approaches could also be used to redesign proteins to selectively bind specific target molecules. By identifying lead sequences using computational methods, the occurrence of functional proteins in libraries can be dramatically increased before any directed evolution experiments in the laboratory.\n\nManfred T. Reetz, Frances Arnold, Donald Hilvert, and Jack W. Szostak are significant researchers in this field.\n\nRecent advances in technology have allowed scientists to view substructures of cells at levels of unprecedented detail. Unfortunately these \"aerial\" pictures offer little information about the mechanics of the biological system in question. To be fully effective, precise imaging systems require a complementary technique that better elucidates the machinery of a cell. By attaching tracking devices (optical probes) to biomolecules \"in vivo\", one can learn far more about cell metabolism, molecular transport, cell-cell interactions and many other processes\n\nSuccessful labeling of a molecule of interest requires specific functionalization of that molecule to react chemospecifically with an optical probe. For a labeling experiment to be considered robust, that functionalization must minimally perturb the system. Unfortunately, these requirements can often be extremely hard to meet. Many of the reactions normally available to organic chemists in the laboratory are unavailable in living systems. Water- and redox- sensitive reactions would not proceed, reagents prone to nucleophilic attack would offer no chemospecificity, and any reactions with large kinetic barriers would not find enough energy in the relatively low-heat environment of a living cell.\nThus, chemists have recently developed a panel of bioorthogonal chemistry that proceed chemospecifically, despite the milieu of distracting reactive materials \"in vivo\".\n\nThe coupling of an optical probe to a molecule of interest must occur within a reasonably short time frame; therefore, the kinetics of the coupling reaction should be highly favorable. Click chemistry is well suited to fill this niche, since click reactions are, by definition, rapid, spontaneous, selective, and high-yielding. Unfortunately, the most famous \"click reaction,\" a [3+2] cycloaddition between an azide and an acyclic alkyne, is copper-catalyzed, posing a serious problem for use \"in vivo\" due to copper's toxicity.\n\nThe issue of copper toxicity can be alleviated using copper-chelating ligands, enabling copper-catalyzed labeling of the surface of live cells.\nTo bypass the necessity for a catalyst, the lab of Dr. Carolyn Bertozzi introduced inherent strain into the alkyne species by using a cyclic alkyne. In particular, cyclooctyne reacts with azido-molecules with distinctive vigor. Further optimization of the reaction led to the use of difluorinated cyclooctynes (DIFOs), which increased yield and reaction rate. Other coupling partners discovered by separate labs to be analogous to cyclooctynes include trans cyclooctene, norbornene, and a cyclobutene-functionalized molecule.\n\nAs mentioned above, the use of bioorthogonal reactions to tag biomolecules requires that one half of the reactive \"click\" pair is installed in the target molecule, while the other is attached to an optical probe. When the probe is added to a biological system, it will selectively conjugate with the target molecule.\n\nThe most common method of installing bioorthogonal reactivity into a target biomolecule is through metabolic labeling. Cells are immersed in a medium where access to nutrients is limited to synthetically modified analogues of standard fuels such as sugars. As a consequence, these altered biomolecules are incorporated into the cells in the same manner as their wild-type brethren. The optical probe is then incorporated into the system to image the fate of the altered biomolecules. Other methods of functionalization include enzymatically inserting azides into proteins, and synthesizing phospholipids conjugated to cyclooctynes.\n\nAs these bioorthogonal reactions are further optimized, they will likely be used for increasingly complex interactions involving multiple different classes of biomolecules. More complex interactions have a smaller margin for error, so increased reaction efficiency is paramount to continued success in optically probing cellular machinery. Also, by minimizing side reactions, the experimental design of a minimally perturbed living system is closer to being realized.\n\nThe advances in modern sequencing technologies in the late 1990s allowed scientists to investigate DNA of communities of organisms in their natural environments, so-called \"eDNA\", without culturing individual species in the lab. This metagenomic approach enabled scientists to study a wide selection of organisms that were previously not characterized due in part to an incompetent growth condition. These sources of eDNA include, but are not limited to, soils, ocean, subsurface, hot springs, hydrothermal vents, polar ice caps, hypersaline habitats, and extreme pH environments. Of the many applications of metagenomics, chemical biologists and microbiologists such as Jo Handelsman, Jon Clardy, and Robert M. Goodman who are pioneers of metagenomics, explored metagenomic approaches toward the discovery of biologically active molecules such as antibiotics.\nFunctional or homology screening strategies have been used to identify genes that produce small bioactive molecules. Functional metagenomic studies are designed to search for specific phenotypes that are associated with molecules with specific characteristics. Homology metagenomic studies, on the other hand, are designed to examine genes to identify conserved sequences that are previously associated with the expression of biologically active molecules.\nFunctional metagenomic studies enable scientists to discover novel genes that encode biologically active molecules. These assays include top agar overlay assays where antibiotics generate zones of growth inhibition against test microbes, and pH assays that can screen for pH change due to newly synthesized molecules using pH indicator on an agar plate. Substrate-induced gene expression screening (SIGEX), a method to screen for the expression of genes that are induced by chemical compounds, has also been used to search genes with specific functions. These led to the discovery and isolation of several novel proteins and small molecules. For example, the Schipper group identified three eDNA derived AHL lactonases that inhibit biofilm formation of Pseudomonas aeruginosa via functional metagenomic assays. However, these functional screening methods require a good design of probes that detect molecules being synthesized and depend on the ability to express metagenomes in a host organism system.\nIn contrast, homology metagenomic studies led to a faster discovery of genes that have homologous sequences as the previously known genes that are responsible for the biosynthesis of biologically active molecules. As soon as the genes are sequenced, scientists can compare thousands of bacterial genomes simultaneously. The advantage over functional metagenomic assays is that homology metagenomic studies do not require a host organism system to express the metagenomes, thus this method can potentially save the time spent on analyzing nonfunctional genomes. These also led to the discovery of several novel proteins and small molecules. For example, Banik et al. screened for clones containing genes associated with the synthesis of teicoplanin and vancomycin-like glycopeptide antibiotics and found two new biosynthetic gene clusters. In addition, an \"in silico\" examination from the Global Ocean Metagenomic Survey found 20 new lantibiotic cyclases. \nThere are challenges to metagenomic approaches to discover new biologically active molecules. Only 40% of enzymatic activities present in a sample can be expressed in \"E. coli.\". In addition, the purification and isolation of eDNA is essential but difficult when the sources of obtained samples are poorly understood. However, collaborative efforts from individuals from diverse fields including bacterial genetics, molecular biology, genomics, bioinformatics, robots, synthetic biology, and chemistry can solve this problem together and potentially lead to the discovery of many important biologically active molecules.\n\nPosttranslational modification of proteins with phosphate groups has proven to be a key regulatory step throughout all biological systems. Phosphorylation events, either phosphorylation by protein kinases or dephosphorylation by phosphatases, result in protein activation or deactivation. These events have an immense impact on the regulation of physiological pathways, which makes the ability to dissect and study these pathways integral to understanding the details of cellular processes. There exist a number of challenges—namely the sheer size of the phosphoproteome, the fleeting nature of phosphorylation events and related physical limitations of classical biological and biochemical techniques—that have limited the advancement of knowledge in this area. A recent review provides a detailed examination of the impact of newly developed chemical approaches to dissecting and studying biological systems both in vitro and in vivo.\n\nThrough the use of a number of classes of small molecule modulators of protein kinases, chemical biologists have been able to gain a better understanding of the effects of protein phosphorylation. For example, nonselective and selective kinase inhibitors, such as a class of pyridinylimidazole compounds described by Wilson, et al., are potent inhibitors useful in the dissection of MAP kinase signaling pathways. These pyridinylimidazole compounds function by targeting the ATP binding pocket. Although this approach, as well as related approaches, with slight modifications, has proven effective in a number of cases, these compounds lack adequate specificity for more general applications. Another class of compounds, mechanism-based inhibitors, combines detailed knowledge of the chemical mechanism of kinase action with previously utilized inhibition motifs. For example, Parang, et al. describe the development of a \"bisubstrate analog\" that inhibits kinase action by binding both the conserved ATP binding pocket and a protein/peptide recognition site on the specific kinase. While there is no published in vivo data on compounds of this type, the structural data acquired from in vitro studies have expanded the current understanding of how a number of important kinases recognize target substrates. Interestingly, many research groups utilized ATP analogs as a chemical probe to study kinases and identify their substrates.\n\nThe development of novel chemical means of incorporating phosphomimetics into proteins has provided important insight into the effects of phosphorylation events. Historically, phosphorylation events have been studied by mutating an identified phosphorylation site (serine, threonine or tyrosine) to an amino acid, such as alanine, that cannot be phosphorylated. While this approach has been successful in some cases, mutations are permanent in vivo and can have potentially detrimental effects on protein folding and stability. Thus, chemical biologists have developed new ways of investigating protein phosphorylation. By installing phospho-serine, phospho-threonine or analogous phosphonate mimics into native proteins, researchers are able to perform in vivo studies to investigate the effects of phosphorylation by extending the amount of time a phosphorylation event occurs while minimizing the often-unfavorable effects of mutations. Protein semisynthesis, or more specifically expressed protein ligation (EPL), has proven to be successful techniques for synthetically producing proteins that contain phosphomimetic molecules at either the C- or the N-terminus. In addition, researchers have built upon an established technique in which one can insert an unnatural amino acid into a peptide sequence by charging synthetic tRNA that recognizes a nonsense codon with an unnatural amino acid. Recent developments indicate that this technique can also be employed in vivo, although, due to permeability issues, these in vivo experiments using phosphomimetic molecules have not yet been possible.\n\nAdvances in chemical biology have also improved upon classical techniques of imaging kinase action. For example, the development of peptide biosensors—peptides containing incorporated fluorophore molecules—allowed for improved temporal resolution in in vitro binding assays. Experimental limitations, however, prevent this technique from being effectively used in vivo. One of the most useful techniques to study kinase action is Fluorescence Resonance Energy Transfer (FRET). To utilize FRET for phosphorylation studies, fluorescent proteins are coupled to both a phosphoamino acid binding domain and a peptide that can by phosphorylated. Upon phosphorylation or dephosphorylation of a substrate peptide, a conformational change occurs that results in a change in fluorescence. FRET has also been used in tandem with Fluorescence Lifetime Imaging Microscopy (FLIM) or fluorescently conjugated antibodies and flow cytometry to provide a detailed, specific, quantitative results with excellent temporal and spatial resolution.\n\nThrough the augmentation of classical biochemical methods as well as the development of new tools and techniques, chemical biologists have improved accuracy and precision in the study of protein phosphorylation.\n\nAdvances in stem-cell biology have typically been driven by discoveries in molecular biology and genetics. These have included optimization of culture conditions for the maintenance and differentiation of pluripotent and multipotent stem-cells and the deciphering of signaling circuits that control stem-cell fate. However, chemical approaches to stem-cell biology have recently received increased attention due to the identification of several small molecules capable of modulating stem-cell fate in vitro. A small molecule approach offers particular advantages over traditional methods in that it allows a high degree of temporal control, since compounds can be added or removed at will, and tandem inhibition/activation of multiple cellular targets.\n\nSmall molecules that modulate stem-cell behavior are commonly identified in high-throughput screens. Libraries of compounds are screened for the induction of a desired phenotypic change in cultured stem-cells. This is usually observed through activation or repression of a fluorescent reporter or by detection of specific cell surface markers by FACS or immunohistochemistry. Hits are then structurally optimized for activity by the synthesis and screening of secondary libraries. The cellular targets of the small molecule can then be identified by affinity chromatography, mass spectrometry, or DNA microarray. \nA trademark of pluripotent stem-cells, such as embryonic stem-cells (ESCs), is the ability to self-renew indefinitely. The conventional use of feeder cells and various exogenous growth factors in the culture of ESCs presents a problem in that the resulting highly variable culture conditions make the long-term expansion of un-differentiated ESCs challenging. Ideally, chemically defined culture conditions could be developed to maintain ESCs in a pluripotent state indefinitely. Toward this goal, the Schultz and Ding labs at the Scripps Research Institute identified a small molecule that can preserve the long-term self-renewal of ESCs in the absence of feeder cells and other exogenous growth factors. This novel molecule, called pluripotin, was found to simultaneously inhibit multiple differentiation inducing pathways.\nThe utility of stem-cells is in their ability to differentiate into all cell types that make up an organism. Differentiation can be achieved in vitro by favoring development toward a particular cell type through the addition of lineage specific growth factors, but this process is typically non-specific and generates low yields of the desired phenotype. Alternatively, inducing differentiation by small molecules is advantageous in that it allows for the development of completely chemically defined conditions for the generation of one specific cell type. A small molecule, neuropathiazol, has been identified which can specifically direct differentiation of multipotent neural stem cells into neurons. Neuropathiazol is so potent that neurons develop even in conditions that normally favor the formation of glial cells, a powerful demonstration of controlling differentiation by chemical means.\nBecause of the ethical issues surrounding ESC research, the generation of pluripotent cells by reprogramming existing somatic cells into a more \"stem-like\" state is a promising alternative to the use of standard ESCs. By genetic approaches, this has recently been achieved in the creation of ESCs by somatic cell nuclear transfer and the generation of induced pluripotent stem-cells by viral transduction of specific genes. From a therapeutic perspective, reprogramming by chemical means would be safer than genetic methods because induced stem-cells would be free of potentially dangerous transgenes. Several examples of small molecules that can de-differentiate somatic cells have been identified. In one report, lineage-committed myoblasts were treated with a compound, named reversine, and observed to revert to a more stem-like phenotype. These cells were then shown to be capable of differentiating into osteoblasts and adipocytes under appropriate conditions.\nStem-cell therapies are currently the most promising treatment for many degenerative diseases. Chemical approaches to stem-cell biology support the development of cell-based therapies by enhancing stem-cell growth, maintenance, and differentiation in vitro. Small molecules that have been shown to modulate stem-cell fate are potential therapeutic candidates and provide a natural lean-in to pre-clinical drug development. Small molecule drugs could promote endogenous stem-cells to differentiate, replacing previously damaged tissues and thereby enhancing the body's own regenerative ability. Further investigation of molecules that modulate stem-cell behavior will only unveil new therapeutic targets.\n\nOrganisms are composed of cells that, in turn, are composed of macromolecules, e.g. proteins, ribosomes, etc. These macromolecules interact with each other, changing their concentration and suffering chemical modifications. The main goal of many biologists is to understand these interactions, using MRI, ESR, electrochemistry, and fluorescence among others. The advantages of fluorescence reside in its high sensitivity, non-invasiveness, safe detection, and ability to modulate the fluorescence signal. Fluorescence was observed mainly from small organic dyes attached to antibodies to the protein of interest. Later, fluorophores could directly recognize organelles, nucleic acids, and important ions in living cells. In the past decade, the discovery of green fluorescent protein (GFP), by Roger Y. Tsien, hybrid system and quantum dots have enable assessing protein location and function more precisely. Three main types of fluorophores are used: small organic dyes, green fluorescent proteins, and quantum dots. Small organic dyes usually are less than 1 kD, and have been modified to increase photostability, enhance brightness, and reduce self-quenching. Quantum dots have very sharp wavelength, high molar absorptivity and quantum yield. Both organic dyes and quantum dyes do not have the ability to recognize the protein of interest without the aid of antibodies, hence they must use immunolabeling. Since the size of the fluorophore-targeting complex typically exceeds 200 kD, it might interfere with multiprotein recognition in protein complexes, and other methods should be use in parallel. An advantage includes diversity of properties and a limitation is the ability of targeting in live cells. Green fluorescent proteins are genetically encoded and can be covalently fused to your protein of interest. A more developed genetic tagging technique is the tetracysteine biarsenical system, which requires modification of the targeted sequence that includes four cysteines, which binds membrane-permeable biarsenical molecules, the green and the red dyes \"FlAsH\" and \"ReAsH\", with picomolar affinity. Both fluorescent proteins and biarsenical tetracysteine can be expressed in live cells, but present major limitations in ectopic expression and might cause lose of function. Giepmans shows parallel applications of targeting methods and fluorophores using GFP and tetracysteine with ReAsH for α-tubulin and β-actin, respectively. After fixation, cells were immunolabeled for the Golgi matrix with QD and for the mitochondrial enzyme cytochrome with Cy5.\n\nFluorescent techniques have been used assess a number of protein dynamics including protein tracking, conformational changes, protein–protein interactions, protein synthesis and turnover, and enzyme activity, among others.\n\nThree general approaches for measuring protein net redistribution and diffusion are single-particle tracking, correlation spectroscopy and photomarking methods. In single-particle tracking, the individual molecule must be both bright and sparse enough to be tracked from one video to the other. Correlation spectroscopy analyzes the intensity fluctuations resulting from migration of fluorescent objects into and out of a small volume at the focus of a laser. In photomarking, a fluorescent protein can be dequenched in a subcellular area with the use of intense local illumination and the fate of the marked molecule can be imaged directly. Michalet and coworkers used quantum dots for single-particle tracking using biotin-quantum dots in HeLa cells.\n\nOne of the best ways to detect conformational changes in proteins is to sandwich said protein between two fluorophores. FRET will respond to internal conformational changes result from reorientation of the fluorophore with respect to the other. Dumbrepatil sandwiched an estrogen receptor between a CFP (cyan fluorescent protein) and a YFP (yellow fluorescent protein) to study conformational changes of the receptor upon binding of a ligand.\n\nFluorophores of different colors can be applied to detect their respective antigens within the cell. If antigens are located close enough to each other, they will appear colocalized and this phenomenon is known as colocalization. Specialized computer software, such as CoLocalizer Pro, can be used to confirm and characterize the degree of colocalization.\n\nFRET can detect dynamic protein–protein interaction in live cells providing the fluorophores get close enough. Galperin \"et al.\" used three fluorescent proteins to study multiprotein interactions in live cells.\n\nTetracysteine biarsenical systems can be used to study protein synthesis and turnover, which requires discrimination of old copies from new copies. In principle, a tetracysteine-tagged protein is labeled with FlAsH for a short time, leaving green labeled proteins. The protein synthesis is then carried out in the presence of ReAsH, labeling the new proteins as red.\n\nOne can also use fluorescence to see endogenous enzyme activity, typically by using a quenched activity based proteomics (qABP). Covalent binding of a qABP to the active site of the targeted enzyme will provide direct evidence concerning if the enzyme is responsible for the signal upon release of the quencher and regain of fluorescence.\n\nThe unique combination of high spatial and temporal resolution, nondestructive compatibility with living cells and organisms, and molecular specificity insure that fluorescence techniques will remain central in the analysis of protein networks and systems biology.\n\n\n\n",
    "id": "1686272",
    "title": "Chemical biology"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=216956",
    "text": "Glycobiology\n\nDefined in the narrowest sense, glycobiology is the study of the structure, biosynthesis, and biology of saccharides (sugar chains or glycans) that are widely distributed in nature. Sugars or saccharides are essential components of all living things and aspects of the various roles they play in biology are researched in various medical, biochemical and biotechnological fields.\n\nAccording to \"Oxford English Dictionary\" the specific term \"glycobiology\" was coined in 1988 by Prof. Raymond Dwek to recognize the coming together of the traditional disciplines of carbohydrate chemistry and biochemistry. This coming together was as a result of a much greater understanding of the cellular and molecular biology of glycans. However, as early as the late nineteenth century pioneering efforts were being made by Emil Fisher to establish the structure of some basic sugar molecules.\n\nSugars may be linked to other types of biological molecule to form glycoconjugates. The enzymatic process of glycosylation creates sugars/saccharides linked to themselves and to other molecules by the glycosidic bond, thereby producing glycans. Glycoproteins, proteoglycans and glycolipids are the most abundant glycoconjugates found in mammalian cells. They are found predominantly on the outer cell wall and in secreted fluids. Glycoconjugates have been shown to be important in cell-cell interactions due to the presence on the cell surface of various glycan binding receptors in addition to the glycoconjugates themselves. In addition to their function in protein folding and cellular attachment, the N-linked glycans of a protein can modulate the protein's function, in some cases acting as an on-off switch.\n\n\"Glycomics, analogous to genomics and proteomics, is the systematic study of all glycan structures of a given cell type or organism\" and is a subset of glycobiology.\n\nPart of the variability seen in saccharide structures is because monosaccharide units may be coupled to each other in many different ways, as opposed to the amino acids of proteins or the nucleotides in DNA, which are always coupled together in a standard fashion. The study of glycan structures is also complicated by the lack of a direct template for their biosynthesis, contrary to the case with proteins where their amino acid sequence is determined by their corresponding gene.\n\nGlycans are secondary gene products and therefore are generated by the coordinated action of many enzymes in the subcellular compartments of a cell. Since the structure of a glycan may depend on the expression, activity and accessibility of the different biosynthetic enzymes, it is not possible to use recombinant DNA technology in order to produce large quantities of glycans for structural and functional studies as it is for proteins.\n\nAdvanced analytical instruments and software programs, when used in combination, can unlock the mystery of glycan structures. Current techniques for structural annotation and analysis of glycans include liquid chromatography (LC), capillary electrophoresis (CE), mass spectrometry (MS), nuclear magnetic resonance (NMR) and lectin arrays.\n\nOne of the most widely used techniques is mass spectrometry which uses three principal units: the ionizer, analyzer and detector.\n\nGlycan arrays, like that offered by the Consortium for Functional Glycomics and Z Biotech LLC, contain carbohydrate compounds that can be screened with lectins or antibodies to define carbohydrate specificity and identify ligands.\n\nMRM is a mass spectrometry-based technique that has recently been used for site-specific glyosylation profiling. Although MRM has been used extensively in metabolomics and proteomics, its high sensitivity and linear response over a wide dynamic range make it especially suited for glycan biomarker research and discovery. MRM is performed on a triple quadrupole (QqQ) instrument, which is set to detect a predetermined precursor ion in the first quadrupole, a fragmented in the collision quadrupole, and a predetermined fragment ion in the third quadrupole. It is a non-scanning technique, wherein each transition is detected individually and the detection of multiple transitions occurs concurrently in duty cycles. This technique is being used to characterize the immune glycome.\n\nDrugs already on the market, such as heparin, erythropoietin and a few anti-flu drugs have proven effective and highlight the importance of glycans as a new class of drug. Additionally, the search for new anti-cancer drugs is opening up new possibilities in glycobiology. Anti-cancer drugs with new and varied action mechanisms together with anti-inflammatory and anti-infection drugs are today undergoing clinical trials. They may alleviate or complete current therapies. Although these glycans are molecules that are difficult to synthesize in a reproducible way, owing to their complex structure, this new field of research is highly encouraging for the future.\n\nGlycobiology, in which recent developments have been made possible by the latest technological advances, helps provide a more specific and precise understanding of skin aging.\nIt has now been clearly established that glycans are major constituents of the skin and play a decisive role in skin homeostasis.\nVital to the proper functioning of skin, glycans undergo both qualitative and quantitative changes in the course of aging. The functions of communication and metabolism are impaired and the skin's architecture is degraded.\n\n\nhttp://www.healthcanal.com/medical-breakthroughs/22037-UGA-scientists-team-define-first-ever-sequence-biologically-important-carbohydrate.html\n\n",
    "id": "216956",
    "title": "Glycobiology"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=21360243",
    "text": "Organography\n\nOrganography (from Greek , \"organo\", \"organ\"; and , \"-graphy\") is the scientific description of the structure and function of the organs of living things.\n\nOrganography as a scientific study starts with Aristotle, who considered the parts of plants as \"organs\" and began to consider the relationship between different organs and different functions. In the 17th century Joachim Jung, clearly articulated that plants are composed of different organ types such as root, stem and leaf, and he went on to define these organ types on the basis of form and position.\n\nIn the following century Caspar Friedrich Wolff was able to follow the development of organs from the \"growing points\" or apical meristems. He noted the commonality of development between foliage leaves and floral leaves (e.g. petals) and wrote: \"In the whole plant, whose parts we wonder at as being, at the first glance, so extraordinarily diverse, I finally perceive and recognize nothing beyond leaves and stem (for the root may be regarded as a stem). Consequently all parts of the plant, except the stem, are modified leaves.\"\n\nSimilar views were propounded at by Goethe in his well-known treatise. He wrote: \"The underlying relationship between the various external parts of the plant, such as the leaves, the calyx, the corolla, the stamens, which develop one after the other and, as it were, out of one another, has long been generally recognized by investigators, and has in fact been specially studied; and the operation by which one and the same organ presents itself to us in various forms has been termed Metamorphosis of Plants.\"\n\n\n",
    "id": "21360243",
    "title": "Organography"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=34939027",
    "text": "Pharmacometabolomics\n\nPharmacometabolomics, also known as pharmacometabonomics, is a field which stems from metabolomics, the quantification and analysis of metabolites produced by the body. It refers to the direct measurement of metabolites in an individual’s bodily fluids, in order to predict or evaluate the metabolism of pharmaceutical compounds, and to better understand the pharmacokinetic profile of a drug. Alternatively, pharmacometabolomics can be applied to measure metabolite levels following the administration of a pharmaceutical compound, in order to monitor the effects of the compound on certain metabolic pathways(pharmacodynamics). This provides detailed mapping of drug effects on metabolism and the pathways that are implicated in mechanism of variation of response to treatment. In addition, the metabolic profile of an individual at baseline (metabotype) provides information about how individuals respond to treatment and highlights heterogeneity within a disease state. All three approaches require the quantification of metabolites found in bodily fluids and tissue, such as blood or urine, and can be used in the assessment of pharmaceutical treatment options for numerous disease states.\n\nPharmacometabolomics is thought to provide information that complements that gained from other omics, namely genomics, transcriptomics, and proteomics. Looking at the characteristics of an individual down through these different levels of detail, there is an increasingly more accurate prediction of a person’s ability to respond to a pharmaceutical compound. The genome, made up of 25 000 genes, can indicate possible errors in drug metabolism; the transcriptome, made up of 85,000 transcripts, can provide information about which genes important in metabolism are being actively transcribed; and the proteome, >10,000,000 members, depicts which proteins are active in the body to carry out these functions. Pharmacometabolomics complements the omics with direct measurement of the products of all of these reactions, but with perhaps a relatively smaller number of members: that was initially projected to be approximately 2200 metabolites, but could be a larger number when gut derived metabolites and xenobiotics are added to the list. Overall, the goal of pharmacometabolomics is to more closely predict or assess the response of an individual to a pharmaceutical compound, permitting continued treatment with the right drug or dosage depending on the variations in their metabolism and ability to respond to treatment.\n\nPharmacometabolomic analyses, through the use of a metabolomics approach, can provide a comprehensive and detailed metabolic profile or “metabolic fingerprint” for an individual patient. Such metabolic profiles can provide a complete overview of individual metabolite or pathway alterations, providing a more realistic depiction of disease phenotypes. This approach can then be applied to the prediction of response to a pharmaceutical compound by patients with a particular metabolic profile. Pharmacometabolomic analyses of drug response are often coupled or followed up with pharmacogenetics studies. Pharmacogenetics focuses on the identification of genetic variations (e.g. single-nucleotide polymorphisms) within patients that may contribute to altered drug responses and overall outcome of a certain treatment. The results of pharmacometabolomics analyses can act to “inform” or “direct” pharmacogenetic analyses by correlating aberrant metabolite concentrations or metabolic pathways to potential alterations at the genetic level. This concept has been established with two seminal publications from studies of antidepressants serotonin reuptake inhibitors where metabolic signatures were able to define pathway implicated in response to the antidepressant and that lead to identification of genetic variants within a key gene within highlighted pathway as being implicated in variation in response. These genetic variants were not identified through genetic analysis alone and hence illustrated how metabolomics can guide and inform genetic data.\n\nAlthough the applications of pharmacometabolomics to personalized medicine are largely only being realized now, the study of an individual’s metabolism has been used to treat disease since the Middle Ages. Early physicians employed a primitive form of metabolomic analysis by smelling, tasting and looking at urine to diagnose disease. Obviously the measurement techniques needed to look at specific metabolites were unavailable at that time, but such technologies have evolved dramatically over the last decade to develop precise, high-throughput devices, as well as the accompanying data analysis software to analyze output. Currently, sample purification processes, such as liquid or gas chromatography, are coupled with either mass spectrometry (MS)-based or nuclear magnetic resonance (NMR)-based analytical methods to characterize the metabolite profiles of individual patients. Continually advancing informatics tools allow for the identification, quantification and classification of metabolites to determine which pathways may influence certain pharmaceutical interventions. One of the earliest studies discussing the principle and applications of pharmacometabolomics was conducted in an animal model to look at the metabolism of paracetamol and liver damage. NMR spectroscopy was used to analyze the urinary metabolic profiles of rats pre- and post-treatment with paracetamol. The analysis revealed a certain metabolic profile associated with increased liver damage following paracetamol treatment. At this point, it was eagerly anticipated that such pharmacometabolomics approaches could be applied to personalized human medicine. Since this publication in 2006, the Pharmacometabolomics Research Network led by Duke University researchers and that included partnerships between centers of excellence in metabolomics, pharmacogenomics and informatics (over sixteen academic centers funded by NIGMS) has been able to illustrate for the first time the power of the pharmacometabolomics approach in informing about treatment outcomes in large clinical studies and with use of drugs that include antidepressants, statins, antihypertensives, antiplatelet therapies and antipsychotics. Totally new concepts emerged from these studies on use of pharmacometabolomics as a tool that can bring a paradigm shift in the field of pharmacology. It illustrated how pharmacometabolomics can enable a Quantitative and Systems Pharmacology approach. \nPharmacometabolomics has been applied for the treatment of numerous human diseases, such as schizophrenia, diabetes, neural disease, depression and cancer.\n\nAs metabolite analyses are being conducted at the individual patient level, pharmacometabolomics may be considered a form of personalized medicine. This field is currently being employed in a predictive manner to determine the potential responses of therapeutic compounds in individual patients, allowing for more customized treatment regimens. It is anticipated that such pharmacometabolomics approaches will lead to the improved ability to predict an individual’s response to a compound, the efficacy and metabolism of it as well as adverse or off-target effects that may take place in the body. The metabolism of certain drugs varies from patient to patient as the copy number of the genes which code for common drug metabolizing enzymes varies within the population, and leads to differences in the ability of an individual to metabolize different compounds. Other important personal factors contributing to an individual’s metabolic profile, such as patient nutritional status, commensal bacteria, age, and pre-existing medical conditions, are also reflected in metabolite assessment., Overall, pharmacometabolomic analyses combined with such approaches as pharmacogenetics, can function to identify the metabolic processes and particular genetic alterations that may compromise the anticipated efficacy of a drug in a particular patient. The results of such analyses can then allow for the modification of treatment regimens to optimize treatment outcome.\n\nPharmacometabolomics may be used in a predictive manner to determine the correct course of action in regards to a patient about to undergo some type of drug treatment. This involves determining the metabolic profile of a patient prior to treatment, and correlating metabolic signatures with the outcome of a pharmaceutical treatment course. Analysis of a patient’s metabolic profile can reveal factors that may contribute to altered drug metabolism, allowing for predictions of the overall efficacy of a proposed treatment, as well as potential drug toxicity risks that may differ from the general population. This approach has been used to identify novel or previously characterized metabolic biomarkers in patients, which can be used to predict the expected outcome of that patient following treatment with a pharmaceutical compound. One example of the clinical application of pharmacometabolomics are studies that looked to identify a predictive metabolic marker for the treatment of major depressive disorder (MDD)., In a study with antidepressant Sertraline, the Pharmacometabolomics Network illustrated that metabolic profile at baseline of patients with major depression can inform about treatment outcomes. In addition the study illustrated the power of metabolomics for defining response to placebo and compared response to placebo to response to sertraline and showed that several pathways were common to both. In another study with escitalopram citalopram, metabolomic analysis of plasma from patients with MDD revealed that variations in glycine metabolism were negatively associated with patient outcome upon treatment with selective serotonin reuptake inhibitors (SSRIs), an important drug class involved in the treatment of this disease.\n\nThe second major application of pharmacometabolomics is the analysis of a patient’s metabolic profile following the administration of a specific therapy. This process is often secondary to a pre-treatment metabolic analysis, allowing for the comparison of pre- and post-treatment metabolite concentrations. This allows for the identification of the metabolic processes and pathways that are being altered by the treatment either intentionally as a designated target of the compound, or unintentionally as a side effect. Furthermore, the concentration and variety of metabolites produced from the compound itself can also be identified, providing information on the rate of metabolism and potentially leading to development of a related compound with increased efficacy or decreased side effects. An example of this approach was used to investigate the effect of several antipsychotic drugs on lipid metabolism in patients treated for schizophrenia. It was hypothesized that these antipsychotic drugs may be altering lipid metabolism in treated patients with schizophrenia, contributing to the weight gain and hypertriglyceridemia. The study monitored lipid metabolites in patients both before and after treatment with antipsychotics. The compiled pre- and post-treatment profiles were then be compared to examine the effect of these compounds on lipid metabolism. Interestingly, the researchers found correlations between treatment with antipsychotic drugs and lipid metabolism, in both a lipid-class-specific and drug-specific manner, establishing new foundations around the concept that pharmacometabolomics provides powerful tools for enabling detailed mapping of drug effects. Additional studies by the Pharmacometabolomics Research Network enabled mapping in ways not possible before effects of statins, atenolol and aspirin. Totally new insights were gained about effect of these drugs on metabolism and they highlighted pathways implicated in response and side effects.\n\nIn order to identify and quantify metabolites produced by the body, various detection methods have been employed. Most often, these involve the use of nuclear magnetic resonance (NMR) spectroscopy or mass spectrometry (MS), providing universal detection, identification and quantification of metabolites in individual patient samples. Although both processes are used in pharmacometabolomic analyses, there are advantages and disadvantages for using either nuclear magnetic resonance (NMR) spectroscopy- or mass spectrometry (MS)-based platforms in this application.\n\nNMR spectroscopy has been utilized for the analysis of biological samples since the 1980s, and can be used as an effective technique for the identification and quantification of both known and unknown metabolites. For details on the principles of this technique, see NMR spectroscopy. In pharmacometabolomics analyses, NMR is advantageous because minimal sample preparation is required. Isolated patient samples typically include blood or urine due to their minimally-invasive acquisition, however, other fluid types and solid tissue samples have also been studied with this approach. Due to the minimal preparation of samples before analysis, samples can be completely recovered following NMR analysis. This permits samples to be repeatedly processed with extremely high levels of reproducibility, as well as maintaining precious patient samples for alternative analysis. The high reproducibility and precision of NMR, coupled with relatively fast processing time (greater than 100 samples per day), makes this process a relatively high-throughput form of sample analysis. One disadvantage of this technique is the relatively poor metabolite detection sensitivity compared to MS-based analysis, leading to a requirement for greater initial sample volume. Furthermore, the initial instrument costs are extremely high, for both NMR and MS equipment.\n\nAn alternative approach to the identification and quantification of patient samples is through the use of mass spectrometry. This approach offers excellent precision and sensitivity in the identification, characterization and quantification of metabolites in multiple patient sample types, such as blood and urine. The mass spectrometry (MS) approach is typically coupled to gas chromatography (GC), in GC-MS or liquid chromatography (LC), in LC-MS, which aid in initially separating out the metabolite components within complex sample mixtures, and can allow for the isolation of particular metabolite subsets for analysis. GC-MS can provide relatively precise quantification of metabolites, as well as chemical structural information that can be compared to pre-existing chemical libraries. GC-MS can be conducted in a relatively high-throughput manner (greater than 100 samples per day) with greater detection sensitivity than NMR analysis. A limitation of GC-MS for this application, however, is that processed metabolite components must be readily volatized for sample processing.\n\nLC-MS initially separates out the components of a sample mixture based on properties such as hydrophobicity, before processing them for identification and quantification by mass spectrometry (MS). Overall, LC-MS is an extremely flexible method for processing most compound types in a somewhat high-throughput manner (20-100 samples a day), also with greater sensitivity than NMR analysis. For both GC-MS and LC-MS there are limitations in the reproducibility of metabolite quantification. Furthermore, sample processing for downstream mass spectrometry (MS) analysis is much more intensive than in NMR application, and results in the destruction of the original sample (via trypsin digestion).\n\nFollowing identification and quantification of metabolites in individual patient samples, NMR and mass spectrometry (MS) output is compiled into a dataset. These datasets include information on the identity and levels of individual metabolites detected within processed samples, as well as characteristics of each metabolite during the detection process (e.g. mass-to-charge ratios for mass spectrometry (MS)-based analysis). Multiple datasets can be created and compiled into large databases for individual patients in order to monitor varying metabolic profiles over a treatment course (i.e. pre- and post-treatment profiles). Each database is then processed through a type of informatics platform with software designed to characterize and analyze the data to generate an overall metabolic profile for the patient. To generate this overall profile, computational programs are designed to:\n\nAlong with the emerging diagnostic capabilities of pharmacometabolomics, there are limitations introduced when individual variability is looked at. The ability to determine an individual’s physiological state by measurement of metabolites is not contested, but the extreme variability that can be introduced by age, nutrition, and commensal organisms suggest problems in creating generalized pharmacometabolomes for patient groups. However, as long as meaningful metabolic signatures can be elucidated to create baseline values, there still exists a possible means of comparison.\n\nIssues surrounding the measurement of metabolites in an individual can also arise from the methodology of metabolite detection, and there are arguments both for and against NMR and mass spectrometry (MS). Other limitations surrounding metabolite analysis include the need for proper handling and processing of samples, as well as proper maintenance and calibration of the analytical and computational equipment. These tasks require skilled and experienced technicians, and potential instrument repair costs due to continuous sample processing can be costly. The cost of the processing and analytical platforms alone is very high, making it difficult for many facilities to afford pharmacometabolomics-based treatment analyses.\n\nPharmacometabolomics may decrease the burden on the healthcare system by better gauging the correct choice of treatment drug and dosage in order to optimize the response of a patient to a treatment. Hopefully, this approach will also ultimately limit the number of adverse drug reactions (ADRs) associated with many treatment regimens. Overall, physicians would be better able to apply more personalized, and potentially more effective, treatments to their patients. It is important to consider, however, that the processing and analysis of the patient samples takes time, resulting in delayed treatment. \nAnother concern about the application of pharmacometabolomics analyses to individual patient care, is deciding who should and who should not receive this in-depth, personalized treatment protocol. Certain diseases and stages of disease would have to be classified according to their requirement of such a treatment plan, but there are no criteria for this classification. Furthermore, not all hospitals and treatment institutes can afford the equipment to process and analyze patient samples on site, but sending out samples takes time and ultimately delays treatment. \nHealth insurance coverage of such procedures may also be an issue. Certain insurance companies may discriminate against the application of this type of sample analysis and metabolite characterization. Furthermore, there would have to be regulations put in place to ensure that there was no discrimination by insurance companies against the metabolic profiles of individual patients (“high metabolizers” vs. risky “low metabolizers”).\n\n\n",
    "id": "34939027",
    "title": "Pharmacometabolomics"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=4062934",
    "text": "Phenomics\n\nPhenomics is an area of biology concerned with the measurement of phenomes (a phenome is the set of physical and biochemical traits belonging to a given organism) as they change in response to genetic mutation and environmental influences. It is used in functional genomics, pharmaceutical research, metabolic engineering and increasingly in phylogenetics.\n\nAn important field of research today is trying to improve, both qualitatively and quantitatively, the capacity to measure phenomes. This include developing high-throughput measurement systems.\n\nFor example, the Australian Plant Phenomics Facility, an initiative of the Australian government, has developed a number of new instruments for comprehensive and fast measurements of phenotypes in both the lab and the field.\n\n\n",
    "id": "4062934",
    "title": "Phenomics"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=9247603",
    "text": "Protistology\n\nProtistology is a scientific discipline devoted to the study of protists, a highly diverse group of eukaryotic organisms. Its field of study overlaps with more traditional disciplines of phycology, mycology, and protozoology, just as protists, which, being a paraphyletic group embrace algae, some organisms regarded previously as primitive fungi, and protozoa (\"animal\" motile protists lacking chloroplasts).\n\nAs the term \"protozoology\" has become dated as our understanding of the evolutionary relationships of the eukaryotes has improved, the term \"protistology\" become more common. For example, the Society of Protozoloogists, founded in 1947, was renamed International Society of Protistologists in 2005. However, the older term persists in some cases (e.g., the Polish journal Acta Protozoologica).\n\nDedicated academic journals include:\n\n\nOther less specialized journals, important to protistology before the appearance of the more specialized:\n\nSome societies:\n\nThe field of protistology was idealized by Haeckel, but its widespread recognition is more recent. In fact, many of the researchers cited below considered themselves as protozoologists, phycologists, mycologists, microbiologists, microscopists, parasitologists, limnologists, biologists, naturalists, zoologists, botanists, etc., but made significant contributions to the field.\n\n",
    "id": "9247603",
    "title": "Protistology"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=9468040",
    "text": "Scotobiology\n\nThe term scotobiology describes the study of biology as directly and specifically affected by darkness, as opposed to photobiology, which describes the biological effects of light.\n\nThe science of scotobiology gathers together under a single descriptive heading a wide range of approaches to the study of the biology of darkness. This includes work on the effects of darkness on the behavior and metabolism of animals, plants, and microbes. Some of this work has been going on for over a century, and lays the foundation for understanding the importance of dark night skies, not only for humans but for all biological species.\n\nThe great majority of biological systems have evolved in a world of alternating day and night and have become irrevocably adapted to and dependent on the daily and seasonally changing patterns of light and darkness. Light is essential for many biological activities such as sight and photosynthesis. These are the focus of the science of photobiology. But the presence of uninterrupted periods of darkness, as well as their alternation with light, is just as important to biological behaviour. Scotobiology studies the positive responses of biological systems to the presence of darkness, and not merely the negative effects caused by the absence of light.\n\nMany of the biological and behavioural activities of plants, animals (including birds and amphibians), insects, and microorganisms are either adversely affected by light pollution at night or can only function effectively either during or as the consequence of nightly darkness. Such activities include foraging, breeding and social behavior in higher animals, amphibians, and insects, which are all affected in various ways if light pollution occurs in their environment. These are not merely photobiological phenomena; light pollution acts by interrupting critical dark-requiring processes. \nBut perhaps the most important scotobiological phenomena relate to the regular periodic alternation of light and darkness. These include breeding behavior in a range of animals, the control of flowering and the induction of winter dormancy in many plants, and the operational control of the human immune system. In many of these biological processes the critical point is the length of the dark period rather than that of the light. For example, \"short-day\" and \"long-day\" plants are, in fact, \"long-night\" and \"short-night\" respectively. That is to say, plants do not measure the length of the light period, but of the dark period. One consequence of artificial light pollution is that even brief periods of relatively bright light during the night may prevent plants or animals (including humans) from measuring the length of the dark period, and therefore from behaving in a normal or required manner. This is a critical aspect of scotobiology, and one of the major areas in the study of the responses of biological systems to darkness.\n\nIn discussing scotobiology, it is important to remember that darkness (the absence of light) is seldom absolute. An important aspect of any scotobiological phenomenon is the level and quality (wavelength) of light that is below the threshold of detection for that phenomenon and in any specific organism. This important variable in scotobiological studies is not always properly noted or examined. There are substantial levels of natural light pollution at night, of which moonlight is usually the strongest. For example, plants that rely on night length to program their behaviour have the capacity to ignore full moonlight during an otherwise dark night. If this ability had not evolved, plants would not be able to respond to changing night-length for such behavioural programs as the initiation of flowering and the onset of dormancy. On the other hand, some animal behavioural patterns are strongly responsive to moonlight. It is thus most important in any scotobiological study to determine the threshold level of light that may be required to interfere with or negate the normal pattern of dark-night activity.\n\nIn 2003, at a symposium on the Ecology of the Night held in Muskoka, Canada, discussion centered around the many effects of night-time light pollution on the biology of a wide range of organisms, but it went far beyond this in describing darkness as a biological imperative for the functioning of biological systems. Presentations focused on the absolute requirement of darkness for many aspects of normal behaviour and metabolism of many organisms and for the normal progression of their life cycles. Because there was no suitable term to describe the Symposium's main focus, the term \"scotobiology\" was introduced. The word is derived from the Greek \"scotos\", σκότος, \"dark,\" and relates to photobiology, which describes the biological effects of light (φῶς, \"phos\"; root: φωτ-, \"phot-\"). The term scotobiology appears not to have been used previously, although related terms such as skototropism and scotophyle have appeared in the literature.\n\n\nA review and the bibliography of the book by Rich and Longcore appear here: \n",
    "id": "9468040",
    "title": "Scotobiology"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=37988325",
    "text": "Lipidology\n\nLipidology is the scientific study of lipids.\n\nCompared to other biomedical fields, lipidology was originally less popular since the constant handling of oils, smears, and greases was unappealing and separation was difficult. The field became more popular following the advent of chromatography which allowed lipids to be isolated and analyzed. The field was further popularized following the cytologic application of the electron microscope which found that many metabolic pathways take place on the cell membrane, whose properties are strongly influenced by its lipid composition.\n\nThe Framingham Heart Study and other epidemiological studies have found a correlation between lipoproteins and cardiovascular disease.\n\n\n",
    "id": "37988325",
    "title": "Lipidology"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1043627",
    "text": "Biomedicine\n\nBiomedicine (i.e. medical biology) is a branch of medical science that applies biological and physiological principles to clinical practice. The branch especially applies to biology and physiology.\nBiomedicine also can relate to many other categories in health and biological related fields. It has been the dominant health system for more than a century.\n\nIt includes many biomedical disciplines and areas of specialty that typically contain the \"bio-\" prefix such as:\n\nMedical biology is the cornerstone of modern health care and laboratory diagnostics. It concerns a wide range of scientific and technological approaches: from an in vitro diagnostics to the in vitro fertilisation, from the molecular mechanisms of a cystic fibrosis to the population dynamics of the HIV virus, from the understanding molecular interactions to the study of the carcinogenesis, from a single-nucleotide polymorphism (SNP) to the gene therapy.\n\nMedical biology based on molecular biology combines all issues of developing molecular medicine into large-scale structural and functional relationships of the human genome, transcriptome, proteome, physiome and metabolome with the particular point of view of devising new technologies for prediction, diagnosis and therapy \n\nBiomedicine involves the study of (patho-) physiological processes with methods from biology and physiology. Approaches range from understanding molecular interactions to the study of the consequences at the in vivo level. These processes are studied with the particular point of view of devising new strategies for diagnosis and therapy.\n\nDepending on the severity of the disease, biomedicine pinpoints a problem within a patient and fixes the problem through medical intervention. Medicine focuses on curing diseases rather than improving one's health.\n\nMolecular biology is the process of synthesis and regulation of a cell’s DNA, RNA, and protein. Molecular biology consists of different techniques including Polymerase chain reaction, Gel electrophoresis, and macromolecule blotting to manipulate DNA.\n\nPolymerase chain reaction is done by placing a mixture of the desired DNA, DNA polymerase, primers, and nucleotide bases into a machine. The machine heats up and cools down at various temperatures to break the hydrogen bonds binding the DNA and allows the nucleotide bases to be added onto the two DNA templates after it has been separated.\n\nGel electrophoresis is a technique used to identify similar DNA between two unknown samples of DNA. This process is done by first preparing an agarose gel. This jelly-like sheet will have wells for DNA to be poured into. An electric current is applied so that the DNA, which is negatively charged due to its phosphate groups is attracted to the positive electrode. Different rows of DNA will move at different speeds because some DNA pieces are larger than others. Thus if two DNA samples show a similar pattern on the gel electrophoresis, one can tell that these DNA samples match.\n\nMacromolecule blotting is a process performed after gel electrophoresis. An alkaline solution is prepared in a container. A sponge is placed into the solution and an agaros gel is placed on top of the sponge. Next, nitrocellulose paper is placed on top of the agarose gel and a paper towels are added on top of the nitrocellulose paper to apply pressure. The alkaline solution is drawn upwards towards the paper towel. During this process, the DNA denatures in the alkaline solution and is carried upwards to the nitrocellulose paper. The paper is then placed into a plastic bag and filled with a solution full of the DNA fragments, called the probe, found in the desired sample of DNA. The probes anneal to the complementary DNA of the bands already found on the nitrocellulose sample. Afterwards, probes are washed off and the only ones present are the ones that have annealed to complementary DNA on the paper. Next the paper is stuck onto an x ray film. The radioactivity of the probes creates black bands on the film, called an autoradiograph. As a result, only similar patterns of DNA to that of the probe are present on the film. This allows us the compare similar DNA sequences of multiple DNA samples. The overall process results in a precise reading of similarities in both similar and different DNA sample.\n\nBiochemistry is the science of the chemical processes which takes place within living organisms. Living organisms need essential elements to survive, consisting of carbon, hydrogen, nitrogen, oxygen, calcium, and phosphorus. These elements make up the four big macromolecules that living organisms need to survive- carbohydrates, lipids, proteins, and nucleic acids.\n\nCarbohydrates, made up of carbon, hydrogen, and oxygen, are energy storing molecules. The simplest one of carbohydrates is glucose, CHO, is used in cellular respiration to produce ATP, adenosine triphosphate, which supplies cells with energy.\n\nProteins are chains of amino acids that function to contract skeletal muscle, function catalysts, transport molecules, and storage molecules. Proteins can facilitate biochemical processes, by lowering the activation energy of a reaction. Hemoglobins are also proteins, that carry oxygen to the cells in an organisms body.\n\nLipids, also known as fats, also serve to store energy, but in the long term. Due to their unique structure, lipids provide more than twice the amount of energy that carbohydrates do. Lipids can be used as insulation, as it is present below the layer of skin in living organisms. Moreover, lipids can be used in hormone production to maintain a healthy hormonal balance and provide structure to your cell walls.\n\nNucleic acids are used to store DNA in every living organism. The two types of nucleic acids are DNA and RNA. DNA is the main genetic information storing substance found oftentimes in the nucleus, which controls the processes that the cell undergoes. DNA consists of two complimentary antiparallel strands consisting varying patterns of nucleotides. RNA is a single strand of DNA, which is transcribed from DNA and used for DNA translation, which is the process for making proteins out of RNA sequences.\n",
    "id": "1043627",
    "title": "Biomedicine"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=40610658",
    "text": "Neuromechanics\n\nAs originally proposed by Enoka, neuromechanics is a field of study that combines concepts from biomechanics and neurophysiology to study human movement. Neuromechanics examines the combined roles of the skeletal, muscular, and nervous systems and how they interact to produce the motion required to complete a motor task.\n\nMuscles signals stimulated by neurological impulses are collected using electromyography (EMG). These muscle signals are indicative of neural activity. In certain instances, EMG data can be indicative of neuroplasticity and learning of motor tasks. The muscular system, specifically skeletal muscle, creates movement around bone joints, and the central nervous system is imperative in directing skeletal muscles in motor movements.\n\nNeuromechanics is the field of study that combines neuroscience and biomechanics in an effort to understand movement and its relationship with the brain. Neuromechanics is an area that attempts to combine the efforts of muscles, sensory organs, pattern generators in the brain, and the central nervous system itself to explain motion. Neuromechanics applications include easing health problems and designing and controlling robotic systems.\n\nNeuroscience is the study of the nervous system. The nervous system is divided into two sub-systems: the peripheral nervous system and the central nervous system.\n\nThe peripheral nervous system is then composed of three sub-systems: the somatic nervous system, the autonomic nervous system, and the visceral nervous system. The autonomic nervous system is also broken down into the sympathetic nervous system, the parasympathetic nervous system, and the enteric nervous system. The nervous system responsible for voluntary motion, including lower limb motion, is the somatic nervous system. Though the somatic nervous system is part of the peripheral nervous system, motion also involves use of elements of the central nervous system: the brain and the spinal cord.\n\nNeuroscience contributes to human neuromechanics by studying how different neurological diseases contribute to biomechanical problems and changes in typical movement. Neuroscience deals with studying the cause of visible problems.\n\nBiomechanics is the study of the structure and function of living systems such as humans, animals, and other organisms by means of mechanics. Much of biomechanics is concerned with simple motor tasks such as walking. Walking can be defined by the gait cycle. The gait cycle is a repetitive event that consists of one full step from heel-strike to the next heel-strike in the same foot. It can be divided into two phases: stance phase and swing phase. Stance phase consists of the time during which the heel strikes the ground to the point in time at which the toe leaves the ground. Swing phase consists of the rest of the gait cycle, the time between the toe leaving the ground to the next heel strike.\n\nBiomechanics contributes to neuromechanics by studying how the body responds to different conditions whether they be neurological diseases or physical impairments. Biomechanics deals with studying the effect that results from such conditions.\n\nThe inverted pendulum theory of gait is a neuromechanical approach to understanding human movement. In the theory, the weight of the body is reduced to a center of mass resting on a massless leg at a single support. The ground reaction force travels from the center of pressure at the bottom of the massless leg to the center of mass at the top of the massless leg. The velocity vector of the center of mass is always perpendicular to the ground reaction force.\n\nWalking consists of alternating single-support and double-support phases. The single-support phase occurs when one leg is in contact with the ground while the double-support phase occurs when two legs are in contact with the ground.\n\nThe inverted pendulum is stabilized by constant feedback from the brain and can operate even in the presence of sensory loss. In animals who have lost all sensory input to the moving limb, the variables produced by gait (center of mass acceleration, velocity of animal, and position of the animal) remain constant between both groups.\n\nDuring postural control, delayed feedback mechanisms are used in the temporal reproduction of task-level functions such as walking. The nervous system takes into account feedback from the center of mass acceleration, velocity, and position of an individual and utilizes the information to predict and plan future movements. Center of mass acceleration is essential in the feedback mechanism as this feedback takes place before any significant displacement data can be determined.\n\nThe inverted pendulum theory directly contradicts the six determinants of gait, another theory for gait analysis. The six determinants of gait predict very high energy expenditure for the sinusoidal motion of the Center of Mass during gait, while the inverted pendulum theory offers the possibility that energy expenditure can be near zero. Paradoxically, the inverted pendulum theory predicts that little to no work is required for walking.\n\nElectromyography (EMG) is a tool used to measure the electrical outputs produced by skeletal muscles upon activation. Motor nerves innervate skeletal muscles and cause contraction upon command from the central nervous system. This contraction is measured by EMG and is typically measured on the scale of millivolts (mV). Another form of EMG data that is analyzed is integrated EMG (iEMG) data. iEMG measures the area under the EMG signal which corresponds to the overall muscle effort rather than the effort at a specific instant.\n\nThere are four instrumentation components used to detect these signals: (1) the signal source, (2) the transducer used to detect the signal, (3) the amplifier, and (4) the signal processing circuit. The signal source refers to the location at which the EMG electrode is place. EMG signal acquisition is dependent on distance from the electrode to the muscle fiber, so placement is imperative. The transducer used to detect the signal is an EMG electrode than transforms the bioelectric signal from the muscle to a readable electric signal. The amplifier reproduces and undistorted bioelectric signal and also allows for noise reduction in the signal. Signal processing involves taking the recorded electrical impulses, filtering them, and enveloping the data.\n\nLatency is a measure of the time span between the activation of a muscle and its peak EMG value. Latency is used as a means to diagnose disorders of the nervous system such as a herniated disc, amyotrophic lateral sclerosis (ALS), or myasthenia gravis (MG). These disorders may cause a disruption of the signal at the muscle, the nerve, or the junction between the muscle and the nerve.\n\nThe use of EMG to identify nervous systems disorders is known as a nerve conduction study (NCS). Nerve conduction studies can only diagnose diseases on the muscular and nerve level. They cannot detect disease in the spinal cord or the brain. In most disorders of the muscle, nerve, or neuromuscular junction, the latency time is increased. This is a result of decreased nerve conduction or electrical stimulation at the site of the muscle. In 50% of patients with cerebral atrophy cases, the M3 spinal reflex latency, was increased and on occasion separated from the M2 spinal reflex response. The separation between the M2 and M3 spinal reflex responses is typically 20 milliseconds, but in patients with cerebral atrophy, the separation was increased to 50 ms. In some cases, however, other muscles can compensate for the muscle suffering from decreased electrical stimulation. In the compensatory muscle, the latency time is actually decreased in order to substitute for the function of the diseased muscle. These kinds of studies are used in neuromechanics to identify motor disorders and their effects on a cellular and electrical level rather than a system motion level.\n\nA muscle synergy is a group of synergistic muscles and agonists that work together to perform a motor task. A muscle synergy is composed of agonist and synergistic muscles. An agonist muscle is a muscle that contracts individually, and it can cause a cascade of motion in neighboring muscles. Synergistic muscles aid the agonist muscles in motor control tasks, but they act against excess motion that the agonists may create.\n\nThe muscle synergy hypothesis is based on the assumption that the central nervous system controls muscle groups independently rather than individual muscles. The muscle synergy hypothesis presents motor control as a three-tiered hierarchy. In tier one, a motor task vector is created by the central nervous system. The central nervous system then transforms the muscle vector to act upon a group of muscle synergies in tier two. Then in tier three, muscle synergies define a specific ratio of the motor task for each muscle and assign it to its respective muscle to act upon the joint to perform the motor task.\n\nRedundancy plays a large role in muscle synergy. Muscle redundancy is a degrees of freedom problem on the muscular level. The central nervous system is presented with the opportunity to coordinate muscle movements, and it must choose one out of many. The muscle redundancy problem is a result of more muscle vectors than dimensions in the task space. Muscles can only generate tension by pulling, not pushing. This results in many muscle force vectors in multiple directions rather than a push and pull in the same direction.\n\nOne debate on muscle synergies is between the prime mover strategy and the cooperation strategy. The prime mover strategy arises when a muscle's vector can act in the same direction as the mechanical action vector, the vector of the limb's motion. The cooperation strategy, however, takes place when no muscle can act directly in the vector direction of the mechanical action resulting in a coordination of multiple muscles to achieve the task. The prime mover strategy over time has declined in popularity as it has been found through electromyography studies that no one muscle consistently provides more force that other muscles that are acting to move about a joint.\n\nThe muscle synergy theory is difficult to falsify. Though experimentation has shown that groups of muscles indeed work together to control motor tasks, neural connections allow for individual muscles to be activated. Though individual muscle activation may contradict muscle synergy, it also obscures it. Activation of individual muscles may override or block the input from and overall effect of muscle synergies.\n\nAdaptation in the neuromechanical sense is the body's ability to change an action to better suit the situation or environment in which it is acting. Adaptation can be a result of injury, fatigue, or practice. Adaptation can be measured in a variety of ways: electromyography, three-dimensional reconstruction of joints, and changes in other variables pertaining to the specific adaptation being studied.\n\nInjury can cause adaptation in a number of ways. Compensation is a large factor in injury adaptation. Compensation is a result of one or more weakened muscles. The brain is given the task to perform a certain motor task, and once a muscle has been weakened, the brain computes energy ratios to send to other muscles to perform the original task in the desired fashion. Change in muscle contribution is not the only byproduct of a muscle-related injury. Change in loading of the joint is another result which, if prolonged, can be harmful for the individual.\n\nMuscle fatigue is the neuromuscular adaptation to challenges over a period of time. The use of motor units over a period of time can result in changes in the motor command from the brain. Since the force of contraction cannot be changed, the brain instead recruits more motor units to achieve maximal muscle contraction. Recruitment of motor units varies from muscle to muscle depending on the upper limit of motor recruitment in the muscle.\n\nAdaptation due to practice can be a result of intended practice such as sports or unintended practice such as wearing an orthosis. In athletes, repetition results in muscle memory. The motor task becomes a long-term memory that can be repeated without much conscious effort. This allows the athlete to focus on fine-tuning their motor task strategy. Resistance to fatigue also comes with practice as the muscle is strengthened, but the speed at which an athlete can complete a motor task is also increased with practice. Volleyball players compared to non-jumpers show more repeatable control of muscles surrounding the knee that is controlled by co-activation in the single jump condition. In the repeated jump condition, both volleyball players and non-jumpers have a linear decrease in normalized jump flight time. Though the normalized linear decrease is the same for athletes and non-athletes, athletes consistently have higher flight times.\n\nThere is also adaptation associated with use of a prosthesis or an orthosis. This operates similarly to adaptation due to fatigue; however, muscles can actually be fatigued or alter their mechanical contribution to a motor task as a result of wearing the orthosis. An ankle foot orthosis is a common solution to injury of the lower limb, specifically around the ankle joint. An ankle foot orthosis can be assistive or resistive. An assistive ankle orthosis encourages ankle movement, and a resistive ankle orthosis inhibits ankle movement. Upon wearing an assistive ankle foot orthosis, individuals have decreased EMG amplitude and joint stiffness over time while the opposite occurs for resistive ankle foot orthoses. Additionally, not only can electromyography readings differ, but the physical path that joints travel along can be altered as well.\n\n",
    "id": "40610658",
    "title": "Neuromechanics"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=42985608",
    "text": "Cognitive biology\n\nCognitive biology is an emerging science that regards natural cognition as a biological function. It is based on the theoretical assumption that every organism—whether a single cell or multicellular—is continually engaged in systematic acts of cognition coupled with intentional behaviors, i.e., a sensory-motor coupling. That is to say, if an organism can sense stimuli in its environment and respond accordingly, it is cognitive. Any explanation of how natural cognition may manifest in an organism is constrained by the biological conditions in which its genes survives from one generation to the next. And since by Darwinian theory the species of every organism is evolving from a common root, three further elements of cognitive biology are required: (i) the study of cognition in one species of organism is useful, through contrast and comparison, to the study of another species’ cognitive abilities; (ii) it is useful to proceed from organisms with simpler to those with more complex cognitive systems, and (iii) the greater the number and variety of species studied in this regard, the more we understand the nature of cognition.\n\nWhile cognitive science endeavors to explain human thought and the conscious mind, the work of cognitive biology is focused on the most fundamental process of cognition for any organism. In the past several decades, biologists have investigated cognition in organisms large and small, both plant and animal. “Mounting evidence suggests that even bacteria grapple with problems long familiar to cognitive scientists, including: integrating information from multiple sensory channels to marshal an effective response to fluctuating conditions; making decisions under conditions of uncertainty; communicating with conspecifics and others (honestly and deceptively); and coordinating collective behaviour to increase the chances of survival.” Without thinking or perceiving as humans would have it, an act of basic cognition is arguably a simple step-by-step process through which an organism senses a stimulus, then finds an appropriate response in its repertoire and enacts the response. However, the biological details of such basic cognition have neither been delineated for a great many species nor sufficiently generalized to stimulate further investigation. This lack of detail is due to the lack of a science dedicated to the task of elucidating the cognitive ability common to all biological organisms. That is to say, a \"science\" of cognitive biology has yet to be established. A prolegomena for such science was presented in 2007 and several authors have published their thoughts on the subject since the late 1970s. Yet as the examples in the next section suggest, there is neither consensus on the theory nor widespread application in practice.\n\nAlthough the two terms are sometimes used synonymously, cognitive biology should not be confused with the biology of cognition in the sense that it is used by adherents to the Chilean School of Biology of Cognition. Also known as the Santiago School, the biology of cognition is based on the work of Francisco Varela and Humberto Maturana, who crafted the doctrine of autopoiesis. Their work began in 1970 while the first mention of cognitive biology by Brian Goodwin (discussed below) was in 1977 from a different perspective.\n\n'Cognitive biology' first appeared in the literature as a paper with that title by Brian C. Goodwin in 1977. There and in several related publications Goodwin explained the advantage of cognitive biology in the context of his work on morphogenesis. He subsequently moved on to other issues of structure, form, and complexity with little further mention of cognitive biology. Without an advocate, Goodwin’s concept of cognitive biology has yet to gain widespread acceptance. \nAside from an essay regarding Goodwin’s conception by Margaret Boden in 1980, the next appearance of ‘cognitive biology’ as a phrase in the literature came in 1986 from a professor of biochemistry, Ladislav Kováč. His conception, based on natural principles grounded in bioenergetics and molecular biology, is briefly discussed below. Kováč’s continued advocacy has had a greater influence in his homeland, Slovakia, than elsewhere partly because several of his most important papers were written and published only in Slovakian.\n\nBy the 1990s, breakthroughs in molecular, cell, evolutionary, and developmental biology generated a cornucopia of data-based theory relevant to cognition. Yet aside from the theorists already mentioned, no one was addressing cognitive biology except for Kováč.\n\nLadislav Kováč's “Introduction to cognitive biology” (Kováč, 1986a) lists ten ‘Principles of Cognitive Biology.’ A closely related thirty page paper was published the following year: “Overview: Bioenergetics between chemistry, genetics and physics.” (Kováč, 1987). Over the following decades, Kováč elaborated, updated, and expanded these themes in frequent publications, including \"Fundamental principles of cognitive biology\" (Kováč, 2000), “Life, chemistry, and cognition” (Kováč, 2006a), \"Information and Knowledge in Biology: Time for Reappraisal” (Kováč, 2007) and \"Bioenergetics: A key to brain and mind\" (Kováč, 2008).\n\nThe concept of cognitive biology is exemplified by this seminar description:\n\nThe University of Adelaide has established a \"Cognitive Biology\" workgroup using this operating concept:\nMembers of the group study the biological literature on simple organisms (e.g., nematode) in regard to cognitive process and look for homologues in more complex organisms (e.g., crow) already well studied. This comparative approach is expected to yield simple cognitive concepts common to all organisms. “It is hoped a theoretically well-grounded toolkit of basic cognitive concepts will facilitate the use and discussion of research carried out in different fields to increase understanding of two foundational issues: what cognition is and what cognition does in the biological context.” (Bold letters from original text.)\nThe group’s choice of name, as they explain on a separate webpage, might have been ‘embodied cognition’ or ‘biological cognitive science.’ But the group chose ‘cognitive biology’ for the sake of (i) emphasis and (ii) method. For the sake of emphasis, (i) “We want to keep the focus on biology because for too long cognition was considered a function that could be almost entirely divorced from its physical instantiation, to the extent that whatever could be said of cognition almost by definition had to be applicable to both organisms and machines.” (ii) The method is to “assume (if only for the sake of enquiry) that cognition is a biological function similar to other biological functions—such as respiration, nutrient circulation, waste elimination, and so on.”\nThe method supposes that the genesis of cognition is biological, i.e., the method is \"biogenic\". The host of the group’s website has said elsewhere that cognitive biology requires a biogenic approach, having identified ten principles of biogenesis in an earlier work. The first four biogenic principles are quoted here to illustrate the depth at which the foundations have been set at the Adelaide school of cognitive biology:\n\n\n\nThe words ‘cognitive’ and ‘biology’ are also used together as the name of a category. The category of \"cognitive biology\" has no fixed content but, rather, the content varies with the user. If the content can only be recruited from \"cognitive science\", then cognitive biology would seem limited to a selection of items in the main set of sciences included by the interdisciplinary concept—cognitive psychology, artificial intelligence, linguistics, philosophy, neuroscience, and cognitive anthropology. These six separate sciences were allied “to bridge the gap between brain and mind” with an interdisciplinary approach in the mid-1970s. Participating scientists were concerned only with human cognition. As it gained momentum, the growth of cognitive science in subsequent decades seemed to offer a big tent to a variety of researchers. Some, for example, considered evolutionary epistemology a fellow-traveler. Others appropriated the keyword, as for example Donald Griffin in 1978, when he advocated the establishment of cognitive ethology.\nMeanwhile, breakthroughs in molecular, cell, evolutionary, and developmental biology generated a cornucopia of data-based theory relevant to cognition. Categorical assignments were problematic. For example, the decision to append \"cognitive\" to a body of biological research on neurons, e.g. the cognitive biology of neuroscience, is separate from the decision to put such body of research in a category named cognitive sciences. No less difficult a decision needs be made—between the computational and constructivist approach to cognition, and the concomitant issue of simulated v. embodied cognitive models—before appending biology to a body of cognitive research, e.g. the cognitive science of artificial life.\nOne solution is to consider \"cognitive biology\" only as a subset of \"cognitive science\". For example, a major publisher’s website displays links to material in a dozen domains of major scientific endeavor. One of which is described thus: “Cognitive science is the study of how the mind works, addressing cognitive functions such as perception and action, memory and learning, reasoning and problem solving, decision-making and consciousness.” Upon its selection from the display, the \"Cognitive Science\" page offers in nearly alphabetical order these topics: \"Cognitive Biology\", Computer Science, Economics, Linguistics, Psychology, Philosophy, and Neuroscience. Linked through that list of topics, upon its selection the \"Cognitive Biology\" page offers a selection of reviews and articles with biological content ranging from cognitive ethology through evolutionary epistemology; cognition and art; evo-devo and cognitive science; animal learning; genes and cognition; cognition and animal welfare; etc.\n\nA different application of the \"cognitive biology\" category is manifest in the 2009 publication of papers presented at a three-day interdisciplinary workshop on “The New Cognitive Sciences” held at the Konrad Lorenz Institute for Evolution and Cognition Research in 2006. The papers were listed under four headings, each representing a different domain of requisite cognitive ability: (i) space, (ii) qualities and objects, (iii) numbers and probabilities, and (iv) social entities. The workshop papers examined topics ranging from “Animals as Natural Geometers” and “Color Generalization by Birds” through “Evolutionary Biology of Limited Attention” and “A comparative Perspective on the Origin of Numerical Thinking” as well as “Neuroethology of Attention in Primates” and ten more with less colorful titles. “[O]n the last day of the workshop the participants agreed [that] the title ‘Cognitive Biology’ sounded like a potential candidate to capture the merging of the cognitive and the life sciences that the workshop aimed at representing.” Thus the publication of Tommasi, et al. (2009), \"Cognitive Biology: Evolutionary and Developmental Perspectives on Mind, Brain and Behavior.\"\nA final example of categorical use comes from an author’s introduction to his 2011 publication on the subject, \"Cognitive Biology: Dealing with Information from Bacteria to Minds\". After discussing the differences between the cognitive and biological sciences, as well as the value of one to the other, the author concludes: “Thus, the object of this book should be considered as an attempt at building a new discipline, that of \"cognitive biology\", which endeavors to bridge these two domains.” There follows a detailed methodology illustrated by examples in biology anchored by concepts from cybernetics (e.g., self-regulatory systems) and quantum information theory (regarding probabilistic changes of state) with an invitation \"to consider system theory together with information theory as the formal tools that may ground biology and cognition as traditional mathematics grounds physics.”\n\n\n\n",
    "id": "42985608",
    "title": "Cognitive biology"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=41810627",
    "text": "Geroscience\n\nGeroscience is an interdisciplinary field that aims to understand, at the molecular level, the relationship between aging, age-related diseases and other conditions that diminish our quality of life. Because aging is the major risk factor for most common chronic diseases and conditions, an understanding of the role of aging in the onset and progression of disease should open up new avenues for disease prevention, amelioration, and cures. Geroscience research spans multiple disciplines including molecular biology, neuroscience, protein chemistry, cell biology, genetics, endocrinology, pharmacology, mathematics, and others. The common goal is to understand the role of aging in age-related loss of quality of life and susceptibility to disease.\n\nAge-related disease is arguably the single greatest threat to human health in the 21st century. Statistics from numerous sources highlight the fact that age-related diseases increasingly represent a true worldwide emergency: for example, by 2030 the national healthcare bill in the US is projected to reach four trillion dollars, with fully 50% of that being required for Americans 65 years and older (see statistics supplied by the National Institute on Aging and the Alliance for Aging Research).\n\nThe traditional approach in biomedical research is to investigate single-diseases and conditions in isolation. A more efficient approach would be investigating the cause(s) of aging itself. Treating aging (the major risk factor for most chronic conditions) the way we currently approach other medical conditions such as hypertension (the major risk factor for CVD), holds the possibility of delaying or preventing many diseases and conditions of later life as a group. Geroscience addresses a wide variety of disease states, including non-life-threatening conditions such as fatigability, resilience and frailty. As such, geroscience is broader in scope than traditional disciplines such as neurodegeneration, cancer biology, and geriatric medicine. It is also distinct from the traditional discipline of gerontology, which has large patient care and social sciences components.\n\nMany human clinical trials have failed in part due to an incomplete picture of the nature of complex chronic diseases of the elderly. The biology of aging field has developed spectacularly over the last twenty-five years, yet the potential impact of these new findings has yet to be achieved. The manipulation of longevity in simple laboratory animals by genetic modification or pharmacological interventions is now commonplace.\nHowever, a central concept of geroscience is that multiple human diseases arise from a common cause: aging itself. The term \"geroscience\" was coined by Buck Institute for Research on Aging scientist Gordon J. Lithgow, in 2007.[3] The scientific discipline was recognized in the U.S. Senate Appropriations on the FY2010 Senate Labor, Health and Human Services and Education Bill. The scientific discipline was recognized in the U.S. Senate Appropriations on the FY2010 Senate Labor, Health and Human Services and Education Bill.\n\nGeroscience was the organizing principle of the Interdisciplinary Research Consortium on Geroscience established at the Buck Institute. This Consortium was funded by the National Institutes of Health Common Fund. The IRC on Geroscience was also discussed in the Senate Report 110-527; quote \"The National Institutes of Health’s (NIH) Interdisciplinary Research Consortium in Geroscience fosters collaboration among biologists, biochemists, geneticists, physicians, physiologists, statisticians, and chemists that will help scientists to better understand age-related diseases and disorders. Examples include studies of the effects of diet on aging and why the aging brain recovers less easily from traumatic brain injury.\n\nIn 2011 the term \"geroscience\" was adopted by a trans-NIH team interested in the same concepts, the Geroscience Interest Group (GSIG). GSIG works with NIH and external stakeholders to promote geroscience research through activities that include workshops, seminars, and publications.\n\nThe NIH, with support from the Alliance for Aging Research and the Gerontological Society of America, hosted “Advances in Geroscience: Impacts on Healthspan and Chronic Disease” on October 30–31, 2013. This scientific conference, which took place on the NIH campus in Bethesda, MD, examined the extent to which the physiological effects of aging represent a common major risk factor for chronic diseases. A subsequent Summit, held in New York City under the auspices of the New York Academy of Sciences focused on the reverse side of the issue: attendees at the “Disease Drivers of Aging” meeting discussed how some chronic diseases and/or their treatments accelerate the aging process, thus rendering patients more vulnerable to additional diseases and ailments most commonly seen in elderly individuals.\n\n",
    "id": "41810627",
    "title": "Geroscience"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=436824",
    "text": "Gerontology\n\nGerontology is the study of the social, cultural, psychological, cognitive, and biological aspects of aging. The word was coined by Ilya Ilyich Mechnikov in 1903, from the Greek γέρων, \"geron\", \"old man\" and -λογία, \"-logia\", \"study of\". The field is distinguished from geriatrics, which is the branch of medicine that specializes in the treatment of existing disease in older adults. Gerontologists include researchers and practitioners in the fields of biology, nursing, medicine, criminology, dentistry, social work, physical and occupational therapy, psychology, psychiatry, sociology, economics, political science, architecture, geography, pharmacy, public health, housing, and anthropology. \n\nThe multidisciplinary nature of gerontology means that there are a number of sub-fields which overlap with gerontology. There are policy issues, for example, involved in government planning and the operation of nursing homes, investigating the effects of an ageing population on society, and the design of residential spaces for older people that facilitate the development of a sense of place or home. Dr. Lawton, a behavioral psychologist at the Philadelphia Geriatric Center, was among the first to recognize the need for living spaces designed to accommodate the elderly, especially those with Alzheimer's disease. As an academic discipline the field is relatively new. The USC Leonard Davis School created the first PhD, master’s and bachelor’s degree programs in gerontology in 1975.\n\nIn the medieval Islamic world, several physicians wrote on issues related to Gerontology. Avicenna's \"The Canon of Medicine\" (1025) offered instruction for the care of the aged, including diet and remedies for problems including constipation. Arabic physician Ibn Al-Jazzar Al-Qayrawani (Algizar, c. 898–980) wrote on the aches and conditions of the elderly (Ammar 1998, p. 4). His scholarly work covers sleep disorders, forgetfulness, how to strengthen memory, and causes of mortality Ishaq ibn Hunayn (died 910) also wrote works on the treatments for forgetfulness (U.S. National Library of Medicine, 1994).\n\nWhile the number of aged humans, and the life expectancy, tended to increase in every century since the 14th, society tended to consider caring for an elderly relative as a family issue. It was not until the coming of the Industrial Revolution that ideas shifted in favor of a societal care-system.\n\nSome early pioneers, such as Michel Eugène Chevreul, who himself lived to be 102, believed that aging itself should be a science to be studied. Élie Metchnikoff coined the term \"gerontology\" 1903\n\nIt was not until the 1940s that pioneers like James Birren began organizing gerontology into its own field. Recognizing that there were experts in many fields all dealing with the older population, it became apparent that a group like the Gerontological Society of America (founded in 1945) was needed. Two decades later, James Birren was appointed as the founding director of the first academic research center devoted exclusively to the study of aging, the Ethel Percy Andrus Gerontology Center. The Baltimore Longitudinal Studies of Aging began in 1958 in order to study physiological changes in healthy middle-aged and older men living in the community by testing them every two years on numerous physiological parameters. In 1967, the University of South Florida and the University of North Texas (formerly North Texas State University) received Older Americans Act training grants from the U.S. Administration on Aging to launch the nation's first degree programs in gerontology, at the master's level. In 1975, the University of Southern California's Leonard Davis School of Gerontology, became the country's first school of gerontology within a university and, later, offered the first PhD in Gerontology degree.\n\nGerontological Education has flourished in the United States since 1967 and degrees at all academic levels are now offered by a number of colleges and universities. One of the pioneering gerontologists, Robert Neil Butler, pushed for care and respect of the elderly. Several university-based centers on aging have been founded such as the Duke University Center on Aging, the University of Georgia Institute of Gerontology, the Center of Aging at the University of Chicago, and the Stanford Center on Longevity. Currently, PhD programs in gerontology are available at Miami University, the University of Kansas, University of Kentucky, University of Maryland Baltimore, University of Massachusetts Boston, and the University of Southern California.\n\nThe world is forecast to undergo rapid population aging in the next several decades. In 1900, there were 3.1 million people aged 65 years and older living in the United States. However, this population continued to grow throughout the 20th century and reached 31.2, 35, and 40.3 million people in 1990, 2000, and 2010, respectively. Notably, in the United States and across the world, the \"baby boomer\" generation began to turn 65 in 2011. Recently, the population aged 65 years and older has grown at a faster rate than the total population in the United States. The total population increased by 9.7%, from 281.4 million to 308.7 million, between 2000 and 2010. However, the population aged 65 years and older increased by 15.1% during the same period. It has been estimated that 25% of the population in the United States and Canada will be aged 65 years and older by 2025. Moreover, by 2050, it is predicted that, for the first time in United States history, the number of individuals aged 60 years and older will be greater than the number of children aged 0 to 14 years. Those aged 85 years and older (oldest-old) are projected to increase from 5.3 million to 21 million by 2050. Adults aged 85–89 years constituted the greatest segment of the oldest-old in 1990, 2000, and 2010. However, the largest percentage point increase among the oldest-old occurred in the 90- to 94-year-old age group, which increased from 25.0% in 1990 to 26.4% in 2010.\n\nWith the rapid growth of the aging population, social work education and training specialized in older adults and practitioners interested in working with older adults are increasingly in demand.\n\nThere has been a considerable disparity between the number of men and women in the older population in the United States. In both 2000 and 2010, women outnumbered men in the older population at every single year of age (e.g., 65 to 100 years and over). The sex ratio, which is a measure used to indicate the balance of males to females in a population, is calculated by taking the number of males divided by the number of females, and multiplying by 100. Therefore, the sex ratio is the number of males per 100 females. In 2010, there were 90.5 males per 100 females in the 65-year-old population. However, this represented an increase from 1990 when there were 82.7 males per 100 females, and from 2000 when the sex ratio was 88.1. Although the gender gap between men and women has narrowed, women continue to have a greater life expectancy and lower mortality rates at older ages relative to men. For example, the Census 2010 reported that there were approximately twice as many women as men living in the United States at 89 years of age (361,309 versus 176,689, respectively).\n\nThe number and percentage of older adults living in the United States vary across the four different regions (Northeast, Midwest, West, and South) defined by the United States census. In 2010, the South contained the greatest number of people aged 65 years and older and 85 years and older. However, proportionately, the Northeast contains the largest percentage of adults aged 65 years and older (14.1%), followed by the Midwest (13.5%), the South (13.0%), and the West (11.9%). Relative to the Census 2000, all geographic regions demonstrated positive growth in the population of adults aged 65 years and older and 85 years and older. The most rapid growth in the population of adults aged 65 years and older was evident in the West (23.5%), which showed an increase from 6.9 million in 2000 to 8.5 million in 2010. Likewise, in the population aged 85 years and older, the West (42.8%) also showed the fastest growth and increased from 806,000 in 2000 to 1.2 million in 2010. It is worth highlighting that Rhode Island was the only state that experienced a reduction in the number of people aged 65 years and older, and declined from 152,402 in 2000 to 151,881 in 2010. Conversely, all states exhibited an increase in the population of adults aged 85 years and older from 2000 to 2010.\n\nBiogerontology is the sub-field of gerontology concerned with the biological aging process, its evolutionary origins, and potential means to intervene in the process. It involves interdisciplinary research on biological aging's causes, effects, and mechanisms. Conservative biogerontologists such as Leonard Hayflick have predicted that the human life expectancy will peak at about 92 years old, while others such as James Vaupel have predicted that in industrialized countries, life expectancies will reach 100 for children born after the year 2000. and some surveyed biogerontologists have predicted life expectancies of two or more centuries. with Aubrey de Grey offering the \"tentative timeframe\" that with adequate funding of research to develop interventions in aging such as Strategies for Engineered Negligible Senescence, \"we have a 50/50 chance of developing technology within about 25 to 30 years from now that will, under reasonable assumptions about the rate of subsequent improvements in that technology, allow us to stop people from dying of aging at any age\", leading to life expectancies of 1,000 years.\n\nBiomedical gerontology, also known as experimental gerontology and life extension, is a sub-discipline of biogerontology that endeavors to slow, prevent, and even reverse aging in both humans and animals. Most \"life extensionists\" believe the human life span can be increased within the next century, if not sooner. Biogerontologists vary in the degree to which they focus on the study of the aging process as a means of mitigating the diseases of aging or extending lifespan, although most agree that extension of lifespan will necessarily flow from reductions in age-related disease and frailty, although some argue that maximum life span cannot be altered or that it is undesirable to try.\n\nIn contrast with biogerontology, geriatrics studies the treatment of disease in aging people.\n\nTheories of aging are numerous and no one theory has been accepted. There is a wide spectrum of the types of theories for the causes of aging with programmed theories on one extreme and error theories on the other. Regardless of the theory, a commonality is that as humans age, functions of the body decline.\n\nWear and tear theories of aging suggest that as an individual ages, body parts such as cells and organs wear out from continued use. Wearing of the body can be attributable to internal or external causes that eventually lead to an accumulation of insults which surpasses the capacity for repair. Due to these internal and external insults, cells lose their ability to regenerate, which ultimately leads to mechanical and chemical exhaustion. Some insults include chemicals in the air, food, or smoke. Other insults may be things such as viruses, trauma, free radicals, cross-linking, and high body temperature.\n\nGenetic theories of aging propose that aging is programmed within each individual's genes. According to this theory, genes dictate cellular longevity. Programmed cell death, or apoptosis, is determined by a \"biological clock\" via genetic information in the nucleus of the cell. Genes responsible for apoptosis provide an explanation for cell death, but are less applicable to death of an entire organism. An increase in cellular apoptosis may correlate to aging, but is not a 'cause of death'. Environmental factors and genetic mutations can influence gene expression and accelerate aging. More recently epigenetics have been explored as a contributing factor. The epigenetic clock, which objectively measures the biological age of cells and tissues, may become useful for testing different biological aging theories.\n\nGeneral imbalance theories of aging suggest that body systems, such as the endocrine, nervous, and immune systems, gradually decline and ultimately fail to function. The rate of failure varies system by system.\n\nAccumulation theories of aging suggest that aging is bodily decline that results from an accumulation of elements. Elements can be foreign and introduced to the body from the environment. Other elements can be the natural result of cell metabolism. An example of an accumulation theory is the Free Radical Theory of Aging. According to this theory, byproducts of regular cell metabolism called free radicals interact with cellular components such as the cell membrane and DNA and cause irreversible damage.\n\nThe idea that free radicals are toxic agents was first proposed by Rebeca Gerschman and colleagues. In 1956, Denham Harman proposed the free-radical theory of aging and even demonstrated that free radical reactions contribute to the degradation of biological systems. Oxidative damage of many types accumulate with age, such as oxidative stress that oxygen-free radicals, because the free radical theory of aging argues that aging results from the damage generated by reactive oxygen species (ROS). ROS are small, highly reactive, oxygen-containing molecules that can damage a complex of cellular components such as fat, proteins, or from DNA, they are naturally generated in small amounts during the body's metabolic reactions. These conditions become more common as we age, including diseases related to aging, such as dementia, cancer and heart disease.\n\nDNA damage has been one of the many causes in diseases related to aging. The stability of the genome is defined by the cells machinery of repair, damage tolerance, and checkpoint pathways that counteracts DNA damage. One hypothesis proposed by Gioacchino Failla in 1958 is that damage accumulation to the DNA causes aging. The hypothesis was developed soon by physicist Leó Szilárd. This theory has changed over the years as new research has discovered new types of DNA damage and mutations, and several theories of aging argue that DNA damage with or without mutations causes aging.\n\nSocial gerontology is a multi-disciplinary sub-field that specializes in studying or working with older adults. Social gerontologists may have degrees or training in social work, nursing, psychology, sociology, demography, public health, or other social science disciplines. Social gerontologists are responsible for educating, researching, and advancing the broader causes of older people.\n\nBecause issues of life span and life extension need numbers to quantify them, there is an overlap with demography. Those who study the demography of the human life span differ from those who study the social demographics of aging.\n\nSeveral theories of aging are developed to observed the aging process of older adults in society as well as how these processes are interpreted by men and women as they age.\n\nActivity theory was developed and elaborated by Cavan, Havighurst, and Albrecht. According to this theory, older adults' self-concept depends on social interactions. In order for older adults to maintain morale in old age, substitutions must be made for lost roles. Examples of lost roles include retirement from a job or loss of a spouse.\n\nActivity is preferable to inactivity because it facilitates well-being on multiple levels. Because of improved general health and prosperity in the older population, remaining active is more feasible now than when this theory was first proposed by Havighurst nearly six decades ago. The activity theory is applicable for a stable, post-industrial society, which offers its older members many opportunities for meaningful participation.Weakness: Some aging persons cannot maintain a middle-aged lifestyle, due to functional limitations, lack of income, or lack of a desire to do so. Many older adults lack the resources to maintain active roles in society. On the flip side, some elders may insist on continuing activities in late life that pose a danger to themselves and others, such as driving at night with low visual acuity or doing maintenance work to the house while climbing with severely arthritic knees. In doing so, they are denying their limitations and engaging in unsafe behaviors.\n\nDisengagement theory was developed by Cumming and Henry. According to this theory, older adults and society engage in a mutual separation from each other. An example of mutual separation is retirement from the workforce. A key assumption of this theory is that older adults lose \"ego-energy\" and become increasingly self-absorbed. Additionally, disengagement leads to higher morale maintenance than if older adults try to maintain social involvement. This theory is heavily criticized for having an escape clause - namely, that older adults who remain engaged in society are unsuccessful adjusters to old age.\n\nGradual withdrawal from society and relationships preserves social equilibrium and promotes self-reflection for elders who are freed from societal roles. It furnishes an orderly means for the transfer of knowledge, capital, and power from the older generation to the young. It makes it possible for society to continue functioning after valuable older members die.\n\nContinuity theory is an elusive concept. On the one hand,\nto exhibit continuity can mean to remain the same, to\nbe uniform, homogeneous, unchanging, even humdrum.\nThis static view of continuity is not very applicable\nto human aging. On the other hand, a dynamic\nview of continuity starts with the idea of a basic\nstructure which persists over time, but it allows for a\nvariety of changes to occur within the context provided\nby the basic structure. The basic structure is\ncoherent: It has an orderly or logical relation of parts\nthat is recognizably unique and that allows us to\ndifferentiate that structure from others. With the\nintroduction of the concept of time, ideas such as\ndirection, sequence, character development, and\nstory line enter into the concept of continuity as it is\napplied to the evolution of a human being. In this\ntheory, a dynamic concept of continuity is developed\nand applied to the issue of adaptation to normal\naging.\n\nA central premise of continuity theory is that, in\nmaking adaptive choices, middle-aged and older\nadults attempt to preserve and maintain existing internal\nand external structures and that they prefer to\naccomplish this objective by using continuity (i.e.,\napplying familiar strategies in familiar arenas of life).\nIn middle and later life, adults are drawn by the\nweight of past experience to use continuity as a\nprimary adaptive strategy for dealing with changes\nassociated with normal aging. To the extent that\nchange builds upon, and has links to, the person's\npast, change is a part of continuity. As a result of both\ntheir own perceptions and pressures from the social\nenvironment, individuals who are adapting to normal\naging are both predisposed and motivated\ntoward inner psychological continuity as well as outward\ncontinuity of social behavior and circumstances.\n\nContinuity theory views both internal and\nexternal continuity as robust adaptive strategies that\nare supported by both individual preference and\nsocial sanctions. Continuity theory consists of general\nadaptive principles that people who are normally \naging could be expected to follow, explanations of\nhow these principles work, and a specification of\ngeneral areas of life in which these principles could\nbe expected to apply. Accordingly, continuity theory\nhas enormous potential as a general theory of\nadaptation to individual aging.\n\nAccording to this theory, older adults born during different time periods form cohorts that define \"age strata\". There are two differences among strata: chronological age and historical experience. This theory makes two arguments. 1. Age is a mechanism for regulating behavior and as a result determines access to positions of power. 2. Birth cohorts play an influential role in the process of social change.\n\nAccording to this theory, which stems from the Life Course Perspective (Bengston and Allen, 1993), aging occurs from birth to death. Aging involves social, psychological, and biological processes. Additionally, aging experiences are shaped by cohort and period effects.\n\nAlso reflecting the life course focus,\nconsider the implications for how societies might function when age-based\nnorms vanish—a consequence of the deinstitutionalization of the life course—\nand suggest that these implications pose new challenges for theorizing aging\nand the life course in postindustrial societies. Dramatic reductions in mortality,\nmorbidity, and fertility over the past several decades have so shaken up the\norganization of the life course and the nature of educational, work, family, and\nleisure experiences that it is now possible for individuals to become old in new\nways. The configurations and content of other life stages are being altered as\nwell, especially for women. In consequence, theories of age and aging will need\nto be reconceptualized.\n\nAccording to this theory, which was developed beginning in the 1960s by Derek Price and Robert Merton and elaborated on by several researchers such as Dale Dannefer, inequalities have a tendency to become more pronounced throughout the aging process. A paradigm of this theory can be expressed in the adage \"the rich get richer and the poor get poorer\". Advantages and disadvantages in early life stages have a profound effect throughout the life span. However, advantages and disadvantages in middle adulthood have a direct influence on economic and health status in later life.\n\nEnvironmental gerontology is a specialization within gerontology that seeks to develop an understanding, means of analysis, opportunities to modify, and interventions that optimize the relationship between the aging person and their physical and social environment, utilizing interdisciplinary perspectives and approaches to attain these goals.\n\nThe field first began to emerge in the 1930s during the first studies on behavioral and social gerontology, and were associated with deterministic explanations of the relationship between aging and environment from genetic parameters and biological. In the 1970s and 1980s, theories of different researchers, as Kurt Lewin (living space model as a function of the person and the environment) and, mainly, M. Powell Lawton (1923–2001) (ecological adaptation model), based on the influence of the interactions between the older person and their environment (adaptation – ambient pressure), confirm the importance of the physical and social environment (objective and subjective) in the understanding of the aging population and the possibility of improving the quality of life in old age.\nEnvironmental Gerontology seeks to understand the socio-spatial implications of aging and its complex relationship with the environment, from analysis at different scales: micro scales (home and family) and macro scales (neighborhood, city, region), to enable social and environmental planning and policies that enable better aging. Among the main contributions of the discipline are the contributions to aging in place (wherever that place may be) and that older people prefer to age in their immediate environment where aspects such as spatial experience and place attachment are important for understanding the process.\n\nThe most famous environmental gerontologist remains M. Powell Lawton (1923–2001).\n",
    "id": "436824",
    "title": "Gerontology"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=48999251",
    "text": "Chemoproteomics\n\nChemoproteomics is an approach to discovering mechanisms for regulating biological pathways for the purpose of designing new pharmaceutical therapies. It is a chemical proteomic method for pharmacological discovery research.\n\nChemoproteomics can enable drug discovery. It is a method for identifying protein targets and potential off-target activity.\n\nA linker molecule connects a small molecule chemical probe (or library of chemical probes) with an analytically detectable bead or other agent for detection or separation.\n\n",
    "id": "48999251",
    "title": "Chemoproteomics"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=49926523",
    "text": "Idiobiology\n\nIdiobiology is a branch of biology which studies individual organisms, or the study of organisms as individuals.\n",
    "id": "49926523",
    "title": "Idiobiology"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=4985778",
    "text": "Forensic biology\n\nForensic biology is the application of biology to law enforcement.\n\nIt includes the subdisciplines of forensic anthropology, forensic botany, forensic entomology, forensic odontology, forensic toxicology and various DNA or protein based techniques.\n\nForensic biology has been used to prove a suspect was at a crime scene, identify illegal products from endangered species, solve crimes by matching crime scene evidence to suspects, investigate airplane bird strikes, and investigate bird collisions with wind turbines.\n\nForensic anthropology is for identification and recovery of remains. In extreme cases where conventional techniques are unable to determine the identity of the remains, anthropologists are sometimes able to deduce certain characteristics based on the skeletal remains. Race, sex, age and stature can often be determined by both measuring the remains and looking for structural clues in the bones.\n\nA Forensic botanist looks to plant life in order to gain information regarding possible crimes. Leaves, seeds and pollen found either on a body or at the scene of a crime can offer valuable information regarding the timescales of a crime and also if the body has been moved between two or more different locations. The forensic study of pollen is known as forensic palynology and can often produce specific findings of location of death, decomposition and time of year. The knowledge of systematics leads to identification of evidences at crime scene. The morphological and anatomical study revels in collection of samples from crime scene and its in vitro analysis. It leads to proper submission of evidences in court of law.\n\nBird remains can be identified, first and foremost from feathers (which are distinctive to a particular species at both macroscopic and microscopic levels).\n\nOdontologists or dentists can be used in order to aid in an identification of degraded remains. Remains that have been buried for a long period or which have undergone fire damage often contain few clues to the identity of the individual. Tooth enamel, as the hardest substance in the human body, often endures and as such odontologists can in some circumstances compare recovered remains to dental records.\n\nForensic toxicology is the use of toxicology and other disciplines such as analytical chemistry, pharmacology and clinical chemistry to aid medical or legal investigation of death, poisoning, and drug use. The primary concern for forensic toxicology is not the legal outcome of the toxicological investigation or the technology utilized, but rather the obtainment and interpretation of results. \n\nDNA-based evidence has become a significant tool that many law enforcement investigators now have at their disposal. DNA evidence can definitively link a suspect to either a crime scene or victim. Nuclear DNA evidence has been recovered from blood, semen, saliva, skin cells and hair. Furthermore, Mitochondrial DNA can be recovered from both bone and teeth dating back thousands of years. Laboratory analysis of DNA evidence generally involves the sample being amplified and quantified by a form of the Polymerase chain reaction known as Quantitative PCR or qPCR. (PCR) amplification of any sample recovered followed by sequencing via Capillary electrophoresis in order to obtain a DNA profile which can be compared to suspect DNA.\n\nDNA can also be extracted from animals and used to at least identify the species, for example bird or bat remains on an airplane or wind turbine.\n\nIn popular culture, forensic biology is frequently portrayed in shows like \"Law & Order\", \"Bones\", \"CSI\", \"Dexter\" and \"Castle\".\n\n\n",
    "id": "4985778",
    "title": "Forensic biology"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=48791",
    "text": "Pathology\n\nPathology (from the Greek roots of \"pathos\" (), meaning \"experience\" or \"suffering\" whence the English word \"path\" is derived by transliteration, and \"-logia\" (), \"study of\") is a significant component of the causal study of pathogens and a major field in modern medicine and diagnosis. Hence, 'the study of paths', by which disease comes. \n\nThe term pathology itself may be used broadly to refer to the study of disease in general, incorporating a wide range of bioscience research fields and medical practices (including plant pathology and veterinary pathology), or more narrowly to describe work within the contemporary medical field of \"general pathology,\" which includes a number of distinct but inter-related medical specialties that diagnose disease—mostly through analysis of tissue, cell, and body fluid samples. Used as a count noun, \"a pathology\" (plural, \"pathologies\") can also refer to the predicted or actual progression of particular diseases (as in the statement \"the many different forms of cancer have diverse pathologies\"), and the affix \"path\" is sometimes used to indicate a state of disease in cases of both physical ailment (as in cardiomyopathy) and psychological conditions (such as psychopathy). Similarly, a pathological condition is one caused by disease, rather than occurring physiologically. A physician practicing pathology is called a pathologist.\n\nAs a field of general inquiry and research, pathology addresses four components of disease: cause, mechanisms of development (pathogenesis), structural alterations of cells (morphologic changes), and the consequences of changes (clinical manifestations). In common medical practice, general pathology is mostly concerned with analyzing known clinical abnormalities that are markers or precursors for both infectious and non-infectious disease and is conducted by experts in one of two major specialties, anatomical pathology and clinical pathology. Further divisions in specialty exist on the basis of the involved sample types (comparing, for example, cytopathology, hematopathology, and histopathology), organs (as in renal pathology), and physiological systems (oral pathology), as well as on the basis of the focus of the examination (as with forensic pathology).\n\nThe sense of the word \"pathology\" as a synonym of \"disease\" or \"pathosis\" is very common in health care. The persistence of this usage despite attempted proscription is discussed elsewhere.\n\nThe study of pathology, including the detailed examination of the body, including dissection and inquiry into specific maladies, dates back to antiquity. Rudimentary understanding of many conditions was present in most early societies and is attested to in the records of the earliest historical societies, including those of the Middle East, India, and China. By the Hellenic period of ancient Greece, a concerted causal study of disease was underway (see Medicine in ancient Greece), with many notable early physicians (such as Hippocrates, for whom the modern Hippocratic Oath is named) having developed methods of diagnosis and prognosis for a number of diseases.The medical practices of the Romans and those of the Byzantines continued from these Greek roots, but, as with many areas of scientific inquiry, growth in understanding of medicine stagnated some after the Classical Era, but continued to slowly develop throughout numerous cultures. Notably, many advances were made in the medieval era of Islam (see Medicine in medieval Islam), during which numerous texts of complex pathologies were developed, also based on the Greek tradition. Even so, growth in complex understanding of disease mostly languished until knowledge and experimentation again began to proliferate in the Renaissance, Enlightenment, and Baroque eras, following the resurgence of the empirical method at new centers of scholarship. By the 17th century, the study of microscopy was underway and examination of tissues had led British Royal Society member Robert Hooke to coin the word \"cell\", setting the stage for later germ theory.\n\nModern medicine was particularly advanced by further developments of the microscope to analyze tissues, to which Rudolf Virchow gave a significant contribution, leading to a slew of research developments.\nBy the late 1920s to early 1930s pathology was deemed a medical specialty. Combined with developments in the understanding of general physiology, by the beginning of the 20th century, the study of pathology had begun to split into a number of rarefied fields and resulting in the development of large number of modern specialties within pathology and related disciplines of diagnostic medicine.\n\nThe modern practice of pathology is divided into a number of subdisciplines within the discrete but deeply interconnected aims of biological research and medical practice. Biomedical research into disease incorporates the work of a vast variety of life science specialists, whereas, in most parts of the world, to be licensed to practice pathology as medical specialty, one has to complete medical school and secure a license to practice medicine. Structurally, the study of disease is divided into many different fields that study or diagnose markers for disease using methods and technologies particular to specific scales, organs, and tissue types. The information in this section mostly concerns pathology as it regards common medical practice in these systems, but each of these specialties is also the subject of voluminous pathology research as regards the disease pathways of specific pathogens and disorders that affect the tissues of these discrete organs or structures. (See also Gross pathology).\n\nAnatomical pathology (\"Commonwealth\") or anatomic pathology (\"United States\") is a medical specialty that is concerned with the diagnosis of disease based on the gross, microscopic, chemical, immunologic and molecular examination of organs, tissues, and whole bodies (as in a general examination or an autopsy). Anatomical pathology is itself divided into subfields, the main divisions being surgical pathology, cytopathology, and forensic pathology. Anatomical pathology is one of two main divisions of the medical practice of pathology, the other being clinical pathology, the diagnosis of disease through the laboratory analysis of bodily fluids and tissues. Sometimes, pathologists practice both anatomical and clinical pathology, a combination known as general pathology.\n\nCytopathology (sometimes referred to as \"cytology\") is a branch of pathology that studies and diagnoses diseases on the cellular level. It is usually used to aid in the diagnosis of cancer, but also helps in the diagnosis of certain infectious diseases and other inflammatory conditions as well as thyroid lesions, diseases involving sterile body cavities (peritoneal, pleural, and cerebrospinal), and a wide range of other body sites. Cytopathology is generally used on samples of free cells or tissue fragments (in contrast to histopathology, which studies whole tissues) and cytopathologic tests are sometimes called smear tests because the samples may be smeared across a glass microscope slide for subsequent staining and microscopic examination. However, cytology samples may be prepared in other ways, including cytocentrifugation.\n\nDermatopathology is a subspecialty of anatomic pathology that focuses on the skin and the rest of the integumentary system as an organ. It is unique, in that there are two paths a physician can take to obtain the specialization. All general pathologists and general dermatologists train in the pathology of the skin, so the term dermatopathologist denotes either of these who has reached a certainly level of accreditation and experience; in the USA, either a general pathologist or a dermatologist can undergo a 1 to 2 year fellowship in the field of dermatopathology. The completion of this fellowship allows one to take a subspecialty board examination, and becomes a board certified dermatopathologist. Dermatologists are able to recognize most skin diseases based on their appearances, anatomic distributions, and behavior. Sometimes, however, those criteria do not lead to a conclusive diagnosis, and a skin biopsy is taken to be examined under the microscope using usual histological tests. In some cases, additional specialized testing needs to be performed on biopsies, including immunofluorescence, immunohistochemistry, electron microscopy, flow cytometry, and molecular-pathologic analysis. One of the greatest challenges of dermatopathology is its scope. More than 1500 different disorders of the skin exist, including cutaneous eruptions (\"rashes\") and neoplasms. Therefore, dermatopathologists must maintain a broad base of knowledge in clinical dermatology, and be familiar with several other specialty areas in Medicine.\n\nForensic pathology focuses on determining the cause of death by post-mortem examination of a corpse or partial remains. An autopsy is typically performed by a coroner or medical examiner, often during criminal investigations; in this role, coroners and medical examiners are also frequently asked to confirm the identity of a corpse. The requirements for becoming a licensed practitioner of forensic pathology varies from country to country (and even within a given nation) but typically a minimal requirement is a medical doctorate with a specialty in general or anatomical pathology with subsequent study in forensic medicine. The methods forensic scientists use to determine death include examination of tissue specimens to identify the presence or absence of natural disease and other microscopic findings, interpretations of toxicology on body tissues and fluids to determine the chemical cause of overdoses, poisonings or other cases involving toxic agents, and examinations of physical trauma. Forensic pathology is a major component in the trans-disciplinary field of forensic science.\n\nHistopathology refers to the microscopic examination of various forms of human tissue. Specifically, in clinical medicine, histopathology refers to the examination of a biopsy or surgical specimen by a pathologist, after the specimen has been processed and histological sections have been placed onto glass slides. This contrasts with the methods of cytopathology, which uses free cells or tissue fragments. Histopathological examination of tissues starts with surgery, biopsy, or autopsy. The tissue is removed from the body of an organism and then placed in a fixative that stabilizes the tissues to prevent decay. The most common fixative is formalin, although frozen section fixing is also common. To see the tissue under a microscope, the sections are stained with one or more pigments. The aim of staining is to reveal cellular components; counterstains are used to provide contrast. Histochemistry refers to the science of using chemical reactions between laboratory chemicals and components within tissue. The histological slides are then interpreted diagnostically and the resulting pathology report describes the histological findings and the opinion of the pathologist. In the case of cancer, this represents the tissue diagnosis required for most treatment protocols.\n\nNeuropathology is the study of disease of nervous system tissue, usually in the form of either surgical biopsies or sometimes whole brains in the case of autopsy. Neuropathology is a subspecialty of anatomic pathology, neurology, and neurosurgery. In many English-speaking countries, neuropathology is considered a subfield of anatomical pathology. A physician who specializes in neuropathology, usually by completing a fellowship after a residency in anatomical or general pathology, is called a neuropathologist. In day-to-day clinical practice, a neuropathologist is a consultant for other physicians. If a disease of the nervous system is suspected, and the diagnosis cannot be made by less invasive methods, a biopsy of nervous tissue is taken from the brain or spinal cord to aid in diagnosis. Biopsy is usually requested after a mass is detected by medical imaging. With autopsies, the principal work of the neuropathologist is to help in the post-mortem diagnosis of various conditions that affect the central nervous system. Biopsies can also consist of the skin. Epidermal nerve fiber density testing (ENFD) is a more recently developed neuropathology test in which a punch skin biopsy is taken to identify small fiber neuropathies by analyzing the nerve fibers of the skin. This test is becoming available in select labs as well as many universities; it replaces the traditional nerve biopsy test as less invasive.\n\nPulmonary pathology is a subspecialty of anatomic (and especially surgical) pathology that deals with diagnosis and characterization of neoplastic and non-neoplastic diseases of the lungs and thoracic pleura. Diagnostic specimens are often obtained via bronchoscopic transbronchial biopsy, CT-guided percutaneous biopsy, or video-assisted thoracic surgery. These tests can be necessary to diagnose between infection, inflammation, or fibrotic conditions.\nRenal pathology is a subspecialty of anatomic pathology that deals with the diagnosis and characterization of disease of the kidneys. In a medical setting, renal pathologists work closely with nephrologists and transplant surgeons, who typically obtain diagnostic specimens via percutaneous renal biopsy. The renal pathologist must synthesize findings from traditional microscope histology, electron microscopy, and immunofluorescence to obtain a definitive diagnosis. Medical renal diseases may affect the glomerulus, the tubules and interstitium, the vessels, or a combination of these compartments.\n\nSurgical pathology is one of the primary areas of practice for most anatomical pathologists. Surgical pathology involves the gross and microscopic examination of surgical specimens, as well as biopsies submitted by surgeons and non-surgeons such as general internists, medical subspecialists, dermatologists, and interventional radiologists. Often an excised tissue sample is the best and most definitive evidence of disease (or lack thereof) in cases where tissue is surgically removed from a patient. These determinations are usually accomplished by a combination of gross (i.e., macroscopic) and histologic (i.e., microscopic) examination of the tissue, and may involve evaluations of molecular properties of the tissue by immunohistochemistry or other laboratory tests.\nThere are two major types of specimens submitted for surgical pathology analysis: biopsies and surgical resections. A biopsy is a small piece of tissue removed primarily for surgical pathology analysis, most often in order to render a definitive diagnosis. Types of biopsies include core biopsies, which are obtained through the use of large-bore needles, sometimes under the guidance of radiological techniques such as ultrasound, CT scan, or magnetic resonance imaging. Incisional biopsies are obtained through diagnostic surgical procedures that remove part of a suspicious lesion, whereas excisional biopsies remove the entire lesion, and are similar to therapeutic surgical resections. Excisional biopsies of skin lesions and gastrointestinal polyps are very common. The pathologist's interpretation of a biopsy is critical to establishing the diagnosis of a benign or malignant tumor, and can differentiate between different types and grades of cancer, as well as determining the activity of specific molecular pathways in the tumor. Surgical resection specimens are obtained by the therapeutic surgical removal of an entire diseased area or organ (and occasionally multiple organs). These procedures are often intended as definitive surgical treatment of a disease in which the diagnosis is already known or strongly suspected, but pathological analysis of these specimens remains important in confirming the previous diagnosis.\n\nClinical pathology is a medical specialty that is concerned with the diagnosis of disease based on the laboratory analysis of bodily fluids such as blood and urine, as well as tissues, using the tools of chemistry, clinical microbiology, hematology and molecular pathology. Clinical pathologists work in close collaboration with medical technologists, hospital administrations, and referring physicians. Clinical pathologists learn to administer a number of visual and microscopic tests and an especially large variety of tests of the biophysical properties of tissue samples involving Automated analysers and cultures. Sometimes the general term \"laboratory medicine specialist\" is used to refer to those working in clinical pathology, including medical doctors, Ph.D.s and doctors of pharmacology. Immunopathology, the study of an organism's immune response to infection, is sometimes considered to fall within the domain of clinical pathology.\n\nHematopathology is the study of diseases of blood cells (including constituents such as white blood cells, red blood cells, and platelets) and the tissues, and organs comprising \nthe hematopoietic system. The term hematopoietic system refers to tissues and organs that produce and/or primarily host hematopoietic cells and includes bone marrow, the lymph nodes, thymus, spleen, and other lymphoid tissues. In the United States, hematopathology is a board certified subspecialty (licensed under the American Board of Pathology) practiced by those physicians who have completed a general pathology residency (anatomic, clinical, or combined) and an additional year of fellowship training in hematology. The hematopathologist reviews biopsies of lymph nodes, bone marrows and other tissues involved by an infiltrate of cells of the hematopoietic system. In addition, the hematopathologist may be in charge of flow cytometric and/or molecular hematopathology studies.\n\nImmunopathology is a branch of clinical pathology that deals with an organism’s immune response to a certain disease. When a foreign antigen enters the body, there is either an antigen specific or nonspecific response to it. These responses are the immune system fighting off the foreign antigens, whether they are deadly or not. Immunopathology could refer to how the foreign antigens cause the immune system to have a response or problems that can arise from an organism’s own immune response on itself. There are certain problems or faults in the immune system that can lead to more serious illness or disease. These diseases can come from one of the following problems. The first would be Hypersensitivity reactions, where there would be a stronger immune response than normal. There are four different types (type one, two, three and four), all with varying types and degrees of an immune response. The problems that arise from each type vary from small allergic reactions to more serious illnesses such as tuberculosis or arthritis. The second kind of complication in the immune system is Autoimmunity, where the immune system would attack itself rather than the antigen. Inflammation is a prime example of autoimmunity, as the immune cells used are self-reactive. A few examples of autoimmune diseases are Type 1 diabetes, Addison’s disease and Celiac disease. The third and final type of complication with the immune system is Immunodeficiency, where the immune system lacks the ability to fight off a certain disease. The immune system’s ability to combat it is either hindered or completely absent. The two types are Primary Immunodeficiency, where the immune system is either missing a key component or does not function properly, and Secondary Immunodeficiency, where disease is obtained from an outside source, like radiation or heat, and therefore cannot function properly. Diseases that can cause immunodeficiency include HIV, AIDS and leukemia. In all vertebrates, there are two different kinds of immune responses: Innate and Adaptive immunity. Innate immunity is used to fight off non-changing antigens and is therefore considered nonspecific. It is usually a more immediate response than the adaptive immune system, usually responding within minutes to hours. It is composed of physical blockades such as the skin, but also contains nonspecific immune cells such as dendritic cells, macrophages, T Cells, and basophils. The second for of immunity is Adaptive immunity. This form of immunity requires recognition of the foreign antigen before a response is produced. Once the antigen is recognized, a specific response is produced in order to destroy the specific antigen. Because of this idea, adaptive immunity is considered to be specific immunity. A key part of adaptive immunity that separates it from innate is the use of memory to combat the antigen in the future. When the antigen is originally introduced, the organism does not have any receptors for the antigen so it must generate them from the first time the antigen is present. The immune system then builds a memory of that antigen, which enables it to recognize the antigen quicker in the future and be able to combat it quicker and more efficiently. The more the system is exposed to the antigen, the quicker it will build up its responsiveness.\n\nRadiation Pathology is study of the interaction between human tissues and radiation, as long as the problems and diseases that can arise from the use of radiation. When human tissue is exposed to radiation, it can be genetically altered and deformed; in turn, this could lead to a variety of illnesses that could be minor or deadly.\n\nMolecular pathology is focused upon the study and diagnosis of disease through the examination of molecules within organs, tissues or bodily fluids. Molecular pathology is multidisciplinary by nature and shares some aspects of practice with both anatomic pathology and clinical pathology, molecular biology, biochemistry, proteomics and genetics. It is often applied in a context that is as much scientific as directly medical and encompasses the development of molecular and genetic approaches to the diagnosis and classification of human diseases, the design and validation of predictive biomarkers for treatment response and disease progression, and the susceptibility of individuals of different genetic constitution to particular disorders. The crossover between molecular pathology and epidemiology is represented by a related field \"molecular pathological epidemiology\".\n\nOral and Maxillofacial Pathology is one of nine dental specialties recognized by the American Dental Association, and is sometimes considered a specialty of both dentistry and pathology. Oral Pathologists must complete three years of post doctoral training in an accredited program and subsequently obtain diplomate status from the American Board of Oral and Maxillofacial Pathology. The specialty focuses on the diagnosis, clinical management and investigation of diseases that affect the oral cavity and surrounding maxillofacial structures including but not limited to odontogenic, infectious, epithelial, salivary gland, bone and soft tissue pathologies. It also significantly intersects with the field of dental pathology. Although concerned with a broad variety of diseases of the oral cavity, they have roles distinct from otorhinolaryngologists (\"ear, nose, and throat\" specialists), and speech pathologists, the latter of which helps diagnose many neurological or neuromuscular conditions relevant to speech phonology or swallowing. Owing to the availability of the oral cavity to non-invasive examination, many conditions in the study of oral disease can be diagnosed, or at least suspected, from gross examination, but biopsies, cell smears, and other tissue analysis remain important diagnostic tools in oral pathology.\n\nIndividual nations vary some in the medical licensing required of pathologists. In the United States, pathologists are physicians (D.O. or M.D.) that have completed a four-year undergraduate program, four years of medical school training, and three to four years of postgraduate training in the form of a pathology residency. Training may be within two primary specialties, as recognized by the American Board of Pathology: anatomical Pathology and clinical Pathology, each of which requires separate board certification. The American Osteopathic Board of Pathology also recognizes four primary specialties: anatomic pathology, dermatopathology, forensic pathology, and laboratory medicine. Pathologists may pursue specialised fellowship training within one or more subspecialties of either anatomical or clinical pathology. Some of these subspecialties permit additional board certification, while others do not.\nIn the United Kingdom, pathologists are physicians licensed by the UK General Medical Council. The training to become a pathologist is under the oversight of the Royal College of Pathologists. After four to six years of undergraduate medical study, trainees proceed to a two-year foundation program. Full-time training in histopathology currently lasts between five and five and a half years and includes specialist training in surgical pathology, cytopathology, and autopsy pathology. It is also possible to take a Royal College of Pathologists diploma in forensic pathology, dermatopathology, or cytopathology, recognising additional specialist training and expertise and to get specialist accreditation in forensic pathology, pediatric pathology, and neuropathology. All postgraduate medical training and education in the UK is overseen by the General Medical Council.\n\nIn France, Pathology is separate in two distinct specialties, anatomical pathology and clinical pathology. Residencies for both lasts four years. Residency in anatomical pathology is open to physicians only, while clinical pathology is open to both physicians and pharmacists. At the end of the second year of clinical pathology residency, residents can choose between general clinical pathology and a specialization in one of the disciplines, but they can not practice anatomical pathology, nor can anatomical pathology residents practice clinical pathology.\n\nThough separate fields in terms of medical practice, a number of areas of inquiry in medicine and\nmedical science either overlap greatly with general pathology, work in tandem with it, or contribute significantly to the understanding of the pathology of a given disease or its course in an individual. As a significant portion of all general pathology practice is concerned with cancer, the practice of oncology is deeply tied to, and dependent upon, the work of both anatomical and clinical pathologists. Biopsy, resection and blood tests are all examples of pathology work that is essential for the diagnoses of many kinds of cancer and for the staging of cancerous masses. In a similar fashion, the tissue and blood analysis techniques of general pathology are of central significance to the investigation of serious infectious disease and as such inform significantly upon the fields of epidemiology, etiology, immunology, and parasitology. General pathology methods are of great importance to biomedical research into disease, wherein they are sometimes referred to as \"experimental\" or \"investigative\" pathology.\n\nMedical imaging is the generating of visual representations of the interior of a body for clinical analysis and medical intervention. Medical imaging reveals details of internal physiology that help medical professionals plan appropriate treatments for tissue infection and trauma. Medical imaging is also central in supplying the biometric data necessary to establish baseline features of anatomy and physiology so as to increase the accuracy with which early or fine-detail abnormalities are detected. These diagnostic techniques are often performed in combination with general pathology procedures and are themselves often essential to developing new understanding of the pathogenesis of a given disease and tracking the progress of disease in specific medical cases. Examples of important subdivisions in medical imaging include radiology (which uses the imaging technologies of X-ray radiography) magnetic resonance imaging, medical ultrasonography (or ultrasound), endoscopy, elastography, tactile imaging, thermography, medical photography, nuclear medicine and functional imaging techniques such as positron emission tomography. Though they do not strictly relay images, readings from diagnostics tests involving electroencephalography, magnetoencephalography, and electrocardiography often give hints as to the state and function of certain tissues in the brain and heart respectively.\n\nPsychopathology is the study of mental illness, particularly of severe disorders. Informed heavily by both psychology and neurology, its purpose is to classify mental illness, elucidate its underlying causes, and guide clinical psychiatric treatment accordingly. Although diagnosis and classification of mental norms and disorders is largely the purview of psychiatry—the results of which are guidelines such as the Diagnostic and Statistical Manual of Mental Disorders, which attempt to classify mental disease mostly on behavioural evidence, though not without controversy—the field is also heavily, and increasingly, informed upon by neuroscience and other of the biological cognitive sciences. Mental or social disorders or behaviours seen as generally unhealthy or excessive in a given individual, to the point where they cause harm or severe disruption to the sufferer's lifestyle, are often called \"pathological\" (e.g., pathological gambling or pathological liar).\n\nAlthough the vast majority of lab work and research in pathology concerns the development of disease in humans, pathology is of significance throughout the biological sciences. Two main catch-all fields exist to represent most complex organisms capable of serving as host to a pathogen or other form of disease: veterinary pathology (concerned with all non-human species of kingdom of Animalia) and phytopathology, which studies disease in plants.\n\nVeterinary pathology covers a vast array of species, but with a significantly smaller number of practitioners, so understanding of disease in non-human animals, especially as regards veterinary practice, varies considerably by species. Nonetheless, significant amounts of pathology research are conducted on animals, for two primary reasons: 1) The origins of diseases are typically zoonotic in nature, and many infectious pathogens have animal vectors and, as such, understanding the mechanisms of action for these pathogens in non-human hosts is essential to the understanding and application of epidemiology and 2) those animals that share physiological and genetic traits with humans can be used as surrogates for the study of the disease and potential treatments as well as the effects of various synthetic products. For this reason, as well as their roles as livestock and companion animals, mammals generally have the largest body of research in veterinary pathology. Animal testing remains a controversial practice, even in cases where it is used to research treatment for human disease. As in human medical pathology, the practice of veterinary pathology is customarily divided into the two main fields of anatomical and clinical pathology.\n\nAlthough the pathogens and their mechanics differ greatly from those of animals, plants are subject to a wide variety of diseases, including those caused by fungi, oomycetes, bacteria, viruses, viroids, virus-like organisms, phytoplasmas, protozoa, nematodes and parasitic plants. Damage caused by insects, mites, vertebrate, and other small herbivores is not considered a part of the domain of plant pathology. The field is deeply connected to plant disease epidemiology and the horticulture of species that are of high importance to the human diet or other uses.\n\n\n\n",
    "id": "48791",
    "title": "Pathology"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=29462590",
    "text": "Mechanobiology\n\nMechanobiology is an emerging field of science at the interface of biology and engineering that focuses on how physical forces and changes in the mechanical properties of cells and tissues contribute to development, cell differentiation, physiology, and disease. A major challenge in the field is understanding mechanotransduction—the molecular mechanisms by which cells sense and respond to mechanical signals.\nWhile medicine has typically looked for the genetic and biochemical basis of disease, advances in mechanobiology suggest that changes in cell mechanics, extracellular matrix structure, or mechanotransduction may contribute to the development of many diseases, including atherosclerosis, fibrosis, asthma, osteoporosis, heart failure, and cancer. There is also a strong mechanical basis for many generalized medical disabilities, such as lower back pain, foot and postural injury, deformity, and irritable bowel syndrome.\n\nThe effectiveness of many of the mechanical therapies already in clinical use shows how important physical forces can be in physiological control. For example, pulmonary surfactant promotes lung development in premature infants; modifying the tidal volumes of mechanical ventilators reduces morbidity and death in patients with acute lung injury; expandable stents physically prevent coronary artery constriction; tissue expanders increase the skin area available for reconstructive surgery; and surgical tension application devices are used for bone fracture healing, orthodontics, cosmetic breast expansion and closure of non-healing wounds. \n\nInsights into the mechanical basis of tissue regulation may also lead to development of improved medical devices, biomaterials, and engineered tissues for tissue repair and reconstruction.\n\nStretch-activated ion channels, caveolae, integrins, cadherins, growth factor receptors, myosin motors, cytoskeletal filaments, nuclei, extracellular matrix, and numerous other molecular structures and signaling molecules have been shown to contribute to cellular mechanotransduction. In addition, endogenous cell-generated traction forces contribute significantly to these responses by modulating tensional prestress within cells, tissues, and organs that govern their mechanical stability, as well as mechanical signal transmission from the macroscale to the nanoscale.\n\nIt is claimed all cells are mechanosensitive. Collectively cells respond to perturbations to their local mechanical environment resulting in tissue-level observations. For example, at the tissue-level an artery will either thicken or thin in response to changes in blood pressure above or below the healthy levels. That is, in the case of increased blood pressure (Hypertension) individual arterial cells experience greater circumferential stress (or tension). In order to alleviate this tension, they produce growth factors, which in turn stimulates proliferation. The net result is increased arterial wall thickness but the stress levels in the artery are restored to the normal levels.\n\nUsing mankind as a macroscopic example, in closed chain function, ground reactive forces, a dynamic architecture and a dynamic equilibrium of forces around joint axes impact the posture to produce tissue stress. This tissue stress can be both beneficial or harmful. Since gravity, hard, unyielding ground surfaces and other factors such as activity level, body weight and health state impact each of us differently there is no one plan of care that will work for every individual. This results in a lifetime of adaptation of tissues via Wolff's and Davis' Laws of Bone and Soft Tissue respectively that can unless compensated and/or corrected lead to breakdown, injury and reduced quality of life on a case to case basis.\n\nAs a further example, the foot has an inherited functional shape which when used will remodel and adapt in predictable manners. Theoretically programs can be establishing for prevention, performance enhancement and quality of life upgrading in addition to the treatment of pathology and pain. These interventions, some day, will cause positive remodeling of bone and soft tissue that will extend and possibly improve the mechanobiological timeline of mankind.\n\n\n\n",
    "id": "29462590",
    "title": "Mechanobiology"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=34413",
    "text": "Zoology\n\nZoology () or animal biology is the branch of biology that studies the animal kingdom, including the structure, embryology, evolution, classification, habits, and distribution of all animals, both living and extinct, and how they interact with their ecosystems. The term is derived from Ancient Greek ζῷον, \"zōion\", i.e. \"animal\" and λόγος, \"logos\", i.e. \"knowledge, study\".\n\nThe history of zoology traces the study of the animal kingdom from ancient to modern times. Although the concept of \"zoology\" as a single coherent field arose much later, the zoological sciences emerged from natural history reaching back to the works of Aristotle and Galen in the ancient Greco-Roman world. This ancient work was further developed in the Middle Ages by Muslim physicians and scholars such as Albertus Magnus. During the Renaissance and early modern period, zoological thought was revolutionized in Europe by a renewed interest in empiricism and the discovery of many novel organisms. Prominent in this movement were Vesalius and William Harvey, who used experimentation and careful observation in physiology, and naturalists such as Carl Linnaeus and Buffon who began to classify the diversity of life and the fossil record, as well as the development and behavior of organisms. Microscopy revealed the previously unknown world of microorganisms, laying the groundwork for cell theory. The growing importance of natural theology, partly a response to the rise of mechanical philosophy, encouraged the growth of natural history (although it entrenched the argument from design).\n\nOver the 18th and 19th centuries, zoology became an increasingly professional scientific discipline. Explorer-naturalists such as Alexander von Humboldt investigated the interaction between organisms and their environment, and the ways this relationship depends on geography, laying the foundations for biogeography, ecology and ethology. Naturalists began to reject essentialism and consider the importance of extinction and the mutability of species. Cell theory provided a new perspective on the fundamental basis of life.\n\nThese developments, as well as the results from embryology and paleontology, were synthesized in Charles Darwin's theory of evolution by natural selection. In 1859, Darwin placed the theory of organic evolution on a new footing, by his discovery of a process by which organic evolution can occur, and provided observational evidence that it had done so.\n\nDarwin gave a new direction to morphology and physiology, by uniting them in a common biological theory: the theory of organic evolution. The result was a reconstruction of the classification of animals upon a genealogical basis, fresh investigation of the development of animals, and early attempts to determine their genetic relationships. The end of the 19th century saw the fall of spontaneous generation and the rise of the germ theory of disease, though the mechanism of inheritance remained a mystery. In the early 20th century, the rediscovery of Mendel's work led to the rapid development of genetics, and by the 1930s the combination of population genetics and natural selection in the modern synthesis created evolutionary biology.\n\nCell biology studies the structural and physiological properties of cells, including their behavior, interactions, and environment. This is done on both the microscopic and molecular levels, for single-celled organisms such as bacteria as well as the specialized cells in multicellular organisms such as humans. Understanding the structure and function of cells is fundamental to all of the biological sciences. The similarities and differences between cell types are particularly relevant to molecular biology.\n\nAnatomy considers the forms of macroscopic structures such as organs and organ systems. It focuses on how organs and organ systems work together in the bodies of humans and animals, in addition to how they work independently. Anatomy and cell biology are two studies that are closely related, and can be categorized under \"structural\" studies.\n\nPhysiology studies the mechanical, physical, and biochemical processes of living organisms by attempting to understand how all of the structures function as a whole. The theme of \"structure to function\" is central to biology. Physiological studies have traditionally been divided into plant physiology and animal physiology, but some principles of physiology are universal, no matter what particular organism is being studied. For example, what is learned about the physiology of yeast cells can also apply to human cells. The field of animal physiology extends the tools and methods of human physiology to non-human species. Physiology studies how for example nervous, immune, endocrine, respiratory, and circulatory systems, function and interact.\n\nEvolutionary research is concerned with the origin and descent of species, as well as their change over time, and includes scientists from many taxonomically oriented disciplines. For example, it generally involves scientists who have special training in particular organisms such as mammalogy, ornithology, herpetology, or entomology, but use those organisms as systems to answer general questions about evolution.\n\nEvolutionary biology is partly based on paleontology, which uses the fossil record to answer questions about the mode and tempo of evolution, and partly on the developments in areas such as population genetics and evolutionary theory. Following the development of DNA fingerprinting techniques in the late 20th century, the application of these techniques in zoology has increased the understanding of animal populations. In the 1980s, developmental biology re-entered evolutionary biology from its initial exclusion from the modern synthesis through the study of evolutionary developmental biology. Related fields often considered part of evolutionary biology are phylogenetics, systematics, and taxonomy.\n\nScientific classification in zoology, is a method by which zoologists group and categorize organisms by biological type, such as genus or species. Biological classification is a form of scientific taxonomy. Modern biological classification has its root in the work of Carl Linnaeus, who grouped species according to shared physical characteristics. These groupings have since been revised to improve consistency with the Darwinian principle of common descent. Molecular phylogenetics, which uses DNA sequences as data, has driven many recent revisions and is likely to continue to do so. Biological classification belongs to the science of zoological systematics.\nMany scientists now consider the five-kingdom system outdated. Modern alternative classification systems generally start with the three-domain system: Archaea (originally Archaebacteria); Bacteria (originally Eubacteria); Eukaryota (including protists, fungi, plants, and animals) These domains reflect whether the cells have nuclei or not, as well as differences in the chemical composition of the cell exteriors.\n\nFurther, each kingdom is broken down recursively until each species is separately classified. The order is:\nDomain; kingdom; phylum; class; order; family; genus; species. The scientific name of an organism is generated from its genus and species. For example, humans are listed as \"Homo sapiens\". \"Homo\" is the genus, and \"sapiens\" the specific epithet, both of them combined make up the species name. When writing the scientific name of an organism, it is proper to capitalize the first letter in the genus and put all of the specific epithet in lowercase. Additionally, the entire term may be italicized or underlined.\n\nThe dominant classification system is called the Linnaean taxonomy. It includes ranks and binomial nomenclature. The classification, taxonomy, and nomenclature of zoological organisms is administered by the International Code of Zoological Nomenclature. A merging draft, BioCode, was published in 1997 in an attempt to standardize nomenclature, but has yet to be formally adopted.\n\nEthology is the scientific and objective study of animal behavior under natural conditions, as opposed to behaviourism, which focuses on behavioral response studies in a laboratory setting. Ethologists have been particularly concerned with the evolution of behavior and the understanding of behavior in terms of the theory of natural selection. In one sense, the first modern ethologist was Charles Darwin, whose book, \"The Expression of the Emotions in Man and Animals,\" influenced many future ethologists.\n\nBiogeography studies the spatial distribution of organisms on the Earth, focusing on topics like plate tectonics, climate change, dispersal and migration, and cladistics. The creation of this study is widely accredited to Alfred Russel Wallace, a British biologist who had some of his work jointly published with Charles Darwin.\n\nAlthough the study of animal life is ancient, its scientific incarnation is relatively modern. This mirrors the transition from natural history to biology at the start of the 19th century. Since Hunter and Cuvier, comparative anatomical study has been associated with morphography, shaping the modern areas of zoological investigation: anatomy, physiology, histology, embryology, teratology and ethology. Modern zoology first arose in German and British universities. In Britain, Thomas Henry Huxley was a prominent figure. His ideas were centered on the morphology of animals. Many consider him the greatest comparative anatomist of the latter half of the 19th century. Similar to Hunter, his courses were composed of lectures and laboratory practical classes in contrast to the previous format of lectures only.\n\nGradually zoology expanded beyond Huxley's comparative anatomy to include the following sub-disciplines:\n\nRelated fields:\n\n\n",
    "id": "34413",
    "title": "Zoology"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=2787",
    "text": "Astrobiology\n\nAstrobiology is the study of the origin, evolution, distribution, and future of life in the universe: extraterrestrial life and life on Earth. Astrobiology addresses the question of whether life exists beyond Earth, and how humans can detect it if it does. The term exobiology is similar but more specific—it covers the search for life beyond Earth, and the effects of extraterrestrial environments on living things.\n\nAstrobiology makes use of physics, chemistry, astronomy, biology, molecular biology, ecology, planetary science, geography, and geology to investigate the possibility of life on other worlds and help recognize biospheres that might be different from that on Earth. The origin and early evolution of life is an inseparable part of the discipline of astrobiology. Astrobiology concerns itself with interpretation of existing scientific data, and although speculation is entertained to give context, astrobiology concerns itself primarily with hypotheses that fit firmly into existing scientific theories. Given more detailed and reliable data from other parts of the universe, the roots of astrobiology itself—physics, chemistry and biology—may have their theoretical bases challenged.\n\nThis interdisciplinary field encompasses research on the origin and evolution of planetary systems, origins of organic compounds in space, rock-water-carbon interactions, abiogenesis on Earth, planetary habitability, research on biosignatures for life detection, and studies on the potential for life to adapt to challenges on Earth and in outer space.\n\nThe chemistry of life may have begun shortly after the Big Bang, 13.8 billion years ago, during a habitable epoch when the Universe was only 10–17 million years old. According to the panspermia hypothesis, microscopic life—distributed by meteoroids, asteroids and other small Solar System bodies—may exist throughout the universe. According to research published in August 2015, very large galaxies may be more favorable to the creation and development of habitable planets than such smaller galaxies as the Milky Way. Nonetheless, Earth is the only place in the universe humans know to harbor life. Estimates of habitable zones around other stars, sometimes referred to as \"Goldilocks zones,\" along with the discovery of hundreds of extrasolar planets and new insights into extreme habitats here on Earth, suggest that there may be many more habitable places in the universe than considered possible until very recently.\n\nCurrent studies on the planet Mars by the \"Curiosity\" and \"Opportunity\" rovers are searching for evidence of ancient life as well as plains related to ancient rivers or lakes that may have been habitable. The search for evidence of habitability, taphonomy (related to fossils), and organic molecules on the planet Mars is now a primary NASA and ESA objective.\n\nThe term was first proposed by the Russian (Soviet) astronomer Gavriil Tikhov in 1953. \"Astrobiology\" is etymologically derived from the Greek , \"astron\", \"constellation, star\"; , \"bios\", \"life\"; and , \"-logia\", \"study\". The synonyms of astrobiology are diverse; however, the synonyms were structured in relation to the most important sciences implied in its development: astronomy and biology. A close synonym is \"exobiology\" from the Greek , \"external\"; Βίος, \"bios\", \"life\"; and λογία, -logia, \"study\". The term exobiology was coined by molecular biologist Joshua Lederberg. Exobiology is considered to have a narrow scope limited to search of life external to Earth, whereas subject area of astrobiology is wider and investigates the link between life and the universe, which includes the search for extraterrestrial life, but also includes the study of life on Earth, its origin, evolution and limits. \n\nAnother term used in the past is xenobiology, (\"biology of the foreigners\") a word used in 1954 by science fiction writer Robert Heinlein in his work The Star Beast. The term xenobiology is now used in a more specialized sense, to mean \"biology based on foreign chemistry\", whether of extraterrestrial or terrestrial (possibly synthetic) origin. Since alternate chemistry analogs to some life-processes have been created in the laboratory, xenobiology is now considered as an extant subject.\n\nWhile it is an emerging and developing field, the question of whether life exists elsewhere in the universe is a verifiable hypothesis and thus a valid line of scientific inquiry. Though once considered outside the mainstream of scientific inquiry, astrobiology has become a formalized field of study. Planetary scientist David Grinspoon calls astrobiology a field of natural philosophy, grounding speculation on the unknown, in known scientific theory. NASA's interest in exobiology first began with the development of the U.S. Space Program. In 1959, NASA funded its first exobiology project, and in 1960, NASA founded an Exobiology Program, which is now one of four main elements of NASA's current Astrobiology Program. In 1971, NASA funded the search for extraterrestrial intelligence (SETI) to search radio frequencies of the electromagnetic spectrum for interstellar communications transmitted by extraterrestrial life outside the Solar System. NASA's Viking missions to Mars, launched in 1976, included three biology experiments designed to look for metabolism of present life on Mars.\n\nAdvancements in the fields of astrobiology, observational astronomy and discovery of large varieties of extremophiles with extraordinary capability to thrive in the harshest environments on Earth, have led to speculation that life may possibly be thriving on many of the extraterrestrial bodies in the universe. A particular focus of current astrobiology research is the search for life on Mars due to this planet's proximity to Earth and geological history. There is a growing body of evidence to suggest that Mars has previously had a considerable amount of water on its surface, water being considered an essential precursor to the development of carbon-based life.\n\nMissions specifically designed to search for current life on Mars were the Viking program and Beagle 2 probes. The Viking results were inconclusive, and Beagle 2 failed minutes after landing. A future mission with a strong astrobiology role would have been the Jupiter Icy Moons Orbiter, designed to study the frozen moons of Jupiter—some of which may have liquid water—had it not been cancelled. In late 2008, the Phoenix lander probed the environment for past and present planetary habitability of microbial life on Mars, and researched the history of water there.\n\nIn November 2011, NASA launched the Mars Science Laboratory mission carrying the \"Curiosity\" rover, which landed on Mars at Gale Crater in August 2012. The \"Curiosity\" rover is currently probing the environment for past and present planetary habitability of microbial life on Mars. On 9 December 2013, NASA reported that, based on evidence from \"Curiosity\" studying Aeolis Palus, Gale Crater contained an ancient freshwater lake which could have been a hospitable environment for microbial life.\n\nThe European Space Agency is currently collaborating with the Russian Federal Space Agency (Roscosmos) and developing the ExoMars astrobiology rover, which is to be launched in 2018. Meanwhile, NASA is developing the Mars 2020 astrobiology rover and sample cacher for a later return to Earth.\n\nWhen looking for life on other planets like Earth, some simplifying assumptions are useful to reduce the size of the task of the astrobiologist. One is the informed assumption that the vast majority of life forms in our galaxy are based on carbon chemistries, as are all life forms on Earth. Carbon is well known for the unusually wide variety of molecules that can be formed around it. Carbon is the fourth most abundant element in the universe and the energy required to make or break a bond is at just the appropriate level for building molecules which are not only stable, but also reactive. The fact that carbon atoms bond readily to other carbon atoms allows for the building of extremely long and complex molecules.\n\nThe presence of liquid water is an assumed requirement, as it is a common molecule and provides an excellent environment for the formation of complicated carbon-based molecules that could eventually lead to the emergence of life. Some researchers posit environments of water-ammonia mixtures as possible solvents for hypothetical types of biochemistry.\n\nA third assumption is to focus on planets orbiting Sun-like stars for increased probabilities of planetary habitability. Very large stars have relatively short lifetimes, meaning that life might not have time to emerge on planets orbiting them. Very small stars provide so little heat and warmth that only planets in very close orbits around them would not be frozen solid, and in such close orbits these planets would be tidally \"locked\" to the star. The long lifetimes of red dwarfs could allow the development of habitable environments on planets with thick atmospheres. This is significant, as red dwarfs are extremely common. (See Habitability of red dwarf systems).\n\nSince Earth is the only planet known to harbor life, there is no evident way to know if any of these simplifying assumptions are correct.\n\nResearch on communication with extraterrestrial intelligence (CETI) focuses on composing and deciphering messages that could theoretically be understood by another technological civilization. Communication attempts by humans have included broadcasting mathematical languages, pictorial systems such as the Arecibo message and computational approaches to detecting and deciphering 'natural' language communication. The SETI program, for example, uses both radio telescopes and optical telescopes to search for deliberate signals from an extraterrestrial intelligence.\n\nWhile some high-profile scientists, such as Carl Sagan, have advocated the transmission of messages, scientist Stephen Hawking has warned against it, suggesting that aliens might simply raid Earth for its resources and then move on.\n\nMost astronomy-related astrobiology research falls into the category of extrasolar planet (exoplanet) detection, the hypothesis being that if life arose on Earth, then it could also arise on other planets with similar characteristics. To that end, a number of instruments designed to detect Earth-sized exoplanets have been considered, most notably NASA's Terrestrial Planet Finder (TPF) and ESA's Darwin programs, both of which have been cancelled. NASA launched the \"Kepler\" mission in March 2009, and the French Space Agency launched the COROT space mission in 2006. There are also several less ambitious ground-based efforts underway.\n\nThe goal of these missions is not only to detect Earth-sized planets, but also to directly detect light from the planet so that it may be studied spectroscopically. By examining planetary spectra, it would be possible to determine the basic composition of an extrasolar planet's atmosphere and/or surface. Given this knowledge, it may be possible to assess the likelihood of life being found on that planet. A NASA research group, the Virtual Planet Laboratory, is using computer modeling to generate a wide variety of virtual planets to see what they would look like if viewed by TPF or Darwin. It is hoped that once these missions come online, their spectra can be cross-checked with these virtual planetary spectra for features that might indicate the presence of life.\n\nAn estimate for the number of planets with intelligent \"communicative\" extraterrestrial life can be gleaned from the Drake equation, essentially an equation expressing the probability of intelligent life as the product of factors such as the fraction of planets that might be habitable and the fraction of planets on which life might arise:\nwhere:\n\nHowever, whilst the rationale behind the equation is sound, it is unlikely that the equation will be constrained to reasonable limits of error any time soon. The problem with the formula is that it is not usable to generate or support hypotheses because it contains factors that can never be verified. The first term, R*, number of stars, is generally constrained within a few orders of magnitude. The second and third terms, \"f\", stars with planets and \"f\", planets with habitable conditions, are being evaluated for the star's neighborhood. Drake originally formulated the equation merely as an agenda for discussion at the Green Bank conference, but some applications of the formula had been taken literally and related to simplistic or pseudoscientific arguments. Another associated topic is the Fermi paradox, which suggests that if intelligent life is common in the universe, then there should be obvious signs of it.\n\nAnother active research area in astrobiology is planetary system formation. It has been suggested that the peculiarities of the Solar System (for example, the presence of Jupiter as a protective shield) may have greatly increased the probability of intelligent life arising on our planet.\n\nBiology cannot state that a process or phenomenon, by being mathematically possible, has to exist forcibly in an extraterrestrial body. Biologists specify what is speculative and what is not.\n\nUntil the 1970s, life was thought to be entirely dependent on energy from the Sun. Plants on Earth's surface capture energy from sunlight to photosynthesize sugars from carbon dioxide and water, releasing oxygen in the process that is then consumed by oxygen-respiring organisms, passing their energy up the food chain. Even life in the ocean depths, where sunlight cannot reach, was thought to obtain its nourishment either from consuming organic detritus rained down from the surface waters or from eating animals that did. The world's ability to support life was thought to depend on its access to sunlight. However, in 1977, during an exploratory dive to the Galapagos Rift in the deep-sea exploration submersible \"Alvin\", scientists discovered colonies of giant tube worms, clams, crustaceans, mussels, and other assorted creatures clustered around undersea volcanic features known as black smokers. These creatures thrive despite having no access to sunlight, and it was soon discovered that they comprise an entirely independent ecosystem. Although most of these multicellular lifeforms need dissolved oxygen (produced by oxygenic photosynthesis) for their aerobic cellular respiration and thus are not completely independent from sunlight by themselves, the basis for their food chain is a form of bacterium that derives its energy from oxidization of reactive chemicals, such as hydrogen or hydrogen sulfide, that bubble up from the Earth's interior. Other lifeforms entirely decoupled from the energy from sunlight are green sulphur bacteria which are capturing geothermal light for anoxygenic photosynthesis or bacteria running chemolithoautotrophy based on the radioactive decay of uranium. This chemosynthesis revolutionized the study of biology and astrobiology by revealing that life need not be sun-dependent; it only requires water and an energy gradient in order to exist.\n\nExtremophiles, organisms able to survive in extreme environments, are a core research element for astrobiologists. Such organisms include biota which are able to survive several kilometers below the ocean's surface near hydrothermal vents and microbes that thrive in highly acidic environments. It is now known that extremophiles thrive in ice, boiling water, acid, alkali, the water core of nuclear reactors, salt crystals, toxic waste and in a range of other extreme habitats that were previously thought to be inhospitable for life. This opened up a new avenue in astrobiology by massively expanding the number of possible extraterrestrial habitats. Characterization of these organisms, their environments and their evolutionary pathways, is considered a crucial component to understanding how life might evolve elsewhere in the universe. For example, some organisms able to withstand exposure to the vacuum and radiation of outer space include the lichen fungi \"Rhizocarpon geographicum\" and \"Xanthoria elegans\", the bacterium \"Bacillus safensis\", \"Deinococcus radiodurans\", \"Bacillus subtilis\", yeast \"Saccharomyces cerevisiae\", seeds from \"Arabidopsis thaliana\" ('mouse-ear cress'), as well as the invertebrate animal Tardigrade. While Tardigrades are not considered true extremophiles, they are considered extremotolerant microorganisms that have contributed to the field of Astrobiology. Their extreme radiation tolerance and presence of DNA protection proteins may provide answers as to whether life can survive away from the protection of the Earth’s atmosphere.\n\nJupiter's moon, Europa, and Saturn's moon, Enceladus, are now considered the most likely locations for extant extraterrestrial life in the Solar System due to their subsurface water oceans where radiogenic and tidal heating enables liquid water to exist.\n\nThe origin of life, known as abiogenesis, distinct from the evolution of life, is another ongoing field of research. Oparin and Haldane postulated that the conditions on the early Earth were conducive to the formation of organic compounds from inorganic elements and thus to the formation of many of the chemicals common to all forms of life we see today. The study of this process, known as prebiotic chemistry, has made some progress, but it is still unclear whether or not life could have formed in such a manner on Earth. The alternative hypothesis of panspermia is that the first elements of life may have formed on another planet with even more favorable conditions (or even in interstellar space, asteroids, etc.) and then have been carried over to Earth—the panspermia hypothesis.\n\nThe cosmic dust permeating the universe contains complex organic compounds (\"amorphous organic solids with a mixed aromatic-aliphatic structure\") that could be created naturally, and rapidly, by stars. Further, a scientist suggested that these compounds may have been related to the development of life on Earth and said that, \"If this is the case, life on Earth may have had an easier time getting started as these organics can serve as basic ingredients for life.\" In September 2012, NASA scientists reported that polycyclic aromatic hydrocarbons (PAHs), subjected to interstellar medium conditions, are transformed through hydrogenation, oxygenation and hydroxylation, to more complex organics – \"a step along the path toward amino acids and nucleotides, the raw materials of proteins and DNA, respectively\".\n\nMore than 20% of the carbon in the universe may be associated with PAHs, possible starting materials for the formation of life. PAHs seem to have been formed shortly after the Big Bang, are widespread throughout the universe, and are associated with new stars and exoplanets.\n\nAstroecology concerns the interactions of life with space environments and resources, in planets, asteroids and comets. On a larger scale, astroecology concerns resources for life about stars in the galaxy through the cosmological future. Astroecology attempts to quantify future life in space, addressing this area of astrobiology.\n\nExperimental astroecology investigates resources in planetary soils, using actual space materials in meteorites. The results suggest that Martian and carbonaceous chondrite materials can support bacteria, algae and plant (asparagus, potato) cultures, with high soil fertilities. The results support that life could have survived in early aqueous asteroids and on similar materials imported to Earth by dust, comets and meteorites, and that such asteroid materials can be used as soil for future space colonies.\n\nOn the largest scale, cosmoecology concerns life in the universe over cosmological times. The main sources of energy may be red giant stars and white and red dwarf stars, sustaining life for 10 years. Astroecologists suggest that their mathematical models may quantify the potential amounts of future life in space, allowing a comparable expansion in biodiversity, potentially leading to diverse intelligent life forms.\n\nAstrogeology is a planetary science discipline concerned with the geology of celestial bodies such as the planets and their moons, asteroids, comets, and meteorites. The information gathered by this discipline allows the measure of a planet's or a natural satellite's potential to develop and sustain life, or planetary habitability.\n\nAn additional discipline of astrogeology is geochemistry, which involves study of the chemical composition of the Earth and other planets, chemical processes and reactions that govern the composition of rocks and soils, the cycles of matter and energy and their interaction with the hydrosphere and the atmosphere of the planet. Specializations include cosmochemistry, biochemistry and organic geochemistry.\n\nThe fossil record provides the oldest known evidence for life on Earth. By examining the fossil evidence, paleontologists are able to better understand the types of organisms that arose on the early Earth. Some regions on Earth, such as the Pilbara in Western Australia and the McMurdo Dry Valleys of Antarctica, are also considered to be geological analogs to regions of Mars, and as such, might be able to provide clues on how to search for past life on Mars.\n\nThe various organic functional groups, composed of hydrogen, oxygen, nitrogen, phosphorus, sulfur, and a host of metals, such as iron, magnesium, and zinc, provide the enormous diversity of chemical reactions necessarily catalyzed by a living organism. Silicon, in contrast, interacts with only a few other atoms, and the large silicon molecules are monotonous compared with the combinatorial universe of organic macromolecules. Indeed, it seems likely that the basic building blocks of life anywhere will be similar those on Earth, in the generality if not in the detail. Although terrestrial life and life that might arise independently of Earth are expected to use many similar, if not identical, building blocks, they also are expected to have some biochemical qualities that are unique. If life has had a comparable impact elsewhere in the Solar System, the relative abundances of chemicals key for its survival – whatever they may be – could betray its presence. Whatever extraterrestrial life may be, its tendency to chemically alter its environment might just give it away.\n\nPeople have long speculated about the possibility of life in settings other than Earth, however, speculation on the nature of life elsewhere often has paid little heed to constraints imposed by the nature of biochemistry. The likelihood that life throughout the universe is probably carbon-based is suggested by the fact that carbon is one of the most abundant of the higher elements. Only two of the natural atoms, carbon and silicon, are known to serve as the backbones of molecules sufficiently large to carry biological information. As the structural basis for life, one of carbon's important features is that unlike silicon, it can readily engage in the formation of chemical bonds with many other atoms, thereby allowing for the chemical versatility required to conduct the reactions of biological metabolism and propagation.\n\nThought on where in the Solar System life might occur, was limited historically by the understanding that life relies ultimately on light and warmth from the Sun and, therefore, is restricted to the surfaces of planets. The three most likely candidates for life in the Solar System are the planet Mars, the Jovian moon Europa, and Saturn's moons Titan, and Enceladus.\n\nMars, Enceladus and Europa are considered likely candidates in the search for life primarily because they may have underground liquid water, a molecule essential for life as we know it for its use as a solvent in cells. Water on Mars is found frozen in its polar ice caps, and newly carved gullies recently observed on Mars suggest that liquid water may exist, at least transiently, on the planet's surface. At the Martian low temperatures and low pressure, liquid water is likely to be highly saline. As for Europa, liquid water likely exists beneath the moon's icy outer crust. This water may be warmed to a liquid state by volcanic vents on the ocean floor, but the primary source of heat is probably tidal heating. On 11 December 2013, NASA reported the detection of \"clay-like minerals\" (specifically, phyllosilicates), often associated with organic materials, on the icy crust of Europa. The presence of the minerals may have been the result of a collision with an asteroid or comet according to the scientists.\n\nAnother planetary body that could potentially sustain extraterrestrial life is Saturn's largest moon, Titan. Titan has been described as having conditions similar to those of early Earth. On its surface, scientists have discovered the first liquid lakes outside Earth, but these lakes seem to be composed of ethane and/or methane, not water. Some scientists think it possible that these liquid hydrocarbons might take the place of water in living cells different from those on Earth. After Cassini data was studied, it was reported on March 2008 that Titan may also have an underground ocean composed of liquid water and ammonia. Additionally, Saturn's moon Enceladus may have an ocean below its icy surface and, according to NASA scientists in May 2011, \"is emerging as the most habitable spot beyond Earth in the Solar System for life as we know it\".\n\nMeasuring the ratio of hydrogen and methane levels on Mars may help determine the likelihood of life on Mars. According to the scientists, \"...low H/CH ratios (less than approximately 40) indicate that life is likely present and active.\" Other scientists have recently reported methods of detecting hydrogen and methane in extraterrestrial atmospheres.\n\nComplex organic compounds of life, including uracil, cytosine and thymine, have been formed in a laboratory under outer space conditions, using starting chemicals such as pyrimidine, found in meteorites. Pyrimidine, like polycyclic aromatic hydrocarbons (PAHs), is the most carbon-rich chemical found in the universe.\n\nThe Rare Earth hypothesis postulates that multicellular life forms found on Earth may actually be more of a rarity than scientists assume. It provides a possible answer to the Fermi paradox which suggests, \"If extraterrestrial aliens are common, why aren't they obvious?\" It is apparently in opposition to the principle of mediocrity, assumed by famed astronomers Frank Drake, Carl Sagan, and others. The Principle of Mediocrity suggests that life on Earth is not exceptional, but rather that life is more than likely to be found on innumerable other worlds.\n\nThe anthropic principle states that fundamental laws of the universe work specifically in a way that life would be possible. The anthropic principle supports the Rare Earth Hypothesis by arguing the overall elements that are needed to support life on Earth are so fine-tuned that it is nearly impossible for another just like it to exist by random chance.\n\nThe systematic search for possible life outside Earth is a valid multidisciplinary scientific endeavor. However, hypotheses and predictions as to its existence and origin vary widely, and at the present, the development of hypotheses firmly grounded on science may be considered astrobiology's most concrete practical application. It has been proposed that viruses are likely to be encountered on other life-bearing planets.\n\n, no evidence of extraterrestrial life has been identified. Examination of the Allan Hills 84001 meteorite, which was recovered in Antarctica in 1984 and originated from Mars, is thought by David McKay, as well as few other scientists, to contain microfossils of extraterrestrial origin; this interpretation is controversial.\n\nYamato 000593, the second largest meteorite from Mars, was found on Earth in 2000. At a microscopic level, spheres are found in the meteorite that are rich in carbon compared to surrounding areas that lack such spheres. The carbon-rich spheres may have been formed by biotic activity according to some NASA scientists.\n\nOn 5 March 2011, Richard B. Hoover, a scientist with the Marshall Space Flight Center, speculated on the finding of alleged microfossils similar to cyanobacteria in CI1 carbonaceous meteorites in the fringe \"Journal of Cosmology\", a story widely reported on by mainstream media. However, NASA formally distanced itself from Hoover's claim. According to American astrophysicist Neil deGrasse Tyson: \"At the moment, life on Earth is the only known life in the universe, but there are compelling arguments to suggest we are not alone.\"\n\n\nOn 17 March 2013, researchers reported that microbial life forms thrive in the Mariana Trench, the deepest spot on the Earth. Other researchers reported that microbes thrive inside rocks up to below the sea floor under of ocean off the coast of the northwestern United States. According to one of the researchers, \"You can find microbes everywhere—they're extremely adaptable to conditions, and survive wherever they are.\" These finds expand the potential habitability of certain niches of other planets.\n\nIn 2004, the spectral signature of methane () was detected in the Martian atmosphere by both Earth-based telescopes as well as by the Mars Express orbiter. Because of solar radiation and cosmic radiation, methane is predicted to disappear from the Martian atmosphere within several years, so the gas must be actively replenished in order to maintain the present concentration. The \"Curiosity\" rover will perform precision measurements of oxygen and carbon isotope ratios in carbon dioxide (CO) and methane (CH) in the atmosphere of Mars in order to distinguish between a geochemical and a biological origin.\n\nIt is possible that some exoplanets may have moons with solid surfaces or liquid oceans that are hospitable. Most of the planets so far discovered outside the Solar System are hot gas giants thought to be inhospitable to life, so it is not yet known whether the Solar System, with a warm, rocky, metal-rich inner planet such as Earth, is of an aberrant composition. Improved detection methods and increased observation time will undoubtedly discover more planetary systems, and possibly some more like ours. For example, NASA's Kepler Mission seeks to discover Earth-sized planets around other stars by measuring minute changes in the star's light curve as the planet passes between the star and the spacecraft. Progress in infrared astronomy and submillimeter astronomy has revealed the constituents of other star systems.\n\n\nEfforts to answer questions such as the abundance of potentially habitable planets in habitable zones and chemical precursors have had much success. Numerous extrasolar planets have been detected using the wobble method and transit method, showing that planets around other stars are more numerous than previously postulated. The first Earth-sized extrasolar planet to be discovered within its star's habitable zone is Gliese 581 c.\n\nResearch into the environmental limits of life and the workings of extreme ecosystems is ongoing, enabling researchers to better predict what planetary environments might be most likely to harbor life. Missions such as the \"Phoenix\" lander, Mars Science Laboratory, ExoMars, Mars 2020 rover to Mars, and the \"Cassini\" probe to Saturn's moons aim to further explore the possibilities of life on other planets in the Solar System.\n\nThe two Viking landers each carried four types of biological experiments to the surface of Mars in the late 1970s. These were the only Mars landers to carry out experiments looking specifically for metabolism by current microbial life on Mars. The landers used a robotic arm to collect soil samples into sealed test containers on the craft. The two landers were identical, so the same tests were carried out at two places on Mars' surface; Viking 1 near the equator and Viking 2 further north. The result was inconclusive, and is still disputed by some scientists.\n\n\"Beagle 2\" was an unsuccessful British Mars lander that formed part of the European Space Agency's 2003 Mars Express mission. Its primary purpose was to search for signs of life on Mars, past or present. Although it landed safely, it was unable to correctly deploy its solar panels and telecom antenna.\n\nEXPOSE is a multi-user facility mounted in 2008 outside the International Space Station dedicated to astrobiology. EXPOSE was developed by the European Space Agency (ESA) for long-term spaceflights that allow exposure of organic chemicals and biological samples to outer space in low Earth orbit.\n\nThe Mars Science Laboratory (MSL) mission landed the \"Curiosity\" rover that is currently in operation on Mars. It was launched 26 November 2011, and landed at Gale Crater on 6 August 2012. Mission objectives are to help assess Mars' habitability and in doing so, determine whether Mars is or has ever been able to support life, collect data for a future human mission, study Martian geology, its climate, and further assess the role that water, an essential ingredient for life as we know it, played in forming minerals on Mars.\n\nThe \"Tanpopo\" mission is an orbital astrobiology experiment investigating the potential interplanetary transfer of life, organic compounds, and possible terrestrial particles in the low Earth orbit. The purpose is to assess the panspermia hypothesis and the possibility of natural interplanetary transport of microbial life as well as prebiotic organic compounds. Early mission results show evidence that some clumps of microorganism can survive for at least one year in space. This may support the idea that clumps greater than 0.5 millimeters of microorganisms could be one way for life to spread from planet to planet.\n\n\n\"ExoMars rover\" is a robotic mission to Mars to search for possible biosignatures of Martian life, past or present. This astrobiological mission is currently under development by the European Space Agency (ESA) in partnership with the Russian Federal Space Agency (Roscosmos); it is planned for a 2018 launch.\n\n\"Mars 2020\" rover mission is under development by NASA for a launch in 2020. It will investigate environments on Mars relevant to astrobiology, investigate its surface geological processes and history, including the assessment of its past habitability and potential for preservation of biosignatures and biomolecules within accessible geological materials. The Science Definition Team is proposing the rover collect and package at least 31 samples of rock cores and soil for a later mission to bring back for more definitive analysis in laboratories on Earth. The rover could make measurements and technology demonstrations to help designers of a human expedition understand any hazards posed by Martian dust and demonstrate how to collect carbon dioxide (CO), which could be a resource for making molecular oxygen (O) and rocket fuel.\n\n\"Europa Clipper\" is a mission planned by NASA for a 2025 launch that will conduct detailed reconnaissance of Jupiter's moon Europa and will investigate whether the icy moon could harbor conditions suitable for life. It will also aid in the selection of future landing sites.\n\n\"Icebreaker Life\" is a lander mission that proposed for NASA's Discovery Program for the 2021 launch opportunity, but it was not selected for development. It would have had a stationary lander that would be a near copy of the successful 2008 \"Phoenix\" and it would have carried an upgraded astrobiology scientific payload, including a 1-meter-long core drill to sample ice-cemented ground in the northern plains to conduct a search for organic molecules and evidence of current or past life on Mars. One of the key goals of the \"Icebreaker Life\" mission is to test the hypothesis that the ice-rich ground in the polar regions has significant concentrations of organics due to protection by the ice from oxidants and radiation.\n\n\"Journey to Enceladus and Titan\" (\"JET\") is an astrobiology mission concept to assess the habitability potential of Saturn's moons Enceladus and Titan by means an orbiter.\n\n\"Enceladus Life Finder\" (\"ELF\") is a proposed astrobiology mission concept for a space probe intended to assess the habitability of the internal aquatic ocean of Enceladus, Saturn's sixth-largest moon.\n\n\"Life Investigation For Enceladus\" (\"LIFE\") is a proposed astrobiology sample-return mission concept. The spacecraft would enter into Saturn orbit and enable multiple flybys through Enceladus' icy plumes to collect icy plume particles and volatiles and return them to Earth on a capsule. The spacecraft may sample Enceladus' plumes, the E ring of Saturn, and the upper atmosphere of Titan.\n\n\"Oceanus\" is an orbiter proposed in 2017 for the New Frontiers mission #4. It would travel to the moon of Saturn, Titan, to assess its habitability. \"Oceanus\" objectives are to reveal Titan's organic chemistry, geology, gravity, topography, collect 3D reconnaissance data, catalog the organics and determine where they may interact with liquid water.\n\n\"Explorer of Enceladus and Titan\" (ET) is an orbiter mission concept that would investigate the evolution and habitability of the Saturnian satellites Enceladus and Titan and was proposed in 2017 by the European Space Agency.\n\n\n\n",
    "id": "2787",
    "title": "Astrobiology"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1663537",
    "text": "Allometry\n\nAllometry is the study of the relationship of body size to shape, anatomy, physiology and finally behaviour, first outlined by Otto Snell in 1892, by D'Arcy Thompson in 1917 in \"On Growth and Form\" and by Julian Huxley in 1932.\n\nAllometry is a well-known study, particularly in statistical shape analysis for its theoretical developments, as well as in biology for practical applications to the differential growth rates of the parts of a living organism's body. One application is in the study of various insect species (e.g., Hercules beetles), where a small change in overall body size can lead to an enormous and disproportionate increase in the dimensions of appendages such as legs, antennae, or horns The relationship between the two measured quantities is often expressed as a power law equation which expresses a remarkable scale symmetry:\n\nor in a logarithmic form:\n\nwhere formula_3 is the scaling exponent of the law. Methods for estimating this exponent from data can use type-2 regressions, such as major axis regression or reduced major axis regression, as these account for the variation in both variables, contrary to least squares regression, which does not account for error variance in the independent variable (e.g., log body mass). Other methods include measurement-error models and a particular kind of principal component analysis.\n\nAllometry often studies shape differences in terms of ratios of the objects' dimensions. Two objects of different size, but common shape, will have their dimensions in the same ratio. Take, for example, a biological object that grows as it matures. Its size changes with age, but the shapes are similar. Studies of ontogenetic allometry often use lizards or snakes as model organisms both because they lack parental care after birth or hatching and because they exhibit a large range of body sizes between the juvenile and adult stage. Lizards often exhibit allometric changes during their ontogeny.\n\nIn addition to studies that focus on growth, allometry also examines shape variation among individuals of a given age (and sex), which is referred to as static allometry. Comparisons of species are used to examine interspecific or evolutionary allometry (see also: Phylogenetic comparative methods).\n\nIsometric scaling happens when proportional relationships are preserved as size changes during growth or over evolutionary time. An example is found in frogs — aside from a brief period during the few weeks after metamorphosis, frogs grow isometrically. Therefore, a frog whose legs are as long as its body will retain that relationship throughout its life, even if the frog itself increases in size tremendously.\n\nIsometric scaling is governed by the square-cube law. An organism which doubles in length isometrically will find that the surface area available to it will increase fourfold, while its volume and mass will increase by a factor of eight. This can present problems for organisms. In the case of above, the animal now has eight times the biologically active tissue to support, but the surface area of its respiratory organs has only increased fourfold, creating a mismatch between scaling and physical demands. Similarly, the organism in the above example now has eight times the mass to support on its legs, but the strength of its bones and muscles is dependent upon their cross-sectional area, which has only increased fourfold. Therefore, this hypothetical organism would experience twice the bone and muscle loads of its smaller version. This mismatch can be avoided either by being \"overbuilt\" when small or by changing proportions during growth, called allometry.\n\nIsometric scaling is often used as a null hypothesis in scaling studies, with 'deviations from isometry' considered evidence of physiological factors forcing allometric growth.\n\nAllometric scaling is any change that deviates from isometry. A classic example discussed by Galileo in his \"Dialogues Concerning Two New Sciences\" is the skeleton of mammals. The skeletal structure becomes much stronger and more robust relative to the size of the body as the body size increases. Allometry is often expressed in terms of a scaling exponent based on body mass, or body length (Snout-vent length, total length etc.). A perfectly isometrically scaling organism would see all volume-based properties change proportionally to the body mass, all surface area-based properties change with mass to the power 2/3, and all length-based properties change with mass to the 1/3 power. If, after statistical analyses, for example, a volume-based property was found to scale to mass to the 0.9 power, then this would be called \"negative allometry\", as the values are smaller than predicted by isometry. Conversely, if a surface area-based property scales to mass to the 0.8 power, the values are higher than predicted by isometry and the organism is said to show \"positive allometry\". One example of positive allometry occurs among species of monitor lizards (family Varanidae), in which the limbs are relatively longer in larger-bodied species. The same is true for some fish, e.g. the muskellunge, the weight of which grows with about the power of 3.325 of its length. A muskellunge will weigh about , while a muskellunge will weigh about , as 33% longer while more than double the weight.\n\nTo determine whether isometry or allometry is present, an expected relationship between variables needs to be determined to compare data to. This is important in determining if the scaling relationship in a dataset deviates from an expected relationship (such as those that follow isometry). The use of tools such as dimensional analysis is very helpful in determining expected slope. This ‘expected’ slope, as it is known, is essential for detecting allometry because scaling variables are comparisons to other things. Saying that mass scales with a slope of 5 in relation to length doesn’t have much meaning unless knowing the isometric slope is 3, meaning in this case, the mass is increasing extremely fast. For example, different sized frogs should be able to jump the same distance according to the geometric similarity model proposed by Hill 1950 and interpreted by Wilson 2000, but in actuality larger frogs do jump longer distances. \nDimensional analysis is extremely useful for balancing units in an equation or in this case, determining expected slope.\n\nA few dimensional examples follow (M=Mass, L=Length, V=Volume, which is also L cubed because a volume is merely length cubed):\n\nTo find the expected slope for the relationship between mass and the characteristic length of an animal (see figure), the units of mass (M=L, because mass is a volume; volumes are lengths cubed) from the Y-axis are divided by the units of the X-axis (in this case, L). The expected slope on a double-logarithmic plot of L/ L in this case is 3 (log(L)/log(L)=3). This is the slope of a straight line, but most data gathered in science do not fall neatly in a straight line, so data transformations are useful. \nIt is also important to keep in mind what is being compared in the data. Comparing a characteristic such as head length to head width might yield different results from comparing head length to body length. That is, different characteristics may scale differently.\n\nA common way to analyze data such as those collected in scaling is to use log-transformation. There are two reasons for log transformation - a biological reason and a statistical reason. Biologically, log-log transformation places numbers into a geometric domain so that proportional deviations are represented consistently, independent of the scale and units of measurement. In biology this is appropriate because many biological phenomena (e.g. growth, reproduction, metabolism, sensation) are fundamentally multiplicative. Statistically, it is beneficial to transform both axes using logarithms and then perform a linear regression. This will normalize the data set and make it easier to analyze trends using the slope of the line. Before analyzing data though, it is important to have a predicted slope of the line to compare the analysis to.\n\nAfter data are log-transformed and linearly regressed, comparisons can then use least squares regression with 95% confidence intervals or reduced major axis analysis. Sometimes the two analyses can yield different results, but often they do not. If the expected slope is outside the confidence intervals, then there is allometry present. If mass in this imaginary animal scaled with a slope of 5 and this was a statistically significant value, then mass would scale very fast in this animal versus the expected value. It would scale with positive allometry. If the expected slope were 3 and in reality in a certain organism mass scaled with 1 (assuming this slope is statistically significant), then it would be negatively allometric.\n\nAnother example: Force is dependent on the cross-sectional area of muscle (CSA), which is L. If comparing force to a length, then the expected slope is 2. \nAlternatively, this analysis may be accomplished with a power regression. Plot the relationship between the data onto a graph. Fit this to a power curve (depending on the stats program, this can be done multiple ways), and it will give an equation with the form: \"y\"=\"Zx\", where \"n\" is the number. That “number” is the relationship between the data points. The downside, to this form of analysis, is that it makes it a little more difficult to do statistical analyses.\n\nMany physiological and biochemical processes (such as heart rate, respiration rate or the maximum reproduction rate) show scaling, mostly associated with the ratio between surface area and mass (or volume) of the animal. The metabolic rate of an individual animal is also subject to scaling.\n\nIn plotting an animal's basal metabolic rate (BMR) against the animal's own body mass, a logarithmic straight line is obtained, indicating a power-law dependence. Overall metabolic rate in animals is generally accepted to show negative allometry, scaling to mass to a power ≈ 0.75, known as Kleiber's law, 1932. This means that larger-bodied species (e.g., elephants) have lower mass-specific metabolic rates and lower heart rates, as compared with smaller-bodied species (e.g., mice). The straight line generated from a double logarithmic scale of metabolic rate in relation to body mass is known as the \"mouse-to-elephant curve\". These relationships of metabolic rates, times, and internal structure have been explained as, \"an elephant is approximately a blown-up gorilla, which is itself a blown-up mouse.\"\n\nMax Kleiber contributed the following allometric equation for relating the BMR to the body mass of an animal. Statistical analysis of the intercept did not vary from 70 and the slope was not varied from 0.75, thus:\nwhere formula_5 is body mass, and metabolic rate is measured in kcal per day.\n\nConsequently, the body mass itself can explain the majority of the variation in the BMR. After the body mass effect, the taxonomy of the animal plays the next most significant role in the scaling of the BMR. The further speculation that environmental conditions play a role in BMR can only be properly investigated once the role of taxonomy is established. The challenge with this lies in the fact that a shared environment also indicates a common evolutionary history and thus a close taxonomic relationship. There are strides currently in research to overcome these hurdles; for example, an analysis in muroid rodents, the mouse, hamster, and vole type, took into account taxonomy. Results revealed the hamster (warm dry habitat) had lowest BMR and the mouse (warm wet dense habitat) had the highest BMR. Larger organs could explain the high BMR groups, along with their higher daily energy needs. Analyses such as these demonstrate the physiological adaptations to environmental changes that animals undergo.\n\nEnergy metabolism is subjected to the scaling of an animal and can be overcome by an individual's body design. The metabolic scope for an animal is the ratio of resting and maximum rate of metabolism for that particular species as determined by oxygen consumption.\nOxygen consumption V and maximum oxygen consumption VO2 max. Oxygen consumption in species that differ in body size and organ system dimensions show a similarity in their charted V distributions indicating that, despite the complexity of their systems, there is a power law dependence of similarity; therefore, universal patterns are observed in diverse animal taxonomy.\n\nAcross a broad range of species, allometric relations are not necessarily linear on a log-log scale. For example, the maximal running speeds of mammals show a complicated relationship with body mass, and the fastest sprinters are of intermediate body size.\n\nThe muscle characteristics of animals are similar in a wide range of animal sizes, though muscle sizes and shapes can and often do vary depending on environmental constraints placed on them. The muscle tissue itself maintains its contractile characteristics and does not vary depending on the size of the animal. Physiological scaling in muscles affects the number of muscle fibers and their intrinsic speed to determine the maximum power and efficiency of movement in a given animal. The speed of muscle recruitment varies roughly in inverse proportion to the cube root of the animal’s weight, such as the intrinsic frequency of the sparrow’s flight muscle compared to that of a stork’s.\n\nFor inter-species allometric relations related to such ecological variables as maximal reproduction rate, attempts have been made to explain scaling within the context of dynamic energy budget theory and the metabolic theory of ecology. However, such ideas have been less successful.\n\nAllometry has been used to study patterns in locomotive principles across a broad range of species. Such research has been done in pursuit of a better understanding of animal locomotion, including the factors that different gaits seek to optimize. Allometric trends observed in extant animals have even been combined with evolutionary algorithms to form realistic hypotheses concerning the locomotive patterns of extinct species. These studies have been made possible by the remarkable similarities among disparate species’ locomotive kinematics and dynamics, “despite differences in morphology and size”.\n\nAllometric study of locomotion involves the analysis of the relative sizes, masses, and limb structures of similarly shaped animals and how these features affect their movements at different speeds. Patterns are identified based on dimensionless Froude numbers, which incorporate measures of animals’ leg lengths, speed or stride frequency, and weight.\n\nAlexander incorporates Froude-number analysis into his “dynamic similarity hypothesis” of gait patterns. Dynamically similar gaits are those between which there are constant coefficients that can relate linear dimensions, time intervals, and forces. In other words, given a mathematical description of gait A and these three coefficients, one could produce gait B, and vice versa. The hypothesis itself is as follows: “animals of different sizes tend to move in dynamically similar fashion whenever the ratio of their speed allows it.” While the dynamic similarity hypothesis may not be a truly unifying principle of animal gait patterns, it is a remarkably accurate heuristic.\n\nIt has also been shown that living organisms of all shapes and sizes utilize spring mechanisms in their locomotive systems, probably in order to minimize the energy cost of locomotion. The allometric study of these systems has fostered a better understanding of why spring mechanisms are so common, how limb compliance varies with body size and speed, and how these mechanisms affect general limb kinematics and dynamics.\n\n\nThe physiological effect of drugs and other substances in many cases scales allometrically.\n\nWest, Brown, and Enquist in 1997 derived a hydrodynamic theory to explain the universal fact that metabolic rate scales as the ¾ power with body weight. They also showed why lifespan scales as the +¼ power and heart rate as the -¼ power. Blood flow (+¾) and resistance (-¾) scale in the same way, leading to blood pressure being constant across species.\n\nHu and Hayton in 2001 discussed whether the basal metabolic rate scale is a ⅔ or ¾ power of body mass. The exponent of ¾ might be used for substances that are eliminated mainly by metabolism, or by metabolism and excretion combined, while ⅔ might apply for drugs that are eliminated mainly by renal excretion.\n\nAn online allometric scaler of drug doses based on the above work is available.\n\nThe US Food and Drug Administration (FDA) published guidance in 2005 giving a flow chart that presents the decisions and calculations used to generate the maximum recommended starting dose in drug clinical trials from animal data.\n\n\"This technical section is not well-written and needs editing\".\n\nThe mass and density of an organism have a large effect on the organism's locomotion through a fluid. For example, a tiny organisms uses flagella and can effectively move through a fluid it is suspended in. Then on the other scale a blue whale that is much more massive and dense in comparison with the viscosity of the fluid, compared to a bacterium in the same medium. The way in which the fluid interacts with the external boundaries of the organism is important with locomotion through the fluid. For streamlined swimmers the resistance or drag determines the performance of the organism. This drag or resistance can be seen in two distinct flow patterns. There is Laminar Flow where the fluid is relatively uninterrupted after the organism moves through it. Turbulent flow is the opposite, where the fluid moves roughly around an organisms that creates vortices that absorb energy from the propulsion or momentum of the organism. Scaling also affects locomotion through a fluid because of the energy needed to propel an organism and to keep up velocity through momentum. The rate of oxygen consumption per gram body size decreases consistently with increasing body size. (Knut Schmidt-Nielson 2004)\n\nIn general, smaller, more streamlined organisms create laminar flow (\"R\" < 0.5x106), whereas larger, less streamlined organisms produce turbulent flow (\"R\" > 2.0×106). Also, increase in velocity (V) increases turbulence, which can be proved using the Reynolds equation. In nature however, organisms such as a 6‘-6” dolphin moving at 15 knots does not have the appropriate Reynolds numbers for laminar flow \"R\" = 107, but exhibit it in nature. Mr. G.A Steven observed and documented dolphins moving at 15 knots alongside his ship leaving a single trail of light when phosphorescent activity in the sea was high. The factors that contribute are:\n\nThe resistance to the motion of an approximately stream-lined solid through a fluid can be expressed by the formula: \"C\"(total surface)\"V\"2/2 V = velocity\n\nNotable Reynolds numbers:\n\nScaling also has an effect on the performance of organisms in fluid. This is extremely important for marine mammals and other marine organisms that rely on atmospheric oxygen to survive and carry out respiration. This can affect how fast an organism can propel itself efficiently and more importantly how long it can dive, or how long and how deep an organism can stay underwater. Heart mass and lung volume are important in determining how scaling can affect metabolic function and efficiency. Aquatic mammals, like other mammals, have the same size heart proportional to their bodies.\n\nMammals have a heart that is about 0.6% of the total body mass across the board from a small mouse to a large Blue Whale. It can be expressed as: Heart Weight = 0.006Mb1.0, where Mb is the body mass of the individual. Lung volume is also directly related to body mass in mammals (slope = 1.02). The lung has a volume of 63 ml for every kg of body mass. In addition, the tidal volume at rest in an individual is 1/10 the lung volume. Also respiration costs with respect to oxygen consumption is scaled in the order of Mb.75. This shows that mammals, regardless of size, have the same size respiratory and cardiovascular systems and it turn have the same amount of blood: About 5.5% of body mass. This means that for a similarly designed marine mammals, the larger the individual the more efficiently they can travel compared to a smaller individual. It takes the same effort to move one body length whether the individual is one meter or ten meters. This can explain why large whales can migrate far distance in the oceans and not stop for rest. It is metabolically less expensive to be larger in body size. This goes for terrestrial and flying animals as well. In fact, for an organism to move any distance, regardless of type from elephants to centipedes, smaller animals consume more oxygen per unit body mass than larger ones. This metabolic advantage that larger animals have makes it possible for larger marine mammals to dive for longer durations of time than their smaller counterparts. That the heart rate is lower means that larger animals can carry more blood, which carries more oxygen. Then in conjuncture with the fact that mammals reparation costs scales in the order of Mb.75 shows how an advantage can be had in having a larger body mass. More simply, a larger whale can hold more oxygen and at the same time demand less metabolically than a smaller whale.\n\nTraveling long distances and deep dives are a combination of good stamina and also moving an efficient speed and in an efficient way to create laminar flow, reducing drag and turbulence. In sea water as the fluid, it traveling long distances in large mammals, such as whales, is facilitated by their neutral buoyancy and have their mass completely supported by the density of the sea water. On land, animals have to expend a portion of their energy during locomotion to fight the effects of gravity.\n\nFlying organisms such as birds are also considered moving through a fluid. In scaling birds of similar shape, it has also been seen that larger individuals have less metabolic cost per kg than smaller species, which would be expected because it holds true for every other form of animal. Birds also have a variance in wing beat frequency. Even with the compensation of larger wings per unit body mass, larger birds also have a slower wing beat frequency, which allows larger birds to fly at higher altitudes, longer distances, and faster absolute speeds than smaller birds. Because of the dynamics of lift-based locomotion and the fluid dynamics, birds have a U-shaped curve for metabolic cost and velocity. Because flight, in air as the fluid, is metabolically more costly at the lowest and the highest velocities. On the other end, small organisms such as insects can make gain advantage from the viscosity of the fluid (air) that they are moving in. A wing-beat timed perfectly can effectively uptake energy from the previous stroke. (Dickinson 2000) This form of wake capture allows an organism to recycle energy from the fluid or vortices within that fluid created by the organism itself. This same sort of wake capture occurs in aquatic organisms as well, and for organisms of all sizes. This dynamic of fluid locomotion allows smaller organisms to gain advantage because the effect on them from the fluid is much greater because of their relatively smaller size.\nAllometric engineering is a method for manipulating allometric relationships within or among groups.\n\nArguing that there are a number of analogous concepts and mechanisms between cities and biological entities, Bettencourt et al. showed a number of scaling relationships between observable properties of a city and the city size. GDP, \"supercreative\" employment, number of inventors, crime, spread of disease, and even pedestrian walking speeds scale with city population.\n\nSome examples of allometric laws:\n\nMany factors go into the determination of body mass and size for a given animal. These factors often affect body size on an evolutionary scale, but conditions such as availability of food and habitat size can act much more quickly on a species. Other examples include the following:\n\n\n\n",
    "id": "1663537",
    "title": "Allometry"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=20874",
    "text": "Mycology\n\nMycology is the branch of biology concerned with the study of fungi, including their genetic and biochemical properties, their taxonomy and their use to humans as a source for tinder, medicine, food, and entheogens, as well as their dangers, such as toxicity or infection. \n\nA biologist specializing in mycology is called a mycologist.\nMycology branches into the field of phytopathology, the study of plant diseases, and the two other disciplines that remain closely related because the vast majority of \"plant\" pathogens are fungi.\n\nHistorically, mycology was a branch of botany because, although fungi are evolutionarily more closely related to animals than to plants, this was not recognized until a few decades ago. Pioneer \"mycologists\" included Elias Magnus Fries, Christian Hendrik Persoon, Anton de Bary, and Lewis David von Schweinitz.\n\nMany fungi produce toxins, antibiotics, and other secondary metabolites. For example, the cosmopolitan (worldwide) genus \"Fusarium\" and their toxins associated with fatal outbreaks of alimentary toxic aleukia in humans were extensively studied by Abraham Joffe.\n\nFungi are fundamental for life on earth in their roles as symbionts, e.g. in the form of mycorrhizae, insect symbionts, and lichens. Many fungi are able to break down complex organic biomolecules such as lignin, the more durable component of wood, and pollutants such as xenobiotics, petroleum, and polycyclic aromatic hydrocarbons. By decomposing these molecules, fungi play a critical role in the global carbon cycle.\n\nFungi and other organisms traditionally recognized as fungi, such as oomycetes and myxomycetes (slime molds), often are economically and socially important, as some cause diseases of animals (such as histoplasmosis) as well as plants (such as Dutch elm disease and Rice blast).\n\nApart from pathogenic fungi, many fungal species are very important in controlling the plant diseases caused by different pathogens. For example, species of the filamentous fungal genus \"Trichoderma\" considered as one of the most important biological control agents as an alternative to chemical based products for effective crop diseases management.\n\nField meetings to find interesting species of fungi are known as 'forays', after the first such meeting organized by the Woolhope Naturalists' Field Club in 1868 and entitled \"A foray among the funguses\"[\"sic\"].\n\nSome fungi can cause disease in humans or other organisms. The study of pathogenic fungi is referred to as medical mycology.\n\nIt is presumed that humans started collecting mushrooms as food in prehistoric times. Mushrooms were first written about in the works of Euripides (480-406 B.C.). The Greek philosopher Theophrastos of Eresos (371-288 B.C.) was perhaps the first to try to systematically classify plants; mushrooms were considered to be plants missing certain organs. It was later Pliny the Elder (23–79 A.D.), who wrote about truffles in his encyclopedia \"Naturalis historia\". The word \"mycology\" comes from the Greek: μύκης (\"mukēs\"), meaning \"fungus\" and the suffix (\"-logia\"), meaning \"study\".\n\nThe Middle Ages saw little advancement in the body of knowledge about fungi. Rather, the invention of the printing press allowed some authors to disseminate superstitions and misconceptions about the fungi that had been perpetuated by the classical authors.\n\nThe start of the modern age of mycology begins with Pier Antonio Micheli's 1737 publication of \"Nova plantarum genera\". Published in Florence, this seminal work laid the foundations for the systematic classification of grasses, mosses and fungi. The term \"mycology\" and the complementary \"mycologist\" were first used in 1836 by M.J. Berkeley.\n\nFor centuries, certain mushrooms have been documented as a folk medicine in China, Japan, and Russia. Although the use of mushrooms in folk medicine is centered largely on the Asian continent, people in other parts of the world like the Middle East, Poland, and Belarus have been documented using mushrooms for medicinal purposes. Certain mushrooms, especially polypores like reishi were thought to be able to benefit a wide variety of health ailments. Medicinal mushroom research in the United States is currently active, with studies taking place at City of Hope National Medical Center, as well as the Memorial Sloan–Kettering Cancer Center.\n\nCurrent research focuses on mushrooms that may have hypoglycemic activity, anti-cancer activity, anti-pathogenic activity, and immune system-enhancing activity. Recent research has found that the oyster mushroom naturally contains the cholesterol-lowering drug lovastatin, mushrooms produce large amounts of vitamin D when exposed to ultraviolet (UV) light, and that certain fungi may be a future source of taxol. To date, penicillin, lovastatin, ciclosporin, griseofulvin, cephalosporin, LSD-25, and statins are the most famous pharmaceuticals that have been isolated from the fifth kingdom of life.\n\n\n",
    "id": "20874",
    "title": "Mycology"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=172384",
    "text": "Lichenology\n\nLichenology is the branch of mycology that studies the lichens, symbiotic organisms made up of an intimate symbiotic association of a microscopic alga (or a cyanobacterium) with a filamentous fungus.\n\nStudy of lichens draws knowledge from several disciplines: mycology, phycology, microbiology and botany. Scholars of lichenology are known as lichenologists.\n\nThe taxonomy of lichens was first intensively investigated by the Swedish botanist Erik Acharius (1757–1819), who is therefore sometimes named the \"father of lichenology\". Acharius was a student of Carl Linnaeus. Some of his more important works on the subject, which marked the beginning of lichenology as a discipline, are:\n\n\nLater lichenologists include the American scientists Vernon Ahmadjian and Edward Tuckerman and the Russian evolutionary biologist Konstantin Merezhkovsky, as well as amateurs such as Louisa Collings.\n\nLichens as a group have received less attention in classical treatises on botany than other groups although the relationship between humans and some species has been documented from early times. Several species have appeared in the works of Dioscorides, Pliny the Elder and Theophrastus although the studies are not very deep. During the first centuries of the modern age they were usually put forward as examples of spontaneous generation and their reproductive mechanisms were totally ignored. For centuries naturalists had included lichens in diverse groups until in the early 18th century a French researcher Joseph Pitton de Tournefort in his \"Institutiones Rei Herbariae\" grouped them into their own genus. He adopted the Latin term lichen, which had already been used by Pliny who had imported it from Theophrastus but up until then this term had not been widely employed. The original meaning of the Greek word λειχήν (leichen) was moss that in its turn derives from the Greek verb λείχω (liekho) to suck because of the great ability of these organisms to absorb water. In its original use the term signified mosses, liverworts as well as lichens. Some forty years later Dillenius in his \"Historia Muscorum\" made the first division of the group created by Tournefort separating the sub-families Usnea, Coralloides and Lichens in response to the morphological characteristics of the lichen thallus.\n\nAfter the revolution in taxonomy brought in by Linnaeus and his new system of classification lichens are retained in the Plant Kingdom forming a single group Lichen with eight divisions within the group according to the morphology of the thallus.\n\nOver the years research is shedding new light into the nature of these organisms still classified as plants. A controversial issue surrounding lichens since the early 19th century is their reproduction. In these years a group of researchers faithful to the tenets of Linnaeus considered that lichens reproduced sexually and had sexual reproductive organs, as in other plants, independent of whether asexual reproduction also occurred. Other researchers only considered asexual reproduction by means of Propagules.\n\nAgainst this background appeared the Swedish botanist Erik Acharius disciple of Linnaeus, who is today considered the father of lichenology, starting the taxonomy of lichens with his pioneering study of Swedish lichens in Lichenographiae Suecicae Prodromus of 1798 or in his Synopsis Methodica Lichenum, Sistens omnes hujus Ordinis Naturalis of 1814. These studies and classifications are the cornerstone of subsequent investigations. In these early years of structuring the new discipline various works of outstanding scientific importance appeared such as Lichenographia Europaea Reformata published in 1831 by Elias Fries or Enumeratio Critico Lichenum Europaeorum 1850 by Ludwig Schaerer in Germany.\n\nScientific publications settled many unknown facts about lichens. In the French publication Annales des Sciences Naturelles in an article of 1852 \"Memorie pour servir a l'Histoire des Lichens Organographique et Physiologique\" by Edmond Tulasne, the reproductive organs or apothecia of lichens was identified.\n\nThese new discoveries were becoming increasingly contradictory for scientists. The apothecium reproductive organ being unique to fungi but absent in other photosynthetic organisms. With improvements in microscopy, algae were identified in the lichen structure, which heightened the contradictions. At first the presence of algae was taken as being due to contamination due to collection of samples in damp conditions and they were not considered as being in a symbiotic relation with the fungal part of the thallus. That the algae continued to multiply showed that they were not mere contaminants.\n\nIt was Anton de Bary a German mycologist who specialised in phytopathology who first suggested in 1865 that lichens were merely the result of parasitism of various fungi of the ascomycetes group by nostoc type algae and others. Successive studies such as those carried out by Andrei Famintsyn and Baranetzky in 1867 showed no dependence of the algal component upon the lichen thallus and that the algal component could live independently of the thallus. It was in 1869 that Simon Schwendener demonstrated that all lichens were the result of fungal attack on the cells of algal cells and that all these algae also exist free in nature. This researcher was the first to recognise the dual nature of lichens as a result of the capture of the algal component by the fungal component. In 1873 Jean-Baptiste Edouard Bornet concluded form studying many different lichen species that the relationship between fungi and algae was purely symbiotic. It was also established that algae could associate with many different fungi to form different lichen phenotypes. \n\nIn 1909 the Russian lichenologist Konstantin Mereschkowski presented a research paper \"The Theory of two Plasms as the basis of Symbiogenesis, A new study on the Origin of Organisms\", which aims to explain a new theory of Symbiogenesis by lichens and other organisms as evidenced by his earlier work \"Nature and Origin of Chromatophores in the Plant Kingdom\". These new ideas can be studied today under the title of the Theory of Endosymbiosis.\n\nDespite the above studies the dual nature of lichens remained no more than a theory until in 1939 the Swiss researcher Eugen A Thomas was able to reproduce in the laboratory the phenotype of the lichen \"Cladonia pyxidata\" by combining its two identified components.\n\nDuring the 20th century botany and mycology were still attempting to solve the two main problems surrounding lichens. On the one hand the definition of lichens and the relationship between the two symbionts and the taxonomic position of these organisms within the plant and fungal kingdoms. There appeared numerous renowned researchers within the field of lichenology Henry Nicollon des Abbayes, William Alfred Weber, Antonina Georgievna Borissova, Irwin M. Brodo, George Albert Llano.\n\nLichenology has found applications beyond biology itself in the field of geology in a technique known as lichenometry where the age of an exposed surface can be found by studying the age of lichens growing on them. Age dating in this way can be absolute or relative because the growth of these organisms can be arrested under various conditions. The technique provides an average age of the older individual lichens providing a minimum age of the medium being studied. Lichenometry relies upon the fact that the maximum diameter of the largest thallus of an epilithic lichen growing on a substrate is directly proportional to the time from first exposure of the area to the environment as seen in studies by Roland Beschel in 1950 and is especially useful in areas exposed for less than 1000 years. Growth is greatest in the first 20 to 100 years with 15–50 mm growth per year and less in the following years with average growth of 2–4 mm per year.\n\nThe difficulty of giving a definition applicable to every known lichen has been debated since lichenologists first recognised the dual nature of lichens. In 1982 the International Association for Lichenology convened a meeting to adopt a single definition of lichen drawing on the proposals of a committee. The chairman of this committee was the renowned researcher Vernon Ahmadjian. The definition finally adopted was that lichen could be considered as the association between a fungus and a photosynthetic symbiont resulting in a thallus of specific structure.\n\nSuch a simple a priori definition soon brought criticism from various lichenologists and there soon emerged reviews and suggestions for amendments. For example, David L. Hawksworth considered the definition imperfect because it is impossible to determine which one thallus is of a specific structure since thalli changed depending upon the substrate and conditions in which they developed. This researcher represents one of the main trends among lichenologists who consider it impossible to give a single definition to lichens since they are a unique type of organism.\n\nToday studies in lichenology are not restricted to the description and taxonomy of lichens but have application in various scientific fields. Especially important are studies on environmental quality that are made through the interaction of lichens with their environment. Lichen is extremely sensitive to various air pollutants, especially to sulphur dioxide, which causes acid rain and prevents water absorption. \n\nAlthough several species of lichen have been used in traditional medicine it was not until the early 20th century that modern science became interested in them. The discovery of various substances with antibacterial action in lichen thalli was essential for scientists to become aware of the possible importance of these organisms to medicine From the 1940s there appeared various works by the noted microbiologist Rufus Paul Burkholder who demonstrated antibacterial action of lichens of the genus \"Usnea\" against \"Bacillus subtilis\" and \"Sarcina lutea\". Studies showed that the substance that inhibited growth of bacteria was usnic acid. Something similar occurred with the substance Ramelina synthesised by the lichen \"Ramalina reticulata\", nevertheless, these substances proved ineffective against Gram negative bacteria such as \"Escherichia coli\" and \"Pseudomonas\". With these investigations the number of antibacterial substances and possible drug targets known to be produced by lichens increased ergosterol, usnic acid etc.\n\nInterest in the potential of substances synthesised by lichens increased with the end of World War II along with the growing interest in all antibiotic substances. In 1947 antibacterial action was identified in extracts of \"Cetraria islandica\" and the compounds identified as responsible for bacterial inhibition were shown to be d-protolichosteric acid and d-1-usnic acid. Further investigations have identified novel antibacterial substances, Alectosarmentin or Atranorin.\n\nAntibacterial action of substances produced by lichens is related to their ability to disrupt bacterial proteins with a subsequent loss of bacterial metabolic capacity. This is possible due to the action of lichen phenolics such as usnic acid derivatives.\n\nFrom the 1950s the lichen product usnic acid was the object of most antitumour research. These studies revealed some in vitro antitumour activity by substances identified in two common lichens \"Peltigera leucophlebia\" and \"Collema flaccidum\".\n\nRecent work in the field of applied biochemistry has shown some antiviral activity with some lichen substances. In 1989 K Hirabayashi presented his investigations on inhibitory lichen polysaccharides in HIV infection.\n\n\n\n",
    "id": "172384",
    "title": "Lichenology"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=38710316",
    "text": "Quantitative biology\n\nQuantitative biology is an umbrella term encompassing the use of mathematical, statistical or computational techniques to study life and living organisms. The central theme and goal of quantitative biology is the creation of predictive models based on fundamental principles governing living systems.\n\nThe subfields of biology that employ quantitative approaches include:\n",
    "id": "38710316",
    "title": "Quantitative biology"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=9819436",
    "text": "Actionbioscience\n\nAction Bioscience is a non-commercial, educational web site sponsored by the American Institute of Biological Sciences (AIBS) . It was created to promote bioscience literacy and bring attention to seven bioscience issues of critical current importance: 1) Biodiversity, 2) Environment, 3) Genomics, 4) Biotechnology, 5) Evolution, 6) New Frontiers, and 7) Science Education\n\nThe organization seeks to:\n\n\nTo meet these goals, the web site provides articles by scientists, science educators, and science students on issues related to seven bioscience challenges: environment, biodiversity, genomics, biotechnology, evolution, new frontiers in science, and bioscience education. In addition, the web site provides educators with original lessons and resources to enhance bioscience teaching. Up-to-date external links are provided at the bottom of each article to help the reader \"learn more\" about or \"get involved\" in the issue. Selected articles are translated into Spanish.\n\nIt won an award as one of the best sci-tech web sites by Scientific American.\n",
    "id": "9819436",
    "title": "Actionbioscience"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=4972643",
    "text": "AP Biology\n\nIn the United States, Advanced Placement Biology (commonly abbreviated to AP Biology or AP Bio), is a course and examination offered by the College Board to high school students as an opportunity to earn placement credit for a college-level biology course. For the 2012–2013 school year, the College Board unveiled a new curriculum with a greater focus on \"scientific practices.\"\n\nThis course is offered to highly motivated students who wish to pursue an interest in the life sciences. The College Board recommends successful completion of high school biology and high school chemistry before commencing AP Biology, although the actual prerequisites vary from school to school and from state to state. Many schools, for example, require no background in biology to take the course.\n\nTopics covered by this course include:\nIn addition to the standard biology topics above, students are required to be familiar with a set of 12 specific biology labs, as well as general lab procedure.\n\nThe makeup of the AP Biology exam is based on the following percentages of three topics:\n\n\nThe AP test for this course consists of two sections. Section I, administered over a period of 90 minutes, consists of 63 multiple-choice questions and 6 mathematics-related fill in questions. Students are allowed to use a four-function, scientific, or graphing calculator. Calculators are generally needed for this mathematics portion, and are allowed on the test day. Section II, which also lasts 90 minutes, consists of 2 essay prompts to be answered comprehensively by the student, as well as 6 shorter essays that require concise answers.\n\nIn the 2007 administration, 144,796 students took the exam from 8,486 schools. In the 2008 administration, the exam grades were recalibrated, resulting in a substantial decrease in the top scores and increase in the bottom scores. In the 2009 and 2010 administrations, 159,580 and 172,512 students took the test, respectively. The 2013 revised test showed a marked decrease in the number of 5s and 1s received.\n\nMost colleges award credit in an introductory biology course for a score of 3 or higher. Higher tier schools generally only accept a score of 4 or 5. \nThe grade distributions since 2008 are shown below:\n\n\n",
    "id": "4972643",
    "title": "AP Biology"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=29698161",
    "text": "Edinburgh BioQuarter\n\nThe Edinburgh BioQuarter is a bioscience community based near the Royal Infirmary of Edinburgh and the Queen's Medical Research Centre in Edinburgh, close to the Roslin Institute for Animal Biology. It is Scotland's key initiative in the development of its life sciences industry, which employs more than 30,000 people in 600 companies.\n\nIn 2007, Scottish Enterprise completed a series of land deals which cleared a 55-acre site for development adjacent to the existing Royal Infimary of Edinburgh and the creation of the BioQuarter was announced. In early 2010, Dr Mike Capaldi joined BioQuarter as Commercialisation Director and the development of an executive team and business creation programme were launched.\n\nThe key partners in the Edinburgh BioQuarter initiative are Scottish Enterprise/Scottish Development International, the University of Edinburgh and NHS Lothian.\n\nFrom 2011, it will be joined at this location by the Scottish Centre for Regenerative Medicine, giving the BioQuarter one of the highest concentrations of research classed officially by the government as \"internationally leading\" (four-star, according to the 2008 Research Assessment Exercise).\n\nEdinburgh BioQuarter currently co-locates an 1000+-bed teaching hospital, the Royal Infirmary of Edinburgh, with the Queen’s Medical Research Institute (ranked #1 in the UK for clinical medical research, according to the 2008 research assessment exercise) and the Clinical Research Imaging Centre, or CRIC, opened by HM the Duke of Edinburgh in late 2010.\n\nOver the last three years, Edinburgh has attracted 53% of all Wellcome Trust grants in Scotland and 62% of Medical Research Council (MRC) funding in Scotland, making it a major centre of research income.\n\nIn November 2011, a new bioincubator building will come on stream offering of office and laboratory space, as well as the new Scottish Centre for Regenerative Medicine (SCRM), headed by Professor Sir Ian Wilmut, creator of \"Dolly the Sheep.\" This new centre will apply Scotland’s world-leading stem cell research to new therapies for conditions such as Parkinson's disease, multiple sclerosis and other conditions.\n\nIn September 2010, author JK Rowling endowed research at the BioQuarter with a £10 million gift to create the Anne Rowling Centre for Multiple Sclerosis.\n\nBy 2016, the new Royal Hospital for Children and Young People will be located at the BioQuarter, together with the Scottish Mind and Body Institute, a cross-disciplinary centre aimed at finding cures or therapies for neurological conditions.\n\nPart of BioQuarter’s purpose is to create new companies based on medical research being undertaken in the NHS and at the University of Edinburgh. For this purpose it has created an innovation competition, offering prizes worth £45,000 to researchers in all disciplines at the University of Edinburgh and NHS Lothian. The first round of these prizes will be awarded in March 2011.\n\nCompanies currently located at or associated with the BioQuarter community include:\n\n\nFurther companies are expected to join the BioQuarter community over the next twelve months as the process of company formation gathers pace. In addition, licensing deals and other partnerships with major international pharmaceutical companies and equipment manufacturers will add to the number of organisations housed at the BioQuarter.\n\n\n",
    "id": "29698161",
    "title": "Edinburgh BioQuarter"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1459978",
    "text": "Fetal pig\n\nFetal pigs are unborn pigs used in elementary as well as advanced biology classes as objects for dissection. Pigs, as a mammalian species, provide a good specimen for the study of physiological systems and processes due to the similarities between many pig and human organs.\n\nAlong with frogs and earthworms, fetal pigs are among the most common animals used in classroom dissection. There are several reasons for this, the main reason being that pigs, like humans, are mammals. Shared traits include common hair, mammary glands, live birth, similar organ systems, metabolic levels, and basic body form. They also allow for the study of fetal circulation, which differs from that of an adult. Secondly, fetal pigs are easy to obtain because they are by-products of the pork industry. Fetal pigs are the unborn piglets of sows that were killed by the meat packing industry. These pigs are not bred and killed for this purpose, but are extracted from the deceased sow’s uterus. Fetal pigs not used in classroom dissections are often used in fertilizer or simply discarded. Thirdly, fetal pigs are cheap, which is an essential component for dissection use by schools. They can be ordered for a little more than $25 at biological product companies. Fourthly, fetal pigs are easy to dissect because of their soft tissue and incompletely developed bones that are still made of cartilage. In addition, they are relatively large with well-developed organs that are easily visible. As long as the pork industry exists, fetal pigs will be relatively abundant, making them the prime choice for classroom dissections.\n\nSeveral peer-reviewed comparative studies have concluded that the educational outcomes of students who are taught basic and advanced biomedical concepts and skills using non-animal methods are equivalent or superior to those of their peers who use animal-based laboratories such as animal dissection.\n\nA systematic review concluded that students taught using non-animal methods demonstrated “superior understanding of complex biological processes, increased learning efficiency, and increased examination results.” It also reported that students’ confidence and satisfaction increased as did their preparedness for laboratories and their information-retrieval and communication abilities.\n\nThree studies at universities across the United States found that students who modeled body systems out of clay were significantly better at identifying the constituent parts of human anatomy than their classmates who performed animal dissection.\n\nAnother study found that students preferred using clay modeling over animal dissection and performed just as well as their cohorts who dissected animals.\n\nThe size of the fetal pig depends on the time allowed for the mother to gestate:\n\nNo studies have found significant data regarding the mother swine’s diet and fetal pig survival rate. However, there is a correlation between a mother pig having a nutritious diet containing proteins, vitamins and minerals during gestation period and the survival rate of piglets. The correlation, however, is not statistically different. Weight is also not a factor of survival rate because a healthier diet does not lead to a heavier offspring or a greater chance of live birth.\n\nThe placenta is used as a means of transferring nutrients from the mother to the fetus. The efficiency at which nutrients are transferred dictates the health and growth of the fetus. FRP, or fetal weight: placental weight ratio, was commonly used to determine placental efficiency. However, increasing FPR does not prove to increase litter size. Instead, a more accurate way of determining fetus growth is through certain characteristics of the placental lining. The placenta is made of a folded trophoblast/endometrial epithelial bilayer. The width and length of the placenta folds are positively related and increase as gestation progresses.\n\nThe width of the placental folds decreases until day 85 of gestation. From here, the width increases with gestation and is at its largest around day 105. The rate at which these folds increase is negatively related to fetus size. Thus, greater fold widths will be seen in smaller fetuses. Although increasing placental fold width does increase the interaction between fetus and mother, nutrient exchange is not most efficient in smaller fetal pigs, as would be expected. Many other factors, including depth of placental folds, are also responsible for these interactions.\n\nThe prenatal development of the fetus includes all the tissue and organ development. Within hours of mating, the sperm and egg undergo fertilization in the oviduct and three days later the egg moves into the uterus. The cells begin to specialize by day six, and attach themselves to the uterus lining by day eleven. From fertilization to day 18, the endoderm, ectoderm and mesoderm have been forming inside the embryo, and are completely formed by day 18, the same day the placenta forms. The endoderm transforms into the lungs, trachea, thyroid gland, and digestive tract of the fetus. The ectoderm has a greater role in the development of the fetus. It forms into the skin, nervous system, enamel of the teeth, lining of the intestine, mammary and sweat glands, hoofs, and hair. The mesoderm forms the major organ components that help keep the fetus alive. It forms the muscles and connective tissues of the body, blood vessels and cells, the skeleton, kidneys, adrenal glands, heart, and the reproductive organs. By day 20, most of the major organs are visible, and the last half of gestation focuses greatly on increasing the size of the fetuses.\n\nThe development of the lymphatic system and the formation of blood circulation occur at different stages of fetal pig development. The first lymphatic organ to become present is the thymus. Lymphocyte builds up in the spleen on the 70th day. By day 77, the thymus is already completely developed and is distinguishable from other organs. Also, follicles are present on the tongue and intestines on day 77. On the 84th day, Periarteriolar lymphoid sheaths appear in the fetal pig. By this time, the liver and bone marrow are active and functional.\n\nStudies have shown that litter size, the amount of floor space during the growing period, and the number of pigs the gilt, or female pig, is placed with while growing affect the reproduction rates of the gilts. Data from a study in 1976 by Nelson and Robinson showed that gilts from a small litter size ovulated more than the gilts from the larger litters. The study suggests stress plays a role in impacting the reproduction. The amount of floor space has been shown to impact the time it takes gilts to reach puberty. An adequate amount of floor space allowed the higher percentage of gilts to reach puberty sooner than those gilts who had less floor space. The gilts placed in smaller groups bore one more pig per litter than gilts in larger groups. Still, the environment in which the fetal gilt develops is significant to the reproductive and physiological development.\nAuthor: Brandon Anderson\n\nFetal pigs are often preserved in formaldehyde, a carcinogenic substance. A 1980 study found that exposure to formaldehyde could possibly cause nasal cancer in rats, leading to research on whether this was possible in humans or not. In 1995 it was concluded by the International Agency for Research on Cancer (IARC) that formaldehyde is a carcinogen for humans.\nIn Albania, fetal pigs are preserved by means of alcohol, and are used as an ingredient in Raki, a local moonshine liquor.\n\nThe anatomy of a fetal pig is similar to that of the adult pig in various aspects. Systems that are similar include the nervous, skeletal, respiratory (neglecting the under developed diaphragm), and muscular. Other important body systems have significant differences from the adult pig.\n\nThere are only a few differences between the circulatory system of an adult pig and a fetal pig, besides from the umbilical arteries and vein. There is a shunt between the wall of the right and left atrium called the foramen ovale. This allows blood to pass directly from the right to left atrium. There is also the ductus arterius which allows blood from the right atrium to be diverted to the aortic arch. Both of these shunts close a few minutes after birth.\n\nThe monogastric digestive system of the fetal pig harbors many similarities with many other mammals. The fetal pig's digestive organs are well developed before birth, although it does not ingest food. These organs include the esophagus, stomach, small and large intestines. Mesenteries serve to connect the organs of the fetal pig together. In order for digestion to occur, the fetal pig would have to ingest food. Instead, it gains much needed nutrition from the mother pig via the umbilical cord. In the adult pig, food will follow the general flow through the esophagus, which can be located behind the tracheae. From the oral cavity, the esophagus leads to the stomach, small intestine, and large intestine. Other organs developing during fetal pig development such as the gallbladder, pancreas and spleen are all critical in contributing to the overall flow of the digestive system because they contain digestive enzymes that will perform chemical digestion of food. After food is digested and nutrients are absorbed, the food follows through the large intestine and solid wastes are excreted through the anus. In the fetal pig however, the metabolic wastes are sent back to the mother through the umbilical cord where the mother excretes the wastes. Other remaining wastes remain in the fetal pig until birth. Then\n\nThe oral cavity of the fetal pig begins developing before birth. The tongue's taste buds, located in the enlarged papillae, facilitate food handling after birth. These taste buds develop during fetal development. Adult pigs have up to 15,000 taste buds, a much larger number than the average human tongue, which has 9,000.\n\nThe dental anatomy of the fetal pig shows differences from adult pigs. The fetal pig develops primary teeth (which are later replaced with permanent teeth). Some may erupt during fetal stage, which is why some of the fetuses show evidence of teeth. Depending on the age of the fetal pig, it is natural to see eruptions of third incisor and canine in the fetal pig. Because the fetal pigs were still in the mother’s uterus, teeth will still form which supports reasons for hollow unerupted teeth that may be seen. Similar to human dental anatomy, the overall dental anatomy of the pig consists of incisors, canines, pre-molars, and molars. Piglets can have 28 teeth total and adult pigs can have 44 teeth total.\n\nThe fetal pig's urogenital system is similar to the adult pig's system with the exception of the reproductive organs. The fetal pig urinary tract is relatively developed and easy to locate during dissection. The kidneys are located behind the abdominal organs and are partially embedded into the dorsal body wall by the spine. The ureters carry the urine to the urinary bladder, the large sack-like organ by the umbilical artery and vein, to the urethra. From there, the urine can be excreted.\n\nIf the fetal pig is a female, there will be a fleshy protrusion ventral near the anus called the genital papilla. The female's internal reproductive system is located below the kidneys. The two sac-like organs attached to the coil-like fallopian tubes are the ovaries. The uterus, which becomes the vagina, is located where the fallopian tubes meet. This system can be difficult to find as it is small as well as extremely dorsal and posterior to the other systems.\n\nMale fetal pigs have an urogenital opening located behind the umbilical cord. The swelling behind the hind legs of the fetal pig is the scrotum. The male's internal reproductive system has two scrotal sacs, which depending on the age of the fetal pig may or may not have developed testes. The epididymis coil on the testes connects to the vas deferens. The vas deferens crosses over the ureter and enters the urethra, which then connects to the penis located just posterior to the skin. Similar to the female system, it may be difficult to identify all parts.\n\n",
    "id": "1459978",
    "title": "Fetal pig"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=23615662",
    "text": "Canadian Stem Cell Foundation\n\nThe Canadian Stem Cell Foundation is an independent, non-profit organization established in 2008 and situated in Ottawa, Ontario. Stem Cell science is a Canadian innovation through the discovery of stem cells by Drs. James Till and Ernest McCulloch. It is globally known as the leading organization for stem cell research and support in the study of treatments and cures for diseases such as cancer, diabetes, blindness and stroke.\n\nTheir first strategy was created in 2013 to determine the concerns and actions required to develop an innovation that can advance stem cell research and clinics. The Canadian Stem Cell Foundation's goals are to invest a strategy for new treatments, sustainable healthcare, therapies and beneficial products. Their goals are beyond their capacity, such as \"using cells to treat respiratory heart diseases, restore lost vision, create a source of insulin-producing cells to treat diabetes, repair damaged spinal cords, reverse the effect of MS, Crohn's disease and other autoimmune disorders, reduce the ravages of Parkinson's disease and reverse tumour formation in the brain, breast and other solid tissues.\" Their other goals are to bring together scientists, institutions, health charities, industry partners, regulators, funders and philanthropists in a universal vision in the developments of stem cell science research and have public and private sectors support in the funding for stem cell research in the long-term.\n\nThere are many organizations involved such as the Stem Cell Network, Health Charities Coalition of Canada, Ontario Stem Cell Initiative, Centre for Commercialization of Regenerative Medicine, Ontario Bioscience Innovation Organization, and Cell CAN Regenerative Medicine and Cell Therapy Network.\n\nTo follow updates regarding \"The Canadian Stem Cell Strategy,\" visit the site: http://www.stemcellfoundation.ca/en/blog/categories/listings/strategy-updates\n\nThe Stem Cell Charter is an appeal launched in September, regarding the support for stem cell research. It is an interactive document that seeks concern for humanity with reference to stem cell science. It was created and written by, Bartha Maria Knoppers of McGill University, with a team of stem cell scientists, patients, ethicists and laypeople. This document is accessible to many individuals around the world, it is not a document for scientists, government officials or physicians, but for all. The charter includes five principles such as, \"responsible science, protection of citizens, intellectual freedom, transparency and integrity.\" The document's goal is to bring people together with the belief that stem cell science has the potential to conquer disease and advance medicine. Moreover, many significant scientists have endorsed the Charter such as, Mick Bhatia, Tim Caulfield, John Dick, Connie Eaves, Norman Iscove, Gordon Keller, Derek van der Kooy, Freda Miller, Andras Nagy, Janer Rossant, Michael Rudnicki, Guy Sauvageau, Jim Till and Sam Weiss.\n\nOver 3000 individuals have signed the charter, any individual can sign the charter by visiting the link: http://www.stemcellfoundation.ca/en/act/charter?view=default\n\nCanadian Stem Cell Foundation(2013) \"Canadian Stem Cell Foundation\" http://www.stemcellfoundation.ca/en/\n",
    "id": "23615662",
    "title": "Canadian Stem Cell Foundation"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=15919867",
    "text": "Francisco J. Ayala School of Biological Sciences\n\nThe Francisco J. Ayala School of Biological Sciences is one of the academic units of the University of California, Irvine (UCI). The school is divided into four departments: Developmental and Cell Biology, Ecology and Evolutionary Biology, Molecular Biology and Biochemistry, and Neurobiology and Behavior. With over 3,700 students it is in the top four largest schools in the university.\nIt is consistently ranked in the top one hundred in \"U.S. News & World Report’s\" yearly list of best graduate schools.\n\nThe Francisco J. Ayala School of Biological Sciences first opened in 1965 at the University of California, Irvine and was one of the first schools founded when the University campus opened. The school's founding Dean, Edward A. Steinhaus, had four founding department chairs and started out with 17 professors.\n\nOn March 12, 2014, the School was officially renamed after UCI professor and donor Francisco J. Ayala by Chancellor Michael V. Drake.\n",
    "id": "15919867",
    "title": "Francisco J. Ayala School of Biological Sciences"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=42036843",
    "text": "Biological illustration\n\nBiological illustration is the use of technical illustration to visually communicate the structure and specific details of biological subjects of study. This can be used to demonstrate anatomy, explain biological functions or interactions, direct surgical procedures, distinguish species, and other applications. The scope of biological illustration can range from the whole organism level to microscopic.\n\nTypes of biological illustrations include:\n\nHistorically, biological illustrations have been in use since the beginning of man's exploration and attempts to understand the world around him. The paleolithic cave paintings were so detailed that we can even recognize species and breeds of many of the depicted animals today. For example, in the Chauvet-Pont-d'Arc Cave (circa 30,000 BC), at least 13 different species have been identified. In one prehistoric cave (circa 15,000 BC), there is a drawing of a mammoth with a darkened area where the heart should be. If this is indeed the intention of the illustration, it would be the world's first anatomical illustration.\n\nIn the Alexandrian era (356 – 323 BC), the Greek physician Herophilus, now known as the father of anatomy, performed public dissections and recorded his findings. In the 1st century AD, Pedanius Dioscorides compiled the \"De Materia Medica\", a collection of medicinal information and recipes, containing illustrations of about 600 plants in all.\n\nDuring the Renaissance, artist and scientist Leonardo DaVinci famously sketched his observations from human dissections, as well as his studies of plants and the flight of birds. In the mid-16th century, the physician Andreas Vesalius compiled and published the \"De humani corporis fabrica\", a collection of textbooks on human anatomy superior to any illustrations that had been produced until that point. In the early 1600s, the explorer Étienne de Flacourt documented his travels to Madagascar, and illustrated the unique fauna there, setting a precedent for future explorers as world travel became a more feasible reality.\n\nDuring his five-year voyage of the HMS Beagle, Charles Darwin wrote and illustrated \"The Voyage of the Beagle\", which was published in 1839. In the beginning of the 20th century, one of the most prolific biological illustrator, Ernst Haeckel, discovered, described, and named thousands of new species, and his published work, \"Kunstformen der Natur\", contained hundreds of prints of various organisms, many of which were first described by Haeckel himself.\n\nBiological illustrations can be found in use in history and anatomy textbooks, nature guides, natural history museums, scientific magazines and journals, botanical gardens, zoos and aquariums, surgical training manuals, and many more applications. Biological illustration can be pursued as a degree in the undergraduate, graduate, and technical college levels. Preparation for a biological illustration career can include a background of art or science, or a combination of both. Skills development in biological illustration can involve two-dimensional art, animation, graphic design, and sculpture (such as necessary in custom prosthetics).\n\nIt is possible to work in biological illustration without a specific degree, but a degree will significantly enhance an illustrator's employment opportunities. Job applications can be submitted to scientific researchers, publishers of scientific manuscripts, research institutions, museums, scientific foundations, commercial book publishers or university presses, individual authors, hospitals and medical training centers, local and state government offices, park services, environmental control offices, special government committees, printers and commercial publishing houses. Employment opportunities in the biological illustration profession are fairly limited, full-time jobs are not often available, and many experienced illustrators are self-employed, on short-term contracts, or work in science communication careers with few illustration duties. Many illustrators prefer the flexibility of their own working arrangements, but this is only possible when they are well established in the field and capable of locating work when needed. Many freelance illustrators supplement their salary with commercial illustration and graphic design projects, as is common in many art careers.\n\nBiological illustration has traditionally employed the techniques of using carbon dust, color pencil, stipple pen and ink, lithography, watercolor and gouache, however digital illustration has recently become more important in the field. Every professional scientific illustration begins with multiple rough sketches. Many details must be discussed between the artist and scientist before a final drawing can be completed, and additional preliminary drawings must be prepared in order to work out aesthetic details.\n\nPen and ink (often a flex nib fountain pen) line illustrations are clean, crisp, clear, and inexpensive to produce, making them ideal for biological illustrations. Ink drawings are typically made on a heavy drawing paper, such as Bristol board.\n\nDigital illustration can be done using a monitor or drawing tablet, computer software such as Adobe Illustrator or Photoshop, or it can be used in post-production after the illustration has been drawn by hand.\n\n",
    "id": "42036843",
    "title": "Biological illustration"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=49032506",
    "text": "SensUs\n\nSensUs is the yearly international student competition on molecular biosensors for healthcare applications. Teams of students at international universities design and build biosensing systems, and demonstrate and present their prototypes to an audience during a public final contest event. The competition aims to stimulate education and innovation in the field of biosensing, with a worldwide scope. The long term aim is to improve the quality of life of patients.\n\nEach year, SensUs defines a theme that represents a current healthcare problem in society. A biomarker is selected and the participating teams have 9 months to develop a biosensor that can measure the selected biomarker. The participating teams have the same functional end goal, but develop creative solutions and unique biosensor prototypes. The prototypes are shown, shared and discussed at the annual SensUs event. \nSensUs is an international platform for learning and innovation in the field of molecular biosensors for healthcare applications. \n\nSensUs 2016 was the first edition of the competition. The aim was to develop biosensors for the measurement of creatinine in blood plasma, which is a key biomarker for Kidney failure. The student teams originated from five universities: University of Leuven, Imperial College London, Uppsala University, Technical University of Denmark, and Eindhoven University of Technology. The teams had worked during 9 months to develop biosensors in their own universities, and thereafter all came to the final contest event on the 9 and 10 of September 2016 at Eindhoven University of Technology. Here they could win four awards; the Analytical Performance award (AP), the Creativity Award (Cr), the Translational Potential award (TP) and the Public Inspiration award (PI). Photos and videos from SensUs 2016 can be found here.\n\nSensUs 2017 was the second edition of the yearly competition. The finals took place on the 8 and 9 of September 2017. Ten student teams from three continents built a biosensor to detect NT-proBNP in blood plasma, which is an important biomarker for Heart failure.\n\nDuring the event the teams showed their biosensors to the public, measured NT-proBNP live and presented pitches on technology and on translation into society. \nThe teams were judged by an international multidisciplinary jury, who conferred awards for Analytical Performance (AP), Creativity (Cr), and Translational Potential (TP). \n\nAbout 500 people visited the event at Eindhoven University of Technology. The event was broadcast on the SensUs Digital platform, including livestreams, live data, Q&A, photos and videos and voting functionality. The cumulative visitor count on SensUs digital was 5000 from 74 countries, of which 1500 people casted their votes for the Public Inspiration award (PI). Photos of SensUs 2017 can be found here.\n\nSensUs 2018 will be the third edition of the SensUs competition. The theme is antibiotic testing for improved healthcare. The biomarker that will be measured by the teams is vancomycin, an important antibiotic drug. Participants are the ten universities of SensUs 2017 and three new universities, from Canada (Université de Montréal), China (Zhejiang University) and Spain (UPC-BarcelonaTech). \nThe finals of SensUs 2018 will take place on the 7 and 8 of September 2018 in Eindhoven, the Netherlands.\n\n",
    "id": "49032506",
    "title": "SensUs"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=18322812",
    "text": "SAT Subject Test in Biology E/M\n\nThe SAT Subject Test in Biology is the name of a one-hour multiple choice test given on biology by The College Board. A student chooses whether to take the test depending upon college entrance requirements for the schools in which the student is planning to apply. Until 1994, the SAT Subject Tests were known as Achievement Tests; and from 1995 until January 2005, they were known as SAT IIs. Of all SAT subject tests, the Biology E/M test is the only SAT II that allows the test taker a choice between the ecological or molecular tests. A set of 60 questions is taken by all test takers for Biology and a choice of 20 questions is allowed between either the E or M tests. This test is graded on a scale between 200 and 800. The average for Molecular is 630 while Ecological is 591.\n\nThis test has 80 multiple-choice questions that are to be answered in one hour. All questions have five answer choices. Students receive one point for each correct answer, lose ¼ of a point for each incorrect answer, and receive 0 points for questions left blank. The student's score is based entirely on his or her performance in answering the multiple-choice questions.\n\nThe questions cover a broad range of topics in general biology. There are more specific questions related respectively on ecological concepts (such as population studies and general Ecology) on the E test and molecular concepts such as DNA structure, translation, and biochemistry on the M test.\n\nThe College Board suggests a year-long course in biology at the college preparatory level, as well as a one-year course in algebra, and lab experience as preparation for the test. The test requires understanding of biological data and concepts, science-related terms, and the ability to effectively synthesize and interpret data from charts, maps, and other visual media. However, most questions from this test are derived from, or are similar to, the pre-2012 AP Biology multiple choice questions. By taking an AP class or a class with similar rigor, one's chances at doing well on this test should improve.\n\n",
    "id": "18322812",
    "title": "SAT Subject Test in Biology E/M"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=342094",
    "text": "List of gene families\n\nThis is a list of gene families or gene complexes, that is sets of genes which occur across a number of different species which often serve similar biological functions. These gene families typically encode functionally related proteins, and sometimes the term gene families is a shorthand for the sets of proteins that the genes encode. They may or may not be physically adjacent on the same chromosome.\n\n\n\n\n\n\n\n",
    "id": "342094",
    "title": "List of gene families"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1936485",
    "text": "Serotype\n\nA serotype or serovar is a distinct variation within a species of bacteria or virus or among immune cells of different individuals. These microorganisms, viruses, or cells are classified together based on their cell surface antigens, allowing the epidemiologic classification of organisms to the sub-species level. A group of serovars with common antigens is called a serogroup or sometimes \"serocomplex\".\n\nSerotyping often plays an essential role in determining species and subspecies. The \"Salmonella\" genus of bacteria, for example, has been determined to have over 2600 serotypes, including \"Salmonella enterica\" serovar Typhimurium, \"S. enterica\" serovar Typhi, and \"S. enterica\" serovar Dublin. \"Vibrio cholerae\", the species of bacteria that causes cholera, has over 200 serotypes, based on cell antigens. Only two of them have been observed to produce the potent enterotoxin that results in cholera: 0:1 and 0:139.\n\nSerotypes were discovered by the American microbiologist Rebecca Lancefield in 1933.\n\nThe immune system is capable of discerning a cell as being 'self' or 'non-self' according to that cell's serotype. In humans, that serotype is largely determined by human leukocyte antigen (HLA), the human version of the major histocompatibility complex. Cells determined to be non-self are usually recognized by the immune system as foreign, causing an immune response, such as hemagglutination. Serotypes differ widely between individuals; therefore, if cells from one human (or animal) are introduced into another random human, those cells are often determined to be non-self because they do not match the self-serotype. For this reason, transplants between genetically non-identical humans often induce a problematic immune response in the recipient, leading to transplant rejection. In some situations this effect can be reduced by serotyping both recipient and potential donors to determine the closest HLA match.\n\nThe Kauffman–White classification scheme is the basis for naming the manifold serovars of \"Salmonella\". To date, more than 2600 different serotypes have been identified. A \"Salmonella\" serotype is determined by the unique combination of reactions of cell surface antigens. The \"O\" antigen is determined by the outermost portion of the Lipopolysaccharide (LPS) and the \"H\" antigen is based on the flagellar (protein) antigens. \nThere are two species of \"Salmonella\": \"Salmonella bongori\" and \"Salmonella enterica\". \"Salmonella enterica\" can be subdivided into six subspecies.\nThe process to identify the serovar of the bacterium consists of finding the formula of cell antigens which represent the variations of the bacteria. First of all, the use of the agglutination method on slides is required in order to get the antigen formula. The agglutination between the antigen and the antibody is made with a specific antisera, which reacts with the antigen to produce a mass. The antigen O is tested with a bacterial suspension from an agar plate whereas the antigen H is tested with a bacterial suspension from a broth culture. The scheme is used to classify the serovar depending on its antigen formula obtained with the reaction of the agglutination.\n\n\n",
    "id": "1936485",
    "title": "Serotype"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=711718",
    "text": "Evolutionary taxonomy\n\nEvolutionary taxonomy, evolutionary systematics or Darwinian classification is a branch of biological classification that seeks to classify organisms using a combination of phylogenetic relationship (shared descent), progenitor-descendant relationship (serial descent), and degree of evolutionary change. This type of taxonomy may consider whole taxa rather than single species, so that groups of species can be inferred as giving rise to new groups. The concept found its most well-known form in the modern evolutionary synthesis of the early 1940s.\n\nEvolutionary taxonomy differs from strict pre-Darwinian Linnaean taxonomy (producing orderly lists only), in that it builds evolutionary trees. While in phylogenetic nomenclature each taxon must consist of a single ancestral node and all its descendants, evolutionary taxonomy allows for groups to be excluded from their parent taxa (e.g. dinosaurs are not considered to \"include\" birds, but to have \"given rise\" to them), thus permitting paraphyletic taxa.\n\nEvolutionary taxonomy arose as a result of the influence of the theory of evolution on Linnaean taxonomy. The idea of translating Linnaean taxonomy into a sort of dendrogram of the Animal and Plant Kingdoms was formulated toward the end of the 18th century, well before Charles Darwin's book \"On the Origin of Species\" was published. The first to suggest that organisms had common descent was Pierre-Louis Moreau de Maupertuis in his 1751 \"Essai de Cosmologie\", Transmutation of species entered wider scientific circles with Erasmus Darwin's 1796 Zoönomia and Jean-Baptiste Lamarck's 1809 \"Philosophie Zoologique\". The idea was popularised in the English-speaking world by the speculative but widely read \"Vestiges of the Natural History of Creation\", published anonymously by Robert Chambers in 1844.\n\nFollowing the appearance of \"On the Origin of Species\", Tree of Life representations became popular in scientific works. In \"On the Origin of Species\", the ancestor remained largely a hypothetical species; Darwin was primarily occupied with showing the principle, carefully refraining from speculating on relationships between living or fossil organisms and using theoretical examples only. In contrast, Chambers had proposed specific hypotheses, the evolution of placental mammals from marsupials, for example.\n\nFollowing Darwin's publication, Thomas Henry Huxley used the fossils of \"Archaeopteryx\" and \"Hesperornis\" to argue that the birds are descendants of the dinosaurs. Thus, a group of extant animals could be tied to a fossil group. The resulting description, that of dinosaurs \"giving rise to\" or being \"the ancestors of\" birds, exhibits the essential hallmark of evolutionary taxonomic thinking.\n\nThe past three decades have seen a dramatic increase in the use of DNA sequences for reconstructing phylogeny and a parallel shift in emphasis from evolutionary taxonomy towards Hennig’s ‘phylogenetic systematics’.\n\nToday, with the advent of modern genomics, scientists in every branch of biology make use of molecular phylogeny to guide their research. One common method is multiple sequence alignment.\n\nCavalier-Smith, G. G. Simpson and Ernst Mayr are some representative evolutionary taxonomists.\n\nEfforts in combining modern methods of cladistics, phylogenetics, and DNA analysis with classical views of taxonomy have recently appeared. Certain authors have found that phylogenetic analysis is acceptable scientifically as long as paraphyly at least for certain groups is allowable. Such a stance is promoted in papers by Tod F. Stuessy and others. A particularly strict form of evolutionary systematics has been presented by Richard H. Zander in a number of papers, but summarized in his \"Framework for Post-Phylogenetic Systematics\".\n\nBriefly, Zander's pluralistic systematics is thus: A method that cannot falsify a hypothesis is as unscientific as a hypothesis that cannot be falsified. Cladistics generates only trees of shared ancestry, not serial ancestry. Taxa evolving seriatim cannot be dealt with by analyzing shared ancestry with cladistic methods. Hypotheses such as adaptive radiation from a single ancestral taxon cannot be falsified with cladistics. Cladistics offers a way to cluster by trait transformations but no evolutionary tree can be entirely dichotomous. Phylogenetics posits shared ancestral taxa as causal agents for dichotomies yet there is no evidence for the existence of such taxa. Molecular systematics uses DNA sequence data for tracking evolutionary changes, thus paraphyly and sometimes phylogenetic polyphyly signal ancestor-descendant transformations at the taxon level, but otherwise molecular phylogenetics makes no provision for extinct paraphyly. Additional transformational analysis is needed to infer serial descent.\n\nThe Besseyan cactus or commagram is the best evolutionary tree for showing both shared and serial ancestry. First, a cladogram or natural key is generated. Generalized ancestral taxa are identified and specialized descendant taxa are noted as coming off the lineage with a line of one color representing the progenitor through time. A Besseyan cactus or commagram is then devised that represents both shared and serial ancestry. Progenitor taxa may have one or more descendant taxa. Support measures in terms of Bayes factors may be given, following Zander's method of transformational analysis using decibans.\n\nCladistic analysis groups taxa by shared traits but incorporates a dichotomous branching model borrowed from phenetics. It is essentially a simplified dichotomous natural key, although reversals are tolerated. The problem, of course, is that evolution is not necessarily dichotomous. An ancestral taxon generating two or more descendants requires a longer, less parsimonious tree. This is why a tree based solely on shared traits is not called an evolutionary tree but merely a cladistic tree. This tree reflects to a large extent evolutionary relationships through trait transformations but ignores relationships made by species-level transformation of extant taxa.\n\nPhylogenetics attempts to inject a serial element by postulating ad hoc, undemonstrable shared ancestors at each node of a cladistic tree. There are in number, for a fully dichotomous cladogram, one less invisible shared ancestor than the number of terminal taxa. We get, then, in effect a dichotomous natural key with an invisible shared ancestor generating each couplet. This cannot imply a process-based explanation without justification of the dichotomy, and supposition of the shared ancestors as causes. The cladistic form of analysis of evolutionary relationships cannot falsify any genuine evolutionary scenario incorporating serial transformation, according to Zander.\n\nZander has detailed methods for generating support measures for molecular serial descent and for morphological serial descent using Bayes factors through deciban addition.\n\nAs more and more fossil groups were found and recognized in the late 19th and early 20th century, palaeontologists worked to understand the history of animals through the ages by linking together known groups. The Tree of life was slowly being mapped out, with fossil groups taking up their position in the tree as understanding increased.\n\nThese groups still retained their formal Linnaean taxonomic ranks. Some of them are paraphyletic in that, although every organism in the group is linked to a common ancestor by an unbroken chain of intermediate ancestors within the group, some other descendants of that ancestor lie outside the group. The evolution and distribution of the various taxa through time is commonly shown as a \"spindle diagram\" (often called a \"Romerogram\" after the American palaeontologist Alfred Romer) where various spindles branch off from each other, with each spindle representing a taxon. The width of the spindles are meant to imply the abundance (often number of families) plotted against time.\n\nVertebrate palaeontology had mapped out the evolutionary sequence of vertebrates as currently understood fairly well by the closing of the 19th century, followed by a reasonable understanding of the evolutionary sequence of the plant kingdom by the early 20th century. The tying together of the various trees into a grand Tree of Life only really became possible with advancements in microbiology and biochemistry in the period between the World Wars.\n\nThe two approaches, evolutionary taxonomy and the phylogenetic systematics derived from Willi Hennig, differ in the use of the word \"monophyletic\". For evolutionary systematicists, \"monophyletic\" means only that a group is derived from a single common ancestor. In phylogenetic nomenclature, there is an added caveat that the ancestral species and all descendants should be included in the group. The term \"holophyletic\" has been proposed for the latter meaning. As an example, amphibians are monophyletic under evolutionary taxonomy, since they have arisen from fishes only once. Under phylogenetic taxonomy, amphibians do not constitute a monophyletic group in that the amniotes (reptiles, birds and mammals) have evolved from an amphibian ancestor and yet are not considered amphibians. Such paraphyletic groups are rejected in phylogenetic nomenclature, but are considered a signal of serial descent by evolutionary taxonomists.\n",
    "id": "711718",
    "title": "Evolutionary taxonomy"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=28734047",
    "text": "Animalia Paradoxa\n\nThese 10 taxa appear in the 1st to 5th editions:\n\nThe above 10 taxa and the 4 taxa following were in the 2nd (1740) edition and the 4th and 5th editions (total 14 entries):\n",
    "id": "28734047",
    "title": "Animalia Paradoxa"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=6198887",
    "text": "ARKive\n\nARKive is a global initiative with the mission of \"promoting the conservation of the world's threatened species, through the power of wildlife imagery\", which it does by locating and gathering films, photographs and audio recordings of the world's species into a centralised digital archive. Its current priority is the completion of audio-visual profiles for the c. 17,000 species on the IUCN Red List of Threatened Species.\n\nThe project is an initiative of Wildscreen, a UK-registered educational charity, based in Bristol. The technical platform was created by Hewlett Packard, as part of the HP Labs' Digital Media Systems research programme.\n\nARKive has the backing of leading conservation organisations, including BirdLife International, Conservation International, International Union for Conservation of Nature (IUCN), The United Nations' World Conservation Monitoring Centre (UNEP-WCMC), and the World Wide Fund for Nature (WWF), as well as leading academic and research institutions, such as the Natural History Museum; Royal Botanic Gardens, Kew; and the Smithsonian Institution. It is a member of the Institutional Council of the Encyclopedia of Life.\n\nTwo ARKive layers for Google Earth, featuring endangered species and species in the Gulf of Mexico have been produced by Google Earth Outreach. The first of these was launched in April 2008 by Wildscreen's Patron, Sir David Attenborough.\n\nThe project formally was launched on 20 May 2003 by its patron, the UK-based natural history presenter, Sir David Attenborough, a long-standing colleague and friend of its chief instigator, the late Christopher Parsons, a former Head of the BBC Natural History Unit. Parsons never lived to see the fruition of the project, succumbing to cancer in November 2002 at the age of 70.\n\nParsons identified a need to provide a centralised safe haven for wildlife films and photographs after discovering that many such records are held in scattered, non-indexed, collections, often with little or no public access, and sometimes in conditions that could lead to loss or damage. He believed the records could be a powerful force in building environmental awareness by bringing scientific names to life. He also saw their preservation as an important educational resource and conservation tool, not least because extinction rates and habitat destruction could mean that images and sounds might be the only legacy of some species’ existence.\n\nHis vision of a permanent, accessible, refuge for audio-visual wildlife material won almost immediate support from many of the world’s major broadcasters, including the BBC, Granada, international state broadcasting corporations and \"National Geographic\" magazine; leading film and photographic libraries, international conservation organisations and academic institutes such as Cornell University.\n\nThe initial feasibility study for creating ARKive was carried out in the late 1980s by conservationist John Burton, but at the time the costs of the technology needed were too far too high, and so it was over a decade later, after the technology had caught up with Christopher Parson's vision (and the costs dropped), that the project was able to get off the ground.\n\nAfter capital development funds of £2m were secured from the Heritage Lottery Fund in 1997 and New Opportunities Fund in 2000, work on building ARKive began as part of the UK's Millennium celebrations, using advanced computerised storage and retrieval technology devised for the project by Hewlett-Packard., with an initial capacity of up to 74 terabytes of data, using redundant hardware and multiple copies of media stored at multiple sites. Media is digitised to the highest available quality without compression and encoded to open standards.\n\nA prototype site was online as early as April 1999. There were several design iterations before the formal launch.\n\nBy the launch date, the project team had researched, catalogued, copied, described and authenticated image, sound and fact files of 1,000 animals, plants and fungi, many of them critically endangered. More multi-media profiles are added every month, starting with British flora and fauna and with species included on the Red List – that is, species that are believed to be closest to extinction, according to research by the World Conservation Union. By January 2006, the database had grown to 2,000 species, 15,000 still images and more than 50 hours of video. By 2010, over 5,500 donors had contributed 70,000 film clips and photos of more than 12,000 species.\n\nThe site was Sunday Times website of the year for 2005. It was a 2010 Webby Award honoree for its outstanding calibre of work, in the 'Education' category, and a 2010 Association of Educational Publishers 'Distinguished Achievement Award' winner, in the category for websites for 9-12 year olds.\n\n\n",
    "id": "6198887",
    "title": "ARKive"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=164897",
    "text": "Three-domain system\n\nThe three-domain system is a biological classification introduced by Carl Woese \"et al.\" in 1977 that divides cellular life forms into archaea, bacteria, and eukaryote domains. In particular, it emphasizes the separation of prokaryotes into two groups, originally called \"Eubacteria\" (now \"Bacteria\") and \"Archaebacteria\" (now \"Archaea\"). Woese argued that, on the basis of differences in 16S rRNA genes, these two groups and the eukaryotes each arose separately from an ancestor with poorly developed genetic machinery, often called a progenote. To reflect these primary lines of descent, he treated each as a domain, divided into several different kingdoms. Woese initially used the term \"kingdom\" to refer to the three primary phylogenic groupings, and this nomenclature was widely used until the term \"domain\" was adopted in 1990.\n\nParts of the three-domain theory have been challenged by scientists such as Radhey Gupta, who argues that the primary division within prokaryotes should be between those surrounded by a single membrane, and those with two membranes.\n\nThe three-domain system adds a level of classification (the domains) \"above\" the kingdoms present in the previously used five- or six-kingdom systems. This classification system recognizes the fundamental divide between the two prokaryotic groups, insofar as archaea appear to be more closely related to eukaryotes than they are to other prokaryotic bacteria. The current system has the following listed kingdoms in the three domains:\n\nDomain Archaea – prokaryotic, no nuclear membrane, distinct biochemistry and RNA markers from bacteria, possess unique ancient evolutionary history for which they are considered some of the oldest species of organisms on Earth; traditionally classified as archaebacteria; often characterized by living in extreme environments. Some examples of archaeal organisms are methanogens which produce the gas methane, halophiles which live in very salty water, and thermoacidophiles which thrive in acidic high temperature water.\n\nDomain Bacteria – prokaryotic, consists of prokaryotic cells possessing primarily \"diacyl glycerol diester lipids\" in their membranes and bacterial rRNA, no nuclear membrane, traditionally classified as bacteria. Most of the known pathogenic prokaryotic organisms belong to bacteria (see for exceptions), and are currently studied more extensively than Archaea. Some examples of bacteria include Cyanobacteria photosynthesizing bacteria that are related to the chloroplasts of eukaryotic plants and algae, Spirochaetes – Gram-negative bacteria that include those causing syphilis and Lyme disease, and Actinobacteria --Gram-positive bacteria including \"Bifidobacterium animalis\" which is present in the human large intestine.\n\nDomain Eukarya – eukaryotes, organisms that contain a membrane-bound nucleus. An inexhaustive list of eukaryotic organisms includes:\n\nEach of the three cell types tends to fit into recurring specialties or roles. Bacteria tend to be the most prolific reproducers, at least in moderate environments. Archaeans tend to adapt quickly to extreme environments, such as high temperatures, high acids, high sulfur, etc. This includes adapting to use a wide variety of food sources. Eukaryotes are the most flexible with regard to forming cooperative colonies, such as in multi-cellular organisms, including humans. In fact, the structure of a Eukaryote is likely to have derived from a joining of different cell types, forming organelles.\n\nParakaryon myojinensis (\"incertae sedis\") is a single-celled organism known by a unique example. \"[T]his organism appears to be a life form distinct from prokaryotes and eukaryotes\", with features of both.\n\nParts of the three-domain theory have been challenged by scientists including Ernst Mayr, Thomas Cavalier-Smith and Radhey Gupta. In particular, Gupta argues that the primary division within prokaryotes should be among those surrounded by a single membrane (monoderm), including gram-positive bacteria and archaebacteria, and those with an inner and outer cell membrane (diderm), including gram-negative bacteria. He claims that sequences of features and phylogenies from some highly conserved proteins are inconsistent with the three-domain theory, and that it should be abandoned despite its widespread acceptance.\n\nRecent work has proposed that Eukarya may have actually branched off from the domain Archea. According to Spang et al. Lokiarchaeota forms a monophyletic group with eukaryotes in phylogenomic analyses. The associated genomes also encode an expanded repertoire of eukaryotic signature proteins that are suggestive of sophisticated membrane remodelling capabilities. These data suggest a two-domain system as opposed to the classical three-domain system.\n",
    "id": "164897",
    "title": "Three-domain system"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=10705831",
    "text": "Isochore (genetics)\n\nIn genetics, an isochore is a large region of DNA (greater than 300 kb) with a high degree uniformity in guanine (G) and cytosine (C): G-\nC and C-G (collectively GC content).\n\nBernardi and colleagues first uncovered the compositional non-uniformity within vertebrate genomes using thermal melting and density gradient\ncentrifugation. The DNA\nfragments extracted by the gradient centrifugation were later termed \"isochores\", which was subsequently defined as \"very long (much greater than 200 KB) DNA segments\" that \"are fairly homogeneous in base composition and belong to a small number of major classes\ndistinguished by differences in guanine-cytosine (GC) content\". Subsequently, the isochores \"grew\"\nand were claimed to be \">300 kb in size.\" The theory proposed that isochore’s\ncomposition varied markedly between \"warm-blooded\" (homeotherm) vertebrates and \"cold-blooded\" (poikilotherm) vertebrates and later became\nknown as the isochore theory.\n\nThe isochore theory purported that the genome of \"warm-blooded\" vertebrates (mammals and birds) are mosaics of long isochoric regions of\nalternating GC-poor and GC-rich composition, as opposed to the genome of \"cold-blooded\" vertebrates (fishes and amphibians) that were supposed to\nlack GC-rich isochores. These findings were explained by the thermodynamic stability hypothesis, attributing genomic structure to body temperature. GC-rich isochores were purported to be a form\nof adaptation to environmental pressures, as an increase in genomic GC-content could protect DNA, RNA, and proteins from degradation by heat.\nDespite its attractive simplicity, the thermodynamic stability hypothesis has been repeatedly shown to be in error \n. Many\nauthors showed the absence of a relationship between temperature and GC-content in vertebrates, while others showed the existence of GC-rich domains in \"cold-blooded\"\nvertebrates such as crocodiles, amphibians, and fish.\n\nThe isochore theory was the first to identify the nonuniformity of nucleotide composition within vertebrate genomes and predict that the genome\nof \"warm-blooded\" vertebrates such as mammals and birds are mosaic of isochores (Bernardi et al. 1985). The human genome, for example, was\ndescribed as a mosaic of alternating low and high GC content isochores belonging to five compositional families, L1, L2, H1, H2, and H3, whose\ncorresponding ranges of GC contents were said to be <38%, 38%-42%, 42%-47%, 47%-52%, and >52%, respectively.\n\nThe main predictions of the isochore theory are that: \n\nTwo opposite explanations that endeavored to explain the formations of isochores were vigorously debated\nas part of the neutralist-selectionist controversy. The first view was that isochores reflect variable mutation processes among genomic regions consistent with the neutral model. Alternatively,\nisochores were posited as a result of natural selection for certain compositional environment required by certain genes. Several hypotheses derive from the\nselectionist view, such as the thermodynamic stability hypothesis and the biased gene conversion\nhypothesis. Thus far, none of the theories\nprovides a comprehensive explanation to the genome structure, and the topic is still under debate.\n\nThe isochore theory became one of the most useful theories in molecular evolution for many\nyears. It was the first and most comprehensive attempt to explain the long-range compositional heterogeneity of vertebrate genomes within an\nevolutionary framework. Despite the interest in the early years in the isochore model, in recent years, the theory’s methodology, terminology,\nand predictions have been challenged.\n\nBecause this theory was proposed in the past century before complete genomes were sequences, it could not be fully tested for nearly 30 years. In\nthe beginning of the 21st century, when the first genomes were made available it was clear that isochores do not exist in the human genome \nnor in other mammalian genomes. When failed to find isochores,\nmany attacked the very existence of isochores. The most important predictor of\nisochores, GC3 was shown to have no predictable power to the GC content of nearby genomic\nregions, refuting findings from over 30 years of research, which were the basis for many isochore studies. Isochore-originators replied that the\nterm was misinterpreted as isochores are not \"homogeneous\" but rather \"fairly homogeneous regions\" with \"a heterogeneous nature (especially) of GC-rich regions at the 5 kb scale\", which only added to the already\ngrowing confusion. The reason for this ongoing frustration was the ambiguous definition of isochores as \"long\" and \"homogeneous\",\nallowed some researchers to discover \"isochores\" and others to dismiss them, although both camps used the same data.\n\nThe unfortunate side effect of this controversy was an \"arms race\" in which isochores are frequently redefined and relabeled following\nconflicting findings that failed to reveal \"mosaic of isochores.\" The\nunfortunate outcomes of this controversy and the following terminological-methodological mud were the loss of interest in isochores by the\nscientific community. When the most important core-concept in isochoric literature, the thermodynamic stability hypothesis, was rejected, the\ntheory lost its appeal. Even today, there is no clear definition to isochores nor is there an algorithm that detects isochores. Isochores are detected manually by visual inspection of GC content curves\n, however because this approach lacks scientific merit and\nis difficult to replicate by independent groups, the findings remain disputed.\n\nAs the study of isochores was \"de facto\" abandoned by most scientists, an alternative theory was\nproposed to describe the compositional organization of genomes in accordance with the most recent genomic studies. The Compositional Domain Model depicts genomes as a medley of short and long homogeneous and nonhomogeneous domains. The theory defines \"compositional domains\" as genomic regions with distinct GC-contents as determined by a computational segmentation algorithm. The homogeneity of compositional domains is compared to that of the chromosome on which they reside using the F-test, which separated them into compositionally homogeneous domains and compositionally nonhomogeneous domains based on the outcome of test. Compositionally homogeneous domains that are sufficiently long (≥ 300 kb) are termed isochores or isochoric domains. These terms are in accordance with the literature as they provide clear distinction between isochoric- and nonisochoric-domains.\n\nA comprehensive study of the human genome unraveled a genomic organization where two-thirds of the genome is a mixture of many short compositionally homogeneous domains and relatively few long ones. The remaining portion of the genome is composed of nonhomogeneous domains. In\nterms of coverage, only 1% of the total number of compositionally homogeneous domains could be considered \"isochores\" which covered less than 20% of the genome.\n\nSince its inception the theory received wide attention and was extensively used to explain findings emerging from over dozen new genome\nsequencing studies.\nopen, such as which evolutionary forces shaped the structure of compositional domain and how do they differ between different species?\n",
    "id": "10705831",
    "title": "Isochore (genetics)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=2783668",
    "text": "Infraspecific name\n\nIn botany, an infraspecific name is the scientific name for any taxon below the rank of species, i.e. an infraspecific taxon. (A \"taxon\", plural \"taxa\", is a group of organisms to be given a particular name.) The scientific names of botanical taxa are regulated by the \"International Code of Nomenclature for algae, fungi, and plants\" (ICN). This specifies a 'three part name' for infraspecific taxa, plus a 'connecting term' to indicate the rank of the name. An example of such a name is \"Astrophytum myriostigma\" subvar. \"glabrum\", the name of a subvariety of the species \"Astrophytum myriostigma\" (bishop's hat cactus).\n\nNames below the rank of species of cultivated kinds of plants and of animals are regulated by different codes of nomenclature and are formed somewhat differently.\n\nArticle 24 of the ICN describes how infraspecific names are constructed. The order of the three parts of an infraspecific name is:\nIt is customary to italicize all three parts of such a name, but not the connecting term. For example:\n\n\nThe recommended abbreviations for ranks below species are, for example:\n\nLike specific epithets, infraspecific epithets cannot be used in isolation as names. Thus the name of a particular species of \"Acanthocalycium\" is \"Acanthocalycium klimpelianum\", which can be abbreviated to \"A. klimpelianum\" where the context makes the genus clear. The species cannot be referred to as just \"klimpelianum\". In the same way, the name of a particular variety of \"Acanthocalycium klimpelianum\" is \"Acanthocalycium klimpelianum\" var. \"macranthum\", which can be abbreviated to \"A. k.\" var. \"macranthum\" where the context makes the species clear. The variety cannot be referred to as just \"macranthum\".\n\nSometimes more than three parts will be given; strictly speaking, this is not a name, but a classification. The ICN gives the example of \"Saxifraga aizoon\" var. \"aizoon\" subvar. \"brevifolia\" f. \"multicaulis\" subf. \"surculosa\"; the name of the subform would be \"Saxifraga aizoon\" subf. \"surculosa\".\n\nFor a proposed infraspecific name to be legitimate it must be in accordance with all the rules of the ICN. Only some of the main points are described here.\n\nA key concept in botanical names is that of a type. In many cases the type will be a particular preserved specimen stored in a herbarium, although there are other kinds of type. Like other names, an infraspecific name is attached to a type. Whether a plant should be given a particular infraspecific name can then be decided by comparing it to the type.\n\nThere is no requirement for a species to be divided into infraspecific taxa, of whatever rank; in other words, a species does not have to have subspecies, varieties, forms, etc. However, if infraspecific ranks are created, then the name of the type of the species must repeat the specific epithet as its infraspecific epithet. The type acquires this name automatically as soon as any infraspecific rank is created. As an example, consider \"Poa secunda\" , whose type specimen is in the Wisconsin State Herbarium.\n\nThe same epithet can be used again within a species, at whatever level, only if the names with the re-used epithet are attached to the same type. Thus there can be a form called \"Poa secunda\" f. \"juncifolia\" as well as the subspecies \"Poa secunda\" subsp. \"juncifolia\" if, and only if, the type specimen of \"Poa secunda\" f. \"juncifolia\" is the same as the type specimen of \"Poa secunda\" subsp. \"juncifolia\" (in other words, if there is a single type specimen whose classification is \"Poa secunda\" subsp. \"juncifolia\" f. \"juncifolia\").\n\nIf two infraspecific taxa which have different types are accidentally given the same epithet, then a homonym has been created. The earliest published name is the legitimate one and the other must be changed.\n\nWhen indicating authors for infraspecific names, it is possible to show either just the author(s) of the final, infraspecific epithet, or the authors of both the specific and the infraspecific epithets. Examples:\n\n\nIn zoological nomenclature, names of taxa below species rank are formed somewhat differently, using a trinomen or 'trinomial name'. No connecting term is required as there is only one rank below species, the subspecies.\n\nThe \"ICN\" does not regulate the names of cultivated plants, of cultivars, i.e. plants specifically created for use in agriculture or horticulture. Such names are regulated by the \"International Code of Nomenclature for Cultivated Plants\" (ICNCP).\n\nAlthough logically below the rank of species (and hence \"infraspecific\"), a cultivar name may be attached to any scientific name at the genus level or below. The minimum requirement is to specify a genus name. For example, \"Achillea\" 'Cerise Queen' is a cultivar; \"Pinus nigra\" 'Arnold Sentinel' is a cultivar of the species \"P. nigra\" (which is propagated vegetatively, by cloning).\n\n",
    "id": "2783668",
    "title": "Infraspecific name"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1478634",
    "text": "Systema Naturae\n\nThe tenth edition of this book (1758) is considered the starting point of zoological nomenclature. In 1766–1768 Linnaeus published the much enhanced 12th edition, the last under his authorship. Another again enhanced work in the same style and entitled \"\"\" was published by Johann Friedrich Gmelin between 1788 and 1793. Since at least the early 1900s zoologists commonly recognized this as the last edition belonging to this series. It was also officially regarded by the ICZN in Opinion 296 (26 Oct 1954) as the 13th edition of \"Systema Naturae\".\n\nLinnaeus (later known as \"Carl von Linné\", after his ennoblement in 1761) published the first edition of ' in the year 1735, during his stay in the Netherlands. As was customary for the scientific literature of its day, the book was published in Latin. In it, he outlined his ideas for the hierarchical classification of the natural world, dividing it into the animal kingdom ('), the plant kingdom ('), and the \"mineral kingdom\" (').\n\nLinnaeus's \"Systema Naturae\" lists only about 10,000 species of organisms, of which about 6,000 are plants and 4,236 are animals. According to the historian of botany William T. Stearn, \"Even in 1753 he believed that the number of species of plants in the whole world would hardly reach 10,000; in his whole career he named about 7,700 species of flowering plants.\"\n\nLinnaeus developed his classification of the plant kingdom in an attempt to describe and understand the natural world as a reflection of the logic of God's creation. His sexual system, where species with the same number of stamens were treated in the same group, was convenient but in his view artificial. Linnaeus believed in God's creation, and that there were no deeper relationships to be expressed. He is frequently quoted to have said. \"God created, Linnaeus organized\". The classification of animals was more natural. For instance, humans were for the first time placed together with other primates, as Anthropomorpha.\n\nAs a result of the popularity of the work, and the number of new specimens sent to him from around the world, Linnaeus kept publishing new and ever-expanding editions of his work. It grew from eleven very large pages in the first edition (1735) to 2,400 pages in the 12th edition (1766–1768). Also, as the work progressed, he made changes: in the first edition, whales were classified as fishes, following the work of Linnaeus' friend and \"father of ichthyology\" Peter Artedi; in the 10th edition, published in 1758, whales were moved into the mammal class. In this same edition, he introduced two-part names (see binomen) for animal species, something that he had done for plant species (see binary name) in the 1753 publication of \"\". The system eventually developed into modern Linnaean taxonomy, a hierarchically organized biological classification.\n\nAfter Linnaeus' health declined in the early 1770s, publication of editions of \"Systema Naturae\" went in two directions. Another Swedish scientist, Johan Andreas Murray issued the \"Regnum Vegetabile\" section separately in 1774 as the \"Systema Vegetabilium\", rather confusingly labelled the 13th edition. Meanwhile, a 13th edition of the entire \"Systema\" appeared in parts between 1788 and 1793. It was as the \"Systema Vegetabilium\" that Linnaeus' work became widely known in England following translation from the Latin by the Lichfield Botanical Society, as \"A System of Vegetables\" (1783–1785).\n\nIn his ', Linnaeus established three kingdoms, namely ', ' and '. This approach, the Animal, Vegetable and Mineral Kingdoms, survives until today in the popular mind, notably in the form of parlour games: \"Is it animal, vegetable or mineral?\". The classification was based on five levels: kingdom, class, order, genus, and species. While species and genus was seen as God-given (or \"natural\"), the three higher levels were seen by Linnaeus as constructs. The concept behind the set ranks being applied to all groups was to make a system that was easy to remember and navigate, a task which most say he succeeded in.\nLinnaeus's work had a huge impact on science; it was indispensable as a foundation for biological nomenclature, now regulated by the Nomenclature Codes. Two of his works, the first edition of the ' (1753) for plants and the 10th edition of the \"Systema Naturæ\" (1758), are accepted among the starting points of nomenclature. Most of his names for species and genera were published at very early dates, thus take priority over those of other, later works. In zoology there is one exception, which is a monograph on Swedish spiders, ', published by Carl Clerck in 1757, so the names established there take priority over the Linnean names. However, his impact on science was not because of the value of his taxonomy. His talent for attracting skillful young students and sending them abroad to collect made his work far more influential than that of his contemporaries. At the close of the 18th century, his system had effectively become the standard system for biological classification.\n\nOnly in the Animal Kingdom is the higher taxonomy of Linnaeus still more or less recognizable and some of these names are still in use, but usually not quite for the same groups as used by Linnaeus. He divided the Animal Kingdom into six classes; in the tenth edition (1758), these were:\n\n\nThe orders and classes of plants, according to his ', were never intended to represent natural groups (as opposed to his ' in his \") but only for use in identification. They were used in that sense well into the 19th century.\n\nThe Linnaean classes for plants, in the Sexual System, were:\n\nLinnaeus's taxonomy of minerals has long since fallen out of use. In the 10th edition, 1758, of the \"\", the Linnaean classes were:\n\n\nThe Gmelin thirteenth (\"decima tertia\") edition of 1788–1793, is likely to be confused with another thirteenth edition prepared by Johan Andreas Murray in 1774, and then subsequently in further editions, all under a revised name of \"Systema Vegetabilium\".\n\nThe dates of publication for Gmelin's edition were the following: \n\n\n",
    "id": "1478634",
    "title": "Systema Naturae"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=6781871",
    "text": "Cline (biology)\n\nIn biology and ecology, an ecocline or simply cline (from \"to possess or exhibit gradient, to lean\") is an ecotone in which a series of biocommunities display a continuous gradient. The term was coined by the English evolutionary biologist Julian Huxley in 1938.\n\nMore technically, clines consist of ecotypes or forms of species that exhibit gradual phenotypic and/or genetic differences over a geographical area, typically as a result of environmental heterogeneity.\nGenetically, clines result from the change of allele frequencies within the gene pool of the group of taxa in question. Clines may manifest in time and/or space.\n\nIn ecology, spatial clines have led to gradient analysis where the abundance and distribution of organisms is rendered by sinusoidal curves on the plane. From these curves can be extracted that populations occupy zones of maximum and minimum presence, according to their special needs and tolerances imposed by their environment.\nTypically, a well-marked cline does not allow for a delineation of subspecies, as it is then impossible, by definition, to draw any further clear dividing lines between populations. In population genetics, a cline could include a spectrum of subspecies, as allele and haplotype frequencies tend to vary over a larger space; moreover, in evolution, genetic lineage sorting usually lags behind the establishment of locally differentiated phenotypes. Regardless, in neither case will such a variation yield different species, as long as the populations, though geographically spread, can interbreed one with another. \n\nRing species are a distinct type of cline where the geographical distribution in question is circular in shape, so that the two ends of the cline overlap with one another, giving two adjacent populations that rarely interbreed due to the cumulative effect of the many changes in phenotype along the cline. The populations elsewhere along the cline interbreed with their geographically adjacent populations as in a standard cline. Ring species present an interesting problem for those who seek to divide the living world into discrete species, as chain species are closely related to speciation (in this case, parapatric).\n\nIn the case of \"Larus\" gulls, the habitats of the end populations even overlap, which introduces questions as to what constitutes a species: nowhere along the cline can a line be drawn between the populations, but they are unable to interbreed.\nHowever a recent study (Liebers et al., 2004) has provided genetic evidence that this example is far more complicated than presented here, and likely does not constitute a typical ring species.\n",
    "id": "6781871",
    "title": "Cline (biology)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1056866",
    "text": "GC-content\n\nIn molecular biology and genetics, GC-content (or guanine-cytosine content) is the percentage of nitrogenous bases on a DNA or RNA molecule that are either guanine or cytosine (from a possibility of four different ones, also including adenine and thymine in DNA and adenine and uracil in RNA). This may refer to a certain fragment of DNA or RNA, or that of the whole genome. When it refers to a fragment of the genetic material, it may denote the GC-content of section of a gene (domain), single gene, group of genes (or gene clusters), or even a non-coding region. G (guanine) and C (cytosine) undergo a specific hydrogen bonding, whereas A (adenine) bonds specifically with T (thymine, in DNA) or U (uracil, in RNA).\n\nThe GC pair is bound by three hydrogen bonds, while AT and AU pairs are bound by two hydrogen bonds. To emphasize this difference in the number of hydrogen bonds, the base pairings can be represented as respectively G≡C versus A=T and A=U. DNA with low GC-content is less stable than DNA with high GC-content; however, the hydrogen bonds themselves do not have a particularly significant impact on stabilization, the stabilization is due mainly to interactions of base stacking.\nIn spite of the higher thermostability conferred to the genetic material, it has been observed that at least some bacteria species with DNA of high GC-content undergo autolysis more readily, thereby reducing the longevity of the cell \"per se\". Due to the thermostability given to the genetic materials in high GC organisms, it was commonly believed that the GC content played a necessary role in adaptation temperatures, a hypothesis that was refuted in 2001.\nHowever, it has been shown that there is a strong correlation between the prokaryotic optimal growth at higher temperatures and the GC content of structured RNAs (such as ribosomal RNA, transfer RNA, and many other non-coding RNAs). The AU base pairs are less stable than the GC base pairs previously attributed to GC bonds containing 3 hydrogen bonds and AU having only 2 hydrogen bonds, making high-GC-content RNA structures more resistant to the effects of high temperatures.\nMore recently, it has been proved that the most stabilizing factor of thermal stability of double stranded nucleic acids is actually due to the base stackings of adjacent bases, rather than the number of hydrogen bonds between the bases.There is more favorable stacking energy for G:C pairs because of the relative positions of exocyclic groups than in the A:U pairs. Additionally, there is a correlation between the order in which the bases stack and thermal stability.\n\nIn PCR experiments, the GC-content of primers are used to predict their annealing temperature to the template DNA. A higher GC-content level indicates a relatively higher melting temperature.\n\nGC content is usually expressed as a percentage value, but sometimes as a ratio (called G+C ratio or GC-ratio). GC-content percentage is calculated as\n\nwhereas the AT/GC ratio is calculated as \n\nThe GC-content percentages as well as GC-ratio can be measured by several means, but one of the simplest methods is to measure what is called the melting temperature of the DNA double helix using spectrophotometry. The absorbance of DNA at a wavelength of 260 nm increases fairly sharply when the double-stranded DNA separates into two single strands when sufficiently heated. The most commonly used protocol for determining GC ratios uses flow cytometry for large number of samples.\n\nIn alternative manner, if the DNA or RNA molecule under investigation has been sequenced then the GC-content can be accurately calculated by simple arithmetic or by using the free online GC calculator.\n\nGC ratios within a genome is found to be markedly variable. These variations in GC ratio within the genomes of more complex organisms result in a mosaic-like formation with islet regions called isochores. This results in the variations in staining intensity in the chromosomes. GC-rich isochores include in them many protein coding genes, and thus determination of ratio of these specific regions contributes in mapping gene-rich regions of the genome.\n\nWithin a long region of genomic sequence, genes are often characterised by having a higher GC-content in contrast to the background GC-content for the entire genome. Evidence of GC ratio with that of length of the coding region of a gene has shown that the length of the coding sequence is directly proportional to higher G+C content. This has been pointed to the fact that the stop codon has a bias towards A and T nucleotides, and, thus, the shorter the sequence the higher the AT bias.\n\nGC content is found to be variable with different organisms, the process of which is envisaged to be contributed to by variation in selection, mutational bias, and biased recombination-associated DNA repair. The species problem in prokaryotic taxonomy has led to various suggestions in classifying bacteria, and the \"ad hoc committee on reconciliation of approaches to bacterial systematics\" has recommended use of GC ratios in higher level hierarchical classification. For example, the Actinobacteria are characterised as \"high GC-content bacteria\". In \"Streptomyces coelicolor\" A3(2), GC content is 72%. The GC-content of Yeast (\"Saccharomyces cerevisiae\") is 38%, and that of another common model organism, thale cress (\"Arabidopsis thaliana\"), is 36%. Because of the nature of the genetic code, it is virtually impossible for an organism to have a genome with a GC-content approaching either 0% or 100%. A species with an extremely low GC-content is \"Plasmodium falciparum\" (GC% = ~20%), and it is usually common to refer to such examples as being AT-rich instead of GC-poor.\n\n\n",
    "id": "1056866",
    "title": "GC-content"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1694221",
    "text": "Crown eukaryotes\n\nCrown eukaryotes are an artificial group of eukaryotic organisms found at the top of molecular phylogenetic trees including both eukaryotes and prokaryotes. They were originally thought to represent a late step of eukaryotic evolution (somewhat similar to a crown group) because they include multicellular and macroscopic lifeforms that represent the majority of the biomass of the planet while accounting for less than 1% of the genetic diversity. However, they are in fact the result of an artificial clustering of eukaryotic organisms with slowly evolving gene sequences. They are thus not a crown that excludes simpler eukaryotes, but correspond roughly to the initial radiation of eukaryotes. All eukaryotic lineages branching below the \"crown\" in phylogenetic trees are misplaced because of the long branch attraction phenomenon.\n\n\n",
    "id": "1694221",
    "title": "Crown eukaryotes"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=4721036",
    "text": "Circumscriptional name\n\nIn biological classification, circumscriptional names are taxon names that are not ruled by ICZN and are defined by the particular set of members included. Circumscriptional names are used mainly for taxa above family-group level (e. g. order or class), but can be also used for taxa of any ranks, as well as for rank-less taxa.\n\nNon-typified names other than those of the genus- or species-group constitute the majority of generally accepted names of taxa higher than superfamily. The ICZN regulates names of taxa up to family group rank (i. e. superfamily). There are no generally accepted rules of naming higher taxa (orders, classes, phyla, etc.). Under the approach of circumscription-based (circumscriptional) nomenclatures, a circumscriptional name is associated with a certain circumscription of a taxon without regard of its rank or position. Circumscriptional nomenclature, parallel with another rank-free nomenclature — hierarchical one, is used in cladoendesis.\n\nSome authors advocate introducing a mandatory standardized typified nomenclature of higher taxa. They suggest all names of higher taxa to be derived in the same manner as family-group names, i.e. by modifying names of type genera with endings to reflect the rank. There is no consensus on what such higher rank endings should be. A number of established practices exist as to the use of typified names of higher taxa, depending on animal group.\n\n\n\n",
    "id": "4721036",
    "title": "Circumscriptional name"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=35590090",
    "text": "Compositional domain\n\nA compositional domain in genetics is a region of DNA with a distinct guanine (G) and cytosine (C) G-C and C-G content (collectively GC content). The homogeneity of compositional domains is compared to that of the chromosome on which they reside. As such, compositional domains can be homogeneous or nonhomogeneous domains. Compositionally homogeneous domains that are sufficiently long (= 300 kb) are termed isochores or isochoric domains.\n\nThe compositional domain model was proposed as an alternative to the isochoric model. The isochore model was proposed by Bernardi and colleagues to explain the observed non-uniformity of genomic fragments in the genome. However, recent sequencing of complete genomic data refuted the isochoric model. Its main predictions were:\n\nThe compositional domain model describes the genome as a mosaic of short and long homogeneous and nonhomogeneous domains. The composition and organization of the domains were shaped by different evolutionary processes that either fused or broke down the domains. This genomic organization model was confirmed in many new genomic studies of cow, honeybee, sea urchin, body louse, \"Nasonia\", beetle, and ant genomes. The human genome was described as consisting of a mixture of compositionally nonhomogeneous domains with numerous short compositionally homogeneous domains and relatively few long ones.\n\n",
    "id": "35590090",
    "title": "Compositional domain"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=5176156",
    "text": "Conserved name\n\nA conserved name or nomen conservandum (plural nomina conservanda, abbreviated as nom. cons.) is a scientific name that has specific nomenclatural protection. \"Nomen conservandum\" is a Latin term, meaning \"a name to be conserved\". The terms are often used interchangeably, such as by the International Code of Nomenclature for algae, fungi, and plants (ICN), while the International Code of Zoological Nomenclature favours \"conserved name\". The process for conserving botanical names is different from that for zoological names. Under the botanical code, names may also be \"suppressed\", nomen rejiciendum (plural nomina rejicienda or nomina utique rejicienda, abbreviated as nom. rej.), or rejected in favour of a particular conserved name, and combinations based on a suppressed name are also listed as \"nom. rej\".\n\nIn botanical nomenclature, conservation is a nomenclatural procedure governed by Art. 14 of the ICN. Its purpose is \nConservation is possible only for names at the rank of family, genus or species.\n\nIt may effect a change in original spelling, type, or (most commonly) priority.\n\nBesides conservation/rejection of names of certain ranks (Art. 14), the ICN also offers the option of outright rejection of a name (nomen utique rejiciendum) also called suppressed name under Article 56, another way of creating a \"nomen rejiciendum\" that cannot be used anymore. Outright rejection is possible for a name at any rank.\n\nRejection (suppression) of individual names is distinct from suppression of works (opera utique oppressa) under article 34, which allows for listing certain taxonomic ranks in certain publications which are considered not to include any validly published names.\n\nConflicting conserved names are treated according to the normal rules of priority. Separate proposals (informally referred to as \"superconservation\" proposals) may be made to protect a conserved name that would be overtaken by another. However, conservation has different consequences depending on the type of name that is conserved:\n\nConserved and rejected names (and suppressed names) are listed in the appendices to the ICN. As of the 2012 (Melbourne) edition, a separate volume holds the bulk of the appendices (except appendix I, on names of hybrids). The substance of the second volume is generated from a database which also holds a history of published proposals and their outcomes, the binding decisions on whether a name is validly published (article 38.4) and on whether it is a homonym (article 53.5). The database can be queried online.\n\n\nIn the course of time there have been different standards for the majority required for a decision. However, for decades the Nomenclature Section has required a 60% majority for an inclusion in the \"Code\", and the Committees have followed this example, in 1996 adopting a 60% majority for a decision.\n\nFor zoology, the term \"conserved name\", rather than \"nomen conservandum\", is used in the \"International Code of Zoological Nomenclature\", although informally both terms are used interchangeably.\n\nIn the glossary of the \"International Code of Zoological Nomenclature\" (the code for names of animals, one of several nomenclature codes), this definition is given:\n\nThis is a more generalized definition than the one for \"nomen protectum\", which is specifically a conserved name that is either a junior synonym or homonym that is in use because the senior synonym or homonym has been made a \"nomen oblitum\" (\"forgotten name\").\n\nAn example of a conserved name is the dinosaur genus name \"Pachycephalosaurus\", which was formally described in 1943. Later, \"Tylosteus\" (which was formally described in 1872) was found to be the same genus as \"Pachycephalosaurus\" (a synonym). By the usual rules, the genus \"Tylosteus\" has precedence and would normally be the correct name. But the International Commission on Zoological Nomenclature (ICZN) ruled that the name \"Pachycephalosaurus\" was to be given precedence and treated as the valid name, because it was in more common use and better known to scientists.\n\nThe ICZN's procedural details are different from those in botany, but the basic operating principle is the same, with petitions submitted to the commission for review.\n\n\n",
    "id": "5176156",
    "title": "Conserved name"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=37237272",
    "text": "Eocyte hypothesis\n\nThe Eocyte hypothesis is a biological classification that indicates eukaryotes emerged within the prokaryotic Crenarchaeota (formerly known as eocytes), a phylum within the archaea. This hypothesis was originally proposed by James A. Lake and colleagues in 1984 based on the discovery that the shapes of ribosomes in the Crenarchaeota and eukaryotes are more similar to each other than to either bacteria or the second major kingdom of archaea, the Euryarchaeota.\n\nThe eocyte hypothesis gained considerable attention after its introduction due to the interest in determining the origin of the eukaryotic cell. This hypothesis has primarily been in contrast with the three-domain system introduced by Carl Woese in 1977. Additional evidence supporting the eocyte hypothesis was published in the 1980s, but despite fairly unequivocal evidence, support waned in favor of the three-domain system.\n\nWith advancements in genomics, the eocyte hypothesis experienced a revival beginning in the mid-2000s. As more archaeal genomes were sequenced, numerous genes coding for eukaryotic traits have been discovered in various archaean phyla, seemingly providing support for the eocyte hypothesis. In addition to a Crenarchaeal origin of eukaryotes, some studies have suggested that eukaryotes may also have originated in the Thaumarchaeota. A superphylum - TACK - has been proposed that includes the Thaumarchaeota, Crenarchaeota, and other groups of archaea, so that this superphylum may be related to the origin of eukaryotes.\n\nAs a result of metagenomic analysis of material found nearby hydrothermal vents, another superphylum -- Asgard -- has been named and proposed to be more closely related to the original eukaryote and a sister group to TACK more recently. \nThe eocyte tree root may be located in the RNA World, that is the root organism may have been a Ribocyte (aka Ribocell). For cellular DNA and DNA handling an \"out of virus\" scenarie has been proposed, i. e. string genetic information in DNA may have been an invention performed by viruses later handed over to Ribicytes twice transforming them as into bacteria as into archaea. \nAll these findings do not change the eocyte tree as given here in principle, but zoom into a higher resolution of it.\n",
    "id": "37237272",
    "title": "Eocyte hypothesis"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=2133291",
    "text": "Pathovar\n\nA pathovar is a bacterial strain or set of strains with the same or similar characteristics, that is differentiated at infrasubspecific level from other strains of the same species or subspecies on the basis of distinctive pathogenicity to one or more plant hosts.\n\nPathovars are named as a ternary or quaternary addition to the species binomial name, for example the bacterium that causes citrus canker \"Xanthomonas axonopodis\", has several pathovars with different host ranges, \"X. axonopodis\" pv. \"citri\" is one of them; the abbreviation 'pv.' means pathovar.\n\nThe type strains of pathovars are pathotypes, which are distinguished from the types (holotype, neotype, etc.) of the species to which the pathovar belongs. \n\n",
    "id": "2133291",
    "title": "Pathovar"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=5685431",
    "text": "Ichnotaxon\n\nAn ichnotaxon (plural ichnotaxa) is defined by the International Code of Zoological Nomenclature as \"a taxon based on the fossilized work of an organism\", that is, the non-human equivalent of an artifact.\n\nIchnotaxa are names used to identify and distinguish morphologically distinctive ichnofossils, more commonly known as trace fossils. They are assigned genus and species ranks by ichnologists, much like organisms in Linnaean taxonomy. These are known as ichnogenera and ichnospecies, respectively. \"Ichnogenus\" and \"ichnospecies\" are commonly abbreviated as \"igen.\" and \"isp.\". The binomial names of ichnospecies and their genera are to be written in italics.\n\nMost researchers classify trace fossils only as far as the ichnogenus rank, based upon trace fossils that resemble each other in morphology but have subtle differences. Some authors have constructed detailed hierarchies up to ichnosuperclass, recognizing such fine detail as to identify ichnosuperorder and ichnoinfraclass, but such attempts are controversial.\n\n\"Ichnotaxa\" comes from the Greek ίχνος, \"ichnos\" meaning \"track\" and ταξις, \"taxis\" meaning \"ordering\".\n\nDue to the chaotic nature of trace fossil classification, several ichnogenera hold names normally affiliated with animal body fossils or plant fossils. For example, many ichnogenera are named with the suffix \"-phycus\" due to misidentification as algae.\n\nEdward Hitchcock was the first to use the now common \"-ichnus\" suffix in 1858, with \"Cochlichnus\".\n\nDue to trace fossils' history of being difficult to classify, there have been several attempts to enforce consistency in the naming of ichnotaxa.\n\nIn 1961, the International Commission on Zoological Nomenclature ruled that most trace fossil taxa named after 1930 would be no longer available.\n\n\n",
    "id": "5685431",
    "title": "Ichnotaxon"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=40547081",
    "text": "Kew Rule\n\nThe Kew Rule was used by some authors to determine the application of synonymous names in botanical nomenclature up to about 1906, but was and still is contrary to codes of botanical nomenclature including the International Code of Nomenclature for algae, fungi, and plants. Index Kewensis, a publication that aimed to list all botanical names for seed plants at the ranks of species and genus, used the Kew Rule until its \"Supplement IV\" was published in 1913 (prepared 1906–1910).\n\nThe Kew Rule applied rules of priority in a more flexible way, so that when transferring a species to a new genus, there was no requirement to retain the epithet of the original species name, and future priority of the new name was counted from the time the species was transferred to the new genus. The effect has been summarized as \"nomenclature used by an established monographer or in a major publication should be adopted\". This is contrary to the modern article 11.4 of the Code of Nomenclature.\n\nThe first discussion in print of what was to become known as the Kew Rule appears to have occurred in 1877 between Henry Trimen and Alphonse Pyramus de Candolle. Trimen did not think it was reasonable for older names discovered in the literature to destabilize the nomenclature that had been well accepted:Probably all botanists are agreed that it is very desirable to retain when possible old specific names, but some of the best authors do not certainly consider themselves bound by any generally accepted rule in this matter. Still less will they be inclined to allow that a writer is at liberty, as M. de Candolle thinks, to reject the specific appellations made by an author whose genera are accepted, in favour of older ones in other genera. It will appear to such that to do this is to needlessly create in each case another synonym.\n\nThe first botanical code of nomenclature that declared itself to be binding was the 1906 \"Règles internationales de la nomenclature botanique adoptées par le Congres International de Botanique de Vienne 1905\" that followed from the 1905 International Botanical Congress. The Kew Rule was outlawed by this code.\n\nThe end of the Kew Rule brought about considerable upheaval in botanical nomenclature. Many new species names were coined to resurrect older epithets, for example, in 1917 Willis Jepson wrote:\n\"\"The plant so long known as \"Brodiaea grandiflora\" Smith ... [was] first published as \"Hookera coronaria\" Salisbury (1806). The correct name, then, is \"Brodiaea coronaria\" Jepson, \"n. comb.\"\"\n\nNames that had previously been conserved to improve the stability of well-known plant names often now no longer required conservation, and other names that had been formed using the Kew Rule and had become well known, were illegitimate. The entire previous list of conserved and rejected names was consequently replaced in 1959 with a reworked list.\n\nPreviously overlooked botanical literature has continued to yield new examples of forgotten older names for more than 100 years since the Kew Rule was banished from the International Code of Nomenclature.\n",
    "id": "40547081",
    "title": "Kew Rule"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=7308831",
    "text": "Nomen novum\n\nIn biological nomenclature, a nomen novum (Latin for \"new name\"), new replacement name (or replacement name, new substitute name, substitute name) is a technical term. It indicates a scientific name that is created specifically to replace another scientific name, but only when this other name can not be used for technical, nomenclatural reasons (for example because it is a homonym: it is spelled the same as an existing, older name); it does not apply when a name is changed for taxonomic reasons (representing a change in scientific insight). It is frequently abbreviated, \"e.g.\" \"nomen nov.\", \"nom. nov.\", or more explicitly by the rank specified, \"e.g.\" \"superordo nov.\"\n\nIn zoology establishing a new replacement name is a nomenclatural act and it must be expressly proposed to substitute a previously established and available name.\n\nOften, the older name cannot be used because another animal was described earlier with exactly the same name. For example, Lindholm discovered in 1913 that a generic name \"Jelskia\" established by Bourguignat in 1877 for a European freshwater snail could not be used because another author Taczanowski had proposed the same name in 1871 for a spider. So Lindholm proposed a new replacement name \"Borysthenia\". This is an objective synonym of \"Jelskia\" Bourguignat, 1877, because he has the same type species, and is used today as \"Borysthenia\".\n\nAlso for names of species new replacement names are often necessary. New replacement names have been proposed since more than 100 years ago. In 1859 Bourguignat saw that the name \"Bulimus cinereus\" Mortillet, 1851 for an Italian snail could not be used because Reeve had proposed exactly the same name in 1848 for a completely different Bolivian snail. Since it was understood even then that the older name has always priority, Bourguignat proposed a new replacement name \"Bulimus psarolenus\", and also added a note why this was necessary. The Italian snail is known until today under the name \"Solatopupa psarolena\" (Bourguignat, 1859).\n\nA new replacement name must obey certain rules, not all of these are well known.\n\nNot every author who proposes a name for a species that already has another name, establishes a new replacement name. An author who writes \"The name of the insect species with the green wings shall be named X, this is the one that the other author has named Y\", does not establish a new replacement name (but a regular new name).\n\nThe International Code of Zoological Nomenclature prescribes that for a new replacement name, an expressed statement must be given by the author, which means an explicit statement concerning the process of replacing the previous name. It is not necessary to employ the term \"nomen novum\", but something must be expressed concerning the act of substituting a name. Implicit evidence (\"everybody knows why the author used that new name\") is not allowed at this occasion. Many zoologists do not know that this expressed statement is necessary, and therefore a variety of names are regarded as having been established as new replacement names (often including names that were mentioned without any description, which is fundamentally contrary to the rules).\n\nThe author who proposes a new replacement name must state exactly which name shall be replaced. It is not possible to mention three available synonyms at once, to be replaced. Usually the author explains why the new replacement name is needed.\n\nSometimes we read \"the species cannot keep this old name \"P. brasiliensis\", because it does not live in Brazil, so I propose a new name \"P. angolana\"\". Even though this would not justify a new replacement name under the Code's rules, the author believed that a new name was necessary and gave an expressed statement concerning the act of replacing. So the name \"P. angolana\" was made available at this occasion, and is an objective synonym of \"P. brasiliensis\".\n\nA new replacement name can only be used for a taxon if the name that it replaces cannot be used, as in the example above with the snail and the spider, or in the other example with the Italian and the Bolivian snail. The animal from Angola must keep its name \"brasiliensis\", because this is the older name.\n\nNew replacement names do not occur very frequently, but they are not extremely rare. About 1% of the currently used zoological names might be new replacement names. There are no exact statistics covering all animal groups. In 2200 names of species and 350 names of genera in European non-marine molluscs, which might be a representative group of animals, 0.7% of the specific and 3.4% of the generic names were correctly established as new replacement names (and a further 0.7% of the specific and 1.7% of the generic names have incorrectly been regarded as new replacement names by some authors).\n\n\n",
    "id": "7308831",
    "title": "Nomen novum"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=2438185",
    "text": "Sister group\n\nA sister group or sister taxon is a phylogenetic term denoting the closest relatives of another given unit in an evolutionary tree. The expression is most easily illustrated by a cladogram: A, B, and C each represent a taxon:\n\nThe sister group to A is B; conversely, the sister group to B is A. Groups A and B, together with all other descendants of their most recent common ancestor, form the clade AB. The sister group to clade AB is C.\n\nThe whole clade ABC is itself a subtree of a larger tree, which offers yet more sister group branches that are related but farther removed from the leaf nodes, such as A, B, and C.\n\nIn cladistic standards, A, B, and C may represent specimens, species, taxon-groups, etc. If they represent species, the term \"sister species\" is sometimes used.\n\nThe term \"sister group\" is used in phylogenetic analysis, and only groups identified in the analysis are labeled as sister groups. An example is in birds, whose sister group is commonly cited as the crocodiles, but that is true only when dealing with extant taxa. The bird family tree is rooted in the dinosaurs, making for a number of extinct groups branching off before coming to the last common ancestor of birds and crocodiles. Thus, the term sister group must be seen as a relative term, with the caveat that the sister group is the closest relative only among the groups/species/specimens that are included in the analysis.\n",
    "id": "2438185",
    "title": "Sister group"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=27813",
    "text": "Systematics\n\nBiological systematics is the study of the diversification of living forms, both past and present, and the relationships among living things through time. Relationships are visualized as evolutionary trees (synonyms: cladograms, phylogenetic trees, phylogenies). Phylogenies have two components: branching order (showing group relationships) and branch length (showing amount of evolution). Phylogenetic trees of species and higher taxa are used to study the evolution of traits (e.g., anatomical or molecular characteristics) and the distribution of organisms (biogeography). Systematics, in other words, is used to understand the evolutionary history of life on Earth.\n\nIn the study of biological systematics, researchers use the different branches to further understand the relationships between differing organisms. These branches are used to determine the applications and uses for modern day systematics.\n\nBiological systematics classifies species by using three specific branches. \"Numerical systematics\", or \"biometry\", uses biological statistics to identify and classify animals. \"Biochemical systematics\" classifies and identifies animals based on the analysis of the material that makes up the living part of a cell—such as the nucleus, organelles, and cytoplasm. \"Experimental systematics\" identifies and classifies animals based on the evolutionary units that comprise a species, as well as their importance in evolution itself. Factors such as mutations, genetic divergence, and hybridization all are considered evolutionary units.\n\nWith the specific branches, researchers are able to determine the applications and uses for modern-day systematics. These applications include: \n\nJohn Lindley provided an early definition of systematics in 1830, although he wrote of \"systematic botany\" rather than using the term \"systematics\".\n\nIn 1970 Michener \"et al.\" defined \"systematic biology\" and \"taxonomy\" (terms that are often confused and used interchangeably) in relationship to one another as follows:\nSystematic biology (hereafter called simply systematics) is the field that (a) provides scientific names for organisms, (b) describes them, (c) preserves collections of them, (d) provides classifications for the organisms, keys for their identification, and data on their distributions, (e) investigates their evolutionary histories, and (f) considers their environmental adaptations. This is a field with a long history that in recent years has experienced a notable renaissance, principally with respect to theoretical content. Part of the theoretical material has to do with evolutionary areas (topics e and f above), the rest relates especially to the problem of classification. Taxonomy is that part of Systematics concerned with topics (a) to (d) above.\nTaxonomy, systematic biology, systematics, biosystematics, scientific classification, biological classification, phylogenetics: At various times in history, all these words have had overlapping, related meanings. However, in modern usage, they can all be considered synonyms of each other.\n\nFor example, Webster's 9th New Collegiate Dictionary of 1987 treats \"classification\", \"taxonomy\", and \"systematics\" as synonyms. According to this work, the terms originated in 1790, c. 1828, and in 1888 respectively. Some claim systematics alone deals specifically with relationships through time, and that it can be synonymous with phylogenetics, broadly dealing with the inferred hierarchy of organisms. This means it would be a subset of taxonomy as it is sometimes regarded, but the inverse is claimed by others.\n\nEuropeans tend to use the terms \"systematics\" and \"biosystematics\" for the study of biodiversity as a whole, whereas North Americans tend to use \"taxonomy\" more frequently. However, taxonomy, and in particular alpha taxonomy, is more specifically the identification, description, and naming (i.e. nomenclature) of organisms,\nwhile \"classification\" focuses on placing organisms within hierarchical groups that show their relationships to other organisms. All of these biological disciplines can deal with both extinct and extant organisms.\n\nSystematics uses taxonomy as a primary tool in understanding, as nothing about an organism's relationships with other living things can be understood without it first being properly studied and described in sufficient detail to identify and classify it correctly. Scientific classifications are aids in recording and reporting information to other scientists and to laymen. The systematist, a scientist who specializes in systematics, must, therefore, be able to use existing classification systems, or at least know them well enough to skilfully justify not using them.\n\nPhenetics was an attempt to determine the relationships of organisms through a measure of overall similarity, making no distinction between plesiomorphies (shared ancestral traits) and apomorphies (derived traits). From the late-20th century onwards, it was superseded by cladistics, which rejects plesiomorphies in attempting to resolve the phylogeny of Earth's various organisms through time. systematists generally make extensive use of molecular biology and of computer programs to study organisms.\n\nTaxonomic characters are the taxonomic attributes that can be used to provide the evidence from which relationships (the phylogeny) between taxa are inferred. Kinds of taxonomic characters:\n\n\n\n",
    "id": "27813",
    "title": "Systematics"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=28676576",
    "text": "Svenska Spindlar\n\nThe book or (Swedish and Latin, respectively, for \"Swedish spiders\") was one of the major works of the Swedish arachnologist and entomologist Carl Alexander Clerck and appeared in Stockholm in the year 1757. It was the first comprehensive book on the spiders of Sweden and one of the first regional monographs of a group of animals worldwide. The full title of the work was \" – \", (\"Swedish spiders into their main genera separated, and as sixty and a few particular species described and with illuminated figures illustrated\") and included 162 pages of text (eight pages were unpaginated) and 6 colour plates. It was published in Swedish, with a Latin translation printed in a slightly smaller font below the Swedish text.\n\nClerck described in detail 67 species of Swedish spiders, and for the first time in a zoological work consistently applied binomial nomenclature as proposed by Carl Linnaeus and used for the first time for botanical names in his 1753 work \"Species Plantarum\", and which he presented in 1758 in the 10th edition of his work \"Systema Naturae\" for more than 4,000 animal species.\n\n\"Svenska Spindlar\" is the first zoological work to make systematic use of binomial nomenclature, and the only pre-Linnaean source to be recognised as a taxonomic authority for such names.\n\nClerck explained in the last (9th of the 2nd part) chapter of his work that in contrast to previous authors he used the term \"spider\" in the strict sense, for animals possessing eight eyes and separated prosoma and opisthosoma, and that his concept of this group of animals did not include Opiliones (because they had two eyes and a broadly joined prosoma and opisthosoma) and other groups of arachnids.\n\nFor all spiders Clerck used a single generic name (\"Araneus\"), to which was added a specific name which consisted of only one word. Each species was presented in the Swedish text with their Latin scientific names, followed by detailed information containing the exact dates when he had found the animals, and a detailed description of eyes, legs and body. The differences between the sexes were also described. Each species was illustrated in impressively accurate drawings printed on coloured copper plates which were bound at the end of the volume.\n\nBecause of the exceptionally thorough treatment of the spider species, the scientific names proposed by Clerck (which were adopted by Carl Linnaeus in his \"Systema Naturae\" in 1758 with only minor modifications) had traditionally been recognized by arachnologists as binomial and available. In 1959 the ICZN Commission decided that Clerck's work should be available for zoological nomenclature, but the International Code of Zoological Nomenclature did not mention Clerck's work. Only after 1999 was this officially recognized in the Code. This means that in case of doubt the spelling of a spider name as from Clerck's 1757 work has priority over that proposed by Linnaeus in 1758 (an example is \"Araneus\" instead of \"Aranea\"), and that Clerck's spiders were the first animals in modern zoology to have obtained an available scientific name in the Linnean system.\n\nIn the late 1800s, Clerck's 1757 work was commonly accepted as the first application of binomial nomenclature to spiders. In 1959 the ICZN Commission ruled that the date 1758 should be used for Clerck's names, this date 1758 was repeated to apply to Clerck's names in the 4th edition of the International Code of Zoological Nomenclature in 1999.\n\nIn a complete binomial name with author and year, the year corresponds to the year of publication of the original source. Since 2000, the ICZN Code includes an exception of this very basic rule. From the beginning on the new provision in the Code has been misunderstood by many researchers who believed that by setting the date for Clerck's work to 1758 (overriding its true date 1757) and the date for \"Systema Naturae\" to 1 January 1758, the priority was changed. In 2007, a case was even brought before the Commission because the researchers were no longer sure whether the generic name should be \"Araneus\" Clerck or \"Aranea\" Linnaeus. In their judgement the year 1758 for Clerck's \"Svenska Spindlar\" could be interpreted in a way that the Linnean work from 1 January 1758 should have priority. In 2009 the Commission saw itself forced to repeat once more, although this was already explicit in the Code's Article 3.1, that the name \"Araneus\" established by Clerck shall have priority and be used for the genus.\n\n\"Svenska Spindlar\" lists the following 67 species of spider; their current identities follow Platnick (2000–2010).\nChapter 2 (Araneidae, Tetragnathidae)\n\nChapter 3 (Theridiidae, Nesticidae, Linyphiidae)\n\nChapter 4 (Agelenidae, Clubionidae)\n\nChapter 5 (Lycosidae, Pisauridae)\n\nChapter 6 (Salticidae)\n\nChapter 7 (Thomisidae, Philodromidae, Sparassidae)\n\nChapter 8 (Cybaeidae)\n",
    "id": "28676576",
    "title": "Svenska Spindlar"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=23963",
    "text": "Phenetics\n\nIn biology, phenetics ( - to appear) , also known as taximetrics, is an attempt to classify organisms based on overall similarity, usually in morphology or other observable traits, regardless of their phylogeny or evolutionary relation. It is closely related to numerical taxonomy which is concerned with the use of numerical methods for taxonomic classification. Many people contributed to the development of phenetics, but the most influential were Peter Sneath and Robert R. Sokal. Their books are still primary references for this sub-discipline, although now out of print.\n\nPhenetics has largely been superseded by cladistics for research into evolutionary relationships among species. However, certain phenetic methods, such as neighbor-joining, have found their way into phylogenetics, as a reasonable approximation of phylogeny when more advanced methods (such as Bayesian inference) are too computationally expensive.\n\nPhenetic techniques include various forms of clustering and ordination. These are sophisticated ways of reducing the variation displayed by organisms to a manageable level. In practice this means measuring dozens of variables, and then presenting them as two- or three-dimensional graphs. Much of the technical challenge in phenetics revolves around balancing the loss of information in such a reduction against the ease of interpreting the resulting graphs.\n\nThe method can be traced back to 1763 and Michel Adanson (in his \"Familles des plantes\") because of two shared basic principles — overall similarity and equal weighting — and modern pheneticists are sometimes called neo-Adansonians.\n\nPhenetic analyses are unrooted, that is, they do not distinguish between plesiomorphies, traits that are inherited from an ancestor, and apomorphies, traits that evolved anew in one or several lineages. A common problem with phenetic analysis is that basal evolutionary grades, which retain many plesiomorphies compared to more advanced lineages, appear to be monophyletic. Phenetic analyses are also liable to be misled by convergent evolution and adaptive radiation. Cladistic methods have attempted to solve those problems.\n\nConsider for example songbirds. These can be divided into two groups - Corvida, which retains ancient characters in phenotype and genotype, and Passerida, which has more modern traits. But only the latter are a group of closest relatives; the former are numerous independent and ancient lineages which are about as distantly related to each other as each single one of them is to the Passerida. In a phenetic analysis, the large degree of overall similarity found among the Corvida will make them appear to be monophyletic too, but their shared traits were present in the ancestors of \"all\" songbirds already. It is the loss of these ancestral traits rather than their presence that signifies which songbirds are more closely related to each other than to other songbirds. However, the requirement that taxa be monophyletic - rather than paraphyletic as in the case of the Corvida - is itself part of the cladistic view of Taxonomy, not necessarily followed to an absolute degree by other schools.\n\nThe two methodologies are not mutually exclusive. There is no reason why, e.g., species identified using phenetics cannot subsequently be subjected to cladistic analysis, to determine their evolutionary relationships. Phenetic methods can also be superior to cladistics when only the \"distinctness\" of related taxa is important, as the computational requirements are lower.\n\nThe history of pheneticism and cladism as rival taxonomic systems is analysed in David Hull's 1988 book \"Science as a Process\".\n\nTraditionally there was a great deal of heated debate between pheneticists and cladists, as both methods were initially proposed to resolve evolutionary relationships. Perhaps the \"high-water mark\" of phenetics were the DNA-DNA hybridization studies by Charles G. Sibley, Jon E. Ahlquist and Burt L. Monroe Jr., from which resulted the 1990 Sibley-Ahlquist taxonomy for birds. Highly controversial at its time, some of its findings (e.g. the Galloanserae) have been vindicated, while others (e.g. the all-inclusive \"Ciconiiformes\" or the \"Corvida\") have been rejected. However, with computers growing increasingly powerful and widespread, more refined cladistic algorithms became available and could put the suggestions of Willi Hennig to the test; as it turned out, the results of cladistic analyses turned out to be superior to those of phenetic methods - at least when it came to resolving phylogenies.\n\nMany systematists continue to use phenetic methods, particularly in addressing species-level questions. While a major goal of taxonomy remains describing the 'tree of life' - the evolutionary path connecting all species - in fieldwork one needs to be able to separate one taxon from another. Classifying diverse groups of closely related organisms that differ very subtly is difficult using a cladistic approach. Phenetics provides numerical tools for examining overall patterns of variation, allowing researchers to identify discrete groups that can be classified as species.\n\nModern applications of phenetics are common in botany, and some examples can be found in most issues of the journal \"Systematic Botany\". Indeed, due to the effects of horizontal gene transfer, polyploid complexes and other peculiarities of plant genomics, phenetic techniques in botany - though less informative altogether - may, in these special cases, be less prone to errors compared with cladistic analysis of DNA sequences.\n\nIn addition, many of the techniques developed by phenetic taxonomists have been adopted and extended by community ecologists, due to a similar need to deal with large amounts of data.\n\n",
    "id": "23963",
    "title": "Phenetics"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=4635563",
    "text": "Quinarian system\n\nThe Quinarian system was a method of zoological classification which had a brief period of popularity in the mid 19th century, especially among British naturalists. It was largely developed by the entomologist William Sharp MacLeay in 1819. The system was further promoted in the works of Nicholas Aylward Vigors, William John Swainson and Johann Jakob Kaup. Swainson's work on ornithology gave wide publicity to the idea. The system had opponents even before the publication of Charles Darwin's \"On the Origin of Species\" (1859 ) which paved the way for evolutionary trees.\n\nQuinarianism gets its name from the emphasis on the number five: it proposed that all taxa are divisible into five subgroups, and if fewer than five subgroups were known, quinarians believed that a missing subgroup remained to be found.\n\nPresumably this arose as a chance observation of some accidental analogies between different groups, but it was erected into a guiding principle by the quinarians. It became increasingly elaborate, proposing that each group of five classes could be arranged in a circle, with those closer together having greater affinities. Typically they were depicted with relatively advanced groups at the top, and supposedly degenerate forms towards the bottom. Each circle could touch or overlap with adjacent circles; the equivalent overlapping of actual groups in nature was called osculation.\n\nAnother aspect of the system was the identification of \"analogies\" across groups:\nQuinarianism was not widely popular outside the United Kingdom (some followers like William Hincks persisted in Canada); it became unfashionable by the 1840s, during which time more complex \"maps\" were made by Hugh Edwin Strickland and Alfred Russel Wallace. Strickland and others specifically rejected the use of relations of \"analogy\" in constructing natural classifications. These systems were eventually discarded in favour of principles of genuinely natural classification, namely based on evolutionary relationship.\n",
    "id": "4635563",
    "title": "Quinarian system"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=3182598",
    "text": "Soft-bodied organism\n\nSoft-bodied organisms are animals that lack skeletons, a group roughly corresponding to the group Vermes as proposed by Carl von Linné. All animals have muscles but, since muscles can only pull, never push, a number of animals have developed hard parts that the muscles can pull on, commonly called skeletons. Such skeletons may be internal, as in vertebrates, or external, like in arthropods. However, a large number of animals groups do very well without hard parts. This include animals like earthworms, jellyfish, tapeworms, squids and an enormous variety of animals from almost every part of the kingdom Animalia.\n\nMost soft-bodied animals are small, but they do make up the majority of the animal biomass. If we were to weigh up all animals on Earth with hard parts against soft-bodied ones, estimates indicate that the biomass of soft-bodied animals would be at least twice that of animals with hard parts, quite possibly much larger. Particularly the roundworms are extremely numerous. The nematodologist Nathan Cobb described the ubiquitous presence of nematodes on Earth as follows:\n\n\"In short, if all the matter in the universe except the nematodes were swept away, our world would still be dimly recognizable, and if, as disembodied spirits, we could then investigate it, we should find its mountains, hills, vales, rivers, lakes, and oceans represented by a film of nematodes. The location of towns would be decipherable, since for every massing of human beings there would be a corresponding massing of certain nematodes. Trees would still stand in ghostly rows representing our streets and highways. The location of the various plants and animals would still be decipherable, and, had we sufficient knowledge, in many cases even their species could be determined by an examination of their erstwhile nematode parasites.\"\n\nNot being a true phylogenetic group, soft-bodied organism vary enormously in anatomy. Cnidarians and flatworms have a single opening to the gut and a diffuse nerve system. The roundworms, annelids, molluscs, the various lophoporate phyla and non-vertebrate chordates have a tubular gut open at both ends. While the majority of the soft-bodied animals typically don't have any kind of skeleton, some do, mainly in the form of stiff cuticulas (roundworms, water bears) or hydrostatic skeletons (annelids).\n\nWhile lack of a skeleton typically restricts the body size of soft-bodied animals on land, marine representatives can grow to very large sizes. The heaviest soft-bodied organisms are likely the giant squids, with maximum weight estimated at for females, while arctic lion's mane jellyfish mat reach comparable sizes. The longest animal on record is also thought to be a soft-bodied organism, a long thread-like bootlace worm, \"Lineus longissimus\" found on a Scottish beach 1864. Siphonophores may grow to considerable sizes too, though they are colonial organisms, and each single animal is small. Most soft-bodied organisms are as small or smaller, even microscopic. The various organisms grouped as mesozoans and the curious Placozoa are typically composed of just a few hundred cells.\n\nThe lack of hard parts in Soft-bodied organisms makes them extremely rare in the fossil record. Accordingly, the evolutionary history of many of the soft-bodied groups are poorly known. The first major find of fossil soft-bodied animals was from the Burgess Shale in Canada. Today, several sites with Burgess Shale type preservation are known, but the history of many groups of soft-bodied animals is still poorly understood.\n",
    "id": "3182598",
    "title": "Soft-bodied organism"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=227053",
    "text": "Superorganism\n\nA superorganism or supraorganism (the latter is less frequently used but more etymologically correct) is a group of interacting organisms of the same species. A community of synergetically interacting organisms of different species is called a holobiont.\n\nThe term superorganism is used most often to describe a social unit of eusocial animals, where division of labour is highly specialised and where individuals are not able to survive by themselves for extended periods. Ants are the best-known example of such a superorganism. A superorganism can be defined as \"a collection of agents which can act in concert to produce phenomena governed by the collective\", phenomena being any activity \"the hive wants\" such as ants collecting food and avoiding predators, or bees choosing a new nest site. Superorganisms tend to exhibit homeostasis, power law scaling, persistent disequilibrium and emergent behaviours.\n\nThe term was coined in 1789 by James Hutton, the \"Father of Geology\", to refer to Earth in the context of geophysiology. The Gaia hypothesis of James Lovelock, and Lynn Margulis as well as the work of Hutton, Vladimir Vernadsky and Guy Murchie, have suggested that the biosphere itself can be considered a superorganism, although this has been disputed. This view relates to systems theory and the dynamics of a complex system.\n\nThe concept of a superorganism raises the question of what is to be considered an individual. Toby Tyrrell's critique of the Gaia hypothesis argues that Earth's climate system does not resemble an animal's physiological system. Planetary biospheres are not tightly regulated in the same way that animal bodies are: \"planets, unlike animals, are not products of evolution. Therefore we are entitled to be highly skeptical (or even outright dismissive) about whether to expect something akin to a \"superorganism\"\". He concludes that \"the superorganism analogy is unwarranted\". However, as Gaia is another systemic level of integration in Nature, precisely has properties that cannot be inferred from their components.\n\nSome scientists have suggested that individual human beings can be thought of as \"superorganisms\"; as a typical human digestive system contains 10 to 10 microorganisms whose collective genome, the microbiome studied by the Human Microbiome Project, contains at least 100 times as many genes as the human genome itself. Salvucci wrote that superorganism is another level of integration that it is observed in nature. These levels include the genomic, the organismal and the ecological levels. The genomic structure of organism reveals the fundamental role of integration and gene shuffling along evolution.\n\nThe nineteenth century thinker Herbert Spencer coined the term \"super-organic\" to focus on social organization (the first chapter of his \"Principles of Sociology\" is entitled \"Super-organic Evolution\"), though this was apparently a distinction between the organic and the social, \"not\" an identity: Spencer explored the holistic nature of society as a social organism while distinguishing the ways in which society did not behave like an organism. For Spencer, the super-organic was an emergent property of interacting organisms, that is, human beings. And, as has been argued by D. C. Phillips, there is a \"difference between emergence and reductionism\".\n\nThe economist Carl Menger expanded upon the evolutionary nature of much social growth, but without ever abandoning methodological individualism. Many social institutions arose, Menger argued, not as \"the result of socially teleological causes, but the unintended result of innumerable efforts of economic subjects pursuing 'individual' interests\".\n\nSpencer and Menger both argued that because it is individuals who choose and act, any social whole should be considered less than an organism, though Menger emphasized this more emphatically. Spencer used the organistic idea to engage in extended analysis of social structure, conceding that it was primarily an analogy. So, for Spencer, the idea of the super-organic best designated a distinct level of social reality above that of biology and psychology, and not a one-to-one identity with an organism. Nevertheless, Spencer maintained that \"every organism of appreciable size is a society\", which has suggested to some that the issue may be terminological.\n\nThe term \"superorganic\" was adopted by the anthropologist Alfred L. Kroeber in 1917. Social aspects of the superorganism concept are analysed in Marshall (2002). Finally, recent work in social psychology has offered the superorganism metaphor as a unifying framework to understand diverse aspects of human sociality, such as religion, conformity, and social identity processes.\n\nSuperorganisms are important in cybernetics, particularly biocybernetics. They exhibit a form of \"distributed intelligence\", a system in which many individual agents with limited intelligence and information are able to pool resources to accomplish a goal beyond the capabilities of the individuals. Existence of such behavior in organisms has many implications for military and management applications, and is being actively researched.\n\n\n\n",
    "id": "227053",
    "title": "Superorganism"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=31032602",
    "text": "Taxon in disguise\n\nIn bacteriology, a taxon in disguise is a species, genus or higher unit of biological classification whose evolutionary history reveals has evolved from another unit of similar or lower rank, making the parent unit paraphyletic. This happens when rapid evolution makes a new species appear radically different from the ancestral group, so that it is not (initially) recognised as belonging to the parent phylogenetic group, leaving the latter an evolutionary grade.\n\nWhile the term is from bacteriology, parallel examples are found throughout the tree of life. E.g. four-footed animals have evolved from piscine ancestors, yet are not generally considered fish. The four footed animals can thus be said to be \"fish in disguise\". In many cases, the paraphyly can be resolved by re-classifying the taxon in question under the parent group, but in bacteriology renaming groups may have serious consequences as it may cause confusion over the identity of pathogens, and is generally avoided for some groups.\n\nThe bacterial genus \"Shigella\" is the cause of bacillary dysentery, a potentially severe infection that claim the lives of over a million people annually. The genus (\"S. dysenteriae\", \"S. flexneri\", \"S. boydii\", \"S. sonnei\") have evolved from the common intestinal bacterium \"Escherichia coli\", rendering that species parephyletic. \"E. coli\" itself can also cause serious dysentery, but the difference in the genetic makeup between the \"E. coli\" and \"Shigella\" causes different medical conditions and symptoms.\n\n\"Escherichia coli\" is a badly classified species, since some strains share only 20% of their genome. Being so diverse it should be given a higher taxonomic ranking. However, due to the medical conditions associated both with \"E. coli\" itself and with \"Shigella\" the current classification will not be changed, to avoid confusion in medical context. \"Shigella\" will thus remain \"\"E. coli\" in disguise\".\n\nIn a similar way, the \"Bacillus\" species of the \"B. cereus\"-group (\"B. anthracis\", \"B. cereus\", \"B . thuringiensis\", \"B. mycoides\", \"B. pseudomycoides\", \"B. weihenstephanensis\" and \"B. medusa\") have 99-100% similar 16S rRNA sequence (97% is a commonly cited adequate species cut-off) and should be considered a single species. Some of the members of the group appear to have arisen from other \"Bacillus\" strains by acquisition of a protein coding plasmid and the group may thus be polyphyletic. For medical reasons (anthrax \"etc.\"), the current arrangement of separate species remain.\n\n",
    "id": "31032602",
    "title": "Taxon in disguise"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=199556",
    "text": "Taxon\n\nIn biology, a taxon (plural taxa; back-formation from \"taxonomy\") is a group of one or more populations of an organism or organisms seen by taxonomists to form a unit. Although neither is required, a taxon is usually known by a particular name and given a particular ranking, especially if and when it is accepted or becomes established. It is not uncommon, however, for taxonomists to remain at odds over what belongs to a taxon and the criteria used for inclusion. If a taxon is given a formal scientific name, its use is then governed by one of the nomenclature codes specifying which scientific name is correct for a particular grouping.\n\nAlthough preceded by Linnaeus's system in \"Systema Naturae\" (10th edition, 1758) and unpublished work by Bernard and Antoine Laurent de Jussieu, the notion of a unit-based \"natural system\" of biological classification was first made widely available in 1805 through the publication, as the introduction to the third edition of Jean-Baptiste Lamarck's \"Flore françoise\", of Augustin Pyramus de Candolle's \"Principes élémentaires de botanique\", an exposition of a system for the \"natural classification\" of plants. Since then, systematists have striven to construct an accurate classification encompassing the diversity of life; today, a \"good\" or \"useful\" taxon is commonly taken to be one that reflects evolutionary relationships. \n\nMany modern systematists, such as advocates of phylogenetic nomenclature, use cladistic methods that require taxa to be monophyletic (all descendants of some ancestor). Their basic unit, therefore, is the clade rather than the taxon. Similarly, among those contemporary taxonomists working with the traditional Linnean (binomial) nomenclature, few propose taxa they know to be paraphyletic. An example of a well-established taxon that is not also a clade is the class Reptilia, the reptiles; birds are descendants of reptiles but are not included in the Reptilia.\n\nThe term \"taxon\" was first used in 1926 by for animal groups. For plants, it was proposed by Herman Johannes Lam in 1948, and it was adopted at the VII International Botanical Congress, held in 1950.\n\nThe Glossary of the \"International Code of Zoological Nomenclature\" (1999) defines a \n\nA taxon can be assigned a taxonomic rank, usually (but not necessarily) when it is given a formal name.\n\n\"Phylum\" applies formally to any biological domain, but traditionally it was always used for animals, whereas \"Division\" was traditionally often used for plants, fungi, etc.\n\nA prefix is used to indicate a ranking of lesser importance. The prefix \"super-\" indicates a rank above, the prefix \"sub-\" indicates a rank below. In zoology the prefix \"infra-\" indicates a rank below \"sub-\". For instance, among the additional ranks of class are superclass, subclass and infraclass.\n\nRank is relative, and restricted to a particular systematic schema. For example, liverworts have been grouped, in various systems of classification, as a family, order, class, or division (phylum). The use of a narrow set of ranks is challenged by users of cladistics; for example, the mere 10 ranks traditionally used between animal families (governed by the ICZN) and animal phyla (usually the highest relevant rank in taxonomic work) often cannot adequately represent the evolutionary history as more about a lineage's phylogeny becomes known. In addition, the class rank is quite often not an evolutionary but a phenetic or paraphyletic group and as opposed to those ranks governed by the ICZN (family-level, genus-level and species-level taxa), can usually not be made monophyletic by exchanging the taxa contained therein. This has given rise to phylogenetic taxonomy and the ongoing development of the \"PhyloCode\", which has been proposed as a new alternative to replace Linnean classification and govern the application of names to clades. Many cladists do not see any need to depart from traditional nomenclature as governed by the ICZN, ICN, etc.\n\n",
    "id": "199556",
    "title": "Taxon"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=9365268",
    "text": "Web-based taxonomy\n\nWeb-based taxonomy is the effort by taxonomists to use the World Wide Web in order to create unified, consensus taxonomies of life on Earth.\n\nIn his 2002 paper on the subject, H. Charles J. Godfray called for the creation of Web-based organisations to collect all the accumulated literature on a taxonomic group into a centralized knowledge base and make this data available through the Web as a unified taxonomy, so that it can be more easily examined and revised. Such a platform would be owned and maintained by a taxonomic working group, governed by an editor or an editorial board. An example of such a platform is FishBase.\n\nThe notion of Web-based consensus taxonomies remains controversial because, as two Australian researchers pointed out, taxonomic names are not fixed but hypotheses, and therefore in constant change.\n\n\n",
    "id": "9365268",
    "title": "Web-based taxonomy"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=5962524",
    "text": "Trace fossil classification\n\nTrace fossils are classified in various ways for different purposes. Traces can be classified taxonomically (by morphology), ethologically (by behavior), and toponomically, that is, according to their relationship to the surrounding sedimentary layers. Except in the rare cases where the original maker of a trace fossil can be identified with confidence, phylogenetic classification of trace fossils is an unreasonable proposition.\n\nThe taxonomic classification of trace fossils parallels the taxonomic classification of organisms under the International Code of Zoological Nomenclature. In trace fossil nomenclature a Latin binomial name is used, just as in animal and plant taxonomy, with a genus and specific epithet. However, the binomial names are not linked to an organism, but rather just a trace fossil. This is due to the rarity of association between a trace fossil and a specific organism or group of organisms. Trace fossils are therefore included in an \"ichnotaxon\" separate from Linnaean taxonomy. When referring to trace fossils, the terms \"ichnogenus\" and \"ichnospecies\" parallel genus and species respectively.\n\nThe most promising cases of phylogenetic classification are those in which similar trace fossils show details complex enough to deduce the makers, such as bryozoan borings, large trilobite trace fossils such as \"Cruziana\", and vertebrate footprints. However, most trace fossils lack sufficiently complex details to allow such classification.\n\nAdolf Seilacher was the first to propose a broadly accepted ethological basis for trace fossil classification. He recognized that most trace fossils are created by animals in one of five main behavioural activities, and named them accordingly:\n\n\nSince the inception of behavioural categorization, several other ethological classes have been suggested and accepted, as follows:\n\n\nOver the years several other behavioural groups have been proposed, but in general they have been quickly discarded by the ichnological community. Some of the failed proposals are listed below, with a brief description.\n\n\nFixichnia is perhaps the group with the most weight as a candidate for the next accepted ethological class, being not fully described by any of the eleven currently accepted categories. There is also potential for the three plant traces (cecidoichnia, corrosichnia and sphenoichnia) to gain recognition in coming years, with little attention having been paid to them since their proposal.\n\nAnother way to classify trace fossils is to look at their relation to the sediment of origin. Martinsson has provided the most widely accepted of such systems, identifying four distinct classes for traces to be separated in this regard:\n\n\nOther classifications have been proposed, but none stray far from the above.\n\nEarly paleontologists originally classified many burrow fossils as the remains of marine algae, as is apparent in ichnogenera named with the \"-phycus\" suffix. Alfred Gabriel Nathorst and Joseph F. James both controversially challenged this incorrect classification, suggesting the reinterpretation of many \"algae\" as marine invertebrate trace fossils.\n\nSeveral attempts to classify trace fossils have been made throughout the history of paleontology. In 1844, Edward Hitchcock proposed two orders: \"Apodichnites\", including footless trails, and \"Polypodichnites\", including trails of organisms with more than four feet.\n\n\n",
    "id": "5962524",
    "title": "Trace fossil classification"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=44432370",
    "text": "Woeseian revolution\n\nThe Woeseian revolution was the progression of the phylogenetic tree of life from two main divisions, known as the Prokarya and Eukarya, into three domains now classified as Bacteria, Archaea, and Eukaryotes. The discovery of the new domain stemmed from the work of biophysicist Carl Woese in 1997 from a principle of evolutionary biology designated as Woese's dogma. It states that the evolution of ribosomal RNA (rRNA) was a necessary precursor to the evolution of modern life forms. Although the three-domain system has been widely accepted, the initial introduction of Woese’s discovery received criticism from the scientific community.\n\nThe basis of phylogenetics was limited by the technology of the time, which led to a greater dependence on phenotypic classification prior to advances that would allow for molecular methods of organization. This was a major reason why the dichotomy of all living things, being either animal or plant in nature, was deemed as an acceptable theory. Without truly understanding the genetic implication of each organismal classification in phylogenies via nucleic acid sequencing of shared molecular material, the phylogenetic tree of life and other such phylogenies would no doubt be incorrect. Woese’s advances in molecular sequencing and phylogenetic organization allowed for a better understanding of the three domains of life - the Bacteria, Archaea, and Eukaryotes. In regards to their varying types of shared rRNA, that of the small subunit rRNA was deemed as the best molecules to sequence in order to distinguish phylogenetic relationships because of its relatively small size, ease of isolation, and universal distribution.\n\nThis reorganization caused for an initial push back, being accepted nearly a decade after its publication. Possible factors that led to the initial criticism of the discovery include Woese’s oligonucleotide cataloging of which he was one of “only two or three people in the world” to be able to execute this method let alone read the films. Also, Woese’s background was in physics whereas most of the research was being done in the realm of microbiologists.\n",
    "id": "44432370",
    "title": "Woeseian revolution"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=36587327",
    "text": "Klepton\n\nIn biology, a klepton (abbr. kl.) and synklepton (abbr sk.) is a species that requires input from another biological taxon (normally from a species which is closely related to the kleptonic species) to complete their reproductive cycle. Specific types of kleptons are \"zygokleptons\", which reproduce by zygogenesis; \"gynokleptons\" which reproduce by gynogenesis, and \"tychokleptons\", which reproduce by a combination of both systems. The term is derived from the Greek, \"kleptein\", \"to steal\".\n\nKleptogenic reproduction results in three potential outcomes. A unisexual female may simply activate cell division in the egg through the presence of a male's sperm without incorporating any of his genetic material—this results in the production of clonal offspring. The female may also incorporate the male's sperm into her egg, but can do so without excising any of her genetic material. This results in increased ploidy levels that range from triploid to pentaploid in wild individuals. Finally, the female also has the option of replacing some of her genetic material with that of the male's, resulting in a \"hybrid\" of sorts without increasing ploidy.\n\nIn the wild, five species of \"Ambystoma\" salamanders contribute to a unisexual complex that reproduces via a combination of gynogenesis and kleptogenesis: \"A. tigrinum\", \"A. barbouri\", \"A. texanum\", \"A. jeffersonium\", and \"A. laterale\". Over twenty genomic combinations have been found in nature, ranging from \"LLJ\" individuals (two \"A. laterale\" and an \"A. jeffersonium\" genome) to \"LJTi\" individuals (an \"A. laterale\", \"A. jeffersonium\", and an \"A. tigrinum\" genome). Every combination, however, contains the genetic information from the \"A. laterale\" species, and analysis of mitochondrial DNA has indicated that these unisexual species most likely diverged from an \"A. barbouri\" individual some 5 million years ago, making them the oldest unisexual vertebrate species on Earth \nThe fact that these salamanders have persisted for so long is remarkable, as it contradicts the notion that a majority of asexual lineages arise when the conditions are right and quickly disappear. It has been argued that this persistence is very much due to the aforementioned \"genome replacement\" strategy that accompanies kleptogenic reproduction—replacing a portion of the maternal genome with paternal DNA in offspring has allowed unisexual individuals to \"refresh\" their genetic material through time. This facet of kleptogenesis was recently ascertained from genetic research that indicates there is no ancestral \"A. laterale\" genome that is maintained from one unisexual to the next, and that there is not a specific \"L\" genome that is found more often than others. \"L\" genetic material found in these salamanders has also not evolved to be substantially unique from sexual genomes.\n\nOther studied species exhibiting the property include the water frogs \"Pelophylax\".\n\n",
    "id": "36587327",
    "title": "Klepton"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=94320",
    "text": "Virus classification\n\nVirus classification is the process of naming viruses and placing them into a taxonomic system. Similar to the classification systems used for cellular organisms, virus classification is the subject of ongoing debate and proposals. This is mainly due to the pseudo-living nature of viruses, which is to say they are non-living particles with some chemical characteristics similar to those of life, or non-cellular life. As such, they do not fit neatly into the established biological classification system in place for cellular organisms.\n\nViruses are mainly classified by phenotypic characteristics, such as morphology, nucleic acid type, mode of replication, host organisms, and the type of disease they cause. Currently, two main schemes are used for the classification of viruses: the International Committee on Taxonomy of Viruses (ICTV) system and Baltimore classification system, which places viruses into one of seven groups. Accompanying this broad method of classification are specific naming conventions and further classification guidelines set out by the ICTV.\n\nA catalogue of all the world's viruses has been proposed; some related preliminary efforts have been accomplished.\n\nSpecies form the basis for any biological classification system. The ICTV had adopted the principle that a virus species is a polythetic class of viruses that constitutes a replicating lineage and occupies a particular ecological niche. In July 2013, the ICTV definition of species changed to state: \"A species is a monophyletic group of viruses whose properties can be distinguished from those of other species by multiple criteria.\"\n\nThe International Committee on Taxonomy of Viruses began to devise and implement rules for the naming and classification of viruses early in the 1970s, an effort that continues to the present. The ICTV is the only body charged by the International Union of Microbiological Societies with the task of developing, refining, and maintaining a universal virus taxonomy. \n\nThe system shares many features with the classification system of cellular organisms, such as taxon structure. However, this system of nomenclature differs from other taxonomic codes on several points. A minor point is that names of orders and families are italicized, unlike in the International Code of Nomenclature for algae, fungi, and plants and International Code of Zoological Nomenclature.\n\nViral classification starts at the level of order and continues as follows, with the taxon suffixes given in italics:\n\nSpecies names often take the form of \"[Disease] virus\", particularly for higher plants and animals.\n\nThe establishment of an order is based on the inference that the virus families it contains have most likely evolved from a common ancestor. The majority of virus families remain unplaced. As of 2012, seven orders, 96 families, 22 subfamilies, 420 genera, and 2,618 species of viruses have been defined by the ICTV. The orders are the \"Caudovirales, Herpesvirales, Ligamenvirales, Mononegavirales, Nidovirales, Picornavirales, Bunyavirales\" and \"Tymovirales\". These orders span viruses with varying host ranges. The \"Ligamenvirales\" (Group I) , infecting archaea, and \"Bunyavirales\" (Group V) are the most recent additions to the classification system.\n\n\"Caudovirales\" are tailed dsDNA (group I) bacteriophages.\n\n\"Herpesvirales\" contain large eukaryotic dsDNA viruses.\n\n\"Ligamenvirales\" contains linear, dsDNA (group I) archaean viruses.\n\n\"Mononegavirales\" include nonsegmented (-) strand ssRNA (Group V) plant and animal viruses.\n\n\"Nidovirales\" are composed of (+) strand ssRNA (Group IV) viruses with vertebrate hosts.\n\n\"Picornavirales\" contains small (+) strand ssRNA viruses that infect a variety of plant, insect and animal hosts.\n\n\"Tymovirales\" contain monopartite (+) ssRNA viruses that infect plants.\n\n\"Bunyavirales\" contain tripartite (-) ssRNA viruses (Group V).\n\nOther variations occur between the orders: \"Nidovirales\", for example, are isolated for their differentiation in expressing structural and nonstructural proteins separately.\n\nIt has been suggested that similarity in virion assembly and structure observed for certain viral groups infecting hosts from different domains of life (e.g., bacterial tectiviruses and eukaryotic adenoviruses or prokaryotic Caudovirales and eukaryotic herpesviruses) reflects an evolutionary relationship between these viruses. Therefore, structural relationship between viruses has been suggested to be used as a basis for defining higher-level taxa - structure-based viral lineages - that could complement the existing ICTV classification scheme.\nBaltimore classification (first defined in 1971) is a classification system that places viruses into one of seven groups depending on a combination of their nucleic acid (DNA or RNA), strandedness (single-stranded or double-stranded), Sense, and method of replication. Named after David Baltimore, a Nobel Prize-winning biologist, these groups are designated by Roman numerals. Other classifications are determined by the disease caused by the virus or its morphology, neither of which are satisfactory due to different viruses either causing the same disease or looking very similar. In addition, viral structures are often difficult to determine under the microscope. Classifying viruses according to their genome means that those in a given category will all behave in a similar fashion, offering some indication of how to proceed with further research. Viruses can be placed in one of the seven following groups:\n\n\n\n\nHolmes (1948) used Carl Linnaeus's system of binomial nomenclature to classify viruses into 3 groups under one order, Virales. They are placed as follows:\n\n\nThe LHT System of Virus Classification is based on chemical and physical characters like nucleic acid (DNA or RNA), Symmetry (Helical or Icosahedral or Complex), presence of envelope, diameter of capsid, number of capsomers. This classification was approved by the Provisional Committee on Nomenclature of Virus (PNVC) of the International Association of Microbiological Societies (1962). It is as follows:\n\nThe following agents are smaller than viruses but have only some of their properties.\n\n\nSatellites depend on co-infection of a host cell with a helper virus for productive multiplication. Their nucleic acids have substantially distinct nucleotide sequences from either their helper virus or host. When a satellite subviral agent encodes the coat protein in which it is encapsulated, it is then called a satellite virus.\n\nPrions, named for their description as \"\"pr\"oteinaceous and \"in\"fectious particles\", lack any detectable (as of 2002) nucleic acids or virus-like particles. They resist inactivation procedures that normally affect nucleic acids.\n\n\n",
    "id": "94320",
    "title": "Virus classification"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=48664569",
    "text": "Cormus\n\nCormus (\"cormi\") – from ancient Greek, κορμός - \"kormόs\" = trunk of a tree with the boughs cut – is an organism made up of a number of individuals, such as, for example, would be formed by a process of budding from a parent stalk where the buds remain attached.\n\n",
    "id": "48664569",
    "title": "Cormus"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=10761675",
    "text": "Form classification\n\nForm classification is the classification of organisms based on their morphology, which does not necessarily reflect their biological relationships. Form classification, generally restricted to palaeontology, reflects uncertainty; the goal of science is to move \"form taxa\" to biological taxa whose affinity is known.\n\nForm taxonomy is restricted to fossils that preserve too few characters for a conclusive taxonomic definition or assessment of their biological affinity, but whose study is made easier if a binomial name is available by which to identify them. The term \"form classification\" is preferred to \"form taxonomy\"; taxonomy suggests that the classification implies a biological affinity, whereas form classification is about giving a name to a group of morphologically-similar organisms that may not be related.\n\nForm taxa are groupings that are based on common overall forms. Early attempts at classification of labyrinthodonts was based on skull shape (the heavily armoured skulls often being the only preserved part). The amount of convergent evolution in the many groups lead to a number of polyphyletic taxa. Such groups are united by a common mode of life, often one that is generalist, in consequence acquiring generally similar body shapes by convergent evolution. Ediacaran biota — whether they are the precursors of the Cambrian explosion of the fossil record, or are unrelated to any modern phylum — can currently only be grouped in \"form taxa\". Other examples include the seabirds and the \"Graculavidae\". The latter were initially described as the earliest family of Neornithes but are nowadays recognized to unite a number of unrelated early neornithine lineages, several of which probably later gave rise to the \"seabird\" form taxon of today.\n\nA \"parataxon\" (not to be confused with parataxonomy), or \"sciotaxon\" (Gr. \"shadow taxon\"), is a classification based on incomplete data: for instance, the larval stage of an organism that cannot be matched up with an adult. It reflects a paucity of data that makes biological classification impossible. A sciotaxon is defined as a taxon thought to be equivalent to a true taxon (orthotaxon), but whose identity cannot be established because the two candidate taxa are preserved in different ways and thus cannot be compared directly.\n\nIn paleobotany, two terms were formerly used in the codes of nomenclature, \"form genera\" and \"organ genera\", to mean groups of fossils of a particular part of a plant, such as a leaf or seed, whose parent plant is not known because the fossils were preserved unattached to the parent plant. A later term \"morphotaxa\" also allows for differences in preservational state. These three terms have been replaced as of 2011 by provisions for \"fossil-taxa\" that are more similar to the provisions for other types of plants.\n\nNames given to organ genera could only be applied to the organs in question, and could not be extended to the entire organism. Fossil-taxon names can cover several parts of an organism, or several preservational states, but do not compete for priority with any names for the same organism that are based on a non-fossil type.\n\nThe part of the plant was often, but not universally, indicated by the use of a suffix in the generic name:\n\n\"Form taxon\" can more casually be used to describe a wastebasket taxon: either a taxon that is not a natural (monophyletic) group but united by shared plesiomorphies, or a presumably artificial group of organisms whose true relationships are not known, being obscured by ecomorphological similarity. Well-known form taxa of this kind include \"ducks\", \"fish\", \"reptiles\" and \"worms\".\n\n",
    "id": "10761675",
    "title": "Form classification"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=9019997",
    "text": "Indigenous (ecology)\n\nIn biogeography, a species is defined as indigenous to a given region or ecosystem if its presence in that region is the result of only natural process, with no human intervention. The term is equivalent to \"native\" in less scientific usage. Every wild organism (as opposed to a domesticated organism) has its own natural range of distribution in which it is regarded as indigenous. Outside this native range, a species may be introduced by human activity; it is then referred to as an \"introduced species\" within the regions where it was anthropogenically introduced.\n\nAn indigenous species is not necessarily endemic. In biology and ecology, endemic means \"exclusively\" native to the biota of a specific place. An indigenous species may occur in areas other than the one under consideration.\n\nThe terms \"endemic\" and \"indigenous\" do not imply that an organism necessarily originated or evolved from where it is found.\n\n\n",
    "id": "9019997",
    "title": "Indigenous (ecology)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=4585070",
    "text": "Non-cellular life\n\nNon-cellular life is life that exists without a cellular structure for at least part of its life cycle. Historically, most (descriptive) definitions of life postulated that a living organism must be composed of one or more cells, but this is no longer considered necessary, and modern criteria allow for forms of life based on other structural arrangements.\n\nThe primary candidates for non-cellular life are viruses. A minority of biologists consider viruses to be living organisms, but most do not. Their primary objection is that no known viruses are capable of autopoiesis, which means they cannot reproduce themselves: they must rely on cells to copy them. However, the recent discovery of giant viruses that possess genes for part of the required translation machinery has raised the prospect that they may have had extinct ancestors that could evolve and replicate independently. Most biologists agree that such an ancestor would be a \"bona fide\" non-cellular lifeform, but its existence and characteristics are still uncertain.\n\nEngineers sometimes use the term \"artificial life\" to refer to software and robots inspired by biological processes, but these do not satisfy any biological definition of life.\n\nThe nature of viruses was unclear for many years following their discovery as pathogens. They were described as poisons or toxins at first, then as \"infectious proteins\", but with advances in microbiology it became clear that they also possessed genetic material, a defined structure, and the ability to spontaneously assemble from their constituent parts. This spurred extensive debate as to whether they should be regarded as fundamentally organic or inorganic — as very small biological organisms or very large biochemical molecules — and since the 1950s many scientists have thought of viruses as existing at the border between chemistry and life; a gray area between living and nonliving.\n\nThe recent discovery of giant viruses (aka giruses, nucleocytoplasmic large DNA viruses, NCLDVs) has reignited this debate, since they are not only physically larger than previously known viruses, but also possess much more extensive genomes, including genes coding for aminoacyl tRNA synthetases, key proteins involved in translation, which were previously thought to be exclusive to cellular organisms. This raises the prospect that the giant viruses may have had extinct ancestors (or even undiscovered ones) capable of engaging in life processes (such as evolution and replication) independent of cells, and that modern viruses lost those abilities secondarily. The viral lineage including this ancestor would be ancient and may have originated alongside the earliest archaea or before the LUCA. If such a virus is discovered, or its past existence supported by further genetic evidence, most biologists agree that it would constitute a \"bona fide\" lifeform, and its descendants (at least the giant viruses, and possibly including all known viruses) could be phylogenetically classified in a fourth domain of life. Discovery of the pandoraviruses, with genomes are even larger than the other giant viruses and exhibiting particularly low homology with the three existing domains, further discredited the traditional view that viruses simply \"pick-pocketed\" all of their genes from cellular organisms, and further supported the \"complex ancestors\" hypothesis.\n\nOngoing research is being conducted in this area, using techniques such as phylogenetic bracketing on the giant viruses to infer characteristics of their proposed progenitor.\n\nFurthermore, Viral replication and self-assembly has implications for the study of the origin of life, as it lends further credence to the hypothesis that life could have started as self-assembling organic molecules.\n\nViroids are the smallest infectious pathogens known, consisting solely of short strands of circular, single-stranded RNA without protein coats. They are mostly plant pathogens, some of which are of commercial importance. Viroid genomes are extremely small in size, ranging from 246 to 467 nucleobases. In comparison, the genome of the smallest known viruses capable of causing an infection by themselves are around 2,000 nucleobases in size. Viroids are the first known representatives of a new biological realm of sub-viral pathogens.\n\nViroid RNA does not code for any protein. Its replication mechanism hijacks RNA polymerase II, a host cell enzyme normally associated with synthesis of messenger RNA from DNA, which instead catalyzes \"rolling circle\" synthesis of new RNA using the viroid's RNA as a template. Some viroids are ribozymes, having catalytic properties which allow self-cleavage and ligation of unit-size genomes from larger replication intermediates.\n\nViroids attained significance beyond plant virology since one possible explanation of their origin is that they represent “living relics” from a hypothetical, ancient, and non-cellular RNA world before the evolution of DNA or protein. This view was first proposed in the 1980s, and regained popularity in the 2010s to explain crucial intermediate steps in the evolution of life from inanimate matter (Abiogenesis).\n\nIn discussing the taxonomic domains of life, the terms \"Acytota\" or \"Aphanobionta\" are occasionally used as the name of a viral kingdom, domain, or empire. The corresponding cellular life name would be Cytota. Non-cellular organisms and cellular life would be the two top-level subdivisions of life, whereby life as a whole would be known as organisms, Naturae, or Vitae. The taxon Cytota would include three top-level subdivisions of its own, the domains Bacteria, Archaea, and Eukarya.\n\n",
    "id": "4585070",
    "title": "Non-cellular life"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=30463",
    "text": "Taxonomy (biology)\n\nTaxonomy () is the science of defining and naming groups of biological organisms on the basis of shared characteristics. Organisms are grouped together into taxa (singular: taxon) and these groups are given a taxonomic rank; groups of a given rank can be aggregated to form a super-group of higher rank, thus creating a taxonomic hierarchy. The principal ranks in modern use are domain, kingdom, phylum (division is sometimes used in botany in place of phylum), class, order, family, genus and species. The Swedish botanist Carl Linnaeus is regarded as the father of taxonomy, as he developed a system known as Linnaean taxonomy for categorization of organisms and binomial nomenclature for naming organisms.\n\nWith the advent of such fields of study as phylogenetics, cladistics, and systematics, the Linnaean system has progressed to a system of modern biological classification based on the evolutionary relationships between organisms, both living and extinct.\n\nThe exact definition of taxonomy varies from source to source, but the core of the discipline remains: the conception, naming, and classification of groups of organisms. As points of reference, recent definitions of taxonomy are presented below:\n\n\nThe varied definitions either place taxonomy as a sub-area of systematics (definition 2), invert that relationship (definition 6), or appear to consider the two terms synonymous. There is some disagreement as to whether biological nomenclature is considered a part of taxonomy (definitions 1 and 2), or a part of systematics outside taxonomy. For example, definition 6 is paired with the following definition of systematics that places nomenclature outside taxonomy:\n\n\nA whole set of terms including taxonomy, systematic biology, systematics, biosystematics, scientific classification, biological classification, and phylogenetics have at times had overlapping meanings – sometimes the same, sometimes slightly different, but always related and intersecting. The broadest meaning of \"taxonomy\" is used here. The term itself was introduced in 1813 by de Candolle, in his \"Théorie élémentaire de la botanique\".\n\nThe term \"alpha taxonomy\" is primarily used today to refer to the discipline of finding, describing, and naming taxa, particularly species. In earlier literature, the term had a different meaning, referring to morphological taxonomy, and the products of research through the end of the 19th century.\n\nWilliam Bertram Turrill introduced the term \"alpha taxonomy\" in a series of papers published in 1935 and 1937 in which he discussed the philosophy and possible future directions of the discipline of taxonomy. … there is an increasing desire amongst taxonomists to consider their problems from wider viewpoints, to investigate the possibilities of closer co-operation with their cytological, ecological and genetical colleagues and to acknowledge that some revision or expansion, perhaps of a drastic nature, of their aims and methods, may be desirable … Turrill (1935) has suggested that while accepting the older invaluable taxonomy, based on structure, and conveniently designated \"alpha\", it is possible to glimpse a far-distant taxonomy built upon as wide a basis of morphological and physiological facts as possible, and one in which \"place is found for all observational and experimental data relating, even if indirectly, to the constitution, subdivision, origin, and behaviour of species and other taxonomic groups\". Ideals can, it may be said, never be completely realized. They have, however, a great value of acting as permanent stimulants, and if we have some, even vague, ideal of an \"omega\" taxonomy we may progress a little way down the Greek alphabet. Some of us please ourselves by thinking we are now groping in a \"beta\" taxonomy.\n\nTurrill thus explicitly excludes from alpha taxonomy various areas of study that he includes within taxonomy as a whole, such as ecology, physiology, genetics, and cytology. He further excludes phylogenetic reconstruction from alpha taxonomy (pp. 365–366).\n\nLater authors have used the term in a different sense, to mean the delimitation of species (not subspecies or taxa of other ranks), using whatever investigative techniques are available, and including sophisticated computational or laboratory techniques. Thus, Ernst Mayr in 1968 defined beta taxonomy as the classification of ranks higher than species.An understanding of the biological meaning of variation and of the evolutionary origin of groups of related species is even more important for the second stage of taxonomic activity, the sorting of species into groups of relatives (\"taxa\") and their arrangement in a hierarchy of higher categories. This activity is what the term classification denotes; it is also referred to as beta taxonomy.\n\nHow species should be defined in a particular group of organisms gives rise to practical and theoretical problems that are referred to as the species problem. The scientific work of deciding how to define species has been called microtaxonomy. By extension, macrotaxonomy is the study of groups at higher taxonomic ranks, from subgenus and above only, than species.\n\nWhile some descriptions of taxonomic history attempt to date taxonomy to ancient civilizations, a truly scientific attempt to classify organisms did not occur until the 18th century. Earlier works were primarily descriptive and focused on plants that were useful in agriculture or medicine. There are a number of stages in this scientific thinking. Early taxonomy was based on arbitrary criteria, the so-called \"artificial systems\", including Linnaeus's system of sexual classification. Later came systems based on a more complete consideration of the characteristics of taxa, referred to as \"natural systems\", such as those of de Jussieu (1789), de Candolle (1813) and Bentham and Hooker (1862–1863). These were pre-evolutionary in thinking. The publication of Charles Darwin's \"On the Origin of Species\" (1859) led to new ways of thinking about classification based on evolutionary relationships. This was the concept of phyletic systems, from 1883 onwards. This approach was typified by those of Eichler (1883) and Engler (1886–1892). The advent of molecular genetics and statistical methodology allowed the creation of the modern era of \"phylogenetic systems\" based on cladistics, rather than morphology alone.\n\nNaming and classifying our surroundings has probably been taking place as long as mankind has been able to communicate. It would always have been important to know the names of poisonous and edible plants and animals in order to communicate this information to other members of the family or group. Medicinal plant illustrations show up in Egyptian wall paintings from c. 1500 BC, indicating that the uses of different species were understood and that a basic taxonomy was in place.\n\nIn Canto 3, chapter 10 of \"Bhagavata Purana\" 6 types of trees are recognised, by name:\n\nOrganisms were first classified by Aristotle (Greece, 384–322 BC) during his stay on the Island of Lesbos. He classified beings by their parts, or in modern terms \"attributes\", such as having live birth, having four legs, laying eggs, having blood, or being warm-bodied. He divided all living things into two groups: plants and animals. Some of his groups of animals, such as \"Anhaima\" (animals without blood, translated as invertebrates) and \"Enhaima\" (animals with blood, roughly the vertebrates), as well as groups like the sharks and cetaceans, are still commonly used today. His student Theophrastus (Greece, 370–285 BC) carried on this tradition, mentioning some 500 plants and their uses in his \"Historia Plantarum\". Again, several plant groups currently still recognized can be traced back to Theophrastus, such as \"Cornus\", \"Crocus\", and \"Narcissus\".\n\nTaxonomy in the Middle Ages was largely based on the Aristotelian system, with additions concerning the philosophical and existential order of creatures. This included concepts such as the Great chain of being in the Western scholastic tradition, again deriving ultimately from Aristotle. Aristotelian system did not classify plants or fungi, due to the lack of microscope at the time, as his ideas were based on arranging the complete world in a single continuum, as per the \"scala naturae\" (the Natural Ladder). This, as well, was taken into consideration in the Great chain of being. Advances were made by scholars such as Procopius, Timotheos of Gaza, Demetrios Pepagomenos, and Thomas Aquinas. Medieval thinkers used abstract philosophical and logical categorizations more suited to abstract philosophy than to pragmatic taxonomy.\n\nDuring the Renaissance, the Age of Reason, and the Enlightenment, categorizing organisms became more prevalent,\nand taxonomic works became ambitious enough to replace the ancient texts. This is sometimes credited to the development of sophisticated optical lenses, which allowed the morphology of organisms to be studied in much greater detail. One of the earliest authors to take advantage of this leap in technology was the Italian physician Andrea Cesalpino (1519–1603), who has been called \"the first taxonomist\". His magnum opus \"De Plantis\" came out in 1583, and described more than 1500 plant species. Two large plant families that he first recognized are still in use today: the Asteraceae and Brassicaceae. Then in the 17th century John Ray (England, 1627–1705) wrote many important taxonomic works. Arguably his greatest accomplishment was \"Methodus Plantarum Nova\" (1682), in which he published details of over 18,000 plant species. At the time, his classifications were perhaps the most complex yet produced by any taxonomist, as he based his taxa on many combined characters. The next major taxonomic works were produced by Joseph Pitton de Tournefort (France, 1656–1708). His work from 1700, \"Institutiones Rei Herbariae\", included more than 9000 species in 698 genera, which directly influenced Linnaeus, as it was the text he used as a young student.\n\nThe Swedish botanist Carl Linnaeus (1707–1778) ushered in a new era of taxonomy. With his major works \"Systema Naturae\" 1st Edition in 1735, \"Species Plantarum\" in 1753, and \"Systema Naturae\" 10th Edition, he revolutionized modern taxonomy. His works implemented a standardized binomial naming system for animal and plant species, which proved to be an elegant solution to a chaotic and disorganized taxonomic literature. He not only introduced the standard of class, order, genus, and species, but also made it possible to identify plants and animals from his book, by using the smaller parts of the flower. Thus the Linnaean system was born, and is still used in essentially the same way today as it was in the 18th century. Currently, plant and animal taxonomists regard Linnaeus' work as the \"starting point\" for valid names (at 1753 and 1758 respectively). Names published before these dates are referred to as \"pre-Linnaean\", and not considered valid (with the exception of spiders published in \"Svenska Spindlar\"). Even taxonomic names published by Linnaeus himself before these dates are considered pre-Linnaean.\n\nWhereas Linnaeus classified for ease of identification, the idea of the Linnaean taxonomy as translating into a sort of dendrogram of the Animal- and Plant Kingdoms was formulated toward the end of the 18th century, well before \"On the Origin of Species\" was published. Among early works exploring the idea of a transmutation of species were Erasmus Darwin's 1796 \"Zoönomia\" and Jean-Baptiste Lamarck's \"Philosophie Zoologique\" of 1809. The idea was popularised in the Anglophone world by the speculative but widely read \"Vestiges of the Natural History of Creation\", published anonymously by Robert Chambers in 1844.\n\nWith Darwin's theory, a general acceptance quickly appeared that a classification should reflect the Darwinian principle of common descent. Tree of life representations became popular in scientific works, with known fossil groups incorporated. One of the first modern groups tied to fossil ancestors was birds. Using the then newly discovered fossils of \"Archaeopteryx\" and \"Hesperornis\", Thomas Henry Huxley pronounced that they had evolved from dinosaurs, a group formally named by Richard Owen in 1842. The resulting description, that of dinosaurs \"giving rise to\" or being \"the ancestors of\" birds, is the essential hallmark of evolutionary taxonomic thinking. As more and more fossil groups were found and recognized in the late 19th and early 20th centuries, palaeontologists worked to understand the history of animals through the ages by linking together known groups. \n\nThe cladistic method (or cladism) has emerged since the 1960s. In 1958, Julian Huxley used the term clade. Later, in 1960, Cain and Harrison introduced the term cladistic. The salient feature is arranging taxa in a hierarchical evolutionary tree, ignoring ranks. A taxon is called monophyletic, if it includes all the descendants of an ancestral form. Groups that have descendant groups removed from them (e.g. dinosaurs, with birds as offspring group) are termed paraphyletic, while groups representing more than one branch from the tree of life are called polyphyletic. The \"International Code of Phylogenetic Nomenclature\" or \"PhyloCode\" is intended to regulate the formal naming of clades. Linnaean ranks will be optional under the \"PhyloCode\", which is intended to coexist with the current, rank-based codes.\n\nWell before Linnaeus, plants and animals were considered separate Kingdoms. Linnaeus used this as the top rank, dividing the physical world into the plant, animal and mineral kingdoms. As advances in microscopy made classification of microorganisms possible, the number of kingdoms increased, five and six-kingdom systems being the most common.\n\nDomains are a relatively new grouping. First proposed in 1977, Carl Woese's three-domain system was not generally accepted until later. One main characteristic of the three-domain method is the separation of Archaea and Bacteria, previously grouped into the single kingdom Bacteria (a kingdom also sometimes called Monera), with the Eukaryota for all organisms whose cells contain a nucleus. A small number of scientists include a sixth kingdom, Archaea, but do not accept the domain method.\n\nThomas Cavalier-Smith, who has published extensively on the classification of protists, has recently proposed that the Neomura, the clade that groups together the Archaea and Eucarya, would have evolved from Bacteria, more precisely from Actinobacteria. His 2004 classification treated the archaeobacteria as part of a subkingdom of the Kingdom Bacteria, i.e. he rejected the three-domain system entirely. Stefan Luketa in 2012 proposed a five \"dominion\" system, adding Prionobiota (acellular and without nucleic acid) and Virusobiota (acellular but with nucleic acid) to the traditional three domains.\nPartial classifications exist for many individual groups of organisms and are revised and replaced as new information becomes available, however comprehensive treatments of most or all life are rarer; two recent examples are that of Adl et al., 2012, which covers eukaryotes only with an emphasis on protists, and Ruggiero et al., 2015, covering both eukaryotes and prokaryotes to the rank of Order, although both exclude fossil representatives.\n\nBiological taxonomy is a sub-discipline of biology, and is generally practiced by biologists known as \"taxonomists\", though enthusiastic naturalists are also frequently involved in the publication of new taxa. Because taxonomy aims to describe and organize life, the work conducted by taxonomists is essential for the study of biodiversity and the resulting field of conservation biology.\n\nBiological classification is a critical component of the taxonomic process. As a result, it informs the user as to what the relatives of the taxon are hypothesized to be. Biological classification uses taxonomic ranks, including among others (in order from most inclusive to least inclusive): Domain, Kingdom, Phylum, Class, Order, Family, Genus, and Species.\n\nThe \"definition\" of a taxon is encapsulated by its description or its diagnosis or by both combined. There are no set rules governing the definition of taxa, but the naming and publication of new taxa is governed by sets of rules. In zoology, the nomenclature for the more commonly used ranks (superfamily to subspecies), is regulated by the \"International Code of Zoological Nomenclature\" (\"ICZN Code\"). In the fields of botany, phycology, and mycology, the naming of taxa is governed by the \"International Code of Nomenclature for algae, fungi, and plants\" (\"ICN\").\n\nThe initial description of a taxon involves five main requirements:\n\n\nHowever, often much more information is included, like the geographic range of the taxon, ecological notes, chemistry, behavior, etc. How researchers arrive at their taxa varies: depending on the available data, and resources, methods vary from simple quantitative or qualitative comparisons of striking features, to elaborate computer analyses of large amounts of DNA sequence data.\n\nAn \"authority\" may be placed after a scientific name. The authority is the name of the scientist or scientists who first validly published the name. For example, in 1758 Linnaeus gave the Asian elephant the scientific name \"Elephas maximus\", so the name is sometimes written as \"\"Elephas maximus\" Linnaeus, 1758\". The names of authors are frequently abbreviated: the abbreviation \"L., for Linnaeus,\" is commonly used. In botany, there is, in fact, a regulated list of standard abbreviations (see list of botanists by author abbreviation). The system for assigning authorities differs slightly between botany and zoology. However, it is standard that if a species' name or placement has been changed since the original description, the original authority's name is placed in parentheses.\n\nIn phenetics, also known as taximetrics, or numerical taxonomy, organisms are classified based on overall similarity, regardless of their phylogeny or evolutionary relationships. It results in a measure of evolutionary \"distance\" between taxa. Phenetic methods have become relatively rare in modern times, largely superseded by cladistic analyses, as phenetic methods do not distinguish plesiomorphic from apomorphic traits. However, certain phenetic methods, such as neighbor joining, have found their way into cladistics, as a reasonable approximation of phylogeny when more advanced methods (such as Bayesian inference) are too computationally expensive.\n\nModern taxonomy uses database technologies to search and catalogue classifications and their documentation. While there is no commonly used database, there are comprehensive databases such as the \"Catalogue of Life\", which attempts to list every documented species. The catalogue listed 1.64 million species for all kingdoms as of April 2016, claiming coverage of more than three quarters of the estimated species known to modern science.\n\n\n",
    "id": "30463",
    "title": "Taxonomy (biology)"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=25466113",
    "text": "Cormophyte\n\nCormophytes are the \"plants differentiated into roots, shoots and leaves, and well adapted for life on land, comprising pteridophytes and the Spermatophyta.\" \nThese plants differ from thallophytes, whose body is referred to as the thallus, i.e. a simple body not differentiated into leaf and stem, as of lichens, multicellular algae and some liverworts.\n",
    "id": "25466113",
    "title": "Cormophyte"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=56209785",
    "text": "List of taxa with candidatus status\n\nThis is a list of taxa with candidatus status.\n\n\n\n\n\n\nUnless otherwise noted (♦), these entries are from LPSN.\n\n\n",
    "id": "56209785",
    "title": "List of taxa with candidatus status"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=1651963",
    "text": "Biomedical cybernetics\n\nBiomedical cybernetics investigates signal processing, decision making and control structures in living organisms. Applications of this research field are in biology, ecology and health sciences.\n\n\n\n\n\n",
    "id": "1651963",
    "title": "Biomedical cybernetics"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=16554664",
    "text": "Living systems\n\nLiving systems are open self-organizing life forms that interact with their environment. These systems are maintained by flows of information, energy and matter.\n\nSome scientists have proposed in the last few decades that a general living systems theory is required to explain the nature of life. Such a general theory, arising out of the ecological and biological sciences, attempts to map general principles for how all living systems work. Instead of examining phenomena by attempting to break things down into components, a general living systems theory explores phenomena in terms of dynamic patterns of the relationships of organisms with their environment.\n\nLiving systems theory is a general theory about the existence of all living systems, their structure, interaction, behavior and development. This work is created by James Grier Miller, which was intended to formalize the concept of life. According to Miller's original conception as spelled out in his magnum opus \"Living Systems\", a \"living system\" must contain each of twenty \"critical subsystems\", which are defined by their functions and visible in numerous systems, from simple cells to organisms, countries, and societies. In \"Living Systems\" Miller provides a detailed look at a number of systems in order of increasing size, and identifies his subsystems in each. \nMiller considers living systems as a subset of all systems. Below the level of living systems, he defines space and time, matter and energy, information and entropy, levels of organization, and physical and conceptual factors, and above living systems ecological, planetary and solar systems, galaxies, etc.\n\nLiving systems according to Parent (1996) are by definition \"open self-organizing systems that have the special characteristics of life and interact with their environment. This takes place by means of information and material-energy exchanges. Living systems can be as simple as a single cell or as complex as a supranational organization such as the European Union. Regardless of their complexity, they each depend upon the same essential twenty subsystems (or processes) in order to survive and to continue the propagation of their species or types beyond a single generation\".\n\nMiller said that systems exist at eight \"nested\" hierarchical levels: cell, organ, organism, group, organization, community, society, and supranational system. At each level, a system invariably comprises twenty critical subsystems, which process matter–energy or information except for the first two, which process both matter–energy and information: reproducer and boundary.\n\nThe processors of matter–energy are: \n\nThe processors of information are:\n\nJames Grier Miller in 1978 wrote a 1,102-page volume to present his living systems theory. He constructed a general theory of living systems by focusing on concrete systems—nonrandom accumulations of matter–energy in physical space–time organized into interacting, interrelated subsystems or components. Slightly revising the original model a dozen years later, he distinguished eight \"nested\" hierarchical levels in such complex structures. Each level is \"nested\" in the sense that each higher level contains the next lower level in a nested fashion.\n\nHis central thesis is that the systems in existence at all eight levels are open systems composed of twenty critical subsystems that process inputs, throughputs, and outputs of various forms of matter–energy and information. Two of these subsystems—reproducer and boundary—process both matter–energy and information. Eight of them process only matter–energy. The other ten process information only. \nAll nature is a continuum. The endless complexity of life is organized into patterns which repeat themselves—theme and variations—at each level of system. These similarities and differences are proper concerns for science. From the ceaseless streaming of protoplasm to the many-vectored activities of supranational systems, there are continuous flows through living systems as they maintain their highly organized steady states.\nSeppänen (1998) says that Miller applied general systems theory on a broad scale to describe all aspects of living systems.\n\nMiller's theory posits that the mutual interrelationship of the components of a system extends across the hierarchical levels. Examples: Cells and organs of a living system thrive on the food the organism obtains from its suprasystem; the member countries of a supranational system reap the benefits accrued from the communal activities to which each one contributes. Miller says that his eclectic theory \"ties together past discoveries from many disciplines and provides an outline into which new findings can be fitted\".\n\nMiller says the concepts of space, time, matter, energy, and information are essential to his theory because the living systems exist in space and are made of matter and energy organized by information. Miller's theory of living systems employs two sorts of spaces: physical or geographical space, and conceptual or abstracted spaces. Time is the fundamental \"fourth dimension\" of the physical space–time continuum/spiral. Matter is anything that has mass and occupies physical space. Mass and energy are equivalent as one can be converted into the other. Information refers to the degrees of freedom that exist in a given situation to choose among signals, symbols, messages, or patterns to be transmitted.\n\nOther relevant concepts are system, structure, process, type, level, echelon, suprasystem, subsystem, transmissions, and steady state. A system can be conceptual, concrete or abstracted. The structure of a system is the arrangement of the subsystems and their components in three-dimensional space at any point of time. Process, which can be reversible or irreversible, refers to change over time of matter–energy or information in a system. Type defines living systems with similar characteristics. Level is the position in a hierarchy of systems. Many complex living systems, at various levels, are organized into two or more echelons. The suprasystem of any living system is the next higher system in which it is a subsystem or component. The totality of all the structures in a system which carry out a particular process is a subsystem. Transmissions are inputs and outputs in concrete systems. Because living systems are open systems, with continually altering fluxes of matter–energy and information, many of their equilibria are dynamic—situations identified as steady states or flux equilibria.\n\nMiller identifies the comparable matter–energy and information processing critical subsystems. Elaborating on the eight hierarchical levels, he defines society, which constitutes the seventh hierarchy, as \"a large, living, concrete system with [community] and lower am levels of living systems as subsystems and components\". Society may include small, primitive, totipotential communities; ancient city–states, and kingdoms; as well as modern nation–states and empires that are not supranational systems. Miller provides general descriptions of each of the subsystems that fit all eight levels.\n\nA supranational system, in Miller's view, \"is composed of two or more societies, some or all of whose processes are under the control of a decider that is superordinate to their highest echelons\". However, he contends that no supranational system with all its twenty subsystems under control of its decider exists today. The absence of a supranational decider precludes the existence of a concrete supranational system. Miller says that studying a supranational system is problematical because its subsystems\n...tend to consist of few components besides the decoder. These systems do little matter-energy processing. The power of component societies [nations] today is almost always greater than the power of supranational deciders. Traditionally, theory at this level has been based upon intuition and study of history rather than data collection. Some quantitative research is now being done, and construction of global-system models and simulations is currently burgeoning.\n\nAt the supranational system level, Miller's emphasis is on international organizations, associations, and groups comprising representatives of societies (nation–states). Miller identifies the subsystems at this level to suit this emphasis. Thus, for example, the reproducer is \"any multipurpose supranational system which creates a single purpose supranational organization\" (p. 914); and the boundary is the \"supranational forces, usually located on or near supranational borders, which defend, guard, or police them\" (p. 914).\n\nNot just those specialized in international communication, but all communication science scholars could pay particular attention to the major contributions of living systems theory (LST) to social systems approaches that Bailey has pointed out:\n\nBailey says that LST, perhaps the \"most integrative\" social systems theory, has made many more contributions that may be easily overlooked, such as: providing a detailed analysis of types of systems; making a distinction between concrete and abstracted systems; discussion of physical space and time; placing emphasis on information processing; providing an analysis of entropy; recognition of totipotential systems, and partipotential systems; providing an innovative approach to the structure–process issue; and introducing the concept of joint subsystem—a subsystem that belongs to two systems simultaneously; of dispersal—lateral, outward, upward, and downward; of inclusion—inclusion of something from the environment that is not part of the system; of artifact—an animal-made or human-made inclusion; of adjustment process, which combats stress in a system; and of critical subsystems, which carry out processes that all living systems need to survive.\n\nLST's analysis of the twenty interacting subsystems, Bailey adds, clearly distinguishing between matter–energy-processing and information-processing, as well as LST's analysis of the eight interrelated system levels, enables us to understand how social systems are linked to biological systems. LST also analyzes the irregularities or \"organizational pathologies\" of systems functioning (e.g., system stress and strain, feedback irregularities, information–input overload). It explicates the role of entropy in social research while it equates negentropy with information and order. It emphasizes both structure and process, as well as their interrelations.\n\nIt omits the analysis of subjective phenomena, and it overemphasizes concrete Q-analysis (correlation of objects) to the virtual exclusion of R-analysis (correlation of variables). By asserting that societies (ranging from totipotential communities to nation-states and non-supranational systems) have greater control over their subsystem components than supranational systems have, it dodges the issue of transnational power over the contemporary social systems. Miller's supranational system bears no resemblance to the modern world-system that Immanuel Wallerstein (1974) described, although both of them were looking at the same living (dissipative) structure.\n\n\n\n",
    "id": "16554664",
    "title": "Living systems"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=18789195",
    "text": "The Seven Pillars of Life\n\nThe Seven Pillars of Life are the essential principles of life described by Daniel E. Koshland in 2002 in order to create a universal definition of life. One stated goal of this universal definition is to aid in understanding and identifying artificial and extraterrestrial life. The seven pillars are Program, Improvisation, Compartmentalization, Energy, Regeneration, Adaptability, and Seclusion. These can be abbreviated as PICERAS.\n\nKoshland defines \"Program\" as an \"organized plan that describes both the ingredients themselves and the kinetics of the interactions among ingredients as the living system persists through time.\" In natural life as it is known on Earth, the program operates through the mechanisms of nucleic acids and amino acids, but the concept of program can apply to other imagined or undiscovered mechanisms.\n\n\"Improvisation\" refers to the living system's ability to change its program in response to the larger environment in which it exists. An example of improvisation on earth is natural selection.\n\n\"Compartmentalization\" refers to the separation of spaces in the living system that allow for separate environments for necessary chemical processes. Compartmentalization is necessary to protect the concentration of the ingredients for a reaction from outside environments.\n\nBecause living systems involve net movement in terms of chemical movement or body movement, and lose energy in those movements through entropy, energy is required for a living system to exist. The main source of energy on Earth is the sun, but other sources of energy exist for life on Earth, such as hydrogen gas or methane, used in chemosynthesis.\n\n\"Regeneration\" in a living system refers to the general compensation for losses and degradation in the various components and processes in the system. This covers the thermodynamic loss in chemical reactions, the wear and tear of larger parts, and the larger decline of components of the system in ageing. Living systems replace these losses by importing molecules from the outside environment, synthesizing new molecules and components, or creating new generations to start the system over again.\n\n\"Adaptability\" is the ability of a living system to respond to needs, dangers, or changes. It is distinguished from improvisation because the response is timely and does not involve a change of the program. Adaptability occurs from a molecular level to a behavioral level through feedback and feedforward systems. For example, an animal seeing a predator will respond to the danger with hormonal changes and escape behavior.\n\n\"Seclusion\" is the separation of chemical pathways and the specificity of the effect of molecules, so that processes can function separately within the living system. In organisms on Earth, proteins aid in seclusion because of their individualized structure that are specific for their function, so that they can efficiently act without affecting separate functions.\n\nY. N. Zhuravlev and V. A. Avetisov have analyzed Koshland's seven pillars from the context of primordial life and, though calling the concept \"elegant,\" point out that the pillars of compartmentalization, program, and seclusion don't apply well to the non-differentiated earliest life.\n\n",
    "id": "18789195",
    "title": "The Seven Pillars of Life"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=44262036",
    "text": "Coleridge's theory of life\n\nRomanticism grew largely out of an attempt to understand not just inert nature, but also vital nature. Romantic works in the realm of art and Romantic medicine were a response to the general failure of the application of method of inertial science to reveal the foundational laws and operant principles of vital nature. German romantic science and medicine sought to understand the nature of the life principle identified by John Hunter as distinct from matter itself via Johan Friedrich Blumenbach's \"Bildungstrieb\" and Romantic medicine's \"Lebenskraft\", as well as Röschlaub's development of the Brunonian system of medicine system of John Brown, in his \"excitation theory\" of life (German:\"Erregbarkeit theorie\"), working also with Schelling's \"Naturphilosophie\", the work of Goethe regarding morphology, and the first dynamic conception of physiology of Richard Saumarez. But it is in Samuel Taylor Coleridge that we find the question of life and vital nature most intensely and comprehensively examined, particularly in his Hints towards the Formation of a more Comprehensive Theory of Life (1818), providing the foundation for Romantic philosophy, science and medicine. The work is key to understanding the relationship of Romantic literature and science.\n\nThe Enlightenment had developed a philosophy and science supported by formidable twin pillars: the first the Cartesian split of mind and matter, the second Newtonian physics, with its conquest of inert nature, both of which focused the mind's gaze on things or objects. For Cartesian philosophy, life existed on the side of matter, not mind; and for the physical sciences, the method that had been so productive for revealing the secrets of inert nature, should be equally productive in examining vital nature. The initial attempt to seek the cause and principle of life in matter was challenged by John Hunter, who held that the principle of life was not to be found nor confined within matter, but existed independently of matter itself, and informed or animated it, that is, he implied, it was the unifying or antecedent cause of the things or what Aristotelean philosophy termed \"natura naturata\" (the outer appearances of nature).\nThis reduction of the question of life to matter, and the corollary, that the method of the inertial sciences was the way to understand the very phenomenon of life, that is, its very nature and essence as a power (\"natura naturans\"), not as manifestations through sense-perceptible appearances (\"natura naturata\"), also reduced the individual to a material-mechanical 'thing' and seemed to render human freedom an untenable concept. It was this that Romanticism challenged, seeking instead to find an approach to the essence of nature as being also vital not simply inert, through a systematic method involving not just physics, but physiology (living functions). For Coleridge, quantitative analysis was anti-realist and needed to be grounded in qualitative analysis ('-ologies') (as was the case with Goethe's approach).\nAt the same time, the Romantics had to deal with the idealistic view that life was a 'somewhat' outside of things, such that the things themselves lost any real existence, a stream coming through Hume and Kant, and also infusing the German natural philosophical stream, German idealism, and in particular \"naturphilosophie\", eventuating scientifically in the doctrine of 'vitalism'. For the Romantics, life is independent of and antecedent to nature, but also infused and suspended in nature, not apart from it, As David Bohm expresses it in more modern terms \"In nature nothing remains constant…everything comes from other things and gives rise to other things. This principle is…at the foundation of the possibility of our understanding nature in a rational way.\"\n\nAnd as Coleridge explained, 'this antecedent unity, or cause and principle of each union, it has since the time of Bacon and Kepler been customary to call a law.\" And as law, \"we derive from it a progressive insight into the necessity and generation of the phenomena of which it is the law.\"\n\nColeridge's was the dominant mind on many issues involving the philosophy and science in his time, as John Stuart Mill acknowledged, along with others since who have studied the history of Romanticism.\n\nFor Coleridge, as for many of his romantic contemporaries, the idea that matter itself can begat life only dealt with the various changes in the arrangement of particles and did not explain life itself as a principle or power that lay behind the material manifestations, \"natura naturans\" or \"the productive power suspended and, as it were, quenched in the product\" Until this were addressed, according to Coleridge, \"we have not yet attained to a science of nature.\"\n\nThis productive power is above sense experience, but not above nature herself, that is, supersensible, but not supernatural, and, thus, not 'occult' as was the case with vitalism. Vitalism failed to distinguish between spirit and nature, and then within nature, between the visible appearances and the invisible, yet very real and not simply hypothesized notion, essence or motivating principle (\"natura naturans\"). Even Newton spoke of things invisible in themselves (though not in their manifestations), such as force, though Comte, the thorough materialist, complained of the use of such terms as the 'force of gravity' as being relics of animism.\n\nMatter was not a 'datum' or thing in and of itself, but rather a product or effect, and for Coleridge, looking at life in its broadest sense, it was the product of a polarity of forces and energies, but derived from a unity which is itself a power, not an abstract or nominal concept, that is Life, and this polar nature of forces within the power of Life is the very law or 'Idea' (in the Platonic sense) of Creation.\n\nFor Coleridge the essence of the universe is motion and motion is driven by a dynamic polarity of forces that is both inherent in the world as potential and acting inherently in all manifestations. This polarity is the very dynamic that acts throughout all of nature, including into the more particular form of 'life biological', as well as of mind and consciousness.\n\nAnd this polarity is dynamic, that is real, though not visible, and not simply logical or abstract. Thus, the polarity results in manifestations that are real, as the opposite powers are not contradictory, but counteracting and inter-penetrating.\n\nThus, then, Life itself is not a thing—a self-subsistent hypostasis—but an act and process...\n\nAnd in that sense Coleridge re-phrases the question \"What is Life?\" to \"What is not Life that really is?\"\n\nThis dynamic polar essence of nature in all its functions and manifestations is a universal law in the order of the law of gravity and other physical laws of inert nature. And, critically, this dynamic polarity of constituent powers of life at all levels is not outside or above nature, but is within nature (\"natura naturans\"), not as a part of the visible product, but as the ulterior natural functions that produce such products or things.\n\nIt is these functions that provided the bridge being sought by Romantic science and medicine, in particular by Andreas Röschlaub and the Brunonian system of medicine, between the inertial science of inert nature (physics) and the vital science of vital nature (physiology) and its therapeutic application or physic (the domain of the physician).\n\nColeridge was influenced by German philosophy, in particular Kant, Fichte and Schelling (Naturphilosophie), as well as the physiology of Blumenbach and the dynamic excitation theory of life of the Brunonian system. He sought a path that was neither the mystical tendency of the earlier vitalists nor the materialistic reductionist approach to natural science, but a dynamic one.\n\nColeridge's challenge was to describe something that was dynamic neither in mystical terms not materialistic ones, but via analogy, drawing from the examples of inertial science. As one writer explains, he uses the examples of electricity, magnetism and gravity not because they are like life, but because they offer a way of understanding powers, forces and energies, which lie at the heart of life. And using these analogies, Coleridge seeks to demonstrate that life is not a material force, but a product of relations amongst forces. Life is not linear and static, but dynamic process of self-regulation and Emergent evolution that results in increasing complexity and individuation. This spiral, upward movement (cf. Goethe's ideas) creates a force for organization that unifies, and is most intense and powerful in that which is most complex and most individual - the self-regulating, enlightened, developed individual mind. But at the same time, this process of life increases interdependence (like the law of comparative advantage in economics) and associational powers of the mind. Thus, he is not talking about an isolated, individual subjective mind, but about the evolution of a higher level of consciousness and thought at the core of the process of life.\n\nAnd the direction of this motion is towards increasing individuation, that is the creation of specific, individual units of things. At the same time, given the dynamic polarity of the world, there must always be an equal and opposite tendency, in this case, that of connection. So, a given of our experience is that man is both an individual, tending in each life and in history generally to greater and greater individualization, and a social creature seeking interaction and connection. It is the dynamic interplay between the individuation and connecting forces that leads to higher and higher individuation.\n\nColeridge makes a further distinction between mathematics and life, the latter being productive or creative, that is, living, and the former ideal. Thus, the mathematical approach that works so well with inert nature, is not suitable for vital nature.\n\nThe counteraction then of the two assumed forces does not depend on their meeting from opposite directions; the power which acts in them is indestructible; it is therefore inexhaustibly re-ebullient; and as something must be the result of these two forces, both alike infinite, and both alike indestructible; and as rest or neutralization cannot be this result; no other conception is possible, but that the product must be a tertium aliquid, [a third thing] or finite generation. Consequently this conception is necessary. Now this tertium aliquid can be no other than an inter-penetration of the counteracting powers, partaking of both… Consequently the 'constituent powers', that have given rise to a body, may then reappear in it as its function: \"a Power, acting in and by its Product or Representative to a predetermined purpose is a Function...the first product of its energy is the thing itself: \"ipsa se posuit et iam facta est ens positum\". Still, however, its productive energy is not exhausted in this product, but overflows, or is effluent, as the specific forces, properties, faculties, of the product. It reappears, in short, as the function of the body...The vital functions are consequents of the \"Vis Vitae Principium Vitale\", and presuppose the Organs, as the Functionaries.\n\nLife, that is, the essential polarity in unity (multeity in unity) in Coleridge’s sense also has a four beat cycle, different from the arid dialectics of abstraction - namely the tension of the polar forces themselves, the charge of their synthesis, the discharge of their product (indifference) and the resting state of this new form (predominance). The product is not a neutralization, but a new form of the essential forces, these forces remaining within, though now as the functions of the form.\n\nTo make it adequate, we must substitute the idea of positive production for that of rest, or mere neutralization. To the fancy alone it is the null-point, or zero, but to the reason it is the punctum saliens, and the power itself in its eminence.\n\nThis dynamic polarity that is Life is expressed at different levels. At its most basic it is Space-Time, with its product - motion. The interplay of both gives us either a line or a circle, and then there are different degrees possible within a given form or “predominance” of forces. Geometry is not conceivable except as the dynamic interplay of space (periphery) and time (point). Space, time and motion are also geometrically represented by width, length (breadth) and depth. And this correspondence is repeated throughout the scale of Life.\n\nMatter, then, is the product of the dynamic forces - repulsion (centrifugal), and attraction (centripetal); it is not itself a productive power. It is also the mass of a given body.\n\nColeridge’s understanding of life is contrasted with the materialist view which is essentially reduced to defining life as that which is the opposite of not-life, or that which resists death, that is, that which is life.\n\nThe problem for Coleridge and the Romantics was that the intellect, 'left to itself' as Bacon stated, was capable of apprehending only the outer forms of nature (natura naturata) and not the inmost, living functions (natura naturans) giving rise to these forms. Thus, effects can only be 'explained' in terms of other effects, not causes. It takes a different capacity to 'see' these living functions, which is an imaginative activity. For Coleridge, there is an innate, primitive or 'primary' imagination that configures invisibly sense-experience into perception, but a rational perception, that is, one raised into consciousness and awareness and then rationally presentable, requires a higher level, what he termed 'secondary imagination', which is able to connect with the thing being experienced, penetrate to its essence in terms of the living dynamics upholding its outer form, and then present the phenomena as and within its natural law, and further, using reason, develop the various principles of its operation.\n\nThis cognitive capacity involved what Coleridge termed the 'inmost sense' or what Goethe termed the Gemüt. It also involved the reactivation of the old Greek noetic capacity, and the ability to 'see' or produce the theory (Greek \"theoria\" from the verb 'to see') dynamic polarities, or natural Laws, the dynamic transcendent (foundational) entities that Plato termed 'Ideas' (\"eidos\").\n\nSince \"natura naturata\" is sustained by \"natura naturans\", and the creative power of \"natura naturans\" is one of a kind with the human mind, itself creative, then there must be a correspondence or connection between the mind and the things we perceive, such that we can overcome the apparent separation between the object and the representation in the mind of the object that came to bedevil Enlightenment thought (Hume, Kant). As one commentator noted \"to speak at all of the unity of intelligence and nature is of course flatly to contradict Descartes.\"\n\nFor Coleridge the power of life lies in every seed as a potential to be unfolded as a result of interaction with the environment (heat, light, air, moisture, etc.), an insight which allowed him to see in the Brunonian system a dynamic polarity in excitation theory.\nColeridge also saw that there was a progressive movement through time and space of life or the law of polarity, from the level of physics (space and time) and the mineral or inert nature (law of gravity, operating through forces of attraction and repulsion), up to man, with his law of resonance in terms of his innate desire to be himself (force of individuation) and to also connect with like-minded (force of connection), as Goethe expressed in his novel \"Elective Affinities\" (\"Wahlverwandschaften\") as well as in his own life's experience.\nEvolution occurred because the original polarity of creation, the very 'Law of Creation', itself gives birth to subsequent polarities, as each pole is itself a unity that can be further polarized (what Wilhelm Reich later termed 'orgonomic functionalism' and what at the biological level constitutes physiology), an insight that would later be taken up by the concept of emergent evolution, including the emergence of mind and consciousness.\n\nAnd that this is so, is also an intimate and shared experience of all humans, as is set out in Reid's Common Sense philosophy. As Coleridge states\n\nThat nature evolves towards a purpose, and that is the unfolding of the human mind and consciousness in all its levels and degrees, is not teleological but a function of the very nature of the law of polarity or creation itself, namely that of increasing individuation of an original unity, what Coleridge termed 'multeity in unity'. As he states, \"without assigning to nature as nature, a conscious purpose\" we must still \"distinguish her agency from a blind and lifeless mechanism.\"\n\nWhile man contains and is subject to the various laws of nature, man as a self-conscious being is also the summa of a process of creation leading to greater mind and consciousness, that is, a creative capacity of imagination. Instead of being a creature of circumstance, man is the creator of them, or at least has that potential.\n\n",
    "id": "44262036",
    "title": "Coleridge's theory of life"
  },
  {
    "url": "https://en.wikipedia.org/wiki?curid=3447756",
    "text": "Carbon-based life\n\nCarbon is a key component of all known life on Earth. Complex molecules are made up of carbon bonded with other elements, especially oxygen, hydrogen and nitrogen. Carbon is abundant on Earth. It is also lightweight and relatively small in size, making it easier for enzymes to manipulate carbon\nmolecules. It is frequently assumed in astrobiology that if life exists somewhere else in the universe, it will also be carbon-based. Critics refer to this assumption as \"carbon chauvinism\".\n\n\"What we normally think of as 'life' is based on chains of carbon atoms, with a few other atoms, such as nitrogen or phosphorus\", per Stephen Hawking in a 2008 lecture, \"carbon [...] has the richest chemistry.\" \nThe most important characteristics of carbon as a basis for the chemistry of life are, that it has four valence bonds, and that the energy required to make or break a bond is at an appropriate level for building molecules, which are stable and reactive. Carbon atoms bond readily to other carbon atoms; this allows the building of arbitrarily long complex molecules and polymers.\n\nThere are not many other elements which even appear to be promising candidates for supporting life, for example, processes such as metabolism. The most frequently suggested alternative is silicon. Silicon is in the same group in the Periodic Table of elements, and has four valence bonds, and bonds to itself, generally in the form of crystal lattices rather than long chains. It is considerably more electropositive than carbon. Silicon compounds do not readily recombine into different permutations in a manner that would plausibly support lifelike processes.\n\nThe most notable groups of chemicals used in the processes of living organisms include:\n\n\nSilicon has been a theme of non-carbon-based-life since it is somewhat similar to carbon in its chemical characteristics. \nIn cinematic and literary science fiction, when man-made machines cross from non-living to living, this new form would be an example of non-carbon-based life. Since the advent of the microprocessor in the late 1960s, these machines are often classed as \"silicon-based life\". Another example of \"silicon-based life\" is the episode \"The Devil in the Dark\" from , in which a living rock creature's biochemistry is based on silicon. Also in The X-Files episode \"Firewalker\" where silicon based organism was discovered in a volcano.\n\nIn the movie adaptation of Arthur C. Clarke's \"2010\" (1984) a character argues, \"Whether we are based on carbon or on silicon makes no fundamental difference; we should each be treated with appropriate respect\". This quote may be the basis of Steve Jobs's quip when he introduced Carbon within MacOS X, \"Carbon. All life forms will be based on it.\"\n\n\n",
    "id": "3447756",
    "title": "Carbon-based life"
  }
]
