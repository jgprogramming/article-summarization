{"url": "https://en.wikipedia.org/wiki?curid=9236", "text": "Evolution\n\nEvolution is change in the heritable characteristics of biological populations over successive generations. Evolutionary processes give rise to biodiversity at every level of biological organisation, including the levels of species, individual organisms, and molecules.\n\nRepeated formation of new species (speciation), change within species (anagenesis), and loss of species (extinction) throughout the evolutionary history of life on Earth are demonstrated by shared sets of morphological and biochemical traits, including shared DNA sequences. These shared traits are more similar among species that share a more recent common ancestor, and can be used to reconstruct a biological \"tree of life\" based on evolutionary relationships (phylogenetics), using both existing species and fossils. The fossil record includes a progression from early biogenic graphite, to microbial mat fossils, to fossilised multicellular organisms. Existing patterns of biodiversity have been shaped both by speciation and by extinction.\n\nIn the mid-19th century, Charles Darwin formulated the scientific theory of evolution by natural selection, published in his book \"On the Origin of Species\" (1859). Evolution by natural selection is a process demonstrated by the observation that more offspring are produced than can possibly survive, along with three facts about populations: 1) traits vary among individuals with respect to morphology, physiology, and behaviour (phenotypic variation), 2) different traits confer different rates of survival and reproduction (differential fitness), and 3) traits can be passed from generation to generation (heritability of fitness). Thus, in successive generations members of a population are replaced by progeny of parents better adapted to survive and reproduce in the biophysical environment in which natural selection takes place.\n\nThis teleonomy is the quality whereby the process of natural selection creates and preserves traits that are seemingly fitted for the functional roles they perform. The processes by which the changes occur, from one generation to another, are called evolutionary processes or mechanisms. The four most widely recognised evolutionary processes are natural selection (including sexual selection), genetic drift, mutation and gene migration due to genetic admixture. Natural selection and genetic drift sort variation; mutation and gene migration create variation.\n\nConsequences of selection can include meiotic drive (unequal transmission of certain alleles), nonrandom mating and genetic hitchhiking.\nIn the early 20th century the modern evolutionary synthesis integrated classical genetics with Darwin's theory of evolution by natural selection through the discipline of population genetics. The importance of natural selection as a cause of evolution was accepted into other branches of biology. Moreover, previously held notions about evolution, such as orthogenesis, evolutionism, and other beliefs about innate \"progress\" within the largest-scale trends in evolution, became obsolete. Scientists continue to study various aspects of evolutionary biology by forming and testing hypotheses, constructing mathematical models of theoretical biology and biological theories, using observational data, and performing experiments in both the field and the laboratory.\n\nAll life on Earth shares a common ancestor known as the last universal common ancestor (LUCA), which lived approximately 3.5–3.8 billion years ago. A December 2017 report stated that 3.45 billion year old Australian rocks once contained microorganisms, the earliest direct evidence of life on Earth. Nonetheless, this should not be assumed to be the first living organism on Earth; a study in 2015 found \"remains of biotic life\" from 4.1 billion years ago in ancient rocks in Western Australia. In July 2016, scientists reported identifying a set of 355 genes from the LUCA of all organisms living on Earth. More than 99 percent of all species that ever lived on Earth are estimated to be extinct. Estimates of Earth's current species range from 10 to 14 million, of which about 1.9 million are estimated to have been named and 1.6 million documented in a central database to date. More recently, in May 2016, scientists reported that 1 trillion species are estimated to be on Earth currently with only one-thousandth of one percent described.\n\nIn terms of practical application, an understanding of evolution has been instrumental to developments in numerous scientific and industrial fields, including agriculture, human and veterinary medicine, and the life sciences in general. Discoveries in evolutionary biology have made a significant impact not just in the traditional branches of biology but also in other academic disciplines, including biological anthropology, and evolutionary psychology. Evolutionary computation, a sub-field of artificial intelligence, involves the application of Darwinian principles to problems in computer science.\n\nThe proposal that one type of organism could descend from another type goes back to some of the first pre-Socratic Greek philosophers, such as Anaximander and Empedocles. Such proposals survived into Roman times. The poet and philosopher Lucretius followed Empedocles in his masterwork \"De rerum natura\" (\"On the Nature of Things\").\n\nIn contrast to these materialistic views, Aristotelianism considered all natural things as actualisations of fixed natural possibilities, known as forms. This was part of a medieval teleological understanding of nature in which all things have an intended role to play in a divine cosmic order. Variations of this idea became the standard understanding of the Middle Ages and were integrated into Christian learning, but Aristotle did not demand that real types of organisms always correspond one-for-one with exact metaphysical forms and specifically gave examples of how new types of living things could come to be.\n\nIn the 17th century, the new method of modern science rejected the Aristotelian approach. It sought explanations of natural phenomena in terms of physical laws that were the same for all visible things and that did not require the existence of any fixed natural categories or divine cosmic order. However, this new approach was slow to take root in the biological sciences, the last bastion of the concept of fixed natural types. John Ray applied one of the previously more general terms for fixed natural types, \"species,\" to plant and animal types, but he strictly identified each type of living thing as a species and proposed that each species could be defined by the features that perpetuated themselves generation after generation. The biological classification introduced by Carl Linnaeus in 1735 explicitly recognised the hierarchical nature of species relationships, but still viewed species as fixed according to a divine plan.\n\nOther naturalists of this time speculated on the evolutionary change of species over time according to natural laws. In 1751, Pierre Louis Maupertuis wrote of natural modifications occurring during reproduction and accumulating over many generations to produce new species. Georges-Louis Leclerc, Comte de Buffon suggested that species could degenerate into different organisms, and Erasmus Darwin proposed that all warm-blooded animals could have descended from a single microorganism (or \"filament\"). The first full-fledged evolutionary scheme was Jean-Baptiste Lamarck's \"transmutation\" theory of 1809, which envisaged spontaneous generation continually producing simple forms of life that developed greater complexity in parallel lineages with an inherent progressive tendency, and postulated that on a local level these lineages adapted to the environment by inheriting changes caused by their use or disuse in parents. (The latter process was later called Lamarckism.) These ideas were condemned by established naturalists as speculation lacking empirical support. In particular, Georges Cuvier insisted that species were unrelated and fixed, their similarities reflecting divine design for functional needs. In the meantime, Ray's ideas of benevolent design had been developed by William Paley into the \"Natural Theology or Evidences of the Existence and Attributes of the Deity\" (1802), which proposed complex adaptations as evidence of divine design and which was admired by Charles Darwin.\n\nThe crucial break from the concept of constant typological classes or types in biology came with the theory of evolution through natural selection, which was formulated by Charles Darwin in terms of variable populations. Partly influenced by \"An Essay on the Principle of Population\" (1798) by Thomas Robert Malthus, Darwin noted that population growth would lead to a \"struggle for existence\" in which favorable variations prevailed as others perished. In each generation, many offspring fail to survive to an age of reproduction because of limited resources. This could explain the diversity of plants and animals from a common ancestry through the working of natural laws in the same way for all types of organism. Darwin developed his theory of \"natural selection\" from 1838 onwards and was writing up his \"big book\" on the subject when Alfred Russel Wallace sent him a version of virtually the same theory in 1858. Their separate papers were presented together at an 1858 meeting of the Linnean Society of London. At the end of 1859, Darwin's publication of his \"abstract\" as \"On the Origin of Species\" explained natural selection in detail and in a way that led to an increasingly wide acceptance of Darwin's concepts of evolution at the expense of alternative theories. Thomas Henry Huxley applied Darwin's ideas to humans, using paleontology and comparative anatomy to provide strong evidence that humans and apes shared a common ancestry. Some were disturbed by this since it implied that humans did not have a special place in the universe.\n\nThe mechanisms of reproductive heritability and the origin of new traits remained a mystery. Towards this end, Darwin developed his provisional theory of pangenesis. In 1865, Gregor Mendel reported that traits were inherited in a predictable manner through the independent assortment and segregation of elements (later known as genes). Mendel's laws of inheritance eventually supplanted most of Darwin's pangenesis theory. August Weismann made the important distinction between germ cells that give rise to gametes (such as sperm and egg cells) and the somatic cells of the body, demonstrating that heredity passes through the germ line only. Hugo de Vries connected Darwin's pangenesis theory to Weismann's germ/soma cell distinction and proposed that Darwin's pangenes were concentrated in the cell nucleus and when expressed they could move into the cytoplasm to change the cells structure. De Vries was also one of the researchers who made Mendel's work well-known, believing that Mendelian traits corresponded to the transfer of heritable variations along the germline. To explain how new variants originate, de Vries developed a mutation theory that led to a temporary rift between those who accepted Darwinian evolution and biometricians who allied with de Vries. In the 1930s, pioneers in the field of population genetics, such as Ronald Fisher, Sewall Wright and J. B. S. Haldane set the foundations of evolution onto a robust statistical philosophy. The false contradiction between Darwin's theory, genetic mutations, and Mendelian inheritance was thus reconciled.\n\nIn the 1920s and 1930s the so-called modern synthesis connected natural selection and population genetics, based on Mendelian inheritance, into a unified theory that applied generally to any branch of biology. The modern synthesis explained patterns observed across species in populations, through fossil transitions in palaeontology, and complex cellular mechanisms in developmental biology. The publication of the structure of DNA by James Watson and Francis Crick in 1953 demonstrated a physical mechanism for inheritance. Molecular biology improved our understanding of the relationship between genotype and phenotype. Advancements were also made in phylogenetic systematics, mapping the transition of traits into a comparative and testable framework through the publication and use of evolutionary trees. In 1973, evolutionary biologist Theodosius Dobzhansky penned that \"nothing in biology makes sense except in the light of evolution,\" because it has brought to light the relations of what first seemed disjointed facts in natural history into a coherent explanatory body of knowledge that describes and predicts many observable facts about life on this planet.\n\nSince then, the modern synthesis has been further extended to explain biological phenomena across the full and integrative scale of the biological hierarchy, from genes to species. One extension, known as evolutionary developmental biology and informally called \"evo-devo,\" emphasises how changes between generations (evolution) acts on patterns of change within individual organisms (development). Since the beginning of the 21st century and in light of discoveries made in recent decades, some biologists have argued for an extended evolutionary synthesis, which would account for the effects of non-genetic inheritance modes, such as epigenetics, parental effects, ecological and cultural inheritance, and evolvability.\n\nEvolution in organisms occurs through changes in heritable traits—the inherited characteristics of an organism. In humans, for example, eye colour is an inherited characteristic and an individual might inherit the \"brown-eye trait\" from one of their parents. Inherited traits are controlled by genes and the complete set of genes within an organism's genome (genetic material) is called its genotype.\n\nThe complete set of observable traits that make up the structure and behaviour of an organism is called its phenotype. These traits come from the interaction of its genotype with the environment. As a result, many aspects of an organism's phenotype are not inherited. For example, suntanned skin comes from the interaction between a person's genotype and sunlight; thus, suntans are not passed on to people's children. However, some people tan more easily than others, due to differences in genotypic variation; a striking example are people with the inherited trait of albinism, who do not tan at all and are very sensitive to sunburn.\n\nHeritable traits are passed from one generation to the next via DNA, a molecule that encodes genetic information. DNA is a long biopolymer composed of four types of bases. The sequence of bases along a particular DNA molecule specify the genetic information, in a manner similar to a sequence of letters spelling out a sentence. Before a cell divides, the DNA is copied, so that each of the resulting two cells will inherit the DNA sequence. Portions of a DNA molecule that specify a single functional unit are called genes; different genes have different sequences of bases. Within cells, the long strands of DNA form condensed structures called chromosomes. The specific location of a DNA sequence within a chromosome is known as a locus. If the DNA sequence at a locus varies between individuals, the different forms of this sequence are called alleles. DNA sequences can change through mutations, producing new alleles. If a mutation occurs within a gene, the new allele may affect the trait that the gene controls, altering the phenotype of the organism. However, while this simple correspondence between an allele and a trait works in some cases, most traits are more complex and are controlled by quantitative trait loci (multiple interacting genes).\n\nRecent findings have confirmed important examples of heritable changes that cannot be explained by changes to the sequence of nucleotides in the DNA. These phenomena are classed as epigenetic inheritance systems. DNA methylation marking chromatin, self-sustaining metabolic loops, gene silencing by RNA interference and the three-dimensional conformation of proteins (such as prions) are areas where epigenetic inheritance systems have been discovered at the organismic level. Developmental biologists suggest that complex interactions in genetic networks and communication among cells can lead to heritable variations that may underlay some of the mechanics in developmental plasticity and canalisation. Heritability may also occur at even larger scales. For example, ecological inheritance through the process of niche construction is defined by the regular and repeated activities of organisms in their environment. This generates a legacy of effects that modify and feed back into the selection regime of subsequent generations. Descendants inherit genes plus environmental characteristics generated by the ecological actions of ancestors. Other examples of heritability in evolution that are not under the direct control of genes include the inheritance of cultural traits and symbiogenesis.\n\nAn individual organism's phenotype results from both its genotype and the influence from the environment it has lived in. A substantial part of the phenotypic variation in a population is caused by genotypic variation. The modern evolutionary synthesis defines evolution as the change over time in this genetic variation. The frequency of one particular allele will become more or less prevalent relative to other forms of that gene. Variation disappears when a new allele reaches the point of fixation—when it either disappears from the population or replaces the ancestral allele entirely.\n\nNatural selection will only cause evolution if there is enough genetic variation in a population. Before the discovery of Mendelian genetics, one common hypothesis was blending inheritance. But with blending inheritance, genetic variance would be rapidly lost, making evolution by natural selection implausible. The Hardy–Weinberg principle provides the solution to how variation is maintained in a population with Mendelian inheritance. The frequencies of alleles (variations in a gene) will remain constant in the absence of selection, mutation, migration and genetic drift.\n\nVariation comes from mutations in the genome, reshuffling of genes through sexual reproduction and migration between populations (gene flow). Despite the constant introduction of new variation through mutation and gene flow, most of the genome of a species is identical in all individuals of that species. However, even relatively small differences in genotype can lead to dramatic differences in phenotype: for example, chimpanzees and humans differ in only about 5% of their genomes.\n\nMutations are changes in the DNA sequence of a cell's genome. When mutations occur, they may alter the product of a gene, or prevent the gene from functioning, or have no effect. Based on studies in the fly \"Drosophila melanogaster\", it has been suggested that if a mutation changes a protein produced by a gene, this will probably be harmful, with about 70% of these mutations having damaging effects, and the remainder being either neutral or weakly beneficial.\n\nMutations can involve large sections of a chromosome becoming duplicated (usually by genetic recombination), which can introduce extra copies of a gene into a genome. Extra copies of genes are a major source of the raw material needed for new genes to evolve. This is important because most new genes evolve within gene families from pre-existing genes that share common ancestors. For example, the human eye uses four genes to make structures that sense light: three for colour vision and one for night vision; all four are descended from a single ancestral gene.\n\nNew genes can be generated from an ancestral gene when a duplicate copy mutates and acquires a new function. This process is easier once a gene has been duplicated because it increases the redundancy of the system; one gene in the pair can acquire a new function while the other copy continues to perform its original function. Other types of mutations can even generate entirely new genes from previously noncoding DNA.\n\nThe generation of new genes can also involve small parts of several genes being duplicated, with these fragments then recombining to form new combinations with new functions. When new genes are assembled from shuffling pre-existing parts, domains act as modules with simple independent functions, which can be mixed together to produce new combinations with new and complex functions. For example, polyketide synthases are large enzymes that make antibiotics; they contain up to one hundred independent domains that each catalyse one step in the overall process, like a step in an assembly line.\n\nIn asexual organisms, genes are inherited together, or \"linked\", as they cannot mix with genes of other organisms during reproduction. In contrast, the offspring of sexual organisms contain random mixtures of their parents' chromosomes that are produced through independent assortment. In a related process called homologous recombination, sexual organisms exchange DNA between two matching chromosomes. Recombination and reassortment do not alter allele frequencies, but instead change which alleles are associated with each other, producing offspring with new combinations of alleles. Sex usually increases genetic variation and may increase the rate of evolution.\n\nThe two-fold cost of sex was first described by John Maynard Smith. The first cost is that in sexually dimorphic species only one of the two sexes can bear young. (This cost does not apply to hermaphroditic species, like most plants and many invertebrates.) The second cost is that any individual who reproduces sexually can only pass on 50% of its genes to any individual offspring, with even less passed on as each new generation passes. Yet sexual reproduction is the more common means of reproduction among eukaryotes and multicellular organisms. The Red Queen hypothesis has been used to explain the significance of sexual reproduction as a means to enable continual evolution and adaptation in response to coevolution with other species in an ever-changing environment.\n\nGene flow is the exchange of genes between populations and between species. It can therefore be a source of variation that is new to a population or to a species. Gene flow can be caused by the movement of individuals between separate populations of organisms, as might be caused by the movement of mice between inland and coastal populations, or the movement of pollen between heavy metal tolerant and heavy metal sensitive populations of grasses.\n\nGene transfer between species includes the formation of hybrid organisms and horizontal gene transfer. Horizontal gene transfer is the transfer of genetic material from one organism to another organism that is not its offspring; this is most common among bacteria. In medicine, this contributes to the spread of antibiotic resistance, as when one bacteria acquires resistance genes it can rapidly transfer them to other species. Horizontal transfer of genes from bacteria to eukaryotes such as the yeast \"Saccharomyces cerevisiae\" and the adzuki bean weevil \"Callosobruchus chinensis\" has occurred. An example of larger-scale transfers are the eukaryotic bdelloid rotifers, which have received a range of genes from bacteria, fungi and plants. Viruses can also carry DNA between organisms, allowing transfer of genes even across biological domains.\n\nLarge-scale gene transfer has also occurred between the ancestors of eukaryotic cells and bacteria, during the acquisition of chloroplasts and mitochondria. It is possible that eukaryotes themselves originated from horizontal gene transfers between bacteria and archaea.\n\nFrom a Neo-Darwinian perspective, evolution occurs when there are changes in the frequencies of alleles within a population of interbreeding organisms. For example, the allele for black colour in a population of moths becoming more common. Mechanisms that can lead to changes in allele frequencies include natural selection, genetic drift, genetic hitchhiking, mutation and gene flow.\n\nEvolution by means of natural selection is the process by which traits that enhance survival and reproduction become more common in successive generations of a population. It has often been called a \"self-evident\" mechanism because it necessarily follows from three simple facts:\n\nMore offspring are produced than can possibly survive, and these conditions produce competition between organisms for survival and reproduction. Consequently, organisms with traits that give them an advantage over their competitors are more likely to pass on their traits to the next generation than those with traits that do not confer an advantage.\n\nThe central concept of natural selection is the evolutionary fitness of an organism. Fitness is measured by an organism's ability to survive and reproduce, which determines the size of its genetic contribution to the next generation. However, fitness is not the same as the total number of offspring: instead fitness is indicated by the proportion of subsequent generations that carry an organism's genes. For example, if an organism could survive well and reproduce rapidly, but its offspring were all too small and weak to survive, this organism would make little genetic contribution to future generations and would thus have low fitness.\n\nIf an allele increases fitness more than the other alleles of that gene, then with each generation this allele will become more common within the population. These traits are said to be \"selected \"for\".\" Examples of traits that can increase fitness are enhanced survival and increased fecundity. Conversely, the lower fitness caused by having a less beneficial or deleterious allele results in this allele becoming rarer—they are \"selected \"against\".\" Importantly, the fitness of an allele is not a fixed characteristic; if the environment changes, previously neutral or harmful traits may become beneficial and previously beneficial traits become harmful. However, even if the direction of selection does reverse in this way, traits that were lost in the past may not re-evolve in an identical form (see Dollo's law). However, a re-activation of dormant genes, as long as they have not been eliminated from the genome and were only suppressed perhaps for hundreds of generations, can lead to the re-occurrence of traits thought to be lost like hindlegs in dolphins, teeth in chickens, wings in wingless stick insects, tails and additional nipples in humans etc. \"Throwbacks\" such as these are known as atavisms.\n\nNatural selection within a population for a trait that can vary across a range of values, such as height, can be categorised into three different types. The first is directional selection, which is a shift in the average value of a trait over time—for example, organisms slowly getting taller. Secondly, disruptive selection is selection for extreme trait values and often results in two different values becoming most common, with selection against the average value. This would be when either short or tall organisms had an advantage, but not those of medium height. Finally, in stabilising selection there is selection against extreme trait values on both ends, which causes a decrease in variance around the average value and less diversity. This would, for example, cause organisms to eventually have a similar height.\n\nA special case of natural selection is sexual selection, which is selection for any trait that increases mating success by increasing the attractiveness of an organism to potential mates. Traits that evolved through sexual selection are particularly prominent among males of several animal species. Although sexually favoured, traits such as cumbersome antlers, mating calls, large body size and bright colours often attract predation, which compromises the survival of individual males. This survival disadvantage is balanced by higher reproductive success in males that show these hard-to-fake, sexually selected traits.\n\nNatural selection most generally makes nature the measure against which individuals and individual traits, are more or less likely to survive. \"Nature\" in this sense refers to an ecosystem, that is, a system in which organisms interact with every other element, physical as well as biological, in their local environment. Eugene Odum, a founder of ecology, defined an ecosystem as: \"Any unit that includes all of the organisms...in a given area interacting with the physical environment so that a flow of energy leads to clearly defined trophic structure, biotic diversity and material cycles (ie: exchange of materials between living and nonliving parts) within the system.\" Each population within an ecosystem occupies a distinct niche, or position, with distinct relationships to other parts of the system. These relationships involve the life history of the organism, its position in the food chain and its geographic range. This broad understanding of nature enables scientists to delineate specific forces which, together, comprise natural selection.\n\nNatural selection can act at different levels of organisation, such as genes, cells, individual organisms, groups of organisms and species. Selection can act at multiple levels simultaneously. An example of selection occurring below the level of the individual organism are genes called transposons, which can replicate and spread throughout a genome. Selection at a level above the individual, such as group selection, may allow the evolution of cooperation, as discussed below.\n\nIn addition to being a major source of variation, mutation may also function as a mechanism of evolution when there are different probabilities at the molecular level for different mutations to occur, a process known as mutation bias. If two genotypes, for example one with the nucleotide G and another with the nucleotide A in the same position, have the same fitness, but mutation from G to A happens more often than mutation from A to G, then genotypes with A will tend to evolve. Different insertion vs. deletion mutation biases in different taxa can lead to the evolution of different genome sizes. Developmental or mutational biases have also been observed in morphological evolution. For example, according to the phenotype-first theory of evolution, mutations can eventually cause the genetic assimilation of traits that were previously induced by the environment.\n\nMutation bias effects are superimposed on other processes. If selection would favor either one out of two mutations, but there is no extra advantage to having both, then the mutation that occurs the most frequently is the one that is most likely to become fixed in a population. Mutations leading to the loss of function of a gene are much more common than mutations that produce a new, fully functional gene. Most loss of function mutations are selected against. But when selection is weak, mutation bias towards loss of function can affect evolution. For example, pigments are no longer useful when animals live in the darkness of caves, and tend to be lost. This kind of loss of function can occur because of mutation bias, and/or because the function had a cost, and once the benefit of the function disappeared, natural selection leads to the loss. Loss of sporulation ability in \"Bacillus subtilis\" during laboratory evolution appears to have been caused by mutation bias, rather than natural selection against the cost of maintaining sporulation ability. When there is no selection for loss of function, the speed at which loss evolves depends more on the mutation rate than it does on the effective population size, indicating that it is driven more by mutation bias than by genetic drift. In parasitic organisms, mutation bias leads to selection pressures as seen in Ehrlichia. Mutations are biased towards antigenic variants in outer-membrane proteins.\n\nGenetic drift is the change in allele frequency from one generation to the next that occurs because alleles are subject to sampling error. As a result, when selective forces are absent or relatively weak, allele frequencies tend to \"drift\" upward or downward randomly (in a random walk). This drift halts when an allele eventually becomes fixed, either by disappearing from the population, or replacing the other alleles entirely. Genetic drift may therefore eliminate some alleles from a population due to chance alone. Even in the absence of selective forces, genetic drift can cause two separate populations that began with the same genetic structure to drift apart into two divergent populations with different sets of alleles.\n\nIt is usually difficult to measure the relative importance of selection and neutral processes, including drift. The comparative importance of adaptive and non-adaptive forces in driving evolutionary change is an area of current research.\n\nThe neutral theory of molecular evolution proposed that most evolutionary changes are the result of the fixation of neutral mutations by genetic drift. Hence, in this model, most genetic changes in a population are the result of constant mutation pressure and genetic drift. This form of the neutral theory is now largely abandoned, since it does not seem to fit the genetic variation seen in nature. However, a more recent and better-supported version of this model is the nearly neutral theory, where a mutation that would be effectively neutral in a small population is not necessarily neutral in a large population. Other alternative theories propose that genetic drift is dwarfed by other stochastic forces in evolution, such as genetic hitchhiking, also known as genetic draft.\n\nThe time for a neutral allele to become fixed by genetic drift depends on population size, with fixation occurring more rapidly in smaller populations. The number of individuals in a population is not critical, but instead a measure known as the effective population size. The effective population is usually smaller than the total population since it takes into account factors such as the level of inbreeding and the stage of the lifecycle in which the population is the smallest. The effective population size may not be the same for every gene in the same population.\n\nRecombination allows alleles on the same strand of DNA to become separated. However, the rate of recombination is low (approximately two events per chromosome per generation). As a result, genes close together on a chromosome may not always be shuffled away from each other and genes that are close together tend to be inherited together, a phenomenon known as linkage. This tendency is measured by finding how often two alleles occur together on a single chromosome compared to expectations, which is called their linkage disequilibrium. A set of alleles that is usually inherited in a group is called a haplotype. This can be important when one allele in a particular haplotype is strongly beneficial: natural selection can drive a selective sweep that will also cause the other alleles in the haplotype to become more common in the population; this effect is called genetic hitchhiking or genetic draft. Genetic draft caused by the fact that some neutral genes are genetically linked to others that are under selection can be partially captured by an appropriate effective population size.\n\nGene flow involves the exchange of genes between populations and between species. The presence or absence of gene flow fundamentally changes the course of evolution. Due to the complexity of organisms, any two completely isolated populations will eventually evolve genetic incompatibilities through neutral processes, as in the Bateson-Dobzhansky-Muller model, even if both populations remain essentially identical in terms of their adaptation to the environment.\n\nIf genetic differentiation between populations develops, gene flow between populations can introduce traits or alleles which are disadvantageous in the local population and this may lead to organisms within these populations evolving mechanisms that prevent mating with genetically distant populations, eventually resulting in the appearance of new species. Thus, exchange of genetic information between individuals is fundamentally important for the development of the biological species concept.\n\nDuring the development of the modern synthesis, Sewall Wright developed his shifting balance theory, which regarded gene flow between partially isolated populations as an important aspect of adaptive evolution. However, recently there has been substantial criticism of the importance of the shifting balance theory.\n\nEvolution influences every aspect of the form and behaviour of organisms. Most prominent are the specific behavioural and physical adaptations that are the outcome of natural selection. These adaptations increase fitness by aiding activities such as finding food, avoiding predators or attracting mates. Organisms can also respond to selection by cooperating with each other, usually by aiding their relatives or engaging in mutually beneficial symbiosis. In the longer term, evolution produces new species through splitting ancestral populations of organisms into new groups that cannot or will not interbreed.\n\nThese outcomes of evolution are distinguished based on time scale as macroevolution versus microevolution. Macroevolution refers to evolution that occurs at or above the level of species, in particular speciation and extinction; whereas microevolution refers to smaller evolutionary changes within a species or population, in particular shifts in gene frequency and adaptation. In general, macroevolution is regarded as the outcome of long periods of microevolution. Thus, the distinction between micro- and macroevolution is not a fundamental one—the difference is simply the time involved. However, in macroevolution, the traits of the entire species may be important. For instance, a large amount of variation among individuals allows a species to rapidly adapt to new habitats, lessening the chance of it going extinct, while a wide geographic range increases the chance of speciation, by making it more likely that part of the population will become isolated. In this sense, microevolution and macroevolution might involve selection at different levels—with microevolution acting on genes and organisms, versus macroevolutionary processes such as species selection acting on entire species and affecting their rates of speciation and extinction.\n\nA common misconception is that evolution has goals, long-term plans, or an innate tendency for \"progress\", as expressed in beliefs such as orthogenesis and evolutionism; realistically however, evolution has no long-term goal and does not necessarily produce greater complexity. Although complex species have evolved, they occur as a side effect of the overall number of organisms increasing and simple forms of life still remain more common in the biosphere. For example, the overwhelming majority of species are microscopic prokaryotes, which form about half the world's biomass despite their small size, and constitute the vast majority of Earth's biodiversity. Simple organisms have therefore been the dominant form of life on Earth throughout its history and continue to be the main form of life up to the present day, with complex life only appearing more diverse because it is more noticeable. Indeed, the evolution of microorganisms is particularly important to modern evolutionary research, since their rapid reproduction allows the study of experimental evolution and the observation of evolution and adaptation in real time.\n\nAdaptation is the process that makes organisms better suited to their habitat. Also, the term adaptation may refer to a trait that is important for an organism's survival. For example, the adaptation of horses' teeth to the grinding of grass. By using the term \"adaptation\" for the evolutionary process and \"adaptive trait\" for the product (the bodily part or function), the two senses of the word may be distinguished. Adaptations are produced by natural selection. The following definitions are due to Theodosius Dobzhansky:\n\nAdaptation may cause either the gain of a new feature, or the loss of an ancestral feature. An example that shows both types of change is bacterial adaptation to antibiotic selection, with genetic changes causing antibiotic resistance by both modifying the target of the drug, or increasing the activity of transporters that pump the drug out of the cell. Other striking examples are the bacteria \"Escherichia coli\" evolving the ability to use citric acid as a nutrient in a long-term laboratory experiment, \"Flavobacterium\" evolving a novel enzyme that allows these bacteria to grow on the by-products of nylon manufacturing, and the soil bacterium \"Sphingobium\" evolving an entirely new metabolic pathway that degrades the synthetic pesticide pentachlorophenol. An interesting but still controversial idea is that some adaptations might increase the ability of organisms to generate genetic diversity and adapt by natural selection (increasing organisms' evolvability).\n\nAdaptation occurs through the gradual modification of existing structures. Consequently, structures with similar internal organisation may have different functions in related organisms. This is the result of a single ancestral structure being adapted to function in different ways. The bones within bat wings, for example, are very similar to those in mice feet and primate hands, due to the descent of all these structures from a common mammalian ancestor. However, since all living organisms are related to some extent, even organs that appear to have little or no structural similarity, such as arthropod, squid and vertebrate eyes, or the limbs and wings of arthropods and vertebrates, can depend on a common set of homologous genes that control their assembly and function; this is called deep homology.\n\nDuring evolution, some structures may lose their original function and become vestigial structures. Such structures may have little or no function in a current species, yet have a clear function in ancestral species, or other closely related species. Examples include pseudogenes, the non-functional remains of eyes in blind cave-dwelling fish, wings in flightless birds, the presence of hip bones in whales and snakes, and sexual traits in organisms that reproduce via asexual reproduction. Examples of vestigial structures in humans include wisdom teeth, the coccyx, the vermiform appendix, and other behavioural vestiges such as goose bumps and primitive reflexes.\n\nHowever, many traits that appear to be simple adaptations are in fact exaptations: structures originally adapted for one function, but which coincidentally became somewhat useful for some other function in the process. One example is the African lizard \"Holaspis guentheri\", which developed an extremely flat head for hiding in crevices, as can be seen by looking at its near relatives. However, in this species, the head has become so flattened that it assists in gliding from tree to tree—an exaptation. Within cells, molecular machines such as the bacterial flagella and protein sorting machinery evolved by the recruitment of several pre-existing proteins that previously had different functions. Another example is the recruitment of enzymes from glycolysis and xenobiotic metabolism to serve as structural proteins called crystallins within the lenses of organisms' eyes.\n\nAn area of current investigation in evolutionary developmental biology is the developmental basis of adaptations and exaptations. This research addresses the origin and evolution of embryonic development and how modifications of development and developmental processes produce novel features. These studies have shown that evolution can alter development to produce new structures, such as embryonic bone structures that develop into the jaw in other animals instead forming part of the middle ear in mammals. It is also possible for structures that have been lost in evolution to reappear due to changes in developmental genes, such as a mutation in chickens causing embryos to grow teeth similar to those of crocodiles. It is now becoming clear that most alterations in the form of organisms are due to changes in a small set of conserved genes.\n\nInteractions between organisms can produce both conflict and cooperation. When the interaction is between pairs of species, such as a pathogen and a host, or a predator and its prey, these species can develop matched sets of adaptations. Here, the evolution of one species causes adaptations in a second species. These changes in the second species then, in turn, cause new adaptations in the first species. This cycle of selection and response is called coevolution. An example is the production of tetrodotoxin in the rough-skinned newt and the evolution of tetrodotoxin resistance in its predator, the common garter snake. In this predator-prey pair, an evolutionary arms race has produced high levels of toxin in the newt and correspondingly high levels of toxin resistance in the snake.\n\nNot all co-evolved interactions between species involve conflict. Many cases of mutually beneficial interactions have evolved. For instance, an extreme cooperation exists between plants and the mycorrhizal fungi that grow on their roots and aid the plant in absorbing nutrients from the soil. This is a reciprocal relationship as the plants provide the fungi with sugars from photosynthesis. Here, the fungi actually grow inside plant cells, allowing them to exchange nutrients with their hosts, while sending signals that suppress the plant immune system.\n\nCoalitions between organisms of the same species have also evolved. An extreme case is the eusociality found in social insects, such as bees, termites and ants, where sterile insects feed and guard the small number of organisms in a colony that are able to reproduce. On an even smaller scale, the somatic cells that make up the body of an animal limit their reproduction so they can maintain a stable organism, which then supports a small number of the animal's germ cells to produce offspring. Here, somatic cells respond to specific signals that instruct them whether to grow, remain as they are, or die. If cells ignore these signals and multiply inappropriately, their uncontrolled growth causes cancer.\n\nSuch cooperation within species may have evolved through the process of kin selection, which is where one organism acts to help raise a relative's offspring. This activity is selected for because if the \"helping\" individual contains alleles which promote the helping activity, it is likely that its kin will \"also\" contain these alleles and thus those alleles will be passed on. Other processes that may promote cooperation include group selection, where cooperation provides benefits to a group of organisms.\n\nSpeciation is the process where a species diverges into two or more descendant species.\n\nThere are multiple ways to define the concept of \"species.\" The choice of definition is dependent on the particularities of the species concerned. For example, some species concepts apply more readily toward sexually reproducing organisms while others lend themselves better toward asexual organisms. Despite the diversity of various species concepts, these various concepts can be placed into one of three broad philosophical approaches: interbreeding, ecological and phylogenetic. The Biological Species Concept (BSC) is a classic example of the interbreeding approach. Defined by Ernst Mayr in 1942, the BSC states that \"species are groups of actually or potentially interbreeding natural populations, which are reproductively isolated from other such groups.\" Despite its wide and long-term use, the BSC like others is not without controversy, for example because these concepts cannot be applied to prokaryotes, and this is called the species problem. Some researchers have attempted a unifying monistic definition of species, while others adopt a pluralistic approach and suggest that there may be different ways to logically interpret the definition of a species.\n\nBarriers to reproduction between two diverging sexual populations are required for the populations to become new species. Gene flow may slow this process by spreading the new genetic variants also to the other populations. Depending on how far two species have diverged since their most recent common ancestor, it may still be possible for them to produce offspring, as with horses and donkeys mating to produce mules. Such hybrids are generally infertile. In this case, closely related species may regularly interbreed, but hybrids will be selected against and the species will remain distinct. However, viable hybrids are occasionally formed and these new species can either have properties intermediate between their parent species, or possess a totally new phenotype. The importance of hybridisation in producing new species of animals is unclear, although cases have been seen in many types of animals, with the gray tree frog being a particularly well-studied example.\n\nSpeciation has been observed multiple times under both controlled laboratory conditions and in nature. In sexually reproducing organisms, speciation results from reproductive isolation followed by genealogical divergence. There are four primary geographic modes of speciation. The most common in animals is allopatric speciation, which occurs in populations initially isolated geographically, such as by habitat fragmentation or migration. Selection under these conditions can produce very rapid changes in the appearance and behaviour of organisms. As selection and drift act independently on populations isolated from the rest of their species, separation may eventually produce organisms that cannot interbreed.\n\nThe second mode of speciation is peripatric speciation, which occurs when small populations of organisms become isolated in a new environment. This differs from allopatric speciation in that the isolated populations are numerically much smaller than the parental population. Here, the founder effect causes rapid speciation after an increase in inbreeding increases selection on homozygotes, leading to rapid genetic change.\n\nThe third mode is parapatric speciation. This is similar to peripatric speciation in that a small population enters a new habitat, but differs in that there is no physical separation between these two populations. Instead, speciation results from the evolution of mechanisms that reduce gene flow between the two populations. Generally this occurs when there has been a drastic change in the environment within the parental species' habitat. One example is the grass \"Anthoxanthum odoratum\", which can undergo parapatric speciation in response to localised metal pollution from mines. Here, plants evolve that have resistance to high levels of metals in the soil. Selection against interbreeding with the metal-sensitive parental population produced a gradual change in the flowering time of the metal-resistant plants, which eventually produced complete reproductive isolation. Selection against hybrids between the two populations may cause reinforcement, which is the evolution of traits that promote mating within a species, as well as character displacement, which is when two species become more distinct in appearance.\n\nFinally, in sympatric speciation species diverge without geographic isolation or changes in habitat. This form is rare since even a small amount of gene flow may remove genetic differences between parts of a population. Generally, sympatric speciation in animals requires the evolution of both genetic differences and non-random mating, to allow reproductive isolation to evolve.\n\nOne type of sympatric speciation involves crossbreeding of two related species to produce a new hybrid species. This is not common in animals as animal hybrids are usually sterile. This is because during meiosis the homologous chromosomes from each parent are from different species and cannot successfully pair. However, it is more common in plants because plants often double their number of chromosomes, to form polyploids. This allows the chromosomes from each parental species to form matching pairs during meiosis, since each parent's chromosomes are represented by a pair already. An example of such a speciation event is when the plant species \"Arabidopsis thaliana\" and \"Arabidopsis arenosa\" crossbred to give the new species \"Arabidopsis suecica\". This happened about 20,000 years ago, and the speciation process has been repeated in the laboratory, which allows the study of the genetic mechanisms involved in this process. Indeed, chromosome doubling within a species may be a common cause of reproductive isolation, as half the doubled chromosomes will be unmatched when breeding with undoubled organisms.\n\nSpeciation events are important in the theory of punctuated equilibrium, which accounts for the pattern in the fossil record of short \"bursts\" of evolution interspersed with relatively long periods of stasis, where species remain relatively unchanged. In this theory, speciation and rapid evolution are linked, with natural selection and genetic drift acting most strongly on organisms undergoing speciation in novel habitats or small populations. As a result, the periods of stasis in the fossil record correspond to the parental population and the organisms undergoing speciation and rapid evolution are found in small populations or geographically restricted habitats and therefore rarely being preserved as fossils.\n\nExtinction is the disappearance of an entire species. Extinction is not an unusual event, as species regularly appear through speciation and disappear through extinction. Nearly all animal and plant species that have lived on Earth are now extinct, and extinction appears to be the ultimate fate of all species. These extinctions have happened continuously throughout the history of life, although the rate of extinction spikes in occasional mass extinction events. The Cretaceous–Paleogene extinction event, during which the non-avian dinosaurs became extinct, is the most well-known, but the earlier Permian–Triassic extinction event was even more severe, with approximately 96% of all marine species driven to extinction. The Holocene extinction event is an ongoing mass extinction associated with humanity's expansion across the globe over the past few thousand years. Present-day extinction rates are 100–1000 times greater than the background rate and up to 30% of current species may be extinct by the mid 21st century. Human activities are now the primary cause of the ongoing extinction event; global warming may further accelerate it in the future.\n\nThe role of extinction in evolution is not very well understood and may depend on which type of extinction is considered. The causes of the continuous \"low-level\" extinction events, which form the majority of extinctions, may be the result of competition between species for limited resources (the competitive exclusion principle). If one species can out-compete another, this could produce species selection, with the fitter species surviving and the other species being driven to extinction. The intermittent mass extinctions are also important, but instead of acting as a selective force, they drastically reduce diversity in a nonspecific manner and promote bursts of rapid evolution and speciation in survivors.\n\nThe Earth is about 4.54 billion years old. The earliest undisputed evidence of life on Earth dates from at least 3.5 billion years ago, during the Eoarchean Era after a geological crust started to solidify following the earlier molten Hadean Eon. Microbial mat fossils have been found in 3.48 billion-year-old sandstone in Western Australia. Other early physical evidence of a biogenic substance is graphite in 3.7 billion-year-old metasedimentary rocks discovered in Western Greenland as well as \"remains of biotic life\" found in 4.1 billion-year-old rocks in Western Australia. According to one of the researchers, \"If life arose relatively quickly on Earth … then it could be common in the universe.\" \n\nMore than 99 percent of all species, amounting to over five billion species, that ever lived on Earth are estimated to be extinct. Estimates on the number of Earth's current species range from 10 million to 14 million, of which about 1.9 million are estimated to have been named and 1.6 million documented in a central database to date, leaving at least 80 percent not yet described.\n\nHighly energetic chemistry is thought to have produced a self-replicating molecule around 4 billion years ago, and half a billion years later the last common ancestor of all life existed. The current scientific consensus is that the complex biochemistry that makes up life came from simpler chemical reactions. The beginning of life may have included self-replicating molecules such as RNA and the assembly of simple cells.\n\nAll organisms on Earth are descended from a common ancestor or ancestral gene pool. Current species are a stage in the process of evolution, with their diversity the product of a long series of speciation and extinction events. The common descent of organisms was first deduced from four simple facts about organisms: First, they have geographic distributions that cannot be explained by local adaptation. Second, the diversity of life is not a set of completely unique organisms, but organisms that share morphological similarities. Third, vestigial traits with no clear purpose resemble functional ancestral traits and finally, that organisms can be classified using these similarities into a hierarchy of nested groups—similar to a family tree. However, modern research has suggested that, due to horizontal gene transfer, this \"tree of life\" may be more complicated than a simple branching tree since some genes have spread independently between distantly related species.\n\nPast species have also left records of their evolutionary history. Fossils, along with the comparative anatomy of present-day organisms, constitute the morphological, or anatomical, record. By comparing the anatomies of both modern and extinct species, paleontologists can infer the lineages of those species. However, this approach is most successful for organisms that had hard body parts, such as shells, bones or teeth. Further, as prokaryotes such as bacteria and archaea share a limited set of common morphologies, their fossils do not provide information on their ancestry.\n\nMore recently, evidence for common descent has come from the study of biochemical similarities between organisms. For example, all living cells use the same basic set of nucleotides and amino acids. The development of molecular genetics has revealed the record of evolution left in organisms' genomes: dating when species diverged through the molecular clock produced by mutations. For example, these DNA sequence comparisons have revealed that humans and chimpanzees share 98% of their genomes and analysing the few areas where they differ helps shed light on when the common ancestor of these species existed.\n\nProkaryotes inhabited the Earth from approximately 3–4 billion years ago. No obvious changes in morphology or cellular organisation occurred in these organisms over the next few billion years. The eukaryotic cells emerged between 1.6–2.7 billion years ago. The next major change in cell structure came when bacteria were engulfed by eukaryotic cells, in a cooperative association called endosymbiosis. The engulfed bacteria and the host cell then underwent coevolution, with the bacteria evolving into either mitochondria or hydrogenosomes. Another engulfment of cyanobacterial-like organisms led to the formation of chloroplasts in algae and plants.\n\nThe history of life was that of the unicellular eukaryotes, prokaryotes and archaea until about 610 million years ago when multicellular organisms began to appear in the oceans in the Ediacaran period. The evolution of multicellularity occurred in multiple independent events, in organisms as diverse as sponges, brown algae, cyanobacteria, slime moulds and myxobacteria. In January 2016, scientists reported that, about 800 million years ago, a minor genetic change in a single molecule called GK-PID may have allowed organisms to go from a single cell organism to one of many cells.\n\nSoon after the emergence of these first multicellular organisms, a remarkable amount of biological diversity appeared over approximately 10 million years, in an event called the Cambrian explosion. Here, the majority of types of modern animals appeared in the fossil record, as well as unique lineages that subsequently became extinct. Various triggers for the Cambrian explosion have been proposed, including the accumulation of oxygen in the atmosphere from photosynthesis.\n\nAbout 500 million years ago, plants and fungi colonised the land and were soon followed by arthropods and other animals. Insects were particularly successful and even today make up the majority of animal species. Amphibians first appeared around 364 million years ago, followed by early amniotes and birds around 155 million years ago (both from \"reptile\"-like lineages), mammals around 129 million years ago, homininae around 10 million years ago and modern humans around 250,000 years ago. However, despite the evolution of these large animals, smaller organisms similar to the types that evolved early in this process continue to be highly successful and dominate the Earth, with the majority of both biomass and species being prokaryotes.\n\nConcepts and models used in evolutionary biology, such as natural selection, have many applications.\n\nArtificial selection is the intentional selection of traits in a population of organisms. This has been used for thousands of years in the domestication of plants and animals. More recently, such selection has become a vital part of genetic engineering, with selectable markers such as antibiotic resistance genes being used to manipulate DNA. Proteins with valuable properties have evolved by repeated rounds of mutation and selection (for example modified enzymes and new antibodies) in a process called directed evolution.\n\nUnderstanding the changes that have occurred during an organism's evolution can reveal the genes needed to construct parts of the body, genes which may be involved in human genetic disorders. For example, the Mexican tetra is an albino cavefish that lost its eyesight during evolution. Breeding together different populations of this blind fish produced some offspring with functional eyes, since different mutations had occurred in the isolated populations that had evolved in different caves. This helped identify genes required for vision and pigmentation.\n\nMany human diseases are not static phenomena, but capable of evolution. Viruses, bacteria, fungi and cancers evolve to be resistant to host immune defences, as well as pharmaceutical drugs. These same problems occur in agriculture with pesticide and herbicide resistance. It is possible that we are facing the end of the effective life of most of available antibiotics and predicting the evolution and evolvability of our pathogens and devising strategies to slow or circumvent it is requiring deeper knowledge of the complex forces driving evolution at the molecular level.\n\nIn computer science, simulations of evolution using evolutionary algorithms and artificial life started in the 1960s and were extended with simulation of artificial selection. Artificial evolution became a widely recognised optimisation method as a result of the work of Ingo Rechenberg in the 1960s. He used evolution strategies to solve complex engineering problems. Genetic algorithms in particular became popular through the writing of John Henry Holland. Practical applications also include automatic evolution of computer programmes. Evolutionary algorithms are now used to solve multi-dimensional problems more efficiently than software produced by human designers and also to optimise the design of systems.\n\nIn the 19th century, particularly after the publication of \"On the Origin of Species\" in 1859, the idea that life had evolved was an active source of academic debate centred on the philosophical, social and religious implications of evolution. Today, the modern evolutionary synthesis is accepted by a vast majority of scientists. However, evolution remains a contentious concept for some theists.\n\nWhile various religions and denominations have reconciled their beliefs with evolution through concepts such as theistic evolution, there are creationists who believe that evolution is contradicted by the creation myths found in their religions and who raise various objections to evolution. As had been demonstrated by responses to the publication of \"Vestiges of the Natural History of Creation\" in 1844, the most controversial aspect of evolutionary biology is the implication of human evolution that humans share common ancestry with apes and that the mental and moral faculties of humanity have the same types of natural causes as other inherited traits in animals. In some countries, notably the United States, these tensions between science and religion have fuelled the current creation–evolution controversy, a religious conflict focusing on politics and public education. While other scientific fields such as cosmology and Earth science also conflict with literal interpretations of many religious texts, evolutionary biology experiences significantly more opposition from religious literalists.\n\nThe teaching of evolution in American secondary school biology classes was uncommon in most of the first half of the 20th century. The Scopes Trial decision of 1925 caused the subject to become very rare in American secondary biology textbooks for a generation, but it was gradually re-introduced later and became legally protected with the 1968 \"Epperson v. Arkansas\" decision. Since then, the competing religious belief of creationism was legally disallowed in secondary school curricula in various decisions in the 1970s and 1980s, but it returned in pseudoscientific form as intelligent design (ID), to be excluded once again in the 2005 \"Kitzmiller v. Dover Area School District\" case.\n\n\nIntroductory reading\n\nAdvanced reading\n\n\n\n", "id": "9236", "title": "Evolution"}
{"url": "https://en.wikipedia.org/wiki?curid=53365898", "text": "Earliest known life forms\n\nThe earliest known life forms on Earth are putative fossilized microorganisms found in hydrothermal vent precipitates. The earliest time that life forms first appeared on Earth is unknown. They may have lived earlier than 3.77 billion years ago, possibly as early as 4.28 billion years ago, not long after the oceans formed 4.41 billion years ago, and not long after the formation of the Earth 4.54 billion years ago. The earliest \"direct\" evidence of life on Earth are fossils of microorganisms permineralized in 3.465-billion-year-old Australian Apex chert rocks.\n\nA life form, or lifeform, is an entity or being that is living.\nEarth remains the only place in the universe known to harbor life forms.\n\nMore than 99% of all species of life forms, amounting to over five billion species, that ever lived on Earth are estimated to be extinct.\n\nSome estimates on the number of Earth's current species of life forms range from 10 million to 14 million, of which about 1.2 million have been documented and over 86 percent have not yet been described. However, a May 2016 scientific report estimates that 1 trillion species are currently on Earth, with only one-thousandth of one percent described. The total number of DNA base pairs on Earth is estimated at 5.0 x 10 with a weight of 50 billion tonnes. In comparison, the total mass of the biosphere has been estimated to be as much as 4 TtC (trillion tons of carbon). In July 2016, scientists reported identifying a set of 355 genes from the Last Universal Common Ancestor (LUCA) of all organisms living on Earth.\n\nThe Earth's biosphere includes soil, hot springs, rock up to or deeper underground, the deepest parts of the ocean, and at least high into the atmosphere. Under certain test conditions, life forms have been observed to thrive in the near-weightlessness of space and to survive in the vacuum of outer space. Life forms appear to thrive in the Mariana Trench, the deepest spot in the Earth's oceans. Other researchers reported related studies that life forms thrive inside rocks up to below the sea floor under of ocean off the coast of the northwestern United States, as well as beneath the seabed off Japan. In August 2014, scientists confirmed the existence of life forms living below the ice of Antarctica. \n\nAccording to one researcher, \"You can find microbes everywhere — they're extremely adaptable to conditions, and survive wherever they are.\"\n\nFossil evidence informs most studies of the origin of life. The age of the Earth is about 4.54 billion years; the earliest undisputed evidence of life on Earth dates from at least 3.5 billion years ago.\n\nThere is evidence that life began much earlier.\n\nIn 2017, fossilized microorganisms, or microfossils, were announced to have been discovered in hydrothermal vent precipitates in the Nuvvuagittuq Belt of Quebec, Canada that may be as old as 4.28 billion years old, the oldest record of life on Earth, suggesting \"an almost instantaneous emergence of life\" after ocean formation 4.41 billion years ago, and not long after the formation of the Earth 4.54 billion years ago. \n\"Remains of life\" have been found in 4.1 billion-year-old rocks in Western Australia.\n\nEvidence of biogenic graphite, and possibly stromatolites, was discovered in 3.7 billion-year-old metasedimentary rocks in southwestern Greenland.\n\nIn May 2017, evidence of life on land may have been found in 3.48 billion-year-old geyserite which is often found around hot springs and geysers, and other related mineral deposits, uncovered in the Pilbara Craton of Western Australia. This complements the November 2013 publication that microbial mat fossils had been found in 3.48 billion-year-old sandstone in Western Australia.\n\nIn November 2017, a study by the University of Edinburgh suggested that life on Earth may have originated from biological particles carried by streams of space dust.\n\nA December 2017 report stated that 3.465-billion-year-old Australian Apex chert rocks once contained microorganisms, the earliest \"direct\" evidence of life on Earth.\n\nIn January 2018, a study found that 4.5 billion-year-old meteorites found on Earth contained liquid water along with prebiotic complex organic substances that may be ingredients for life.\n\nAccording to biologist Stephen Blair Hedges, \"If life arose relatively quickly on Earth … then it could be common in the universe.\"\n\n", "id": "53365898", "title": "Earliest known life forms"}
{"url": "https://en.wikipedia.org/wiki?curid=30038", "text": "Thomas Henry Huxley\n\nThomas Henry Huxley (; 4 May 1825 – 29 June 1895) was an English biologist specialising in comparative anatomy. He is known as \"Darwin's Bulldog\" for his advocacy of Charles Darwin's theory of evolution.\n\nHuxley's famous debate in 1860 with Samuel Wilberforce was a key moment in the wider acceptance of evolution and in his own career. Huxley had been planning to leave Oxford on the previous day, but, after an encounter with Robert Chambers, the author of \"Vestiges\", he changed his mind and decided to join the debate. Wilberforce was coached by Richard Owen, against whom Huxley also debated about whether humans were closely related to apes.\n\nHuxley was slow to accept some of Darwin's ideas, such as gradualism, and was undecided about natural selection, but despite this he was wholehearted in his public support of Darwin. Instrumental in developing scientific education in Britain, he fought against the more extreme versions of religious tradition.\n\nOriginally coining the term in 1869, Huxley elaborated on \"agnosticism\" in 1889 to frame the nature of claims in terms of what is knowable and what is not. Huxley statesAgnosticism, in fact, is not a creed, but a method, the essence of which lies in the rigorous application of a single principle... the fundamental axiom of modern science... In matters of the intellect, follow your reason as far as it will take you, without regard to any other consideration... In matters of the intellect, do not pretend that conclusions are certain which are not demonstrated or demonstrable. Use of that term has continued to the present day (see Thomas Henry Huxley and agnosticism).\n\nHuxley had little formal schooling and was virtually self-taught. He became perhaps the finest comparative anatomist of the later 19th century. He worked on invertebrates, clarifying relationships between groups previously little understood. Later, he worked on vertebrates, especially on the relationship between apes and humans. After comparing \"Archaeopteryx\" with \"Compsognathus\", he concluded that birds evolved from small carnivorous dinosaurs, a theory widely accepted today.\n\nThe tendency has been for this fine anatomical work to be overshadowed by his energetic and controversial activity in favour of evolution, and by his extensive public work on scientific education, both of which had significant effects on society in Britain and elsewhere.\n\nThomas Henry Huxley was born in Ealing, which was then a village in Middlesex. He was the second youngest of eight children of George Huxley and Rachel Withers. Like some other British scientists of the nineteenth century such as Alfred Russel Wallace, Huxley was brought up in a literate middle-class family which had fallen on hard times. His father was a mathematics teacher at Ealing School until it closed, putting the family into financial difficulties. As a result, Thomas left school at age 10, after only two years of formal schooling.\n\nDespite this unenviable start, Huxley was determined to educate himself. He became one of the great autodidacts of the nineteenth century. At first he read Thomas Carlyle, James Hutton's \"Geology\", and Hamilton's \"Logic\". In his teens he taught himself German, eventually becoming fluent and used by Charles Darwin as a translator of scientific material in German. He learned Latin, and enough Greek to read Aristotle in the original.\n\nLater on, as a young adult, he made himself an expert, first on invertebrates, and later on vertebrates, all self-taught. He was skilled in drawing and did many of the illustrations for his publications on marine invertebrates. In his later debates and writing on science and religion his grasp of theology was better than most of his clerical opponents. Huxley, a boy who left school at ten, became one of the most knowledgeable men in Britain.\n\nHe was apprenticed for short periods to several medical practitioners: at 13 to his brother-in-law John Cooke in Coventry, who passed him on to Thomas Chandler, notable for his experiments using mesmerism for medical purposes. Chandler's practice was in London's Rotherhithe amidst the squalor endured by the Dickensian poor. Here Thomas would have seen poverty, crime and rampant disease at its worst. Next, another brother-in-law took him on: John Salt, his eldest sister's husband. Now 16, Huxley entered Sydenham College (behind University College Hospital), a cut-price anatomy school whose founder, Marshall Hall, discovered the reflex arc. All this time Huxley continued his programme of reading, which more than made up for his lack of formal schooling.\n\nA year later, buoyed by excellent results and a silver medal prize in the Apothecaries' yearly competition, Huxley was admitted to study at Charing Cross Hospital, where he obtained a small scholarship. At Charing Cross, he was taught by Thomas Wharton Jones, Professor of Ophthalmic Medicine and Surgery at University College London. Jones had been Robert Knox's assistant when Knox bought cadavers from Burke and Hare. The young Wharton Jones, who acted as go-between, was exonerated of crime, but thought it best to leave Scotland. He was a fine teacher, up-to-date in physiology and also an ophthalmic surgeon. In 1845, under Wharton Jones' guidance, Huxley published his first scientific paper demonstrating the existence of a hitherto unrecognised layer in the inner sheath of hairs, a layer that has been known since as Huxley's layer. No doubt remembering this, and of course knowing his merit, later in life Huxley organised a pension for his old tutor.\n\nAt twenty he passed his First M.B. examination at the University of London, winning the gold medal for anatomy and physiology. However, he did not present himself for the final (Second M.B.) exams and consequently did not qualify with a university degree. His apprenticeships and exam results formed a sufficient basis for his application to the Royal Navy.\n\nAged 20, Huxley was too young to apply to the Royal College of Surgeons for a licence to practise, yet he was 'deep in debt'. So, at a friend's suggestion, he applied for an appointment in the Royal Navy. He had references on character and certificates showing the time spent on his apprenticeship and on requirements such as dissection and pharmacy. Sir William Burnett, the Physician General of the Navy, interviewed him and arranged for the College of Surgeons to test his competence (by means of a \"viva voce\").\n\nFinally Huxley was made Assistant Surgeon ('surgeon's mate') to HMS \"Rattlesnake\", about to start for a voyage of discovery and surveying to New Guinea and Australia. The \"Rattlesnake\" left England on 3 December 1846 and, once they had arrived in the southern hemisphere, Huxley devoted his time to the study of marine invertebrates. He began to send details of his discoveries back to England, where publication was arranged by Edward Forbes FRS (who had also been a pupil of Knox). Both before and after the voyage Forbes was something of a mentor to Huxley.\n\nHuxley's paper \"On the anatomy and the affinities of the family of Medusae\" was published in 1849 by the Royal Society in its \"Philosophical Transactions\". Huxley united the Hydroid and Sertularian polyps with the Medusae to form a class to which he subsequently gave the name of \"Hydrozoa\". The connection he made was that all the members of the class consisted of two cell layers, enclosing a central cavity or stomach. This is characteristic of the phylum now called the \"Cnidaria\". He compared this feature to the serous and mucous structures of embryos of higher animals. When at last he got a grant from the Royal Society for the printing of plates, Huxley was able to summarise this work in \"The Oceanic Hydrozoa\", published by the Ray Society in 1859.\n\nThe value of Huxley's work was recognised and, on returning to England in 1850, he was elected a Fellow of the Royal Society. In the following year, at the age of twenty-six, he not only received the Royal Society Medal but was also elected to the Council. He met Joseph Dalton Hooker and John Tyndall, who remained his lifelong friends. The Admiralty retained him as a nominal assistant-surgeon, so he might work on the specimens he collected and the observations he made during the voyage of the \"Rattlesnake\". He solved the problem of \"Appendicularia\", whose place in the animal kingdom Johannes Peter Müller had found himself wholly unable to assign. It and the Ascidians are both, as Huxley showed, tunicates, today regarded as a sister group to the vertebrates in the phylum \"Chordata\". Other papers on the morphology of the cephalopods and on brachiopods and rotifers are also noteworthy. The \"Rattlesnake\"'s official naturalist, John MacGillivray, did some work on botany, and proved surprisingly good at notating Australian aboriginal languages. He wrote up the voyage in the standard Victorian two volume format.\n\nHuxley effectively resigned from the navy (by refusing to return to active service) and, in July 1854, he became Professor of Natural History at the Royal School of Mines and naturalist to the British Geological Survey in the following year. In addition, he was Fullerian Professor at the Royal Institution 1855–58 and 1865–67; Hunterian Professor at the Royal College of Surgeons 1863–69; President of the British Association for the Advancement of Science 1869–1870; President of the Quekett Microscopical Club 1878; President of the Royal Society 1883–85; Inspector of Fisheries 1881–85; and President of the Marine Biological Association 1884–1890.\n\nThe thirty-one years during which Huxley occupied the chair of natural history at the Royal School of Mines included work on vertebrate palaeontology and on many projects to advance the place of science in British life. Huxley retired in 1885, after a bout of depressive illness which started in 1884. He resigned the presidency of the Royal Society in mid-term, the Inspectorship of Fisheries, and his chair (as soon as he decently could) and took six months' leave. His pension was a fairly handsome £1500 a year.\n\nIn 1890, he moved from London to Eastbourne where he edited the nine volumes of his \"Collected Essays\". In 1894 he heard of Eugene Dubois' discovery in Java of the remains of \"Pithecanthropus erectus\" (now known as \"Homo erectus\"). Finally, in 1895, he died of a heart attack (after contracting influenza and pneumonia), and was buried in North London at St Marylebone. This small family plot had been purchased upon the death of his beloved youngest son Noel, who died of scarlet fever in 1860; Huxley's wife Henrietta Anne née Heathorn and son Noel are also buried there. No invitations were sent out, but two hundred people turned up for the ceremony; they included Joseph Dalton Hooker, William Henry Flower, Mulford B. Foster, Edwin Lankester, Joseph Lister and, apparently, Henry James.\n\nHuxley and his wife had five daughters and three sons:\n\nNoel Huxley (1856–1860), died aged 4.\nJessie Oriana Huxley (1856–1927), married architect Fred Waller in 1877.\nMarian Huxley (1859–1887), married artist John Collier in 1879.\nLeonard Huxley (1860–1933), married Julia Arnold.\nRachel Huxley (1862–1934), married civil engineer Alfred Eckersley in 1884.\nHenrietta (Nettie) Huxley (1863–1940), married Harold Roller, travelled Europe as a singer.\nHenry Huxley (1865–1946), became a fashionable general practitioner in London.\nEthel Huxley (1866–1941) married artist John Collier (widower of sister) in 1889.\n\nFrom 1870 onwards, Huxley was to some extent drawn away from scientific research by the claims of public duty. He served on eight Royal Commissions, from 1862 to 1884. From 1871 to 1880 he was a Secretary of the Royal Society and from 1883 to 1885 he was president. He was president of the Geological Society from 1868 to 1870. In 1870, he was president of the British Association at Liverpool and, in the same year was elected a member of the newly constituted London School Board. He was president of the Quekett Microscopical Club from 1877 to 1879. He was the leading person amongst those who reformed the Royal Society, persuaded government about science, and established scientific education in British schools and universities. Before him, science was mostly a gentleman's occupation; after him, science was a profession.\n\nHe was awarded the highest honours then open to British men of science. The Royal Society, who had elected him as Fellow when he was 25 (1851), awarded him the Royal Medal the next year (1852), a year before Charles Darwin got the same award. He was the youngest biologist to receive such recognition. Then later in life came the Copley Medal in 1888 and the Darwin Medal in 1894; the Geological Society awarded him the Wollaston Medal in 1876; the Linnean Society awarded him the Linnean Medal in 1890. There were many other elections and appointments to eminent scientific bodies; these and his many academic awards are listed in the \"Life and Letters\". He turned down many other appointments, notably the Linacre chair in zoology at Oxford and the Mastership of University College, Oxford.\n\nIn 1873 the King of Sweden made Huxley, Hooker and Tyndall Knights of the Order of the Polar Star: they could wear the insignia but not use the title in Britain. Huxley collected many honorary memberships of foreign societies, academic awards and honorary doctorates from Britain and Germany. He also became foreign member of the Royal Netherlands Academy of Arts and Sciences in 1892.\n\nAs recognition of his many public services he was given a pension by the state, and was appointed Privy Councillor in 1892.\n\nDespite his many achievements he was given no award by the British state until late in life. In this he did better than Darwin, who got no award of any kind from the state. (Darwin's proposed knighthood was vetoed by ecclesiastical advisers, including Wilberforce) Perhaps Huxley had commented too often on his dislike of honours, or perhaps his many assaults on the traditional beliefs of organised religion made enemies in the establishment—he had vigorous debates in print with Benjamin Disraeli, William Ewart Gladstone and Arthur Balfour, and his relationship with Lord Salisbury was less than tranquil.\n\nHuxley was for about thirty years evolution's most effective advocate, and for some Huxley was \"\"the\" premier advocate of science in the nineteenth century [for] the whole English-speaking world\".\n\nThough he had many admirers and disciples, his retirement and later death left British zoology somewhat bereft of leadership. He had, directly or indirectly, guided the careers and appointments of the next generation, but none were of his stature. The loss of Francis Balfour in 1882, climbing the Alps just after he was appointed to a chair at Cambridge, was a tragedy. Huxley thought he was \"the only man who can carry out my work\": the deaths of Balfour and W. K. Clifford were \"the greatest losses to science in our time\".\n\nThe first half of Huxley's career as a palaeontologist is marked by a rather strange predilection for 'persistent types', in which he seemed to argue that evolutionary advancement (in the sense of major new groups of animals and plants) was rare or absent in the Phanerozoic. In the same vein, he tended to push the origin of major groups such as birds and mammals back into the Palaeozoic era, and to claim that no order of plants had ever gone extinct.\n\nMuch paper has been consumed by historians of science ruminating on this strange and somewhat unclear idea. Huxley was wrong to pitch the loss of orders in the Phanerozoic as low as 7%, and he did not estimate the number of new orders which evolved. Persistent types sat rather uncomfortably next to Darwin's more fluid ideas; despite his intelligence, it took Huxley a surprisingly long time to appreciate some of the implications of evolution. However, gradually Huxley moved away from this conservative style of thinking as his understanding of palaeontology, and the discipline itself, developed.\n\nHuxley's detailed anatomical work was, as always, first-rate and productive. His work on fossil fish shows his distinctive approach: whereas pre-Darwinian naturalists collected, identified and classified, Huxley worked mainly to reveal the evolutionary relationships between groups.\n\nThe lobed-finned fish (such as coelacanths and lung fish) have paired appendages whose internal skeleton is attached to the shoulder or pelvis by a single bone, the humerus or femur. His interest in these fish brought him close to the origin of tetrapods, one of the most important areas of vertebrate palaeontology.\n\nThe study of fossil reptiles led to his demonstrating the fundamental affinity of birds and reptiles, which he united under the title of \"Sauropsida\". His papers on \"Archaeopteryx\" and the origin of birds were of great interest then and still are.\n\nApart from his interest in persuading the world that man was a primate, and had descended from the same stock as the apes, Huxley did little work on mammals, with one exception. On his tour of America Huxley was shown the remarkable series of fossil horses, discovered by O. C. Marsh, in Yale's Peabody Museum. Marsh was part palaeontologist, part robber baron, a man who had hunted buffalo and met Red Cloud (in 1874). Funded by his uncle George Peabody, Marsh had made some remarkable discoveries:\nthe huge Cretaceous aquatic bird \"Hesperornis\", and the dinosaur footprints along the Connecticut River were worth the trip by themselves, but the horse fossils were really special.\n\nThe collection at that time went from the small four-toed forest-dwelling \"Orohippus\" from the Eocene through three-toed species such as \"Miohippus\" to species more like the modern horse. By looking at their teeth he could see that, as the size grew larger and the toes reduced, the teeth changed from those of a browser to those of a grazer. All such changes could be explained by a general alteration in habitat from forest to grassland. And, it is now known, that is what did happen over large areas of North America from the Eocene to the Pleistocene: the ultimate causative agent was global temperature reduction (see Paleocene–Eocene Thermal Maximum). The modern account of the evolution of the horse has many other members, and the overall appearance of the tree of descent is more like a bush than a straight line.\n\nThe horse series also strongly suggested that the process was gradual, and that the origin of the modern horse lay in North America, not in Eurasia. If so, then something must have happened to horses in North America, since none were there when Europeans arrived. The experience was enough for Huxley to give credence to Darwin's gradualism, and to introduce the story of the horse into his lecture series.\n\nHuxley was originally not persuaded of \"development theory\", as evolution was once called. This can be seen in his savage review of Robert Chambers' \"Vestiges of the Natural History of Creation\", a book which contained some quite pertinent arguments in favour of evolution. Huxley had also rejected Lamarck's theory of transmutation, on the basis that there was insufficient evidence to support it. All this scepticism was brought together in a lecture to the Royal Institution, which made Darwin anxious enough to set about an effort to change young Huxley's mind. It was the kind of thing Darwin did with his closest scientific friends, but he must have had some particular intuition about Huxley, who was from all accounts a most impressive person even as a young man.\n\nHuxley was therefore one of the small group who knew about Darwin's ideas before they were published (the group included Joseph Dalton Hooker and Charles Lyell). The first publication by Darwin of his ideas came when Wallace sent Darwin his famous paper on natural selection, which was presented by Lyell and Hooker to the Linnean Society in 1858 alongside excerpts from Darwin's notebook and a Darwin letter to Asa Gray. Huxley's famous response to the idea of natural selection was \"How extremely stupid not to have thought of that!\" However, he never conclusively made up his mind about whether natural selection was the main method for evolution, though he did admit it was a hypothesis which was a good working basis.\n\nLogically speaking, the prior question was whether evolution had taken place at all. It is to this question that much of Darwin's \"On the Origin of Species\" was devoted. Its publication in 1859 completely convinced Huxley of evolution and it was this and no doubt his admiration of Darwin's way of amassing and using evidence that formed the basis of his support for Darwin in the debates that followed the book's publication.\n\nHuxley's support started with his anonymous favourable review of the \"Origin\" in the \"Times\" for 26 December 1859, and continued with articles in several periodicals, and in a lecture at the Royal Institution in February 1860. At the same time, Richard Owen, whilst writing an extremely hostile anonymous review of the \"Origin\" in the \"Edinburgh Review\", also primed Samuel Wilberforce who wrote one in the \"Quarterly Review\", running to 17,000 words. The authorship of this latter review was not known for sure until Wilberforce's son wrote his biography. So it can be said that, just as Darwin groomed Huxley, so Owen groomed Wilberforce; and both the proxies fought public battles on behalf of their principals as much as themselves. Though we do not know the exact words of the Oxford debate, we do know what Huxley thought of the review in the \"Quarterly\":\n\nSince Lord Brougham assailed Dr Young, the world has seen no such specimen of the insolence of a shallow pretender to a Master in Science as this remarkable production, in which one of the most exact of observers, most cautious of reasoners, and most candid of expositors, of this or any other age, is held up to scorn as a \"flighty\" person, who endeavours \"to prop up his utterly rotten fabric of guess and speculation,\" and whose \"mode of dealing with nature\" is reprobated as \"utterly dishonourable to Natural Science.\"\n\nIf I confine my retrospect of the reception of the \"Origin of Species\" to a twelvemonth, or thereabouts, from the time of its publication, I do not recollect anything quite so foolish and unmannerly as the \"Quarterly Review\" article...\n\nHuxley said \"I am Darwin's bulldog\". While the second half of Darwin's life was lived mainly within his family, the younger combative Huxley operated mainly out in the world at large. A letter from Huxley to Ernst Haeckel (2 November 1871) states: \"The dogs have been snapping at [Darwin's] heels too much of late.\" At Oxford and Cambridge Universities, \"Bulldog\" was and still is student slang for a university policeman, whose job was to corral errant students and maintain their moral rectitude.\n\nFamously, Huxley responded to Wilberforce in the debate at the British Association meeting, on Saturday 30 June 1860 at the Oxford University Museum. Huxley's presence there had been encouraged on the previous evening when he met Robert Chambers, the Scottish publisher and author of \"Vestiges\", who was walking the streets of Oxford in a dispirited state, and begged for assistance. The debate followed the presentation of a paper by John William Draper, and was chaired by Darwins's former botany tutor John Stevens Henslow. Darwin's theory was opposed by the Lord Bishop of Oxford, Samuel Wilberforce, and those supporting Darwin included Huxley and their mutual friends Hooker and Lubbock. The platform featured Brodie and Professor Beale, and Robert FitzRoy, who had been captain of HMS \"Beagle\" during Darwin's voyage, spoke against Darwin.\n\nWilberforce had a track record against evolution as far back as the previous Oxford B.A. meeting in 1847 when he attacked Chambers' \"Vestiges\". For the more challenging task of opposing the \"Origin\", and the implication that man descended from apes, he had been assiduously coached by Richard Owen – Owen stayed with him the night before the debate. On the day Wilberforce repeated some of the arguments from his \"Quarterly Review\" article (written but not yet published), then ventured onto slippery ground. His famous jibe at Huxley (as to whether Huxley was descended from an ape on his mother's side or his father's side) was probably unplanned, and certainly unwise. Huxley's reply to the effect that he would rather be descended from an ape than a man who misused his great talents to suppress debate—the exact wording is not certain—was widely recounted in pamphlets and a spoof play.\n\nThe letters of Alfred Newton include one to his brother giving an eye-witness account of the debate, and written less than a month afterwards. Other eyewitnesses, with one or two exceptions (Hooker especially thought \"he\" had made the best points), give similar accounts, at varying dates after the event. The general view was and still is that Huxley got much the better of the exchange though Wilberforce himself thought he had done quite well. In the absence of a verbatim report differing perceptions are difficult to judge fairly; Huxley wrote a detailed account for Darwin, a letter which does not survive; however, a letter to his friend Frederick Daniel Dyster does survive with an account just three months after the event.\n\nOne effect of the debate was to increase hugely Huxley's visibility amongst educated people, through the accounts in newspapers and periodicals. Another consequence was to alert him to the importance of public debate: a lesson he never forgot. A third effect was to serve notice that Darwinian ideas could not be easily dismissed: on the contrary, they would be vigorously defended against orthodox authority. A fourth effect was to promote professionalism in science, with its implied need for scientific education. A fifth consequence was indirect: as Wilberforce had feared, a defence of evolution did undermine literal belief in the Old Testament, especially the Book of Genesis. Many of the liberal clergy at the meeting were quite pleased with the outcome of the debate; they were supporters, perhaps, of the controversial \"Essays and Reviews\". Thus both on the side of science, and on the side of religion, the debate was important, and its outcome significant. (see also below)\n\nThat Huxley and Wilberforce remained on courteous terms after the debate (and able to work together on projects such as the Metropolitan Board of Education) says something about both men, whereas Huxley and Owen were never reconciled.\n\nFor nearly a decade his work was directed mainly to the relationship of man to the apes. This led him directly into a clash with Richard Owen, a man widely disliked for his behaviour whilst also being admired for his capability. The struggle was to culminate in some severe defeats for Owen. Huxley's Croonian Lecture, delivered before the Royal Society in 1858 on \"The Theory of the Vertebrate Skull\" was the start. In this, he rejected Owen's theory that the bones of the skull and the spine were homologous, an opinion previously held by Goethe and Lorenz Oken.\n\nFrom 1860–63 Huxley developed his ideas, presenting them in lectures to working men, students and the general public, followed by publication. Also in 1862 a series of talks to working men was printed lecture by lecture as pamphlets, later bound up as a little green book; the first copies went on sale in December. Other lectures grew into Huxley's most famous work \"Evidence as to Man's place in Nature\" (1863) where he addressed the key issues long before Charles Darwin published his \"Descent of Man\" in 1871.\n\nAlthough Darwin did not publish his \"Descent of Man\" until 1871, the general debate on this topic had started years before (there was even a precursor debate in the 18th century between Monboddo and Buffon). Darwin had dropped a hint when, in the conclusion to the \"Origin\", he wrote: \"In the distant future... light will be thrown on the origin of man and his history\". Not so distant, as it turned out. A key event had already occurred in 1857 when Richard Owen presented (to the Linnean Society) his theory that man was marked off from all other mammals by possessing features of the brain peculiar to the genus \"Homo\". Having reached this opinion, Owen separated man from all other mammals in a subclass of its own. No other biologist held such an extreme view. Darwin reacted \"Man...as distinct from a chimpanzee [as] an ape from a platypus... I cannot swallow that!\" Neither could Huxley, who was able to demonstrate that Owen's idea was completely wrong.\nThe subject was raised at the 1860 BA Oxford meeting, when Huxley flatly contradicted Owen, and promised a later demonstration of the facts. In fact, a number of demonstrations were held in London and the provinces. In 1862 at the Cambridge meeting of the B.A. Huxley's friend William Flower gave a public dissection to show that the same structures (the posterior horn of the lateral ventricle and hippocampus minor) were indeed present in apes. The debate was widely publicised, and parodied as the \"Great Hippocampus Question\". It was seen as one of Owen's greatest blunders, revealing Huxley as not only dangerous in debate, but also a better anatomist.\n\nOwen conceded that there was something that could be called a hippocampus minor in the apes, but stated that it was much less developed and that such a presence did not detract from the overall distinction of simple brain size.\n\nHuxley's ideas on this topic were summed up in January 1861 in the first issue (new series) of his own journal, the \"Natural History Review\": \"the most violent scientific paper he had ever composed\". This paper was reprinted in 1863 as chapter 2 of \"Man's Place in Nature\", with an addendum giving his account of the Owen/Huxley controversy about the ape brain. In his \"Collected Essays\" this addendum was removed.\n\nThe extended argument on the ape brain, partly in debate and partly in print, backed by dissections and demonstrations, was a landmark in Huxley's career. It was highly important in asserting his dominance of comparative anatomy, and in the long run more influential in establishing evolution amongst biologists than was the debate with Wilberforce. It also marked the start of Owen's decline in the esteem of his fellow biologists.\n\nThe following was written by Huxley to Rolleston before the BA meeting in 1861:\n\nDuring those years there was also work on human fossil anatomy and anthropology. In 1862 he examined the Neanderthal skull-cap, which had been discovered in 1857. It was the first pre-\"sapiens\" discovery of a fossil man, and it was immediately clear to him that the brain case was surprisingly large.\n\nPerhaps less productive was his work on physical anthropology, a topic which fascinated the Victorians. Huxley classified the human races into nine categories, and discussed them under four headings as: Australoid, Negroid, Xanthocroic and Mongoloid types. Such classifications depended mainly on appearance and anatomical characteristics.\n\nHuxley was certainly not slavish in his dealings with Darwin. As shown in every biography, they had quite different and rather complementary characters. Important also, Darwin was a field naturalist, but Huxley was an anatomist, so there was a difference in their experience of nature. Lastly, Darwin's views on science were different from Huxley's views. For Darwin, natural selection was the best way to explain evolution because it explained a huge range of natural history facts and observations: it solved problems. Huxley, on the other hand, was an empiricist who trusted what he could see, and some things are not easily seen. With this in mind, one can appreciate the debate between them, Darwin writing his letters, Huxley never going quite so far as to say he thought Darwin was right.\n\nHuxley's reservations on natural selection were of the type \"until selection and breeding can be seen to give rise to varieties which are infertile with each other, natural selection cannot be proved\". Huxley's position on selection was agnostic; yet he gave no credence to any other theory. Despite this concern about evidence, Huxley saw that if evolution came about through variation, reproduction and selection then other things would also be subject to the same pressures. This included ideas because they are invented, imitated and selected by humans: ‘The struggle for existence holds as much in the intellectual as in the physical world. A theory is a species of thinking, and its right to exist is coextensive with its power of resisting extinction by its rivals.’ This is the same idea as meme theory put forward by Richard Dawkins in 1976.\n\nDarwin's part in the discussion came mostly in letters, as was his wont, along the lines: \"The empirical evidence you call for is both impossible in practical terms, and in any event unnecessary. It's the same as asking to see every step in the transformation (or the splitting) of one species into another. My way so many issues are clarified and problems solved; no other theory does nearly so well\".\n\nHuxley's reservation, as Helena Cronin has so aptly remarked, was contagious: \"it spread itself for years among all kinds of doubters of Darwinism\". One reason for this doubt was that comparative anatomy could address the question of descent, \"but not the question of mechanism\".\n\nHuxley was a pallbearer at the funeral of Charles Darwin on 26 April 1882.\n\nIn November 1864, Huxley succeeded in launching a dining club, the X Club, composed of like-minded people working to advance the cause of science; not surprisingly, the club consisted of most of his closest friends. There were nine members, who decided at their first meeting that there should be no more. The members were: Huxley, John Tyndall, J. D. Hooker, John Lubbock (banker, biologist and neighbour of Darwin), Herbert Spencer (social philosopher and sub-editor of the Economist), William Spottiswoode (mathematician and the Queen's Printer), Thomas Hirst (Professor of Physics at University College London), Edward Frankland (the new Professor of Chemistry at the Royal Institution) and George Busk, zoologist and palaeontologist (formerly surgeon for HMS \"Dreadnought\"). All except Spencer were Fellows of the Royal Society. Tyndall was a particularly close friend; for many years they met regularly and discussed issues of the day. On more than one occasion Huxley joined Tyndall in the latter's trips into the Alps and helped with his investigations in glaciology.\n\nThere were also some quite significant X-Club satellites such as William Flower and George Rolleston, (Huxley protegés), and liberal clergyman Arthur Stanley, the Dean of Westminster. Guests such as Charles Darwin and Hermann von Helmholtz were entertained from time to time.\n\nThey would dine early on first Thursdays at a hotel, planning what to do; high on the agenda was to change the way the Royal Society Council did business. It was no coincidence that the Council met later that same evening. First item for the Xs was to get the Copley Medal for Darwin, which they managed after quite a struggle.\n\nThe next step was to acquire a journal to spread their ideas. This was the weekly \"Reader\", which they bought, revamped and redirected. Huxley had already become part-owner of the \"Natural History Review\" bolstered by the support of Lubbock, Rolleston, Busk and Carpenter (X-clubbers and satellites). The journal was switched to pro-Darwinian lines and relaunched in January 1861. After a stream of good articles the \"NHR\" failed after four years; but it had helped at a critical time for the establishment of evolution. The \"Reader\" also failed, despite its broader appeal which included art and literature as well as science. The periodical market was quite crowded at the time, but most probably the critical factor was Huxley's time; he was simply over-committed, and could not afford to hire full-time editors. This occurred often in his life: Huxley took on too many ventures, and was not so astute as Darwin at getting others to do work for him.\n\nHowever, the experience gained with the \"Reader\" was put to good use when the X Club put their weight behind the founding of \"Nature\" in 1869. This time no mistakes were made: above all there was a permanent editor (though not full-time), Norman Lockyer, who served until 1919, a year before his death. In 1925, to celebrate his centenary, \"Nature\" issued a supplement devoted to Huxley.\n\nThe peak of the X Club's influence was from 1873 to 1885 as Hooker, Spottiswoode and Huxley were Presidents of the Royal Society in succession. Spencer resigned in 1889 after a dispute with Huxley over state support for science. After 1892 it was just an excuse for the surviving members to meet. Hooker died in 1911, and Lubbock (now Lord Avebury) was the last surviving member.\n\nHuxley was also an active member of the Metaphysical Society, which ran from 1869 to 1880. It was formed around a nucleus of clergy and expanded to include all kinds of opinions. Tyndall and Huxley later joined The Club (founded by Dr. Johnson) when they could be sure that Owen would not turn up.\n\nWhen Huxley himself was young there were virtually no degrees in British universities in the biological sciences and few courses. Most biologists of his day were either self-taught, or took medical degrees. When he retired there were established chairs in biological disciplines in most universities, and a broad consensus on the curricula to be followed. Huxley was the single most influential person in this transformation.\n\nIn the early 1870s the Royal School of Mines moved to new quarters in South Kensington; ultimately it would become one of the constituent parts of Imperial College London. The move gave Huxley the chance to give more prominence to laboratory work in biology teaching, an idea suggested by practice in German universities. In the main, the method was based on the use of carefully chosen types, and depended on the dissection of anatomy, supplemented by microscopy, museum specimens and some elementary physiology at the hands of Foster.\n\nThe typical day would start with Huxley lecturing at 9am, followed by a program of laboratory work supervised by his demonstrators. Huxley's demonstrators were picked men—all became leaders of biology in Britain in later life, spreading Huxley's ideas as well as their own. Michael Foster became Professor of Physiology at Cambridge; E. Ray Lankester became Jodrell Professor of Zoology at University College London (1875–91), Professor of Comparative Anatomy at Oxford (1891–98) and Director of the Natural History Museum (1898–1907); S.H. Vines became Professor of Botany at Cambridge; W.T. Thiselton-Dyer became Hooker's successor at Kew (he was already Hooker's son-in-law!); T. Jeffery Parker became Professor of Zoology and Comparative Anatomy at University College, Cardiff; and William Rutherford became the Professor of Physiology at Edinburgh. William Flower, Conservator to the Hunterian Museum, and THH's assistant in many dissections, became Sir William Flower, Hunterian Professor of Comparative Anatomy and, later, Director of the Natural History Museum. It's a remarkable list of disciples, especially when contrasted with Owen who, in a longer professional life than Huxley, left no disciples at all. \"No one fact tells so strongly against Owen... as that he has never reared one pupil or follower\".\n\nHuxley's courses for students were so much narrower than the man himself that many were bewildered by the contrast: \"The teaching of zoology by use of selected animal types has come in for much criticism\"; Looking back in 1914 to his time as a student, Sir Arthur Shipley said \"Darwin's later works all dealt with living organisms, yet our obsession was with the dead, with bodies preserved, and cut into the most refined slices\". E.W MacBride said \"Huxley... would persist in looking at animals as material structures and not as living, active beings; in a word... he was a necrologist. To put it simply, Huxley preferred to teach what he had actually seen with his own eyes.\n\nThis largely morphological program of comparative anatomy remained at the core of most biological education for a hundred years until the advent of cell and molecular biology and interest in evolutionary ecology forced a fundamental rethink. It is an interesting fact that the methods of the field naturalists who led the way in developing the theory of evolution (Darwin, Wallace, Fritz Müller, Henry Bates) were scarcely represented at all in Huxley's program. Ecological investigation of life in its environment was virtually non-existent, and theory, evolutionary or otherwise, was at a discount. Michael Ruse finds no mention of evolution or Darwinism in any of the exams set by Huxley, and confirms the lecture content based on two complete sets of lecture notes.\n\nSince Darwin, Wallace and Bates did not hold teaching posts at any stage of their adult careers (and Műller never returned from Brazil) the imbalance in Huxley's program went uncorrected. It is surely strange that Huxley's courses did not contain an account of the evidence collected by those naturalists of life in the tropics; evidence which they had found so convincing, and which caused their views on evolution by natural selection to be so similar. Desmond suggests that \"[biology] had to be simple, synthetic and assimilable [because] it was to train teachers and had no other heuristic function\". That must be part of the reason; indeed it does help to explain the stultifying nature of much school biology. But zoology as taught at all levels became far too much the product of one man.\n\nHuxley was comfortable with comparative anatomy, at which he was the greatest master of the day. He was not an all-round naturalist like Darwin, who had shown clearly enough how to weave together detailed factual information and subtle arguments across the vast web of life. Huxley chose, in his teaching (and to some extent in his research) to take a more straightforward course, concentrating on his personal strengths.\n\nHuxley was also a major influence in the direction taken by British schools: in November 1870 he was voted onto the London School Board. In primary schooling, he advocated a wide range of disciplines, similar to what is taught today: reading, writing, arithmetic, art, science, music, etc. In secondary education he recommended two years of basic liberal studies followed by two years of some upper-division work, focusing on a more specific area of study. A practical example of the latter is his famous 1868 lecture \"On a Piece of Chalk\" which was first published as an essay in \"Macmillan's Magazine\" in London later that year. The piece reconstructs the geological history of Britain from a simple piece of chalk and demonstrates science as \"organized common sense\".\n\nHuxley supported the reading of the Bible in schools. This may seem out of step with his agnostic convictions, but he believed that the Bible's significant moral teachings and superb use of language were relevant to English life. \"I do not advocate burning your ship to get rid of the cockroaches\".\nHowever, what Huxley proposed was to create an \"edited version\" of the Bible, shorn of \"shortcomings and errors... statements to which men of science absolutely and entirely demur... These tender children [should] not be taught that which you do not yourselves believe\". The Board voted against his idea, but it also voted against the idea that public money should be used to support students attending church schools. Vigorous debate took place on such points, and the debates were minuted in detail. Huxley said \"I will never be a party to enabling the State to sweep the children of this country into denominational schools\". The Act of Parliament which founded board schools permitted the reading of the Bible, but did not permit any denominational doctrine to be taught.\n\nIt may be right to see Huxley's life and work as contributing to the secularisation of British society which gradually occurred over the following century. Ernst Mayr said \"It can hardly be doubted that [biology] has helped to undermine traditional beliefs and value systems\"  — and Huxley more than anyone else was responsible for this trend in Britain. Some modern Christian apologists consider Huxley the father of antitheism, though he himself maintained that he was an agnostic, not an atheist. He was, however, a lifelong and determined opponent of almost all organised religion throughout his life, especially the \"Roman Church... carefully calculated for the destruction of all that is highest in the moral nature, in the intellectual freedom, and in the political freedom of mankind\". In the same line of thought, in an article in \"Popular Science\", Huxley used the expression \"the so-called Christianity of Catholicism,\" explaining: \"I say 'so-called' not by way of offense, but as a protest against the monstruous assumption that Catholic Christianity is explicitly or implictly contained in any trust-worthy record of the teaching of Jesus of Nazareth.\"\n\nVladimir Lenin remarked (in \"Materialism and empirio-criticism\") \"In Huxley's case... agnosticism serves as a fig-leaf for materialism\" (see also the Debate with Wilberforce above).\n\nHuxley's interest in education went still further than school and university classrooms; he made a great effort to reach interested adults of all kinds: after all, he himself was largely self-educated. There were his lecture courses for working men, many of which were published afterwards, and there was the use he made of journalism, partly to earn money but mostly to reach out to the literate public. For most of his adult life he wrote for periodicals—the \"Westminster Review\", the \"Saturday Review\", the \"Reader\", the \"Pall Mall Gazette\", \"Macmillan's Magazine\", the \"Contemporary Review\". Germany was still ahead in formal science education, but interested people in Victorian Britain could use their initiative and find out what was going on by reading periodicals and using the lending libraries.\n\nIn 1868 Huxley became Principal of the South London Working Men's College in Blackfriars Road. The moving spirit was a portmanteau worker, Wm. Rossiter, who did most of the work; the funds were put up mainly by F.D. Maurice's Christian Socialists. At sixpence for a course and a penny for a lecture by Huxley, this was some bargain; and so was the free library organised by the college, an idea which was widely copied. Huxley thought, and said, that the men who attended were as good as any country squire.\n\nThe technique of printing his more popular lectures in periodicals which were sold to the general public was extremely effective. A good example was \"The physical basis of life\", a lecture given in Edinburgh on 8 November 1868. Its theme — that vital action is nothing more than \"the result of the molecular forces of the protoplasm which displays it\" — shocked the audience, though that was nothing compared to the uproar when it was published in the \"Fortnightly Review\" for February 1869. John Morley, the editor, said \"No article that had appeared in any periodical for a generation had caused such a sensation\". The issue was reprinted seven times and protoplasm became a household word; \"Punch\" added 'Professor Protoplasm' to his other soubriquets.\n\nThe topic had been stimulated by Huxley seeing the cytoplasmic streaming in plant cells, which is indeed a sensational sight. For these audiences Huxley's claim that this activity should not be explained by words such as vitality, but by the working of its constituent chemicals, was surprising and shocking. Today we would perhaps emphasise the extraordinary structural arrangement of those chemicals as the key to understanding what cells do, but little of that was known in the nineteenth century.\n\nWhen the Archbishop of York thought this 'new philosophy' was based on Auguste Comte's positivism, Huxley corrected him: \"Comte's philosophy [is just] Catholicism minus Christianity\" (Huxley 1893 vol 1 of Collected Essays \"Methods & Results\" 156). A later version was \"[positivism is] sheer Popery with M. Comte in the chair of St Peter, and with the names of the saints changed\". (lecture on \"The scientific aspects of positivism\" Huxley 1870 \"Lay Sermons, Addresses and Reviews\" p149). Huxley's dismissal of positivism damaged it so severely that Comte's ideas withered in Britain.\n\nDuring his life, and especially in the last ten years after retirement, Huxley wrote on many issues relating to the humanities.\n\nPerhaps the best known of these topics is \"Evolution and Ethics\", which deals with the question of whether biology has anything particular to say about moral philosophy. Both Huxley and his grandson Julian Huxley gave Romanes Lectures on this theme. For a start, Huxley dismisses religion as a source of moral authority. Next, he believes the mental characteristics of man are as much a product of evolution as the physical aspects. Thus, our emotions, our intellect, our tendency to prefer living in groups and spend resources on raising our young are part and parcel of our evolution, and therefore inherited.\n\nDespite this, the \"details\" of our values and ethics are not inherited: they are partly determined by our culture, and partly chosen by ourselves. Morality and duty are often at war with natural instincts; ethics cannot be derived from the \"struggle for existence\": \"Of moral purpose I see not a trace in nature. That is an article of exclusively human manufacture.\" It is therefore our responsibility to make ethical choices (see Ethics and Evolutionary ethics). This seems to put Huxley as a compatibilist in the Free Will vs Determinism debate. In this argument Huxley is diametrically opposed to his old friend Herbert Spencer.\n\nHuxley's dissection of Rousseau's views on man and society is another example of his later work. The essay undermines Rousseau's ideas on man as a preliminary to undermining his ideas on the ownership of property. Characteristic is: \"The doctrine that all men are, in any sense, or have been, at any time, free and equal, is an utterly baseless fiction.\"\n\nHuxley's method of argumentation (his strategy and tactics of persuasion in speech and print) is itself much studied. His career included controversial debates with scientists, clerics and politicians; persuasive discussions with Royal Commissions and other public bodies; lectures and articles for the general public, and a mass of detailed letter-writing to friends and other correspondents. A large number of textbooks have excerpted his prose for anthologies.\n\nHuxley worked on ten Royal and other commissions (titles somewhat shortened here). The Royal Commission is the senior investigative forum in the British constitution. A rough analysis shows that five commissions involved science and scientific education; three involved medicine and three involved fisheries. Several involve difficult ethical and legal issues. All deal with possible changes to law and/or administrative practice.\n\n\n\nIn 1855, he married Henrietta Anne Heathorn (1825–1915), an English émigrée whom he had met in Sydney. They kept correspondence until he was able to send for her. They had five daughters and three sons:\n\n\nHuxley's relationships with his relatives and children were genial by the standards of the day—so long as they lived their lives in an honourable manner, which some did not. After his mother, his eldest sister Lizzie was the most important person in his life until his own marriage. He remained on good terms with his children, more than can be said of many Victorian fathers. This excerpt from a letter to Jessie, his eldest daughter is full of affection:\n\nHuxley's descendants include children of Leonard Huxley:\n\n\nOther significant descendants of Huxley, such as Sir Crispin Tickell, are treated in the Huxley family.\n\nBiographers have sometimes noted the occurrence of mental illness in the Huxley family. His father became \"sunk in worse than childish imbecility of mind\", and later died in Barming Asylum; brother George suffered from \"extreme mental anxiety\" and died in 1863 leaving serious debts. Brother James, a well known psychiatrist and Superintendent of Kent County Asylum, was at 55 \"as near mad as any sane man can be\"; and there is more. His favourite daughter, the artistically talented Mady (Marian), who became the first wife of artist John Collier, was troubled by mental illness for years. She died of pneumonia in her mid-twenties.\n\nAbout Huxley himself we have a more complete record. As a young apprentice to a medical practitioner, aged thirteen or fourteen, Huxley was taken to watch a post-mortem dissection. Afterwards he sank into a 'deep lethargy' and though Huxley ascribed this to dissection poisoning, Bibby and others may be right to suspect that emotional shock precipitated the depression. Huxley recuperated on a farm, looking thin and ill.\n\nThe next episode we know of in Huxley's life when he suffered a debilitating depression was on the third voyage of HMS \"Rattlesnake\" in 1848. Huxley had further periods of depression at the end of 1871, and again in 1873. Finally, in 1884 he sank into another depression, and this time it precipitated his decision to retire in 1885, at the age of only 60. This is enough to indicate the way depression (or perhaps a moderate bi-polar disorder) interfered with his life, yet unlike some of the other family members, he was able to function extremely well at other times.\n\nThe problems continued sporadically into the third generation. Two of Leonard's sons suffered serious depression: Trevennen committed suicide in 1914 and Julian suffered a breakdown in 1913, and five more later in life.\n\nDarwin's ideas and Huxley's controversies gave rise to many cartoons and satires. It was the debate about man's place in nature that roused such widespread comment: cartoons are so numerous as to be almost impossible to count; Darwin's head on a monkey's body is one of the visual clichés of the age. The \"Great Hippocampus Question\" attracted particular attention:\n\n\n\n\n\n\n \n", "id": "30038", "title": "Thomas Henry Huxley"}
{"url": "https://en.wikipedia.org/wiki?curid=1574457", "text": "BioBlitz\n\nA BioBlitz, also written without capitals as bioblitz, is an intense period of biological surveying in an attempt to record all the living species within a designated area. Groups of scientists, naturalists and volunteers conduct an intensive field study over a continuous time period (e.g., usually 24 hours). There is a public component to many BioBlitzes, with the goal of getting the public interested in biodiversity. To encourage more public participation, these BioBlitzes are often held in urban parks or nature reserves close to cities.\n\nA BioBlitz has different opportunities and benefits than a traditional, scientific field study. Some of these potential benefits include:\n\nThe term \"BioBlitz\" was first coined by U.S. National Park Service naturalist Susan Rudy while assisting with the first BioBlitz. The first BioBlitz was held at Kenilworth Aquatic Gardens, Washington D.C. in 1996. Approximately 1000 species were identified at this first event. This first accounting of biodiversity was organized by Sam Droege (USGS) and Dan Roddy (NPS) with the assistance of other government scientists. The public and especially the news media were invited. Since the success of the first bioblitz, many organisations around the world have repeated this concept.\n\nSince then, most BioBlitz contain a public component so that adults, kids, teens and anyone interested can join experts and scientists in the field. Participating in these hands-on field studies is a fun and exciting way for people to learn about biodiversity and better understand how to protect it.\n\nIn 1998, Harvard biologist E.O. Wilson and Massachusetts wildlife expert Peter Alden developed a program to catalog the organisms around Walden Pond. This led to a statewide program known as Biodiversity Days. This concept is very similar to a BioBlitz and occasionally the two terms are used interchangeably.\n\nA variation on the BioBlitz, the Blogger Blitz began in 2007. Rather than gather volunteers and scientists at one location, participant blogs pledged to conduct individual surveys of biodiveristy. These results were then compiled and mapped. The purpose of this blitz is not to survey down to species level across all taxonomic groups, but rather to raise awareness about biodiversity and provide a general snapshot of diversity.\n\nFrom 2006 through 2016 National Geographic Society and the US National Park Service partnered to put on a Bioblitz in a different National Park each year culminating in a Bioblitz across the National Park Service in 2016 as part of the National Park Service Centennial Celebration. The iNaturalist platform was used as the recording tool for the 2014, 2015, and 2016 Centennial Bioblitzes in this series. \"The National Park Service and National Geographic hosted the first ever nationwide National Parks BioBlitz with scientists, park managers, and the public serving as citizen scientists making more than 60,000 observations on the mobile app iNaturalist. This endeavor has currently documented nearly 7,000 species, including species that are new to parks’ species lists, and engaged an estimated 80,000 public participants.\" Check out the iNaturalist results.\n\nHighlights of the 2016 nationwide BioBlitz include:\n\n\nActive Bioblitz\nInactive and historic BioBlitz\n\n\nBioBlitz Events in Hungary are organized by the Hungarian Biodiversity Research Society http://www.biodiverzitasnap.hu/ since 2006, starting with the eco-village Gyürüfű and its surroundings in Baranya County. Since then the Society organizes BioBlitz Events (called also Biodiversity Days) every year, sometimes even several events a year, during which 60-80 experts and researchers contribute to a profound momentary inventory of a chosen area in Hungary, and from time to time in cross-border areas in joint-projects with neighbour countries. The Hungarian Biodiversity Research Society invites local inhabitants and the interested public to join their events, and focusses in its outreach to young local and regional pupils and their teachers just like students from Hungary and abroad. The BioBlitz Events are taking place in partnership with the local National Park Directories, Municipalities and Civil Organisations. A rather fresh approach is the involvement of high school students during their obligatory community/voluntary work into research and field work in the topics of biodiversity and nature protection based upon long term co-operation contracts with schools and educational centres. The main goals pursued by the Hungarian Biodiversity Research Society are to promote the correct understanding of biodiversity in its true context, based upon data collection, monitoring, research and expertise, passing on knowledge from generation to generation and outreach to the broader public. It also aims to strengthen national and international networks. The results of the BioBlitz Events are published in print and on-line media and serve mainly as fundamentals for maintenance-instructions for protected areas and for appropriate natural-resource management, but also for educational purposes.\n\n\n\n\n\n\n\n\n\n\n\nBristol Natural History Consortium now host the National BioBlitz Network. (www.bnhc.org.uk)\n\n\n\n", "id": "1574457", "title": "BioBlitz"}
{"url": "https://en.wikipedia.org/wiki?curid=10300128", "text": "Systematic Census of Australian Plants\n\nThe Systematic census of Australian plants, with chronologic, literary and geographic annotations, more commonly known as the Systematic Census of Australian Plants, also known by its standard botanic abbreviation Syst. Census Austral. Pl., is a survey of the vascular flora of Australia prepared by Government botanist for the state of Victoria Ferdinand von Mueller and published in 1882.\n\nVon Mueller describes the development of the census in the preface of the volume as an extension of the seven volumes of the \"Flora Australiensis\" written by George Bentham. A new flora was necessary since as more areas of Australia were explored and settled, the flora of the island-continent became better collected and described. The first census increased the number of described species from the 8125 in \"Flora Australiensis\" to 8646. The book records all the known species indigenous to Australia and Norfolk Island; with records of species distribution. \n\nVon Mueller noted that by 1882 it had become difficult to distinguish some introduced species from native ones:\nThe lines of demarkation between truly indigenous and more recently immigrated plants can no longer in all cases be drawn with precision; but whereas \"Alchemilla vulgaris\" and \"Veronica serpyllifolia\" were found along with several European Carices in untrodden parts of the Australian Alps during the author's earliest explorations, \"Alchemilla arvensis\" and \"Veronica peregrina\" were at first only noticed near settlements. The occurrence of \"Arabis glabra\", \"Geum urbanum\", \"Agiimonia eupatoria\", \"Eupatorium cannabinum\", \"Cavpesium cernuum\" and some others may therefore readily be disputed as indigenous, and some questions concerning the nativity of various of our plants will probably remain for ever involved in doubts.\n\nIn 1889 an updated edition of the census was published, the \"Second Systematic Census\" increased the number of described species to 8839. Von Mueller dedicated both works to Joseph Dalton Hooker and A. P. de Candolle.\n\nThe work is of historic significance as the first Australian flora written in Australia. Following its publication, research and writing on the flora of Australia has largely been carried out in Australia. \n\n\n", "id": "10300128", "title": "Systematic Census of Australian Plants"}
{"url": "https://en.wikipedia.org/wiki?curid=22022351", "text": "Census of Coral Reefs\n\nThe Census of Coral Reefs (CReefs) is a field project of the Census of Marine Life that surveys the biodiversity of coral reef ecosystems internationally. The project works to study what species live in coral reef ecosystems, to develop standardized protocols for studying coral reef ecosystems, and to increase access to and exchange of information about coral reefs scattered throughout the globe. \n\nThe CReefs project uses the implementation of autonomous reef-monitoring structures (ARMS) to study the species that inhabit coral reefs. These structures are placed on the sea floor in areas where coral reefs exist, where they are left for one year. At the end of the year, the ARMvS is pulled to the surface, along with the species which have inhabited it, for analysis. \n\nCoral reefs are thought to be the most organically different of all marine ecosystems. Major declines in key reef ecosystems suggest a decline in reef population throughout the world due to environmental stresses. The vulnerability of coral reef ecosystems is expected to increase significantly in response to climate change. The reefs are also being threatened by induced coral bleaching, ocean acidification, sea-level rise, and changing storm tracks. Reef biodiversity could be in danger of being lost before it is even documented, and researchers will be left with a limited and poor understanding of these complex ecosystems.\n\nIn an attempt to enhance global understanding of reef biodiversity, the goals of the CReefs Census of Coral Reef Ecosystems were to conduct a diverse global census of coral reef ecosystems. And increase access to and exchange of coral reef data throughout the world. Because coral reefs are the most diverse and among the most threatened of all marine ecosystems, there is great justification to learn more about them.\n", "id": "22022351", "title": "Census of Coral Reefs"}
{"url": "https://en.wikipedia.org/wiki?curid=22023966", "text": "Global Census of Marine Life on Seamounts\n\nGlobal Census of Marine Life on Seamounts (commonly CenSeam) is a global scientific initiative, launched in 2005, that is designed to expand the knowledge base of marine life at seamounts. Seamounts are underwater mountains, not necessarily volcanic in origin, which often form subsurface archipelagoes and are found throughout the world's ocean basins, with almost half in the Pacific. There are estimated to be as many as 100,000 seamounts at least one kilometer in height, and more if lower rises are included. However, they have not been explored very much—in fact, only about half of one percent have been sampled—and almost every expedition to a seamount discovers new species and new information. There is evidence that seamounts can host concentrations of biologic diversity, each with its own unique local ecosystem; they seem to affect oceanic currents, resulting among other things in local concentration of plankton which in turn attracts species that graze on it, and indeed are probably a significant overall factor in biogeography of the oceans. They also may serve as way stations in the migration of whales and other pelagic species. Despite being poorly studied, they are heavily targeted by commercial fishing, including dredging. In addition they are of interest to potential seabed mining.\n\nThe overall goal of CenSeam is \"to determine the role of seamounts in the biogeography, biodiversity, productivity, and evolution of marine organisms, and to evaluate the effects of human exploitation on seamounts.\" To this effect, the group organizes and contributes to various research efforts about seamount biodiversity. Specifically, the project aims to act as a standardized scaffold for future studies and samplings, citing inefficiency and incompatibility between individual research efforts in the past. To give a scale of their mission, there are an estimated 100,000 seamounts in the ocean, but only 350 of them have been sampled, and only about 100 sampled thoroughly. Although sampling all 100,000 seamounts is infeasible, major seamounts can be sampled in such a way.\n\nCenSeam is a subdivision of the Census of Marine Life program. Organisationally, the components of CenSeam consist of a secretariat (Malcolm Clark, Mireille Consalvey, Ashley Rowden and Karen Stocks) which is hosted by the National Institute of Water and Atmospheric Research in Wellington, New Zealand; an international steering committee; a taxonomic advisory panel; and two working groups, Data Analysis and Standardisation.\n\nIn 2008 CenSeam began collaborating with the International Seabed Authority to study effects of seabed mining on seamount ecosystems.\n\n\n", "id": "22023966", "title": "Global Census of Marine Life on Seamounts"}
{"url": "https://en.wikipedia.org/wiki?curid=7391667", "text": "Census of Marine Life\n\nThe Census of Marine Life was a 10-year scientific initiative, involving a global network of researchers in more than 80 nations, engaged to assess and explain the diversity, distribution, and abundance of life in the oceans. The world's first comprehensive Census of Marine Life — past, present, and future — was released in 2010 in London.\n\nThe Census consists of three major component themes organized around the questions:\n\n\nCensus researchers undertook the task of constructing the history of marine animal populations since human predation became important, roughly the last 500 years. This program component is the History of Marine Animal Populations (HMAP).\n\nThe largest component of the Census involved investigating what now lives in the world's oceans through 14 field projects. Each sampled the biota in one of six realms of the global oceans using a range of technologies. Details of these field projects are provided below.\n\nForecasting what will live in the oceans involves modeling and simulation. This component program was the Future of Marine Animal Populations (FMAP). This group focused on integrating data from different sources and creating statistical and analytical tools to make predictions for marine populations and ecosystems.\n\nThe global initiative required a state-of-the-art data assimilation framework, and this effort, the Ocean Biogeographic Information System (OBIS), forms the fourth component program of the Census. The vision is that users will be able to click on maps of the oceans on their laptop or desktop anywhere in the world and bring up Census data on what is reported to live in the ocean zone of interest. At the end of 2010, OBIS contained more than 30 million records. OBIS is designed to make sharing data easy, helping to improve understanding of the patterns and processes that govern marine life.\n\nCoML was engaged in 14 field projects:\n\n• The Census of Diversity of Abyssal Marine Life documented actual species diversity of abyssal plains as a basis for global change research and for a better understanding of historical causes and actual ecological factors regulating biodiversity. CeDAMar collected reliable data on the large-scale distribution of one of the largest and most inaccessible environments on the planet.\n\nThe Census of Antarctic Marine Life (CAML) investigated the distribution and abundance of Antarctica’s marine biodiversity, how it is affected by climate change, and how change will alter the nature of the ecosystem services currently provided by the Southern Ocean for the benefit of mankind.\n\nThe Arctic Ocean Diversity (ArcOD) Census of Marine Life project documented the present Arctic Ocean biodiversity on a Pan-Arctic scale. The emphasis of this program was on biodiversity because processes are critically impacted by the composition of biota involved in them. The operational approach was to help coordinate, encourage and support research efforts designed to examine the biodiversity in the three major realms: sea ice, water column and seafloor.\n\nCOMARGE was a field project of the Census of Marine Life launched in 2005. The project described biodiversity patterns of benthic and bentho-demersal communities on continental margins, with a focus on multiple habitats and spatial scales, and identifying the contribution of environmental heterogeneities to these patterns. The project is led by Myriam Sibuet and managed by Lenaick Menot, both of France.\n\nThe Pacific Ocean Shelf Tracking (POST) program developed and promoted the application of new electronic tagging technology to study the marine life history of Pacific salmon. A major area of focus of the POST project involved the development of a permanent continental-scale marine telemetry system. POST's array sits on the seabed of the continental shelf and upstream in several major rivers and is used to monitor the movements of not only salmon but many other types of marine animals along the shelf. Tracking data generated from the array can be applied toward the development of fishery management policies aimed at the sustainable harvest of resources, and to the understanding and conservation of other marine and diadromous species.\n\nBy enhancing global understanding of reef biodiversity, the CoML Census of Coral Reef Ecosystems (CReefs) conducted a taxonomically diversified global census of coral reef ecosystems, increased tropical taxonomic knowledge, developed new, universal protocols (e.g. DNA-based technologies and long term sampling devices) and increased access to and exchange of coral reef data dispersed throughout the world.\n\nThe International Census of Marine Microbes (ICoMM) facilitated the inventory of microbial diversity and developed a strategy to catalogue all known diversity of single-cell organisms inclusive of the Bacteria, Archaea, Protista and associated viruses, and explored and discovered unknown microbial diversity, and placed that knowledge into appropriate ecological and evolutionary contexts.\n\nMAR-ECO described the patterns of distribution, abundance, and the trophic relationships among the organisms inhabiting the waters over and around the Mid-Atlantic Ridge. It identified and modeled the ecological processes that cause variability in these patterns. The project focused on fish, crustaceans, cephalopods, and gelatinous plankton and other actively swimming organisms, but there was focus on top predators such as seabirds and cetaceans, which interact with the more surface environment.\n\nThe Natural Geography In Shore Areas (NaGISA) project was a collaborative effort aimed at inventorying and monitoring habitat specific biodiversity in the global nearshore. The international character of the project and its target zone were reflected in the word \"nagisa\", which is Japanese for the narrow coastal zone where the land meets the sea. NaGISA held a unique position in the Census of Marine Life as an ambassador project, linking CoML to local interests around the world. NaGISA's first aim was to drawi up a global baseline of nearshore biodiversity and then use its network to continue monitoring those same shores for the next 50 years.\n\nThe Gulf of Maine was selected as the ecosystem pilot study for CoML. This program gained knowledge to enable ecosystem-based management in a large marine environment. The program advanced knowledge of both biodiversity and ecological processes over a range of habitats and food-chain levels, from plankton to whales.\n\nCenSeam was a global study of seamount ecosystems, to determine their role in the biogeography, biodiversity, productivity, and evolution of marine organisms, and to evaluate the effects of human exploitation. CenSeam commenced in 2005 and the CenSeam science community, with particular input from CenSeam's Data Analysis Working Group (DAWG), defined two overarching priority themes (1) What factors drive community composition and diversity on seamounts, including any differences between seamounts and other habitat types? (2) What are the impacts of human activities on seamount community structure and function?\n\nThe Tagging of Pacific Predators (TOPP) research program was a collaboration among scientists from the U.S., Australia, Canada, Mexico, Japan, France and the UK, that applied new technologies to understanding the environmental basis for movements and behaviors of large pelagic animals in the North Pacific. With new electronic tags, TOPP scientists followed the migrations of marine fishes, turtles, birds, pinnipeds, whales and Humboldt squid as they crisscrossed the Pacific basin. The results answer basic questions about the animals' biology including where they feed and breed, and what migration corridors they use.\n\nChEss improved the knowledge of the biodiversity and biogeography of species from deep-water chemosynthetically-driven ecosystems at a global scale and increased the understanding of the processes that shape these communities. Scientist located under the ChEss umbrella were global in nature. ChEss addressed the main questions of CoML on diversity, abundance and distribution of marine species, within the realm of deep-water reducing environments such as hydrothermal vents, cold seeps, whale falls, sunken wood and areas of low oxygen that intersect with continental margins and seamounts. ChEss scientists combined results from research on all these systems in order to understand the phylogeographic relationships amongst all deep-water chemosynthetic ecosystems.\n\nChEss scientist, Michel Segonzac, was co-finder of the popular media sensation \"Kiwa hirsuta\".\n\nThe Census of Marine Zooplankton (CMarZ) worked toward a taxonomically comprehensive assessment of biodiversity of animal plankton throughout the world oceans. The project produced information on zooplankton species diversity, biomass, biogeographical distribution, genetic diversity, and community structure. The taxonomic focus was the animals that drift with ocean currents throughout their lives (i.e., the holozooplankton). This assemblage currently includes ~7,000 described species in fifteen phyla. The Census encompassed unique marine environments and those likely to be inhabited by endemic and undescribed zooplankton species.\n\nThree non-field projects in which CoML is engaged are:\nThe Ocean Biogeographic Information System, or OBIS, is an international information system focused on marine biodiversity. It provides expert geo-referenced data on marine species and currently contains more than 30 million georeferenced, accurately identified species records from more than 800 databases. OBIS provides spatial query tools for visualizing relationships among species and their environment. This information is readily and freely accessible by the Internet and requires no special software to use.\n\nFMAP attempted to describe and synthesize globally changing patterns of species abundance, distribution, and diversity, and to model the effects of fishing, climate change and other key variables on those patterns. This work was done across ocean realms and with an emphasis on understanding past changes and predicting future scenarios.\n\nThe History of Marine Animal Populations (HMAP) improved the understanding of ecosystem dynamics, specifically with regard to long-term changes in stock abundance, the ecological impact of large-scale harvesting by man, and the role of marine resources in the historical development of human society. Since the earliest historical records, man has harvested a variety of different animals from the oceans. While ecologists have traditionally aimed to identify the current conditions of many of the animal populations affected both directly and indirectly by harvesting, much less focus has been given to the status of affected populations in earlier times. HMAP created a historical reference point of marine populations against which modern populations can be compared to determine how ocean ecosystems are changing with respect to human impact and even climate change.\n\nA Mapping and Visualization Team based at Duke University's Marine Geospatial Ecology Lab developed and shared methods to display the results of the ten-year Census of Marine Life.\n\nOne of the global goals of the Census of Marine Life was strengthening support for marine biodiversity research at the national or regional level that will continue after the CoML has concluded. Following in the footsteps of other successful global research programs, several nations and groups of nations brought together regional CoML stakeholders, such as researchers, government and non-government agencies, and resource managers, to assess the status of knowledge of marine biodiversity in their waters. These assessments led to the organization of National or Regional Implementation Committees (NRICs) to implement more local programs and improve the geographic scope of CoML and its projects. By engaging scientists, funding agencies, policy-makers and the broad user community, these national and regional committees identified their research and data priorities for marine biodiversity and find ways to make them happen by building partnerships, exploring funding opportunities for local science, and promoting CoML to local audiences. The committees worked under the umbrella of the CoML.\n\nAreas of the globe represented by an NRIC include:\n\nCoML was a partner with the Encyclopedia of Life in their effort to create descriptive web-page records for all the known species on Earth. CoML was an integral part on creating pages for marine species.\n\nCoML was a partner with Barcode of Life to help create DNA barcoding for the anticipated 230,000 marine species known thus far. CoML provided scientific data to Barcode of Life to greatly accelerate the process of creating DNA barcodes.\n\nGoogle and Census of Marine Life partnered on Google Earth 5.0. Ocean in Google Earth contains a layer devoted to the Census of Marine Life that allows users to follow scientists from the Census on expeditions and see marine life and features found during the Census.\n\nThe Census of Marine Life housed a Synthesis Group that integrated and synthesized the large amounts of information produced by CoML projects into common themes and overarching messages. This group produced a comprehensive and cohesive suite of products released to the public in 2010. Products include books, webpages, collections, articles, maps, and many others.\n\n\n\n", "id": "7391667", "title": "Census of Marine Life"}
{"url": "https://en.wikipedia.org/wiki?curid=22034606", "text": "Census of Marine Zooplankton\n\nThe Census of Marine Zooplankton is a field project of the Census of Marine Life that has aimed to produce a global assessment of the species diversity, biomass, biogeographic distribution, and genetic diversity of more than 7,000 described species of zooplankton that drift the ocean currents throughout their lives. CMarZ focuses on the deep sea, under-sampled regions, and biodiversity hotspots. \n\nTechnology plays a great role in CMarZ's research, including the use of integrated morphological and molecular sampling through DNA Barcoding. CMarZ makes its datasets available via the CMarZ Database.\n\n", "id": "22034606", "title": "Census of Marine Zooplankton"}
{"url": "https://en.wikipedia.org/wiki?curid=28105864", "text": "Biogeography of Deep-Water Chemosynthetic Ecosystems\n\nThe Biogeography of Deep-Water Chemosynthetic Ecosystems is a field project of the Census of Marine Life programme (CoML). The main aim of ChEss is to determine the biogeography of deep-water chemosynthetic ecosystems at a global scale and to understand the processes driving these ecosystems. ChEss addresses the main questions of CoML on diversity, abundance and distribution of marine species, focusing on deep-water reducing environments such as hydrothermal vents, cold seeps, whale falls, sunken wood and areas of low oxygen that intersect with continental margins and seamounts.\n\nDeep-sea hydrothermal vents and their associated fauna were first discovered along the Galapagos Rift in the eastern Pacific in 1977. Vents are now known to occur along all active mid ocean ridges and back-arc spreading centres, from fast to ultra-slow spreading ridges. The interest in chemosynthetic environments was strengthened by the discovery of chemosynthetic-based fauna at cold seeps along the base of the Florida Escarpment in 1983. Cold seeps occur along active and passive continental margins. More recently, the study of chemosynthtetic fauna has extended to the communities that develop in other reducing habitats such as whale falls, sunken wood and areas of oxygen minima when they intersect with the margin or seamounts.\nSince the first discovery of hydrothermal vents, more than 600 species have been described from vents and seeps. This is equivalent of 1 new description every 2 weeks(!). As biologists, geochemists, and physicists combine research efforts in these systems, new species will certainly be discovered. Moreover, because of the extreme conditions of the vent and seep habitat, certain species may have specific physiological adaptations with interesting results for the biochemical and medical industry.\n\nThese globally distributed, ephemeral and insular habitats that support endemic faunas offer natural laboratories for studies on dispersal, isolation and evolution. Here, hydrographic and topographic controls on biodiversity and biogeography might be much more readily resolved than in systems where climate and human activity obscure their role. In addition, hydrothermal vents have been suggested to be the habitat of the origin of life. These hypotheses are being used by ChEss researchers in collaboration with NASA to develop programmes to search for life in planets or moons of the outer space.\n\nOnly a small fraction of the global ridge system (~65000 km) and of the vast continental margin regions have been explored and their communities described. It is the aim of ChEss to improve the knowledge on the diversity, abundance and distribution of species from vents, seeps and other reducing habitats at a global scale, understanding their the abiotic and biotic processes that shape and maintain these ecosystems and their biogeography.\n\nMain ChEss Science Questions\n\n\"Objective 1. To create a centralised database\"\n\nTo create a centralised database, ChEssBase, of deep-water vent, cold seep, whalefall and OMZ species. ChEssBase is a web-based database that incorporates archived and newly collected biological material. The database is geo- and bio-referenced. ChEssBase is available online and has been integrated with OBIS.\n\n\"Objective 2. To develop a long-term field programme\"\n\nTo develop a long-term field programme to locate potential vent and seep sites and continue research on whalefalls and OMZ sites. The field programme aims to explain the main gaps in our knowledge of the diversity, abundance and distribution of chemosynthetic species globally. A limited number of target areas have been selected where specific scientific questions relevant to biogeographical issues will be answered.\n\nThe target areas have been grouped into two categories. Category I, combined areas: Area A: Equatorial Atlantic Belt region; Area B: the SE Pacific region; Area C: NZ region; Area D: the Arctic and Antarctic regions, within the International Polar Year. Category II, specific areas: 1 – The ice-covered Gakkel Ridge, 2 – the (ultra)-slow ridges of the Norwegian-Greenland Sea, 3 – the northern MAR between the Iceland and Azores hot-spots; 4 – the Brazilian continental margin, 5 – the East Scotia Ridge and Bransfield Strait, 6 – the SW Indian Ridge, 7 – the Central Indian Ridge.\nDuring the field programme, ChEss will promote the development and refinement of deep-towed, remotely operated (ROV) and autonomous underwater (AUV) vehicle technologies to locate, map and sample new chemosynthetic systems. Using optical, chemical and acoustic techniques, ChEss researchers hope to gain a better understanding of not only biogeographical patterns, but to determine the processes driving these ecosystems.\n\nDuring the field programme, ChEss will promote the development and refinement of deep-towed, remotely operated (ROV) and autonomous underwater (AUV) vehicle technologies to locate, map and sample new chemosynthetic systems. Using optical, chemical and acoustic techniques, ChEss researchers hope to gain a better understanding of not only biogeographical patterns, but to determine the processes driving these ecosystems.\n\n\"Objective 3: Outreach and Education\"\n\nChEss has multi-lingual education pages related to vents, seeps and whalefalls. There is a dedicated page for key outreach initiatives such as live cruise diaries, open days, schools activities etc.\n\nChEss has joined forces with the other deep-sea CoML projects and this has resulted in the creation of the DEep-Sea Education and Outreach group (DESEO) that has produced a book \"Deeper than Light\" published in 5 languages.\n\n", "id": "28105864", "title": "Biogeography of Deep-Water Chemosynthetic Ecosystems"}
{"url": "https://en.wikipedia.org/wiki?curid=42192153", "text": "Gradsect\n\nA gradsect or gradient-directed transect is a low-input, high-return sampling method where the aim is to maximise information about the distribution of biota in any area of study. Most living things are rarely distributed at random, their placement being largely determined by a hierarchy of environmental factors. For this reason, standard statistical designs based on purely random sampling or systematic (e.g. grid-based) systems tend to be less efficient in recovering information about the distribution of taxa than sample designs that are purposively directed instead along deterministic environmental gradients.\n\nEcologists have long been aware of the significance of environmental gradient based approaches to better understand community dynamics and this is reflected especially in the work of Robert Whittaker (1967) and others. Although in practice, life-scientists intuitively sample gradients, until the early 1980s there was little formal theoretical or empirical support for such an approach, sample design being driven largely by traditional statistical methods based on probability theory incorporating random sampling.\n\nIntensively sampled landscape-based surveys in Australia provided a reference platform for developing and testing a less logistically demanding and yet statistically acceptable gradient-based survey design that avoided the need for random or purely grid-based sampling. These initial studies and subsequently developed statistical support for purposive, gradient-based survey provided a formalized, practical alternative to more logistically demanding traditional designs. It was here the term gradsect was coined that coupled purposive, transect sampling with a hierarchical framework of environmental gradients considered to be key determinants of species distribution.\n\nIn constructing a gradsect, existing information is initially reviewed in which a hierarchy of environmental gradients is first identified either by visual means (maps, aerial photographs etc..) or through numerical analysis or spatial analysis of institutional or other data sources. A typical regional gradsect for example may be constructed according to a primary climate gradient (temperature, moisture, seasonality) then a secondary gradient (geomorphology, lithology, major and minor drainage systems), a tertiary gradient possibly represented by a local soil catena or local land use farming system or finer scale gradient levels representing local vegetational sequences. Through an inspection of spatial overlays of all gradients, a minimum number of sample locations is then purposively located to reflect, as far as possible, total environmental variation. For logistic and other purposes (such as improving the capacity to locate rare species) the steepest gradients are usually selected. In this way an \"ideal\" gradsect is constructed that may then be modified to accommodate logistic tradeoffs. The selection discipline requires that the fullest possible range of each hierarchical level is sampled. This commonly results in a set of progressively nested clusters of sample sites contained within the overarching primary gradient that may not reflect a linear distribution. At relatively local landscape scale, a primary gradients may be represented by salinity levels or water depth as in tidal wetlands or micro-topographic relief as in forest margins or a riparian zone. For most practical purposes, transects are commonly laid out along contours perpendicular to the main direction of the gradient. Iterative spatial analysis of environmental layers over a digital elevation model can then be used to identify areas requiring additional sampling thereby improving environmental representativeness.\n\nInitial studies in gradsect development revealed considerable logistic and other advantages over more traditional non-gradient-based survey designs concerned primarily with random sampling. This finding is now widely supported especially in biodiversity and other areas of environmental surveying and conservation design (see Applications next). Apart from improved logistic efficiency, the gradsect method seeks to maximise environmental representativeness which has the dual advantage of potentially improving location of rarities and enhancing spatial modelling of species distribution. Because the underlying statistical model is not based on probability theory, gradsect sampling cannot be used to estimate numbers of species or other biological attributes per unit area. For that purpose some measure of random sampling needs to be built into the sample design.\n\nSince the publication of gradsect theory in 1984, subsequent vegetational and landscape studies in regional Australia (Austin and Heyligers 1989; Ludwig and Tongway (1995) were followed by a successful evaluation of the method in faunal surveys in South Africa (Wessels et al.). Since then applications involving gradsects have ranged from habitat suitability studies of fungi (Shearer and Crane 2011 ), termites (Gillison et al. 2003) other macro invertebrates (Lawes et al. 2005 ); birds (Damalas 2005) small and large mammals (Laurance 1994; Ramono et al. 2009). Vegetation studies using gradsects have been widely applied in many countries ranging from tidal wetlands (Parker et al. 2011) and agricultural cropping systems and forested landscape mosaics (Gillison et al. 2004) to infectious diseases (Boone et al. 2000 ). At broader geographic and national scales (Grossman et al., 1998, 2007; USA/NPS 2012) gradsects have been applied to guide field sampling and forest mapping in mountainous terrain (Sandman and Lertzmann 2003) as well as wide-ranging remote sensing applications (Mallinis et al. 2008; Rocchini et al. 2011 ).\n", "id": "42192153", "title": "Gradsect"}
{"url": "https://en.wikipedia.org/wiki?curid=43518148", "text": "Butterfly count\n\nA butterfly count is an event that is organized for the purpose of identifying and counting butterflies in a specific geographical area. Butterfly counts occur regularly in North America and Europe.\n\nThe counts are conducted by interested, mostly non-professional, residents of the area who maintain an interest in determining the numbers and species of butterflies in their locale. A butterfly count usually occurs at a specific time during the year and is sometimes coordinated to occur with other counts which may include a park, county, entire state or country. The results of the counts are usually shared with other interested parties including professional lepidopterists and researchers. The data gathered during a count can indicate population changes and health within a species.\n\nProfessional, universities, clubs, elementary and secondary schools, other educational providers, nature preserves, parks and amateur organizations can organize a count. The participants often receive training to help them identify the butterfly species. The North American Butterfly Association has organized over 400 counts in 2014.\n\nThere are several methods for counting butterflies currently in use, with the notable division being between restricted and open searches. Most counts are designed to count all butterflies observed in a locality.\n\nCounts may be targeted at single species and, in some cases, butterflies are observed and counted as they move from one area to another. A heavily researched example of butterfly migration is the annual migration of monarch butterflies in North America. Some programs will tag butterflies to trace their migration routes, but these are migratory programs and not butterfly counts. Butterfly counts are sometimes done where there is a concentration (a roost) of a species of butterflies in an area. One example of this is the winter count of western monarch butterflies as they roost together at sites in California, northern Mexico and Arizona.\n\nFrequently referred to as \"Pollard Transects\" or \"Pollard Walks\" in North America, a transect is a protocol designed to standardize the recording of butterfly observations, the initial format was outlined by Ernie Pollard in 1977. The transect protocol involves one observer walking a fixed path at a constant pace, multiple times in a season. Butterflies are counted when they are seen within a prescribed distance from the path, often 2.5 meters on either side of the path, and only when the butterflies are seen in front of, or above, the observer (i.e., no backtracking). A second person may work with the observer to identify and/or photograph insects spotted by the observer. Transects should not change from year to year and ideally should sample a variety of habitats.\n\nExamples of long-running restricted searches are Art Shapiro's Butterfly Project in the US (started in 1972), and the UK Butterfly Monitoring Scheme (started in 1976).\n\nOpen searches, also sometimes referred to as \"checklist searches\", are intended to focus on the presence and abundance of butterflies in a given area. They can be single events such as the North American Butterfly Association's July 1 and July 4 counts in Canada and the U.S. respectively, or they can be regular or ad hoc counts conducted by individuals or groups. The lack of formal structure makes them suitable for many citizen science programs.\n\nIn terms of the relative outcomes or the efficacy of open vs. restricted searches, studies have shown that open searches are more likely to find a greater number of species in a given area. Royer, et al. note that one reason for this is that during an open search, the \"observer is free to search out places where butterflies typically would breed or congregate\" rather than follow the fixed path of a transect.\n\nTo promote the broad public participation in butterfly monitoring, researcher from Austria propose to combine a simplified assessment scheme on group level executed by laypeople, with detailed assessments from butterfly experts. To evaluate their approach they compared data collected by pupils with independent assessments of professional butterfly experts. Beside some identification uncertainties data collected by trained and supervised pupils were successfully used to predict the general habitat quality for butterflies. \n\nOpportunistic or incidental sightings are a butterfly sightings that are not part of a formal count. Observers may note signal butterflies or multiple species. An example of an opportunistic sighting is observing a butterfly in garden and reporting it.\n\nDescribed as a \"special type of open search\", atlas projects are generally targeted at a specific geographical area such as a province or state. The goal is to assess the presence or absence of species, usually over a multi-year period. Each atlas program will design its own data requirements but as they are measuring abundance and presence, they tend to accept data from transects, counts and opportunistic sightings to build a database. The longest running atlas program in North America is the Ontario Butterfly Atlas Online, which is supported by the Toronto Entomologists' Association and began collecting data in 1969.\nTransects and open searches are not as comprehensive in tropical locations due to issues such as density of flora and the heigh of the forest canopy. A count system using bait stations with fermenting fruit has been used to assess specific populations.\n\nParticipants are encouraged to employ a number of techniques to quantify large aggregations by making estimates of butterflies:\n\n\nThe number of butterflies can be estimated by the area size they inhabit, for example, in the overwintering population present in Mexico the population expressed in hectares.\nButterflies can be counted in their egg, larvae and instar number.\n\nButterflies are sometimes captured, tagged and recovered. The number of tags recovered in a specific area is used to determine population size and direction of flight.\n\n\n", "id": "43518148", "title": "Butterfly count"}
{"url": "https://en.wikipedia.org/wiki?curid=21957291", "text": "Census of Antarctic Marine Life\n\nThe Census of Antarctic Marine Life (CAML) is a field project of the Census of Marine Life that researches the marine biodiversity of Antarctica, how it is affected by climate change, and how this change is altering the ecosystem of the Southern Ocean.\n\nThe program started in 2005 as a 5-year initiative with the scientific goal being to study the evolution of life in Antarctic waters, to determine how this has influenced the diversity of the present biota, and use these observations to predict how it might respond to future change. However, due to modern and extravagant changes within technology, we are able to witness and influence biodiversity reproduction and development. This enables us to gain further insight toward characteristics that allow such biodiversity to flourish within this barren desert referred to as the Arctic and Antarctic.\n\nCAML has collected its data from 18 Antarctic research vessels during the International Polar Year, which is freely accessible at Scientific Committee on Antarctic Research Marine Biodiversity Information Network (SCAR-MarBIN). The Register of Antarctic Marine Species has 9,350 verified species (16,500 taxa) in 17 phyla, from microbes to whales. For 1500 species the DNA barcode is available.\n\nThe information from CAML is a robust baseline against which future change may be measured.\n\n", "id": "21957291", "title": "Census of Antarctic Marine Life"}
{"url": "https://en.wikipedia.org/wiki?curid=51485577", "text": "Great Elephant Census\n\nThe Great Elephant Census—the largest wildlife survey in history—was an African-wide census designed to provide accurate data about the number and distribution of African elephants by using standardized aerial surveys of hundreds of thousands of square miles or terrain in Africa.\n\nThe census was completed and published in the online journal \"PeerJ\" on 31 August 2016 at a cost of 7 million.\n\nScientists believe that prior to European colonization there were as many as 20 million African elephants. By 1979 only 1.3 million elephants remained on the continent.\n\nA pan-African elephant census has not been conducted since the 1970s. The idea of a modern census was devised by Elephants Without Borders and supported, both financially and logistically, by Paul G. Allen. It was also supported by other organizations and individuals, including African Parks, Frankfurt Zoological Society, Wildlife Conservation Society, The Nature Conservancy, IUCN African Elephant Specialist Group, Howard Frederick, Mike Norton-Griffith, Kevin Dunham, Chris Touless and Curtice Griffin with the report released in September 2016.\n\nMike Chase, the founder of Elephants Without Borders, was the lead scientist of the census. Chase lead a group of 90 scientists and 286 crew in 18 African countries for over two years to collect the data. During this time the team flew a distance of over , equivalent to flying to the moon and a quarter of the way back, in over 10,000 hours of collecting data. The area covered represents 93% of the elephants known range.\n\nForest Elephants which live in central and western Africa were excluded from the survey.\n\nThe final report was released on 31 August 2016 in Honolulu at the IUCN World Conservation Congress. Data collected showed a 30 percent decline in the population of African savanna elephant in 15 of the 18 countries surveyed. The reduction occurred between 2007 and 2014, representing a loss of approximately 144,000 elephants.\n\nThe total population of Africa's savannah elephants is 352,271, far lower than previously estimated.\n\nThree countries with significant elephant population were not surveyed; Namibia which would not release figures and both South Sudan and the Central African Republic where the survey was postponed as a result of armed conflict.\n\nThe rate of decline in the population is also accelerating and reached 8% in 2014. The loss of population is primarily a result of poaching with the elephants being killed for their tusks. The ivory is then illegally sold in China and the United States. It is estimated approximately 100 elephants are killed every day for ivory. Loss of habitat is another reason for the drastic reduction in population.\n\n84% of the population was sighted in legally protected areas, and high numbers of carcasses also being found in the same protected areas.\n", "id": "51485577", "title": "Great Elephant Census"}
{"url": "https://en.wikipedia.org/wiki?curid=21957226", "text": "Census of Diversity of Abyssal Marine Life\n\nThe Census of Diversity of Abyssal Marine Life (CeDAMar) is a field project of the Census of Marine Life that studies the species diversity of one of the largest and most inaccessible environments on the planet, the abyssal plain. CeDAMar uses data to create an estimation of global species diversity and provide a better understanding of the history of deep-sea fauna, including its present diversity and dependence on environmental parameters. CeDAMar initiatives aim to identify centers of high biodiversity useful for planning both commercial and conservation efforts, and are able to be used in future studies on the effects of climate change on the deep sea.\n\nAs of May 2009, participation by upwards of 56 institutions in 17 countries has resulted in the publication of nearly 300 papers. Results of CeDAMar-related research were also published in a 2010 textbook on deep-sea biodiversity by Michael Rex and Ron Etter, members of CeDAMar's Scientific Steering Committee.()\n\nCeDAMar is led by Dr. Pedro Martinez Arbizu of Germany and Dr. Craig Smith, USA.\n\n", "id": "21957226", "title": "Census of Diversity of Abyssal Marine Life"}
{"url": "https://en.wikipedia.org/wiki?curid=2785135", "text": "Imbibition\n\nImbibition is a special type of diffusion when water is absorbed by solids-colloids causing an enormous increase in volume. Examples include the absorption of water by seeds and dry wood. If it were not for the pressure due to imbibition, seedlings would not have been able to emerge out of soil into the open; they probably would not have been able to establish.\n\nImbibition is also diffusion since water surface potential movement is along a concentration gradient; the seeds and other such materials have almost no water hence they absorb water easily. Water potential gradient between the absorbent and the liquid imbibed is essential for imbibition. In addition, for any substance to imbibe any liquid, affinity between the adsorbant and the liquid is also a pre-requesite.\n\nImbibition occurs when a wetting fluid displaces a non-wetting fluid, contrary to drainage where a non-wetting phase displaces the wetting fluid. The two processes are governed by different mechanisms.\n\nOne example of imbibition that is found in nature is the absorption of water by hydrophilic colloids. Matrix potential contributes significantly to water in such substances. Examples of plant material which exhibit imbibition are dry seeds before germination. Imbibition can also entrain the genetic clock that controls circadian rhythms in Arabidopsis thaliana and (probably) other plants. Another example is that of imbibition in the Amott test.\n\nDifferent types of organic substances have different imbibing capacities. Proteins have a very high imbibing capacity then starch less and cellulose least. That is why proteinaceous pea seeds swell more on imbibition than starchy wheat seeds.\n\nImbibition of water increases the volume of the imbibant, which results in imbibitional pressure. This pressure can be of tremendous magnitude. This fact can be demonstrated by the splitting of rocks by inserting dry wooden stalks in the crevices of the rocks and soaking them in water, a technique used by early Egyptians to cleave stone blocks.\n\nSkin grafts (split thickness and full thickness) receive oxygenation and nutrition via imbibition, maintaining cellular viability until the processes of inosculation and revascularisation have re-established a new blood supply within these tissues.\n\n", "id": "2785135", "title": "Imbibition"}
{"url": "https://en.wikipedia.org/wiki?curid=1525710", "text": "Assimilation (biology)\n\nBiological assimilation, or bio-assimilation, is the combination of two processes to supply cells with nutrients. The first is the process of absortion of vitamins, minerals, and other chemicals from food within the gastrointestinal tract. In humans, this is done with a chemical breakdown (enzymes and acids) and physical breakdown (oral mastication and stomach churning). The second process of bio-assimilation is the chemical alteration of substances in the bloodstream by the liver or cellular secretions. Although a few similar compounds can be absorbed in digestion bio-assimilation, the bioavailability of many compounds is dictated by this second process since both the liver and cellular secretions can be very specific in their metabolic action (see chirality). This second process is where the absorbed food reaches the cells via the liver.\n\nMost foods are composed of largely indigestible components depending on the enzymes and effectiveness of an animal's digestive tract. The most well-known of these indigestible compounds is cellulose; the basic chemical polymer in the makeup of plant cell walls. Most animals, however, do not produce cellulase; the enzyme needed to digest cellulose. However some animal and species have developed symbiotic relationships with cellulase-producing bacteria (see termites and metamonads.) This allows termites to use the energy-dense cellulose carbohydrate. Other such enzymes are known to significantly improve bio-assimilation of nutrients. Because of the use of bacterial derivatives enzymatic dietary supplements now contain such enzymes as amylase, glucoamylase, protease, invertase, peptidase, lipase, lactase, phytase, and cellulase. These enzymes improve the overall bioassimilation in the digestive tract but are still not proven to increase bloodstream bioavailability. \nBasically the enzymes and other breakdowns make the bigger substances of food smaller so they can go through the rest of their digestion more easily.\n\n\n", "id": "1525710", "title": "Assimilation (biology)"}
{"url": "https://en.wikipedia.org/wiki?curid=10523316", "text": "Degranulation\n\nDegranulation is a cellular process that releases antimicrobial cytotoxic or other molecules from secretory vesicles called granules found inside some cells. It is used by several different cells involved in the immune system, including granulocytes (neutrophils, basophils, and eosinophils) and mast cells. It is also used by certain lymphocytes such as natural killer (NK) cells and cytotoxic T cells, whose main purpose is to destroy invading microorganisms.\n\nAntigens interact with IgE molecules already bound to high affinity Fc receptors on the surface of mast cells to induce degranulation, via the activation of tyrosine kinases within the cell. The mast cell releases a mixture of compounds, including histamine, proteoglycans, serotonin, and serine proteases from its cytoplasmic granules.\n\nIn a similar mechanism, activated eosinophils release preformed mediators such as major basic protein, and enzymes such as peroxidase, following interaction between their Fc receptors and IgE molecules that are bound to large parasites like helminths.\n\nFour kinds of granules exist in neutrophils that display differences in content and regulation. Secretory vesicles are the most likely to release their contents by degranulation, followed by gelatinase granules, specific granules, and azurophil granules.\nCytotoxic T cells and NK cells release molecules like perforin and granzymes by\na process of directed exocytosis to kill infected target cells.\n\n", "id": "10523316", "title": "Degranulation"}
{"url": "https://en.wikipedia.org/wiki?curid=10469455", "text": "Immunoglobulin class switching\n\nImmunoglobulin class switching, also known as isotype switching, isotypic commutation or class-switch recombination (CSR), is a biological mechanism that changes a B cell's production of immunoglobulin (antibodies) from one type to another, such as from the isotype IgM to the isotype IgG. During this process, the constant-region portion of the antibody heavy chain is changed, but the variable region of the heavy chain stays the same (the terms \"variable\" and \"constant\" refer to changes or lack thereof between antibodies that target different epitopes). Since the variable region does not change, class switching does not affect antigen specificity. Instead, the antibody retains affinity for the same antigens, but can interact with different effector molecules.\n\nClass switching occurs after activation of a mature B cell via its membrane-bound antibody molecule (or B cell receptor) to generate the different classes of antibody, all with the same variable domains as the original antibody generated in the immature B cell during the process of V(D)J recombination, but possessing distinct constant domains in their heavy chains.\n\nNaïve mature B cells produce both IgM and IgD, which are the first two heavy chain segments in the immunoglobulin locus. After activation by antigen, these B cells proliferate. If these activated B cells encounter specific signaling molecules via their CD40 and cytokine receptors (both modulated by T helper cells), they undergo antibody class switching to produce IgG, IgA or IgE antibodies. During class switching, the constant region of the immunoglobulin heavy chain changes but the variable regions, and therefore antigenic specificity, stay the same. This allows different daughter cells from the same activated B cell to produce antibodies of different isotypes or subtypes (e.g. IgG1, IgG2 etc.).\n\nThe order of the heavy chain exons are as follows:\n\n\nClass switching occurs by a mechanism called class switch recombination (CSR) binding. Class switch recombination is a biological mechanism that allows the class of antibody produced by an activated B cell to change during a process known as isotype or class switching. During CSR, portions of the antibody heavy chain locus are removed from the chromosome, and the gene segments surrounding the deleted portion are rejoined to retain a functional antibody gene that produces antibody of a different isotype. Double-stranded breaks are generated in DNA at conserved nucleotide motifs, called switch (S) regions, which are upstream from gene segments that encode the constant regions of antibody heavy chains; these occur adjacent to all heavy chain constant region genes with the exception of the δ-chain. DNA is nicked and broken at two selected S-regions by the activity of a series of enzymes, including Activation-Induced (Cytidine) Deaminase (AID), uracil DNA glycosylase and apyrimidic/apurinic (AP)-endonucleases. The intervening DNA between the S-regions is subsequently deleted from the chromosome, removing unwanted μ or δ heavy chain constant region exons and allowing substitution of a γ, α or ε constant region gene segment. The free ends of the DNA are rejoined by a process called non-homologous end joining (NHEJ) to link the variable domain exon to the desired downstream constant domain exon of the antibody heavy chain. In the absence of non-homologous end joining, free ends of DNA may be rejoined by an alternative pathway biased toward microhomology joins. With the exception of the μ and δ genes, only one antibody class is expressed by a B cell at any point in time.\nWhile class switch recombination is mostly a deletional process, rearranging a chromosome in \"cis\", it can also occur (in 10 to 20% of cases, depending upon the Ig class) as an inter-chromosomal translocation mixing immunoglobulin heavy chain genes from both alleles.\n\nT cell cytokines modulate class switching in mouse (Table 1) and human (Table 2). These cytokines may have suppressive effect on production of IgM.\n\nIn addition to the highly repetitive structure of the target S regions, the process of class switching needs S regions to be first transcribed and spliced out of the immunoglobulin heavy chain transcripts (where they lie within introns). Chromatin remodeling, accessibility to transcription and to AID and synapsis of broken S regions are under the control of a large super-enhancer, located downstream the more distal Calpha gene, the 3' regulatory region (3'RR). In some occasions, the 3'RR super-enhancer can itself be targeted by AID and undergo DNA breaks and junction with Sµ, which then deletes the Ig heavy chain locus and defines locus suicide recombination (LSR).\n\n", "id": "10469455", "title": "Immunoglobulin class switching"}
{"url": "https://en.wikipedia.org/wiki?curid=909363", "text": "Biotinylation\n\nIn biochemistry, biotinylation is the process of covalently attaching biotin to a protein, nucleic acid or other molecule. Biotinylation is rapid, specific and is unlikely to perturb the natural function of the molecule due to the small size of biotin (MW = 244.31 g/mol). Biotin binds to streptavidin and avidin with an extremely high affinity, fast on-rate, and high specificity, and these interactions are exploited in many areas of biotechnology to isolate biotinylated molecules of interest. Biotin-binding to streptavidin and avidin is resistant to extremes of heat, pH and proteolysis, making capture of biotinylated molecules possible in a wide variety of environments. Also, multiple biotin molecules can be conjugated to a protein of interest, which allows binding of multiple streptavidin, avidin or Neutravidin protein molecules and increases the sensitivity of detection of the protein of interest. There is a large number of biotinylation reagents available that exploit the wide range of possible labelling methods. Due to the strong affinity between biotin and streptavidin, the purification of biotinylated proteins has been a widely used approach to identify protein-protein interactions and post-translational events such as ubiquitylation in molecular biology.\n\nProteins can be biotinylated chemically or enzymatically. Chemical biotinylation utilises various conjugation chemistries to yield nonspecific biotinylation of amines, carboxylates, sulfhydryls and carbohydrates (e.g., NHS-coupling gives biotinylation of any primary amines in the protein). Enzymatic biotinylation results in biotinylation of a specific lysine within a certain sequence by a bacterial biotin ligase. Most chemical biotinylation reagents consist of a reactive group attached via a linker to the valeric acid side chain of biotin. As the biotin binding pocket in avidin / streptavidin is buried beneath the protein surface, biotinylation reagents possessing a longer linker are desirable, as they enable the biotin molecule to be more accessible to binding avidin/streptavidin/Neutravidin protein. This linker can also mediate the solubility of biotinylation reagents; linkers that incorporate poly(ethylene) glycol (PEG) can make water-insoluble reagents soluble or increase the solubility of biotinylation reagents that are already soluble to some extent.\n\nIn contrast to chemical biotinylation methods, enzymatic biotinylation allows biotin to be linked at exactly one residue present in the protein. This biotinylation reaction can also go to completion, meaning that the product is generated with high uniformity and can be linked to streptavidin in a defined orientation e.g. for MHC multimers. Enzymatic biotinylation is most often carried out by fusing the protein of interest at its N-terminus, C-terminus or at an internal loop to a 15 amino acid peptide, termed AviTag or Acceptor Peptide (AP). This tag serves as a recognition site for an \"E. coli\" biotin holoenzyme synthetase, also known as biotin ligase (BirA). Once tagged, the protein is then incubated with BirA allowing biotinylation to take place in the presence of biotin and ATP. Enzymatic biotinylation can be carried out \"in vitro\" but BirA also reacts specifically with its target peptide inside mammalian and bacterial cells and at the cell surface, while other cellular proteins are not modified. Enzymatic biotinylation can also take place \"in vivo\" typically through the co-expression of an Avitag tagged protein and BirA. Another way to biotinylate proteins is by fusing the target protein to the biotin carboxyl carrier protein (BCCP). A protein fused by BCCP can be recognized by biotin molecules \"in vivo\" and attach to it.\n\nThe most common targets for modifying protein molecules are primary amine groups that are present as lysine side chain epsilon-amines and N-terminal α-amines. Amine-reactive biotinylation reagents can be divided into two groups based on water solubility.\n\nN-hydroxysuccinimide (NHS) esters have poor solubility in aqueous solutions. For reactions in aqueous solution, they must first be dissolved in an organic solvent, then diluted into the aqueous reaction mixture. The most commonly used organic solvents for this purpose are dimethyl sulfoxide (DMSO) and dimethyl formamide (DMF), which are compatible with most proteins at low concentrations. Because of the hydrophobicity of NHS-esters, NHS biotinylation reagents can also diffuse through the cell membrane, meaning that they will biotinylate both internal and external components of a cell.\n\nSulfo-NHS esters are more soluble in water and should be dissolved in water just before use because they hydrolyze easily. The water solubility of sulfo-NHS-esters stems from their sulfonate group on the N-hydroxysuccinimide ring and eliminates the need to dissolve the reagent in an organic solvent. Sulfo-NHS-esters of biotin also can be used as cell surface biotinylation reagents, because they do not penetrate the cell membrane.\n\nThe chemical reactions of NHS- and sulfo-NHS esters are essentially identical, in that they both react spontaneously with amines to form an amide bond. Because the target for the ester is a deprotonated primary amine, the reaction is favored under basic conditions (above pH 7). Hydrolysis of the NHS ester is a major competing reaction, and the rate of hydrolysis increases with increasing pH. NHS- and sulfo-NHS-esters have a half-life of several hours at pH 7 but only a few minutes at pH 9.\n\nThere is some flexibility in the conditions for conjugating NHS-esters to primary amines. Incubation temperatures can range from 4-37 °C, pH values in the reaction range from 7-9, and incubation times range from a few minutes to 12 hours. Buffers containing amines (such as Tris or glycine) must be avoided, because they compete with the reaction.\n\nAn alternative to primary amine biotinylation is to label sulfhydryl groups with biotin. Because free sulfhydryl groups are less prevalent on most proteins compared to primary amines, sulfhydryl biotinylation is useful when primary amines are located in the regulatory domain(s) of the target protein or when a reduced level of biotinylation is required. Sulfhydryl-reactive groups such as maleimides, haloacetyls and pyridyl disulfides, require free sulfhydryl groups for conjugation; disulfide bonds must first be reduced to free up the sulfhydryl groups for biotinylation. If no free sulfhydryl groups are available, lysines can be modified with various thiolation reagents (Traut's reagent, SAT(PEG4), SATA and SATP), resulting in the addition of a free sulfhydryl. Sulfhydryl biotinylation is performed at a slightly lower pH (6.5-7.5) than labeling with NHS esters. \n\nBesides whole proteins, biotinylated peptides can be synthesized by introducing a cysteine (Cys) residue during synthesis at the terminus of the amino acid chain to get a site specific and oriented biotinylation. Nucleotides can also be biotinylated by incorporation of biotinylated nucleotides.\n\nCarboxyl groups are found on the C-terminal ends of proteins and on glutamate and aspartate amino acid side chains. Biotinylation reagents that target carboxyl groups do not have a carboxyl-reactive moiety per se but instead rely on a carbodiimide crosslinker such as EDC to bind the primary amine on the biotinylation reagents to the carboxyl group on the target protein.\n\nBiotinylation at carboxyl groups occur at pH 4.5–5.5. To prevent crossreactivity of the crosslinker with buffer constituents, buffers should not contain primary amines (e.g., Tris, glycine) or carboxyls (e.g., acetate, citrate); MES buffer is an ideal choice.\n\nGlycoproteins can be biotinylated by modifying the carbohydrate residues to aldehydes, which then react with hydrazine- or alkoxyamine-based biotinylation reagents. Sodium periodate oxidizes the sialic acids on glycoproteins to aldehydes to form these stable linkages at pH 4–6.\n\nPolyclonal antibodies are heavily glycosylated, and because glycosylation does not interfere with the antibody activity, biotinylating the glycosyl groups is an ideal strategy to generate biotinylated antibodies.\n\nOligonucleotides are readily biotinylated in the course of oligonucleotide synthesis by the phosphoramidite method using commercial biotin phosphoramidite. Upon the standard deprotection, the conjugates obtained can be purified using reverse-phase or anion-exchange HPLC\n\nPhotoactivatable biotinylation reagents are ideal when primary amines, sulfhydryls, carboxyls and carbohydrates are not available for labeling. These reagents rely on aryl azides, which become activated by ultraviolet light (UV; >350 nm), which then react at C-H and N-H bonds. Because these types of bonds occur independent of the type of amino acid, this type of biotinylation is termed \"non-specific\".\n\nPhotoactivatable biotinylation reagents can also be used to activate biotinylation at specific times in an experiment or during certain reaction conditions, by simply exposing the reaction to UV light at the specific time or condition.\n\nThe biotin tag can be used in affinity chromatography together with a column that has avidin (or streptavidin or Neutravidin) bound to it, which is the natural ligand for biotin. However, harsh conditions (e.g., 6M GuHCl at pH 1.5) are needed to break the avidin/streptavidin - biotin interaction, which will most likely denature the protein carrying the biotin tag. If isolation of the tagged protein is needed, it is better to tag the protein with iminobiotin. This biotin analogue gives strong binding to avidin/streptavidin at alkaline pH, but the affinity is reduced upon lowering the pH. Therefore, an iminobiotin-tagged functional protein can be released from an avidin/streptavidin column by decreasing the pH (to around pH 4).\n\nThis tag can also be used in detection of the protein via anti-biotin antibodies or avidin/streptavidin-tagged detection strategies such as enzyme reporters (e.g., horseradish peroxidase, alkaline phosphatase) or fluorescent probes. This can be useful in localization by fluorescent or electron microscopy, ELISA assays, ELISPOT assays, western blots and other immunoanalytical methods. Detection with monovalent streptavidin can avoid clustering or aggregation of the biotinylated target.\n\nThe non-covalent bond formed between biotin and avidin or streptavidin has a binding affinity that is higher than most antigen and antibody bonds and approaches the strength of a covalent bond. This very tight binding makes labeling proteins with biotin a useful tool for applications such as affinity chromatography using immobilized avidin or streptavidin to separate the biotinylated protein from a mixture of other proteins and biochemicals. Biotinylated protein such as biotinylated bovine serum albumin (BSA) is used in solid-phase assays as a coating on the well surface in multiwell assay plates. Biotinylation of red blood cells has been used as a means of determining total blood volume without the use of radiolabels such as chromium 51, allowing volume determinations in low birth weight infants and pregnant women who could not otherwise be exposed to the required doses of radioactivity. Furthermore, biotinylation of MHC molecules to create MHC multimers has become a useful tool for identifying and isolating antigen-specific T-cell populations. More recently, \"in vivo\" protein biotinylation was developed to study protein-protein interactions and proximity in living cells\n\nReaction conditions for biotinylation are chosen so that the target molecule (e.g., an antibody) is labeled with sufficient biotin molecules to purify or detect the molecule, but not so much that the biotin interferes with the function of the molecule.\n\nThe HABA (2-(4-hydroxyazobenzene) benzoic acid) assay can be used to determine the extent of biotinylation. HABA dye is bound to avidin or streptavidin and yields a characteristic absorbance. When biotinylated proteins or other molecules are introduced, the biotin displaces the dye, resulting in a change in absorbance at 500 nm. This change is directly proportional to the level of biotin in the sample. The disadvantage of the HABA assay is that it uses large amounts of sample.\n\nExtent of biotinylation can also be measured by streptavidin gel-shift, since streptavidin remains bound to biotin during agarose gel electrophoresis or polyacrylamide gel electrophoresis. The proportion of target biotinylated can be measured via the change in band intensity of the target with or without excess streptavidin, seen quickly and quantitatively for biotinylated proteins by Coomassie Brilliant Blue staining.\n\n", "id": "909363", "title": "Biotinylation"}
{"url": "https://en.wikipedia.org/wiki?curid=1872854", "text": "Biochemical cascade\n\nA biochemical cascade, also known as a signaling cascade or signaling pathway, is a series of chemical reactions which are initiated by a stimulus (first messenger) acting on a receptor that is transduced to the cell interior through second messengers (which amplify the initial signal) and ultimately to effector molecules, resulting in a cell response to the initial stimulus. At each step of the signaling cascade, various controlling factors are involved to regulate cellular actions, responding effectively to cues about their changing internal and external environments.\n\nCells require a full and functional cellular machinery to live. When they belong to complex multicellular organisms, they need to communicate among themselves and work for symbiosis in order to give life to the organism. These communications between cells triggers intracellular signaling cascades, termed signal transduction pathways, that regulate specific cellular functions. Each signal transduction occurs with a primary extracellular messenger that binds to a transmembrane or nuclear receptor, initiating intracellular signals. The complex formed produces or releases second messengers that integrate and adapt the signal, amplifying it, by activating molecular targets, which in turn trigger effectors that will lead to the desired cellular response.\n\nSignal transduction is realized by activation of specific receptors and consequent production/delivery of second messengers, as Ca or cAMP. These molecules operate as signal transducers, triggering intracellular cascades and in turn amplifying the initial signal.\nTwo main signal transduction mechanisms have been identified, via nuclear receptors, or via transmembrane receptors. In the first one, first messenger cross through the cell membrane, binding and activating intracellular receptors localized at nucleus or cytosol, which then act as transcriptional factors regulating directly gene expression. This is possible due to the lipophilic nature of those ligands, mainly hormones. In the signal transduction via transmembrane receptors, first messenger bind to the extracellular domain of transmembrane receptor activating it. This receptors may have intrinsic catalytic activity or may be coupled to effector enzymes, or may also be associated to ionic channels. Therefore, there are four main transmembrane receptor types: G protein coupled receptors (GPCRs), tyrosine kinase receptors (RTKs), serine/threonine kinase receptors (RSTKs), and ligand-gated ion channels (LGICs).\nSecond messengers can be classified into three classes: \n\nThe cellular response in signal transduction cascades involves alteration of the expression of effector genes or activation/inhibition of targeted proteins. Regulation of protein activity mainly involves phosphorylation/dephosphorylation events, leading to its activation or inhibition. It is the case for the vast majority of responses as a consequence of the binding of the primary messengers to membrane receptors. This response is quick, as it involves regulation of molecules that are already present in the cell. On the other hand, the induction or repression of the expression of genes requires the binding of transcriptional factors to the regulatory sequences of these genes. The transcriptional factors are activated by the primary messengers, in most cases, due to their function as nuclear receptors for these messengers. The secondary messengers like DAG or Ca could also induce or repress gene expression, via transcriptional factors. This response is slower than the first because it involves more steps, like transcription of genes and then the effect of newly formed proteins in a specific target. The target could be a protein or another gene.\n\nIn biochemistry, several important enzymatic cascades and signal transduction cascades participate in metabolic pathways or signaling networks, in which enzymes are usually involved to catalyze the reactions. For example, the tissue factor pathway in the coagulation cascade of secondary hemostasis is the primary pathway leading to fibrin formation, and thus, the initiation of blood coagulation. The pathways are a series of reactions, in which a zymogen (inactive enzyme precursor) of a serine protease and its glycoprotein co-factors are activated to become active components that then catalyze the next reaction in the cascade, ultimately resulting in cross-linked fibrin.\n\nAnother example, sonic hedgehog signaling pathway, is one of the key regulators of embryonic development and is present in all bilaterians. Different parts of the embryo have different concentrations of hedgehog signaling proteins, which give cells information to make the embryo develop properly and correctly into a head or a tail. When the pathway malfunctions, it can result in diseases like basal cell carcinoma. Recent studies point to the role of hedgehog signaling in regulating adult stem cells involved in maintenance and regeneration of adult tissues. The pathway has also been implicated in the development of some cancers. Drugs that specifically target hedgehog signaling to fight diseases are being actively developed by a number of pharmaceutical companies. Most biochemical cascades are series of events, in which one event triggers the next, in a linear fashion. Negative cascades, however, include events that are in a circular fashion, or can cause or be caused by multiple events.\n\nBiochemical cascades include: \nNegative cascades include:\n\nAdhesion is an essential process to epithelial cells so that epithelium can be formed and cells can be in permanent contact with extracellular matrix and other cells. Several pathways exist to accomplish this communication and adhesion with environment. But the main signalling pathways are the cadherin and integrin pathways.\nThe cadherin pathway is present in adhesion junctions or in desmosomes and it is responsible for epithelial adhesion and communication with adjacent cells. Cadherin is a transmembrane glycoprotein receptor that establishes contact with another cadherin present in the surface of a neighbour cell forming an adhesion complex. This adhesion complex is formed by β-catenin and α-catenin, and p120 is essential for its stabilization and regulation. This complex then binds to actin, leading to polymerization. For actin polymerization through the cadherin pathway, proteins of the Rho GTPases family are also involved. This complex is regulated by phosphorylation, which leads to downregulation of adhesion. Several factors can induce the phosphorylation, like EGF, HGF or v-Src. The cadherin pathway also has an important function in survival and proliferation because it regulates the concentration of cytoplamatic β-catenin. When β-catenin is free in the cytoplasm, normally it is degraded, however if the Wnt signalling is activated, β-catenin degradation is inhibited and it is translocated to the nucleus where it forms a complex with transcription factors. This leads to activation of genes responsible for cell proliferation and survival. So the cadherin-catenin complex is essential for cell fate regulation. \nIntegrins are heterodimeric glycoprotein receptors that recognize proteins present in the extracellular matrix, like fibronectin and laminin. In order to function, integrins have to form complexes with ILK and Fak proteins. For adhesion to the extracellular matrix, ILK activate the Rac and Cdc42 proteins and leading to actin polymerization. ERK also leads to actin polymerization trough activation of cPLA2. Recruitment of FAK by integrin leads to Akt activation and this inhibits pro-apoptotic factors like BAD and Bax. When adhesion through integrins do not occur the pro-apoptotic factors are not inhibited and resulting in apoptosis.\n\nThe hepatocyte is a complex and multifunctional differentiated cell whose cell response will be influenced by the zone in hepatic lobule, because concentrations of oxygen and toxic substances present in the hepatic sinusoids change from periportal zone to centrilobular zone10. The hepatocytes of the intermediate zone have the appropriate morphological and functional features since they have the environment with average concentrations of oxygen and other substances. \nThis specialized cell is capable of:\nThe hepatocyte also regulates other functions for constitutive synthesis of proteins (albumin, ALT and AST) that influences the synthesis or activation of other molecules (synthesis of urea and essential amino acids), activate vitamin D, utilization of vitamin K, transporter expression of vitamin A and conversion of thyroxine.\n\nPurinergic signalling has an essential role at interactions between neurons and glia cells, allowing these to detect action potentials and modulate neuronal activity, contributing for intra and extracellular homeostasis regulation. Besides purinergic neurotransmitter, ATP acts as a trophic factor at cellular development and growth, being involved on microglia activation and migration, and also on axonal myelination by oligodendrocytes. There are two main types of purinergic receptors, P1 binding to adenosine, and P2 binding to ATP or ADP, presenting different signalling cascades.\nThe Nrf2/ARE signalling pathway has a fundamental role at fighting against oxidative stress, to which neurons are especially vulnerable due to its high oxygen consumption and high lipid content. This neuroprotective pathway involves control of neuronal activity by perisynaptic astrocytes and neuronal glutamate release, with the establishment of tripartite synapses. The Nrf2/ARE activation leads to a higher expression of enzymes involved in glutathione syntheses and metabolism, that have a key role in antioxidant response.\nThe LKB1/NUAK1 signalling pathway regulates terminal axon branching at cortical neurons, via local immobilized mitochondria capture. Besides NUAK1, LKB1 kinase acts under other effectors enzymes as SAD-A/B and MARK, therefore regulating neuronal polarization and axonal growth, respectively. These kinase cascades implicates also Tau and others MAP.\nAn extended knowledge of these and others neuronal pathways could provide new potential therapeutic targets for several neurodegenerative chronic diseases as Alzheimer's, Parkinson's and Huntington's disease, and also amyotrophic lateral sclerosis.\n\nThe blood cells (erythrocytes, leukocytes and platelets) are produced by hematopoiesis.\nThe erythrocytes have as main function the O delivery to the tissues, and this transfer occurs by diffusion and is determined by the O tension (PO). The erythrocyte is able to feel the tissue need for O and cause a change in vascular caliber, through the pathway of ATP release, which requires an increase in cAMP, and are regulated by the phosphodiesterase (PDE). This pathway can be triggered via two mechanisms: physiological stimulus (like reduced O2 tension) and activation of the prostacyclin receptor (IPR). This pathway includes heterotrimeric G proteins, adenylyl cyclase (AC), protein kinase A (PKA), cystic fibrosis transmembrane conductance regulator (CFTR), and a final conduit that transport ATP to vascular lumen (pannexin 1 or voltage-dependent anion channel (VDAC)). The released ATP acts on purinergic receptors on endothelial cells, triggering the synthesis and release of several vasodilators, like nitric oxide (NO) and prostacyclin (PGI). \nThe current model of leukocyte adhesion cascade includes many steps mentioned in Table 1. The integrin-mediated adhesion of leukocytes to endothelial cells is related with morphological changes in both leukocytes and endothelial cells, which together support leukocyte migration through the venular walls. Rho and Ras small GTPases are involved in the principal leukocyte signaling pathways underlying chemokine-stimulated integrin-dependent adhesion, and have important roles in regulating cell shape, adhesion and motility.\nAfter a vascular injury occurs, platelets are activated by locally exposed collagen (glycoprotein (GP) VI receptor), locally generated thrombin (PAR1 and PAR4 receptors), platelet-derived thromboxane A2 (TxA2) (TP receptor) and ADP (P2Y1 and P2Y12 receptors) that is either released from damaged cells or secreted from platelet dense granules. The von Willebrand factor (VWF) serves as an essential accessory molecule. In general terms, platelet activation initiated by agonist takes to a signaling cascade that leads to an increase of the cytosolic calcium concentration. Consequently, the integrin αβ is activated and the binding to fibrinogen allows the aggregation of platelets to each other. The increase of cytosolic calcium also leads to shape change and TxA2 synthesis, leading to signal amplification.\n\nThe main goal of biochemical cascades in lymphocytes is the secretion of molecules that can suppress altered cells or eliminate pathogenic agents, through proliferation, differentiation and activation of these cells. Therefore, the antigenic receptors play a central role in signal transduction in lymphocytes, because when antigens interact with them lead to a cascade of signal events. These receptors, that recognize the antigen soluble (B cells) or linked to a molecule on Antigen Presenting Cells (T cells), don't have long cytoplasm tails, so they are anchored to signal proteins, which contain a long cytoplasmic tails with a motif that can be phosphorylated (ITAM – immunoreceptor tyrosine-based activation motif) and resulting in different signal pathways. The antigen receptor and signal protein form a stable complex, named BCR or TCR, in B or T cells, respectively. The family Src is essential for signal transduction in these cells, because it's responsible for phosphorylation of ITAMs. Therefore, Lyn and Lck, in lymphocytes B and T, respectively, phosphorylate ITAMs after the antigen recognition and the conformational change of the receptor, which leads to the binding of Syk/Zap-70 kinases to ITAM and its activation. Syk kinase is specific of lymphocytes B and Zap-70 is present in T cells. After activation of these enzymes, some adaptor proteins are phosphorylated, like BLNK (B cells) and LAT (T cells). These proteins after phosphorylation become activated and allow binding of others enzymes that continue the biochemical cascade. One example of a protein that binds to adaptor proteins and become activated is PLC that is very important in the lymphocyte signal pathways. PLC is responsible for PKC activation, via DAG and Ca, which leads to phosphorylation of CARMA1 molecule, and formation of CBM complex. This complex activates Iκκ kinase, which phosphorylates I-κB, and then allows the translocation of NF-κB to the nucleus and transcription of genes encoding cytokines, for example. Others transcriptional factors like NFAT and AP1 complex are also important for transcription of cytokines. The differentiation of B cells to plasma cells is also an example of a signal mechanism in lymphocytes, induced by a cytokine receptor. In this case, some interleukins bind to a specific receptor, which leads to activation of MAPK/ERK pathway. Consequently, the BLIMP1 protein is translated and inhibits PAX5, allowing immunoglobulin genes transcription and activation of XBP1 (important for the secretory apparatus formation and enhancing of protein synthesis). Also, the coreceptors (CD28/CD19) play an important role because they can improve the antigen/receptor binding and initiate parallel cascade events, like activation o PI3 Kinase. PIP3 then is responsible for activation of several proteins, like vav (leads to activation of JNK pathway, which consequently leads to activation of c-Jun) and btk (can also activate PLC).\n\nThe Wnt signaling pathway can be divided in canonical and non-canonical. The canonical signaling involves binding of Wnt to Frizzled and LRP5 co-receptor, leading to GSK3 fosforilation and inhibition of β-catenin degradation, resulting in its accumulation and translocation to the nucleus, where it acts as a transcription factor. The non-canonical Wnt signaling can be divided in planar cell polarity (PCP) pathway and Wnt/calcium pathway. It is characterized by binding of Wnt to Frizzled and activation of G proteins and to an increase of intracellular levels of calcium through mechanisms involving PKC 50. The Wnt signaling pathway plays a significative role in osteoblastogenesis and bone formation, inducing the differentiation of mesenquimal pluripotent cells in osteoblasts and inhibiting the RANKL/RANK pathway and osteoclastogenesis.\n\nRANKL is a member of the TNF superfamily of ligands. Through binding to the RANK receptor it activates various molecules, like NF-kappa B, MAPK, NFAT and PI3K52. The RANKL/RANK signaling pathway regulates osteoclastogenesis, as well as, the survival and activation of osteoclasts.\n\nAdenosine is very relevant in bone metabolism, as it plays a role in formation and activation of both osteoclasts and osteoblasts. Adenosine acts by binding to purinergic receptors and influencing adenilyl cyclase activity and the formation of cAMP and PKA 54. Adenosine may have opposite effects on bone metabolism, because while certain purinergic receptors stimulate adenilyl cyclase activity, others have the opposite effect. Under certain circumstances adenosine stimulates bone destruction and in other situations it promotes bone formation, depending on the purinergic receptor that is being activated.\n\nSelf-renewal and differentiation abilities are exceptional properties of stem cells. These cells can be classified by their differentiation capacity, which progressively decrease with development, in totipotents, pluripotents, multipotents and unipotents.\n\nSelf-renewal process is highly regulated from cell cycle and genetic transcription control. There are some signaling pathways, such as LIF/JAK/STAT3 (Leukemia inhibitory factor/Janus kinase/Signal transducer and activator of transcription 3) and BMP/SMADs/Id (Bone morphogenetic proteins/ Mothers against decapentaplegic/ Inhibitor of differentiation), mediated by transcription factors, epigenetic regulators and others components, and they are responsible for self-renewal genes expression and inhibition of differentiation genes expression, respectively.\n\nAt cell cycle level there is an increase of complexity of the mechanisms in somatic stem cells. However, it is observed a decrease of self-renewal potential with age. These mechanisms are regulated by p16-CDK4/6-Rb and p19-p53-P21 signaling pathways. Embryonic stem cells have constitutive cyclin E-CDK2 activity, which hyperphosphorylates and inactivates Rb. This leads to a short G1 phase of the cell cycle with rapid G1-S transition and little dependence on mitogenic signals or D cyclins for S phase entry. In fetal stem cells, mitogens promote a relatively rapid G1-S transition through cooperative action of cyclin D-CDK4/6 and cyclin E-CDK2 to inactivate Rb family proteins. p16 and p19 expression are inhibited by Hmga2-dependent chromatin regulation. Many young adult stem cells are quiescent most of the time. In the absence of mitogenic signals, cyclin-CDKs and the G1-S transition are suppressed by cell cycle inhibitors including Ink4 and Cip/Kip family proteins. As a result, Rb is hypophosphorylated and inhibits E2F, promoting quiescence in G0-phase of the cell cycle. Mitogen stimulation mobilizes these cells into cycle by activating cyclin D expression. In old adult stem cells, let-7 microRNA expression increases, reducing Hmga2 levels and increasing p16 and p19 levels. This reduces the sensitivity of stem cells to mitogenic signals by inhibiting cyclin-CDK complexes. As a result, either stem cells cannot enter the cell cycle, or cell division slows in many tissues.\n\nExtrinsic regulation is made by signals from the niche, where stem cells are found, which is able to promote quiescent state and cell cycle activation in somatic stem cells. Asymmetric division is characteristic of somatic stem cells, maintaining the reservoir of stem cells in the tissue and production of specialized cells of the same.\n\nStem cells show an elevated therapeutic potential, mainly in hemato-oncologic pathologies, such as leukemia and lymphomas. Little groups of stem cells were found into tumours, calling cancer stem cells. There are evidences that these cells promote tumor growth and metastasis.\n\nThe oocyte is the female cell involved in reproduction. There is a close relationship between the oocyte and the surrounding follicular cells which is crucial to the development of both. GDF9 and BMP15 produced by the oocyte bind to BMPR2 receptors on follicular cells activating SMADs 2/3, ensuring follicular development. Concomitantly, oocyte growth is initiated by binding of KITL to its receptor KIT in the oocyte, leading to the activation of PI3K/Akt pathway, allowing oocyte survival and development. During embryogenesis, oocytes initiate meiosis and stop in prophase I. This arrest is maintained by elevated levels of cAMP within the oocyte. It was recently suggested that cGMP cooperates with cAMP to maintain the cell cycle arrest. During meitotic maturation, the LH peak that precedes ovulation activates MAPK pathway leading to gap junction disruption and breakdown of communication between the oocyte and the follicular cells. PDE3A is activated and degrades cAMP, leading to cell cycle progression and oocyte maturation. The LH surge also leads to the production of progesterone and prostaglandins that induce the expression of ADAMTS1 and other proteases, as well as their inhibitors. This will lead to degradation of the follicular wall, but limiting the damage and ensuring that the rupture occurs in the appropriate location, releasing the oocyte into the Fallopian tubes. Oocyte activation depends on fertilization by sperm. It is initiated with sperm's attraction induced by prostaglandins produced by the oocyte, which will create a gradient that will influence the sperm's direction and velocity. After fusion with the oocyte, PLC ζ of the spermatozoa is released into the oocyte leading to an increase in Ca2+ levels that will activate CaMKII which will degrade MPF, leading to the resumption of meiosis. The increased Ca levels will induce the exocytosis of cortical granules that degrade ZP receptors, used by sperm to penetrate the oocyte, blocking polyspermy. Deregulation of these pathways will lead to several diseases like, oocyte maturation failure syndrome which results in infertility. Increasing our molecular knowledge of oocyte development mechanisms could improve the outcome of assisted reproduction procedures, facilitating conception.\n\nSpermatozoon is the male gamete. After ejaculation this cell is not mature, so it can't fertilize the oocyte. To have the ability to fertilize the female gamete, this cell suffers capacitation and acrosome reaction in female reproductive tract. The signaling pathways best described for spermatozoon involve these processes. The cAMP/PKA signaling pathway leads to sperm cells capacitation; however, adenilyl cyclase in sperm cells is different from the somatic cells. Adenilyl cyclase in spermatozoon doesn't recognize G proteins, so it is stimulated by bicarbonate and Ca ions. Then, it converts ATP into cAMP, which activates PKA. PKA leads to protein tyrosine phosphorylation.\nPhospholipase C (PLC) is involved in acrosome reaction. ZP3 is a glycoprotein present in zona pelucida and it interacts with receptors in spermatozoon. So, ZP3 can activate G protein coupled receptors and tyrosine kinase receptors, that leads to production of PLC. PLC cleaves the phospholipid phosphatidylinositol 4,5-bisphosphate (PIP2) into diacyl glycerol (DAG) and inositol 1,4,5-trisphosphate (IP3). IP3 is released as a soluble structure into the cytosol and DAG remains bound to the membrane. IP3 binds to IP3 receptors, present in acrosome membrane. In addition, calcium and DAG together work to activate protein kinase C, which goes on to phosphorylate other molecules, leading to altered cellular activity. These actions cause an increase in cytosolic concentration of Ca that leads to dispersion of actin and consequently promotes plasmatic membrane and outer acrosome membrane fusion.\nProgesterone is a steroid hormone produced in cumulus oophorus. In somatic cells it binds to receptors in nucleus; however, in spermatozoon its receptors are present in plasmatic membrane. This hormone activates AKT that leads to activation of other protein kinases, involved in capacitation and acrosome reaction.\nWhen ROS (reactive oxygen species) are present in high concentration, they can affect the physiology of cells, but when they are present in moderated concentration they are important for acrosome reaction and capacitation. ROS can interact with cAMP/PKA and progesterone pathway, stimulating them. ROS also interacts with ERK pathway that leads to activation of Ras, MEK and MEK-like proteins. These proteins activate protein tyrosine kinase (PTK) that phosphorylates various proteins important for capacitation and acrosome reaction.\n\nVarious signalling pathways, as FGF, WNT and TGF-β pathways, regulate the processes involved in embryogenesis.\n\nFGF (Fibroblast Growth Factor) ligands bind to receptors tyrosine kinase, FGFR (Fibroblast Growth Factor Receptors), and form a stable complex with co-receptors HSPG (Heparan Sulphate Proteoglycans) that will promote autophosphorylation of the intracellular domain of FGFR and consequent activation of four main pathways: MAPK/ERK, PI3K, PLCγ and JAK/STAT.\nThe WNT pathway allows β-catenin function in gene transcription, once the interaction between WNT ligand and G protein-coupled receptor Frizzled inhibits GSK-3 (Glycogen Synthase Kinase-3) and thus formation of β-catenin destruction complex. Although there is some controversy about the effects of this pathway in embryogenesis, it is thought that WNT signalling induces primitive streak, mesoderm and endoderm formation.\nIn TGF-β (Transforming Growth Factor β) pathway, BMP (Bone Morphogenic Protein), Activin and Nodal ligands bind to their receptors and activate Smads that bind to DNA and promote gene transcription. Activin is necessary for mesoderm and specially endoderm differentiation, and Nodal and BMP are involved in embryo patterning. BMP is also responsible for formation of extra-embryonic tissues before and during gastrulation, and for early mesoderm differentiation, when Activin and FGF pathways are activated.\n\nPathway building has been performed by individual groups studying a network of interest (e.g., immune signaling pathway) as well as by large bioinformatics consortia (e.g., the Reactome Project) and commercial entities (e.g., Ingenuity Systems). Pathway building is the process of identifying and integrating the entities, interactions, and associated annotations, and populating the knowledge base. Pathway construction can have either a data-driven objective (DDO) or a knowledge-driven objective (KDO). Data-driven pathway construction is used to generate relationship information of genes or proteins identified in a specific experiment such as a microarray study. Knowledge-driven pathway construction entails development of a detailed pathway knowledge　base for particular domains of interest, such as a cell type, disease, or system. The curation process of a biological pathway entails identifying and structuring content, mining information manually and/or computationally, and assembling a knowledgebase using appropriate software tools. A schematic illustrating the major steps involved in the data-driven and knowledge-driven construction processes.\n\nFor either DDO or KDO pathway construction, the first step is to mine pertinent information from relevant information sources about the entities and interactions. The information retrieved is assembled using appropriate formats, information standards, and pathway building tools to obtain a pathway prototype. The pathway is further refined to include context-specific annotations such as species, cell/tissue type, or disease type. The pathway can then be verified by the domain experts and updated by the curators based on appropriate feedback. Recent attempts to improve knowledge integration have led to refined classifications of cellular entities, such as GO, and to the assembly of structured knowledge repositories. Data repositories, which contain information regarding sequence data, metabolism, signaling, reactions, and interactions are a major source of information for pathway building. A few useful databases are described in the following table.\nLegend: Y – Yes, N – No; BIND – Biomolecular Interaction Network Database, DIP – Database of Interacting Proteins, GNPV – Genome Network Platform Viewer, HPRD = Human Protein Reference Database, MINT – Molecular Interaction database, MIPS – Munich Information center for Protein Sequences, UNIHI – Unified Human Interactome, OPHID – Online Predicted Human Interaction Database, EcoCyc – Encyclopaedia of E. Coli Genes and Metabolism, MetaCyc – aMetabolic Pathway database, KEGG – Kyoto Encyclopedia of Genes and Genomes, PANTHER – Protein Analysis Through Evolutionary Relationship database, STKE – Signal Transduction Knowledge Environment, PID – The Pathway Interaction Database, BioPP – Biological Pathway Publisher. A comprehensive list of resources can be found at http://www.pathguide.org.\n\nThe increasing amount of genomic and molecular information is the basis for understanding higher-order biological systems, such as the cell and the organism, and their interactions with the environment, as well as for medical, industrial and other practical applications. The KEGG resource (http://www.genome.jp/kegg/) provides a reference knowledge base for linking genomes to biological systems, categorized as building blocks in the genomic space (KEGG GENES), the chemical space (KEGG LIGAND), wiring diagrams of interaction networks and reaction networks (KEGG PATHWAY), and ontologies for pathway reconstruction (BRITE database).\nThe KEGG PATHWAY database is a collection of manually drawn pathway maps for metabolism, genetic information processing, environmental information processing such as signal transduction, ligand–receptor interaction and cell communication, various other cellular processes and human diseases, all based on extensive survey of published literature.\n\nGene Map Annotator and Pathway Profiler (GenMAPP) (http://www.genmapp.org/) a free, open-source, stand-alone computer program is designed for organizing, analyzing, and sharing genome scale data in the context of biological pathways. GenMAPP database support multiple gene annotations and species as well as custom species database creation for a potentially unlimited number of species. Pathway resources are expanded by utilizing homology information to translate pathway content between species and extending existing pathways with data derived from conserved protein interactions and coexpression. A new mode of data visualization including time-course, single nucleotide polymorphism (SNP), and splicing, has been implemented with GenMAPP database to support analysis of complex data. GenMAPP also offers innovative ways to display and share data by incorporating HTML export of analyses for entire sets of pathways as organized web pages (http://www.genmapp.org/tutorials/Converting-MAPPs-between-species.pdf). In short, GenMAPP provides a means to rapidly interrogate complex experimental data for pathway-level changes in a diverse range of organisms.\n\nGiven the genetic makeup of an organism, the complete set of possible reactions constitutes its reactome. Reactome, located at http://www.reactome.org is a curated, peer-reviewed resource of human biological processes/pathway data. The basic unit of the Reactome database is a reaction; reactions are then grouped into causal chains to form pathways The Reactome data model allows us to represent many diverse processes in the human system, including the pathways of intermediary metabolism, regulatory pathways, and signal transduction, and high-level processes, such as the cell cycle. Reactome provides a qualitative framework, on which quantitative data can be superimposed. Tools have been developed to facilitate custom data entry and annotation by expert biologists, and to allow visualization and exploration of the finished dataset as an interactive process map. Although the primary curational domain is pathways from Homo sapiens, electronic projections of human pathways onto other organisms are regularly created via putative orthologs, thus making Reactome relevant to model organism research communities. The database is publicly available under open source terms, which allows both its content and its software infrastructure to be freely used and redistributed. Studying whole transcriptional profiles and cataloging protein–protein interactions has yielded much valuable biological information, from the genome or proteome to the physiology of an organism, an organ, a tissue or even a single cell. The Reactome database containing a framework of possible reactions which, when combined with expression and enzyme kinetic data, provides the infrastructure for quantitative models, therefore, an integrated view of biological processes, which links such gene products and can be systematically mined by using bioinformatics applications. Reactome data available in a variety of standard formats, including BioPAX, SBML and PSI-MI, and also enable data exchange with other pathway databases, such as the Cycs, KEGG and amaze, and molecular interaction databases, such as BIND and HPRD. The next data release will cover apoptosis, including the death receptor signaling pathways, and the Bcl2 pathways, as well as pathways involved in hemostasis. Other topics currently under development include several signaling pathways, mitosis, visual phototransduction and hematopoeisis. In summary, Reactome provides high-quality curated summaries of fundamental biological processes in humans in a form of biologist-friendly visualization of pathways data, and is an open-source project.\n\nIn the post-genomic age, high-throughput sequencing and gene/protein profiling techniques have transformed biological research by enabling comprehensive monitoring of a biological system, yielding a list of differentially expressed genes or proteins, which is useful in identifying genes that may have roles in a given phenomenon or phenotype. With DNA microarrays and genome-wide gene engineering, it is possible to screen global gene expression profiles to contribute a wealth of genomic data to the public domain. With RNA interference, it is possible to distill the inferences contained in the experimental literature and primary databases into knowledge bases that consist of annotated representations of biological pathways. In this case, individual genes and proteins are known to be involved in biological processes, components, or structures, as well as how and where gene products interact with each other. Pathway-oriented approaches for analyzing microarray data, by grouping long lists of individual genes, proteins, and/or other biological molecules according to the pathways they are involved in into smaller sets of related genes or proteins, which reduces the complexity, have proven useful for connecting genomic data to specific biological processes and systems. Identifying active pathways that differ between two conditions can have more explanatory power than a simple list of different genes or proteins. In addition, a large number of pathway analytic methods exploit pathway knowledge in public repositories such as Gene Ontology (GO) or Kyoto Encyclopedia of Genes and Genomes (KEGG), rather than inferring pathways from molecular measurements. Furthermore, different research focuses have given the word \"pathway\" different meanings. For example, 'pathway' can denote a metabolic pathway involving a sequence of enzyme-catalyzed reactions of small molecules, or a signaling pathway involving a set of protein phosphorylation reactions and gene regulation events. Therefore, the term \"pathway analysis\" has a very broad application. For instance, it can refer to the analysis physical interaction networks (e.g., protein–protein interactions), kinetic simulation of pathways, and steady-state pathway analysis (e.g., flux-balance analysis), as well as its usage in the inference of pathways from expression and sequence data. Several functional enrichment analysis tools and algorithms have been developed to enhance data interpretation. The existing knowledge base–driven pathway analysis methods in each generation have been summarized in recent literature.\n\nA program package MatchMiner was used to scan HUGO names for cloned genes of interest are scanned, then are input into GoMiner, which leveraged the GO to identify the biological processes, functions and components represented in the gene profile. Also, Database for Annotation, Visualization, and Integrated Discovery (DAVID) and KEGG database can be used for the analysis of microarray expression data and the analysis of each GO biological process (P), cellular component (C), and molecular function (F) ontology. In addition, DAVID tools can be used to analyze the roles of genes in metabolic pathways and show the biological relationships between genes or gene-products and may represent metabolic pathways. These two databases also provide bioinformatics tools online to combine specific biochemical information on a certain organism and facilitate the interpretation of biological meanings for experimental data. By using a combined approach of Microarray-Bioinformatic technologies, a potential metabolic mechanism contributing to colorectal cancer (CRC) has been demonstrated Several environmental factors may be involved in a series of points along the genetic pathway to CRC. These include genes associated with bile acid metabolism, glycolysis metabolism and fatty acid metabolism pathways, supporting a hypothesis that some metabolic alternations observed in colon carcinoma may occur in the development of CRC.\n\nCellular models are instrumental in dissecting a complex pathological process into simpler molecular events. Parkinson's disease (PD) is multifactorial and clinically heterogeneous; the aetiology of the sporadic (and most common) form is still unclear and only a few molecular mechanisms have been clarified so far in the neurodegenerative cascade. In such a multifaceted picture, it is particularly important to identify experimental models that simplify the study of the different networks of proteins and genes involved. Cellular models that reproduce some of the features of the neurons that degenerate in PD have contributed to many advances in our comprehension of the pathogenic flow of the disease. In particular, the pivotal biochemical pathways (i.e. apoptosis and oxidative stress, mitochondrial impairment and dysfunctional mitophagy, unfolded protein stress and improper removal of misfolded proteins) have been widely explored in cell lines, challenged with toxic insults or genetically modified. The central role of a-synuclein has generated many models aiming to elucidate its contribution to the dysregulation of various cellular processes. Classical cellular models appear to be the correct choice for preliminary studies on the molecular action of new drugs or potential toxins and for understanding the role of single genetic factors. Moreover, the availability of novel cellular systems, such as cybrids or induced pluripotent stem cells, offers the chance to exploit the advantages of an in vitro investigation, although mirroring more closely the cell population being affected.\n\nSynaptic degeneration and death of nerve cells are defining features of Alzheimer's disease (AD), the most prevalent age-related neurodegenerative disorders. In AD, neurons in the hippocampus and basal forebrain (brain regions that subserve learning and memory functions) are selectively vulnerable. Studies of postmortem brain tissue from AD people have provided evidence for increased levels of oxidative stress, mitochondrial dysfunction and impaired glucose uptake in vulnerable neuronal populations. Studies of animal and cell culture models of AD suggest that increased levels of oxidative stress (membrane lipid peroxidation, in particular) may disrupt neuronal energy metabolism and ion homeostasis, by impairing the function of membrane ion-motive ATPases, glucose and glutamate transporters. Such oxidative and metabolic compromise may thereby render neurons vulnerable to excitotoxicity and apoptosis. Recent studies suggest that AD can manifest systemic alterations in energy metabolism (e.g., increased insulin resistance and dysregulation of glucose metabolism). Emerging evidence that dietary restriction can forestall the development of AD is consistent with a major \"metabolic\" component to these disorders, and provides optimism that these devastating brain disorders of aging may be largely preventable.\n\n", "id": "1872854", "title": "Biochemical cascade"}
{"url": "https://en.wikipedia.org/wiki?curid=1740374", "text": "Thrifty phenotype\n\nThe thrifty phenotype hypothesis says that reduced fetal growth is strongly associated with a number of chronic conditions later in life. This increased susceptibility results from adaptations made by the fetus in an environment limited in its supply of nutrients. The thrifty phenotype is a component of the Fetal Origins Hypothesis. These chronic conditions include coronary heart disease, stroke, diabetes, and hypertension.\n\nProponents of this idea say that in poor nutritional conditions, a pregnant woman can modify the development of her unborn child such that it will be prepared for survival in an environment in which resources are likely to be short, resulting in a thrifty phenotype (Hales & Barker, 1992). It is sometimes called Barker's hypothesis, after Professor David J. P. Barker, researching at the University of Southampton who published the theory in 1990.\n\nThe thrifty phenotype hypothesis suggests that early-life metabolic adaptations help in survival of the organism by selecting an appropriate trajectory of growth in response to environmental cues. Recently, some scientists have proposed that the thrifty phenotype prepares the organism for its likely adult environment in long term.\n\nHowever, environmental changes during early development may result in the selected trajectory becoming inappropriate, resulting in adverse effects on health. This paradox generates doubts about whether the thrifty phenotype is adaptive for human offspring. Thus, the thrifty phenotype should be considered as the capacity of all offspring to respond to environmental cues during early ontogenetic development. It has been suggested that the thrifty phenotype is the consequence of three unlike adaptive processes: maternal effects, niche construction and developmental plasticity, which all are influenced by the brain. While developmental plasticity demonstrates an adaptation by the offspring, niche construction and parental effects are result of parental selections rather than offspring fitness. Therefore, the thrifty phenotype can be described as a manipulation of offspring phenotype for the benefit of maternal fitness. The information that enters offspring phenotype during early development mirror the mother’s own developmental experience and the quality of the environment during her own maturation rather than predicting the possible future environment of the offspring\n\nMany human diseases in adulthood are related to growth patterns during early life, determining early-life nutrition as the underlying mechanism. Individuals with a thrifty phenotype will have \"a smaller body size, a lowered metabolic rate and a reduced level of behavioural activity… adaptations to an environment that is chronically short of food\" (Bateson & Martin, 1999). Those with a thrifty phenotype who actually develop in an affluent environment may be more prone to metabolic disorders, such as obesity and type II diabetes, whereas those who have received a positive maternal forecast will be adapted to good conditions and therefore better able to cope with rich diets. This idea (Barker, 1992) is now widely (if not universally) accepted and is a source of concern for societies undergoing a transition from sparse to better nutrition (Robinson, 2001).\n\nRisk factors of thrifty phenotype include advanced maternal age and placental insufficiency.\n\nThe ability to conserve, acquire and expend energy is believed to be an innate, ancient trait that is imbedded in the genome in a way that is quite protected against mutations. These changes are also believed to possibly be inherited across generations. Leptin has been identified as a possible gene for the acquisition of these thrifty traits.\n\nOn a larger anatomic scale, the molecular mechanisms are broadly caused by a suboptimal environment in the reproductive tract or maternal physiological adaptations to pregnancy.\n\n\n", "id": "1740374", "title": "Thrifty phenotype"}
{"url": "https://en.wikipedia.org/wiki?curid=19916613", "text": "Radical (chemistry)\n\nIn chemistry, a radical (more precisely, a free radical) is an atom, molecule, or ion that has an unpaired valence electron.\nWith some exceptions, these unpaired electrons make free radicals highly chemically reactive towards other substances, or even towards themselves: their molecules will often spontaneously dimerize or polymerize if they come in contact with each other. Most radicals are reasonably stable only at very low concentrations in inert media or in a vacuum.\n\nA notable example of a free radical is the hydroxyl radical (HO•), a molecule that has one unpaired electron on the oxygen atom. Two other examples are triplet oxygen and triplet carbene (:) which have two unpaired electrons. In contrast, the hydroxyl anion () is not a radical, since the unpaired electron is resolved by the addition of an electron; singlet oxygen and singlet carbene are not radicals as the two electrons are paired.\n\nFree radicals may be created in a number of ways, including synthesis with very dilute or rarefied reagents, reactions at very low temperatures, or breakup of larger molecules. The latter can be affected by any process that puts enough energy into the parent molecule, such as ionizing radiation, heat, electrical discharges, electrolysis, and chemical reactions. Radicals are intermediate stages in many chemical reactions.\n\nFree radicals play an important role in combustion, atmospheric chemistry, polymerization, plasma chemistry, biochemistry, and many other chemical processes. In living organisms, the free radicals superoxide and nitric oxide and their reaction products regulate many processes, such as control of vascular tone and thus blood pressure. They also play a key role in the intermediary metabolism of various biological compounds. Such radicals can even be messengers in a process dubbed redox signaling. A radical may be trapped within a \"solvent cage\" or be otherwise bound.\n\nUntil late in the 20th century the word \"radical\" was used in chemistry to indicate any connected group of atoms, such as a methyl group or a carboxyl, whether it was part of a larger molecule or a molecule on its own. The qualifier \"free\" was then needed to specify the unbound case. Following recent nomenclature revisions, a part of a larger molecule is now called a functional group or substituent, and \"radical\" now implies \"free\". However, the old nomenclature may still appear in some books.\n\nThe term radical was already in use when the now obsolete radical theory was developed. Louis-Bernard Guyton de Morveau introduced the phrase \"radical\" in 1785 and the phrase was employed by Antoine Lavoisier in 1789 in his Traité Élémentaire de Chimie. A radical was then identified as the root base of certain acids (the Latin word \"radix\" meaning \"root\"). Historically, the term \"radical\" in radical theory was also used for bound parts of the molecule, especially when they remain unchanged in reactions. These are now called functional groups. For example, methyl alcohol was described as consisting of a methyl \"radical\" and a hydroxyl \"radical\". Neither are radicals in the modern chemical sense, as they are permanently bound to each other, and have no unpaired, reactive electrons; however, they can be observed as radicals in mass spectrometry when broken apart by irradiation with energetic electrons.\n\nIn a modern context the first organic (carbon–containing) free radical identified was triphenylmethyl radical, (CH)C•. This species was discovered by Moses Gomberg in 1900 at the University of Michigan USA. In 1933 Morris Kharash and Frank Mayo proposed that free radicals were responsible for anti-Markovnikov addition of hydrogen bromide to allyl bromide.\n\nIn chemical equations, free radicals are frequently denoted by a dot placed immediately to the right of the atomic symbol or molecular formula as follows:\n\nRadical reaction mechanisms use single-headed arrows to depict the movement of single electrons:\n\nThe homolytic cleavage of the breaking bond is drawn with a 'fish-hook' arrow to distinguish from the usual movement of two electrons depicted by a standard curly arrow. The second electron of the breaking bond also moves to pair up with the attacking radical electron; this is not explicitly indicated in this case.\n\nFree radicals also take part in radical addition and radical substitution as reactive intermediates. Chain reactions involving free radicals can usually be divided into three distinct processes. These are \"initiation\", \"propagation\", and \"termination\".\n\nThe formation of radicals may involve the breaking of covalent bonds by homolysis, a process that requires significant amounts of energy. Such energies are known as homolytic bond dissociation energies, usually abbreviated as \"Δ\"H\"°\". Splitting H into 2H•, for example, requires a Δ\"H\"° of +435 kJ·mol, while splitting Cl into 2Cl• requires a Δ\"H\"° of +243 kJ·mol.\n\nThe energy needed to break a specific bond (generally covalent) between two atoms known as bond energy is a result of all the relative attractions and repulsions between the atoms of the molecule, however the most relevant are the bond's atoms and the immediate neighbors. As an approximation the most important parameters that influence the bonding between two atoms in a molecule are the mutual energy match and overlap of covalent orbitals and the repulsion between nonbonding orbitals. Likewise, radicals requiring more energy to form are less stable than those requiring less energy. An additional barrier can be the selection rule. Propagation, however, is very exothermic.\n\nRadical formation through homolytic bond cleavage most often happens between two atoms of similar electronegativity; in organic chemistry, this is often between the O–O bond in peroxide species or between O–N bonds. Radicals may also be formed by single-electron oxidation or reduction of an atom or molecule: an example is the production of superoxide by the electron transport chain. Early studies in organometallic chemistry – especially F. A. Paneth and K. Hahnfeld's studies of tetra-alkyl lead species during the 1930s – supported the heterolytic fission of bonds and a radical-based mechanism. Although radical ions do exist, most species are electrically neutral.\n\nAlthough radicals are generally short-lived due to their reactivity, there are long-lived radicals. These are categorized as follows:\n\nThe prime example of a stable radical is molecular dioxygen (O). Another common example is nitric oxide (NO). Organic radicals can be long lived if they occur in a conjugated π system, such as the radical derived from α-tocopherol (vitamin E). There are also hundreds of examples of thiazyl radicals, which show low reactivity and remarkable thermodynamic stability with only a very limited extent of π resonance stabilization.\n\nPersistent radical compounds are those whose longevity is due to steric crowding around the radical center, which makes it physically difficult for the radical to react with another molecule. Examples of these include Gomberg's triphenylmethyl radical, Fremy's salt (Potassium nitrosodisulfonate, (KSO)NO·), aminoxyls, (general formula RNO·) such as TEMPO, TEMPOL, nitronyl nitroxides, and azephenylenyls and radicals derived from PTM (perchlorophenylmethyl radical) and TTM (tris(2,4,6-trichlorophenyl)methyl radical). Persistent radicals are generated in great quantity during combustion, and \"may be responsible for the oxidative stress resulting in cardiopulmonary disease and probably cancer that has been attributed to exposure to airborne fine particles.\"\n\nDiradicals are molecules containing two radical centers. Multiple radical centers can exist in a molecule. Atmospheric oxygen naturally exists as a diradical in its ground state as triplet oxygen. The low reactivity of atmospheric oxygen is due to its diradical state. Non-radical states of dioxygen are actually less stable than the diradical. The relative stability of the oxygen diradical is primarily due to the spin-forbidden nature of the triplet-singlet transition required for it to grab electrons, i.e., \"oxidize\". The diradical state of oxygen also results in its paramagnetic character, which is demonstrated by its attraction to an external magnet. Diradicals can also occur in metal-oxo complexes, lending themselves for studies of spin forbidden reactions in transition metal chemistry.\n\nRadical alkyl intermediates are stabilized by similar physical processes to carbocations: as a general rule, the more substituted the radical center is, the more stable it is. This directs their reactions. Thus, formation of a tertiary radical (RC·) is favored over secondary (RHC·), which is favored over primary (RHC·). Likewise, radicals next to functional groups such as carbonyl, nitrile, and ether are more stable than tertiary alkyl radicals.\n\nRadicals attack double bonds. However, unlike similar ions, such radical reactions are not as much directed by electrostatic interactions. For example, the reactivity of nucleophilic ions with α,β-unsaturated compounds (C=C–C=O) is directed by the electron-withdrawing effect of the oxygen, resulting in a partial positive charge on the carbonyl carbon. There are two reactions that are observed in the ionic case: the carbonyl is attacked in a direct addition to carbonyl, or the vinyl is attacked in conjugate addition, and in either case, the charge on the nucleophile is taken by the oxygen. Radicals add rapidly to the double bond, and the resulting α-radical carbonyl is relatively stable; it can couple with another molecule or be oxidized. Nonetheless, the electrophilic/neutrophilic character of radicals has been shown in a variety of instances. One example is the alternating tendency of the copolymerization of maleic anhydride (electrophilic) and styrene (slightly nucleophilic).\n\nIn intramolecular reactions, precise control can be achieved despite the extreme reactivity of radicals. In general, radicals attack the closest reactive site the most readily. Therefore, when there is a choice, a preference for five-membered rings is observed: four-membered rings are too strained, and collisions with carbons six or more atoms away in the chain are infrequent.\n\nTriplet carbenes and nitrenes, which are diradicals, have distinctive chemistry.\n\nA familiar free-radical reaction is combustion. The oxygen molecule is a stable diradical, best represented by ·O-O·. Because spins of the electrons are parallel, this molecule is stable. While the ground state of oxygen is this unreactive spin-unpaired (triplet) diradical, an extremely reactive spin-paired (singlet) state is available. For combustion to occur, the energy barrier between these must be overcome. This barrier can be overcome by heat, requiring high temperatures. The triplet-singlet transition is also \"forbidden\". This presents an additional barrier to the reaction. It also means molecular oxygen is relatively unreactive at room temperature except in the presence of a catalytic heavy atom such as iron or copper.\n\nCombustion consists of various radical chain reactions that the singlet radical can initiate. The flammability of a given material strongly depends on the concentration of free radicals that must be obtained before initiation and propagation reactions dominate leading to combustion of the material. Once the combustible material has been consumed, termination reactions again dominate and the flame dies out. As indicated, promotion of propagation or termination reactions alters flammability. For example, because lead itself deactivates free radicals in the gasoline-air mixture, tetraethyl lead was once commonly added to gasoline. This prevents the combustion from initiating in an uncontrolled manner or in unburnt residues (engine knocking) or premature ignition (preignition).\n\nWhen a hydrocarbon is burned, a large number of different oxygen radicals are involved. Initially, hydroperoxyl radical (HOO·) are formed. These then react further to give organic hydroperoxides that break up into hydroxyl radicals (HO·).\n\nIn addition to combustion, many polymerization reactions involve free radicals. As a result, many plastics, enamels, and other polymers are formed through radical polymerization. For instance, drying oils and alkyd paints harden due to radical crosslinking by oxygen from the atmosphere.\n\nRecent advances in radical polymerization methods, known as living radical polymerization, include:\nThese methods produce polymers with a much narrower distribution of molecular weights.\n\nThe most common radical in the lower atmosphere is molecular dioxygen. Photodissociation of source molecules produces other free radicals. In the lower atmosphere, the most important examples of free radical production are the photodissociation of nitrogen dioxide to give an oxygen atom and nitric oxide (see below), which plays a key role in smog formation—and the photodissociation of ozone to give the excited oxygen atom O(1D) (see below). The net and return reactions are also shown ( and , respectively).\n\nIn the upper atmosphere, a particularly important source of radicals is the photodissociation of normally unreactive chlorofluorocarbons (CFCs) by solar ultraviolet radiation, or by reactions with other stratospheric constituents (see eq. 1 below). These reactions give off the chlorine radical, Cl•, which reacts with ozone in a catalytic chain reaction ending in Ozone depletion and regeneration of the chlorine radical, allowing it to reparticipate in the reaction (see – below). Such reactions are believed to be the primary cause of depletion of the ozone layer (the net result is shown in below), and this is why the use of chlorofluorocarbons as refrigerants has been restricted.\n\nFree radicals play an important role in a number of biological processes. Many of these are necessary for life, such as the intracellular killing of bacteria by phagocytic cells such as granulocytes and macrophages. Researchers have also implicated free radicals in certain cell signalling processes, known as redox signaling. Some of these signaling molecules involve the free radical-induce peroxidation of tissue stores of polyunsaturated fatty acids such as linoleic acid, arachidonic acid, and docosahexaenoic acid. For example, free radical attack of linoleic acid produces a series of 13-Hydroxyoctadecadienoic acids and 9-Hydroxyoctadecadienoic acids which may act to regulate localized tissue inflammatory and/or healing responses, pain perception, and the proliferation of malignant cells. Free radical attacks on arachidonic acid and docosahexaenoic acid produce a similar but broader array of signaling products.\n\nBy a DPPH method on free radical inhibitor abilities of the methanolic extracts of plant sample of Cannabis sativa, were found to be 52.98%, where those of ascorbic acid is 80.76% and quercetin 72.13%, respectively. Cannabis sativa showed that extracts have the proton donating ability and could serve as free radical inhibitor or scavengers, acting possibly as primary antioxidants.\n\nThe two most important oxygen-centered free radicals are superoxide and hydroxyl radical. They derive from molecular oxygen under reducing conditions. However, because of their reactivity, these same free radicals can participate in unwanted side reactions resulting in cell damage. Excessive amounts of these free radicals can lead to cell injury and death, which may contribute to many diseases such as cancer, stroke, myocardial infarction, diabetes and major disorders. Many forms of cancer are thought to be the result of reactions between free radicals and DNA, potentially resulting in mutations that can adversely affect the cell cycle and potentially lead to malignancy. Some of the symptoms of aging such as atherosclerosis are also attributed to free-radical induced oxidation of cholesterol to 7-ketocholesterol. In addition free radicals contribute to alcohol-induced liver damage, perhaps more than alcohol itself. Free radicals produced by cigarette smoke are implicated in inactivation of alpha 1-antitrypsin in the lung. This process promotes the development of emphysema.\n\nFree radicals may also be involved in Parkinson's disease, senile and drug-induced deafness, schizophrenia, and Alzheimer's. The classic free-radical syndrome, the iron-storage disease hemochromatosis, is typically associated with a constellation of free-radical-related symptoms including movement disorder, psychosis, skin pigmentary melanin abnormalities, deafness, arthritis, and diabetes mellitus. The free-radical theory of aging proposes that free radicals underlie the aging process itself. Similarly, the process of mitohormesis suggests that repeated exposure to free radicals may extend life span.\n\nBecause free radicals are necessary for life, the body has a number of mechanisms to minimize free-radical-induced damage and to repair damage that occurs, such as the enzymes superoxide dismutase, catalase, glutathione peroxidase and glutathione reductase. In addition, antioxidants play a key role in these defense mechanisms. These are often the three vitamins, vitamin A, vitamin C and vitamin E and polyphenol antioxidants. Furthermore, there is good evidence indicating that bilirubin and uric acid can act as antioxidants to help neutralize certain free radicals. Bilirubin comes from the breakdown of red blood cells' contents, while uric acid is a breakdown product of purines. Too much bilirubin, though, can lead to jaundice, which could eventually damage the central nervous system, while too much uric acid causes gout.\n\nReactive oxygen species or ROS are species such as superoxide, hydrogen peroxide, and hydroxyl radical, commonly associated with cell damage. ROS form as a natural by-product of the normal metabolism of oxygen and have important roles in cell signaling.\n\nOxybenzone has been found to form free radicals in sunlight, and therefore may be associated with cell damage as well. This only occurred when it was combined with other ingredients commonly found in sunscreens, like titanium oxide and octyl methoxycinnamate.\n\nROS attack the polyunsaturated fatty acid, linoleic acid, to form a series of 13-Hydroxyoctadecadienoic acid and 9-Hydroxyoctadecadienoic acid products that serve as signaling molecules that may trigger responses that counter the tissue injury which caused their formation. ROS attacks other polyunsaturated fatty acids, e.g. arachidonic acid and docosahexaenoic acid, to produce a similar series of signaling products.\n\nIn most fields of chemistry, the historical definition of radicals contends that the molecules have nonzero spin. However, in fields including spectroscopy, chemical reaction, and astrochemistry, the definition is slightly different. Gerhard Herzberg, who won the Nobel prize for his research into the electron structure and geometry of radicals, suggested a looser definition of free radicals: \"any transient (chemically unstable) species (atom, molecule, or ion)\". The main point of his suggestion is that there are many chemically unstable molecules that have zero spin, such as C, C, CH and so on. This definition is more convenient for discussions of transient chemical processes and astrochemistry; therefore researchers in these fields prefer to use this loose definition.\n\nFree radical diagnostic techniques include:\n\n\n", "id": "19916613", "title": "Radical (chemistry)"}
{"url": "https://en.wikipedia.org/wiki?curid=1783345", "text": "Bioerosion\n\nBioerosion describes the erosion of hard ocean substrates – and less often terrestrial substrates – by living organisms. Marine bioerosion can be caused by mollusks, polychaete worms, phoronids, sponges, crustaceans, echinoids, and fish; it can occur on coastlines, on coral reefs, and on ships; its mechanisms include biotic boring, drilling, rasping, and scraping. On dry land, bioerosion is typically performed by pioneer plants or plant-like organisms such as lichen, and mostly chemical (e.g. by acidic secretions on limestone) or mechanical (e.g. by roots growing into cracks) in nature.\n\nBioerosion of coral reefs generates the fine and white coral sand characteristic of tropical islands. The coral is converted to sand by internal bioeroders such as algae, fungi, bacteria (microborers) and sponges (Clionaidae), bivalves (including \"Lithophaga\"), sipunculans, polychaetes, acrothoracican barnacles and phoronids, generating extremely fine sediment with diameters of 10 to 100 micrometres. External bioeroders include sea urchins (such as \"Diadema\") and chitons. These forces in concert produce a great deal of erosion. Sea urchin erosion of calcium carbonate has been reported in some reefs at annual rates exceeding 20 kg/m². \nFish also erode coral while eating algae. Parrotfish cause a great deal of bioerosion using well developed jaw muscles, tooth armature, and a pharyngeal mill, to grind ingested material into sand-sized particles. Bioerosion of coral reef aragonite by parrotfish can range from 1017.7±186.3 kg/yr (0.41±0.07 m³/yr) for \"Chlorurus gibbus\" and 23.6±3.4 kg/yr (9.7 10-³±1.3 10-³ m²/yr) for \"Chlorurus sordidus\" (Bellwood, 1995).\n\nBioerosion is also well known in the fossil record on shells and hardgrounds (Bromley, 1970), with traces of this activity stretching back well into the Precambrian (Taylor & Wilson, 2003). Macrobioerosion, which produces borings visible to the naked eye, shows two distinct evolutionary radiations. One was in the Middle Ordovician (the Ordovician Bioerosion Revolution; see Wilson & Palmer, 2006) and the other in the Jurassic (see Taylor & Wilson, 2003; Bromley, 2004; Wilson, 2007). Microbioerosion also has a long fossil record and its own radiations (see Glaub & Vogel, 2004; Glaub et al., 2007).\n\n\n\n\n", "id": "1783345", "title": "Bioerosion"}
{"url": "https://en.wikipedia.org/wiki?curid=737798", "text": "Hydroxyl radical\n\nThe hydroxyl radical, OH, is the neutral form of the hydroxide ion (OH). Hydroxyl radicals are highly reactive (easily becoming hydroxyl groups) and consequently short-lived; however, they form an important part of radical chemistry. Most notably hydroxyl radicals are produced from the decomposition of hydroperoxides (ROOH) or, in atmospheric chemistry, by the reaction of excited atomic oxygen with water. It is also an important radical formed in radiation chemistry, since it leads to the formation of hydrogen peroxide and oxygen, which can enhance corrosion and SCC in coolant systems subjected to radioactive environments. Hydroxyl radicals are also produced during UV-light dissociation of HO (suggested in 1879) and likely in Fenton chemistry, where trace amounts of reduced transition metals catalyze peroxide-mediated oxidations of organic compounds.\n\nIn organic synthesis, hydroxyl radicals are most commonly generated by photolysis of 1-hydroxy-2(1\"H\")-pyridinethione.\n\nThe hydroxyl radical is often referred to as the \"detergent\" of the troposphere because it reacts with many pollutants, decomposing them through \"cracking\", often acting as the first step to their removal. It also has an important role in eliminating some greenhouse gases like methane and ozone. The rate of reaction with the hydroxyl radical often determines how long many pollutants last in the atmosphere, if they do not undergo photolysis or are rained out. For instance methane, which reacts relatively slowly with hydroxyl radical, has an average lifetime of >5 years and many CFCs have lifetimes of 50 years or more. Pollutants, such as larger hydrocarbons, can have very short average lifetimes of less than a few hours.\n\nThe first reaction with many volatile organic compounds (VOCs) is the removal of a hydrogen atom, forming water and an alkyl radical (R).\nThe alkyl radical will typically react rapidly with oxygen forming a peroxy radical.\n\nThe fate of this radical in the troposphere is dependent on factors such as the amount of sunlight, pollution in the atmosphere and the nature of the alkyl radical that formed it.\n\nHydroxyl radicals can occasionally be produced as a byproduct of immune action. Macrophages and microglia most frequently generate this compound when exposed to very specific pathogens, such as certain bacteria. The destructive action of hydroxyl radicals has been implicated in several neurological autoimmune diseases such as HAND when immune cells become over-activated and toxic to neighboring healthy cells.\n\nThe hydroxyl radical can damage virtually all types of macromolecules: carbohydrates, nucleic acids (mutations), lipids (lipid peroxidation), and amino acids (e.g. conversion of Phe to \"m\"-Tyrosine and \"o\"-Tyrosine). The hydroxyl radical has a very short \"in vivo\" half-life of approximately 10 seconds and a high reactivity. This makes it a very dangerous compound to the organism.\n\nUnlike superoxide, which can be detoxified by superoxide dismutase, the hydroxyl radical cannot be eliminated by an enzymatic reaction.\n\nThe hydroxyl OH radical is one of the main chemical species controlling the oxidizing capacity of the global Earth atmosphere. This oxidizing reactive species has a major impact on the concentrations and distribution of greenhouse gases and pollutants in the Earth atmosphere. It is the most widespread oxidizer in the troposphere, the lowest part of the atmosphere. Understanding OH variability is important to evaluating human impacts on the atmosphere and climate. The OH species has a lifetime in the Earth atmosphere of less than one second. Understanding the role of OH in the oxidation process of methane (CH) present in the atmosphere to first carbon monoxide (CO) and then carbon dioxide (CO) is important for assessing the residence time of this greenhouse gas, the overall carbon budget of the troposphere, and its influence on the process of global warming. The lifetime of OH radicals in the Earth atmosphere is very short, therefore OH concentrations in the air are very low and very sensitive techniques are required for its direct detection. Global average hydroxyl radical concentrations have been measured indirectly by analyzing methyl chloroform (CHCCl) present in the air. The results obtained by Montzka \"et al.\" (2011) shows that the interannual variability in OH estimated from CHCCl measurements is small, indicating that global OH is generally well buffered against perturbations. This small variability is consistent with measurements of methane and other trace gases primarily oxidized by OH, as well as global photochemical model calculations.\n\nIn 2014, researchers reported their discovery of a \"hole\" or absence of hydroxyl throughout the entire depth of the troposphere across a large region of the tropical West Pacific. They suggested that this hole is permitting large quantities of ozone-degrading chemicals to reach the stratosphere, and that this may be significantly reinforcing ozone depletion in the polar regions with potential consequences for the climate of the Earth.\n\nThe first experimental evidence for the presence of 18 cm absorption lines of the hydroxyl (OH) radical in the radio absorption spectrum of Cassiopeia A was obtained by Weinreb et al. (Nature, Vol. 200, pp. 829, 1963) based on observations made during the period October 15–29, 1963.\n\nOH is a diatomic molecule. The electronic angular momentum along the molecular axis is +1 or −1, and the electronic spin angular momentum \"S\" = . Because of the orbit-spin coupling, the spin angular momentum can be oriented in parallel or anti arallel directions to the orbital angular momentum, producing the splitting into Π and Π states. The Π ground state of OH is split by lambda doubling interaction (an interaction between the nuclei rotation and the unpaired electron motion around its orbit). Hyperfine interaction with the unpaired spin of the proton further splits the levels.\n\nIn order to study gas phase interstellar chemistry, it is convenient to distinguish two types of interstellar clouds: diffuse clouds, with and , and dense clouds, with and density . Ion chemical routes in both dense and diffuse clouds have been established for some works (Hartquist, \"Molecular Astrophysics\", 1990).\n\nThe OH radical is linked with the production of HO in molecular clouds. Studies of OH distribution in Taurus Molecular Cloud-1 (TMC-1) suggest that in dense gas, OH is mainly formed by dissociative recombination of HO. Dissociative recombination is the reaction in which a molecular ion recombines with an electron and dissociates into neutral fragments. Important formation mechanisms for OH are:\n\nExperimental data on association reactions of H and OH suggest that radiative association involving atomic and diatomic neutral radicals may be considered as an effective mechanism for the production of small neutral molecules in the interstellar clouds. The formation of O occurs in the gas phase via the neutral exchange reaction between O and OH, which is also the main sink for OH in dense regions.\n\nWe can see that atomic oxygen takes part both in the production and destruction of OH, so the abundance of OH depends mainly on the H abundance. Then, important chemical pathways leading from OH radicals are:\n\nRate constants can be derived from the dataset published in a website. Rate constants have the form:\n\nThe following table has the rate constants calculated for a typical temperature in a dense cloud .\nFormation rates \"r\" can be obtained using the rate constants \"k\"(\"T\") and the abundances of the reactants species C and D:\n\nwhere [Y] represents the abundance of the species Y. In this approach, abundances were taken from \"The UMIST database for astrochemistry 2006\", and the values are relatives to the H density. Following table shows the ratio \"r\"/\"r\" in order to get a view of the most important reactions.\n\nThe results suggest that (1a) reaction is the most prominent reaction in dense clouds. It is in concordance with Harju et al. 2000.\n\nThe next table shows the results by doing the same procedure for destruction reaction:\nResults shows that reaction 1A is the main sink for OH in dense clouds.\n\nDiscoveries of the microwave spectra of a considerable number of molecules prove the existence of rather complex molecules in the interstellar clouds, and provides the possibility to study dense clouds, which are obscured by the dust they contain. The OH molecule has been observed in the interstellar medium since 1963 through its 18-cm transitions. In the subsequent years OH was observed by its rotational transitions at far infrared wavelengths, mainly in the Orion region. Because each rotational level of OH is split in by lambda doubling, astronomers can observe a wide variety of energy states from the ground state.\n\nVery high densities are required to thermalize the rotational transitions of OH, so it is difficult to detect far-infrared emission lines from a quiescent molecular cloud. Even at H densities of 10 cm, dust must be optically thick at infrared wavelengths. But the passage of a shock wave through a molecular cloud is precisely the process which can bring the molecular gas out of equilibrium with the dust, making observations of far-infrared emission lines possible. A moderately fast shock may produce a transient raise in the OH abundance relative to hydrogen. So, it is possible that far-infrared emission lines of OH can be a good diagnostic of shock conditions.\n\nDiffuse clouds are of astronomical interest because they play a primary role in the evolution and thermodynamics of ISM. Observation of the abundant atomic hydrogen in 21 cm has shown good signal-to-noise ratio in both emission and absorption. Nevertheless, HI observations have a fundamental difficulty when they are directed at low mass regions of the hydrogen nucleus, as the center part of a diffuse cloud: the thermal width of the hydrogen lines are of the same order as the internal velocities of structures of interest, so cloud components of various temperatures and central velocities are indistinguishable in the spectrum. Molecular line observations in principle do not suffer from this problem. Unlike HI, molecules generally have excitation temperature \"T\" ≪ \"T\", so that emission is very weak even from abundant species. CO and OH are the most easily studied candidate molecules. CO has transitions in a region of the spectrum (wavelength < 3 mm) where there are not strong background continuum sources, but OH has the 18 cm emission, line convenient for absorption observations. Observation studies provide the most sensitive means of detections of molecules with subthermal excitation, and can give the opacity of the spectral line, which is a central issue to model the molecular region.\n\nStudies based in the kinematic comparison of OH and HI absorption lines from diffuse clouds are useful in determining their physical conditions, specially because heavier elements provide higher velocity resolution.\n\nOH masers, a type of astrophysical maser, were the first masers to be discovered in space and have been observed in more environments than any other type of maser.\n\nIn the Milky Way, OH masers are found in stellar masers (evolved stars), interstellar masers (regions of massive star formation), or in the interface between supernova remnants and molecular material. Interstellar OH masers are often observed from molecular material surrounding ultracompact H II regions (UC H II). But there are masers associated with very young stars that have yet to create UC H II regions. This class of OH masers appears to form near the edges of very dense material, place where HO masers form, and where total densities drop rapidly and UV radiation form young stars can dissociate the HO molecules. So, observations of OH masers in these regions, can be an important way to probe the distribution of the important HO molecule in interstellar shocks at high spatial resolutions.\n\nHydroxyl radicals play a key role in the oxidative destruction of organic pollutant using a series of methodologies collectively known as advanced oxidation processes (AOPs). The destruction of pollutants in AOPs is based on the non-selective reaction of hydroxyl radicals on organic compounds. It is highly effective against a series of pollutants including pesticides, pharmaceutical compounds, dyes, etc.\n\n\n", "id": "737798", "title": "Hydroxyl radical"}
{"url": "https://en.wikipedia.org/wiki?curid=13745014", "text": "Xenohormesis\n\nXenohormesis explains how certain molecules such as plant polyphenols, which indicate stress in the plants, can have a longevity-conferring effect in consumers of plant (i.e. mammals). It was first used in the paper \"Small molecules that regulate lifespan: evidence for xenohormesis\". by David Sinclair and colleagues from Harvard Medical School. Further studies then picked up the term.\n\nIf the plants an animal is eating are under stress, their increased polyphenol content may signal forthcoming famine conditions. It could be advantageous for the animal to begin to react—i.e. to hunker down to prepare for the lean times to come. The effects researchers have observed from resveratrol may be just such a response.\n\n", "id": "13745014", "title": "Xenohormesis"}
{"url": "https://en.wikipedia.org/wiki?curid=31756786", "text": "Sclerobiont\n\nSclerobionts are collectively known as organisms living in or on any kind of hard substrate (Taylor and Wilson, 2003). A few examples of sclerobionts include \"Entobia\" borings, \"Gastrochaenolites\" borings, \"Oichnus\" borings, \"Talpina\" borings, serpulids, encrusting oysters, encrusting foraminiferans, \"Stomatopora\" bryozoans, and \"“Berenicea”\" bryozoans. \n\nBioerosion\n", "id": "31756786", "title": "Sclerobiont"}
{"url": "https://en.wikipedia.org/wiki?curid=24544", "text": "Photosynthesis\n\nPhotosynthesis is a process used by plants and other organisms to convert light energy into chemical energy that can later be released to fuel the organisms' activities (energy transformation). This chemical energy is stored in carbohydrate molecules, such as sugars, which are synthesized from carbon dioxide and water – hence the name \"photosynthesis\", from the Greek φῶς, \"phōs\", \"light\", and σύνθεσις, \"synthesis\", \"putting together\". In most cases, oxygen is also released as a waste product. Most plants, most algae, and cyanobacteria perform photosynthesis; such organisms are called photoautotrophs. Photosynthesis is largely responsible for producing and maintaining the oxygen content of the Earth's atmosphere, and supplies all of the organic compounds and most of the energy necessary for life on Earth.\n\nAlthough photosynthesis is performed differently by different species, the process always begins when energy from light is absorbed by proteins called reaction centres that contain green chlorophyll pigments. In plants, these proteins are held inside organelles called chloroplasts, which are most abundant in leaf cells, while in bacteria they are embedded in the plasma membrane. In these light-dependent reactions, some energy is used to strip electrons from suitable substances, such as water, producing oxygen gas. The hydrogen freed by the splitting of water is used in the creation of two further compounds that serve as short-term stores of energy, enabling its transfer to drive other reactions: these compounds are reduced nicotinamide adenine dinucleotide phosphate (NADPH) and adenosine triphosphate (ATP), the \"energy currency\" of cells.\n\nIn plants, algae and cyanobacteria, long-term energy storage in the form of sugars is produced by a subsequent sequence of reactions called the Calvin cycle; some bacteria use different mechanisms, such as the reverse Krebs cycle, to achieve the same end. In the Calvin cycle, atmospheric carbon dioxide is incorporated into already existing organic carbon compounds, such as ribulose bisphosphate (RuBP). Using the ATP and NADPH produced by the light-dependent reactions, the resulting compounds are then reduced and removed to form further carbohydrates, such as glucose.\n\nThe first photosynthetic organisms probably evolved early in the evolutionary history of life and most likely used reducing agents such as hydrogen or hydrogen sulfide, rather than water, as sources of electrons. Cyanobacteria appeared later; the excess oxygen they produced contributed directly to the oxygenation of the Earth, which rendered the evolution of complex life possible. Today, the average rate of energy capture by photosynthesis globally is approximately 130 terawatts, which is about three times the current power consumption of human civilization.\nPhotosynthetic organisms also convert around 100–115 thousand million metric tonnes of carbon into biomass per year.\n\nPhotosynthetic organisms are photoautotrophs, which means that they are able to synthesize food directly from carbon dioxide and water using energy from light. However, not all organisms that use light as a source of energy carry out photosynthesis; photoheterotrophs use organic compounds, rather than carbon dioxide, as a source of carbon. In plants, algae, and cyanobacteria, photosynthesis releases oxygen. This is called \"oxygenic photosynthesis\" and is by far the most common type of photosynthesis used by living organisms. Although there are some differences between oxygenic photosynthesis in plants, algae, and cyanobacteria, the overall process is quite similar in these organisms. There are also many varieties of anoxygenic photosynthesis, used mostly by certain types of bacteria, which consume carbon dioxide but do not release oxygen.\n\nCarbon dioxide is converted into sugars in a process called carbon fixation; photosynthesis captures energy from sunlight to convert carbon dioxide into carbohydrate. Carbon fixation is an endothermic redox reaction. In general outline, photosynthesis is the opposite of cellular respiration; in the latter, glucose and other compounds are oxidized to produce carbon dioxide and water, and to release chemical energy (an exothermic reaction) to drive the organism's metabolism. The two processes, reduction of carbon dioxide to carbohydrate and then later oxidation of the carbohydrate, are distinct: photosynthesis and cellular respiration take place through a different sequence of chemical reactions and in different cellular compartments.\n\nThe general equation for photosynthesis as first proposed by Cornelius van Niel is therefore:\n\nSince water is used as the electron donor in oxygenic photosynthesis, the equation for this process is:\n\nThis equation emphasizes that water is both a reactant in the light-dependent reaction and a product of the light-independent reaction, but canceling \"n\" water molecules from each side gives the net equation:\n\nOther processes substitute other compounds (such as arsenite) for water in the electron-supply role; for example some microbes use sunlight to oxidize arsenite to arsenate: The equation for this reaction is:\n\nPhotosynthesis occurs in two stages. In the first stage, \"light-dependent reactions\" or \"light reactions\" capture the energy of light and use it to make the energy-storage molecules ATP and NADPH. During the second stage, the \"light-independent reactions\" use these products to capture and reduce carbon dioxide.\n\nMost organisms that utilize oxygenic photosynthesis use visible light for the light-dependent reactions, although at least three use shortwave infrared or, more specifically, far-red radiation.\n\nSome organisms employ even more radical variants of photosynthesis. Some archea use a simpler method that employs a pigment similar to those used for vision in animals. The bacteriorhodopsin changes its configuration in response to sunlight, acting as a proton pump. This produces a proton gradient more directly, which is then converted to chemical energy. The process does not involve carbon dioxide fixation and does not release oxygen, and seems to have evolved separately from the more common types of photosynthesis.\n\nIn photosynthetic bacteria, the proteins that gather light for photosynthesis are embedded in cell membranes. In its simplest form, this involves the membrane surrounding the cell itself. However, the membrane may be tightly folded into cylindrical sheets called thylakoids, or bunched up into round vesicles called \"intracytoplasmic membranes\". These structures can fill most of the interior of a cell, giving the membrane a very large surface area and therefore increasing the amount of light that the bacteria can absorb.\n\nIn plants and algae, photosynthesis takes place in organelles called chloroplasts. A typical plant cell contains about 10 to 100 chloroplasts. The chloroplast is enclosed by a membrane. This membrane is composed of a phospholipid inner membrane, a phospholipid outer membrane, and an intermembrane space. Enclosed by the membrane is an aqueous fluid called the stroma. Embedded within the stroma are stacks of thylakoids (grana), which are the site of photosynthesis. The thylakoids appear as flattened disks. The thylakoid itself is enclosed by the thylakoid membrane, and within the enclosed volume is a lumen or thylakoid space. Embedded in the thylakoid membrane are integral and peripheral membrane protein complexes of the photosynthetic system.\n\nPlants absorb light primarily using the pigment chlorophyll. The green part of the light spectrum is not absorbed but is reflected which is the reason that most plants have a green color. Besides chlorophyll, plants also use pigments such as carotenes and xanthophylls. Algae also use chlorophyll, but various other pigments are present, such as phycocyanin, carotenes, and xanthophylls in green algae, phycoerythrin in red algae (rhodophytes) and fucoxanthin in brown algae and diatoms resulting in a wide variety of colors.\n\nThese pigments are embedded in plants and algae in complexes called antenna proteins. In such proteins, the pigments are arranged to work together. Such a combination of proteins is also called a light-harvesting complex.\n\nAlthough all cells in the green parts of a plant have chloroplasts, the majority of those are found in specially adapted structures called leaves. Certain species adapted to conditions of strong sunlight and aridity, such as many Euphorbia and cactus species, have their main photosynthetic organs in their stems. The cells in the interior tissues of a leaf, called the mesophyll, can contain between 450,000 and 800,000 chloroplasts for every square millimeter of leaf. The surface of the leaf is coated with a water-resistant waxy cuticle that protects the leaf from excessive evaporation of water and decreases the absorption of ultraviolet or blue light to reduce heating. The transparent epidermis layer allows light to pass through to the palisade mesophyll cells where most of the photosynthesis takes place.\n\nIn the light-dependent reactions, one molecule of the pigment chlorophyll absorbs one photon and loses one electron. This electron is passed to a modified form of chlorophyll called pheophytin, which passes the electron to a quinone molecule, starting the flow of electrons down an electron transport chain that leads to the ultimate reduction of NADP to NADPH. In addition, this creates a proton gradient (energy gradient) across the chloroplast membrane, which is used by ATP synthase in the synthesis of ATP. The chlorophyll molecule ultimately regains the electron it lost when a water molecule is split in a process called photolysis, which releases a dioxygen (O) molecule as a waste product.\n\nThe overall equation for the light-dependent reactions under the conditions of non-cyclic electron flow in green plants is:\n\nNot all wavelengths of light can support photosynthesis. The photosynthetic action spectrum depends on the type of accessory pigments present. For example, in green plants, the action spectrum resembles the absorption spectrum for chlorophylls and carotenoids with absorption peaks in violet-blue and red light. In red algae, the action spectrum is blue-green light, which allows these algae to use the blue end of the spectrum to grow in the deeper waters that filter out the longer wavelengths (red light) used by above ground green plants. The non-absorbed part of the light spectrum is what gives photosynthetic organisms their color (e.g., green plants, red algae, purple bacteria) and is the least effective for photosynthesis in the respective organisms.\n\nIn plants, light-dependent reactions occur in the thylakoid membranes of the chloroplasts where they drive the synthesis of ATP and NADPH. The light-dependent reactions are of two forms: cyclic and non-cyclic.\n\nIn the non-cyclic reaction, the photons are captured in the light-harvesting antenna complexes of photosystem II by chlorophyll and other accessory pigments (see diagram at right). The absorption of a photon by the antenna complex frees an electron by a process called photoinduced charge separation. The antenna system is at the core of the chlorophyll molecule of the photosystem II reaction center. That freed electron is transferred to the primary electron-acceptor molecule, pheophytin. As the electrons are shuttled through an electron transport chain (the so-called Z-scheme shown in the diagram), it initially functions to generate a chemiosmotic potential by pumping proton cations (H) across the membrane and into the thylakoid space. An ATP synthase enzyme uses that chemiosmotic potential to make ATP during photophosphorylation, whereas NADPH is a product of the terminal redox reaction in the \"Z-scheme\". The electron enters a chlorophyll molecule in Photosystem I. There it is further excited by the light absorbed by that photosystem. The electron is then passed along a chain of electron acceptors to which it transfers some of its energy. The energy delivered to the electron acceptors is used to move hydrogen ions across the thylakoid membrane into the lumen. The electron is eventually used to reduce the co-enzyme NADP with a H to NADPH (which has functions in the light-independent reaction); at that point, the path of that electron ends.\n\nThe cyclic reaction is similar to that of the non-cyclic, but differs in that it generates only ATP, and no reduced NADP (NADPH) is created. The cyclic reaction takes place only at photosystem I. Once the electron is displaced from the photosystem, the electron is passed down the electron acceptor molecules and returns to photosystem I, from where it was emitted, hence the name \"cyclic reaction\".\n\nThe NADPH is the main reducing agent produced by chloroplasts, which then goes on to provide a source of energetic electrons in other cellular reactions. Its production leaves chlorophyll in photosystem I with a deficit of electrons (chlorophyll has been oxidized), which must be balanced by some other reducing agent that will supply the missing electron. The excited electrons lost from chlorophyll from photosystem I are supplied from the electron transport chain by plastocyanin. However, since photosystem II is the first step of the \"Z-scheme\", an external source of electrons is required to reduce its oxidized chlorophyll \"a\" molecules. The source of electrons in green-plant and cyanobacterial photosynthesis is water. Two water molecules are oxidized by four successive charge-separation reactions by photosystem II to yield a molecule of diatomic oxygen and four hydrogen ions; the electrons yielded are transferred to a redox-active tyrosine residue that then reduces the oxidized chlorophyll \"a\" (called P680) that serves as the primary light-driven electron donor in the photosystem II reaction center. That photo receptor is in effect reset and is then able to repeat the absorption of another photon and the release of another photo-dissociated electron. The oxidation of water is catalyzed in photosystem II by a redox-active structure that contains four manganese ions and a calcium ion; this oxygen-evolving complex binds two water molecules and contains the four oxidizing equivalents that are used to drive the water-oxidizing reaction (Dolai's S-state diagrams). Photosystem II is the only known biological enzyme that carries out this oxidation of water. The hydrogen ions released contribute to the transmembrane chemiosmotic potential that leads to ATP synthesis. Oxygen is a waste product of light-dependent reactions, but the majority of organisms on Earth use oxygen for cellular respiration, including photosynthetic organisms.\n\nIn the light-independent (or \"dark\") reactions, the enzyme RuBisCO captures CO from the atmosphere and, in a process called the Calvin-Benson cycle, it uses the newly formed NADPH and releases three-carbon sugars, which are later combined to form sucrose and starch. The overall equation for the light-independent reactions in green plants is\n\nCarbon fixation produces the intermediate three-carbon sugar product, which is then converted to the final carbohydrate products. The simple carbon sugars produced by photosynthesis are then used in the forming of other organic compounds, such as the building material cellulose, the precursors for lipid and amino acid biosynthesis, or as a fuel in cellular respiration. The latter occurs not only in plants but also in animals when the energy from plants is passed through a food chain.\n\nThe fixation or reduction of carbon dioxide is a process in which carbon dioxide combines with a five-carbon sugar, ribulose 1,5-bisphosphate, to yield two molecules of a three-carbon compound, glycerate 3-phosphate, also known as 3-phosphoglycerate. Glycerate 3-phosphate, in the presence of ATP and NADPH produced during the light-dependent stages, is reduced to glyceraldehyde 3-phosphate. This product is also referred to as 3-phosphoglyceraldehyde (PGAL) or, more generically, as triose phosphate. Most (5 out of 6 molecules) of the glyceraldehyde 3-phosphate produced is used to regenerate ribulose 1,5-bisphosphate so the process can continue. The triose phosphates not thus \"recycled\" often condense to form hexose phosphates, which ultimately yield sucrose, starch and cellulose. The sugars produced during carbon metabolism yield carbon skeletons that can be used for other metabolic reactions like the production of amino acids and lipids.\n\nIn hot and dry conditions, plants close their stomata to prevent water loss. Under these conditions, will decrease and oxygen gas, produced by the light reactions of photosynthesis, will increase, causing an increase of photorespiration by the oxygenase activity of ribulose-1,5-bisphosphate carboxylase/oxygenase and decrease in carbon fixation. Some plants have evolved mechanisms to increase the concentration in the leaves under these conditions.\n\nPlants that use the C carbon fixation process chemically fix carbon dioxide in the cells of the mesophyll by adding it to the three-carbon molecule phosphoenolpyruvate (PEP), a reaction catalyzed by an enzyme called PEP carboxylase, creating the four-carbon organic acid oxaloacetic acid. Oxaloacetic acid or malate synthesized by this process is then translocated to specialized bundle sheath cells where the enzyme RuBisCO and other Calvin cycle enzymes are located, and where released by decarboxylation of the four-carbon acids is then fixed by RuBisCO activity to the three-carbon 3-phosphoglyceric acids. The physical separation of RuBisCO from the oxygen-generating light reactions reduces photorespiration and increases fixation and, thus, the photosynthetic capacity of the leaf. plants can produce more sugar than plants in conditions of high light and temperature. Many important crop plants are plants, including maize, sorghum, sugarcane, and millet. Plants that do not use PEP-carboxylase in carbon fixation are called C plants because the primary carboxylation reaction, catalyzed by RuBisCO, produces the three-carbon 3-phosphoglyceric acids directly in the Calvin-Benson cycle. Over 90% of plants use carbon fixation, compared to 3% that use carbon fixation; however, the evolution of in over 60 plant lineages makes it a striking example of convergent evolution.\n\nXerophytes, such as cacti and most succulents, also use PEP carboxylase to capture carbon dioxide in a process called Crassulacean acid metabolism (CAM). In contrast to metabolism, which \"spatially\" separates the fixation to PEP from the Calvin cycle, CAM \"temporally\" separates these two processes. CAM plants have a different leaf anatomy from plants, and fix the at night, when their stomata are open. CAM plants store the mostly in the form of malic acid via carboxylation of phosphoenolpyruvate to oxaloacetate, which is then reduced to malate. Decarboxylation of malate during the day releases inside the leaves, thus allowing carbon fixation to 3-phosphoglycerate by RuBisCO. Sixteen thousand species of plants use CAM.\n\nCyanobacteria possess carboxysomes, which increase the concentration of around RuBisCO to increase the rate of photosynthesis. An enzyme, carbonic anhydrase, located within the carboxysome releases CO from the dissolved hydrocarbonate ions (HCO). Before the CO diffuses out it is quickly sponged up by RuBisCO, which is concentrated within the carboxysomes. HCO ions are made from CO outside the cell by another carbonic anhydrase and are actively pumped into the cell by a membrane protein. They cannot cross the membrane as they are charged, and within the cytosol they turn back into CO very slowly without the help of carbonic anhydrase. This causes the HCO ions to accumulate within the cell from where they diffuse into the carboxysomes. Pyrenoids in algae and hornworts also act to concentrate around rubisco.\n\nThe overall process of photosynthesis takes place in four stages:\n\nPlants usually convert light into chemical energy with a photosynthetic efficiency of 3–6%.\nAbsorbed light that is unconverted is dissipated primarily as heat, with a small fraction (1–2%) re-emitted as chlorophyll fluorescence at longer (redder) wavelengths. This fact allows measurement of the light reaction of photosynthesis by using chlorophyll fluorometers.\n\nActual plants' photosynthetic efficiency varies with the frequency of the light being converted, light intensity, temperature and proportion of carbon dioxide in the atmosphere, and can vary from 0.1% to 8%. By comparison, solar panels convert light into electric energy at an efficiency of approximately 6–20% for mass-produced panels, and above 40% in laboratory devices.\n\nThe efficiency of both light and dark reactions can be measured but the relationship between the two can be complex. For example, the ATP and NADPH energy molecules, created by the light reaction, can be used for carbon fixation or for photorespiration in C plants. Electrons may also flow to other electron sinks. For this reason, it is not uncommon for authors to differentiate between work done under non-photorespiratory conditions and under photorespiratory conditions.\nChlorophyll fluorescence of photosystem II can measure the light reaction, and Infrared gas analyzers can measure the dark reaction. It is also possible to investigate both at the same time using an integrated chlorophyll fluorometer and gas exchange system, or by using two separate systems together. Infrared gas analyzers and some moisture sensors are sensitive enough to measure the photosynthetic assimilation of CO, and of ΔHO using reliable methods CO is commonly measured in μmols/m/s, parts per million or volume per million and H0 is commonly measured in mmol/m/s or in mbars. By measuring CO assimilation, ΔHO, leaf temperature, barometric pressure, leaf area, and photosynthetically active radiation or PAR, it becomes possible to estimate, “A” or carbon assimilation, “E” or transpiration, “gs” or stomatal conductance, and Ci or intracellular CO. However, it is more common to used chlorophyll fluorescence for plant stress measurement, where appropriate, because the most commonly used measuring parameters FV/FM and Y(II) or F/FM’ can be made in a few seconds, allowing the measurement of larger plant populations.\n\nGas exchange systems that offer control of CO levels, above and below ambient, allow the common practice of measurement of A/Ci curves, at different CO levels, to characterize a plant’s photosynthetic response.\nIntegrated chlorophyll fluorometer – gas exchange systems allow a more precise measure of photosynthetic response and mechanisms. While standard gas exchange photosynthesis systems can measure Ci, or substomatal CO levels, the addition of integrated chlorophyll fluorescence measurements allows a more precise measurement of C to replace Ci. The estimation of CO at the site of carboxylation in the chloroplast, or C, becomes possible with the measurement of mesophyll conductance or g using an integrated system.\n\nPhotosynthesis measurement systems are not designed to directly measure the amount of light absorbed by the leaf. But analysis of chlorophyll-fluorescence, P700- and P515-absorbance and gas exchange measurements reveal detailed information about e.g. the photosystems, quantum efficiency and the CO assimilation rates. With some instruments even wavelength-dependency of the photosynthetic efficiency can be analyzed.\n\nA phenomenon known as quantum walk increases the efficiency of the energy transport of light significantly. In the photosynthetic cell of an algae, bacterium, or plant, there are light-sensitive molecules called chromophores arranged in an antenna-shaped structure named a photocomplex. When a photon is absorbed by a chromophore, it is converted into a quasiparticle referred to as an exciton, which jumps from chromophore to chromophore towards the reaction center of the photocomplex, a collection of molecules that traps its energy in a chemical form that makes it accessible for the cell's metabolism. The exciton's wave properties enable it to cover a wider area and try out several possible paths simultaneously, allowing it to instantaneously \"choose\" the most efficient route, where it will have the highest probability of arriving at its destination in the minimum possible time. Because that quantum walking takes place at temperatures far higher than quantum phenomena usually occur, it is only possible over very short distances, due to obstacles in the form of destructive interference that come into play. These obstacles cause the particle to lose its wave properties for an instant before it regains them once again after it is freed from its locked position through a classic \"hop\". The movement of the electron towards the photo center is therefore covered in a series of conventional hops and quantum walks.\n\nEarly photosynthetic systems, such as those in green and purple sulfur and green and purple nonsulfur bacteria, are thought to have been anoxygenic, and used various other molecules as electron donors rather than water. Green and purple sulfur bacteria are thought to have used hydrogen and sulfur as electron donors. Green nonsulfur bacteria used various amino and other organic acids as an electron donor. Purple nonsulfur bacteria used a variety of nonspecific organic molecules. The use of these molecules is consistent with the geological evidence that Earth's early atmosphere was highly reducing at that time.\n\nFossils of what are thought to be filamentous photosynthetic organisms have been dated at 3.4 billion years old.\n\nThe main source of oxygen in the Earth's atmosphere derives from oxygenic photosynthesis, and its first appearance is sometimes referred to as the oxygen catastrophe. Geological evidence suggests that oxygenic photosynthesis, such as that in cyanobacteria, became important during the Paleoproterozoic era around 2 billion years ago. Modern photosynthesis in plants and most photosynthetic prokaryotes is oxygenic. Oxygenic photosynthesis uses water as an electron donor, which is oxidized to molecular oxygen () in the photosynthetic reaction center.\n\nSeveral groups of animals have formed symbiotic relationships with photosynthetic algae. These are most common in corals, sponges and sea anemones. It is presumed that this is due to the particularly simple body plans and large surface areas of these animals compared to their volumes. In addition, a few marine mollusks \"Elysia viridis\" and \"Elysia chlorotica\" also maintain a symbiotic relationship with chloroplasts they capture from the algae in their diet and then store in their bodies. This allows the mollusks to survive solely by photosynthesis for several months at a time. Some of the genes from the plant cell nucleus have even been transferred to the slugs, so that the chloroplasts can be supplied with proteins that they need to survive.\n\nAn even closer form of symbiosis may explain the origin of chloroplasts. Chloroplasts have many similarities with photosynthetic bacteria, including a circular chromosome, prokaryotic-type ribosome, and similar proteins in the photosynthetic reaction center. The endosymbiotic theory suggests that photosynthetic bacteria were acquired (by endocytosis) by early eukaryotic cells to form the first plant cells. Therefore, chloroplasts may be photosynthetic bacteria that adapted to life inside plant cells. Like mitochondria, chloroplasts possess their own DNA, separate from the nuclear DNA of their plant host cells and the genes in this chloroplast DNA resemble those found in cyanobacteria. DNA in chloroplasts codes for redox proteins such as those found in the photosynthetic reaction centers. The CoRR Hypothesis proposes that this Co-location is required for Redox Regulation.\n\nThe biochemical capacity to use water as the source for electrons in photosynthesis evolved once, in a common ancestor of extant cyanobacteria. The geological record indicates that this transforming event took place early in Earth's history, at least 2450–2320 million years ago (Ma), and, it is speculated, much earlier. Because the Earth's atmosphere contained almost no oxygen during the estimated development of photosynthesis, it is believed that the first photosynthetic cyanobacteria did not generate oxygen. Available evidence from geobiological studies of Archean (>2500 Ma) sedimentary rocks indicates that life existed 3500 Ma, but the question of when oxygenic photosynthesis evolved is still unanswered. A clear paleontological window on cyanobacterial evolution opened about 2000 Ma, revealing an already-diverse biota of blue-green algae. Cyanobacteria remained the principal primary producers of oxygen throughout the Proterozoic Eon (2500–543 Ma), in part because the redox structure of the oceans favored photoautotrophs capable of nitrogen fixation. Green algae joined blue-green algae as the major primary producers of oxygen on continental shelves near the end of the Proterozoic, but it was only with the Mesozoic (251–65 Ma) radiations of dinoflagellates, coccolithophorids, and diatoms did the primary production of oxygen in marine shelf waters take modern form. Cyanobacteria remain critical to marine ecosystems as primary producers of oxygen in oceanic gyres, as agents of biological nitrogen fixation, and, in modified form, as the plastids of marine algae.\n\nAlthough some of the steps in photosynthesis are still not completely understood, the overall photosynthetic equation has been known since the 19th century.\n\nJan van Helmont began the research of the process in the mid-17th century when he carefully measured the mass of the soil used by a plant and the mass of the plant as it grew. After noticing that the soil mass changed very little, he hypothesized that the mass of the growing plant must come from the water, the only substance he added to the potted plant. His hypothesis was partially accurate — much of the gained mass also comes from carbon dioxide as well as water. However, this was a signaling point to the idea that the bulk of a plant's biomass comes from the inputs of photosynthesis, not the soil itself.\n\nJoseph Priestley, a chemist and minister, discovered that, when he isolated a volume of air under an inverted jar, and burned a candle in it, the candle would burn out very quickly, much before it ran out of wax. He further discovered that a mouse could similarly \"injure\" air. He then showed that the air that had been \"injured\" by the candle and the mouse could be restored by a plant.\n\nIn 1778, Jan Ingenhousz, repeated Priestley's experiments. He discovered that it was the influence of sunlight on the plant that could cause it to revive a mouse in a matter of hours.\n\nIn 1796, Jean Senebier, a Swiss pastor, botanist, and naturalist, demonstrated that green plants consume carbon dioxide and release oxygen under the influence of light. Soon afterward, Nicolas-Théodore de Saussure showed that the increase in mass of the plant as it grows could not be due only to uptake of CO but also to the incorporation of water. Thus, the basic reaction by which photosynthesis is used to produce food (such as glucose) was outlined.\n\nCornelis Van Niel made key discoveries explaining the chemistry of photosynthesis. By studying purple sulfur bacteria and green bacteria he was the first to demonstrate that photosynthesis is a light-dependent redox reaction, in which hydrogen reduces carbon dioxide.\n\nRobert Emerson discovered two light reactions by testing plant productivity using different wavelengths of light. With the red alone, the light reactions were suppressed. When blue and red were combined, the output was much more substantial. Thus, there were two photosystems, one absorbing up to 600 nm wavelengths, the other up to 700 nm. The former is known as PSII, the latter is PSI. PSI contains only chlorophyll \"a\", PSII contains primarily chlorophyll \"a\" with most of the available chlorophyll \"b\", among other pigment. These include phycobilins, which are the red and blue pigments of red and blue algae respectively, and fucoxanthol for brown algae and diatoms. The process is most productive when the absorption of quanta are equal in both the PSII and PSI, assuring that input energy from the antenna complex is divided between the PSI and PSII system, which in turn powers the photochemistry.\nRobert Hill thought that a complex of reactions consisting of an intermediate to cytochrome b (now a plastoquinone), another is from cytochrome f to a step in the carbohydrate-generating mechanisms. These are linked by plastoquinone, which does require energy to reduce cytochrome f for it is a sufficient reductant. Further experiments to prove that the oxygen developed during the photosynthesis of green plants came from water, were performed by Hill in 1937 and 1939. He showed that isolated chloroplasts give off oxygen in the presence of unnatural reducing agents like iron oxalate, ferricyanide or benzoquinone after exposure to light. The Hill reaction is as follows:\n\nwhere A is the electron acceptor. Therefore, in light, the electron acceptor is reduced and oxygen is evolved.\n\nSamuel Ruben and Martin Kamen used radioactive isotopes to determine that the oxygen liberated in photosynthesis came from the water.\n\nMelvin Calvin and Andrew Benson, along with James Bassham, elucidated the path of carbon assimilation (the photosynthetic carbon reduction cycle) in plants. The carbon reduction cycle is known as the Calvin cycle, which ignores the contribution of Bassham and Benson. Many scientists refer to the cycle as the Calvin-Benson Cycle, Benson-Calvin, and some even call it the Calvin-Benson-Bassham (or CBB) Cycle.\n\nNobel Prize-winning scientist Rudolph A. Marcus was able to discover the function and significance of the electron transport chain.\n\nOtto Heinrich Warburg and Dean Burk discovered the I-quantum photosynthesis reaction that splits the CO, activated by the respiration.\n\nLouis N.M. Duysens and Jan Amesz discovered that chlorophyll a will absorb one light, oxidize cytochrome f, chlorophyll a (and other pigments) will absorb another light, but will reduce this same oxidized cytochrome, stating the two light reactions are in series.\n\nIn 1893, Charles Reid Barnes proposed two terms, \"photosyntax\" and \"photosynthesis\", for the biological process of \"synthesis of complex carbon compounds out of carbonic acid, in the presence of chlorophyll, under the influence of light\". Over time, the term \"photosynthesis\" came into common usage as the term of choice. Later discovery of anoxygenic photosynthetic bacteria and photophosphorylation necessitated redefinition of the term.\n\nAfter WWII at late 1940 at the University of California, Berkeley, the details of photosynthetic carbon metabolism were sorted out by the chemists Melvin Calvin, Andrew Benson, James Bassham and a score of students and researchers utilizing the carbon-14 isotope and paper chromatography techniques. The pathway of CO2 fixation by the algae \"Chlorella\" in a fraction of a second in light resulted in a 3 carbon molecule called phosphoglyceric acid (PGA). For that original and ground-breaking work, a Nobel Prize in Chemistry was awarded to Melvin Calvin in 1961. In parallel, plant physiologists studied leaf gas exchanges using the new method of infrared gas analysis and a leaf chamber where the net photosynthetic rates ranged from 10 to 13 u mole CO2/square metere.sec., with the conclusion that all terrestrial plants having the same photosynthetic capacities that were light saturated at less than 50% of sunlight.\n\nLater in 1958-1963 at Cornell University, field grown maize was reported to have much greater leaf photosynthetic rates of 40 u mol CO2/square meter.sec and was not saturated at near full sunlight. This higher rate in maize was almost double those observed in other species such as wheat and soybean, indicating that large differences in photosynthesis exist among higher plants. At the University of Arizona, detailed gas exchange research on more than 15 species of monocot and dicot uncovered for the first time that differences in leaf anatomy are crucial factors in differentiating photosynthetic capacities among species. In tropical grasses, including maize, sorghum, sugarcane, Bermuda grass and in the dicot amaranthus, leaf photosynthetic rates were around 38−40 u mol CO2/square meter.sec., and the leaves have two types of green cells, i. e. outer layer of mesophyll cells surrounding a tightly packed cholorophyllous vascular bundle sheath cells. This type of anatomy was termed Kranz anatomy in the 19th century by the botanist Gottlieb Haberlandt while studying leaf anatomy of sugarcane. Plant species with the greatest photosynthetic rates and Kranz anatomy showed no apparent photorespiration, very low CO2 compensation point, high optimum temperature, high stomatal resistances and lower mesophyll resistances for gas diffusion and rates never saturated at full sun light. The research at Arizona was designated Citation Classic by the ISI 1986. These species was later termed C4 plants as the first stable compound of CO2 fixation in light has 4 carbon as malate and aspartate. Other species that lack Kranz anatomy were termed C3 type such as cotton and sunflower, as the first stable carbon compound is the 3-carbon PGA acid. At 1000 ppm CO2 in measuring air, both the C3 and C4 plants had similar leaf photosynthetic rates around 60 u mole CO2/square meter.sec. indicating the suppression of photorespiration in C3 plants.\n\nThere are three main factors affecting photosynthesis and several corollary factors. The three main are:\n\nThe process of photosynthesis provides the main input of free energy into the biosphere, and is one of four main ways in which radiation is important for plant life.\n\nThe radiation climate within plant communities is extremely variable, with both time and space.\n\nIn the early 20th century, Frederick Blackman and Gabrielle Matthaei investigated the effects of light intensity (irradiance) and temperature on the rate of carbon assimilation.\nThese two experiments illustrate several important points: First, it is known that, in general, photochemical reactions are not affected by temperature. However, these experiments clearly show that temperature affects the rate of carbon assimilation, so there must be two sets of reactions in the full process of carbon assimilation. These are the light-dependent 'photochemical' temperature-independent stage, and the light-independent, temperature-dependent stage. Second, Blackman's experiments illustrate the concept of limiting factors. Another limiting factor is the wavelength of light. Cyanobacteria, which reside several meters underwater, cannot receive the correct wavelengths required to cause photoinduced charge separation in conventional photosynthetic pigments. To combat this problem, a series of proteins with different pigments surround the reaction center. This unit is called a phycobilisome.\n\nAs carbon dioxide concentrations rise, the rate at which sugars are made by the light-independent reactions increases until limited by other factors. RuBisCO, the enzyme that captures carbon dioxide in the light-independent reactions, has a binding affinity for both carbon dioxide and oxygen. When the concentration of carbon dioxide is high, RuBisCO will fix carbon dioxide. However, if the carbon dioxide concentration is low, RuBisCO will bind oxygen instead of carbon dioxide. This process, called photorespiration, uses energy, but does not produce sugars.\n\nRuBisCO oxygenase activity is disadvantageous to plants for several reasons:\n\nThe salvaging pathway for the products of RuBisCO oxygenase activity is more commonly known as photorespiration, since it is characterized by light-dependent oxygen consumption and the release of carbon dioxide.\n\n", "id": "24544", "title": "Photosynthesis"}
{"url": "https://en.wikipedia.org/wiki?curid=33548640", "text": "PI curve\n\nThe PI (photosynthesis-irradiance) curve is a graphical representation of the empirical relationship between solar irradiance and photosynthesis. A derivation of the Michaelis–Menten curve, it shows the generally positive correlation between light intensity and photosynthetic rate. Plotted along the \"x\"-axis is the independent variable, light intensity (irradiance), while the \"y\"-axis is reserved for the dependent variable, photosynthetic rate.\n\nThe PI curve can be applied to terrestrial and marine reactions but is most commonly used to explain ocean-dwelling phytoplankton’s photosynthetic response to changes in light intensity. Using this tool to approximate biological productivity is important because phytoplankton contribute ~50% of total global carbon fixation and are important suppliers to the marine food web.\n\nWithin the scientific community, the curve can be referred to as the PI, PE or Light Response Curve. While individual researchers may have their own preferences, all are readily acceptable for use in the literature. Regardless of nomenclature, the photosynthetic rate in question can be described in terms of carbon (C) fixed per unit per time. Since individuals vary in size, it is also useful to normalise C concentration to Chlorophyll a (an important photosynthetic pigment) to account for specific biomass.\n\nAs far back as 1905, marine researchers attempted to develop an equation to be used as the standard in establishing the relationship between solar irradiance and photosynthetic production. Several groups had relative success, but in 1976 a comparison study conducted by Alan Jassby and Trevor Platt, researchers at the Bedford Institute of Oceanography in Dartmouth, Nova Scotia, reached a conclusion that solidified the way in which a PI curve is developed. After evaluating the eight most-used equations, Jassby and Platt revealed that an adaptation of the Michaelis-Menten equation, previously used in enzyme kinetics, best served the relationship demonstration. Their findings were so conclusive that the Michaelis–Menten equation remains the standard for PI curve generation.\n\nThere are two simple derivations of the equation that are commonly used to generate the hyperbolic curve. The first assumes photosynthetic rate increases with increasing light intensity until Pmax is reached and continues to photosynthesise at the maximum rate thereafter.\n\nBoth Pmax and the initial slope of the curve, ΔP/ΔI, are species-specific, and are influenced by a variety of factors, such as nutrient concentration, temperature and the physiological capabilities of the individual. Light intensity is influenced by latitudinal position and undergo daily and seasonal fluxes which will also affect the overall photosynthetic capacity of the individual. These three parameters are predictable and can be used to predetermine the general PI curve a population should follow.\n\nAs can be seen in the graph, two species can have different responses to the same incremental changes in light intensity. Population A (in blue) has an initial rate higher than that of Population B (in red) and also exhibits a stronger rate change to increased light intensities at lower irradiance. Therefore, Population A will dominate in an environment with lower light availability. Although Population B has a slower photosynthetic response to increases in light intensity its Pmax is higher than that of Population A. This allows for eventual population dominance at greater light intensities. There are many determining factors influencing population success; using the PI curve to elicit predictions of rate flux to environmental changes is useful for monitoring phytoplankton bloom dynamics and ecosystem stability.\n\nThe second equation accounts for the phenomenon of photoinhibition. In the upper few meters of the ocean, phytoplankton may be subjected to irradiance levels that damage the chlorophyll-a pigment inside the cell, subsequently decreasing photosynthetic rate. The response curve depicts photoinhibition as a decrease in photosynthetic rate at light intensities stronger than those necessary for achievement of Pmax.\n\nTerms not included in the above equation are:\n\nData sets showing interspecific differences and population dynamics.\n\nThe hyperbolic response between photosynthesis and irradiance, depicted by the PI curve, is important for assessing phytoplankton population dynamics, which influence many aspects of the marine environment.\n\n\n", "id": "33548640", "title": "PI curve"}
{"url": "https://en.wikipedia.org/wiki?curid=33799190", "text": "N-philes\n\nN-philes are group of radical molecules which are specifically attracted to the C=N bonds, defying often the selectivity rules of electrophilic attack. N-philes can often masquerade as electrophiles, where acyl radicals are excellent examples which interact with Pi electrons of aryl groups.\n", "id": "33799190", "title": "N-philes"}
{"url": "https://en.wikipedia.org/wiki?curid=14241428", "text": "Mineralization (biology)\n\nIn biology, mineralization refers to a process where an inorganic substance precipitates in an organic matrix. This may be due to normal biological processes that take place during the life of an organism such as the formation of bones, egg shells, teeth, coral, and other exoskeletons. This term may also refer to abnormal processes that result in kidney and gall stones.\n\nMineralization can be subdivided into different categories depending on the following: the organisms or processes that create chemical conditions necessary for mineral formation, the origin of the substrate at the site of mineral precipitation, and the degree of control that the substrate has on crystal morphology, composition, and growth. These subcategories include: biomineralization, organomineralization, and inorganic mineralization, which can be subdivided further. However, usage of these terms vary widely in scientific literature because there are no standardized definitions.The following definitions are based largely on a paper written by Dupraz et al. (2009), which provided a framework for differentiating these terms.\n\nBiomineralization, biologically-controlled mineralization, occurs when crystal morphology, growth, composition, and location is completely controlled by the cellular processes of a specific organism. Examples include the shells of invertebrates, such as molluscs and brachiopods. Additionally, mineralization of collagen provides the crucial compressive strength for the bones, cartilage, and teeth of vertebrates.\n\nThis type of mineralization includes both biologically-induced mineralization and biologically-influenced mineralization.\n\nInorganic mineralization is a completely abiotic process. Chemical conditions necessary for mineral formation develop via environmental processes, such as evaporation or degassing. Furthermore, the substrate for mineral deposition is abiotic (i.e. contains no organic compounds) and there is no control on crystal morphology or composition. Examples of this type of mineralization include cave formations, such as stalagmites and stalactites.\n\nBiological mineralization can also take place as a result of fossilization. \nSee also calcification.\n\nBone mineralization occurs in human body by cells called osteoblasts.\n", "id": "14241428", "title": "Mineralization (biology)"}
{"url": "https://en.wikipedia.org/wiki?curid=11344743", "text": "Biomolecular engineering\n\nBiomolecular engineering is the application of engineering principles and practices to the purposeful manipulation of molecules of biological origin. Biomolecular engineers integrate knowledge of biological processes with the core knowledge of chemical engineering in order to focus on molecular level solutions to issues and problems in the life sciences related to the environment, agriculture, energy, industry, food production, biotechnology and medicine. Biomolecular engineers purposefully manipulate carbohydrates, proteins, nucleic acids and lipids within the framework of the relation between their structure (see: nucleic acid structure, carbohydrate chemistry, protein structure,), function (see: protein function) and properties and in relation to applicability to such areas as environmental remediation, crop and livestock production, biofuel cells and biomolecular diagnostics. Fundamental attention is given to the thermodynamics and kinetics of molecular recognition in enzymes, antibodies, DNA hybridization, bio-conjugation/bio-immobilization and bioseparations. Attention is also given to the rudiments of engineered biomolecules in cell signaling, cell growth kinetics, biochemical pathway engineering and bioreactor engineering. Biomolecular engineers are leading the major shift towards understanding and controlling the molecular mechanisms that define life as we know it.\n\nDuring World War II, the need for large quantities of penicillin of acceptable quality brought together chemical engineers and microbiologists to focus on penicillin production. This created the right conditions to start a chain of reactions that lead to the creation of the field of biomolecular engineering. Biomolecular engineering was first defined in 1992 by the National Institutes of Health as research at the interface of chemical engineering and biology with an emphasis at the molecular level\". Although first defined as research, biomolecular engineering has since become an academic discipline and a field of engineering practice. Herceptin, a humanized Mab for breast cancer treatment, became the first drug designed by a biomolecular engineering approach and was approved by the FDA. Also, \"Biomolecular Engineering\" was a former name of the journal \"New Biotechnology\".\n\nBio-inspired technologies of the future can help explain biomolecular engineering. Looking at the Moore's law \"Prediction\", in the future quantum and biology-based processors are \"big\" technologies. With the use of biomolecular engineering, the way our processors work can be manipulated in order to function in the same sense a biological cell work. Biomolecular engineering has the potential to become one of the most important scientific disciplines because of its advancements in the analyses of gene expression patterns as well as the purposeful manipulation of many important biomolecules to improve functionality. Research in this field may lead to new drug discoveries, improved therapies, and advancement in new bioprocess technology. With the increasing knowledge of biomolecules, the rate of finding new high-value molecules including but not limited to antibodies, enzymes, vaccines, and therapeutic peptides will continue to accelerate. Biomolecular engineering will produce new designs for therapeutic drugs and high-value biomolecules for treatment or prevention of cancers, genetic diseases, and other types of metabolic diseases. Also, there is anticipation of industrial enzymes that are engineered to have desirable properties for process improvement as well the manufacturing of high-value biomolecular products at a much lower production cost. Using recombinant technology, new antibiotics that are active against resistant strains will also be produced.\n\nBiomolecular engineering deals with the manipulation of many key biomolecules. These include, but are not limited to, proteins, carbohydrates, nucleic acids, and lipids. These molecules are the basic building blocks of life and by controlling, creating, and manipulating their form and function there are many new avenues and advantages available to society. Since every biomolecule is different, there are a number of techniques used to manipulate each one respectively.\n\nProteins are polymers that are made up of amino acid chains linked with peptide bonds. They have four distinct levels of structure: primary, secondary, tertiary, and quaternary. \nPrimary structure refers to the amino acid backbone sequence. Secondary structure focuses on minor conformations that develop as a result of the hydrogen bonding between the amino acid chain. If most of the protein contains intermolecular hydrogen bonds it is said to be fibrillar, and the majority of its secondary structure will be beta sheets. However, if the majority of the orientation contains intramolecular hydrogen bonds, then the protein is referred to as globular and mostly consists of alpha helices. There are also conformations that consist of a mix of alpha helices and beta sheets as well as a beta helixes with an alpha sheets.\n\nThe tertiary structure of proteins deal with their folding process and how the overall molecule is arranged. Finally, a quaternary structure is a group of tertiary proteins coming together and binding.\nWith all of these levels, proteins have a wide variety of places in which they can be manipulated and adjusted. Techniques are used to affect the amino acid sequence of the protein (site-directed mutagenesis), the folding and conformation of the protein, or the folding of a single tertiary protein within a quaternary protein matrix. \nProteins that are the main focus of manipulation are typically enzymes. These are proteins that act as catalysts for biochemical reactions. By manipulating these catalysts, the reaction rates, products, and effects can be controlled. Enzymes and proteins are important to the biological field and research that there are specific subsets of engineering focusing only on proteins and enzymes.\n\nCarbohydrates are another important biomolecule. These are polymers, called polysaccharides, which are made up of chains of simple sugars connected via glycosidic bonds. These monosaccharides consist of a five to six carbon ring that contains carbon, hydrogen, and oxygen - typically in a 1:2:1 ratio, respectively. Common monosaccharides are glucose, fructose, and ribose. When linked together monosaccharides can form disaccharides, oligosaccharides, and polysaccharides: the nomenclature is dependent on the number of monosaccharides linked together. Common dissacharides, two monosaccharides joined together, are sucrose, maltose, and lactose. Important polysaccharides, links of many monosaccharides, are cellulose, starch, and chitin.\n\nCellulose is a polysaccharide made up of beta 1-4 linkages between repeat glucose monomers. It is the most abundant source of sugar in nature and is a major part of the paper industry. \nStarch is also a polysaccharide made up of glucose monomers; however, they are connected via an alpha 1-4 linkage instead of beta. Starches, particularly amylase, are important in many industries, including the paper, cosmetic, and food. \nChitin is a derivation of cellulose, possessing an acetamide group instead of an –OH on one of its carbons. Acetimide group is deacetylated the polymer chain is then called chitosan. Both of these cellulose derivatives are a major source of research for the biomedical and food industries. They have been shown to assist with blood clotting, have antimicrobial properties, and dietary applications. A lot of engineering and research is focusing on the degree of deacetylation that provides the most effective result for specific applications.\n\nNucleic acids are macromolecules that consist of DNA and RNA which are biopolymers consisting of chains of biomolecules. These two molecules are the genetic code and template that make life possible. Manipulation of these molecules and structures causes major changes in function and expression of other macromolecules. Nucleosides are glycosylamines containing a nucleobase bound to either ribose or deoxyribose sugar via a beta-glycosidic linkage. The sequence of the bases determine the genetic code. Nucleotides are nucleosides that are phosphorylated by specific kinases via a phosphodiester bond. Nucleotides are the repeating structural units of nucleic acids. The nucleotides are made of a nitrogenous base, a pentose (ribose for RNA or deoxyribose for DNA), and three phosphate groups. See, Site-directed mutagenesis, recombinant DNA, and ELISAs.\n\nLipids are biomolecules that are made up of glycerol derivatives bonded with fatty acid chains. Glycerol is a simple polyol that has a formula of C3H5(OH)3. Fatty acids are long carbon chains that have a carboxylic acid group at the end. The carbon chains can be either saturated with hydrogen; every carbon bond is occupied by a hydrogen atom or a single bond to another carbon in the carbon chain, or they can be unsaturated; namely, there are double bonds between the carbon atoms in the chain. Common fatty acids include lauric acid, stearic acid, and oleic acid. The study and engineering of lipids typically focuses on the manipulation of lipid membranes and encapsulation. Cellular membranes and other biological membranes typically consist of a phospholipid bilayer membrane, or a derivative thereof. Along with the study of cellular membranes, lipids are also important molecules for energy storage. By utilizing encapsulation properties and thermodynamic characteristics, lipids become significant assets in structure and energy control when engineering molecules.\n\nRecombinant DNA are DNA biomolecules that contain genetic sequences that are not native to the organism’s genome. Using recombinant techniques, it is possible to insert, delete, or alter a DNA sequence precisely without depending on the location of restriction sites. Recombinant DNA is used for a wide range of applications.\n\nThe traditional method for creating recombinant DNA typically involves the use of plasmids in the host bacteria. The plasmid contains a genetic sequence corresponding to the recognition site of a restriction endonuclease, such as EcoR1. After foreign DNA fragments, which have also been cut with the same restriction endonuclease, have been inserted into host cell, the restriction endonuclease gene is expressed by applying heat, or by introducing a biomolecule, such as arabinose. Upon expression, the enzyme will cleave the plasmid at its corresponding recognition site creating sticky ends on the plasmid. Ligases then joins the sticky ends to the corresponding sticky ends of the foreign DNA fragments creating a recombinant DNA plasmid.\n\nAdvances in genetic engineering have made the modification of genes in microbes quite efficient allowing constructs to be made in about a weeks worth of time. It has also made it possible to modify the organism's genome itself. Specifically, use of the genes from the bacteriophage lambda are used in recombination. This mechanism, known as recombineering, utilizes the three proteins Exo, Beta, and Gam, which are created by the genes exo, bet, and gam respectively. Exo is a double stranded DNA exonuclease with 5’ to 3’ activity. It cuts the double stranded DNA leaving 3’ overhangs. Beta is a protein that binds to single stranded DNA and assists homologous recombination by promoting annealing between the homology regions of the inserted DNA and the chromosomal DNA. Gam functions to protect the DNA insert from being destroyed by native nucleases within the cell.\n\nRecombinant DNA can be engineered for a wide variety of purposes. The techniques utilized allow for specific modification of genes making it possible to modify any biomolecule. It can be engineered for laboratory purposes, where it can be used to analyze genes in a given organism. In the pharmaceutical industry, proteins can be modified using recombination techniques. Some of these proteins include human insulin. Recombinant insulin is synthesized by inserting the human insulin gene into \"E. coli\", which then produces insulin for human use. Other proteins, such as human growth hormone, factor VIII, and hepatitis B vaccine are produced using similar means. Recombinant DNA can also be used for diagnostic methods involving the use of the ELISA method. This makes it possible to engineer antigens, as well as the enzymes attached, to recognize different substrates or be modified for bioimmobilization. Recombinant DNA is also responsible for many products found in the agricultural industry. Genetically modified food, such as golden rice, has been engineered to have increased production of vitamin A for use in societies and cultures where dietary vitamin A is scarce. Other properties that have been engineered into crops include herbicide-resistance and insect-resistance.\n\nSite-directed mutagenesis is a technique that has been around since the 1970s. The early days of research in this field yielded discoveries about the potential of certain chemicals such as bisulfite and aminopurine to change certain bases in a gene. This research continued, and other processes were developed to create certain nucleotide sequences on a gene, such as the use of restriction enzymes to fragment certain viral strands and use them as primers for bacterial plasmids. The modern method, developed by Michael Smith in 1978, uses an oligonucleotide that is complementary to a bacterial plasmid with a single base pair mismatch or a series of mismatches.\n\nSite directed mutagenesis is a valuable technique that allows for the replacement of a single base in an oligonucleotide or gene. The basics of this technique involve the preparation of a primer that will be a complementary strand to a wild type bacterial plasmid. This primer will have a base pair mismatch at the site where the replacement is desired. The primer must also be long enough such that the primer will anneal to the wild type plasmid. After the primer anneals, a DNA polymerase will complete the primer. When the bacterial plasmid is replicated, the mutated strand will be replicated as well. The same technique can be used to create a gene insertion or deletion. Often, an antibiotic resistant gene is inserted along with the modification of interest and the bacteria are cultured on an antibiotic medium. The bacteria that were not successfully mutated will not survive on this medium, and the mutated bacteria can easily be cultured.\n\nSite-directed mutagenesis can be useful for many different reasons. A single base pair replacement, could change a codon, and thus replace an amino acid in a protein. This is useful for studying the way certain proteins behave. It is also useful because enzymes can be purposefully manipulated by changing certain amino acids. If an amino acid is changed that is in close proximity to the active site, the kinetic parameters may change drastically, or the enzyme might behave in a different way. Another application of site directed mutagenesis is exchanging an amino acid residue far from the active site with a lysine residue or cysteine residue. These amino acids make it easier to covalently bond the enzyme to a solid surface, which allows for enzyme re-use and use of enzymes in continuous processes. Sometimes, amino acids with non-natural functional groups (such as ketones and azides) are added to proteins These additions may be for ease of bioconjugation, or to study the effects of amino acid changes on the form and function of the proteins. The coupling of site directed mutagenesis and PCR are being utilized to reduce interleukin-6 activity in cancerous cells. The bacteria \"bacillus subtilis\" is often used in site directed mutagenesis. The bacteria secretes an enzyme called subtilisin through the cell wall. Biomolecular engineers can purposely manipulate this gene to essentially make the cell a factory for producing whatever protein the insertion in the gene codes.\n\nBio-immobilization and bio-conjugation is the purposeful manipulation of a biomolecule’s mobility by chemical or physical means to obtain a desired property. Immobilization of biomolecules allows exploiting characteristics of the molecule under controlled environments. For example\n, the immobilization of glucose oxidase on calcium alginate gel beads can be used in a bioreactor. The resulting product will not need purification to remove the enzyme because it will remain linked to the beads in the column. Examples of types of biomolecules that are immobilized are enzymes, organelles, and complete cells. \nBiomolecules can be immobilized using a range of techniques. The most popular are physical entrapment, adsorption, and covalent modification.\n\nBecause immobilization restricts the biomolecule, care must be given to ensure that functionality is not entirely lost. Variables to consider are pH, temperature, solvent choice, ionic strength, orientation of active sites due to conjugation. For enzymes, the conjugation will lower the kinetic rate due to a change in the 3-dimensional structure, so care must be taken to ensure functionality is not lost. \nBio-immobilization is used in technologies such as diagnostic bioassays, biosensors, ELISA, and bioseparations. Interleukin (IL-6) can also be bioimmobilized on biosensors. The ability to observe these changes in IL-6 levels is important in diagnosing an illness. A cancer patient will have elevated IL-6 level and monitoring those levels will allow the physician to watch the disease progress. A direct immobilization of IL-6 on the surface of a biosensor offers a fast alternative to ELISA.\n\nThe polymerase chain reaction (PCR) is a scientific technique that is used to replicate a piece of a DNA molecule by several orders of magnitude. PCR implements a cycle of repeated heated and cooling known as thermal cycling along with the addition of DNA primers and DNA polymerases to selectively replicate the DNA fragment of interest. The technique was developed by Kary Mullis in 1983 while working for the Cetus Corporation. Mullis would go on to win the Nobel Prize in Chemistry in 1993 as a result of the impact that PCR had in many areas such as DNA cloning, DNA sequencing, and gene analysis.\n\nA number of biomolecular engineering strategies have played a very important role in the development and practice of PCR. For instance a crucial step in ensuring the accurate replication of the desired DNA fragment is the creation of the correct DNA primer. The most common method of primer synthesis is by the phosphoramidite method. This method includes the biomolecular engineering of a number of molecules to attain the desired primer sequence. The most prominent biomolecular engineering technique seen in this primer design method is the initial bioimmobilization of a nucleotide to a solid support. This step is commonly done via the formation of a covalent bond between the 3’-hydroxy group of the first nucleotide of the primer and the solid support material.\n\nFurthermore, as the DNA primer is created certain functional groups of nucleotides to be added to the growing primer require blocking to prevent undesired side reactions. This blocking of functional groups as well as the subsequent de-blocking of the groups, coupling of subsequent nucleotides, and eventual cleaving from the solid support are all methods of manipulation of biomolecules that can be attributed to biomolecular engineering. The increase in interleukin levels is directly proportional to the increased death rate in breast cancer patients. PCR paired with Western blotting and ELISA help define the relationship between cancer cells and IL-6.\n\nEnzyme-linked immunosorbent assay is an assay that utilizes the principle of antibody-antigen recognition to test for the presence of certain substances. The three main types of ELISA tests which are indirect ELISA, sandwich ELISA, and competitive ELISA all rely on the fact that antibodies have an affinity for only one specific antigen. Furthermore, these antigens or antibodies can be attached to enzymes which can react to create a colorimetric result indicating the presence of the antibody or antigen of interest. Enzyme linked immunosorbent assays are used most commonly as diagnostic tests to detect HIV antibodies in blood samples to test for HIV, human chorionic gonadotropin molecules in urine to indicate pregnancy, and Mycobacterium tuberculosis antibodies in blood to test patients for tuberculosis. Furthermore, ELISA is also widely used as a toxicology screen to test people's serum for the presence of illegal drugs.\n\nAlthough there are three different types of solid state enzyme-linked immunosorbent assays, all three types begin with the bioimmobilization of either an antibody or antigen to a surface. This bioimmobilization is the first instance of biomolecular engineering that can be seen in ELISA implementation. This step can be performed in a number of ways including a covalent linkage to a surface which may be coated with protein or another substance. The bioimmobilization can also be performed via hydrophobic interactions between the molecule and the surface. Because there are many different types of ELISAs used for many different purposes the biomolecular engineering that this step requires varies depending on the specific purpose of the ELISA.\n\nAnother biomolecular engineering technique that is used in ELISA development is the bioconjugation of an enzyme to either an antibody or antigen depending on the type of ELISA. There is much to consider in this enzyme bioconjugation such as avoiding interference with the active site of the enzyme as well as the antibody binding site in the case that the antibody is conjugated with enzyme. This bioconjugation is commonly performed by creating crosslinks between the two molecules of interest and can require a wide variety of different reagents depending on the nature of the specific molecules.\n\nInterleukin (IL-6) is a signaling protein that has been known to be present during an immune response. The use of the sandwich type ELISA quantifies the presence of this cytokine within spinal fluid or bone marrow samples.\n\nBiomolecular engineering is an extensive discipline with applications in many different industries and fields. As such, it is difficult to pinpoint a general perspective on the Biomolecular engineering profession. The biotechnology industry, however, provides an adequate representation. The biotechnology industry, or biotech industry, encompasses all firms that use biotechnology to produce goods or services or to perform biotechnology research and development. In this way, it encompasses many of the industrial applications of the biomolecular engineering discipline. By examination of the biotech industry, it can be gathered that the principal leader of the industry is the United States, followed by France and Spain. It is also true that the focus of the biotechnology industry and the application of biomolecular engineering is primarily clinical and medical. People are willing to pay for good health, so most of the money directed towards the biotech industry stays in health-related ventures.\n\nScaling up a process involves using data from an experimental-scale operation (model or pilot plant) for the design of a large (scaled-up) unit, of commercial size. Scaling up is a crucial part of commercializing a process. For example, insulin produced by genetically modified Escherichia coli bacteria was initialized on a lab-scale, but to be made commercially viable had to be scaled up to an industrial level. In order to achieve this scale-up a lot of lab data had to be used to design commercial sized units. For example, one of the steps in insulin production involves the crystallization of high purity glargin insulin. In order to achieve this process on a large scale we want to keep the Power/Volume ratio of both the lab-scale and large-scale crystallizers the same in order to achieve homogeneous mixing. \nWe also assume the lab-scale crystallizer has geometric similarity to the large-scale crystallizer. Therefore,\n\nP/V α Nd\n\nwhere d= crystallizer impeller diameter\n\nN= impeller rotation rate\n\nA broad term encompassing all engineering applied to the life sciences. This field of study utilizes the principles of biology along with engineering principles to create marketable products. Some bioengineering applications include:\n\nBiochemistry is the study of chemical processes in living organisms, including, but not limited to, living matter. Biochemical processes govern all living organisms and living processes and the field of biochemistry seeks to understand and manipulate these processes.\n\n\n\nBioelectrical engineering involves the electrical fields generated by living cells or organisms. Examples include the electric potential developed between muscles or nerves of the body. This discipline requires knowledge in the fields of electricity and biology to understand and utilize these concepts to improve or better current bioprocesses or technology.\n\n\nBiomedical engineering is a sub category of bioengineering that uses many of the same principles but focuses more on the medical applications of the various engineering developments. Some applications of biomedical engineering include:\n\n\nChemical engineering is the processing of raw materials into chemical products. It involves preparation of raw materials to produce reactants, the chemical reaction of these reactants under controlled conditions, the separation of products, the recycle of byproducts, and the disposal of wastes. Each step involves certain basic building blocks called “unit operations,” such as extraction, filtration, and distillation. These unit operations are found in all chemical processes. Biomolecular engineering is a subset of Chemical Engineering that applies these same principles to the processing of chemical substances made by living organisms.\n\nThe discipline of biomolecular engineering has become ever more prevalent with the better understanding and advancement of current sciences and technologies. In previous years, biomolecular engineering was not a well-known career path, but the growth in popularity of this subject has resulted in new programs offered to undergraduate and graduate students.\n\nNewly developed and offered undergraduate programs across the United States, often coupled to the chemical engineering program, allow students to achieve a B.S. degree. According to ABET (Accreditation Board for Engineering and Technology), biomolecular engineering curricula \"must provide thorough grounding in the basic sciences including chemistry, physics, and biology, with some content at an advanced level… [and] engineering application of these basic sciences to design, analysis, and control, of chemical, physical, and/or biological processes.\" Common curricula consist of major engineering courses including transport, thermodynamics, separations, and kinetics, with additions of life sciences courses including biology and biochemistry, and including specialized biomolecular courses focusing on cell biology, nano- and biotechnology, biopolymers, etc.\n\nTo further education in biomolecular engineering studies, the option to get an M.S. or Ph.D. is becoming ever more available in various colleges and universities.\n\n\n\n", "id": "11344743", "title": "Biomolecular engineering"}
{"url": "https://en.wikipedia.org/wiki?curid=35215882", "text": "Isothermal microcalorimetry\n\nIsothermal microcalorimetry (IMC) is a laboratory method for real-time monitoring and dynamic analysis of chemical, physical and biological processes. Over a period of hours or days, IMC determines the onset, rate, extent and energetics of such processes for specimens in small ampoules (e.g. 3–20 ml) at a constant set temperature (c. 15 °C–150 °C).\n\nIMC accomplishes this dynamic analysis by measuring and recording vs. elapsed time the net rate of heat flow (μJ/sec = μW) to or from the specimen ampoule, and the cumulative amount of heat (J) consumed or produced.\n\nIMC is a powerful and versatile analytical tool for four closely related reasons:\n\nThe IMC method of studying rates of processes is thus broadly applicable, provides real-time continuous data, and is sensitive. The measurement is simple to make, takes place unattended and is non-interfering (e.g. no fluorescent or radioactive markers are needed).\n\nHowever, there are two main caveats that must be heeded in use of IMC:\n\nIn general, possible applications of IMC are only limited by the imagination of the person who chooses to employ it as an analytical tool and the physical constraints of the method. Besides the two general limitations (main caveats) described above, these constraints include specimen and ampoule size, and the temperatures at which measurements can be made. IMC is generally best suited to evaluating processes which take place over hours or days. IMC has been used in an extremely wide range of applications, and many examples are discussed in this article, supported by references to published literature. Applications discussed range from measurement of slow oxidative degradation of polymers and instability of hazardous industrial chemicals to detection of bacteria in urine and evaluation of the effects of drugs on parasitic worms. \"The present emphasis in this article is applications of the latter type—biology and medicine.\"\n\nCalorimetry is the science of measuring the heat of chemical reactions or physical changes. Calorimetry is performed with a calorimeter.\n\nIsothermal microcalorimetry (IMC) is a laboratory method for real-time, continuous measurement of the heat flow rate (μJ/sec = μW) and cumulative amount of heat (J) consumed or produced at essentially constant temperature by a specimen placed in an IMC instrument. Such heat is due to chemical or physical changes taking place in the specimen. The heat flow is proportional to the aggregate rate of changes taking place at a given time. The aggregate heat produced during a given time interval is proportional to the cumulative amount of aggregate changes which have taken place.\n\nIMC is thus a means for dynamic, quantitative evaluation of the rates and energetics of a broad range of rate processes, including biological processes. A rate process is defined here as a physical and/or chemical change whose progress over time can be described either empirically or by a mathematical model (Bibliography: Glasstone, et al. 1941 and Johnson, et al. 1974 and Rate equation).\n\nThe simplest use of IMC is detecting that one or more rate processes are taking place in a specimen because heat is being produced or consumed at a rate that is greater than the detection limit of the instrument used. This can be a useful, for example, as a general indicator that a solid or liquid material is not inert but instead is changing at a given temperature. In biological specimens containing a growth medium, appearance over time of a detectable and rising heat flow signal is a simple general indicator of the presence of some type of replicating cells.\nHowever, for most applications it is paramount to know, by some means, what process or processes are being measured by monitoring heat flow. In general this entails first having detailed physical, chemical and biological knowledge of the items placed in an IMC ampoule before it is placed in an IMC instrument for evaluation of heat flow over time. It is also then necessary to analyze the ampoule contents after IMC measurements of heat flow have been made for one or more periods of time. Also, logic-based variations in ampoule contents can be used to identify the specific source or sources of heat flow. When rate process and heat flow relationships have been established, it is then possible to rely directly on the IMC data.\n\nWhat IMC can measure in practice depends in part on specimen dimensions, and they are necessarily constrained by instrument design. A given commercial instrument typically accepts specimens of up to a fixed diameter and height. Instruments accepting specimens with dimensions of up to ca. 1 or 2 cm in diameter x ca. 5 cm in height are typical. In a given instrument larger specimens of a given type usually produce greater heat flow signals, and this can augment detection and precision.\n\nFrequently, specimens are simple 3 to 20 ml cylindrical ampoules (Fig. 1) containing materials whose rate processes are of interest—e.g. solids, liquids, cultured cells—or any combination of these or other items expected to result in production or consumption of heat. Many useful IMC measurements can be carried out using simple sealed ampoules, and glass ampoules are common since glass is not prone to undergoing heat-producing chemical or physical changes. However, metal or polymeric ampoules are sometimes employed. Also, instrument/ampoule systems are available which allow injection or controlled through-flow of gasses or liquids and/or provide specimen mechanical stirring.\n\nCommercial IMC instruments allow heat flow measurements at temperatures ranging from ca. 15 °C – 150 °C. The range for a given instrument may be somewhat different.\n\nIMC is extremely sensitive – e.g. heat from slow chemical reactions in specimens weighing a few grams, taking place at reactant consumption rates of a few percent per year, can be detected and quantified in a matter of days. Examples include gradual oxidation of polymeric implant materials and shelf life studies of solid pharmaceutical drug formulations (Applications (Solid Materials)).\n\nAlso the rate of metabolic heat production of e.g. a few thousand living cells, microorganisms or protozoa in culture in an IMC ampoule can be measured. The amount of such metabolic heat can be correlated (through experimentation) with the number of cells or organisms present. Thus, IMC data can be used to monitor in real time the number of cells or organisms present and the net rate of growth or decline in this number (Applications (Biology and Medicine)).\n\nAlthough some non-biological applications of IMC are discussed (Applications (Solid Materials)) the present emphasis in this article is on the use of IMC in connection with biological processes (Applications (Biology and Medicine)).\n\nA graphic display of a common type of IMC data is shown in Fig. 2. At the top is a plot of recorded heat flow (μ W=μ J/s) vs. time from a specimen in a sealed ampoule, due to an exothermic rate process which begins, accelerates, reaches a peak heat flow and then subsides. Such data are directly useful (e.g. detection of a process and its duration under fixed conditions) but the data are also easily assessed mathematically to determine process parameters. For example, Fig. 2 also shows an integration of the heat flow data, giving accumulated heat (J) vs. time. As shown, parameters such as the maximum growth (heat generation) rate of the process, and the duration time of the lag phase before the process reaches maximum heat can be calculated from the integrated data. Calculations using heat flow rate data stored as computer files are easily automated. Analyzing IMC data in this manner to determine growth parameters has important applications the life sciences (Applications (Biology and Medicine)). Also, heat flow rates obtained at a series of temperatures can be used to obtain the activation energy of the process being evaluated (Hardison et al. 2003).\n\nLavoisier and Laplace are credited with creating and using the first isothermal calorimeter in ca. 1780 (Bibliography: Lavoisier A & Laplace PS 1780). Their instrument employed ice to produce a relatively constant temperature in a confined space. They realized that when they placed a heat-producing specimen on the ice (e.g. a live animal), the mass of liquid water produced by the melting ice was directly proportional to the heat produced by the specimen.\n\nMany modern IMC instrument designs stem from work done in Sweden in the late 1960s and early 1970s (Wadsö 1968, Suurkuusk & Wadsö 1974). This work took advantage of the parallel development of solid state electronic devices—particularly commercial availability of small thermoelectric effect (Peltier-Seebeck) devices for converting heat flow into voltage—and vice versa.\n\nIn the 1980s, multi-channel designs emerged (Suurkuusk 1982), which allow parallel evaluation of multiple specimens. This greatly increased the power and usefulness of IMC and led to efforts to fine-tune the method (Thorén et al. 1989). Much of the further design and development done in the 1990s was also accomplished in Sweden by Wadsö and Suurkuusk and their colleagues. This work took advantage of the parallel development of personal computer technology which greatly augmented the ability to easily store, process and interpret heat flow vs. time data.\n\nInstrument development work since the 1990s has taken further advantage of the continued development of solid-state electronics and personal computer technology. This has created IMC instruments of increasing sensitivity and stability, numbers of parallel channels, and even greater ability to conveniently record, store and rapidly process IMC data. In connection with wider use, substantial attention has been paid to creating standards for describing the performance of IMC instruments (e.g. precision, accuracy, sensitivity) and for methods of calibration (Wadsö and Goldberg 2001).\n\nModern IMC instruments are actually semi-adiabatic—i.e. heat transfer between the specimen and its surroundings is not zero (adiabatic), because IMC measurement of heat flow depends on the existence of a small temperature differential—ca. 0.001 °C. However, because the differential is so low, IMC measurements are essentially isothermal. Fig. 3. shows an overview of an IMC instrument which contains 48 separate heat flow measurement modules. One module is shown. The module's measuring unit is typically a Peltier-Seebeck device. The device produces a voltage proportional to the temperature difference between a specimen which is producing or consuming heat and a thermally inactive reference which is at the temperature of the heat sink. The temperature difference is in turn proportional to the rate at which the specimen is producing or consuming heat (see Calibration below). All the modules in an instrument use the same heat sink and thermostat and thus all produce data at the same set temperature. However, it is generally possible to start and stop measurements in each ampoule independently. In a highly parallel (e.g. 48-channel) instrument like the one shown in Fig. 3, this makes it possible to perform (start and stop) several different experiments whenever it is convenient to do so.\n\nAlternatively, IMC instruments can be equipped with duplex modules which yield signals proportional to the heat flow difference between two ampoules. One of two such duplex ampoules is often a blank or control—i.e. a specimen which does not contain the material producing the rate process of interest, but whose content is otherwise identical to that which is in the specimen ampoule. This provides a means for eliminating minor heat-producing reactions which are not of interest—for example gradual chemical changes over a period of days in a cell culture medium at the measurement temperature. Many useful IMC measurements can be carried out using simple sealed ampoules. However, as mentioned above, instrument/ampoule systems are available which allow or even control flow of gasses or liquids to and/or from the specimens and/or provide specimen mechanical stirring.\n\nHeat flow is usually measured relative to a reference insert, as shown in Fig. 3. This is typically a metal coupon that is chemically and physically stable at any temperature in the instrument's operating range and thus will not produce or consume heat itself. For best performance, the reference should have a heat capacity close to that of the specimen (e.g. IMC ampoule plus contents).\n\nCommercial IMC instruments are often operated as heat conduction (hc) calorimeters in which heat produced by the specimen (i.e. material in an ampoule) flows to the heat sink, typically an aluminum block contained in a thermostat (e.g. constant temperature bath). As mentioned above, an IMC instrument operating in hc mode is not precisely isothermal because small differences between the set temperature and the specimen temperature necessarily exist—so that there is measurable heat flow. However, small variations in specimen temperature do not significantly affect heat sink temperature because the heat capacity of the heat sink is much higher than the specimen—usually ca. 100×.\n\nHeat transfer between the specimen and the heat sink takes place through a Peltier-Seebeck device, allowing dynamic measurement of heat produced or consumed. In research-quality instruments, thermostat/heat sink temperature is typically accurate to <nowiki>«/nowiki> ±0.1 K and maintained within ca. <nowiki>«/nowiki> ±100 μK/24h. The precision with which heat sink temperature is maintained over time is a major determinant of the precision of the heat flow measurements over time. An advantage of hc mode is a large dynamic range. Heat flows of ca. 50,000 μ W can be measured with a precision of ca. ±0.2 μW. Thus measuring a heat flow of ca. <nowiki»</nowiki>0.2 μW above baseline constitutes detection of heat flow, although a more conservative detection of 10× the precision limit is often used.\n\nSome IMC instruments operate (or can also be operated) as power compensation (pc) calorimeters. In this case, in order to maintain the specimen at the set temperature, heat produced is compensated using a Peltier-Seebeck device. Heat consumed is compensated either by an electric heater or by reversing the polarity of the device (van Herwaarden, 2000). If a given instrument is operated in pc mode rather than hc, the precision of heat flow measurement remains the same (e.g. ca. ±0.2 μW). The advantage of compensation mode is a smaller time constant – i.e. the time needed to detect a given heat flow pulse is ca.10X shorter than in conduction mode. The disadvantage is a ca. 10X smaller dynamic range compared to hc mode.\n\nFor operation in either hc or pc mode, routine calibration in commercial instruments is usually accomplished with built-in electric heaters. The performance of the electrical heaters can in turn be validated using specimens of known heat capacity or which produce chemical reactions whose heat production per unit mass is known from thermodynamics (Wadsö and Goldberg 2001). In either hc or pc mode, the resulting signal is a computer-recordable voltage, calibrated to represent specimen μ W-range heat flow vs. time. Specifically, if no significant thermal gradients exist in the specimen, then P = e <nowiki>[</nowiki>U <nowiki>+</nowiki> t (dU/dt)<nowiki>]</nowiki>, where P is heat flow (i.e. μ W), ε is the calibration constant, U the measured potential difference across the thermopile, and t the time constant. Under steady-state conditions—for example during the release of a constant electrical calibration current, this simplifies to P = e U. (Wadsö and Goldberg 2001).\n\nMany highly useful IMC measurements can be conducted in sealed ampoules (Fig. 1) which offer advantages of simplicity, protection from contamination and (where needed) a substantial margin of bio-safety for persons handling or exposed to the ampoules. A closed ampoule can contain any desired combination of solids, liquids, gasses or items of biologic origin. Initial gas composition in the ampoule head space can be controlled by sealing the ampoule in the desired gas environment.\n\nHowever, there are also IMC instrument/ampoule designs which permit controlled flow of gas or liquid through the ampoule during measurement and/or mechanical stirring. Also, with proper accessories, some IMC instruments can be operated as ITC (isothermal titration calorimetry) instruments. The topic of ITC is covered elsewhere (see Isothermal titration calorimetry). In addition some IMC instruments can record heat flow while the temperature is slowly changed (scanned) over time. The scanning rate has to be slow-ca. ± 2 K°/h-in order to keep IMC-scale specimens (e.g. a few grams) sufficiently close to the heat sink temperature (<nowiki>«/nowiki>ca. 0.1 °C). Fast scanning of temperature is the province of Differential Scanning Calorimetry (DSC) instruments which generally use much smaller specimens. Some DSC instruments can be operated in IMC mode, but the small ampoule (and therefore specimen) size needed for scanning limit the utility and sensitivity of DSC instruments used in IMC mode.\n\nHeat flow rate (μJ/sec = μW) measurements are accomplished by first setting an IMC instrument thermostat at a selected temperature and allowing the instrument<nowiki>'</nowiki>s heat sink to stabilize at that temperature. If an IMC instrument operating at one temperature is set to a new temperature, re-stabilization at the new temperature setting may take several hours—even a day. As explained above, achievement and maintenance of a precisely stable temperature is fundamental to achieving precise heat flow measurements in the μW range over extended times (e.g. days).\n\nAfter temperature stabilization, if an externally prepared ampoule (or some solid specimen of ampoule dimensions) is used, it is slowly introduced (e.g. lowered) into an instrument<nowiki>'</nowiki>s measurement module, usually in a staged operation. The purpose is to ensure that by the time the ampoule/specimen is in the measurement position, its temperature is close to (within c. 0.001 °C) of the measurement temperature. This is so that any heat flow then measured is due to specimen rate processes rather than due to a continuing process of bringing the specimen to the set temperature. The time for introduction of a specimen in a 3–20 ml IMC ampoule into measurement position is ca. 40 minutes in many instruments. This means that heat flow from any processes which take place within a specimen during that the introduction period will not be recorded.\n\nIf an in-place ampoule is used, and some agent or specimen is injected, this also produces a period of instability, but it is on the order ca. 1 minute. Fig. 5 provides examples of both the long period needed to stabilize an instrument if an ampoule is introduced directly, and the short period of instability due to injection.\n\nAfter the introduction process, specimen heat flow can be precisely recorded continuously, for as long as it is of interest. The extreme stability of research-grade instruments (<nowiki>«/nowiki> ±100 μK/24h ) means that accurate measurements can be (and often are) made for a period of days. Since the heat flow signal is essentially readable in real time, it serves as a means for deciding whether or not heat flow of interest is still occurring. Also, modern instruments store heat flow vs. time data as computer files, so both real-time and retrospective graphic display and mathematical analysis of data are possible.\n\nAs indicated below, IMC has many advantages as a method for analyzing rate processes, but there are also some caveats that must be heeded.\n\nAny rate process can be studied—if suitable specimens will fit IMC instrument module geometry, and proceed at rates amenable to IMC methodology (see above). As shown under Applications, IMC is in use to quantify an extremely wide range of rate processes in vitro—e.g. from solid-state stability of polymers (Hardison et al. 2003) to efficacy of drug compounds against parasitic worms (Maneck et al. 2011). IMC can also determine the aggregate rate of uncharacterized, complex, or multiple interactions (Lewis & Daniels). This is especially useful for comparative screening—e.g. the effects of different combinations of material composition and/or fabrication processes on overall physico-chemical stability.\n\nIMC heat flow data are obtained as voltage fluctuations vs. time, stored as computer files and can be displayed essentially in real time—as the rate process is occurring. The heat flow-related voltage is continuous over time, but in modern instruments it is normally sampled digitally. The frequency of digital sampling can be controlled as needed—i.e. frequent sampling of rapid heat flow changes for better time resolution or slower sampling of slow changes in order to limit data file size.\n\nIMC is sensitive enough to detect and quantify in short times (hours, days) reactions which consume only a few percent of reactants over long times (months). IMC thus avoids long waits often needed until enough reaction product has accumulated for conventional (e.g. chemical) assays. This applies to both physical and biological specimens (see Applications).\n\nAt each combination of specimen variables and set temperature of interest, IMC provides direct determination of the heat flow kinetics and cumulative heat of rate processes. This avoids any need to assume that a rate process remains the same when temperature or other controlled variables are changed before an IMC measurement.\n\nFor comparisons of the effect of experimental variables (e.g. initial concentrations) on rate processes, IMC does not require development and use of chemical or other assay methods. If absolute data are required (e.g. quantity of product produced by a process), then assays can be conducted in parallel on specimens identical to those used for IMC (and/or on IMC specimens after IMC runs). The resultant assay data is used to calibrate the rate data obtained by IMC.\n\nIMC does not require adding markers (e.g. fluorescent or radioactive substances) to capture rate processes. Unadulterated specimens can be used, and after an IMC run, the specimen is unchanged (except by the processes which have taken place). The post-IMC specimen can be subjected to any kind of physical, chemical, morphological or other evaluation of interest.\n\nAs indicated in the methodology description, when the IMC method of inserting a sealed ampoule is used, it is not possible to capture heat flow during the first ca. 40 minutes while the specimen is slowly being brought to the set temperature. In this mode therefore, IMC is best suited to studying processes which start slowly or occur slowly at a given temperature. This caveat also applies to the time \"before\" insertion—i.e. time elapsed between preparing a specimen (in which a rate process may then start) and starting the IMC insertion process (Charlebois \"et al.\" 2003). This latter effect is usually minimized if the temperature chosen for IMC is substantially higher (e.g. 37 °C) than the temperature at which the specimen is prepared (e.g. 25 °C).\n\nIMC captures the \"aggregate\" heat production or consumption resulting from all processes taking place within a specimen, including for example\n\n\nThus great care must be taken in experimental planning and design to identify all possible processes which may be taking place. It is often necessary to design and conduct preliminary studies intended to systematically determine if multiple processes are taking place and if so, their contributions to aggregate heat flow. One strategy, in order to eliminate extraneous heat flow data, is to compare heat flow for a specimen in which the rate process of interest is taking place with that from a blank specimen which includes everything in the specimen of interest—except the item which will undergo the rate process of interest. This can be directly accomplished with instruments having duplex IMC modules which report the net heat flow difference between two ampoules.\n\nAfter a discussion of some special sources of IMC application information, several specific categories of IMC analysis of rate processes are covered, and recent examples (with literature references) are discussed in each category.\n\nThe Bibliography lists the four extensive volumes of the Handbook of Thermal Analysis and Calorimetry: Vol. 1 Principles and Practice (1998), Vol. 2 Applications to Inorganic and Miscellaneous Materials (2003), Vol. 3 Applications to Polymers and Plastics (2002), and Vol. 4 From Macromolecules to Man (1999). These constitute a prime source of information on (and literature references to) IMC applications and examples published prior to ca. 2000.\n\nSome IMC instrument manufacturers have assembled application notes, and make them available to the public. The notes are often (but not always) adaptations of journal papers. An example is the Microcalorimetry Compendium Vol. I and II offered by TA Instruments, Inc. and listed in the Bibliography.\n\n\"Proteins\" the first section of notes in Vol. I, is not of interest here, as it describes studies employing Isothermal titration calorimetry. The subsequent sections of Vol. I, Life & Biological Sciences and Pharmaceuticals contain application notes for both IMC and Differential scanning calorimetry. Vol. II of the compendium is devoted almost entirely to IMC applications. Its sections are entitled Cement, Energetics, Material and Other. A possible drawback to these two specific compendia is that none of the notes are dated. Although the compendia were published in 2009, some of the notes describe IMC instruments which were in use years ago and are no longer available. Thus, some of the notes, while still relevant and instructive, often describe studies done before 2000.\n\nIn general, possible applications of IMC are only limited by the imagination of the person who chooses to employ IMC as an analytical tool—within the previously described constraints presented by existing IMC instruments and methodology. This is because it is a universal means for monitoring any chemical, physical or biological rate process. Below are some IMC application categories with examples in each. In most categories, there are many more published examples than those mentioned and referenced. The categories are somewhat arbitrary and often overlap. A different set of categories might be just as logical, and more categories could be added.\n\nIMC is widely used for studying the rates of formation of a variety of materials by various processes. It is best suited to study processes which occur slowly—i.e. over hours or days. A prime example is the study of hydration and setting reactions of calcium mineral cement formulations. One paper provides an overview (Gawlicki, et al. 2010) and another describes a simple approach (Evju 2003). Other studies focus on insights into cement hydration provided by IMC combined with IR spectroscopy (Ylmen et al. 2010) and on using IMC to study the influence of compositional variables on cement hydration and setting times (Xu et al. 2011).\n\nIMC can also be conveniently used to study the rate and amount of hydration (in air of known humidity) of calcium minerals or other minerals. To provide air of known humidity for such studies, small containers of saturated salt solutions can be placed in an IMC ampoule along with a non-hydrated mineral specimen. The ampoule is then sealed and introduced into an IMC instrument. The saturated salt solution keeps the air in the ampoule at a known rH, and various common salt solutions provide humidities ranging from e.g. 32-100% rH. Such studies have been performed on μm size range calcium hydroxyapatite particles and calcium-containing bioactive glass \"nano\" particles (Doostmohammadi et al. 2011).\n\nIMC is well suited for rapidly quantifying the rates of slow changes in materials (Willson et al. 1995). Such evaluations are variously described as studies of stability, degradation or shelf life. \nFor example, IMC has been widely used for many years in shelf life studies of solid drug formulations in the pharmaceutical industry (Pikal et al. 1989, Hansen et al. 1990, Konigbauer et al. 1992.) IMC has the ability to detect slow degradation during simulated shelf storage far sooner than conventional analytical methods and without the need to employ chemical assay techniques. IMC is also a rapid, sensitive method for determining the often functionally crucial amorphous content of drugs such as nifedipine (Vivoda et al. 2011).\n\nIMC can be used for rapidly determining the rate of slow changes in industrial polymers. For example, gamma radiation sterilization of a material frequently used for surgical implants—ultra-high-molecular-weight polyethylene (UHMWPE)—is known to produce free radicals in the polymer. The result is slow oxidation and gradual undesirable embrittlement of the polymer on the shelf or in vivo. IMC could detect oxidation-related heat and quantified an oxidation rate of ca. 1% per year in irradiated UHMWPE at room temperature in air (Charlebois et al. 2003). In a related study the activation energy was determined from measurements at a series of temperatures (Hardison et al. 2003).\n\nIMC is also of great utility in evaluating the \"runaway potential\" of materials which are significant fire or explosion hazards. For example, it has been used to determine autocatalytic kinetics of cumene hydroperoxide (CHP), an intermediate which is used in the chemical industry and whose sudden decomposition has caused a number of fires and explosions. Fig. 4 Shows the IMC data documenting thermal decomposition of CHP at 5 different temperatures (Chen et al. 2008).\n\nThe term metabolismics can be used to describe studies of the quantitative measurement of the rate at which heat is produced or consumed vs. time by cells (including microbes) in culture, by tissue specimens, or by small whole organisms. As described subsequently, metabolismics can be useful as a diagnostic tool; especially in either (a) identifying the nature of a specimen from its heat flow vs. time signature under a given set of conditions, or (b) determining the effects of e.g. pharmaceutical compounds on metabolic processes, organic growth or viability. Metabolismics is related to metabolomics. The latter is the systematic study of the unique chemical fingerprints that specific cellular processes leave behind; i.e. the study of their small-molecule metabolite profiles. When IMC is used to determine metabolismics, the products of the metabolic processes studied are subsequently available for metabolomics studies. Since IMC does not employ biochemical or radioactive markers, the post-IMC specimens consist only of metabolic products and remaining culture medium (if any was used). If metabolismics and metabolomics are used together, they can provide a comprehensive record of a metabolic process taking place in vitro: its rate and energetics, and its metabolic products.\n\nTo determine metabolismics using IMC, there must of course be sufficient cells, tissue or organisms initially present (or present later if replication is taking place during IMC measurements) to generate a heat flow signal above a given instrument's detection limit. A landmark 2002 general paper on the topic of metabolism provides an excellent perspective from which to consider IMC metabolismic studies (see Bibliography, West, Woodruff and Brown 2002). It describes how metabolic rates are related and how they scale over the entire range from \"molecules and mitochondria to cells and mammals\". Importantly for IMC, the authors also note that while the metabolic rate of a given type of mammalian cell in vivo declines markedly with increasing animal size (mass), the size of the donor animal has no effect on the metabolic rate of the cell when cultured in vitro.\n\nMammalian cells in culture have a metabolic rate of ca. 30×10 W/cell (Figs. 2 and 3 in Bibliography: West, Woodruff and Brown 2002). By definition, IMC instruments have a sensitivity of at least 1×10 W (i.e. 1 μ W). Therefore, the metabolic heat of ca. 33,000 cells is detectable. Based on this sensitivity, IMC was used to perform a large number of pioneering studies of cultured mammalian cell metabolismics in the 1970s and 1980s in Sweden. One paper (Monti 1990) serves as an extensive guide to work done up until 1990. It includes explanatory text and 42 references to IMC studies of heat flow from cultured human erythrocytes, platelets, lymphocytes, lymphoma cells, granulocytes, adipocytes, skeletal muscle and myocardial tissue. The studies were done to determine how and where IMC might be used as a clinical diagnostic method and/or provide insights into metabolic differences between cells from healthy persons and persons with various diseases or health problems.\n\nDevelopments since ca. 2000 in IMC (e.g. massively parallel instruments, real-time, computer-based storage and analysis of heat flow data) have stimulated further use of IMC in cultured cell biology. For example, IMC has been evaluated for assessing antigen-induced lymphocyte proliferation (Murigande et al. 2009) and revealed aspects of proliferation not seen using a conventional non-continuous radioactive marker assay method. IMC has also been applied to the field of tissue engineering. One study (Santoro et al. 2011) demonstrated that IMC could be used to measure the growth (i.e. proliferation) rate in culture of human chondrocytes harvested for tissue engineering use. It showed that IMC can potentially serve to determine the effectiveness of different growth media formulations and also determine whether cells donated by a given individual can be grown efficiently enough to consider using them to produce engineered tissue.\n\nIMC has also been used to measure the metabolic response of cultured macrophages to surgical implant wear debris. IMC showed that the response was stronger to μm size range particles of polyethylene than to similarly sized Co alloy particles (Charlebois et al. 2002). A related paper covers the general topic of applying IMC in the field of synthetic solid materials used in surgery and medicine (Lewis and Daniels 2003)\".\n\nAt least two studies have suggested IMC can be of substantial use in tumor pathology. In one study (Bäckman 1990), the heat production rate of T-lymphoma cells cultured in suspension was measured. Changes in temperature and pH induced significant variations, but stirring rate and cell concentration did not. A more direct study of possible diagnostic use (Kallerhoff et al. 1996) produced promising results. For the uro-genital tissue biopsy specimens studied, the results showed\n\nAs of this writing (2012) IMC has not become widely used in cultured cell toxicology even though it has been used periodically and successfully since the 1980s. IMC is advantageous in toxicology when it is desirable to observe cultured cell metabolism in real time and to quantify the rate of metabolic decline as a function of the concentration of a possibly toxic agent. One of the earliest reports (Ankerst et al. 1986) of IMC use in toxicology was a study of antibody-dependent cellular toxicity (ADCC) against human melanoma cells of various combinations of antiserum, monoclonal antibodies and also peripheral blood lymphocytes as effector cells. Kinetics of melanoma cell metabolic heat flow vs. time in closed ampoules were measured for 20 hours. The authors concluded that \n\nIMC is also being used in environmental toxicology. In an early study (Thorén 1992) toxicity against monolayers of alveolar macrophages of particles of MnO, TiO and SiO (silica) were evaluated. IMC results were in accord with results obtained by fluorescein ester staining and microscopic image analysis—except that IMC showed toxic effects of quartz not discernable by image analysis. This latter observation—in accord with known alveolar effects—indicated to the authors that IMC was a more sensitive technique.\n\nMuch more recently (Liu et al. 2007), IMC has been shown to provide dynamic metabolic data which assess toxicity against fibroblasts of Cr(VI) from potassium chromate. Fig. 5 shows baseline results determining the metabolic heat flow from cultured fibroblasts prior to assessing the effects of Cr(VI). The authors concluded that \n\nSimple closed ampoule IMC has also been used and advocated for assessing the cultured cell toxicity of candidate surgical implant materials—and thus serve as a biocompatibility screening method. In one study (Xie et al. 2000) porcine renal tubular cells in culture were exposed to both polymers and titanium metal in the form of \"microplates\" having known surface areas of a few cm. The authors concluded that IMC \n\nIn another implant materials study (Doostmohammadi et al. 2011) both a rapidly growing yeast culture and a human chondrocyte culture were exposed to particles (diam.<nowiki>«/nowiki> 50 μm) of calcium hydroxyapatite (HA) and bioactive (calcium-containing) silica glass. The glass particles slowed or curtailed yeast growth as a function of increasing particle concentration. The HA particles had much less effect and never entirely curtailed yeast growth at the same concentrations. The effects of both particle types on chondrocyte growth were minimal at the concentration employed. The authors concluded that \n\"The cytotoxicity of particulate materials such as bioactive glass and hydroxyapatite particles can be evaluated using the microcalorimetry method. This is a modern method for in vitro study of biomaterials biocompatibility and cytotoxicity which can be used alongside the old conventional assays.\"\n\nPublications describing use of IMC in microbiology began in the 1980s (Jesperson 1982). While some IMC microbiology studies have been directed at viruses (Heng et al. 2005) and fungi (Antoci et al. 1997), most have been concerned with bacteria. A recent paper (Braissant et al. 2010) provides a general introduction to IMC metabolismic methods in microbiology and an overview of applications in medical and environmental microbiology. The paper also explains how heat flow vs. time data for bacteria in culture are an exact expression—as they occur over time—of the fluctuations in microorganism metabolic activity and replication rates in a given medium (Fig. 6).\n\nIn general, bacteria are about 1/10 the size of mammalian cells and produce perhaps 1/10 as much metabolic heat-i.e. ca. 3x10 W/cell. Thus, compared to mammalian cells (see above) ca. 10X as many bacteria—ca. 330,000—must be present to produce detectable heat flow—i.e. 1 μW. However, many bacteria replicate orders of magnitude more rapidly in culture than mammalian cells, often doubling their number in a matter of minutes (see Bacterial growth). As a result, a small initial number of bacteria in culture and initially undetectable by IMC rapidly produce a detectable number. For example, 100 bacteria doubling every 20 minutes will in less than 4 hours produce <nowiki»</nowiki>330,000 bacteria and thus an IMC-detectable heat flow. Consequently, IMC can be used for easy, rapid detection of bacteria in the medical field. Examples include detection of bacteria in human blood platelet products (Trampuz et al. 2007) and urine (Bonkat \"et al.\" 2011) and rapid detection of tuberculosis (Braissant et al. 2010, Rodriguez et al. 2011). Fig. 7 shows an example of detection times of Tuberculosis bacteria as a function of the initial amount of bacteria present in a closed IMC ampoule containing a culture medium.\n\nFor microbes in growth media in closed ampoules, IMC heat flow data can also be used to closely estimate basic microbial growth parameters; i.e. maximum growth rate and duration time of the lag phase before maximum growth rate is achieved. This is an important special application of the basic analysis of these parameters explained previously (Overview (Data Obtained)).\n\n\"Unfortunately, the IMC literature contains some published papers in which the relation between heat flow data and microbial growth in closed ampoules has been misunderstood. However, in 2013 an extensive clarification was published, describing (a) details of the relation between IMC heat flow data and microbial growth, (b) selection of mathematical models which describe microbial growth and (c) determination of microbial growth parameters from IMC data using these models (Braissant et al. 2013).\"\n\nIn a logical extension of the ability of IMC to detect and quantify bacterial growth, known concentrations of antbiotics can be added to bacterial culture, and IMC can then be used to quantify their effects on viability and growth. Closed ampoule IMC can easily capture basic pharmacologic information—e.g. minimum inhibitory concentration (MIC) of an antibiotic needed to stop growth of a given organism. In addition it can simultaneously provide dynamic growth parameters—lag time and maximum growth rate (see Fig. 2, Howell et al. 2011, Braissant et al. 2013), which assess mechanisms of action. Bactericidal action (see Bactericide) is indicated by an increased lag time as a function of increasing antibiotic concentration, while bacteriostatic action (see Bacteriostatic agent) is indicated by a decrease in growth rate with concentration. The IMC approach to antibiotic assessment has been demonstrated for a number of a types of bacteria and antibiotics (von Ah et al. 2009). Closed ampoule IMC can also rapidly differentiate between normal and resistant strains of bacteria such as \"Staphylococcus aureus\" (von Ah et al. 2008, Baldoni et al. 2009). IMC has also been used to assess the effects of disinfectants on the viability of mouth bacteria adhered to dental implant materials (Astasov-Frauenhoffer et al. 2011). In a related earlier study, IMC was used to measure the heat of adhesion of dental bacteria to glass (Hauser- Gerspach et al. 2008).\n\nAnalogous successful use of IMC to determine the effects of antitumor drugs on tumor cells in culture within a few hours has been demonstrated (Schön and Wadsö 1988). Rather than the closed-ampoule approach, an IMC setup was used which allowed drug injection into stirred specimens.\n\nAs of this writing (2013), IMC has been used less widely in mammalian cell in vitro pharmacodynamic studies than in microbial studies.\n\nIt is possible to use IMC to perform metabolismic studies of living multicellular organisms—if they are small enough to be placed in IMC ampoules (Lamprecht & Becker 1988). IMC studies have been made of insect pupa metabolism during ventilating movements (Harak et al. 1996) and effects of chemical agents on pupal growth (Kuusik et al. 1995). IMC has also proved effective in assessing the effects of aging on nematode worm metabolism (Braekman et al. 2002).\n\nIMC has also proved highly useful for in vitro assessments of the effects of pharmaceuticals on tropical parasitic worms (Manneck et al. 2011-1, Maneck et al. 2011-2, Kirchhofer et al. 2011). An interesting feature of these studies is the use of a simple manual injection system for introducing the pharmaceuticals into sealed ampoules containing the worms. Also, IMC not only documents the general metabolic decline over time due to the drugs, but also the overall frequency of worm motor activity and its decline in amplitude over time as reflected in fluctuations in the heat flow data.\n\nBecause of its versatility, IMC can be an effective tool in the fields of plant and environmental biology. In an early study (Hansen et al. 1989), the metabolic rate of larch tree clone tissue specimens was measured. The rate was predictive of long-term tree growth rates, was consistent for specimens from a given tree and was found to correlate with known variations in the long-term growth of clones from different trees.\n\nBacterial oxalotrophic metabolism is common in the environment, particularly in soils. Oxalotrophic bacteria are capable of using oxalate as a sole carbon and energy source. Closed-ampoule IMC was used to study metabolism of oxalotrophic soil bacteria exposed to both an optimized medium containing potassium oxalate as the sole carbon source and a model soil (Bravo et al. 2011). Using an optimized medium, growth of six different strains of soil bacteria was easily monitored and reproducibly quantified and differentiated over a period days. IMC measurement of bacterial metabolic heat flow in the model soil was more difficult, but a proof of concept was demonstrated.\n\nMoonmilk is a white, creamy material found in caves. It is a non-hardening, fine crystalline precipitate from limestone and is composed mainly of calcium and/or magnesium carbonates. Microbes may be involved in its formation. It is difficult to infer microbial activities in moonmilk from standard static chemical and microscopic assays of moonmilk composition and structure. Closed ampoule IMC has been used to solve this problem (Braissant, Bindscheidler et al. 2011). It was possible to determine the growth rates of chemoheterotrophic microbial communities on moonmilk after the addition of various carbon sources simulating mixes that would be brought into contact with moonmilk due to snow melt or rainfall. Metabolic activity was high and comparable to that found in some soils.\n\nHarris \"et al.\" (2012), studying differing fertilizer input regimes, found that, when expressed as heat output per unit soil microbial biomass, microbial communities under organic fertilizer regimes produced less waste heat than those under inorganic regimes.\n\nIMC has been shown to have diverse uses in food science and technology. An overview (Wadsö and Galindo 2009) discusses successful applications in assessing vegetable cutting wound respiration, cell death from blanching, milk fermentation, microbiological spoilage prevention, thermal treatment and shelf life. Another publication (Galindo et al. 2005) reviews the successful use of IMC for monitoring and predicting quality changes during storage of minimally processed fruits and vegetables.\n\nIMC has also proven effective in accomplishing enzymatic assays for orotic acid in milk (Anastasi et al. 2000) and malic acid in fruits, wines and other beverages and also cosmetic products (Antonelli et al. 2008). IMC has also been used to assess the efficacy of anti-browning agents on fresh- cut potatoes (Rocculi et al. 2007). IMC has also proven effective in assessing the extent to which low-energy pulsed electric fields (PEFs) affect the heat of germination of barley seeds—important in connection with their use in producing malted beverages (Dymek et al. 2012).\n\n\n\n", "id": "35215882", "title": "Isothermal microcalorimetry"}
{"url": "https://en.wikipedia.org/wiki?curid=21426616", "text": "Proofreading (biology)\n\nThe term proofreading is used in genetics to refer to the error-correcting processes, first proposed by John Hopfield and Jacques Ninio, involved in DNA replication, immune system specificity, enzyme-substrate recognition among many other processes that require enhanced specificity. The proofreading mechanisms of Hopfield and Ninio are non-equilibrium active processes that consume ATP to enhance specificity of various biochemical reactions.\n\nIn bacteria, all three DNA polymerases (I, II and III) have the ability to proofread, using 3’ → 5’ exonuclease activity. When an incorrect base pair is recognized, DNA polymerase reverses its direction by one base pair of DNA and excises the mismatched base. Following base excision, the polymerase can re-insert the correct base and replication can continue.\n\nIn eukaryotes only the polymerases that deal with the elongation (delta and epsilon) have proofreading ability (3’ → 5’ exonuclease activity).\n\nProofreading also occurs in mRNA translation for \"protein\" synthesis. In this case, one mechanism is release of any incorrect aminoacyl-tRNA before peptide bond formation.\n\nThe extent of proofreading in DNA replication determines the mutation rate, and is different in different species.\nFor example, loss of proofreading due to mutations in the DNA polymerase epsilon gene results in a hyper-mutated genotype with >100 mutations per Mbase of DNA in human colorectal cancers.\n\nThe extent of proofreading in other molecular processes can depend on the effective population size of the species and the number of genes affected by the same proofreading mechanism.\n\n", "id": "21426616", "title": "Proofreading (biology)"}
{"url": "https://en.wikipedia.org/wiki?curid=33329683", "text": "Spontaneous absolute asymmetric synthesis\n\nSpontaneous absolute asymmetric synthesis is a chemical phenomenon that \"stochastically\" generates chirality based on autocatalysis and small fluctuations in the ratio of enantiomers present in a racemic mixture. In certain reactions which initially do not contain chiral information, stochastically distributed enantiomeric excess can be observed. The phenomenon is different from chiral amplification, where enantiomeric excess is present from the beginning and not stochastically distributed. Hence, when the experiment is repeated many times, the average enantiomeric excess approaches 0%. The phenomenon has important implications concerning the origin of homochirality in nature.\n", "id": "33329683", "title": "Spontaneous absolute asymmetric synthesis"}
{"url": "https://en.wikipedia.org/wiki?curid=56566", "text": "Chronobiology\n\nChronobiology is a field of biology that examines periodic (cyclic) phenomena in living organisms and their adaptation to solar- and lunar-related rhythms. These cycles are known as biological rhythms. Chronobiology comes from the ancient Greek χρόνος (\"chrónos\", meaning \"time\"), and biology, which pertains to the study, or science, of life. The related terms \"chronomics\" and \"chronome\" have been used in some cases to describe either the molecular mechanisms involved in chronobiological phenomena or the more quantitative aspects of chronobiology, particularly where comparison of cycles between organisms is required.\n\nChronobiological studies include but are not limited to comparative anatomy, physiology, genetics, molecular biology and behavior of organisms within biological rhythms mechanics. Other aspects include epigenetics, development, reproduction, ecology and evolution.\n\nThe variations of the timing and duration of biological activity in living organisms occur for many essential biological processes. These occur (a) in animals (eating, sleeping, mating, hibernating, migration, cellular regeneration, etc.), (b) in plants (leaf movements, photosynthetic reactions, etc.), and in microbial organisms such as fungi and protozoa. They have even been found in bacteria, especially among the cyanobacteria (aka blue-green algae, see bacterial circadian rhythms). The most important rhythm in chronobiology is the circadian rhythm, a roughly 24-hour cycle shown by physiological processes in all these organisms. The term \"circadian\" comes from the Latin \"circa\", meaning \"around\" and \"dies\", \"day\", meaning \"approximately a day.\" It is regulated by circadian clocks.\n\nThe circadian rhythm can further be broken down into routine cycles during the 24-hour day:\n\nWhile circadian rhythms are defined as endogenously regulated, other biological cycles may be regulated by exogenous signals. In some cases, multi-trophic systems may exhibit rhythms driven by the circadian clock of one of the members (which may also be influenced or reset by external factors). The endogenous plant cycles may regulate the activity of the bacterium by controlling availability of plant-produced photosynthate.\n\nMany other important cycles are also studied, including:\n\nWithin each cycle, the time period during which the process is more active is called the \"acrophase\". When the process is less active, the cycle is in its \"bathyphase\" or \"trough\" phase. The particular moment of highest activity is the \"peak\" or \"maximum\"; the lowest point is the \"nadir\". How high (or low) the process gets is measured by the \"amplitude\".\n\nA circadian cycle was first observed in the 18th century in the movement of plant leaves by the French scientist Jean-Jacques d'Ortous de Mairan. In 1751 Swedish botanist and naturalist Carl Linnaeus (Carl von Linné) designed a flower clock using certain species of flowering plants. By arranging the selected species in a circular pattern, he designed a clock that indicated the time of day by the flowers that were open at each given hour. For example, among members of the daisy family, he used the hawk's beard plant which opened its flowers at 6:30 am and the hawkbit which did not open its flowers until 7 am.\n\nThe 1960 symposium at Cold Spring Harbor Laboratory laid the groundwork for the field of chronobiology.\n\nIt was also in 1960 that Patricia DeCoursey invented the phase response curve, one of the major tools used in the field since.\n\nFranz Halberg of the University of Minnesota, who coined the word \"circadian\", is widely considered the \"father of American chronobiology.\" However, it was Colin Pittendrigh and not Halberg who was elected to lead the \"Society for Research in Biological Rhythms\" in the 1970s. Halberg wanted more emphasis on the human and medical issues while Pittendrigh had his background more in evolution and ecology. With Pittendrigh as leader, the Society members did basic research on all types of organisms, plants as well as animals. More recently it has been difficult to get funding for such research on any other organisms than mice, rats, humans and fruit flies.\n\nMore recently, light therapy and melatonin administration have been explored by Alfred J. Lewy (OHSU), Josephine Arendt (University of Surrey, UK) and other researchers as a means to reset animal and human circadian rhythms. Additionally, the presence of low-level light at night accelerates circadian re-entrainment of hamsters of all ages by 50%; this is thought to be related to simulation of moonlight.\n\nHumans can be morning people or evening people; these variations are called chronotypes for which there are various assessment tools and biological markers.\n\nIn the second half of 20th century, substantial contributions and formalizations have been made by Europeans such as Jürgen Aschoff and Colin Pittendrigh, who pursued different but complementary views on the phenomenon of entrainment of the circadian system by light (parametric, continuous, tonic, gradual vs. nonparametric, discrete, phasic, instantaneous, respectively).\n\nThere is also a food-entrainable biological clock, which is not confined to the suprachiasmatic nucleus. The location of this clock has been disputed. Working with mice, however, Fuller \"et al.\" concluded that the food-entrainable clock seems to be located in the dorsomedial hypothalamus. During restricted feeding, it takes over control of such functions as activity timing, increasing the chances of the animal successfully locating food resources.\n\nChronobiology is an interdisciplinary field of investigation. It interacts with medical and other research fields such as sleep medicine, endocrinology, geriatrics, sports medicine, space medicine and photoperiodism.\n\nIn spite of the similarity of the name to legitimate biological rhythms, the theory and practice of biorhythms is a classic example of pseudoscience. It attempts to describe a set of cyclic variations in human behavior based on a person's birth date. It is not a part of chronobiology.\n\n\n", "id": "56566", "title": "Chronobiology"}
{"url": "https://en.wikipedia.org/wiki?curid=13653300", "text": "Kinetic proofreading\n\nKinetic proofreading (or kinetic amplification) is a mechanism for error correction in biochemical reactions, proposed independently by John Hopfield (1974) and Jacques Ninio (1975). Kinetic proofreading allows enzymes to discriminate between two possible reaction pathways leading to correct or incorrect products with an accuracy higher than what one would predict based on the difference in the activation energy between these two pathways.\n\nIncreased specificity is obtained by introducing an irreversible step exiting the pathway, with reaction intermediates leading to incorrect products more likely to prematurely exit the pathway than reaction intermediates leading to the correct product. If the exit step is fast relative to the next step in the pathway, the specificity can be increased by a factor of up to the ratio between the two exit rate constants. (If the next step is fast relative to the exit step, specificity will not be increased because there will not be enough time for exit to occur.) This can be repeated more than once to increase specificity further.\n\nIn protein synthesis, the error rate is on the order of 1 in 10,000. This means that when a ribosome is matching anticodons of tRNA to the codons of mRNA, it matches complementary sequences correctly nearly all the time. Hopfield noted that because of how similar the substrates are (the difference between a wrong codon and a right codon can be as small as a difference in a single base), an error rate that small is unachievable with a one-step mechanism. Both wrong and right tRNA can bind to the ribosome, and if the ribosome can only discriminate between them by complementary matching of the anticodon, it must rely on the small free energy difference between binding three matched complementary bases or only two.\n\nA one-shot machine which tests whether the codons match or not by examining whether the codon and anticodon are bound will not be able to tell the difference between wrong and right codon with an error rate less than formula_1 unless the free energy difference is at least 10kT, which is much larger than the free energy difference for single codon binding. This is a thermodynamic bound, so it cannot be evaded by building a different machine. However, this can be overcome by kinetic proofreading, which introduces an irreversible step through the input of energy.\n\nAnother molecular recognition mechanism, which does \"not\" require expenditure of free energy is that of conformational proofreading. The incorrect product may also be formed but hydrolyzed at a greater rate than the correct product, giving the possibility of theoretically infinite specificity the longer you let this reaction run, but at the cost of large amounts of the correct product as well. (Thus there is a tradeoff between product production and its efficiency.) The hydrolytic activity may be on the same enzyme, as in DNA polymerases with editing functions, or on different enzymes.\n\nHopfield suggested a simple way to achieve smaller error rates using a molecular ratchet which takes many irreversible steps, each testing to see if the sequences match. At each step, energy is expended and specificity (the ratio of correct substrate to incorrect substrate at that point in the pathway) increases.\n\nThe requirement for energy in each step of the ratchet is due to the need for the steps to be irreversible; for specificity to increase, entry of substrate and analogue must occur largely through the entry pathway, and exit largely through the exit pathway. If entry were an equilibrium, the earlier steps would form a pre-equilibrium and the specificity benefits of entry into the pathway (less likely for the substrate analogue) would be lost; if the exit step were an equilibrium, then the substrate analogue would be able to re-enter the pathway through the exit step, bypassing the specificity of earlier steps altogether.\n\nAlthough one test will only be able to discriminate between mismatched and matched sequences a fraction formula_2 of the time, two tests will both fail only formula_3 of the time, and N tests will fail formula_4 of the time. In terms of free energy, the discrimination power of N successive tests for two states with a free energy formula_5 is the same as one test between two states with a free energy formula_6.\n\nTo achieve an error rate of formula_1 requires several comparison steps. Hopfield predicted on the basis of this theory that there is a multistage ratchet in the ribosome which tests the match several times before incorporating the next amino acid into the protein.\n\n\nBiochemical processes that use kinetic proofreading to improve specificity implement the delay-inducing multistep ratchet by a variety of distinct biochemical networks. Nonetheless, many such networks result in the times to completion of the molecular assembly and the proofreading steps (also known as the first passage time) that approach a near-universal, exponential shape for high proofreading rates and large network sizes. Since exponential completion times are characteristic of a two-state Markov process, this observation makes kinetic proofreading one of only a few examples of biochemical processes where structural complexity results in a much simpler large-scale, phenomenological dynamics.\n\nThe increase in specificity, or the overall amplification factor of a kinetic proofreading network that may include multiple pathways and especially loops is intimately related to the topology of the network: the specificity grows exponentially with the number of loops in the network. An example is homologous recombination in which the number of loops scales like the square of DNA length. The universal completion time emerges precisely in this regime of large number of loops and high amplification.\n\n", "id": "13653300", "title": "Kinetic proofreading"}
{"url": "https://en.wikipedia.org/wiki?curid=28734201", "text": "Transcellular transport\n\nTranscellular transport involves the transportation of solutes by a cell \"through\" a cell. One classic example is the movement of glucose from the intestinal lumen to extracellular fluid by epithelial cells.\n\nEpithelial cells use primary and secondary active transport, often in conjunction with passive diffusion through ion channels, to produce transcellular transport across epithelial tissues. This transport can either be absorption, transport from lumen (apical membrane surface) to blood, or secretion, transport from blood (basolateral membrane surface) to lumen.\n\nThe transcellular pathway of transport is important in the intestinal absorption of drug molecules, the other being the paracellular pathway. The transcellular pathway of transport include transcellular diffusion, active carrier mediated transportation, and transcytosis. The transcelluar diffusion simply involves the movement of solutes based on a diffusion gradient moving from an area of high concentration to an area of low concentration, however, the cell membrane is a hydrophobic environment and will not allow the passive diffusion of charged, hydrophilic, or zwitterion molecules. Active transport involves the use of energy to transport specific substrates across barriers, even against the concentration gradient. Macromolecules can sometimes be transported through transcytosis.\n\nIn contrast, paracellular transport is the transfer of substances across an epithelium by passing through an intercellular space between the cells.\n", "id": "28734201", "title": "Transcellular transport"}
{"url": "https://en.wikipedia.org/wiki?curid=8694335", "text": "Biocommunication (science)\n\nIn the study of the biological sciences, biocommunication is any specific type of communication within (intraspecific) or between (interspecific) species of plants, animals, fungi, protozoa and microorganisms. Communication basically means sign-mediated interactions following three levels of (syntactic, pragmatic and semantic) rules. Signs in most cases are chemical molecules (semiochemicals), but also tactile, or as in animals also visual and auditive. Biocommunication of animals may include vocalizations (as between competing bird species), or pheromone production (as between various species of insects), chemical signals between plants and animals (as in tannin production used by vascular plants to warn away insects), and chemically mediated communication between plants and within plants. \n\nBiocommunication of fungi demonstrates that mycelia communication integrates interspecific sign-mediated interactions between fungal organisms soil bacteria and plant root cells without which plant nutrition could not be organized. Biocommunication of Archaea represents keylevels of sign-mediated interactions in the evolutionarily oldest akaryotes.\n\nBiocommunication theory may be considered to be a branch of biosemiotics. Whereas Biosemiotics studies the production and interpretation of signs and codes, biocommunication theory investigates concrete interactions mediated by signs. Accordingly, syntactic, semantic, and pragmatic aspects of biocommunication processes are distinguished. Biocommunication specific to animals (animal communication) is considered a branch of zoosemiotics. The semiotic study of molecular genetics, can be considered a study of biocommunication at its most basic level. \n\nGiven the complexity and range of biological organisms and the further complexity within the neural organization of any particular animal organism, there is a variety of biocommunication languages.\n\nA hierarchy of biocommunication languages in animals has been proposed by Subhash Kak: these languages, in order of increasing generality, are associative, re-organizational, and quantum. The three types of formal languages of the Chomsky hierarchy map into the associative language class.\n\n", "id": "8694335", "title": "Biocommunication (science)"}
{"url": "https://en.wikipedia.org/wiki?curid=123418", "text": "Transdifferentiation\n\nTransdifferentiation, also known as lineage reprogramming, is a process in which one mature somatic cell transforms into another mature somatic cell without undergoing an intermediate pluripotent state or progenitor cell type. It is a type of metaplasia, which includes all cell fate switches, including the interconversion of stem cells. Current uses of transdifferentiation include disease modeling and drug discovery and in the future may include gene therapy and regenerative medicine. The term 'transdifferentiation' was originally coined by Selman and Kafatos in 1974 to describe a change in cell properties as cuticle producing cells became salt-secreting cells in silk moths undergoing metamorphosis.\n\nDavis et al. 1987 reported the first instance of transdifferentiation where a cell changed from one adult cell type to another. Forcing mouse embryonic fibroblasts to express MyoD was found to be sufficient to turn those cells into myoblasts.\n\nThere are no known instances where adult cells change directly from one lineage to another except \"Turritopsis dohrnii\". Rather, cells dedifferentiate and then redifferentiate into the cell type of interest. In newts when the eye lens is removed, pigmented epithelial cells de-differentiate and then redifferentiate into the lens cells.\nIn the pancreas, it has been demonstrated that alpha cells can spontaneously switch fate and transdifferentiate into beta cells in both healthy and diabetic human and mouse pancreatic islets.\nWhile it was previously believed that oesophageal cells were developed from the transdifferentiation of smooth muscle cells, that has been shown to be false.\n\nThe first example of functional transdifferentiation has been provided by Ferber et al. by induce a shift in the developmental fate of cells in liver and convert them into 'pancreatic beta-cell-like' cells. The cells induced a wide, functional and long-lasting transdifferentiation process that reduced the effects of hyperglycemia in diabetic mice. Moreover, the trans-differentiated beta-like cells were found to be resistant to the autoimmune attack that characterizes type 1 diabetes.\n\nThe second step was to undergo transdifferentiation in human specimens. By transducing liver cells with a single gene, Sapir et al. were able to induce human liver cells to transdifferentiate into human beta cells.\n\nThis approach has been demonstrated in mice, rat, xenopus and human tissues (Al-Hasani et al., 2013).\n\nSchematic model of the hepatocyte-to-beta cell transdifferentiation process. Hepatocytes are obtained by liver biopsy from diabetic patient, cultured and expanded ex vivo, transduced with a PDX1 virus, transdifferentiated into functional insulin-producing beta cells, and transplanted back into the patient.\n\nIn this approach, transcription factors from progenitor cells of the target cell type are transfected into a somatic cell to induce transdifferentiation. There exists two different means of determining which transcription factors to use: by starting with a large pool and narrowing down factors one by one or by starting with one or two and adding more. One theory to explain the exact specifics is that ectopic TFs directs the cell to an earlier progenitor state and then redirects it towards a new cell type. Rearrangement of the chromatin structure via DNA methylation or histone modification may play a role as well. Here is a list of in vitro examples and in vivo examples. In vivo methods of transfecting specific mouse cells utilize the same kinds of vectors as in vitro experiments, except that the vector is injected into a specific organ. Zhou et al. (2008) injected Ngn3, Pdx1 and Mafa into the dorsal splenic lobe (pancreas) of mice to reprogram pancreatic exocrine cells into β-cells in order to ameliorate hyperglycaemia.\n\nSomatic cells are first transfected with pluripotent reprogramming factors temporarily (Oct4, Sox2, Nanog, etc.) before being transfected with the desired inhibitory or activating factors. Here is a list of \nexamples in vitro.\n\nThe DNA methylation inhibitor, 5-azacytidine is also known to promote phenotypic transdifferentiation of cardiac cells to skeletal myoblasts.\n\nThe transcription factors serve as a short term trigger to an irreversible process. The transdifferentiation liver cells observed 8 months after one single injection of pdx1.\n\nThe ectopic transcription factors turn off the host repertoire of gene expression in each of the cells. However, the alternate desired repertoire is being turned on only in a subpopulation of predisposed cells. Despite the massive dedifferentiation – lineage tracing approach indeed demonstrates that transdifferentiation originates in adult cells.\n\nDetermining the unique set of cellular factors that is needed to be manipulated for each cell conversion is a long and costly process that involved much trial and error. As a result, this first step of identifying the key set of cellular factors for cell conversion is the major obstacle researchers face in the field of cell reprogramming. An international team of researchers have developed an algorithm, called Mogrify(1), that can predict the optimal set of cellular factors required to convert one human cell type to another.\nWhen tested, Mogrify was able to accurately predict the set of cellular factors required for previously published cell conversions correctly. To further validate Mogrify's predictive ability, the team conducted two novel cell conversions in the laboratory using human cells, and these were successful in both attempts solely using the predictions of Mogrify. Mogrify has been made available online for other researchers and scientists.\n\nWhen examining transdifferentiated cells, it is important to look for markers of the target cell type and the absence of donor cell markers which can be accomplished using green fluorescent protein or immunodetection. It is also important to examine the cell function, epigenome, transcriptome, and proteome profiles. Cells can also be evaluated based upon their ability to integrate into the corresponding tissue in vivo and functionally replace its natural counterpart. In one study, transdifferentiating tail-tip fibroblasts into hepatocyte-like cells using transcription factors Gata4, Hnf1α and Foxa3, and inactivation of p19(Arf) restored hepatocyte-like liver functions in only half of the mice using survival as a means of evaluation.\n\nGenerally transdifferentiation that occurs in mouse cells does not translate in effectiveness or speediness in human cells. Pang et al. found that while transcription factors Ascl1, Brn2 and Myt1l turned mouse cells into mature neurons, the same set of factors only turned human cells into immature neurons. However, the addition of NeuroD1 was able to increase efficiency and help cells reach maturity.\n\nThe order of expression of transcription factors can direct the fate of the cell. Iwasaki et al. (2006) showed that in hematopoietic lineages, the expression timing of Gata-2 and (C/EBPalpha) can change whether or not a lymphoid-committed progenitors can differentiate into granulocyte/monocyte progenitor, eosinophil, basophil or bipotent basophil/mast cell progenitor lineages.\n\nIt has been found for induced pluripotent stem cells that when injected into mice, the immune system of the synergeic mouse rejected the teratomas forming. Part of this may be because the immune system recognized epigenetic markers of specific sequences of the injected cells. However, when embryonic stem cells were injected, the immune response was much lower. Whether or not this will occur within transdifferentiated cells remains to be researched.\n\nIn order to accomplish transfection, one may use integrating viral vectors such as lentiviruses or retroviruses, non-integrating vectors such as Sendai viruses or adenoviruses, microRNAs and a variety of other methods including using proteins and plasmids; one example is the non-viral delivery of transcription factor-encoding plasmids with a polymeric carrier to elicit neuronal transdifferentiation of fibroblasts. When foreign molecules enter cells, one must take into account the possible drawbacks and potential to cause tumorous growth. Integrating viral vectors have the chance to cause mutations when inserted into the genome. One method of going around this is to excise the viral vector once reprogramming has occurred, an example being Cre-Lox recombination Non-integrating vectors have other issues concerning efficiency of reprogramming and also the removal of the vector. Other methods are relatively new fields and much remains to be discovered.\n\n\n", "id": "123418", "title": "Transdifferentiation"}
{"url": "https://en.wikipedia.org/wiki?curid=18801936", "text": "The Chemical Basis of Morphogenesis\n\n\"The Chemical Basis of Morphogenesis\" is an article written by the English mathematician Alan Turing in 1952 describing the way in which natural patterns such as stripes, spots and spirals may arise naturally out of a homogeneous, uniform state. The theory, which can be called a reaction–diffusion theory of morphogenesis, has served as a basic model in theoretical biology.\n\nReaction–diffusion systems have attracted much interest as a prototype model for pattern formation. Patterns such as fronts, spirals, targets, hexagons, stripes and dissipative solitons are found in various types of reaction-diffusion systems in spite of large discrepancies e.g. in the local reaction terms.\nReaction-diffusion processes form one class of explanation for the embryonic development of animal coats and skin pigmentation. Another reason for the interest in reaction-diffusion systems is that although they represent nonlinear partial differential equations, there are often possibilities for an analytical treatment.\n\n", "id": "18801936", "title": "The Chemical Basis of Morphogenesis"}
{"url": "https://en.wikipedia.org/wiki?curid=34969847", "text": "Field cancerization\n\nField cancerization (also termed field change, field change cancerization, field carcinogenesis, cancer field effect or premalignant field defect) is a biological process in which large areas of cells at a tissue surface or within an organ are affected by a carcinogenic alteration(s). The process arises from exposure to an injurious environment, often over a lengthy period.\n\nThe initial step in field cancerization is associated with various molecular lesions such as acquired genetic mutations and epigenetic changes, occurring over a widespread, multi-focal \"field\". These initial molecular changes may subsequently progress to cytologically recognizable premalignant foci of dysplasia, and eventually to carcinoma in situ (CIS) or cancer. The image of a longitudinally opened colon resection on this page shows an area of a colon resection that likely has a field cancerization or field defect. It has one cancer and four premalignant polyps.\nField cancerization can occur in any tissue. Prominent examples of field cancerization include premalignant field defects in head and neck cancer, lung cancer, colorectal cancer, Barrett's esophagus, skin, breast ducts and bladder. Field cancerization has implications for cancer surveillance and treatment. Despite adequate resection and being histologically normal, the remaining locoregional tissue has an increased risk for developing multiple independent cancers, either synchronously or metachronously.\n\nA common carcinogenic alteration, found in many cancers and in their adjacent field defects from which the cancers likely arose, is reduced expression of one or more DNA repair enzymes. Since reduced DNA repair expression is often present in a field cancerization or a field defect, it is likely to have been an early step in progression to the cancer.\n\nField defects associated with gastrointestinal tract cancers also commonly displayed reduced apoptosis competence, aberrant proliferation and genomic instability. Field defects of the gastrointestinal tract that show those common faults occurred in the oropharynx, esophagus, stomach, bile duct, pancreas, small intestine and colon/rectum.\n\nThe field defect adjacent to a colon cancer consists of the inner surface of the colon (the epithelium) that has about 1 million crypts (indentations in the surface of the epithelium). Each crypt has about 5,000 cells in the shape of a test-tube and all 5,000 cells of the crypt are generated from the few stem cells at the base of the crypt. The stem cells at the base of the crypt can undergo \"crypt conversion\" where a stem cell with a selective advantage takes over the stem cell niche, and all cells of that crypt display consistent expression (high or low) of a protein being evaluated.\n\nThe diagram shows results obtained by Facista et al. A particular colon resection from a colon cancer patient was evaluated for expression of 3 different DNA repair enzymes: KU86 (active in the non-homologous end joining pathway), ERCC1 (active in the nucleotide excision DNA repair pathway) and PMS2 (active in the mismatch DNA repair pathway). The percent of crypts in 6 tissue samples taken within the field defect were evaluated for frequency of high levels of expression of each of the repair proteins. Almost every crypt in all tissue samples from this patient showed high expression of KU86. However, the majority of crypts in all 6 tissue samples were reduced or absent in protein expression of ERCC1 and PMS2. The crypts with reduced or absent expression of ERCC1 or PMS2 usually occurred in large patches of adjacent crypts. Both ERCC1 and PMS2, in these tissue samples, were thought to be deficient due to epigenetic alterations.\n", "id": "34969847", "title": "Field cancerization"}
{"url": "https://en.wikipedia.org/wiki?curid=26805", "text": "Sex\n\nOrganisms of many species are specialized into male and female varieties, each known as a sex, with some falling in between being intersex. Sexual reproduction involves the combining and mixing of genetic traits: specialized cells known as gametes combine to form offspring that inherit traits from each parent. Gametes can be identical in form and function (known as isogamy), but in many cases an asymmetry has evolved such that two sex-specific types of gametes (heterogametes) exist (known as anisogamy).\n\nAmong humans and other mammals, males typically carry XY chromosomes, whereas females typically carry XX chromosomes, which are a part of the XY sex-determination system. Other animals have a sex-determination system as well, such as the ZW sex-determination system in birds, and the X0 sex-determination system in insects.\n\nThe gametes produced by an organism are determined by its sex: males produce male gametes (spermatozoa, or sperm, in animals; pollen in plants) while females produce female gametes (ova, or egg cells); individual organisms which produce both male and female gametes are termed hermaphroditic. Frequently, physical differences are associated with the different sexes of an organism; these sexual dimorphisms can reflect the different reproductive pressures the sexes experience. For instance, mate choice and sexual selection can accelerate the evolution of physical differences between the sexes.\n\nOne of the basic properties of life is reproduction, the capacity to generate new individuals, and sex is an aspect of this process. Life has evolved from simple stages to more complex ones, and so have the reproduction mechanisms. Initially the reproduction was a replicating process that consists in producing new individuals that contain the same genetic information as the original or parent individual. This mode of reproduction is called \"asexual\", and it is still used by many species, particularly unicellular, but it is also very common in multicellular organisms. In sexual reproduction, the genetic material of the offspring comes from two different individuals. As sexual reproduction developed by way of a long process of evolution, intermediates exist. Bacteria, for instance, reproduce asexually, but undergo a process by which a part of the genetic material of an individual (donor) is transferred to an other (recipient).\n\nDisregarding intermediates, the basic distinction between asexual and sexual reproduction is the way in which the genetic material is processed. Typically, prior to an asexual division, a cell duplicates its genetic information content, and then divides. This process of cell division is called mitosis. In sexual reproduction, there are special kinds of cells that divide without prior duplication of its genetic material, in a process named meiosis. The resulting cells are called gametes, and contain only half the genetic material of the parent cells. These gametes are the cells that are prepared for the sexual reproduction of the organism. Sex comprises the arrangements that enable sexual reproduction, and has evolved alongside the reproduction system, starting with similar gametes (isogamy) and progressing to systems that have different gamete types, such as those involving a large female gamete (ovum) and a small male gamete (sperm).\n\nIn complex organisms, the sex organs are the parts that are involved in the production and exchange of gametes in sexual reproduction. Many species, particularly animals, have sexual specialization, and their populations are divided into male and female individuals. Conversely, there are also species in which there is no sexual specialization, and the same individuals both contain masculine and feminine reproductive organs, and they are called hermaphrodites. This is very frequent in plants.\n\nSexual reproduction first probably evolved about a billion years ago within ancestral single-celled eukaryotes. The reason for the evolution of sex, and the reason(s) it has survived to the present, are still matters of debate. Some of the many plausible theories include: that sex creates variation among offspring, sex helps in the spread of advantageous traits, that sex helps in the removal of disadvantageous traits, and that sex facilitates repair of germ-line DNA.\n\nSexual reproduction is a process specific to eukaryotes, organisms whose cells contain a nucleus and mitochondria. In addition to animals, plants, and fungi, other eukaryotes (e.g. the malaria parasite) also engage in sexual reproduction. Some bacteria use conjugation to transfer genetic material between cells; while not the same as sexual reproduction, this also results in the mixture of genetic traits.\n\nThe defining characteristic of sexual reproduction in eukaryotes is the difference between the gametes and the binary nature of fertilization. Multiplicity of gamete types within a species would still be considered a form of sexual reproduction. However, no third gamete is known in multicellular animals.\n\nWhile the evolution of sex dates to the prokaryote or early eukaryote stage, the origin of chromosomal sex determination may have been fairly early in eukaryotes (see Evolution of anisogamy). The ZW sex-determination system is shared by birds, some fish and some crustaceans. XY sex determination is used by most mammals, but also some insects, and plants (\"Silene latifolia\"). X0 sex-determination is found in certain insects.\n\nNo genes are shared between the avian ZW and mammal XY chromosomes, and from a comparison between chicken and human, the Z chromosome appeared similar to the autosomal chromosome 9 in human, rather than X or Y, suggesting that the ZW and XY sex-determination systems do not share an origin, but that the sex chromosomes are derived from autosomal chromosomes of the common ancestor of birds and mammals.\nA paper from 2004 compared the chicken Z chromosome with platypus X chromosomes and suggested that the two systems are related.\n\nSexual reproduction in eukaryotes is a process whereby organisms form offspring that combine genetic traits from both parents. Chromosomes are passed on from one generation to the next in this process. Each cell in the offspring has half the chromosomes of the mother and half of the father.\nGenetic traits are contained within the deoxyribonucleic acid (DNA) of chromosomes—by combining one of each type of chromosomes from each parent, an organism is formed containing a doubled set of chromosomes. This double-chromosome stage is called \"diploid\", while the single-chromosome stage is \"haploid\". Diploid organisms can, in turn, form haploid cells (gametes) that randomly contain one of each of the chromosome pairs, via meiosis. Meiosis also involves a stage of chromosomal crossover, in which regions of DNA are exchanged between matched types of chromosomes, to form a new pair of mixed chromosomes. Crossing over and fertilization (the recombining of single sets of chromosomes to make a new diploid) result in the new organism containing a different set of genetic traits from either parent.\n\nIn many organisms, the haploid stage has been reduced to just gametes specialized to recombine and form a new diploid organism; in others, the gametes are capable of undergoing cell division to produce multicellular haploid organisms. In either case, gametes may be externally similar, particularly in size (isogamy), or may have evolved an asymmetry such that the gametes are different in size and other aspects (anisogamy).\nBy convention, the larger gamete (called an ovum, or egg cell) is considered female, while the smaller gamete (called a spermatozoon, or sperm cell) is considered male. An individual that produces exclusively large gametes is female, and one that produces exclusively small gametes is male. An individual that produces both types of gametes is a hermaphrodite; in some cases hermaphrodites are able to self-fertilize and produce offspring on their own, without a second organism.\n\nMost sexually reproducing animals spend their lives as diploid organisms, with the haploid stage reduced to single cell gametes. The gametes of animals have male and female forms—spermatozoa and egg cells. These gametes combine to form embryos which develop into a new organism.\n\nThe male gamete, a spermatozoon (produced within a testicle), is a small cell containing a single long flagellum which propels it.\nSpermatozoa are extremely reduced cells, lacking many cellular components that would be necessary for embryonic development. They are specialized for motility, seeking out an egg cell and fusing with it in a process called fertilization.\n\nFemale gametes are egg cells (produced within ovaries), large immobile cells that contain the nutrients and cellular components necessary for a developing embryo.\nEgg cells are often associated with other cells which support the development of the embryo, forming an egg. In mammals, the fertilized embryo instead develops within the female, receiving nutrition directly from its mother.\n\nAnimals are usually mobile and seek out a partner of the opposite sex for mating. Animals which live in the water can mate using external fertilization, where the eggs and sperm are released into and combine within the surrounding water. Most animals that live outside of water, however, must transfer sperm from male to female to achieve internal fertilization.\n\nIn most birds, both excretion and reproduction is done through a single posterior opening, called the cloaca—male and female birds touch cloaca to transfer sperm, a process called \"cloacal kissing\". In many other terrestrial animals, males use specialized sex organs to assist the transport of sperm—these male sex organs are called intromittent organs. In humans and other mammals this male organ is the penis, which enters the female reproductive tract (called the vagina) to achieve insemination—a process called sexual intercourse. The penis contains a tube through which semen (a fluid containing sperm) travels. In female mammals the vagina connects with the uterus, an organ which directly supports the development of a fertilized embryo within (a process called gestation).\n\nBecause of their motility, animal sexual behavior can involve coercive sex. Traumatic insemination, for example, is used by some insect species to inseminate females through a wound in the abdominal cavity—a process detrimental to the female's health.\n\nLike animals, plants have developed specialized male and female gametes. Within seed plants, male gametes are contained within hard coats, forming pollen. The female gametes of plants are contained within ovules; once fertilized by pollen these form seeds which, like eggs, contain the nutrients necessary for the development of the embryonic plant.\n\nMany plants have flowers and these are the sexual organs of those plants. Flowers are usually hermaphroditic, producing both male and female gametes. The female parts, in the center of a flower, are the pistils, each unit consisting of a carpel, a style and a stigma. One or more of these reproductive units may be merged to form a single compound pistil. Within the carpels are ovules which develop into seeds after fertilization. The male parts of the flower are the stamens: these consist of long filaments arranged between the pistil and the petals that produce pollen in anthers at their tips. When a pollen grain lands upon the stigma on top of a carpel's style, it germinates to produce a pollen tube that grows down through the tissues of the style into the carpel, where it delivers male gamete nuclei to fertilize an ovule that eventually develops into a seed.\n\nIn pines and other conifers the sex organs are conifer cones and have male and female forms. The more familiar female cones are typically more durable, containing ovules within them. Male cones are smaller and produce pollen which is transported by wind to land in female cones. As with flowers, seeds form within the female cone after pollination.\n\nBecause plants are immobile, they depend upon passive methods for transporting pollen grains to other plants. Many plants, including conifers and grasses, produce lightweight pollen which is carried by wind to neighboring plants. Other plants have heavier, sticky pollen that is specialized for transportation by insects. The plants attract these insects or larger animals such as humming birds and bats with nectar-containing flowers. These animals transport the pollen as they move to other flowers, which also contain female reproductive organs, resulting in pollination.\n\nMost fungi reproduce sexually, having both a haploid and diploid stage in their life cycles. These fungi are typically isogamous, lacking male and female specialization: haploid fungi grow into contact with each other and then fuse their cells. In some of these cases the fusion is asymmetric, and the cell which donates only a nucleus (and not accompanying cellular material) could arguably be considered \"male\".\n\nSome fungi, including baker's yeast, have mating types that create a duality similar to male and female roles. Yeast with the same mating type will not fuse with each other to form diploid cells, only with yeast carrying the other mating type.\n\nFungi produce mushrooms as part of their sexual reproduction. Within the mushroom diploid cells are formed, later dividing into haploid spores—the height of the mushroom aids the dispersal of these sexually produced offspring.\n\nThe most basic sexual system is one in which all organisms are hermaphrodites, producing both male and female gametes— this is true of some animals (e.g. snails) and the majority of flowering plants. In many cases, however, specialization of sex has evolved such that some organisms produce only male or only female gametes. The biological cause for an organism developing into one sex or the other is called sex determination.\n\nIn the majority of species with sex specialization, organisms are either male (producing only male gametes) or female (producing only female gametes). Exceptions are common—for example, the roundworm \"C. elegans\" has an hermaphrodite and a male sex (a system called androdioecy).\n\nSometimes an organism's development is intermediate between male and female, a condition called intersex. Sometimes intersex individuals are called \"hermaphrodite\"; but, unlike biological hermaphrodites, intersex individuals are unusual cases and are not typically fertile in both male and female aspects.\n\nIn genetic sex-determination systems, an organism's sex is determined by the genome it inherits. Genetic sex-determination usually depends on asymmetrically inherited sex chromosomes which carry genetic features that influence development; sex may be determined either by the presence of a sex chromosome or by how many the organism has. Genetic sex-determination, because it is determined by chromosome assortment, usually results in a 1:1 ratio of male and female offspring.\n\nHumans and other mammals have an XY sex-determination system: the Y chromosome carries factors responsible for triggering male development. The \"default sex,\" in the absence of a Y chromosome, is female-like. Thus, XX mammals are female and XY are male. In humans, biological sex is determined by five factors present at birth: the presence or absence of a Y chromosome (which alone determines the individual's \"genetic sex\"), the type of gonads, the sex hormones, the internal reproductive anatomy (such as the uterus in females), and the external genitalia.\n\nXY sex determination is found in other organisms, including the common fruit fly and some plants. In some cases, including in the fruit fly, it is the number of X chromosomes that determines sex rather than the presence of a Y chromosome (see below).\n\nIn birds, which have a ZW sex-determination system, the opposite is true: the W chromosome carries factors responsible for female development, and default development is male. In this case ZZ individuals are male and ZW are female. The majority of butterflies and moths also have a ZW sex-determination system. In both XY and ZW sex determination systems, the sex chromosome carrying the critical factors is often significantly smaller, carrying little more than the genes necessary for triggering the development of a given sex.\n\nMany insects use a sex determination system based on the number of sex chromosomes. This is called X0 sex-determination—the 0 indicates the absence of the sex chromosome. All other chromosomes in these organisms are diploid, but organisms may inherit one or two X chromosomes. In field crickets, for example, insects with a single X chromosome develop as male, while those with two develop as female. In the nematode \"C. elegans\" most worms are self-fertilizing XX hermaphrodites, but occasionally abnormalities in chromosome inheritance regularly give rise to individuals with only one X chromosome—these X0 individuals are fertile males (and half their offspring are male).\n\nOther insects, including honey bees and ants, use a haplodiploid sex-determination system. In this case diploid individuals are generally female, and haploid individuals (which develop from unfertilized eggs) are male. This sex-determination system results in highly biased sex ratios, as the sex of offspring is determined by fertilization rather than the assortment of chromosomes during meiosis.\n\nFor many species, sex is not determined by inherited traits, but instead by environmental factors experienced during development or later in life. Many reptiles have temperature-dependent sex determination: the temperature embryos experience during their development determines the sex of the organism. In some turtles, for example, males are produced at lower incubation temperatures than females; this difference in critical temperatures can be as little as 1–2 °C.\n\nMany fish change sex over the course of their lifespan, a phenomenon called sequential hermaphroditism. In clownfish, smaller fish are male, and the dominant and largest fish in a group becomes female. In many wrasses the opposite is true—most fish are initially female and become male when they reach a certain size. Sequential hermaphrodites may produce both types of gametes over the course of their lifetime, but at any given point they are either female or male.\n\nIn some ferns the default sex is hermaphrodite, but ferns which grow in soil that has previously supported hermaphrodites are influenced by residual hormones to instead develop as male.\n\nMany animals and some plants have differences between the male and female sexes in size and appearance, a phenomenon called sexual dimorphism. Sex differences in humans include, generally, a larger size and more body hair in men; women have breasts, wider hips, and a higher body fat percentage. In other species, the differences may be more extreme, such as differences in coloration or bodyweight.\n\nSexual dimorphisms in animals are often associated with sexual selection – the competition between individuals of one sex to mate with the opposite sex. Antlers in male deer, for example, are used in combat between males to win reproductive access to female deer. In many cases the male of a species is larger than the female. Mammal species with extreme sexual size dimorphism tend to have highly polygynous mating systems—presumably due to selection for success in competition with other males—such as the elephant seals. Other examples demonstrate that it is the preference of females that drive sexual dimorphism, such as in the case of the stalk-eyed fly.\n\nOther animals, including most insects and many fish, have larger females. This may be associated with the cost of producing egg cells, which requires more nutrition than producing sperm—larger females are able to produce more eggs. For example, female southern black widow spiders are typically twice as long as the males. Occasionally this dimorphism is extreme, with males reduced to living as parasites dependent on the female, such as in the anglerfish. Some plant species also exhibit dimorphism in which the females are significantly larger than the males, such as in the moss \"Dicranum\" and the liverwort \"Sphaerocarpos\". There is some evidence that, in these genera, the dimorphism may be tied to a sex chromosome, or to chemical signalling from females.\n\nIn birds, males often have a more colourful appearance and may have features (like the long tail of male peacocks) that would seem to put the organism at a disadvantage (e.g. bright colors would seem to make a bird more visible to predators). One proposed explanation for this is the handicap principle. This hypothesis says that, by demonstrating he can survive with such handicaps, the male is advertising his genetic fitness to females—traits that will benefit daughters as well, who will not be encumbered with such handicaps.\n\n\n", "id": "26805", "title": "Sex"}
{"url": "https://en.wikipedia.org/wiki?curid=47772789", "text": "Biotin hydrazide\n\nBiotin hydrazide is a biotinyl derivative that can be used as a probe for the determination of protein carbonylation. It readily forms Schiff bases with carbonyl groups.\n", "id": "47772789", "title": "Biotin hydrazide"}
{"url": "https://en.wikipedia.org/wiki?curid=5652450", "text": "Biological process\n\nBiological processes are the processes vital for a living organism to live. Biological processes are made up of many chemical reactions or other events that are involved in the persistence and transformation of life forms. Metabolism and homeostasis are examples.\n\nRegulation of biological processes occurs when any process is modulated in its frequency, rate or extent. Biological processes are regulated by many means; examples include the control of gene expression, protein modification or interaction with a protein or substrate molecule.\n\n\nhttp://geneontology.org/page/biological-process-ontology-guidelines\n", "id": "5652450", "title": "Biological process"}
{"url": "https://en.wikipedia.org/wiki?curid=379303", "text": "Gas exchange\n\nGas exchange is the biological process by which gases move passively by diffusion across a surface. Typically, this surface is - or contains - a biological membrane that forms the boundary between an organism and its extracellular environment.\n\nGases are constantly consumed and produced by cellular and metabolic reactions in most living things, so an efficient system for gas exchange between, ultimately, the interior of the cell(s) and the external environment is required. Small, particularly unicellular organisms, such as bacteria and protozoa, have a high surface-area to volume ratio. In these creatures the gas exchange membrane is typically the cell membrane. Some small multicellular organisms, such as flatworms, are also able to perform sufficient gas exchange across the skin or cuticle that surrounds their bodies. However, in most larger organisms, which have a small surface-area to volume ratios, specialised structures with convoluted surfaces such as gills, pulmonary alveoli and spongy mesophyll provide the large area needed for effective gas exchange. These convoluted surfaces may sometimes be internalised into the body of the organism. This is the case with the alveoli, which form the inner surface of the mammalian lung, the spongy mesophyll, which is found inside the leaves of some kinds of plant, or the gills of those molluscs that have them, which are found in the mantle cavity.\n\nIn aerobic organisms, gas exchange is particularly important for respiration, which involves the uptake of oxygen () and release of carbon dioxide (). Conversely, in oxygenic photosynthetic organisms such as most land plants, uptake of carbon dioxide and release of both oxygen and water vapour are the main gas-exchange processes occurring during the day. Other gas-exchange processes are important in less familiar organisms: \"e.g.\" carbon dioxide, methane and hydrogen are exchanged across the cell membrane of methanogenic archaea. In nitrogen fixation by diazotrophic bacteria, and denitrification by heterotrophic bacteria (such as \"Paracoccus denitrificans\" and various pseudomonads), nitrogen gas is exchanged with the environment, being taken up by the former and released into it by the latter, while giant tube worms rely on bacteria to oxidize hydrogen sulfide extracted from their deep sea environment, using dissolved oxygen in the water as an electron acceptor.\n\nThe exchange of gases occurs as a result of diffusion down a concentration gradient. Gas molecules move from a region in which they are at high concentration to one in which they are at low concentration. Diffusion is a passive process, meaning that no energy is required to power the transport, and it follows Fick’s Law: \n\nIn relation to a typical biological system, where two compartments ('inside' and 'outside'), are separated by a membrane barrier, and where a gas is allowed to spontaneously diffuse down its concentration gradient:\n\n\nFig. 1. Fick's Law for gas-exchange surface\n\nGases must first dissolve in a liquid in order to diffuse across a membrane, so all biological gas exchange systems require a moist environment. In general, the higher the concentration gradient across the gas-exchanging surface, the faster the rate of diffusion across it. Conversely, the thinner the gas-exchanging surface (for the same concentration difference), the faster the gases will diffuse across it.\n\nIn the equation above, \"J\" is the flux expressed per unit area, so increasing the area will make no difference to its value. However, an increase in the available surface area, will increase the \"amount\" of gas that can diffuse in a given time. This is because the amount of gas diffusing per unit time (d\"q\"/d\"t\") is the product of \"J\" and the area of the gas-exchanging surface, \"A\":\n\nSingle-celled organisms such as bacteria and amoebae do not have specialised gas exchange surfaces, because they can take advantage of the high surface area they have relative to their volume. The amount of gas an organism produces (or requires) in a given time will be in rough proportion to the volume of its cytoplasm. The volume of a unicellular organism is very small, therefore it produces (and requires) a relatively small amount of gas in a given time. In comparison to this small volume, the surface area of its cell membrane is very large, and adequate for its gas-exchange needs without further modification. However, as an organism increases in size, its surface area and volume do not scale in the same way. Consider an imaginary organism that is a cube of side-length, \"L\". Its volume increases with the cube (\"L\") of its length, but its external surface area increases only with the square (\"L\") of its length. This means the external surface rapidly becomes inadequate for the rapidly increasing gas-exchange needs of a larger volume of cytoplasm. Additionally, the thickness of the surface that gases must cross (d\"x\" in Fick's Law) can also be larger in larger organisms: in the case of a single-celled organism, a typical cell membrane is only 10 nm thick; but in larger organisms such as roundworms (Nematoda) the equivalent exchange surface - the cuticle - is substantially thicker at 0.5 µm.\n\nIn multicellular organisms therefore, specialised respiratory organs such as gills or lungs are often used to provide the additional surface area for the required rate of gas exchange with the external environment. However the distances between the gas exchanger and the deeper tissues are often too great for diffusion to meet gaseous requirements of these tissues. The gas exchangers are therefore frequently coupled to gas-distributing circulatory systems, which transport the gases evenly to all the body tissues regardless of their distance from the gas exchanger.\n\nSome multicellular organisms such as flatworms (Platyhelminthes) are relatively large but very thin, allowing their outer body surface to act as a gas exchange surface without the need for a specialised gas exchange organ. Flatworms therefore lack gills or lungs, and also lack a circulatory system. Other multicellular organisms such as sponges (Porifera) have an inherently high surface area, because they are very porous and/or branched. Sponges do not require a circulatory system or specialised gas exchange organs, because their feeding strategy involves one-way pumping of water through their porous bodies using flagellated collar cells. Each cell of the sponge's body is therefore exposed to a constant flow of fresh oxygenated water. They can therefore rely on diffusion across their cell membranes to carry out the gas exchange needed for respiration.\n\nIn organisms that have circulatory systems associated with their specialized gas-exchange surfaces, a great variety of systems are used for the interaction between the two.\n\nIn a countercurrent flow system, air (or, more usually, the water containing dissolved air) is drawn in the \"opposite\" direction to the flow of blood in the gas exchanger. A countercurrent system such as this maintains a steep concentration gradient along the length of the gas-exchange surface (see lower diagram in Fig. 2). This is the situation seen in the gills of fish and many other aquatic creatures. The gas-containing environmental water is drawn unidirectionally across the gas-exchange surface, with the blood-flow in the gill capillaries beneath flowing in the opposite direction. Although this theoretically allows almost complete transfer of a respiratory gas from one side of the exchanger to the other, in fish less than 80% of the oxygen in the water flowing over the gills is generally transferred to the blood.\n\nAlternative arrangements are cross current systems found in birds. and dead-end air-filled sac systems found in the lungs of mammals. In a cocurrent flow system, the blood and gas (or the fluid containing the gas) move in the same direction through the gas exchanger. This means the magnitude of the gradient is variable along the length of the gas-exchange surface, and the exchange will eventually stop when an equilibrium has been reached (see upper diagram in Fig. 2).\nCocurrent flow gas exchange systems are not known to be used in nature.\n\nThe gas exchanger in mammals is internalized to form lungs, as it is in most of the larger land animals. Gas exchange occurs in microscopic dead-end air-filled sacs called alveoli, where a very thin membrane (called the blood-air barrier) separates the blood in the alveolar capillaries (in the walls of the alveoli) from the alveolar air in the sacs.\n\nThe membrane across which gas exchange takes place in the alveoli (i.e. the blood-air barrier) is extremely thin (in humans, on average, 2.2 μm thick). It consists of the alveolar epithelial cells, their basement membranes and the endothelial cells of the pulmonary capillaries (Fig. 4). The large surface area of the membrane comes from the folding of the membrane into about 300 million alveoli, with diameters of approximately 75-300 µm each. This provides an extremely large surface area (approximately 145 m) across which gas exchange can occur.\n\nAir is brought to the alveoli in small doses (called the tidal volume), by breathing in (inhalation) and out (exhalation) through the respiratory airways, a set of relatively narrow and moderately long tubes which start at the nose or mouth and end in the alveoli of the lungs in the chest. Air moves in and out through the same set of tubes, in which the flow is in one direction during inhalation, and in the opposite direction during exhalation.\n\nDuring each inhalation, at rest, approximately 500 ml of fresh air flows in through the nose. Its is warmed and moistened as it flows through the nose and pharynx. By the time it reaches the trachea the inhaled air's temperature is 37 °C and it is saturated with water vapor. On arrival in the alveoli it is diluted and thoroughly mixed with the approximately 2.5–3.0 liters of air that remained in the alveoli after the last exhalation. This relatively large volume of air that is semi-permanently present in the alveoli throughout the breathing cycle is known as the functional residual capacity (FRC).\n\nAt the beginning of inhalation the airways are filled with unchanged alveolar air, left over from the last exhalation. This is the dead space volume, which is usually about 150 ml. It is the first air to re-enter the alveoli during inhalation. Only after the dead space air has returned to the alveoli does the remainder of the tidal volume (500 ml - 150 ml = 350 ml) enter the alveoli. The entry of such a small volume of fresh air with each inhalation, ensures that the composition of the FRC hardly changes during the breathing cycle (Fig. 5). The alveolar partial pressure of oxygen remains very close to 13–14 kPa (100 mmHg), and the partial pressure of carbon dioxide varies minimally around 5.3 kPa (40 mmHg) throughout the breathing cycle (of inhalation and exhalation). The corresponding partial pressures of oxygen and carbon dioxide in the ambient (dry) air at sea level are 21 kPa (160 mmHg) and 0.04 kPa (0.3 mmHg) respectively.\nThis alveolar air, which constitutes the FRC, completely surrounds the blood in the alveolar capillaries (Fig. 6). Gas exchange in mammals occurs between this alveolar air (which differs significantly from fresh air) and the blood in the alveolar capillaries. The gases on either side of the gas exchange membrane equilibrate by simple diffusion. This ensures that the partial pressures of oxygen and carbon dioxide in the blood leaving the alveolar capillaries, and ultimately circulates throughout the body, are the same as those in the FRC.\n\nThe marked difference between the composition of the alveolar air and that of the ambient air can be maintained because the functional residual capacity is contained in dead-end sacs connected to the outside air by long, narrow, tubes (the airways: nose, pharynx, larynx, trachea, bronchi and their branches and sub-branches down to the bronchioles). This anatomy, and the fact that the lungs are not emptied and re-inflated with each breath, provides mammals with a \"portable atmosphere\", whose composition differs significantly from the present-day ambient air.\n\nThe composition of the air in the FRC is carefully monitored, by measuring the partial pressures of oxygen and carbon dioxide in the arterial blood. If either gas pressure deviates from normal, reflexes are elicited that change the rate and depth of breathing in such a way that normality is restored within seconds or minutes.\n\nAll the blood returning from the body tissues to the right side of the heart flows through the alveolar capillaries before being pumped around the body again. On its passage through the lungs the blood comes into close contact with the alveolar air, separated from it by a very thin diffusion membrane which is only, on average, about 2 μm thick. The gas pressures in the blood will therefore rapidly equilibrate with those in the alveoli, ensuring that the arterial blood that circulates to all the tissues throughout the body has an oxygen tension of 13−14 kPa (100 mmHg), and a carbon dioxide tension of 5.3 kPa (40 mmHg). These arterial partial pressures of oxygen and carbon dioxide are homeostatically controlled. A rise in the arterial formula_3, and, to a lesser extent, a fall in the arterial formula_3, will reflexly cause deeper and faster breathing till the blood gas tensions return to normal. The converse happens when the carbon dioxide tension falls, or, again to a lesser extent, the oxygen tension rises: the rate and depth of breathing are reduced till blood gas normality is restored.\n\nSince the blood arriving in the alveolar capillaries has a formula_3 of, on average, 6 kPa (45 mmHg), while the pressure in the alveolar air is 13 kPa (100 mmHg), there will be a net diffusion of oxygen into the capillary blood, changing the composition of the 3 liters of alveolar air slightly. Similarly, since the blood arriving in the alveolar capillaries has a formula_3 of also about 6 kPa (45 mmHg), whereas that of the alveolar air is 5.3 kPa (40 mmHg), there is a net movement of carbon dioxide out of the capillaries into the alveoli. The changes brought about by these net flows of individual gases into and out of the functional residual capacity necessitate the replacement of about 15% of the alveolar air with ambient air every 5 seconds or so. This is very tightly controlled by the continuous monitoring of the arterial blood gas tensions (which accurately reflect partial pressures of the respiratory gases in the alveolar air) by the aortic bodies, the carotid bodies, and the blood gas and pH sensor on the anterior surface of the medulla oblongata in the brain. There are also oxygen and carbon dioxide sensors in the lungs, but they primarily determine the diameters of the bronchioles and pulmonary capillaries, and are therefore responsible for directing the flow of air and blood to different parts of the lungs.\n\nIt is only as a result of accurately maintaining the composition of the 3 liters alveolar air that with each breath some carbon dioxide is discharged into the atmosphere and some oxygen is taken up from the outside air. If more carbon dioxide than usual has been lost by a short period of hyperventilation, respiration will be slowed down or halted until the alveolar formula_3 has returned to 5.3 kPa (40 mmHg). It is therefore strictly speaking untrue that the primary function of the respiratory system is to rid the body of carbon dioxide “waste”. In fact the total concentration of carbon dioxide in arterial blood is about 26 mM (or 58 ml/100 ml), compared to the concentration of oxygen in saturated arterial blood of about 9 mM (or 20 ml/100 ml blood). This large concentration of carbon dioxide plays a pivotal role in the determination and maintenance of the pH of the extracellular fluids. The carbon dioxide that is breathed out with each breath could probably be more correctly be seen as a byproduct of the body’s extracellular fluid carbon dioxide and pH homeostats\n\nIf these homeostats are compromised, then a respiratory acidosis, or a respiratory alkalosis will occur. In the long run these can be compensated by renal adjustments to the H and HCO concentrations in the plasma; but since this takes time, the hyperventilation syndrome can, for instance, occur when agitation or anxiety cause a person to breathe fast and deeply thus blowing off too much CO from the blood into the outside air, precipitating a set of distressing symptoms which result from an excessively high pH of the extracellular fluids.\n\nOxygen has a very low solubility in water, and is therefore carried in the blood loosely combined with hemoglobin. The oxygen is held on the hemoglobin by four ferrous iron-containing heme groups per hemoglobin molecule. When all the heme groups carry one O molecule each the blood is said to be “saturated” with oxygen, and no further increase in the partial pressure of oxygen will meaningfully increase the oxygen concentration of the blood. Most of the carbon dioxide in the blood is carried as HCO ions in the plasma. However the conversion of dissolved CO into HCO (through the addition of water) is too slow for the rate at which the blood circulates through the tissues on the one hand, and alveolar capillaries on the other. The reaction is therefore catalyzed by carbonic anhydrase, an enzyme inside the red blood cells. The reaction can go in either direction depending on the prevailing partial pressure of carbon dioxide. A small amount of carbon dioxide is carried on the protein portion of the hemoglobin molecules as carbamino groups. The total concentration of carbon dioxide (in the form of bicarbonate ions, dissolved CO, and carbamino groups) in arterial blood (i.e. after it has equilibrated with the alveolar air) is about 26 mM (or 58 ml/100 ml), compared to the concentration of oxygen in saturated arterial blood of about 9 mM (or 20 ml/100 ml blood).\n\nThe dissolved oxygen content in fresh water is approximately 8–10 ml/liter compared to that of air which is 210 ml/liter. Water is 800 times more dense than air and 100 times more viscous. Therefore, oxygen has a diffusion rate in air 10,000 times greater than in water. The use of sac-like lungs to remove oxygen from water would therefore not be efficient enough to sustain life. Rather than using lungs, gaseous exchange takes place across the surface of highly vascularized gills. Gills are specialised organs containing filaments, which further divide into lamellae. The lamellae contain capillaries that provide a large surface area and short diffusion distances, as their walls are extremely thin. Gill rakers are found within the exchange system in order to filter out food, and keep the gills clean.\n\nGills use a countercurrent flow system that increases the efficiency of oxygen-uptake (and waste gas loss). Oxygenated water is drawn in through the mouth and passes over the gills in one direction while blood flows through the lamellae in the opposite direction. This countercurrent maintains steep concentration gradients along the entire length of each capillary (see the diagram in the \"Interaction with circulatory systems\" section above). Oxygen is able to continually diffuse down its gradient into the blood, and the carbon dioxide down its gradient into the water. The deoxygenated water will eventually pass out through the operculum (gill cover). Although countercurrent exchange systems theoretically allow an almost complete transfer of a respiratory gas from one side of the exchanger to the other, in fish less than 80% of the oxygen in the water flowing over the gills is generally transferred to the blood.\n\nAmphibians have three main organs involved in gas exchange: the lungs, the skin, and the gills, which can be used singly or in a variety of different combinations. The relative importance of these structures differs according to the age, the environment and species of the amphibian. The skin of amphibians and their larvae is highly vascularised, leading to relatively efficient gas exchange when the skin is moist. The larvae of amphibians, such as the pre-metamorphosis tadpole stage of frogs, also have external gills. The gills are absorbed into the body during metamorphosis, after which the lungs will then take over. The lungs are usually simpler than in the other land vertebrates, with few internal septa and larger alveoli; however, toads, which spend more time on land, have a larger alveolar surface with more developed lungs. To increase the rate of gas exchange by diffusion, amphibians maintain the concentration gradient across the respiratory surface using a process called buccal pumping. The lower floor of the mouth is moved in a \"pumping\" manner, which can be observed by the naked eye.\n\nAll reptiles breathe using lungs. In squamates (the lizards and snakes) ventilation is driven by the axial musculature, but this musculature is also used during movement, so some squamates rely on buccal pumping to maintain gas exchange efficiency.\n\nDue to the rigidity of turtle and tortoise shells, significant expansion and contraction of the chest is difficult. Turtles and tortoises depend on muscle layers attached to their shells, which wrap around their lungs to fill and empty them. Some aquatic turtles can also pump water into a highly vascularised mouth or cloaca to achieve gas-exchange.\n\nCrocodiles have a structure similar to the mammalian diaphragm - the diaphragmaticus - but this muscle helps create a unidirectional flow of air through the lungs rather than a tidal flow: this is more similar to the air-flow seen in birds than that seen in mammals. During inhalation, the diaphragmaticus pulls the liver back, inflating the lungs into the space this creates. Air flows into the lungs from the bronchus during inhalation, but during exhalation, air flows out of the lungs into the bronchus by a different route: this one-way movement of gas is achieved by aerodynamic valves in the airways.\n\nBirds have lungs but no diaphragm. They rely mostly on air sacs for ventilation. These air sacs do not play a direct role in gas exchange, but help to move air unidirectionally across the gas exchange surfaces in the lungs. During inhalation, fresh air is taken from the trachea down into the posterior air sacs and into the parabronchi which lead from the posterior air sacs into the lung. The air that enters the lungs joins the air which is already in the lungs, and is drawn forward across the gas exchanger into anterior air sacs. During exhalation, the posterior air sacs force air into the same parabronchi of the lungs, flowing in the same direction as during inhalation, allowing continuous gas exchange irrespective of the breathing cycle. Air exiting the lungs during exhalation joins the air being expelled from the anterior air sacs (both consisting of \"spent air\" that has passed through the gas exchanger) entering the trachea to be exhaled (Fig. 10). Selective bronchoconstriction at the various bronchial branch points ensures that the air does not ebb and flow through the bronchi during inhalation and exhalation, as it does in mammals, but follows the paths described above.\n\nThe unidirectional airflow through the parabronchi exchanges respiratory gases with a \"crosscurrent\" blood flow (Fig. 9). The partial pressure of O (formula_3) in the parabronchioles declines along their length as O diffuses into the blood. The capillaries leaving the exchanger near the entrance of airflow take up more O than capillaries leaving near the exit end of the parabronchi. When the contents of all capillaries mix, the final formula_3 of the mixed pulmonary venous blood is higher than that of the exhaled air, but lower than that of the inhaled air.\n\nGas exchange in plants is dominated by the roles of carbon dioxide, oxygen and water vapor. is the only carbon source for autotrophic growth by photosynthesis, and when a plant is actively photosynthesising in the light, it will be taking up carbon dioxide, and losing water vapor and oxygen. At night, plants respire, and gas exchange partly reverses: water vapor is still lost (but to a smaller extent), but oxygen is now taken up and carbon dioxide released.\nPlant gas exchange occurs mostly through the leaves. Gases diffuse into and out of the intercellular spaces within the leaf through pores called stomata, which are typically found on the lower surface of the leaf. Gases enter into the photosynthetic tissue of the leaf through dissolution onto the moist surface of the palisade and spongy mesophyll cells. The spongy mesophyll cells are loosely packed, allowing for an increased surface area, and subsequently an increased rate of gas-exchange. Uptake of carbon dioxide necessarily results in some loss of water vapor, because both molecules enter and leave by the same stomata, so plants experience a gas exchange dilemma: gaining enough without losing too much water. Therefore, water loss from other parts of the leaf is minimised by the waxy cuticle on the leaf's epidermis. The size of a stoma is regulated by the opening and closing of its two guard cells: the turgidity of these cells determines the state of the stomatal opening, and this itself is regulated by water stress. Plants showing crassulacean acid metabolism are drought-tolerant xerophytes and perform almost all their gas-exchange at night, because it is only during the night that these plants open their stomata. By opening the stomata only at night, the water vapor loss associated with carbon dioxide uptake is minimised. However, this comes at the cost of slow growth: the plant has to store the carbon dioxide in the form of malic acid for use during the day, and it cannot store unlimited amounts.\n\nGas exchange measurements are important tools in plant science: this typically involves sealing the plant (or part of a plant) in a chamber and measuring changes in the concentration of carbon dioxide with an infrared gas analyzer. If the environmental conditions (humidity, concentration, light and temperature) are fully controlled, the measurements of uptake and water release reveal important information about the assimilation and transpiration rates. The intercellular concentration reveals important information about the photosynthetic condition of the plants. Simpler methods can be used in specific circumstances: hydrogencarbonate indicator can be used to monitor the consumption of in a solution containing a single plant leaf at different levels of light intensity, and oxygen generation by the pondweed \"Elodea\" can be measured by simply collecting the gas in a submerged test-tube containing a small piece of the plant.\n\nThe mechanism of gas exchange in invertebrates depends their size, feeding strategy, and habitat (aquatic or terrestrial).\n\nThe sponges (Porifera) are sessile creatures, meaning they are unable to move on their own and normally remain attached to their substrate. They obtain nutrients through the flow of water across their cells, and they exchange gases by simple diffusion across their cell membranes. Pores called ostia draw water into the sponge and the water is subsequently circulated through the sponge by cells called choanocytes which have hair-like structures that move the water through the sponge.\n\nThe cnidarians include corals, sea anemones, jellyfish and hydras. These animals are always found in aquatic environments, ranging from fresh water to salt water. They do not have any dedicated respiratory organs; instead, every cell in their body can absorb oxygen from the surrounding water, and release waste gases to it. One key disadvantage of this feature is that cnidarians can die in environments where water is stagnant, as they deplete the water of its oxygen supply. Corals often form symbiosis with other organisms, particularly photosynthetic dinoflagellates. In this symbiosis, the coral provides shelter and the other organism provides nutrients to the coral, including oxygen.\n\nThe roundworms (Nematoda), flatworms (Platyhelminthes), and many other small invertebrate animals living in aquatic or otherwise wet habitats do not have a dedicated gas-exchange surface or circulatory system. They instead rely on diffusion of and directly across their cuticle. The cuticle is the semi-permeable outermost layer of their bodies.\n\nOther aquatic invertebrates such as most molluscs (Mollusca) and larger crustaceans (Crustacea) such as lobsters, have gills analogous to those of fish, which operate in a similar way.\nUnlike the invertebrates groups mentioned so far, insects are usually terrestrial, and exchange gases across a moist surface in direct contact with the atmosphere, rather than in contact with surrounding water. The insect's exoskeleton is impermeable to gases, including water vapor, so they have a more specialised gas exchange system, requiring gases to be directly transported to the tissues via a complex network of tubes. This respiratory system is separated from their circulatory system. Gases enter and leave the body through openings called spiracles, located laterally along the thorax and abdomen. Similar to plants, insects are able to control the opening and closing of these spiracles, but instead of relying on turgor pressure, they rely on muscle contractions. These contractions result in an insect's abdomen being pumped in and out. The spiracles are connected to tubes called tracheae, which branch repeatedly and ramify into the insect's body. These branches terminate in specialised tracheole cells which provides a thin, moist surface for efficient gas exchange, directly with cells.\n\nThe other main group of terrestrial arthropod, the arachnids (spiders, scorpion, mites, and their relatives) typically perform gas exchange with a book lung.\n\n", "id": "379303", "title": "Gas exchange"}
{"url": "https://en.wikipedia.org/wiki?curid=49223016", "text": "Electrolithoautotroph\n\nAn Electrolithoautotroph is an organism which feeds on electricity. These organisms use electricity to convert carbon dioxide to organic matters by using electrons directly taken from solid-inorganic electron donors. Electrolithoautotrophs are microorganisms which are found in the deep crevices of the ocean. The warm, mineral-rich environment provides a rich source of nutrients. The electron source for carbon assimilation from diffusible Fe(2+) ions to an electrode under the condition that electrical current is the only source of energy and electrons. Electrolithoautotrophs form a third metabolic pathway compared to photosynthesis (plants converting light into sugar) and chemosynthesis (animals consuming food)\n", "id": "49223016", "title": "Electrolithoautotroph"}
{"url": "https://en.wikipedia.org/wiki?curid=40861730", "text": "Replicate (biology)\n\nIn the biological sciences, a replicate is an exact copy of a sample that is being analyzed, such as a cell, organism or molecule, on which exactly the same procedure is done. This is often done in order to check for experimental or procedural error. In the absence of error replicates should yield the same result. However, replicates are not independent tests of the hypothesis because they are still the same sample, and so do not test for variation between samples.\n\nReplicates are often created to test the quality and repeatability of a procedure, or for a destructive procedure where preserving the original sample is desirable. They are also sometimes inappropriately used to inflate the apparent number of observations in a sample, creating an illusion of statistical significance.\n\n", "id": "40861730", "title": "Replicate (biology)"}
{"url": "https://en.wikipedia.org/wiki?curid=17018118", "text": "Basic rest-activity cycle\n\nThe basic rest-activity cycle (BRAC) is a physiological arousal mechanism in humans proposed by Nathaniel Kleitman, hypothesized to occur during both sleep and wakefulness. \n\nEmpirically, it is a ultradian rhythm of approximately 90 minutes (80–120 minutes) that is characterized by different level of excitement and rest. The cycle is mediated by the human biological clock. It is most readily observed in stages of sleep, for example, rapid eye movement sleep (REM) and the delta activity cycle.\n\nWhen awake, our brainwaves are faster during the first half of the cycle, when we feel alert and focused, and then our brainwaves slow; in the last 20 minutes when we feel dreamy and perhaps a little tired, while our body is being readied for the alert part of the following cycle. \n\nWhen asleep, our brainwaves first slow and then rise, and the BRAC occurs as stages of sleep—first falling into deep sleep, then rising into the REM stage, when dreams occur.\n", "id": "17018118", "title": "Basic rest-activity cycle"}
{"url": "https://en.wikipedia.org/wiki?curid=53959022", "text": "Cyclomorphosis\n\nCyclomorphosis (also known as seasonal polyphenism) is the name given to the occurrence of cyclic or seasonal changes in the phenotype of an organism through successive generations.\n\nIt occurs in small aquatic invertebrates that reproduce by parthenogenesis and give rise to several generations annually. It occurs especially in marine planktonic animals, and is thought to be caused by the epigenetic effect of environmental cues on the organism, thereby altering the course of their development.\n", "id": "53959022", "title": "Cyclomorphosis"}
{"url": "https://en.wikipedia.org/wiki?curid=344974", "text": "Chemosynthesis\n\nIn biochemistry, chemosynthesis is the biological conversion of one or more carbon-containing molecules (usually carbon dioxide or methane) and nutrients into organic matter using the oxidation of inorganic compounds (e.g., hydrogen gas, hydrogen sulfide) or methane as a source of energy, rather than sunlight, as in photosynthesis. Chemoautotrophs, organisms that obtain carbon through chemosynthesis, are phylogenetically diverse, but also groups that include conspicuous or biogeochemically-important taxa include the sulfur-oxidizing gamma and epsilon proteobacteria, the aquificaeles, the methanogenic archaea and the neutrophilic iron-oxidizing bacteria.\n\nMany microorganisms in dark regions of the oceans use chemosynthesis to produce biomass from single carbon molecules. Two categories can be distinguished. In the rare sites at which hydrogen molecules (H) are available, the energy available from the reaction between CO and H (leading to production of methane, CH) can be large enough to drive the production of biomass. Alternatively, in most oceanic environments, energy for chemosynthesis derives from reactions in which substances such as hydrogen sulfide or ammonia are oxidized. This may occur with or without the presence of oxygen.\n\nMany chemosynthetic microorganisms are consumed by other organisms in the ocean, and symbiotic associations between chemosynthesizers and respiring heterotrophs are quite common. Large populations of animals can be supported by chemosynthetic secondary production at hydrothermal vents, methane clathrates, cold seeps, whale falls, and isolated cave water.\n\nIt has been hypothesized that chemosynthesis may support life below the surface of Mars, Jupiter's moon Europa, and other planets. Chemosynthesis may have also been the first type of metabolism that evolved on Earth, leading the way for cellular respiration and photosynthesis to develop later.\n\nGiant tube worms use bacteria in their trophosome to fix carbon dioxide (using hydrogen sulfide as an energy source) and produce sugars and amino acids.\nSome reactions produce sulfur:\nInstead of releasing oxygen gas as in photosynthesis, the process produces solid globules of sulfur. In bacteria capable of chemosynthesis, such as purple sulfur bacteria, yellow globules of sulfur are present and visible in the cytoplasm.\n\nIn 1890, Sergei Nikolaevich Vinogradskii (or Winogradsky) proposed a novel type of life process called \"anorgoxydant\". His discovery suggested that some microbes could live solely on inorganic matter and emerged during his physiological research in the 1880s in Strassburg and Zurich on sulfur, iron, and nitrogen bacteria.\n\nIn 1897, Wilhelm Pfeffer coined the term \"chemosynthesis\" for the energy production by oxidation of inorganic substances, in association with autotrophic carbon dioxide assimilation - what would be named today as chemolithoautotrophy. Later, the term would be expanded to include also chemoorganoautotrophs, which are organisms that use organic energy substrates in order to assimilate carbon dioxide. Thus, chemosynthesis can be seen as a synonym of chemoautotrophy.\n\nThe term \"chemotrophy\", less restrictive, would be introduced in the 1940s by André Lwoff for the production of energy by the oxidation of electron donors, organic or not, associated with auto- or heterotrophy.\n\nThe suggestion of Vinogradskii was confirmed nearly 90 years later, when hydrothermal ocean vents were predicted to exist in the 1970s. The hot springs and strange creatures were discovered by \"Alvin\", the world's first deep-sea submersible, in 1977 at the Galapagos Rift. At about the same time, Harvard graduate student Colleen Cavanaugh proposed chemosynthetic bacteria that oxidize sulfides or elemental sulfur as a mechanism by which tube worms could survive near hydrothermal vents. Cavanaugh later managed to confirm that this was indeed the method by which the worms could thrive, and is generally credited with the discovery of chemosynthesis.\n\nA 2004 television series hosted by Bill Nye named chemosynthesis as one of the 100 greatest scientific discoveries of all time.\n\nIn 2013, researchers reported their discovery of bacteria living in the rock of the oceanic crust below the thick layers of sediment, and apart from the hydrothermal vents that form along the edges of the tectonic plates. Preliminary findings are that these bacteria subsist on the hydrogen produced by chemical reduction of olivine by seawater circulating in the small veins that permeate the basalt that comprises oceanic crust. The bacteria synthesize methane by combining hydrogen and carbon dioxide.\n\n", "id": "344974", "title": "Chemosynthesis"}
{"url": "https://en.wikipedia.org/wiki?curid=23017175", "text": "Eidonomy\n\nEidonomy is the study of the external appearance of an organism. It is thus the opposite of anatomy, which refers to internal morphology.\nWhile predominant early in the history of biology it is little studied in particular anymore as it is ripe with the effects of convergent evolution. It thus yields less new information about organisms than anatomy, and therefore the external appearance of lifeforms is usually studied as part of general investigations in morphology, e.g. in the context of phylogenetic research.\n\n\n", "id": "23017175", "title": "Eidonomy"}
{"url": "https://en.wikipedia.org/wiki?curid=2355918", "text": "Ethnobiology\n\n]\n\nEthnobiology is the scientific study of the way living things are treated or used by different human cultures. It studies the dynamic relationships between people, biota, and environments, from the distant past to the immediate present.\n\n\"People-biota-environment\" interactions around the world are documented and studied through time, across cultures, and across disciplines in a search for valid, reliable answers to two 'defining' questions: \"How and in what ways do human societies use nature, and how and in what ways do human societies view nature?\"\n\nNaturalists have been interested in local biological knowledge since the time Europeans started colonising the world, from the 15th century onwards. Paul Sillitoe wrote that:\n\nLocal biological knowledge, collected and sampled over these early centuries significantly informed the early development of modern biology:\n\nEthnobiology itself, as a distinctive practice, only emerged during the 20th century as part of the records then being made about other peoples, and other cultures. As a practice, it was nearly always ancillary to other pursuits when documenting others' languages, folklore, and natural resource use. Roy Ellen commented that:\n\nThis 'first phase' in the development of ethnobiology as a practice has been described as still having an essentially utilitarian purpose, often focusing on identifying those 'native' plants, animals and technologies of some potential use and value within increasingly dominant western economic systems\n\nArising out of practices in Phase I (above) came a 'second phase' in the development of 'ethnobiology', with researchers now striving to better document and better understand how other peoples' themselves \"conceptualise and categorise\" the natural world around them. In Sillitoe's words:\n\nThis 'second' phase is marked:\n\nBy the turn of the 21st century ethnobiological practices, research, and findings have had a significant impact and influence across a number of fields of biological inquiry including ecology, conservation biology, development studies, and political ecology.\n\nThe Society of Ethnobiology advises on its web page:\nEthnobiology is a rapidly growing field of research, gaining professional, student, and public interest .. internationally\nEthnobiology has come out from its place as an ancillary practice in the shadows of other core pursuits, to arise as a whole field of inquiry and research in its own right: taught within many tertiary institutions and educational programmes around the world; with its own methods manuals, its own readers, and its own textbooks\n\nAll societies make use of the biological world in which they are situated, but there are wide differences in use, informed by perceived need, available technology, and the culture's sense of morality and sustainability. Ethnobiologists investigate what lifeforms are used for what purposes, the particular techniques of use, the reasons for these choices, and symbolic and spiritual implications of them.\n\nDifferent societies divide the living world up in different ways. Ethnobiologists attempt to record the words used in particular cultures for living things, from the most specific terms (analogous to species names in Linnean biology) to more general terms (such as 'tree' and even more generally 'plant'). They also try to understand the overall structure or hierarchy of the classification system (if there is one; there is ongoing debate as to whether there must always be an implied hierarchy.\n\nSocieties invest themselves and their world with meaning partly through their answers to questions like \"how did the world happen?\", \"how and why did people come to be?\", \"what are proper practices, and why?\", and \"what realities exist beyond or behind our physical experience?\" Understanding these elements of a societies' perspective is important to cultural research in general, and ethnobiologists investigate how a societies' view of the natural world informs and is informed by them.\n\nIn order to live effectively in a given place, a people needs to understand the particulars of their environment, and many traditional societies have complex and subtle understandings of the places in which they live. Ethnobiologists seek to share in these understandings, subject to ethical concerns regarding intellectual property and cultural appropriation.\n\nIn cross cultural ethnobiology research, two or more communities participate simultaneously. This enables the researcher to compare how a bio-resource is used by different communities.\n\nStudies and writings within ethnobiology draw upon research from fields including archaeology, geography, linguistics, systematics, population biology, ecology, cultural anthropology, ethnography, pharmacology, nutrition, conservation, and sustainable development.\n\nThrough much of the history of ethnobiology, its practitioners were primarily from dominant cultures, and the benefit of their work often accrued to the dominant culture, with little control or benefit invested in the indigenous peoples whose practice and knowledge they recorded.\n\nJust as many of those indigenous societies work to assert legitimate control over physical resources such as traditional lands or artistic and ritual objects, many work to assert legitimate control over their intellectual property.\n\nIn an age when the potential exists for large profits from the discovery of, for example, new food crops or medicinal plants, modern ethnobiologists must consider intellectual property rights, the need for informed consent, the potential for harm to informants, and their \"debt to the societies in which they work\".\n\nFurthermore, these questions must be considered not only in light of western industrialized nations' common understanding of ethics and law, but also in light of the ethical and legal standards of the societies from which the ethnobiologist draws information.\n\nParticipatory ethnobotanical lab and field research station to encourage the involvement of local youth in detailed nutrition methodological propagation and restoration of local threatened species. Will provide a unique opportunity to further its goal of protecting our natural heritage.\n\n", "id": "2355918", "title": "Ethnobiology"}
{"url": "https://en.wikipedia.org/wiki?curid=2356015", "text": "Folk biology\n\nFolk biology or folkbiology is the cognitive study of how people classify and reason about the organic world. Humans everywhere classify animals and plants into obvious species-like groups. The relationship between a folk taxonomy and a scientific classification can assist in understanding how evolutionary theory deals with the apparent constancy of \"common species\" and the organic processes centering on them. From the vantage of evolutionary psychology, such natural systems are arguably routine \"habits of mind\", a sort of heuristic used to make sense of the natural world.\n\n", "id": "2356015", "title": "Folk biology"}
{"url": "https://en.wikipedia.org/wiki?curid=578784", "text": "Photobiology\n\nPhotobiology is the scientific study of the interactions of light (technically, non-ionizing radiation) and living organisms. The field includes the study of photophysics, photochemistry, photosynthesis, photomorphogenesis, visual processing, circadian rhythms, photomovement, bioluminescence, and ultraviolet radiation effects.\n\nThe division between ionizing radiation and nonionizing radiation is typically considered to be 10 eV, the energy required to ionize an oxygen atom.\n\n\n", "id": "578784", "title": "Photobiology"}
{"url": "https://en.wikipedia.org/wiki?curid=20324690", "text": "Archaeobiology\n\nArchaeobiology, the study of the biology of ancient times through archaeological materials, is a subspecialty of archaeology. It can be seen as a blanket term for paleobotany, animal osteology, zooarchaeology, microbiology, and many other sub-disciplines. Specifically, plant and animal remains are also called ecofacts. Sometimes these ecofacts can be left by humans and sometimes they can be naturally occurring. Archaeobiology tends to focus on more recent finds, so the difference between archaeobiology and palaeontology is mainly one of date: archaeobiologists typically work with more recent, non-fossilised material found at archaeological sites. Only very rarely are archaeobiological excavations performed at sites with no sign of human presence.\n\nThe prime interest of paleobotany is to reconstruct the vegetation that people in the past would have encountered in a particular place and time. Plant studies have always been overshadowed by faunal studies because bones are more conspicuous than plant remains when excavating. Collection of plant remains could everything including pollen, soil, diatoms, wood, plant remains and phytoliths. Phytoliths are sediments and diatoms are water deposits. Each plant remain can tell the archaeologist different things about the environment during a certain time period. Animal remains were the first evidence used by 19th century archaeologists. Today, archaeologists use faunal remains as a guide to the environment. It helps archaeologists understand whether the fauna were present naturally or through activities of carnivores or people. Archaeologists deal with macrofauna and microfauna. Microfauna are better indicators of climate and environmental change than larger species. These can be as small as a bug or as big as a fish or bird. Macrofauna helps archaeologists build a picture of past human diet.\n\nBacteria and Protists form two separate kingdoms, but both are fairly similar when compared. Bacteria are prokaryotic microorganisms, while protists are a group of eukaryotic organisms. Because both are microorganisms, both fall under the study of microbiology and special techniques are required for archaeologists to even see them.\nArchaeologists, in order to find these microorganisms in a site, have to first take samples from the site and bring them in for lab analysis. Once in the lab, they can use equipment such as optical microscopes, in order to actually see evidence of micro organic remains. Archaeologists that look at these microorganisms do not actually find the living bacteria or protist, but instead find indentations left behind in material from where they had been. Depending on where the indentations were in the strata, archaeologists can determine the age of the microorganisms.\n\nPaleomycology is the study of fungi in the fossil record. The study of past fungi can lead to the evolutionary past. Much of fungi are made up of parasites of animals, plants or insects. Most of the contemporary fungi resemble its ancestors, dating back over a million years ago. For example, “In the Dominican amber, a mosquito was found with several types of parasitic fungi growing on its outside cuticle. What is interesting is that the fungi resemble modern day fungi in class Trichomycetes, which are common gut-inhabiting zygomycetes of insects, but they differ from Trichomycetes in that the fungi are on the outside of the insect rather than the inside.” The study of ancient fungi can be used to track the evolution of fungi through millions of years.\n\nThe study of osteology is a study of bones and can be a subdiscipline in archeology. Osteologists in archeology reconstruct bones of humans or animals from the past to find more about the past civilizations. . Osteology is used in archaeology to determine the age, gender, and ethnicity of the remains. It is also helpful to rebuild past societies’ cultural background. Using the remains from the past can help modern archaeologist uncover the past from what they ate to their daily activities. This can help uncover the mysteries of past histories.\n\n", "id": "20324690", "title": "Archaeobiology"}
{"url": "https://en.wikipedia.org/wiki?curid=2146241", "text": "Hydrobiology\n\nHydrobiology is the science of life and life processes in water. Much of modern hydrobiology can be viewed as a sub-discipline of ecology but the sphere of hydrobiology includes taxonomy, economic biology, industrial biology, morphology, physiology etc. The one distinguishing aspect is that all relate to aquatic organisms. Much work is closely related to limnology and can be divided into lotic system ecology (flowing waters) and lentic system ecology (still waters).\n\nOne of the significant areas of current research is eutrophication. Special attention is paid to biotic interactions in plankton assemblage including the microbial loop, the mechanism of influencing water blooms, phosphorus load and lake turnover. Another subject of research is the acidification of mountain lakes. Long-term studies are carried out on changes in the ionic composition of the water of rivers, lakes and reservoirs in connection with acid rain and fertilisation. One goal of current research is elucidation of the basic environmental functions of the ecosystem in reservoirs, which are important for water quality management and water supply.\n\nMuch of the early work of hydrobiologists concentrated on the biological processes utilised in sewage treatment and water purification especially slow sand filters. Other historically important work sought to provide biotic indices for classifying waters according to the biotic communities that they supported. This work continues to this day in Europe in the development of classification tools for assessing water bodies for the EU water framework directive.\n\nThe following are the research interests of hydrobiologists:\n\n\n\n", "id": "2146241", "title": "Hydrobiology"}
{"url": "https://en.wikipedia.org/wiki?curid=536188", "text": "Phenology\n\nPhenology is the study of periodic plant and animal life cycle events and how these are influenced by seasonal and interannual variations in climate, as well as habitat factors (such as elevation). The word, coined by the Belgian botanist Charles Morren around 1849, is derived from the Greek φαίνω (\"phainō\"), \"to show, to bring to light, make to appear\" + λόγος (\"logos\"), amongst others \"study, discourse, reasoning\" and indicates that phenology has been principally concerned with the dates of first occurrence of biological events in their annual cycle. Examples include the date of emergence of leaves and flowers, the first flight of butterflies and the first appearance of migratory birds, the date of leaf colouring and fall in deciduous trees, the dates of egg-laying of birds and amphibia, or the timing of the developmental cycles of temperate-zone honey bee colonies. In the scientific literature on ecology, the term is used more generally to indicate the time frame for any seasonal biological phenomena, including the dates of last appearance (e.g., the seasonal phenology of a species may be from April through September).\n\nBecause many such phenomena are very sensitive to small variations in climate, especially to temperature, phenological records can be a useful proxy for temperature in historical climatology, especially in the study of climate change and global warming. For example, viticultural records of grape harvests in Europe have been used to reconstruct a record of summer growing season temperatures going back more than 500 years.\nIn addition to providing a longer historical baseline than instrumental measurements, phenological observations provide high temporal resolution of ongoing changes related to global warming.\n\nObservations of phenological events have provided indications of the progress of the natural calendar since ancient agricultural times. Many cultures have traditional phenological proverbs and sayings which indicate a time for action: \"When the sloe tree is white as a sheet, sow your barley whether it be dry or wet\" or attempt to forecast future climate: \"If oak's before ash, you're in for a splash. If ash before oak, you're in for a soak\". But the indications can be pretty unreliable, as an alternative version of the rhyme shows: \"If the oak is out before the ash, 'Twill be a summer of wet and splash; If the ash is out before the oak,'Twill be a summer of fire and smoke.\" Theoretically, though, these are not mutually exclusive, as one forecasts immediate conditions and one forecasts future conditions.\n\nThe North American Bird Phenology Program at USGS Patuxent Wildlife Research Center (PWRC) is in possession of a collection of millions of bird arrival and departure date records for over 870 species across North America, dating between 1880 and 1970. This program, originally started by Wells W. Cooke, involved over 3,000 observers including many notable naturalists of the time. The program ran for 90 years and came to a close in 1970 when other programs starting up at PWRC took precedence. The program was again started in 2009 to digitize the collection of records and now with the help of citizens worldwide, each record is being transcribed into a database which will be publicly accessible for use.\n\nThe English naturalists Gilbert White and William Markwick reported the seasonal events of more than 400 plant and animal species, Gilbert White in Selborne, Hampshire and William Markwick in Battle, Sussex over a 25-year period between 1768 and 1793. The data, reported in White's \"Natural History and Antiquities of Selborne\" are reported as the earliest and latest dates for each event over 25 years; so annual changes cannot therefore be determined.\n\nIn Japan and China the time of blossoming of cherry and peach trees is associated with ancient festivals and some of these dates can be traced back to the eighth century. Such historical records may, in principle, be capable of providing estimates of climate at dates before instrumental records became available. For example, records of the harvest dates of the pinot noir grape in Burgundy have been used in an attempt to reconstruct spring–summer temperatures from 1370 to 2003; the reconstructed values during 1787–2000 have a correlation with Paris instrumental data of about 0.75.\n\nRobert Marsham is the founding father of modern phenological recording. Marsham was a wealthy landowner who kept systematic records of \"Indications of spring\" on his estate at Stratton Strawless, Norfolk, from 1736. These were in the form of dates of the first occurrence of events such as flowering, bud burst, emergence or flight of an insect. Consistent records of the same events or \"phenophases\" were maintained by generations of the same family over unprecedentedly long periods of time, eventually ending with the death of Mary Marsham in 1958, so that trends can be observed and related to long-term climate records. The data show significant variation in dates which broadly correspond with warm and cold years. Between 1850 and 1950 a long-term trend of gradual climate warming is observable, and during this same period the Marsham record of oak leafing dates tended to become earlier.\n\nAfter 1960 the rate of warming accelerated, and this is mirrored by increasing earliness of oak leafing, recorded in the data collected by Jean Combes in Surrey. Over the past 250 years, the first leafing date of oak appears to have advanced by about 8 days, corresponding to overall warming on the order of 1.5 °C in the same period.\n\nTowards the end of the 19th century the recording of the appearance and development of plants and animals became a national pastime, and between 1891 and 1948 a programme of phenological recording was organised across the British Isles by the Royal Meteorological Society (RMS). Up to 600 observers submitted returns in some years, with numbers averaging a few hundred. During this period 11 main plant phenophases were consistently recorded over the 58 years from 1891–1948, and a further 14 phenophases were recorded for the 20 years between 1929 and 1948. The returns were summarised each year in the Quarterly Journal of the RMS as \"The Phenological Reports\". The 58-year data have been summarised by Jeffree (1960), and show that flowering dates could be as many as 21 days early and as many as 34 days late, with extreme earliness greatest in summer flowering species, and extreme lateness in spring flowering species. In all 25 species, the timings of all phenological events are significantly related to temperature, indicating that phenological events are likely to get earlier as climate warms.\n\n\"The Phenological Reports\" ended suddenly in 1948 after 58 years, and Britain was without a national recording scheme for almost 50 years, just at a time when climate change was becoming evident. During this period, important contributions were made by individual dedicated observers. The naturalist and author Richard Fitter recorded the First Flowering Date (FFD) of 557 species of British flowering plants in Oxfordshire between about 1954 and 1990. Writing in Science in 2002, Richard Fitter and his son Alistair Fitter found that \"the average FFD of 385 British plant species has advanced by 4.5 days during the past decade compared with the previous four decades.\" They note that FFD is sensitive to temperature, as is generally agreed, that \"150 to 200 species may be flowering on average 15 days earlier in Britain now than in the very recent past\" and that these earlier FFDs will have \"profound ecosystem and evolutionary consequences\".\n\nIn the last decade, national recording in Britain has been resumed by the UK Phenology network , run by Woodland Trust and the Centre for Ecology and Hydrology and the BBC Springwatch survey. There is a USA National Phenology Network in which both professional scientists and lay recorders participate, a European Phenology Network that has monitoring, research and educational remits and many other countries such as Canada (Alberta Plantwatch and Saskatchewan PlantWatch), China and Australia have phenological programs.\n\nIn eastern North America, almanacs are traditionally used for information on action phenology (in agriculture), taking into account the astronomical positions at the time.\nWilliam Felker has studied phenology in Ohio, US, since 1973 and now publishes \"Poor Will's Almanack\", a phenological almanac for farmers (not to be confused with a late 18th-century almanac by the same name).\n\nRecent technological advances in studying the earth from space have resulted in a new field of phenological research that is concerned with observing the phenology of whole ecosystems and stands of vegetation on a global scale using proxy approaches. These methods complement the traditional phenological methods which recorded the first occurrences of individual species and phenophases.\n\nThe most successful of these approaches is based on tracking the temporal change of a Vegetation Index (like Normalized Difference Vegetation Index(NDVI)). NDVI makes use of the vegetation's typical low reflection in the red (red energy is mostly absorbed by growing plants for Photosynthesis) and strong reflection in the Near Infrared (Infrared energy is mostly reflected by plants due to their cellular structure). Due to its robustness and simplicity, NDVI has become one of the most popular remote sensing based products. Typically, a vegetation index is constructed in such a way that the attenuated reflected sunlight energy (1% to 30% of incident sunlight) is amplified by ratio-ing red and NIR following this equation:\n\nThe evolution of the vegetation index through time, depicted by the graph above, exhibits a strong correlation with the typical green vegetation growth stages (emergence, vigor/growth, maturity, and harvest/senescence). These temporal curves are analyzed to extract useful parameters about the vegetation growing season (start of season, end of season, length of growing season, etc.). Other growing season parameters could potentially be extracted, and global maps of any of these growing season parameters could then be constructed and used in all sorts of climatic change studies.\n\nA noteworthy example of the use of remote sensing based phenology is the work of Ranga Myneni from Boston University. This work showed an apparent increase in vegetation productivity that most likely resulted from the increase in temperature and lengthening of the growing season in the boreal forest. Another example based on the MODIS enhanced vegetation index (EVI) reported by Alfredo Huete at the University of Arizona and colleagues showed that the Amazon Rainforest, as opposed to the long held view of a monotonous growing season or growth only during the wet rainy season, does in fact exhibit growth spurts during the dry season.\n\nHowever, these phenological parameters are only an approximation of the true biological growth stages. This is mainly due to the limitation of current space-based remote sensing, especially the spatial resolution, and the nature of vegetation index. A pixel in an image does not contain a pure target (like a tree, a shrub, etc.) but contains a mixture of whatever intersected the sensor's field of view.\n\n\n", "id": "536188", "title": "Phenology"}
{"url": "https://en.wikipedia.org/wiki?curid=11791198", "text": "Bioclimatology\n\nBioclimatology is the interdisciplinary field of science that studies the interactions between the biosphere and the Earth's atmosphere on time scales of the order of seasons or longer (by opposition to biometeorology).\n\nClimate processes largely control the distribution, size, shape and properties of living organisms on Earth. For instance, the general circulation of the atmosphere on a planetary scale broadly determines the location of large deserts or the regions subject to frequent precipitation, which, in turn, greatly determine which organisms can naturally survive in these environments. Furthermore, changes in climates, whether due to natural processes or to human interferences, may progressively modify these habitats and cause overpopulation or extinction of indigenous species.\n\nThe biosphere, for its part, and in particular continental vegetation, which constitutes over 99% of the total biomass, has played a critical role in establishing and maintaining the chemical composition of the Earth's atmosphere, especially during the early evolution of the planet (See History of Earth for more details on this topic). Currently, the terrestrial vegetation exchanges some 60 billion tons of carbon with the atmosphere on an annual basis (through processes of carbon fixation and carbon respiration), thereby playing a critical role in the carbon cycle. On a global and annual basis, small imbalances between these two major fluxes, as do occur through changes in land cover and land use, contribute to the current increase in atmospheric carbon dioxide.\n\n", "id": "11791198", "title": "Bioclimatology"}
{"url": "https://en.wikipedia.org/wiki?curid=3929318", "text": "Biocybernetics\n\nBiocybernetics is the application of cybernetics to biological science, composed of biological disciplines that benefit from the application of cybernetics including neurology and multicellular systems. Biocybernetics plays a major role in systems biology, seeking to integrate different levels of information to understand how biological systems function.\n\nBiocybernetics is an abstract science and is a fundamental part of theoretical biology, based upon the principles of systemics.\n\nBiocybernetics is a conjoined word from bio (Greek: βίο / life) and cybernetics (Greek: κυβερνητική / controlling-governing). Although the extended form of the word is biological cybernetics, the field is most commonly referred to as biocybernetics in scientific papers. Bioinformatics may also be properly referred to as bio informatics. \n\nEarly proponents of biocybernetics include Ross Ashby, Hans Drischel, and Norbert Wiener among others. Popular papers published by each scientist are listed below.\n\nRoss Ashby, \"Introduction to Cybernetics\", 1956\n\nNorbert Wiener, \"Cybernetics or Control and Communication in the Animal and the Machine\", 1948\n\nPapers and research that delve into topics involving biocybernetics may be found under a multitude of similar names, including molecular cybernetics, neurocybernetics, and cellular cybernetics. Such fields involve disciplines that specify certain aspects of the study of the living organism (for example, neurocybernetics focuses on the study neurological models in organisms). \n\n\n\n", "id": "3929318", "title": "Biocybernetics"}
{"url": "https://en.wikipedia.org/wiki?curid=1686272", "text": "Chemical biology\n\nChemical biology is a scientific discipline spanning the fields of chemistry and biology. The discipline involves the application of chemical techniques, analysis, and often small molecules produced through synthetic chemistry, to the study and manipulation of biological systems. In contrast to biochemistry, which involves to study of the chemistry of biomolecules and regulation of biochemical pathways within and between cells, chemical biology deals with chemistry \"applied to\" biology.\n\nSome forms of chemical biology attempt to answer biological questions by directly probing living systems at the chemical level. In contrast to research using biochemistry, genetics, or molecular biology, where mutagenesis can provide a new version of the organism, cell, or biomolecule of interest, chemical biology probes systems \"in vitro\" and \"in vivo\" with small molecules that have been designed for a specific purpose or identified on the basis of biochemical or cell-based screening (see chemical genetics).\n\nChemical biology is one of several interdisciplinary sciences that tend to differ from older, reductionist fields and whose goals are to achieve a description of scientific holism. Chemical biology has scientific, historical and philosophical roots in medicinal chemistry, supramolecular chemistry, bioorganic chemistry, pharmacology, genetics, biochemistry, and metabolic engineering. \n\nProteomics investigates the proteome, the set of expressed proteins at a given time under defined conditions. As a discipline, proteomics has moved past rapid protein identification and has developed into a biological assay for quantitative analysis of complex protein samples by comparing protein changes in differently perturbed systems. Current goals in proteomics include determining protein sequences, abundance and any post-translational modifications. Also of interest are protein–protein interactions, cellular distribution of proteins and understanding protein activity. Another important aspect of proteomics is the advancement of technology to achieve these goals.\n\nProtein levels, modifications, locations, and interactions are complex and dynamic properties. With this complexity in mind, experiments need to be carefully designed to answer specific questions especially in the face of the massive amounts of data that are generated by these analyses. The most valuable information comes from proteins that are expressed differently in a system being studied. These proteins can be compared relative to each other using quantitative proteomics, which allows a protein to be labeled with a mass tag. Proteomic technologies must be sensitive and robust, it is for these reasons, the mass spectrometer has been the workhorse of protein analysis. The high precision of mass spectrometry can distinguish between closely related species and species of interest can be isolated and fragmented within the instrument. Its applications to protein analysis was only possible in the late 1980s with the development of protein and peptide ionization with minimal fragmentation. These breakthroughs were ESI and MALDI. Mass spectrometry technologies are modular and can be chosen or optimized to the system of interest.\n\nChemical biologists are poised to impact proteomics through the development of techniques, probes and assays with synthetic chemistry for the characterization of protein samples of high complexity. These approaches include the development of enrichment strategies, chemical affinity tags and probes.\n\nSamples for Proteomics contain a myriad of peptide sequences, the sequence of interest may be highly represented or of low abundance. However, for successful MS analysis the peptide should be enriched within the sample. Reduction of sample complexity is achieved through selective enrichment using affinity chromatography techniques. This involves targeting a peptide with a distinguishing feature like a biotin label or a post translational modification. Interesting methods have been developed that include the use of antibodies, lectins to capture glycoproteins, immobilized metal ions to capture phosphorylated peptides and suicide enzyme substrates to capture specific enzymes. Here, chemical biologists can develop reagents to interact with substrates, specifically and tightly, to profile a targeted functional group on a proteome scale. Development of new enrichment strategies is needed in areas like non-ser/thr/tyr phosphorylation sites and other post translational modifications. Other methods of decomplexing samples relies on upstream chromatographic separations.\n\nChemical synthesis of affinity tags has been crucial to the maturation of quantitative proteomics. iTRAQ, Tandem mass tags (TMT) and Isotope-coded affinity tag (ICAT) are protein mass-tags that consist of a covalently attaching group, a mass (isobaric or isotopic) encoded linker and a handle for isolation. Varying mass-tags bind to different proteins as a sort of footprint such that when analyzing cells of differing perturbations, the levels of each protein can be compared relatively after enrichment by the introduced handle. Other methods include SILAC and heavy isotope labeling. These methods have been adapted to identify complexing proteins by labeling a bait protein, pulling it down and analyzing the proteins it has complexed. \nAnother method creates an internal tag by introducing novel amino acids that are genetically encoded in prokaryotic and eukaryotic organisms. These modifications create a new level of control and can facilitate photocrosslinking to probe protein–protein interactions. In addition, keto, acetylene, azide, thioester, boronate, and dehydroalanine- containing amino acids can be used to selectively introduce tags, and novel chemical functional groups into proteins.\n\nTo investigate enzymatic activity as opposed to total protein, activity-based reagents have been developed to label the enzymatically active form of proteins (see Activity-based proteomics). For example, serine hydrolase- and cysteine protease-inhibitors have been converted to suicide inhibitors. This strategy enhances the ability to selectively analyze low abundance constituents through direct targeting. Structures that mimic these inhibitors could be introduced with modifications that will aid proteomic analysis- like an identification handle or mass tag. Enzyme activity can also be monitored through converted substrate. This strategy relies on using synthetic substrate conjugates that contain moieties that are acted upon by specific enzymes. The product conjugates are then captured by an affinity reagent and analyzed. The measured concentration of product conjugate allow the determination of the enzyme velocity. Identification of enzyme substrates (of which there may be hundreds or thousands, many of which unknown) is a problem of significant difficulty in proteomics and is vital to the understanding of signal transduction pathways in cells; techniques for labelling cellular substrates of enzymes is an area chemical biologists can address. A method that has been developed uses \"analog-sensitive\" kinases to label substrates using an unnatural ATP analog, facilitating visualization and identification through a unique handle.\n\nWhile DNA, RNA and proteins are all encoded at the genetic level, there exists a separate system of trafficked molecules in the cell that are not encoded directly at any direct level: sugars. Thus, glycobiology is an area of dense research for chemical biologists. For instance, live cells can be supplied with synthetic variants of natural sugars in order to probe the function of the sugars in vivo. Carolyn Bertozzi, previously at University of California, Berkeley, has developed a method for site-specifically reacting molecules the surface of cells that have been labeled with synthetic sugars.\n\nChemical biologists used automated synthesis of many diverse compounds in order to experiment with effects of small molecules on biological processes. More specifically, they observe changes in the behaviors of proteins when small molecules bind to them. Such experiments may supposedly lead to discovery of small molecules with antibiotic or chemotherapeutic properties. These approaches are identical to those employed in the discipline of pharmacology.\n\nChemical biologists are also interested in developing new small-molecule and biomolecule-based tools to study biological processes, often by molecular imaging techniques. The field of molecular sensing was popularized by Roger Tsien's work developing calcium-sensing fluorescent compounds as well as pioneering the use of GFP, for which he was awarded the 2008 Nobel Prize in Chemistry. Today, researchers continue to utilize basic chemical principles to develop new compounds for the study of biological metabolites and processes.\n\nMany research programs are also focused on employing natural biomolecules to perform a task or act as support for a new chemical method or material. In this regard, researchers have shown that DNA can serve as a template for synthetic chemistry, self-assembling proteins can serve as a structural scaffold for new materials, and RNA can be evolved \"in vitro\" to produce new catalytic function.\n\nA common form of aggregation is long, ordered spindles called amyloid fibrils that are implicated in Alzheimer’s disease and that have been shown to consist of cross-linked beta sheet regions perpendicular to the backbone of the polypeptide. Another form of aggregation occurs with prion proteins, the glycoproteins found with Creutzfeldt–Jakob disease and bovine spongiform encephalopathy. In both structures, aggregation occurs through hydrophobic interactions and water must be excluded from the binding surface before aggregation can occur. A movie of this process can be seen in \"Chemical and Engineering News\". The diseases associated with misfolded proteins are life-threatening and extremely debilitating, which makes them an important target for chemical biology research.\n\nThrough the transcription and translation process, DNA encodes for specific sequences of amino acids. The resulting polypeptides fold into more complex secondary, tertiary, and quaternary structures to form proteins. Based on both the sequence and the structure, a particular protein is conferred its cellular function. However, sometimes the folding process fails due to mutations in the genetic code and thus the amino acid sequence or due to changes in the cell environment (e.g. pH, temperature, reduction potential, etc.). Misfolding occurs more often in aged individuals or in cells exposed to a high degree of oxidative stress, but a fraction of all proteins misfold at some point even in the healthiest of cells.\n\nNormally when a protein does not fold correctly, molecular chaperones in the cell can encourage refolding back into its active form. When refolding is not an option, the cell can also target the protein for degradation back into its component amino acids via proteolytic, lysosomal, or autophagic mechanisms. However, under certain conditions or with certain mutations, the cells can no longer cope with the misfolded protein(s) and a disease state results. Either the protein has a loss-of-function, such as in cystic fibrosis, in which it loses activity or cannot reach its target, or the protein has a gain-of-function, such as with Alzheimer's disease, in which the protein begins to aggregate causing it to become insoluble and non-functional.\n\nProtein misfolding has previously been studied using both computational approaches as well as \"in vivo\" biological assays in model organisms such as \"Drosophila melanogaster\" and \"C. elegans\". Computational models use a \"de novo\" process to calculate possible protein structures based on input parameters such as amino acid sequence, solvent effects, and mutations. This method has the shortcoming that the cell environment has been drastically simplified, which limits the factors that influence folding and stability. On the other hand, biological assays can be quite complicated to perform \"in vivo\" with high-throughput like efficiency and there always remains the question of how well lower organism systems approximate human systems.\nDobson et al. propose combining these two approaches such that computational models based on the organism studies can begin to predict what factors will lead to protein misfolding. Several experiments have already been performed based on this strategy. In experiments on \"Drosophila\", different mutations of beta amyloid peptides were evaluated based on the survival rates of the flies as well as their motile ability. The findings from the study show that the more a protein aggregates, the more detrimental the neurological dysfunction. Further studies using transthyretin, a component of cerebrospinal fluid that binds to beta amyloid peptide deterring aggregation but can itself aggregate especially when mutated, indicate that aggregation prone proteins may not aggregate where they are secreted and rather are deposited in specific organs or tissues based on each mutation. Kelly et al. have shown that the more stable, both kinetically and thermodynamically, a misfolded protein is the more likely the cell is to secrete it from the endoplasmic reticulum rather than targeting the protein for degradation. In addition, the more stress that a cell feels from misfolded proteins the more probable new proteins will misfold. These experiments as well as others having begun to elucidate both the intrinsic and extrinsic causes of misfolding as well as how the cell recognizes if proteins have folded correctly.\n\nAs more information is obtained on how the cell copes with misfolded proteins, new therapeutic strategies begin to emerge. An obvious path would be prevention of misfolding. However, if protein misfolding cannot be avoided, perhaps the cell's natural mechanisms for degradation can be bolstered to better deal with the proteins before they begin to aggregate. Before these ideas can be realized, many more experiments need to be done to understand the folding and degradation machinery as well as what factors lead to misfolding. More information about protein misfolding and how it relates to disease can be found in the recently published book by Dobson, Kelly, and Rameriz-Alvarado entitled Protein Misfolding Diseases Current and Emerging Principles and Therapies.\n\nIn contrast to the traditional biotechnological practice of obtaining peptides or proteins by isolation from cellular hosts through cellular protein production, advances in chemical techniques for the synthesis and ligation of peptides has allowed for the total synthesis of some peptides and proteins. Chemical synthesis of proteins is a valuable tool in chemical biology as it allows for the introduction of non-natural amino acids as well as residue specific incorporation of \"posttranslational modifications\" such as phosphorylation, glycosylation, acetylation, and even ubiquitination. These capabilities are valuable for chemical biologists as non-natural amino acids can be used to probe and alter the functionality of proteins, while post translational modifications are widely known to regulate the structure and activity of proteins. Although strictly biological techniques have been developed to achieve these ends, the chemical synthesis of peptides often has a lower technical and practical barrier to obtaining small amounts of the desired protein. Given the widely recognized importance of proteins as cellular catalysts and recognition elements, the ability to precisely control the composition and connectivity of polypeptides is a valued tool in the chemical biology community and is an area of active research.\nWhile chemists have been making peptides for over 100 years, the ability to efficiently and quickly synthesize short peptides came of age with the development of Bruce Merrifield's solid phase peptide synthesis (SPPS). Prior to the development of SPPS, the concept of step-by-step polymer synthesis on an insoluble support was without chemical precedent. The use of a covalently bound insoluble polymeric support greatly simplified the process of peptide synthesis by reducing purification to a simple \"filtration and wash\" procedure and facilitated a boom in the field of peptide chemistry. The development and \"optimization\" of SPPS took peptide synthesis from the hands of the specialized peptide synthesis community and put it into the hands of the broader chemistry, biochemistry, and now chemical biology community. SPPS is still the method of choice for linear synthesis of polypeptides up to 50 residues in length and has been implemented in commercially available automated peptide synthesizers. One inherent shortcoming in any procedure that calls for repeated coupling reactions is the buildup of side products resulting from incomplete couplings and side reactions. This places the upper bound for the synthesis of linear polypeptide lengths at around 50 amino acids, while the \"average\" protein consists of 250 amino acids. Clearly, there was a need for development of \"non-linear\" methods to allow synthetic access to the average protein.\n\nAlthough the shortcomings of linear SPPS were recognized not long after its inception, it took until the early 1990s for effective methodology to be developed to ligate small peptide fragments made by SPPS, into protein sized polypeptide chains (for recent review of peptide ligation strategies, see review by Dawson \"et al.\"). The oldest and best developed of these methods is termed native chemical ligation. Native chemical ligation was unveiled in a 1994 paper from the laboratory of Stephen B. H. Kent. Native chemical ligation involves the coupling of a C-terminal thioester and an N-terminal cysteine residue, ultimately resulting in formation of a \"native\" amide bond. Further refinements in native chemical ligation have allowed for kinetically controlled coupling of multiple peptide fragments, allowing access to moderately sized peptides such as an HIV-protease dimer and human lysozyme. Even with the successes and attractive features of native chemical ligation, there are still some drawbacks in the utilization of this technique. Some of these drawbacks include the installation and preservation of a reactive C-terminal thioester, the requirement of an N-terminal cysteine residue (which is the second-least-common amino acid in proteins), and the requirement for a sterically unincumbering C-terminal residue.\n\nOther strategies that have been used for the ligation of peptide fragments using the acyl transfer chemistry first introduced with native chemical ligation include expressed protein ligation, sulfurization/desulfurization techniques, and use of removable thiol auxiliaries.\n\nExpressed protein ligation allows for the biotechnological installation of a C-terminal thioester using intein biochemistry, thereby allowing the appendage of a synthetic N-terminal peptide to the recombinantly produced C-terminal portion. This technique allows for access to much larger proteins, as only the N-terminal portion of the resulting protein has to be chemically synthesized. Both sulfurization/desulfurization techniques and the use of removable thiol auxiliaries involve the installation of a synthetic thiol moiety to carry out the standard native chemical ligation chemistry, followed by removal of the auxiliary/thiol. These techniques help to overcome the requirement of an N-terminal cysteine needed for standard native chemical ligation, although the steric requirements for the C-terminal residue are still limiting.\nA final category of peptide ligation strategies include those methods not based on native chemical ligation type chemistry. Methods that fall in this category include the traceless Staudinger ligation, azide-alkyne dipolar cycloadditions, and imine ligations.\n\nMajor contributors in this field today include Stephen B. H. Kent, Philip E. Dawson, and Tom W. Muir, as well as many others involved in methodology development and applications of these strategies to biological problems.\n\nOne of the primary goals of protein engineering is the design of novel peptides or proteins with a desired structure and chemical activity. Because our knowledge of the relationship between primary sequence, structure, and function of proteins is limited, rational design of new proteins with enzymatic activity is extremely challenging. Directed evolution, repeated cycles of genetic diversification followed by a screening or selection process, can be used to mimic Darwinian evolution in the laboratory to design new proteins with a desired activity.\n\nSeveral methods exist for creating large libraries of sequence variants. Among the most widely used are subjecting DNA to UV radiation or chemical mutagens, error-prone PCR, degenerate codons, or recombination. Once a large library of variants is created, selection or screening techniques are used to find mutants with a desired attribute. Common selection/screening techniques include fluorescence-activated cell sorting (FACS), mRNA display, phage display, or \"in vitro\" compartmentalization. Once useful variants are found, their DNA sequence is amplified and subjected to further rounds of diversification and selection. Since only proteins with the desired activity are selected, multiple rounds of directed evolution lead to proteins with an accumulation beneficial traits.\n\nThere are two general strategies for choosing the starting sequence for a directed evolution experiment: \"de novo\" design and redesign. In a protein design experiment, an initial sequence is chosen at random and subjected to multiple rounds of directed evolution. For example, this has been employed successfully to create a family of ATP-binding proteins with a new folding pattern not found in nature. Random sequences can also be biased towards specific folds by specifying the characteristics (such as polar vs. nonpolar) but not the specific identity of each amino acid in a sequence. Among other things, this strategy has been used to successfully design four-helix bundle proteins. Because it is often thought that a well-defined structure is required for activity, biasing a designed protein towards adopting a specific folded structure is likely to increase the frequency of desirable variants in constructed libraries.\n\nIn a protein redesign experiment, an existing sequence serves as the starting point for directed evolution. In this way, old proteins can be redesigned for increased activity or new functions. Protein redesign has been used for protein simplification, creation of new quaternary structures, and topological redesign of a chorismate mutase. To develop enzymes with new activities, one can take advantage of promiscuous enzymes or enzymes with significant side reactions. In this regard, directed evolution has been used on γ-humulene synthase, an enzyme that creates over 50 different sesquiterpenes, to create enzymes that selectively synthesize individual products. Similarly, completely new functions can be selected for from existing protein scaffolds. In one example of this, an RNA ligase was created from a zinc finger scaffold after 17 rounds of directed evolution. This new enzyme catalyzes a chemical reaction not known to be catalyzed by any natural enzyme.\n\nComputational methods, when combined with experimental approaches, can significantly assist both the design and redesign of new proteins through directed evolution. Computation has been used to design proteins with unnatural folds, such as a right-handed coiled coil. These computational approaches could also be used to redesign proteins to selectively bind specific target molecules. By identifying lead sequences using computational methods, the occurrence of functional proteins in libraries can be dramatically increased before any directed evolution experiments in the laboratory.\n\nManfred T. Reetz, Frances Arnold, Donald Hilvert, and Jack W. Szostak are significant researchers in this field.\n\nRecent advances in technology have allowed scientists to view substructures of cells at levels of unprecedented detail. Unfortunately these \"aerial\" pictures offer little information about the mechanics of the biological system in question. To be fully effective, precise imaging systems require a complementary technique that better elucidates the machinery of a cell. By attaching tracking devices (optical probes) to biomolecules \"in vivo\", one can learn far more about cell metabolism, molecular transport, cell-cell interactions and many other processes\n\nSuccessful labeling of a molecule of interest requires specific functionalization of that molecule to react chemospecifically with an optical probe. For a labeling experiment to be considered robust, that functionalization must minimally perturb the system. Unfortunately, these requirements can often be extremely hard to meet. Many of the reactions normally available to organic chemists in the laboratory are unavailable in living systems. Water- and redox- sensitive reactions would not proceed, reagents prone to nucleophilic attack would offer no chemospecificity, and any reactions with large kinetic barriers would not find enough energy in the relatively low-heat environment of a living cell.\nThus, chemists have recently developed a panel of bioorthogonal chemistry that proceed chemospecifically, despite the milieu of distracting reactive materials \"in vivo\".\n\nThe coupling of an optical probe to a molecule of interest must occur within a reasonably short time frame; therefore, the kinetics of the coupling reaction should be highly favorable. Click chemistry is well suited to fill this niche, since click reactions are, by definition, rapid, spontaneous, selective, and high-yielding. Unfortunately, the most famous \"click reaction,\" a [3+2] cycloaddition between an azide and an acyclic alkyne, is copper-catalyzed, posing a serious problem for use \"in vivo\" due to copper's toxicity.\n\nThe issue of copper toxicity can be alleviated using copper-chelating ligands, enabling copper-catalyzed labeling of the surface of live cells.\nTo bypass the necessity for a catalyst, the lab of Dr. Carolyn Bertozzi introduced inherent strain into the alkyne species by using a cyclic alkyne. In particular, cyclooctyne reacts with azido-molecules with distinctive vigor. Further optimization of the reaction led to the use of difluorinated cyclooctynes (DIFOs), which increased yield and reaction rate. Other coupling partners discovered by separate labs to be analogous to cyclooctynes include trans cyclooctene, norbornene, and a cyclobutene-functionalized molecule.\n\nAs mentioned above, the use of bioorthogonal reactions to tag biomolecules requires that one half of the reactive \"click\" pair is installed in the target molecule, while the other is attached to an optical probe. When the probe is added to a biological system, it will selectively conjugate with the target molecule.\n\nThe most common method of installing bioorthogonal reactivity into a target biomolecule is through metabolic labeling. Cells are immersed in a medium where access to nutrients is limited to synthetically modified analogues of standard fuels such as sugars. As a consequence, these altered biomolecules are incorporated into the cells in the same manner as their wild-type brethren. The optical probe is then incorporated into the system to image the fate of the altered biomolecules. Other methods of functionalization include enzymatically inserting azides into proteins, and synthesizing phospholipids conjugated to cyclooctynes.\n\nAs these bioorthogonal reactions are further optimized, they will likely be used for increasingly complex interactions involving multiple different classes of biomolecules. More complex interactions have a smaller margin for error, so increased reaction efficiency is paramount to continued success in optically probing cellular machinery. Also, by minimizing side reactions, the experimental design of a minimally perturbed living system is closer to being realized.\n\nThe advances in modern sequencing technologies in the late 1990s allowed scientists to investigate DNA of communities of organisms in their natural environments, so-called \"eDNA\", without culturing individual species in the lab. This metagenomic approach enabled scientists to study a wide selection of organisms that were previously not characterized due in part to an incompetent growth condition. These sources of eDNA include, but are not limited to, soils, ocean, subsurface, hot springs, hydrothermal vents, polar ice caps, hypersaline habitats, and extreme pH environments. Of the many applications of metagenomics, chemical biologists and microbiologists such as Jo Handelsman, Jon Clardy, and Robert M. Goodman who are pioneers of metagenomics, explored metagenomic approaches toward the discovery of biologically active molecules such as antibiotics.\nFunctional or homology screening strategies have been used to identify genes that produce small bioactive molecules. Functional metagenomic studies are designed to search for specific phenotypes that are associated with molecules with specific characteristics. Homology metagenomic studies, on the other hand, are designed to examine genes to identify conserved sequences that are previously associated with the expression of biologically active molecules.\nFunctional metagenomic studies enable scientists to discover novel genes that encode biologically active molecules. These assays include top agar overlay assays where antibiotics generate zones of growth inhibition against test microbes, and pH assays that can screen for pH change due to newly synthesized molecules using pH indicator on an agar plate. Substrate-induced gene expression screening (SIGEX), a method to screen for the expression of genes that are induced by chemical compounds, has also been used to search genes with specific functions. These led to the discovery and isolation of several novel proteins and small molecules. For example, the Schipper group identified three eDNA derived AHL lactonases that inhibit biofilm formation of Pseudomonas aeruginosa via functional metagenomic assays. However, these functional screening methods require a good design of probes that detect molecules being synthesized and depend on the ability to express metagenomes in a host organism system.\nIn contrast, homology metagenomic studies led to a faster discovery of genes that have homologous sequences as the previously known genes that are responsible for the biosynthesis of biologically active molecules. As soon as the genes are sequenced, scientists can compare thousands of bacterial genomes simultaneously. The advantage over functional metagenomic assays is that homology metagenomic studies do not require a host organism system to express the metagenomes, thus this method can potentially save the time spent on analyzing nonfunctional genomes. These also led to the discovery of several novel proteins and small molecules. For example, Banik et al. screened for clones containing genes associated with the synthesis of teicoplanin and vancomycin-like glycopeptide antibiotics and found two new biosynthetic gene clusters. In addition, an \"in silico\" examination from the Global Ocean Metagenomic Survey found 20 new lantibiotic cyclases. \nThere are challenges to metagenomic approaches to discover new biologically active molecules. Only 40% of enzymatic activities present in a sample can be expressed in \"E. coli.\". In addition, the purification and isolation of eDNA is essential but difficult when the sources of obtained samples are poorly understood. However, collaborative efforts from individuals from diverse fields including bacterial genetics, molecular biology, genomics, bioinformatics, robots, synthetic biology, and chemistry can solve this problem together and potentially lead to the discovery of many important biologically active molecules.\n\nPosttranslational modification of proteins with phosphate groups has proven to be a key regulatory step throughout all biological systems. Phosphorylation events, either phosphorylation by protein kinases or dephosphorylation by phosphatases, result in protein activation or deactivation. These events have an immense impact on the regulation of physiological pathways, which makes the ability to dissect and study these pathways integral to understanding the details of cellular processes. There exist a number of challenges—namely the sheer size of the phosphoproteome, the fleeting nature of phosphorylation events and related physical limitations of classical biological and biochemical techniques—that have limited the advancement of knowledge in this area. A recent review provides a detailed examination of the impact of newly developed chemical approaches to dissecting and studying biological systems both in vitro and in vivo.\n\nThrough the use of a number of classes of small molecule modulators of protein kinases, chemical biologists have been able to gain a better understanding of the effects of protein phosphorylation. For example, nonselective and selective kinase inhibitors, such as a class of pyridinylimidazole compounds described by Wilson, et al., are potent inhibitors useful in the dissection of MAP kinase signaling pathways. These pyridinylimidazole compounds function by targeting the ATP binding pocket. Although this approach, as well as related approaches, with slight modifications, has proven effective in a number of cases, these compounds lack adequate specificity for more general applications. Another class of compounds, mechanism-based inhibitors, combines detailed knowledge of the chemical mechanism of kinase action with previously utilized inhibition motifs. For example, Parang, et al. describe the development of a \"bisubstrate analog\" that inhibits kinase action by binding both the conserved ATP binding pocket and a protein/peptide recognition site on the specific kinase. While there is no published in vivo data on compounds of this type, the structural data acquired from in vitro studies have expanded the current understanding of how a number of important kinases recognize target substrates. Interestingly, many research groups utilized ATP analogs as a chemical probe to study kinases and identify their substrates.\n\nThe development of novel chemical means of incorporating phosphomimetics into proteins has provided important insight into the effects of phosphorylation events. Historically, phosphorylation events have been studied by mutating an identified phosphorylation site (serine, threonine or tyrosine) to an amino acid, such as alanine, that cannot be phosphorylated. While this approach has been successful in some cases, mutations are permanent in vivo and can have potentially detrimental effects on protein folding and stability. Thus, chemical biologists have developed new ways of investigating protein phosphorylation. By installing phospho-serine, phospho-threonine or analogous phosphonate mimics into native proteins, researchers are able to perform in vivo studies to investigate the effects of phosphorylation by extending the amount of time a phosphorylation event occurs while minimizing the often-unfavorable effects of mutations. Protein semisynthesis, or more specifically expressed protein ligation (EPL), has proven to be successful techniques for synthetically producing proteins that contain phosphomimetic molecules at either the C- or the N-terminus. In addition, researchers have built upon an established technique in which one can insert an unnatural amino acid into a peptide sequence by charging synthetic tRNA that recognizes a nonsense codon with an unnatural amino acid. Recent developments indicate that this technique can also be employed in vivo, although, due to permeability issues, these in vivo experiments using phosphomimetic molecules have not yet been possible.\n\nAdvances in chemical biology have also improved upon classical techniques of imaging kinase action. For example, the development of peptide biosensors—peptides containing incorporated fluorophore molecules—allowed for improved temporal resolution in in vitro binding assays. Experimental limitations, however, prevent this technique from being effectively used in vivo. One of the most useful techniques to study kinase action is Fluorescence Resonance Energy Transfer (FRET). To utilize FRET for phosphorylation studies, fluorescent proteins are coupled to both a phosphoamino acid binding domain and a peptide that can by phosphorylated. Upon phosphorylation or dephosphorylation of a substrate peptide, a conformational change occurs that results in a change in fluorescence. FRET has also been used in tandem with Fluorescence Lifetime Imaging Microscopy (FLIM) or fluorescently conjugated antibodies and flow cytometry to provide a detailed, specific, quantitative results with excellent temporal and spatial resolution.\n\nThrough the augmentation of classical biochemical methods as well as the development of new tools and techniques, chemical biologists have improved accuracy and precision in the study of protein phosphorylation.\n\nAdvances in stem-cell biology have typically been driven by discoveries in molecular biology and genetics. These have included optimization of culture conditions for the maintenance and differentiation of pluripotent and multipotent stem-cells and the deciphering of signaling circuits that control stem-cell fate. However, chemical approaches to stem-cell biology have recently received increased attention due to the identification of several small molecules capable of modulating stem-cell fate in vitro. A small molecule approach offers particular advantages over traditional methods in that it allows a high degree of temporal control, since compounds can be added or removed at will, and tandem inhibition/activation of multiple cellular targets.\n\nSmall molecules that modulate stem-cell behavior are commonly identified in high-throughput screens. Libraries of compounds are screened for the induction of a desired phenotypic change in cultured stem-cells. This is usually observed through activation or repression of a fluorescent reporter or by detection of specific cell surface markers by FACS or immunohistochemistry. Hits are then structurally optimized for activity by the synthesis and screening of secondary libraries. The cellular targets of the small molecule can then be identified by affinity chromatography, mass spectrometry, or DNA microarray. \nA trademark of pluripotent stem-cells, such as embryonic stem-cells (ESCs), is the ability to self-renew indefinitely. The conventional use of feeder cells and various exogenous growth factors in the culture of ESCs presents a problem in that the resulting highly variable culture conditions make the long-term expansion of un-differentiated ESCs challenging. Ideally, chemically defined culture conditions could be developed to maintain ESCs in a pluripotent state indefinitely. Toward this goal, the Schultz and Ding labs at the Scripps Research Institute identified a small molecule that can preserve the long-term self-renewal of ESCs in the absence of feeder cells and other exogenous growth factors. This novel molecule, called pluripotin, was found to simultaneously inhibit multiple differentiation inducing pathways.\nThe utility of stem-cells is in their ability to differentiate into all cell types that make up an organism. Differentiation can be achieved in vitro by favoring development toward a particular cell type through the addition of lineage specific growth factors, but this process is typically non-specific and generates low yields of the desired phenotype. Alternatively, inducing differentiation by small molecules is advantageous in that it allows for the development of completely chemically defined conditions for the generation of one specific cell type. A small molecule, neuropathiazol, has been identified which can specifically direct differentiation of multipotent neural stem cells into neurons. Neuropathiazol is so potent that neurons develop even in conditions that normally favor the formation of glial cells, a powerful demonstration of controlling differentiation by chemical means.\nBecause of the ethical issues surrounding ESC research, the generation of pluripotent cells by reprogramming existing somatic cells into a more \"stem-like\" state is a promising alternative to the use of standard ESCs. By genetic approaches, this has recently been achieved in the creation of ESCs by somatic cell nuclear transfer and the generation of induced pluripotent stem-cells by viral transduction of specific genes. From a therapeutic perspective, reprogramming by chemical means would be safer than genetic methods because induced stem-cells would be free of potentially dangerous transgenes. Several examples of small molecules that can de-differentiate somatic cells have been identified. In one report, lineage-committed myoblasts were treated with a compound, named reversine, and observed to revert to a more stem-like phenotype. These cells were then shown to be capable of differentiating into osteoblasts and adipocytes under appropriate conditions.\nStem-cell therapies are currently the most promising treatment for many degenerative diseases. Chemical approaches to stem-cell biology support the development of cell-based therapies by enhancing stem-cell growth, maintenance, and differentiation in vitro. Small molecules that have been shown to modulate stem-cell fate are potential therapeutic candidates and provide a natural lean-in to pre-clinical drug development. Small molecule drugs could promote endogenous stem-cells to differentiate, replacing previously damaged tissues and thereby enhancing the body's own regenerative ability. Further investigation of molecules that modulate stem-cell behavior will only unveil new therapeutic targets.\n\nOrganisms are composed of cells that, in turn, are composed of macromolecules, e.g. proteins, ribosomes, etc. These macromolecules interact with each other, changing their concentration and suffering chemical modifications. The main goal of many biologists is to understand these interactions, using MRI, ESR, electrochemistry, and fluorescence among others. The advantages of fluorescence reside in its high sensitivity, non-invasiveness, safe detection, and ability to modulate the fluorescence signal. Fluorescence was observed mainly from small organic dyes attached to antibodies to the protein of interest. Later, fluorophores could directly recognize organelles, nucleic acids, and important ions in living cells. In the past decade, the discovery of green fluorescent protein (GFP), by Roger Y. Tsien, hybrid system and quantum dots have enable assessing protein location and function more precisely. Three main types of fluorophores are used: small organic dyes, green fluorescent proteins, and quantum dots. Small organic dyes usually are less than 1 kD, and have been modified to increase photostability, enhance brightness, and reduce self-quenching. Quantum dots have very sharp wavelength, high molar absorptivity and quantum yield. Both organic dyes and quantum dyes do not have the ability to recognize the protein of interest without the aid of antibodies, hence they must use immunolabeling. Since the size of the fluorophore-targeting complex typically exceeds 200 kD, it might interfere with multiprotein recognition in protein complexes, and other methods should be use in parallel. An advantage includes diversity of properties and a limitation is the ability of targeting in live cells. Green fluorescent proteins are genetically encoded and can be covalently fused to your protein of interest. A more developed genetic tagging technique is the tetracysteine biarsenical system, which requires modification of the targeted sequence that includes four cysteines, which binds membrane-permeable biarsenical molecules, the green and the red dyes \"FlAsH\" and \"ReAsH\", with picomolar affinity. Both fluorescent proteins and biarsenical tetracysteine can be expressed in live cells, but present major limitations in ectopic expression and might cause lose of function. Giepmans shows parallel applications of targeting methods and fluorophores using GFP and tetracysteine with ReAsH for α-tubulin and β-actin, respectively. After fixation, cells were immunolabeled for the Golgi matrix with QD and for the mitochondrial enzyme cytochrome with Cy5.\n\nFluorescent techniques have been used assess a number of protein dynamics including protein tracking, conformational changes, protein–protein interactions, protein synthesis and turnover, and enzyme activity, among others.\n\nThree general approaches for measuring protein net redistribution and diffusion are single-particle tracking, correlation spectroscopy and photomarking methods. In single-particle tracking, the individual molecule must be both bright and sparse enough to be tracked from one video to the other. Correlation spectroscopy analyzes the intensity fluctuations resulting from migration of fluorescent objects into and out of a small volume at the focus of a laser. In photomarking, a fluorescent protein can be dequenched in a subcellular area with the use of intense local illumination and the fate of the marked molecule can be imaged directly. Michalet and coworkers used quantum dots for single-particle tracking using biotin-quantum dots in HeLa cells.\n\nOne of the best ways to detect conformational changes in proteins is to sandwich said protein between two fluorophores. FRET will respond to internal conformational changes result from reorientation of the fluorophore with respect to the other. Dumbrepatil sandwiched an estrogen receptor between a CFP (cyan fluorescent protein) and a YFP (yellow fluorescent protein) to study conformational changes of the receptor upon binding of a ligand.\n\nFluorophores of different colors can be applied to detect their respective antigens within the cell. If antigens are located close enough to each other, they will appear colocalized and this phenomenon is known as colocalization. Specialized computer software, such as CoLocalizer Pro, can be used to confirm and characterize the degree of colocalization.\n\nFRET can detect dynamic protein–protein interaction in live cells providing the fluorophores get close enough. Galperin \"et al.\" used three fluorescent proteins to study multiprotein interactions in live cells.\n\nTetracysteine biarsenical systems can be used to study protein synthesis and turnover, which requires discrimination of old copies from new copies. In principle, a tetracysteine-tagged protein is labeled with FlAsH for a short time, leaving green labeled proteins. The protein synthesis is then carried out in the presence of ReAsH, labeling the new proteins as red.\n\nOne can also use fluorescence to see endogenous enzyme activity, typically by using a quenched activity based proteomics (qABP). Covalent binding of a qABP to the active site of the targeted enzyme will provide direct evidence concerning if the enzyme is responsible for the signal upon release of the quencher and regain of fluorescence.\n\nThe unique combination of high spatial and temporal resolution, nondestructive compatibility with living cells and organisms, and molecular specificity insure that fluorescence techniques will remain central in the analysis of protein networks and systems biology.\n\n\n\n", "id": "1686272", "title": "Chemical biology"}
{"url": "https://en.wikipedia.org/wiki?curid=216956", "text": "Glycobiology\n\nDefined in the narrowest sense, glycobiology is the study of the structure, biosynthesis, and biology of saccharides (sugar chains or glycans) that are widely distributed in nature. Sugars or saccharides are essential components of all living things and aspects of the various roles they play in biology are researched in various medical, biochemical and biotechnological fields.\n\nAccording to \"Oxford English Dictionary\" the specific term \"glycobiology\" was coined in 1988 by Prof. Raymond Dwek to recognize the coming together of the traditional disciplines of carbohydrate chemistry and biochemistry. This coming together was as a result of a much greater understanding of the cellular and molecular biology of glycans. However, as early as the late nineteenth century pioneering efforts were being made by Emil Fisher to establish the structure of some basic sugar molecules.\n\nSugars may be linked to other types of biological molecule to form glycoconjugates. The enzymatic process of glycosylation creates sugars/saccharides linked to themselves and to other molecules by the glycosidic bond, thereby producing glycans. Glycoproteins, proteoglycans and glycolipids are the most abundant glycoconjugates found in mammalian cells. They are found predominantly on the outer cell wall and in secreted fluids. Glycoconjugates have been shown to be important in cell-cell interactions due to the presence on the cell surface of various glycan binding receptors in addition to the glycoconjugates themselves. In addition to their function in protein folding and cellular attachment, the N-linked glycans of a protein can modulate the protein's function, in some cases acting as an on-off switch.\n\n\"Glycomics, analogous to genomics and proteomics, is the systematic study of all glycan structures of a given cell type or organism\" and is a subset of glycobiology.\n\nPart of the variability seen in saccharide structures is because monosaccharide units may be coupled to each other in many different ways, as opposed to the amino acids of proteins or the nucleotides in DNA, which are always coupled together in a standard fashion. The study of glycan structures is also complicated by the lack of a direct template for their biosynthesis, contrary to the case with proteins where their amino acid sequence is determined by their corresponding gene.\n\nGlycans are secondary gene products and therefore are generated by the coordinated action of many enzymes in the subcellular compartments of a cell. Since the structure of a glycan may depend on the expression, activity and accessibility of the different biosynthetic enzymes, it is not possible to use recombinant DNA technology in order to produce large quantities of glycans for structural and functional studies as it is for proteins.\n\nAdvanced analytical instruments and software programs, when used in combination, can unlock the mystery of glycan structures. Current techniques for structural annotation and analysis of glycans include liquid chromatography (LC), capillary electrophoresis (CE), mass spectrometry (MS), nuclear magnetic resonance (NMR) and lectin arrays.\n\nOne of the most widely used techniques is mass spectrometry which uses three principal units: the ionizer, analyzer and detector.\n\nGlycan arrays, like that offered by the Consortium for Functional Glycomics and Z Biotech LLC, contain carbohydrate compounds that can be screened with lectins or antibodies to define carbohydrate specificity and identify ligands.\n\nMRM is a mass spectrometry-based technique that has recently been used for site-specific glyosylation profiling. Although MRM has been used extensively in metabolomics and proteomics, its high sensitivity and linear response over a wide dynamic range make it especially suited for glycan biomarker research and discovery. MRM is performed on a triple quadrupole (QqQ) instrument, which is set to detect a predetermined precursor ion in the first quadrupole, a fragmented in the collision quadrupole, and a predetermined fragment ion in the third quadrupole. It is a non-scanning technique, wherein each transition is detected individually and the detection of multiple transitions occurs concurrently in duty cycles. This technique is being used to characterize the immune glycome.\n\nDrugs already on the market, such as heparin, erythropoietin and a few anti-flu drugs have proven effective and highlight the importance of glycans as a new class of drug. Additionally, the search for new anti-cancer drugs is opening up new possibilities in glycobiology. Anti-cancer drugs with new and varied action mechanisms together with anti-inflammatory and anti-infection drugs are today undergoing clinical trials. They may alleviate or complete current therapies. Although these glycans are molecules that are difficult to synthesize in a reproducible way, owing to their complex structure, this new field of research is highly encouraging for the future.\n\nGlycobiology, in which recent developments have been made possible by the latest technological advances, helps provide a more specific and precise understanding of skin aging.\nIt has now been clearly established that glycans are major constituents of the skin and play a decisive role in skin homeostasis.\nVital to the proper functioning of skin, glycans undergo both qualitative and quantitative changes in the course of aging. The functions of communication and metabolism are impaired and the skin's architecture is degraded.\n\n\nhttp://www.healthcanal.com/medical-breakthroughs/22037-UGA-scientists-team-define-first-ever-sequence-biologically-important-carbohydrate.html\n\n", "id": "216956", "title": "Glycobiology"}
{"url": "https://en.wikipedia.org/wiki?curid=21360243", "text": "Organography\n\nOrganography (from Greek , \"organo\", \"organ\"; and , \"-graphy\") is the scientific description of the structure and function of the organs of living things.\n\nOrganography as a scientific study starts with Aristotle, who considered the parts of plants as \"organs\" and began to consider the relationship between different organs and different functions. In the 17th century Joachim Jung, clearly articulated that plants are composed of different organ types such as root, stem and leaf, and he went on to define these organ types on the basis of form and position.\n\nIn the following century Caspar Friedrich Wolff was able to follow the development of organs from the \"growing points\" or apical meristems. He noted the commonality of development between foliage leaves and floral leaves (e.g. petals) and wrote: \"In the whole plant, whose parts we wonder at as being, at the first glance, so extraordinarily diverse, I finally perceive and recognize nothing beyond leaves and stem (for the root may be regarded as a stem). Consequently all parts of the plant, except the stem, are modified leaves.\"\n\nSimilar views were propounded at by Goethe in his well-known treatise. He wrote: \"The underlying relationship between the various external parts of the plant, such as the leaves, the calyx, the corolla, the stamens, which develop one after the other and, as it were, out of one another, has long been generally recognized by investigators, and has in fact been specially studied; and the operation by which one and the same organ presents itself to us in various forms has been termed Metamorphosis of Plants.\"\n\n\n", "id": "21360243", "title": "Organography"}
{"url": "https://en.wikipedia.org/wiki?curid=34939027", "text": "Pharmacometabolomics\n\nPharmacometabolomics, also known as pharmacometabonomics, is a field which stems from metabolomics, the quantification and analysis of metabolites produced by the body. It refers to the direct measurement of metabolites in an individual’s bodily fluids, in order to predict or evaluate the metabolism of pharmaceutical compounds, and to better understand the pharmacokinetic profile of a drug. Alternatively, pharmacometabolomics can be applied to measure metabolite levels following the administration of a pharmaceutical compound, in order to monitor the effects of the compound on certain metabolic pathways(pharmacodynamics). This provides detailed mapping of drug effects on metabolism and the pathways that are implicated in mechanism of variation of response to treatment. In addition, the metabolic profile of an individual at baseline (metabotype) provides information about how individuals respond to treatment and highlights heterogeneity within a disease state. All three approaches require the quantification of metabolites found in bodily fluids and tissue, such as blood or urine, and can be used in the assessment of pharmaceutical treatment options for numerous disease states.\n\nPharmacometabolomics is thought to provide information that complements that gained from other omics, namely genomics, transcriptomics, and proteomics. Looking at the characteristics of an individual down through these different levels of detail, there is an increasingly more accurate prediction of a person’s ability to respond to a pharmaceutical compound. The genome, made up of 25 000 genes, can indicate possible errors in drug metabolism; the transcriptome, made up of 85,000 transcripts, can provide information about which genes important in metabolism are being actively transcribed; and the proteome, >10,000,000 members, depicts which proteins are active in the body to carry out these functions. Pharmacometabolomics complements the omics with direct measurement of the products of all of these reactions, but with perhaps a relatively smaller number of members: that was initially projected to be approximately 2200 metabolites, but could be a larger number when gut derived metabolites and xenobiotics are added to the list. Overall, the goal of pharmacometabolomics is to more closely predict or assess the response of an individual to a pharmaceutical compound, permitting continued treatment with the right drug or dosage depending on the variations in their metabolism and ability to respond to treatment.\n\nPharmacometabolomic analyses, through the use of a metabolomics approach, can provide a comprehensive and detailed metabolic profile or “metabolic fingerprint” for an individual patient. Such metabolic profiles can provide a complete overview of individual metabolite or pathway alterations, providing a more realistic depiction of disease phenotypes. This approach can then be applied to the prediction of response to a pharmaceutical compound by patients with a particular metabolic profile. Pharmacometabolomic analyses of drug response are often coupled or followed up with pharmacogenetics studies. Pharmacogenetics focuses on the identification of genetic variations (e.g. single-nucleotide polymorphisms) within patients that may contribute to altered drug responses and overall outcome of a certain treatment. The results of pharmacometabolomics analyses can act to “inform” or “direct” pharmacogenetic analyses by correlating aberrant metabolite concentrations or metabolic pathways to potential alterations at the genetic level. This concept has been established with two seminal publications from studies of antidepressants serotonin reuptake inhibitors where metabolic signatures were able to define pathway implicated in response to the antidepressant and that lead to identification of genetic variants within a key gene within highlighted pathway as being implicated in variation in response. These genetic variants were not identified through genetic analysis alone and hence illustrated how metabolomics can guide and inform genetic data.\n\nAlthough the applications of pharmacometabolomics to personalized medicine are largely only being realized now, the study of an individual’s metabolism has been used to treat disease since the Middle Ages. Early physicians employed a primitive form of metabolomic analysis by smelling, tasting and looking at urine to diagnose disease. Obviously the measurement techniques needed to look at specific metabolites were unavailable at that time, but such technologies have evolved dramatically over the last decade to develop precise, high-throughput devices, as well as the accompanying data analysis software to analyze output. Currently, sample purification processes, such as liquid or gas chromatography, are coupled with either mass spectrometry (MS)-based or nuclear magnetic resonance (NMR)-based analytical methods to characterize the metabolite profiles of individual patients. Continually advancing informatics tools allow for the identification, quantification and classification of metabolites to determine which pathways may influence certain pharmaceutical interventions. One of the earliest studies discussing the principle and applications of pharmacometabolomics was conducted in an animal model to look at the metabolism of paracetamol and liver damage. NMR spectroscopy was used to analyze the urinary metabolic profiles of rats pre- and post-treatment with paracetamol. The analysis revealed a certain metabolic profile associated with increased liver damage following paracetamol treatment. At this point, it was eagerly anticipated that such pharmacometabolomics approaches could be applied to personalized human medicine. Since this publication in 2006, the Pharmacometabolomics Research Network led by Duke University researchers and that included partnerships between centers of excellence in metabolomics, pharmacogenomics and informatics (over sixteen academic centers funded by NIGMS) has been able to illustrate for the first time the power of the pharmacometabolomics approach in informing about treatment outcomes in large clinical studies and with use of drugs that include antidepressants, statins, antihypertensives, antiplatelet therapies and antipsychotics. Totally new concepts emerged from these studies on use of pharmacometabolomics as a tool that can bring a paradigm shift in the field of pharmacology. It illustrated how pharmacometabolomics can enable a Quantitative and Systems Pharmacology approach. \nPharmacometabolomics has been applied for the treatment of numerous human diseases, such as schizophrenia, diabetes, neural disease, depression and cancer.\n\nAs metabolite analyses are being conducted at the individual patient level, pharmacometabolomics may be considered a form of personalized medicine. This field is currently being employed in a predictive manner to determine the potential responses of therapeutic compounds in individual patients, allowing for more customized treatment regimens. It is anticipated that such pharmacometabolomics approaches will lead to the improved ability to predict an individual’s response to a compound, the efficacy and metabolism of it as well as adverse or off-target effects that may take place in the body. The metabolism of certain drugs varies from patient to patient as the copy number of the genes which code for common drug metabolizing enzymes varies within the population, and leads to differences in the ability of an individual to metabolize different compounds. Other important personal factors contributing to an individual’s metabolic profile, such as patient nutritional status, commensal bacteria, age, and pre-existing medical conditions, are also reflected in metabolite assessment., Overall, pharmacometabolomic analyses combined with such approaches as pharmacogenetics, can function to identify the metabolic processes and particular genetic alterations that may compromise the anticipated efficacy of a drug in a particular patient. The results of such analyses can then allow for the modification of treatment regimens to optimize treatment outcome.\n\nPharmacometabolomics may be used in a predictive manner to determine the correct course of action in regards to a patient about to undergo some type of drug treatment. This involves determining the metabolic profile of a patient prior to treatment, and correlating metabolic signatures with the outcome of a pharmaceutical treatment course. Analysis of a patient’s metabolic profile can reveal factors that may contribute to altered drug metabolism, allowing for predictions of the overall efficacy of a proposed treatment, as well as potential drug toxicity risks that may differ from the general population. This approach has been used to identify novel or previously characterized metabolic biomarkers in patients, which can be used to predict the expected outcome of that patient following treatment with a pharmaceutical compound. One example of the clinical application of pharmacometabolomics are studies that looked to identify a predictive metabolic marker for the treatment of major depressive disorder (MDD)., In a study with antidepressant Sertraline, the Pharmacometabolomics Network illustrated that metabolic profile at baseline of patients with major depression can inform about treatment outcomes. In addition the study illustrated the power of metabolomics for defining response to placebo and compared response to placebo to response to sertraline and showed that several pathways were common to both. In another study with escitalopram citalopram, metabolomic analysis of plasma from patients with MDD revealed that variations in glycine metabolism were negatively associated with patient outcome upon treatment with selective serotonin reuptake inhibitors (SSRIs), an important drug class involved in the treatment of this disease.\n\nThe second major application of pharmacometabolomics is the analysis of a patient’s metabolic profile following the administration of a specific therapy. This process is often secondary to a pre-treatment metabolic analysis, allowing for the comparison of pre- and post-treatment metabolite concentrations. This allows for the identification of the metabolic processes and pathways that are being altered by the treatment either intentionally as a designated target of the compound, or unintentionally as a side effect. Furthermore, the concentration and variety of metabolites produced from the compound itself can also be identified, providing information on the rate of metabolism and potentially leading to development of a related compound with increased efficacy or decreased side effects. An example of this approach was used to investigate the effect of several antipsychotic drugs on lipid metabolism in patients treated for schizophrenia. It was hypothesized that these antipsychotic drugs may be altering lipid metabolism in treated patients with schizophrenia, contributing to the weight gain and hypertriglyceridemia. The study monitored lipid metabolites in patients both before and after treatment with antipsychotics. The compiled pre- and post-treatment profiles were then be compared to examine the effect of these compounds on lipid metabolism. Interestingly, the researchers found correlations between treatment with antipsychotic drugs and lipid metabolism, in both a lipid-class-specific and drug-specific manner, establishing new foundations around the concept that pharmacometabolomics provides powerful tools for enabling detailed mapping of drug effects. Additional studies by the Pharmacometabolomics Research Network enabled mapping in ways not possible before effects of statins, atenolol and aspirin. Totally new insights were gained about effect of these drugs on metabolism and they highlighted pathways implicated in response and side effects.\n\nIn order to identify and quantify metabolites produced by the body, various detection methods have been employed. Most often, these involve the use of nuclear magnetic resonance (NMR) spectroscopy or mass spectrometry (MS), providing universal detection, identification and quantification of metabolites in individual patient samples. Although both processes are used in pharmacometabolomic analyses, there are advantages and disadvantages for using either nuclear magnetic resonance (NMR) spectroscopy- or mass spectrometry (MS)-based platforms in this application.\n\nNMR spectroscopy has been utilized for the analysis of biological samples since the 1980s, and can be used as an effective technique for the identification and quantification of both known and unknown metabolites. For details on the principles of this technique, see NMR spectroscopy. In pharmacometabolomics analyses, NMR is advantageous because minimal sample preparation is required. Isolated patient samples typically include blood or urine due to their minimally-invasive acquisition, however, other fluid types and solid tissue samples have also been studied with this approach. Due to the minimal preparation of samples before analysis, samples can be completely recovered following NMR analysis. This permits samples to be repeatedly processed with extremely high levels of reproducibility, as well as maintaining precious patient samples for alternative analysis. The high reproducibility and precision of NMR, coupled with relatively fast processing time (greater than 100 samples per day), makes this process a relatively high-throughput form of sample analysis. One disadvantage of this technique is the relatively poor metabolite detection sensitivity compared to MS-based analysis, leading to a requirement for greater initial sample volume. Furthermore, the initial instrument costs are extremely high, for both NMR and MS equipment.\n\nAn alternative approach to the identification and quantification of patient samples is through the use of mass spectrometry. This approach offers excellent precision and sensitivity in the identification, characterization and quantification of metabolites in multiple patient sample types, such as blood and urine. The mass spectrometry (MS) approach is typically coupled to gas chromatography (GC), in GC-MS or liquid chromatography (LC), in LC-MS, which aid in initially separating out the metabolite components within complex sample mixtures, and can allow for the isolation of particular metabolite subsets for analysis. GC-MS can provide relatively precise quantification of metabolites, as well as chemical structural information that can be compared to pre-existing chemical libraries. GC-MS can be conducted in a relatively high-throughput manner (greater than 100 samples per day) with greater detection sensitivity than NMR analysis. A limitation of GC-MS for this application, however, is that processed metabolite components must be readily volatized for sample processing.\n\nLC-MS initially separates out the components of a sample mixture based on properties such as hydrophobicity, before processing them for identification and quantification by mass spectrometry (MS). Overall, LC-MS is an extremely flexible method for processing most compound types in a somewhat high-throughput manner (20-100 samples a day), also with greater sensitivity than NMR analysis. For both GC-MS and LC-MS there are limitations in the reproducibility of metabolite quantification. Furthermore, sample processing for downstream mass spectrometry (MS) analysis is much more intensive than in NMR application, and results in the destruction of the original sample (via trypsin digestion).\n\nFollowing identification and quantification of metabolites in individual patient samples, NMR and mass spectrometry (MS) output is compiled into a dataset. These datasets include information on the identity and levels of individual metabolites detected within processed samples, as well as characteristics of each metabolite during the detection process (e.g. mass-to-charge ratios for mass spectrometry (MS)-based analysis). Multiple datasets can be created and compiled into large databases for individual patients in order to monitor varying metabolic profiles over a treatment course (i.e. pre- and post-treatment profiles). Each database is then processed through a type of informatics platform with software designed to characterize and analyze the data to generate an overall metabolic profile for the patient. To generate this overall profile, computational programs are designed to:\n\nAlong with the emerging diagnostic capabilities of pharmacometabolomics, there are limitations introduced when individual variability is looked at. The ability to determine an individual’s physiological state by measurement of metabolites is not contested, but the extreme variability that can be introduced by age, nutrition, and commensal organisms suggest problems in creating generalized pharmacometabolomes for patient groups. However, as long as meaningful metabolic signatures can be elucidated to create baseline values, there still exists a possible means of comparison.\n\nIssues surrounding the measurement of metabolites in an individual can also arise from the methodology of metabolite detection, and there are arguments both for and against NMR and mass spectrometry (MS). Other limitations surrounding metabolite analysis include the need for proper handling and processing of samples, as well as proper maintenance and calibration of the analytical and computational equipment. These tasks require skilled and experienced technicians, and potential instrument repair costs due to continuous sample processing can be costly. The cost of the processing and analytical platforms alone is very high, making it difficult for many facilities to afford pharmacometabolomics-based treatment analyses.\n\nPharmacometabolomics may decrease the burden on the healthcare system by better gauging the correct choice of treatment drug and dosage in order to optimize the response of a patient to a treatment. Hopefully, this approach will also ultimately limit the number of adverse drug reactions (ADRs) associated with many treatment regimens. Overall, physicians would be better able to apply more personalized, and potentially more effective, treatments to their patients. It is important to consider, however, that the processing and analysis of the patient samples takes time, resulting in delayed treatment. \nAnother concern about the application of pharmacometabolomics analyses to individual patient care, is deciding who should and who should not receive this in-depth, personalized treatment protocol. Certain diseases and stages of disease would have to be classified according to their requirement of such a treatment plan, but there are no criteria for this classification. Furthermore, not all hospitals and treatment institutes can afford the equipment to process and analyze patient samples on site, but sending out samples takes time and ultimately delays treatment. \nHealth insurance coverage of such procedures may also be an issue. Certain insurance companies may discriminate against the application of this type of sample analysis and metabolite characterization. Furthermore, there would have to be regulations put in place to ensure that there was no discrimination by insurance companies against the metabolic profiles of individual patients (“high metabolizers” vs. risky “low metabolizers”).\n\n\n", "id": "34939027", "title": "Pharmacometabolomics"}
{"url": "https://en.wikipedia.org/wiki?curid=4062934", "text": "Phenomics\n\nPhenomics is an area of biology concerned with the measurement of phenomes (a phenome is the set of physical and biochemical traits belonging to a given organism) as they change in response to genetic mutation and environmental influences. It is used in functional genomics, pharmaceutical research, metabolic engineering and increasingly in phylogenetics.\n\nAn important field of research today is trying to improve, both qualitatively and quantitatively, the capacity to measure phenomes. This include developing high-throughput measurement systems.\n\nFor example, the Australian Plant Phenomics Facility, an initiative of the Australian government, has developed a number of new instruments for comprehensive and fast measurements of phenotypes in both the lab and the field.\n\n\n", "id": "4062934", "title": "Phenomics"}
{"url": "https://en.wikipedia.org/wiki?curid=9247603", "text": "Protistology\n\nProtistology is a scientific discipline devoted to the study of protists, a highly diverse group of eukaryotic organisms. Its field of study overlaps with more traditional disciplines of phycology, mycology, and protozoology, just as protists, which, being a paraphyletic group embrace algae, some organisms regarded previously as primitive fungi, and protozoa (\"animal\" motile protists lacking chloroplasts).\n\nAs the term \"protozoology\" has become dated as our understanding of the evolutionary relationships of the eukaryotes has improved, the term \"protistology\" become more common. For example, the Society of Protozoloogists, founded in 1947, was renamed International Society of Protistologists in 2005. However, the older term persists in some cases (e.g., the Polish journal Acta Protozoologica).\n\nDedicated academic journals include:\n\n\nOther less specialized journals, important to protistology before the appearance of the more specialized:\n\nSome societies:\n\nThe field of protistology was idealized by Haeckel, but its widespread recognition is more recent. In fact, many of the researchers cited below considered themselves as protozoologists, phycologists, mycologists, microbiologists, microscopists, parasitologists, limnologists, biologists, naturalists, zoologists, botanists, etc., but made significant contributions to the field.\n\n", "id": "9247603", "title": "Protistology"}
{"url": "https://en.wikipedia.org/wiki?curid=9468040", "text": "Scotobiology\n\nThe term scotobiology describes the study of biology as directly and specifically affected by darkness, as opposed to photobiology, which describes the biological effects of light.\n\nThe science of scotobiology gathers together under a single descriptive heading a wide range of approaches to the study of the biology of darkness. This includes work on the effects of darkness on the behavior and metabolism of animals, plants, and microbes. Some of this work has been going on for over a century, and lays the foundation for understanding the importance of dark night skies, not only for humans but for all biological species.\n\nThe great majority of biological systems have evolved in a world of alternating day and night and have become irrevocably adapted to and dependent on the daily and seasonally changing patterns of light and darkness. Light is essential for many biological activities such as sight and photosynthesis. These are the focus of the science of photobiology. But the presence of uninterrupted periods of darkness, as well as their alternation with light, is just as important to biological behaviour. Scotobiology studies the positive responses of biological systems to the presence of darkness, and not merely the negative effects caused by the absence of light.\n\nMany of the biological and behavioural activities of plants, animals (including birds and amphibians), insects, and microorganisms are either adversely affected by light pollution at night or can only function effectively either during or as the consequence of nightly darkness. Such activities include foraging, breeding and social behavior in higher animals, amphibians, and insects, which are all affected in various ways if light pollution occurs in their environment. These are not merely photobiological phenomena; light pollution acts by interrupting critical dark-requiring processes. \nBut perhaps the most important scotobiological phenomena relate to the regular periodic alternation of light and darkness. These include breeding behavior in a range of animals, the control of flowering and the induction of winter dormancy in many plants, and the operational control of the human immune system. In many of these biological processes the critical point is the length of the dark period rather than that of the light. For example, \"short-day\" and \"long-day\" plants are, in fact, \"long-night\" and \"short-night\" respectively. That is to say, plants do not measure the length of the light period, but of the dark period. One consequence of artificial light pollution is that even brief periods of relatively bright light during the night may prevent plants or animals (including humans) from measuring the length of the dark period, and therefore from behaving in a normal or required manner. This is a critical aspect of scotobiology, and one of the major areas in the study of the responses of biological systems to darkness.\n\nIn discussing scotobiology, it is important to remember that darkness (the absence of light) is seldom absolute. An important aspect of any scotobiological phenomenon is the level and quality (wavelength) of light that is below the threshold of detection for that phenomenon and in any specific organism. This important variable in scotobiological studies is not always properly noted or examined. There are substantial levels of natural light pollution at night, of which moonlight is usually the strongest. For example, plants that rely on night length to program their behaviour have the capacity to ignore full moonlight during an otherwise dark night. If this ability had not evolved, plants would not be able to respond to changing night-length for such behavioural programs as the initiation of flowering and the onset of dormancy. On the other hand, some animal behavioural patterns are strongly responsive to moonlight. It is thus most important in any scotobiological study to determine the threshold level of light that may be required to interfere with or negate the normal pattern of dark-night activity.\n\nIn 2003, at a symposium on the Ecology of the Night held in Muskoka, Canada, discussion centered around the many effects of night-time light pollution on the biology of a wide range of organisms, but it went far beyond this in describing darkness as a biological imperative for the functioning of biological systems. Presentations focused on the absolute requirement of darkness for many aspects of normal behaviour and metabolism of many organisms and for the normal progression of their life cycles. Because there was no suitable term to describe the Symposium's main focus, the term \"scotobiology\" was introduced. The word is derived from the Greek \"scotos\", σκότος, \"dark,\" and relates to photobiology, which describes the biological effects of light (φῶς, \"phos\"; root: φωτ-, \"phot-\"). The term scotobiology appears not to have been used previously, although related terms such as skototropism and scotophyle have appeared in the literature.\n\n\nA review and the bibliography of the book by Rich and Longcore appear here: \n", "id": "9468040", "title": "Scotobiology"}
{"url": "https://en.wikipedia.org/wiki?curid=37988325", "text": "Lipidology\n\nLipidology is the scientific study of lipids.\n\nCompared to other biomedical fields, lipidology was originally less popular since the constant handling of oils, smears, and greases was unappealing and separation was difficult. The field became more popular following the advent of chromatography which allowed lipids to be isolated and analyzed. The field was further popularized following the cytologic application of the electron microscope which found that many metabolic pathways take place on the cell membrane, whose properties are strongly influenced by its lipid composition.\n\nThe Framingham Heart Study and other epidemiological studies have found a correlation between lipoproteins and cardiovascular disease.\n\n\n", "id": "37988325", "title": "Lipidology"}
{"url": "https://en.wikipedia.org/wiki?curid=1043627", "text": "Biomedicine\n\nBiomedicine (i.e. medical biology) is a branch of medical science that applies biological and physiological principles to clinical practice. The branch especially applies to biology and physiology.\nBiomedicine also can relate to many other categories in health and biological related fields. It has been the dominant health system for more than a century.\n\nIt includes many biomedical disciplines and areas of specialty that typically contain the \"bio-\" prefix such as:\n\nMedical biology is the cornerstone of modern health care and laboratory diagnostics. It concerns a wide range of scientific and technological approaches: from an in vitro diagnostics to the in vitro fertilisation, from the molecular mechanisms of a cystic fibrosis to the population dynamics of the HIV virus, from the understanding molecular interactions to the study of the carcinogenesis, from a single-nucleotide polymorphism (SNP) to the gene therapy.\n\nMedical biology based on molecular biology combines all issues of developing molecular medicine into large-scale structural and functional relationships of the human genome, transcriptome, proteome, physiome and metabolome with the particular point of view of devising new technologies for prediction, diagnosis and therapy \n\nBiomedicine involves the study of (patho-) physiological processes with methods from biology and physiology. Approaches range from understanding molecular interactions to the study of the consequences at the in vivo level. These processes are studied with the particular point of view of devising new strategies for diagnosis and therapy.\n\nDepending on the severity of the disease, biomedicine pinpoints a problem within a patient and fixes the problem through medical intervention. Medicine focuses on curing diseases rather than improving one's health.\n\nMolecular biology is the process of synthesis and regulation of a cell’s DNA, RNA, and protein. Molecular biology consists of different techniques including Polymerase chain reaction, Gel electrophoresis, and macromolecule blotting to manipulate DNA.\n\nPolymerase chain reaction is done by placing a mixture of the desired DNA, DNA polymerase, primers, and nucleotide bases into a machine. The machine heats up and cools down at various temperatures to break the hydrogen bonds binding the DNA and allows the nucleotide bases to be added onto the two DNA templates after it has been separated.\n\nGel electrophoresis is a technique used to identify similar DNA between two unknown samples of DNA. This process is done by first preparing an agarose gel. This jelly-like sheet will have wells for DNA to be poured into. An electric current is applied so that the DNA, which is negatively charged due to its phosphate groups is attracted to the positive electrode. Different rows of DNA will move at different speeds because some DNA pieces are larger than others. Thus if two DNA samples show a similar pattern on the gel electrophoresis, one can tell that these DNA samples match.\n\nMacromolecule blotting is a process performed after gel electrophoresis. An alkaline solution is prepared in a container. A sponge is placed into the solution and an agaros gel is placed on top of the sponge. Next, nitrocellulose paper is placed on top of the agarose gel and a paper towels are added on top of the nitrocellulose paper to apply pressure. The alkaline solution is drawn upwards towards the paper towel. During this process, the DNA denatures in the alkaline solution and is carried upwards to the nitrocellulose paper. The paper is then placed into a plastic bag and filled with a solution full of the DNA fragments, called the probe, found in the desired sample of DNA. The probes anneal to the complementary DNA of the bands already found on the nitrocellulose sample. Afterwards, probes are washed off and the only ones present are the ones that have annealed to complementary DNA on the paper. Next the paper is stuck onto an x ray film. The radioactivity of the probes creates black bands on the film, called an autoradiograph. As a result, only similar patterns of DNA to that of the probe are present on the film. This allows us the compare similar DNA sequences of multiple DNA samples. The overall process results in a precise reading of similarities in both similar and different DNA sample.\n\nBiochemistry is the science of the chemical processes which takes place within living organisms. Living organisms need essential elements to survive, consisting of carbon, hydrogen, nitrogen, oxygen, calcium, and phosphorus. These elements make up the four big macromolecules that living organisms need to survive- carbohydrates, lipids, proteins, and nucleic acids.\n\nCarbohydrates, made up of carbon, hydrogen, and oxygen, are energy storing molecules. The simplest one of carbohydrates is glucose, CHO, is used in cellular respiration to produce ATP, adenosine triphosphate, which supplies cells with energy.\n\nProteins are chains of amino acids that function to contract skeletal muscle, function catalysts, transport molecules, and storage molecules. Proteins can facilitate biochemical processes, by lowering the activation energy of a reaction. Hemoglobins are also proteins, that carry oxygen to the cells in an organisms body.\n\nLipids, also known as fats, also serve to store energy, but in the long term. Due to their unique structure, lipids provide more than twice the amount of energy that carbohydrates do. Lipids can be used as insulation, as it is present below the layer of skin in living organisms. Moreover, lipids can be used in hormone production to maintain a healthy hormonal balance and provide structure to your cell walls.\n\nNucleic acids are used to store DNA in every living organism. The two types of nucleic acids are DNA and RNA. DNA is the main genetic information storing substance found oftentimes in the nucleus, which controls the processes that the cell undergoes. DNA consists of two complimentary antiparallel strands consisting varying patterns of nucleotides. RNA is a single strand of DNA, which is transcribed from DNA and used for DNA translation, which is the process for making proteins out of RNA sequences.\n", "id": "1043627", "title": "Biomedicine"}
{"url": "https://en.wikipedia.org/wiki?curid=40610658", "text": "Neuromechanics\n\nAs originally proposed by Enoka, neuromechanics is a field of study that combines concepts from biomechanics and neurophysiology to study human movement. Neuromechanics examines the combined roles of the skeletal, muscular, and nervous systems and how they interact to produce the motion required to complete a motor task.\n\nMuscles signals stimulated by neurological impulses are collected using electromyography (EMG). These muscle signals are indicative of neural activity. In certain instances, EMG data can be indicative of neuroplasticity and learning of motor tasks. The muscular system, specifically skeletal muscle, creates movement around bone joints, and the central nervous system is imperative in directing skeletal muscles in motor movements.\n\nNeuromechanics is the field of study that combines neuroscience and biomechanics in an effort to understand movement and its relationship with the brain. Neuromechanics is an area that attempts to combine the efforts of muscles, sensory organs, pattern generators in the brain, and the central nervous system itself to explain motion. Neuromechanics applications include easing health problems and designing and controlling robotic systems.\n\nNeuroscience is the study of the nervous system. The nervous system is divided into two sub-systems: the peripheral nervous system and the central nervous system.\n\nThe peripheral nervous system is then composed of three sub-systems: the somatic nervous system, the autonomic nervous system, and the visceral nervous system. The autonomic nervous system is also broken down into the sympathetic nervous system, the parasympathetic nervous system, and the enteric nervous system. The nervous system responsible for voluntary motion, including lower limb motion, is the somatic nervous system. Though the somatic nervous system is part of the peripheral nervous system, motion also involves use of elements of the central nervous system: the brain and the spinal cord.\n\nNeuroscience contributes to human neuromechanics by studying how different neurological diseases contribute to biomechanical problems and changes in typical movement. Neuroscience deals with studying the cause of visible problems.\n\nBiomechanics is the study of the structure and function of living systems such as humans, animals, and other organisms by means of mechanics. Much of biomechanics is concerned with simple motor tasks such as walking. Walking can be defined by the gait cycle. The gait cycle is a repetitive event that consists of one full step from heel-strike to the next heel-strike in the same foot. It can be divided into two phases: stance phase and swing phase. Stance phase consists of the time during which the heel strikes the ground to the point in time at which the toe leaves the ground. Swing phase consists of the rest of the gait cycle, the time between the toe leaving the ground to the next heel strike.\n\nBiomechanics contributes to neuromechanics by studying how the body responds to different conditions whether they be neurological diseases or physical impairments. Biomechanics deals with studying the effect that results from such conditions.\n\nThe inverted pendulum theory of gait is a neuromechanical approach to understanding human movement. In the theory, the weight of the body is reduced to a center of mass resting on a massless leg at a single support. The ground reaction force travels from the center of pressure at the bottom of the massless leg to the center of mass at the top of the massless leg. The velocity vector of the center of mass is always perpendicular to the ground reaction force.\n\nWalking consists of alternating single-support and double-support phases. The single-support phase occurs when one leg is in contact with the ground while the double-support phase occurs when two legs are in contact with the ground.\n\nThe inverted pendulum is stabilized by constant feedback from the brain and can operate even in the presence of sensory loss. In animals who have lost all sensory input to the moving limb, the variables produced by gait (center of mass acceleration, velocity of animal, and position of the animal) remain constant between both groups.\n\nDuring postural control, delayed feedback mechanisms are used in the temporal reproduction of task-level functions such as walking. The nervous system takes into account feedback from the center of mass acceleration, velocity, and position of an individual and utilizes the information to predict and plan future movements. Center of mass acceleration is essential in the feedback mechanism as this feedback takes place before any significant displacement data can be determined.\n\nThe inverted pendulum theory directly contradicts the six determinants of gait, another theory for gait analysis. The six determinants of gait predict very high energy expenditure for the sinusoidal motion of the Center of Mass during gait, while the inverted pendulum theory offers the possibility that energy expenditure can be near zero. Paradoxically, the inverted pendulum theory predicts that little to no work is required for walking.\n\nElectromyography (EMG) is a tool used to measure the electrical outputs produced by skeletal muscles upon activation. Motor nerves innervate skeletal muscles and cause contraction upon command from the central nervous system. This contraction is measured by EMG and is typically measured on the scale of millivolts (mV). Another form of EMG data that is analyzed is integrated EMG (iEMG) data. iEMG measures the area under the EMG signal which corresponds to the overall muscle effort rather than the effort at a specific instant.\n\nThere are four instrumentation components used to detect these signals: (1) the signal source, (2) the transducer used to detect the signal, (3) the amplifier, and (4) the signal processing circuit. The signal source refers to the location at which the EMG electrode is place. EMG signal acquisition is dependent on distance from the electrode to the muscle fiber, so placement is imperative. The transducer used to detect the signal is an EMG electrode than transforms the bioelectric signal from the muscle to a readable electric signal. The amplifier reproduces and undistorted bioelectric signal and also allows for noise reduction in the signal. Signal processing involves taking the recorded electrical impulses, filtering them, and enveloping the data.\n\nLatency is a measure of the time span between the activation of a muscle and its peak EMG value. Latency is used as a means to diagnose disorders of the nervous system such as a herniated disc, amyotrophic lateral sclerosis (ALS), or myasthenia gravis (MG). These disorders may cause a disruption of the signal at the muscle, the nerve, or the junction between the muscle and the nerve.\n\nThe use of EMG to identify nervous systems disorders is known as a nerve conduction study (NCS). Nerve conduction studies can only diagnose diseases on the muscular and nerve level. They cannot detect disease in the spinal cord or the brain. In most disorders of the muscle, nerve, or neuromuscular junction, the latency time is increased. This is a result of decreased nerve conduction or electrical stimulation at the site of the muscle. In 50% of patients with cerebral atrophy cases, the M3 spinal reflex latency, was increased and on occasion separated from the M2 spinal reflex response. The separation between the M2 and M3 spinal reflex responses is typically 20 milliseconds, but in patients with cerebral atrophy, the separation was increased to 50 ms. In some cases, however, other muscles can compensate for the muscle suffering from decreased electrical stimulation. In the compensatory muscle, the latency time is actually decreased in order to substitute for the function of the diseased muscle. These kinds of studies are used in neuromechanics to identify motor disorders and their effects on a cellular and electrical level rather than a system motion level.\n\nA muscle synergy is a group of synergistic muscles and agonists that work together to perform a motor task. A muscle synergy is composed of agonist and synergistic muscles. An agonist muscle is a muscle that contracts individually, and it can cause a cascade of motion in neighboring muscles. Synergistic muscles aid the agonist muscles in motor control tasks, but they act against excess motion that the agonists may create.\n\nThe muscle synergy hypothesis is based on the assumption that the central nervous system controls muscle groups independently rather than individual muscles. The muscle synergy hypothesis presents motor control as a three-tiered hierarchy. In tier one, a motor task vector is created by the central nervous system. The central nervous system then transforms the muscle vector to act upon a group of muscle synergies in tier two. Then in tier three, muscle synergies define a specific ratio of the motor task for each muscle and assign it to its respective muscle to act upon the joint to perform the motor task.\n\nRedundancy plays a large role in muscle synergy. Muscle redundancy is a degrees of freedom problem on the muscular level. The central nervous system is presented with the opportunity to coordinate muscle movements, and it must choose one out of many. The muscle redundancy problem is a result of more muscle vectors than dimensions in the task space. Muscles can only generate tension by pulling, not pushing. This results in many muscle force vectors in multiple directions rather than a push and pull in the same direction.\n\nOne debate on muscle synergies is between the prime mover strategy and the cooperation strategy. The prime mover strategy arises when a muscle's vector can act in the same direction as the mechanical action vector, the vector of the limb's motion. The cooperation strategy, however, takes place when no muscle can act directly in the vector direction of the mechanical action resulting in a coordination of multiple muscles to achieve the task. The prime mover strategy over time has declined in popularity as it has been found through electromyography studies that no one muscle consistently provides more force that other muscles that are acting to move about a joint.\n\nThe muscle synergy theory is difficult to falsify. Though experimentation has shown that groups of muscles indeed work together to control motor tasks, neural connections allow for individual muscles to be activated. Though individual muscle activation may contradict muscle synergy, it also obscures it. Activation of individual muscles may override or block the input from and overall effect of muscle synergies.\n\nAdaptation in the neuromechanical sense is the body's ability to change an action to better suit the situation or environment in which it is acting. Adaptation can be a result of injury, fatigue, or practice. Adaptation can be measured in a variety of ways: electromyography, three-dimensional reconstruction of joints, and changes in other variables pertaining to the specific adaptation being studied.\n\nInjury can cause adaptation in a number of ways. Compensation is a large factor in injury adaptation. Compensation is a result of one or more weakened muscles. The brain is given the task to perform a certain motor task, and once a muscle has been weakened, the brain computes energy ratios to send to other muscles to perform the original task in the desired fashion. Change in muscle contribution is not the only byproduct of a muscle-related injury. Change in loading of the joint is another result which, if prolonged, can be harmful for the individual.\n\nMuscle fatigue is the neuromuscular adaptation to challenges over a period of time. The use of motor units over a period of time can result in changes in the motor command from the brain. Since the force of contraction cannot be changed, the brain instead recruits more motor units to achieve maximal muscle contraction. Recruitment of motor units varies from muscle to muscle depending on the upper limit of motor recruitment in the muscle.\n\nAdaptation due to practice can be a result of intended practice such as sports or unintended practice such as wearing an orthosis. In athletes, repetition results in muscle memory. The motor task becomes a long-term memory that can be repeated without much conscious effort. This allows the athlete to focus on fine-tuning their motor task strategy. Resistance to fatigue also comes with practice as the muscle is strengthened, but the speed at which an athlete can complete a motor task is also increased with practice. Volleyball players compared to non-jumpers show more repeatable control of muscles surrounding the knee that is controlled by co-activation in the single jump condition. In the repeated jump condition, both volleyball players and non-jumpers have a linear decrease in normalized jump flight time. Though the normalized linear decrease is the same for athletes and non-athletes, athletes consistently have higher flight times.\n\nThere is also adaptation associated with use of a prosthesis or an orthosis. This operates similarly to adaptation due to fatigue; however, muscles can actually be fatigued or alter their mechanical contribution to a motor task as a result of wearing the orthosis. An ankle foot orthosis is a common solution to injury of the lower limb, specifically around the ankle joint. An ankle foot orthosis can be assistive or resistive. An assistive ankle orthosis encourages ankle movement, and a resistive ankle orthosis inhibits ankle movement. Upon wearing an assistive ankle foot orthosis, individuals have decreased EMG amplitude and joint stiffness over time while the opposite occurs for resistive ankle foot orthoses. Additionally, not only can electromyography readings differ, but the physical path that joints travel along can be altered as well.\n\n", "id": "40610658", "title": "Neuromechanics"}
{"url": "https://en.wikipedia.org/wiki?curid=42985608", "text": "Cognitive biology\n\nCognitive biology is an emerging science that regards natural cognition as a biological function. It is based on the theoretical assumption that every organism—whether a single cell or multicellular—is continually engaged in systematic acts of cognition coupled with intentional behaviors, i.e., a sensory-motor coupling. That is to say, if an organism can sense stimuli in its environment and respond accordingly, it is cognitive. Any explanation of how natural cognition may manifest in an organism is constrained by the biological conditions in which its genes survives from one generation to the next. And since by Darwinian theory the species of every organism is evolving from a common root, three further elements of cognitive biology are required: (i) the study of cognition in one species of organism is useful, through contrast and comparison, to the study of another species’ cognitive abilities; (ii) it is useful to proceed from organisms with simpler to those with more complex cognitive systems, and (iii) the greater the number and variety of species studied in this regard, the more we understand the nature of cognition.\n\nWhile cognitive science endeavors to explain human thought and the conscious mind, the work of cognitive biology is focused on the most fundamental process of cognition for any organism. In the past several decades, biologists have investigated cognition in organisms large and small, both plant and animal. “Mounting evidence suggests that even bacteria grapple with problems long familiar to cognitive scientists, including: integrating information from multiple sensory channels to marshal an effective response to fluctuating conditions; making decisions under conditions of uncertainty; communicating with conspecifics and others (honestly and deceptively); and coordinating collective behaviour to increase the chances of survival.” Without thinking or perceiving as humans would have it, an act of basic cognition is arguably a simple step-by-step process through which an organism senses a stimulus, then finds an appropriate response in its repertoire and enacts the response. However, the biological details of such basic cognition have neither been delineated for a great many species nor sufficiently generalized to stimulate further investigation. This lack of detail is due to the lack of a science dedicated to the task of elucidating the cognitive ability common to all biological organisms. That is to say, a \"science\" of cognitive biology has yet to be established. A prolegomena for such science was presented in 2007 and several authors have published their thoughts on the subject since the late 1970s. Yet as the examples in the next section suggest, there is neither consensus on the theory nor widespread application in practice.\n\nAlthough the two terms are sometimes used synonymously, cognitive biology should not be confused with the biology of cognition in the sense that it is used by adherents to the Chilean School of Biology of Cognition. Also known as the Santiago School, the biology of cognition is based on the work of Francisco Varela and Humberto Maturana, who crafted the doctrine of autopoiesis. Their work began in 1970 while the first mention of cognitive biology by Brian Goodwin (discussed below) was in 1977 from a different perspective.\n\n'Cognitive biology' first appeared in the literature as a paper with that title by Brian C. Goodwin in 1977. There and in several related publications Goodwin explained the advantage of cognitive biology in the context of his work on morphogenesis. He subsequently moved on to other issues of structure, form, and complexity with little further mention of cognitive biology. Without an advocate, Goodwin’s concept of cognitive biology has yet to gain widespread acceptance. \nAside from an essay regarding Goodwin’s conception by Margaret Boden in 1980, the next appearance of ‘cognitive biology’ as a phrase in the literature came in 1986 from a professor of biochemistry, Ladislav Kováč. His conception, based on natural principles grounded in bioenergetics and molecular biology, is briefly discussed below. Kováč’s continued advocacy has had a greater influence in his homeland, Slovakia, than elsewhere partly because several of his most important papers were written and published only in Slovakian.\n\nBy the 1990s, breakthroughs in molecular, cell, evolutionary, and developmental biology generated a cornucopia of data-based theory relevant to cognition. Yet aside from the theorists already mentioned, no one was addressing cognitive biology except for Kováč.\n\nLadislav Kováč's “Introduction to cognitive biology” (Kováč, 1986a) lists ten ‘Principles of Cognitive Biology.’ A closely related thirty page paper was published the following year: “Overview: Bioenergetics between chemistry, genetics and physics.” (Kováč, 1987). Over the following decades, Kováč elaborated, updated, and expanded these themes in frequent publications, including \"Fundamental principles of cognitive biology\" (Kováč, 2000), “Life, chemistry, and cognition” (Kováč, 2006a), \"Information and Knowledge in Biology: Time for Reappraisal” (Kováč, 2007) and \"Bioenergetics: A key to brain and mind\" (Kováč, 2008).\n\nThe concept of cognitive biology is exemplified by this seminar description:\n\nThe University of Adelaide has established a \"Cognitive Biology\" workgroup using this operating concept:\nMembers of the group study the biological literature on simple organisms (e.g., nematode) in regard to cognitive process and look for homologues in more complex organisms (e.g., crow) already well studied. This comparative approach is expected to yield simple cognitive concepts common to all organisms. “It is hoped a theoretically well-grounded toolkit of basic cognitive concepts will facilitate the use and discussion of research carried out in different fields to increase understanding of two foundational issues: what cognition is and what cognition does in the biological context.” (Bold letters from original text.)\nThe group’s choice of name, as they explain on a separate webpage, might have been ‘embodied cognition’ or ‘biological cognitive science.’ But the group chose ‘cognitive biology’ for the sake of (i) emphasis and (ii) method. For the sake of emphasis, (i) “We want to keep the focus on biology because for too long cognition was considered a function that could be almost entirely divorced from its physical instantiation, to the extent that whatever could be said of cognition almost by definition had to be applicable to both organisms and machines.” (ii) The method is to “assume (if only for the sake of enquiry) that cognition is a biological function similar to other biological functions—such as respiration, nutrient circulation, waste elimination, and so on.”\nThe method supposes that the genesis of cognition is biological, i.e., the method is \"biogenic\". The host of the group’s website has said elsewhere that cognitive biology requires a biogenic approach, having identified ten principles of biogenesis in an earlier work. The first four biogenic principles are quoted here to illustrate the depth at which the foundations have been set at the Adelaide school of cognitive biology:\n\n\n\nThe words ‘cognitive’ and ‘biology’ are also used together as the name of a category. The category of \"cognitive biology\" has no fixed content but, rather, the content varies with the user. If the content can only be recruited from \"cognitive science\", then cognitive biology would seem limited to a selection of items in the main set of sciences included by the interdisciplinary concept—cognitive psychology, artificial intelligence, linguistics, philosophy, neuroscience, and cognitive anthropology. These six separate sciences were allied “to bridge the gap between brain and mind” with an interdisciplinary approach in the mid-1970s. Participating scientists were concerned only with human cognition. As it gained momentum, the growth of cognitive science in subsequent decades seemed to offer a big tent to a variety of researchers. Some, for example, considered evolutionary epistemology a fellow-traveler. Others appropriated the keyword, as for example Donald Griffin in 1978, when he advocated the establishment of cognitive ethology.\nMeanwhile, breakthroughs in molecular, cell, evolutionary, and developmental biology generated a cornucopia of data-based theory relevant to cognition. Categorical assignments were problematic. For example, the decision to append \"cognitive\" to a body of biological research on neurons, e.g. the cognitive biology of neuroscience, is separate from the decision to put such body of research in a category named cognitive sciences. No less difficult a decision needs be made—between the computational and constructivist approach to cognition, and the concomitant issue of simulated v. embodied cognitive models—before appending biology to a body of cognitive research, e.g. the cognitive science of artificial life.\nOne solution is to consider \"cognitive biology\" only as a subset of \"cognitive science\". For example, a major publisher’s website displays links to material in a dozen domains of major scientific endeavor. One of which is described thus: “Cognitive science is the study of how the mind works, addressing cognitive functions such as perception and action, memory and learning, reasoning and problem solving, decision-making and consciousness.” Upon its selection from the display, the \"Cognitive Science\" page offers in nearly alphabetical order these topics: \"Cognitive Biology\", Computer Science, Economics, Linguistics, Psychology, Philosophy, and Neuroscience. Linked through that list of topics, upon its selection the \"Cognitive Biology\" page offers a selection of reviews and articles with biological content ranging from cognitive ethology through evolutionary epistemology; cognition and art; evo-devo and cognitive science; animal learning; genes and cognition; cognition and animal welfare; etc.\n\nA different application of the \"cognitive biology\" category is manifest in the 2009 publication of papers presented at a three-day interdisciplinary workshop on “The New Cognitive Sciences” held at the Konrad Lorenz Institute for Evolution and Cognition Research in 2006. The papers were listed under four headings, each representing a different domain of requisite cognitive ability: (i) space, (ii) qualities and objects, (iii) numbers and probabilities, and (iv) social entities. The workshop papers examined topics ranging from “Animals as Natural Geometers” and “Color Generalization by Birds” through “Evolutionary Biology of Limited Attention” and “A comparative Perspective on the Origin of Numerical Thinking” as well as “Neuroethology of Attention in Primates” and ten more with less colorful titles. “[O]n the last day of the workshop the participants agreed [that] the title ‘Cognitive Biology’ sounded like a potential candidate to capture the merging of the cognitive and the life sciences that the workshop aimed at representing.” Thus the publication of Tommasi, et al. (2009), \"Cognitive Biology: Evolutionary and Developmental Perspectives on Mind, Brain and Behavior.\"\nA final example of categorical use comes from an author’s introduction to his 2011 publication on the subject, \"Cognitive Biology: Dealing with Information from Bacteria to Minds\". After discussing the differences between the cognitive and biological sciences, as well as the value of one to the other, the author concludes: “Thus, the object of this book should be considered as an attempt at building a new discipline, that of \"cognitive biology\", which endeavors to bridge these two domains.” There follows a detailed methodology illustrated by examples in biology anchored by concepts from cybernetics (e.g., self-regulatory systems) and quantum information theory (regarding probabilistic changes of state) with an invitation \"to consider system theory together with information theory as the formal tools that may ground biology and cognition as traditional mathematics grounds physics.”\n\n\n\n", "id": "42985608", "title": "Cognitive biology"}
{"url": "https://en.wikipedia.org/wiki?curid=41810627", "text": "Geroscience\n\nGeroscience is an interdisciplinary field that aims to understand, at the molecular level, the relationship between aging, age-related diseases and other conditions that diminish our quality of life. Because aging is the major risk factor for most common chronic diseases and conditions, an understanding of the role of aging in the onset and progression of disease should open up new avenues for disease prevention, amelioration, and cures. Geroscience research spans multiple disciplines including molecular biology, neuroscience, protein chemistry, cell biology, genetics, endocrinology, pharmacology, mathematics, and others. The common goal is to understand the role of aging in age-related loss of quality of life and susceptibility to disease.\n\nAge-related disease is arguably the single greatest threat to human health in the 21st century. Statistics from numerous sources highlight the fact that age-related diseases increasingly represent a true worldwide emergency: for example, by 2030 the national healthcare bill in the US is projected to reach four trillion dollars, with fully 50% of that being required for Americans 65 years and older (see statistics supplied by the National Institute on Aging and the Alliance for Aging Research).\n\nThe traditional approach in biomedical research is to investigate single-diseases and conditions in isolation. A more efficient approach would be investigating the cause(s) of aging itself. Treating aging (the major risk factor for most chronic conditions) the way we currently approach other medical conditions such as hypertension (the major risk factor for CVD), holds the possibility of delaying or preventing many diseases and conditions of later life as a group. Geroscience addresses a wide variety of disease states, including non-life-threatening conditions such as fatigability, resilience and frailty. As such, geroscience is broader in scope than traditional disciplines such as neurodegeneration, cancer biology, and geriatric medicine. It is also distinct from the traditional discipline of gerontology, which has large patient care and social sciences components.\n\nMany human clinical trials have failed in part due to an incomplete picture of the nature of complex chronic diseases of the elderly. The biology of aging field has developed spectacularly over the last twenty-five years, yet the potential impact of these new findings has yet to be achieved. The manipulation of longevity in simple laboratory animals by genetic modification or pharmacological interventions is now commonplace.\nHowever, a central concept of geroscience is that multiple human diseases arise from a common cause: aging itself. The term \"geroscience\" was coined by Buck Institute for Research on Aging scientist Gordon J. Lithgow, in 2007.[3] The scientific discipline was recognized in the U.S. Senate Appropriations on the FY2010 Senate Labor, Health and Human Services and Education Bill. The scientific discipline was recognized in the U.S. Senate Appropriations on the FY2010 Senate Labor, Health and Human Services and Education Bill.\n\nGeroscience was the organizing principle of the Interdisciplinary Research Consortium on Geroscience established at the Buck Institute. This Consortium was funded by the National Institutes of Health Common Fund. The IRC on Geroscience was also discussed in the Senate Report 110-527; quote \"The National Institutes of Health’s (NIH) Interdisciplinary Research Consortium in Geroscience fosters collaboration among biologists, biochemists, geneticists, physicians, physiologists, statisticians, and chemists that will help scientists to better understand age-related diseases and disorders. Examples include studies of the effects of diet on aging and why the aging brain recovers less easily from traumatic brain injury.\n\nIn 2011 the term \"geroscience\" was adopted by a trans-NIH team interested in the same concepts, the Geroscience Interest Group (GSIG). GSIG works with NIH and external stakeholders to promote geroscience research through activities that include workshops, seminars, and publications.\n\nThe NIH, with support from the Alliance for Aging Research and the Gerontological Society of America, hosted “Advances in Geroscience: Impacts on Healthspan and Chronic Disease” on October 30–31, 2013. This scientific conference, which took place on the NIH campus in Bethesda, MD, examined the extent to which the physiological effects of aging represent a common major risk factor for chronic diseases. A subsequent Summit, held in New York City under the auspices of the New York Academy of Sciences focused on the reverse side of the issue: attendees at the “Disease Drivers of Aging” meeting discussed how some chronic diseases and/or their treatments accelerate the aging process, thus rendering patients more vulnerable to additional diseases and ailments most commonly seen in elderly individuals.\n\n", "id": "41810627", "title": "Geroscience"}
{"url": "https://en.wikipedia.org/wiki?curid=436824", "text": "Gerontology\n\nGerontology is the study of the social, cultural, psychological, cognitive, and biological aspects of aging. The word was coined by Ilya Ilyich Mechnikov in 1903, from the Greek γέρων, \"geron\", \"old man\" and -λογία, \"-logia\", \"study of\". The field is distinguished from geriatrics, which is the branch of medicine that specializes in the treatment of existing disease in older adults. Gerontologists include researchers and practitioners in the fields of biology, nursing, medicine, criminology, dentistry, social work, physical and occupational therapy, psychology, psychiatry, sociology, economics, political science, architecture, geography, pharmacy, public health, housing, and anthropology. \n\nThe multidisciplinary nature of gerontology means that there are a number of sub-fields which overlap with gerontology. There are policy issues, for example, involved in government planning and the operation of nursing homes, investigating the effects of an ageing population on society, and the design of residential spaces for older people that facilitate the development of a sense of place or home. Dr. Lawton, a behavioral psychologist at the Philadelphia Geriatric Center, was among the first to recognize the need for living spaces designed to accommodate the elderly, especially those with Alzheimer's disease. As an academic discipline the field is relatively new. The USC Leonard Davis School created the first PhD, master’s and bachelor’s degree programs in gerontology in 1975.\n\nIn the medieval Islamic world, several physicians wrote on issues related to Gerontology. Avicenna's \"The Canon of Medicine\" (1025) offered instruction for the care of the aged, including diet and remedies for problems including constipation. Arabic physician Ibn Al-Jazzar Al-Qayrawani (Algizar, c. 898–980) wrote on the aches and conditions of the elderly (Ammar 1998, p. 4). His scholarly work covers sleep disorders, forgetfulness, how to strengthen memory, and causes of mortality Ishaq ibn Hunayn (died 910) also wrote works on the treatments for forgetfulness (U.S. National Library of Medicine, 1994).\n\nWhile the number of aged humans, and the life expectancy, tended to increase in every century since the 14th, society tended to consider caring for an elderly relative as a family issue. It was not until the coming of the Industrial Revolution that ideas shifted in favor of a societal care-system.\n\nSome early pioneers, such as Michel Eugène Chevreul, who himself lived to be 102, believed that aging itself should be a science to be studied. Élie Metchnikoff coined the term \"gerontology\" 1903\n\nIt was not until the 1940s that pioneers like James Birren began organizing gerontology into its own field. Recognizing that there were experts in many fields all dealing with the older population, it became apparent that a group like the Gerontological Society of America (founded in 1945) was needed. Two decades later, James Birren was appointed as the founding director of the first academic research center devoted exclusively to the study of aging, the Ethel Percy Andrus Gerontology Center. The Baltimore Longitudinal Studies of Aging began in 1958 in order to study physiological changes in healthy middle-aged and older men living in the community by testing them every two years on numerous physiological parameters. In 1967, the University of South Florida and the University of North Texas (formerly North Texas State University) received Older Americans Act training grants from the U.S. Administration on Aging to launch the nation's first degree programs in gerontology, at the master's level. In 1975, the University of Southern California's Leonard Davis School of Gerontology, became the country's first school of gerontology within a university and, later, offered the first PhD in Gerontology degree.\n\nGerontological Education has flourished in the United States since 1967 and degrees at all academic levels are now offered by a number of colleges and universities. One of the pioneering gerontologists, Robert Neil Butler, pushed for care and respect of the elderly. Several university-based centers on aging have been founded such as the Duke University Center on Aging, the University of Georgia Institute of Gerontology, the Center of Aging at the University of Chicago, and the Stanford Center on Longevity. Currently, PhD programs in gerontology are available at Miami University, the University of Kansas, University of Kentucky, University of Maryland Baltimore, University of Massachusetts Boston, and the University of Southern California.\n\nThe world is forecast to undergo rapid population aging in the next several decades. In 1900, there were 3.1 million people aged 65 years and older living in the United States. However, this population continued to grow throughout the 20th century and reached 31.2, 35, and 40.3 million people in 1990, 2000, and 2010, respectively. Notably, in the United States and across the world, the \"baby boomer\" generation began to turn 65 in 2011. Recently, the population aged 65 years and older has grown at a faster rate than the total population in the United States. The total population increased by 9.7%, from 281.4 million to 308.7 million, between 2000 and 2010. However, the population aged 65 years and older increased by 15.1% during the same period. It has been estimated that 25% of the population in the United States and Canada will be aged 65 years and older by 2025. Moreover, by 2050, it is predicted that, for the first time in United States history, the number of individuals aged 60 years and older will be greater than the number of children aged 0 to 14 years. Those aged 85 years and older (oldest-old) are projected to increase from 5.3 million to 21 million by 2050. Adults aged 85–89 years constituted the greatest segment of the oldest-old in 1990, 2000, and 2010. However, the largest percentage point increase among the oldest-old occurred in the 90- to 94-year-old age group, which increased from 25.0% in 1990 to 26.4% in 2010.\n\nWith the rapid growth of the aging population, social work education and training specialized in older adults and practitioners interested in working with older adults are increasingly in demand.\n\nThere has been a considerable disparity between the number of men and women in the older population in the United States. In both 2000 and 2010, women outnumbered men in the older population at every single year of age (e.g., 65 to 100 years and over). The sex ratio, which is a measure used to indicate the balance of males to females in a population, is calculated by taking the number of males divided by the number of females, and multiplying by 100. Therefore, the sex ratio is the number of males per 100 females. In 2010, there were 90.5 males per 100 females in the 65-year-old population. However, this represented an increase from 1990 when there were 82.7 males per 100 females, and from 2000 when the sex ratio was 88.1. Although the gender gap between men and women has narrowed, women continue to have a greater life expectancy and lower mortality rates at older ages relative to men. For example, the Census 2010 reported that there were approximately twice as many women as men living in the United States at 89 years of age (361,309 versus 176,689, respectively).\n\nThe number and percentage of older adults living in the United States vary across the four different regions (Northeast, Midwest, West, and South) defined by the United States census. In 2010, the South contained the greatest number of people aged 65 years and older and 85 years and older. However, proportionately, the Northeast contains the largest percentage of adults aged 65 years and older (14.1%), followed by the Midwest (13.5%), the South (13.0%), and the West (11.9%). Relative to the Census 2000, all geographic regions demonstrated positive growth in the population of adults aged 65 years and older and 85 years and older. The most rapid growth in the population of adults aged 65 years and older was evident in the West (23.5%), which showed an increase from 6.9 million in 2000 to 8.5 million in 2010. Likewise, in the population aged 85 years and older, the West (42.8%) also showed the fastest growth and increased from 806,000 in 2000 to 1.2 million in 2010. It is worth highlighting that Rhode Island was the only state that experienced a reduction in the number of people aged 65 years and older, and declined from 152,402 in 2000 to 151,881 in 2010. Conversely, all states exhibited an increase in the population of adults aged 85 years and older from 2000 to 2010.\n\nBiogerontology is the sub-field of gerontology concerned with the biological aging process, its evolutionary origins, and potential means to intervene in the process. It involves interdisciplinary research on biological aging's causes, effects, and mechanisms. Conservative biogerontologists such as Leonard Hayflick have predicted that the human life expectancy will peak at about 92 years old, while others such as James Vaupel have predicted that in industrialized countries, life expectancies will reach 100 for children born after the year 2000. and some surveyed biogerontologists have predicted life expectancies of two or more centuries. with Aubrey de Grey offering the \"tentative timeframe\" that with adequate funding of research to develop interventions in aging such as Strategies for Engineered Negligible Senescence, \"we have a 50/50 chance of developing technology within about 25 to 30 years from now that will, under reasonable assumptions about the rate of subsequent improvements in that technology, allow us to stop people from dying of aging at any age\", leading to life expectancies of 1,000 years.\n\nBiomedical gerontology, also known as experimental gerontology and life extension, is a sub-discipline of biogerontology that endeavors to slow, prevent, and even reverse aging in both humans and animals. Most \"life extensionists\" believe the human life span can be increased within the next century, if not sooner. Biogerontologists vary in the degree to which they focus on the study of the aging process as a means of mitigating the diseases of aging or extending lifespan, although most agree that extension of lifespan will necessarily flow from reductions in age-related disease and frailty, although some argue that maximum life span cannot be altered or that it is undesirable to try.\n\nIn contrast with biogerontology, geriatrics studies the treatment of disease in aging people.\n\nTheories of aging are numerous and no one theory has been accepted. There is a wide spectrum of the types of theories for the causes of aging with programmed theories on one extreme and error theories on the other. Regardless of the theory, a commonality is that as humans age, functions of the body decline.\n\nWear and tear theories of aging suggest that as an individual ages, body parts such as cells and organs wear out from continued use. Wearing of the body can be attributable to internal or external causes that eventually lead to an accumulation of insults which surpasses the capacity for repair. Due to these internal and external insults, cells lose their ability to regenerate, which ultimately leads to mechanical and chemical exhaustion. Some insults include chemicals in the air, food, or smoke. Other insults may be things such as viruses, trauma, free radicals, cross-linking, and high body temperature.\n\nGenetic theories of aging propose that aging is programmed within each individual's genes. According to this theory, genes dictate cellular longevity. Programmed cell death, or apoptosis, is determined by a \"biological clock\" via genetic information in the nucleus of the cell. Genes responsible for apoptosis provide an explanation for cell death, but are less applicable to death of an entire organism. An increase in cellular apoptosis may correlate to aging, but is not a 'cause of death'. Environmental factors and genetic mutations can influence gene expression and accelerate aging. More recently epigenetics have been explored as a contributing factor. The epigenetic clock, which objectively measures the biological age of cells and tissues, may become useful for testing different biological aging theories.\n\nGeneral imbalance theories of aging suggest that body systems, such as the endocrine, nervous, and immune systems, gradually decline and ultimately fail to function. The rate of failure varies system by system.\n\nAccumulation theories of aging suggest that aging is bodily decline that results from an accumulation of elements. Elements can be foreign and introduced to the body from the environment. Other elements can be the natural result of cell metabolism. An example of an accumulation theory is the Free Radical Theory of Aging. According to this theory, byproducts of regular cell metabolism called free radicals interact with cellular components such as the cell membrane and DNA and cause irreversible damage.\n\nThe idea that free radicals are toxic agents was first proposed by Rebeca Gerschman and colleagues. In 1956, Denham Harman proposed the free-radical theory of aging and even demonstrated that free radical reactions contribute to the degradation of biological systems. Oxidative damage of many types accumulate with age, such as oxidative stress that oxygen-free radicals, because the free radical theory of aging argues that aging results from the damage generated by reactive oxygen species (ROS). ROS are small, highly reactive, oxygen-containing molecules that can damage a complex of cellular components such as fat, proteins, or from DNA, they are naturally generated in small amounts during the body's metabolic reactions. These conditions become more common as we age, including diseases related to aging, such as dementia, cancer and heart disease.\n\nDNA damage has been one of the many causes in diseases related to aging. The stability of the genome is defined by the cells machinery of repair, damage tolerance, and checkpoint pathways that counteracts DNA damage. One hypothesis proposed by Gioacchino Failla in 1958 is that damage accumulation to the DNA causes aging. The hypothesis was developed soon by physicist Leó Szilárd. This theory has changed over the years as new research has discovered new types of DNA damage and mutations, and several theories of aging argue that DNA damage with or without mutations causes aging.\n\nSocial gerontology is a multi-disciplinary sub-field that specializes in studying or working with older adults. Social gerontologists may have degrees or training in social work, nursing, psychology, sociology, demography, public health, or other social science disciplines. Social gerontologists are responsible for educating, researching, and advancing the broader causes of older people.\n\nBecause issues of life span and life extension need numbers to quantify them, there is an overlap with demography. Those who study the demography of the human life span differ from those who study the social demographics of aging.\n\nSeveral theories of aging are developed to observed the aging process of older adults in society as well as how these processes are interpreted by men and women as they age.\n\nActivity theory was developed and elaborated by Cavan, Havighurst, and Albrecht. According to this theory, older adults' self-concept depends on social interactions. In order for older adults to maintain morale in old age, substitutions must be made for lost roles. Examples of lost roles include retirement from a job or loss of a spouse.\n\nActivity is preferable to inactivity because it facilitates well-being on multiple levels. Because of improved general health and prosperity in the older population, remaining active is more feasible now than when this theory was first proposed by Havighurst nearly six decades ago. The activity theory is applicable for a stable, post-industrial society, which offers its older members many opportunities for meaningful participation.Weakness: Some aging persons cannot maintain a middle-aged lifestyle, due to functional limitations, lack of income, or lack of a desire to do so. Many older adults lack the resources to maintain active roles in society. On the flip side, some elders may insist on continuing activities in late life that pose a danger to themselves and others, such as driving at night with low visual acuity or doing maintenance work to the house while climbing with severely arthritic knees. In doing so, they are denying their limitations and engaging in unsafe behaviors.\n\nDisengagement theory was developed by Cumming and Henry. According to this theory, older adults and society engage in a mutual separation from each other. An example of mutual separation is retirement from the workforce. A key assumption of this theory is that older adults lose \"ego-energy\" and become increasingly self-absorbed. Additionally, disengagement leads to higher morale maintenance than if older adults try to maintain social involvement. This theory is heavily criticized for having an escape clause - namely, that older adults who remain engaged in society are unsuccessful adjusters to old age.\n\nGradual withdrawal from society and relationships preserves social equilibrium and promotes self-reflection for elders who are freed from societal roles. It furnishes an orderly means for the transfer of knowledge, capital, and power from the older generation to the young. It makes it possible for society to continue functioning after valuable older members die.\n\nContinuity theory is an elusive concept. On the one hand,\nto exhibit continuity can mean to remain the same, to\nbe uniform, homogeneous, unchanging, even humdrum.\nThis static view of continuity is not very applicable\nto human aging. On the other hand, a dynamic\nview of continuity starts with the idea of a basic\nstructure which persists over time, but it allows for a\nvariety of changes to occur within the context provided\nby the basic structure. The basic structure is\ncoherent: It has an orderly or logical relation of parts\nthat is recognizably unique and that allows us to\ndifferentiate that structure from others. With the\nintroduction of the concept of time, ideas such as\ndirection, sequence, character development, and\nstory line enter into the concept of continuity as it is\napplied to the evolution of a human being. In this\ntheory, a dynamic concept of continuity is developed\nand applied to the issue of adaptation to normal\naging.\n\nA central premise of continuity theory is that, in\nmaking adaptive choices, middle-aged and older\nadults attempt to preserve and maintain existing internal\nand external structures and that they prefer to\naccomplish this objective by using continuity (i.e.,\napplying familiar strategies in familiar arenas of life).\nIn middle and later life, adults are drawn by the\nweight of past experience to use continuity as a\nprimary adaptive strategy for dealing with changes\nassociated with normal aging. To the extent that\nchange builds upon, and has links to, the person's\npast, change is a part of continuity. As a result of both\ntheir own perceptions and pressures from the social\nenvironment, individuals who are adapting to normal\naging are both predisposed and motivated\ntoward inner psychological continuity as well as outward\ncontinuity of social behavior and circumstances.\n\nContinuity theory views both internal and\nexternal continuity as robust adaptive strategies that\nare supported by both individual preference and\nsocial sanctions. Continuity theory consists of general\nadaptive principles that people who are normally \naging could be expected to follow, explanations of\nhow these principles work, and a specification of\ngeneral areas of life in which these principles could\nbe expected to apply. Accordingly, continuity theory\nhas enormous potential as a general theory of\nadaptation to individual aging.\n\nAccording to this theory, older adults born during different time periods form cohorts that define \"age strata\". There are two differences among strata: chronological age and historical experience. This theory makes two arguments. 1. Age is a mechanism for regulating behavior and as a result determines access to positions of power. 2. Birth cohorts play an influential role in the process of social change.\n\nAccording to this theory, which stems from the Life Course Perspective (Bengston and Allen, 1993), aging occurs from birth to death. Aging involves social, psychological, and biological processes. Additionally, aging experiences are shaped by cohort and period effects.\n\nAlso reflecting the life course focus,\nconsider the implications for how societies might function when age-based\nnorms vanish—a consequence of the deinstitutionalization of the life course—\nand suggest that these implications pose new challenges for theorizing aging\nand the life course in postindustrial societies. Dramatic reductions in mortality,\nmorbidity, and fertility over the past several decades have so shaken up the\norganization of the life course and the nature of educational, work, family, and\nleisure experiences that it is now possible for individuals to become old in new\nways. The configurations and content of other life stages are being altered as\nwell, especially for women. In consequence, theories of age and aging will need\nto be reconceptualized.\n\nAccording to this theory, which was developed beginning in the 1960s by Derek Price and Robert Merton and elaborated on by several researchers such as Dale Dannefer, inequalities have a tendency to become more pronounced throughout the aging process. A paradigm of this theory can be expressed in the adage \"the rich get richer and the poor get poorer\". Advantages and disadvantages in early life stages have a profound effect throughout the life span. However, advantages and disadvantages in middle adulthood have a direct influence on economic and health status in later life.\n\nEnvironmental gerontology is a specialization within gerontology that seeks to develop an understanding, means of analysis, opportunities to modify, and interventions that optimize the relationship between the aging person and their physical and social environment, utilizing interdisciplinary perspectives and approaches to attain these goals.\n\nThe field first began to emerge in the 1930s during the first studies on behavioral and social gerontology, and were associated with deterministic explanations of the relationship between aging and environment from genetic parameters and biological. In the 1970s and 1980s, theories of different researchers, as Kurt Lewin (living space model as a function of the person and the environment) and, mainly, M. Powell Lawton (1923–2001) (ecological adaptation model), based on the influence of the interactions between the older person and their environment (adaptation – ambient pressure), confirm the importance of the physical and social environment (objective and subjective) in the understanding of the aging population and the possibility of improving the quality of life in old age.\nEnvironmental Gerontology seeks to understand the socio-spatial implications of aging and its complex relationship with the environment, from analysis at different scales: micro scales (home and family) and macro scales (neighborhood, city, region), to enable social and environmental planning and policies that enable better aging. Among the main contributions of the discipline are the contributions to aging in place (wherever that place may be) and that older people prefer to age in their immediate environment where aspects such as spatial experience and place attachment are important for understanding the process.\n\nThe most famous environmental gerontologist remains M. Powell Lawton (1923–2001).\n", "id": "436824", "title": "Gerontology"}
{"url": "https://en.wikipedia.org/wiki?curid=48999251", "text": "Chemoproteomics\n\nChemoproteomics is an approach to discovering mechanisms for regulating biological pathways for the purpose of designing new pharmaceutical therapies. It is a chemical proteomic method for pharmacological discovery research.\n\nChemoproteomics can enable drug discovery. It is a method for identifying protein targets and potential off-target activity.\n\nA linker molecule connects a small molecule chemical probe (or library of chemical probes) with an analytically detectable bead or other agent for detection or separation.\n\n", "id": "48999251", "title": "Chemoproteomics"}
{"url": "https://en.wikipedia.org/wiki?curid=49926523", "text": "Idiobiology\n\nIdiobiology is a branch of biology which studies individual organisms, or the study of organisms as individuals.\n", "id": "49926523", "title": "Idiobiology"}
{"url": "https://en.wikipedia.org/wiki?curid=4985778", "text": "Forensic biology\n\nForensic biology is the application of biology to law enforcement.\n\nIt includes the subdisciplines of forensic anthropology, forensic botany, forensic entomology, forensic odontology, forensic toxicology and various DNA or protein based techniques.\n\nForensic biology has been used to prove a suspect was at a crime scene, identify illegal products from endangered species, solve crimes by matching crime scene evidence to suspects, investigate airplane bird strikes, and investigate bird collisions with wind turbines.\n\nForensic anthropology is for identification and recovery of remains. In extreme cases where conventional techniques are unable to determine the identity of the remains, anthropologists are sometimes able to deduce certain characteristics based on the skeletal remains. Race, sex, age and stature can often be determined by both measuring the remains and looking for structural clues in the bones.\n\nA Forensic botanist looks to plant life in order to gain information regarding possible crimes. Leaves, seeds and pollen found either on a body or at the scene of a crime can offer valuable information regarding the timescales of a crime and also if the body has been moved between two or more different locations. The forensic study of pollen is known as forensic palynology and can often produce specific findings of location of death, decomposition and time of year. The knowledge of systematics leads to identification of evidences at crime scene. The morphological and anatomical study revels in collection of samples from crime scene and its in vitro analysis. It leads to proper submission of evidences in court of law.\n\nBird remains can be identified, first and foremost from feathers (which are distinctive to a particular species at both macroscopic and microscopic levels).\n\nOdontologists or dentists can be used in order to aid in an identification of degraded remains. Remains that have been buried for a long period or which have undergone fire damage often contain few clues to the identity of the individual. Tooth enamel, as the hardest substance in the human body, often endures and as such odontologists can in some circumstances compare recovered remains to dental records.\n\nForensic toxicology is the use of toxicology and other disciplines such as analytical chemistry, pharmacology and clinical chemistry to aid medical or legal investigation of death, poisoning, and drug use. The primary concern for forensic toxicology is not the legal outcome of the toxicological investigation or the technology utilized, but rather the obtainment and interpretation of results. \n\nDNA-based evidence has become a significant tool that many law enforcement investigators now have at their disposal. DNA evidence can definitively link a suspect to either a crime scene or victim. Nuclear DNA evidence has been recovered from blood, semen, saliva, skin cells and hair. Furthermore, Mitochondrial DNA can be recovered from both bone and teeth dating back thousands of years. Laboratory analysis of DNA evidence generally involves the sample being amplified and quantified by a form of the Polymerase chain reaction known as Quantitative PCR or qPCR. (PCR) amplification of any sample recovered followed by sequencing via Capillary electrophoresis in order to obtain a DNA profile which can be compared to suspect DNA.\n\nDNA can also be extracted from animals and used to at least identify the species, for example bird or bat remains on an airplane or wind turbine.\n\nIn popular culture, forensic biology is frequently portrayed in shows like \"Law & Order\", \"Bones\", \"CSI\", \"Dexter\" and \"Castle\".\n\n\n", "id": "4985778", "title": "Forensic biology"}
{"url": "https://en.wikipedia.org/wiki?curid=48791", "text": "Pathology\n\nPathology (from the Greek roots of \"pathos\" (), meaning \"experience\" or \"suffering\" whence the English word \"path\" is derived by transliteration, and \"-logia\" (), \"study of\") is a significant component of the causal study of pathogens and a major field in modern medicine and diagnosis. Hence, 'the study of paths', by which disease comes. \n\nThe term pathology itself may be used broadly to refer to the study of disease in general, incorporating a wide range of bioscience research fields and medical practices (including plant pathology and veterinary pathology), or more narrowly to describe work within the contemporary medical field of \"general pathology,\" which includes a number of distinct but inter-related medical specialties that diagnose disease—mostly through analysis of tissue, cell, and body fluid samples. Used as a count noun, \"a pathology\" (plural, \"pathologies\") can also refer to the predicted or actual progression of particular diseases (as in the statement \"the many different forms of cancer have diverse pathologies\"), and the affix \"path\" is sometimes used to indicate a state of disease in cases of both physical ailment (as in cardiomyopathy) and psychological conditions (such as psychopathy). Similarly, a pathological condition is one caused by disease, rather than occurring physiologically. A physician practicing pathology is called a pathologist.\n\nAs a field of general inquiry and research, pathology addresses four components of disease: cause, mechanisms of development (pathogenesis), structural alterations of cells (morphologic changes), and the consequences of changes (clinical manifestations). In common medical practice, general pathology is mostly concerned with analyzing known clinical abnormalities that are markers or precursors for both infectious and non-infectious disease and is conducted by experts in one of two major specialties, anatomical pathology and clinical pathology. Further divisions in specialty exist on the basis of the involved sample types (comparing, for example, cytopathology, hematopathology, and histopathology), organs (as in renal pathology), and physiological systems (oral pathology), as well as on the basis of the focus of the examination (as with forensic pathology).\n\nThe sense of the word \"pathology\" as a synonym of \"disease\" or \"pathosis\" is very common in health care. The persistence of this usage despite attempted proscription is discussed elsewhere.\n\nThe study of pathology, including the detailed examination of the body, including dissection and inquiry into specific maladies, dates back to antiquity. Rudimentary understanding of many conditions was present in most early societies and is attested to in the records of the earliest historical societies, including those of the Middle East, India, and China. By the Hellenic period of ancient Greece, a concerted causal study of disease was underway (see Medicine in ancient Greece), with many notable early physicians (such as Hippocrates, for whom the modern Hippocratic Oath is named) having developed methods of diagnosis and prognosis for a number of diseases.The medical practices of the Romans and those of the Byzantines continued from these Greek roots, but, as with many areas of scientific inquiry, growth in understanding of medicine stagnated some after the Classical Era, but continued to slowly develop throughout numerous cultures. Notably, many advances were made in the medieval era of Islam (see Medicine in medieval Islam), during which numerous texts of complex pathologies were developed, also based on the Greek tradition. Even so, growth in complex understanding of disease mostly languished until knowledge and experimentation again began to proliferate in the Renaissance, Enlightenment, and Baroque eras, following the resurgence of the empirical method at new centers of scholarship. By the 17th century, the study of microscopy was underway and examination of tissues had led British Royal Society member Robert Hooke to coin the word \"cell\", setting the stage for later germ theory.\n\nModern medicine was particularly advanced by further developments of the microscope to analyze tissues, to which Rudolf Virchow gave a significant contribution, leading to a slew of research developments.\nBy the late 1920s to early 1930s pathology was deemed a medical specialty. Combined with developments in the understanding of general physiology, by the beginning of the 20th century, the study of pathology had begun to split into a number of rarefied fields and resulting in the development of large number of modern specialties within pathology and related disciplines of diagnostic medicine.\n\nThe modern practice of pathology is divided into a number of subdisciplines within the discrete but deeply interconnected aims of biological research and medical practice. Biomedical research into disease incorporates the work of a vast variety of life science specialists, whereas, in most parts of the world, to be licensed to practice pathology as medical specialty, one has to complete medical school and secure a license to practice medicine. Structurally, the study of disease is divided into many different fields that study or diagnose markers for disease using methods and technologies particular to specific scales, organs, and tissue types. The information in this section mostly concerns pathology as it regards common medical practice in these systems, but each of these specialties is also the subject of voluminous pathology research as regards the disease pathways of specific pathogens and disorders that affect the tissues of these discrete organs or structures. (See also Gross pathology).\n\nAnatomical pathology (\"Commonwealth\") or anatomic pathology (\"United States\") is a medical specialty that is concerned with the diagnosis of disease based on the gross, microscopic, chemical, immunologic and molecular examination of organs, tissues, and whole bodies (as in a general examination or an autopsy). Anatomical pathology is itself divided into subfields, the main divisions being surgical pathology, cytopathology, and forensic pathology. Anatomical pathology is one of two main divisions of the medical practice of pathology, the other being clinical pathology, the diagnosis of disease through the laboratory analysis of bodily fluids and tissues. Sometimes, pathologists practice both anatomical and clinical pathology, a combination known as general pathology.\n\nCytopathology (sometimes referred to as \"cytology\") is a branch of pathology that studies and diagnoses diseases on the cellular level. It is usually used to aid in the diagnosis of cancer, but also helps in the diagnosis of certain infectious diseases and other inflammatory conditions as well as thyroid lesions, diseases involving sterile body cavities (peritoneal, pleural, and cerebrospinal), and a wide range of other body sites. Cytopathology is generally used on samples of free cells or tissue fragments (in contrast to histopathology, which studies whole tissues) and cytopathologic tests are sometimes called smear tests because the samples may be smeared across a glass microscope slide for subsequent staining and microscopic examination. However, cytology samples may be prepared in other ways, including cytocentrifugation.\n\nDermatopathology is a subspecialty of anatomic pathology that focuses on the skin and the rest of the integumentary system as an organ. It is unique, in that there are two paths a physician can take to obtain the specialization. All general pathologists and general dermatologists train in the pathology of the skin, so the term dermatopathologist denotes either of these who has reached a certainly level of accreditation and experience; in the USA, either a general pathologist or a dermatologist can undergo a 1 to 2 year fellowship in the field of dermatopathology. The completion of this fellowship allows one to take a subspecialty board examination, and becomes a board certified dermatopathologist. Dermatologists are able to recognize most skin diseases based on their appearances, anatomic distributions, and behavior. Sometimes, however, those criteria do not lead to a conclusive diagnosis, and a skin biopsy is taken to be examined under the microscope using usual histological tests. In some cases, additional specialized testing needs to be performed on biopsies, including immunofluorescence, immunohistochemistry, electron microscopy, flow cytometry, and molecular-pathologic analysis. One of the greatest challenges of dermatopathology is its scope. More than 1500 different disorders of the skin exist, including cutaneous eruptions (\"rashes\") and neoplasms. Therefore, dermatopathologists must maintain a broad base of knowledge in clinical dermatology, and be familiar with several other specialty areas in Medicine.\n\nForensic pathology focuses on determining the cause of death by post-mortem examination of a corpse or partial remains. An autopsy is typically performed by a coroner or medical examiner, often during criminal investigations; in this role, coroners and medical examiners are also frequently asked to confirm the identity of a corpse. The requirements for becoming a licensed practitioner of forensic pathology varies from country to country (and even within a given nation) but typically a minimal requirement is a medical doctorate with a specialty in general or anatomical pathology with subsequent study in forensic medicine. The methods forensic scientists use to determine death include examination of tissue specimens to identify the presence or absence of natural disease and other microscopic findings, interpretations of toxicology on body tissues and fluids to determine the chemical cause of overdoses, poisonings or other cases involving toxic agents, and examinations of physical trauma. Forensic pathology is a major component in the trans-disciplinary field of forensic science.\n\nHistopathology refers to the microscopic examination of various forms of human tissue. Specifically, in clinical medicine, histopathology refers to the examination of a biopsy or surgical specimen by a pathologist, after the specimen has been processed and histological sections have been placed onto glass slides. This contrasts with the methods of cytopathology, which uses free cells or tissue fragments. Histopathological examination of tissues starts with surgery, biopsy, or autopsy. The tissue is removed from the body of an organism and then placed in a fixative that stabilizes the tissues to prevent decay. The most common fixative is formalin, although frozen section fixing is also common. To see the tissue under a microscope, the sections are stained with one or more pigments. The aim of staining is to reveal cellular components; counterstains are used to provide contrast. Histochemistry refers to the science of using chemical reactions between laboratory chemicals and components within tissue. The histological slides are then interpreted diagnostically and the resulting pathology report describes the histological findings and the opinion of the pathologist. In the case of cancer, this represents the tissue diagnosis required for most treatment protocols.\n\nNeuropathology is the study of disease of nervous system tissue, usually in the form of either surgical biopsies or sometimes whole brains in the case of autopsy. Neuropathology is a subspecialty of anatomic pathology, neurology, and neurosurgery. In many English-speaking countries, neuropathology is considered a subfield of anatomical pathology. A physician who specializes in neuropathology, usually by completing a fellowship after a residency in anatomical or general pathology, is called a neuropathologist. In day-to-day clinical practice, a neuropathologist is a consultant for other physicians. If a disease of the nervous system is suspected, and the diagnosis cannot be made by less invasive methods, a biopsy of nervous tissue is taken from the brain or spinal cord to aid in diagnosis. Biopsy is usually requested after a mass is detected by medical imaging. With autopsies, the principal work of the neuropathologist is to help in the post-mortem diagnosis of various conditions that affect the central nervous system. Biopsies can also consist of the skin. Epidermal nerve fiber density testing (ENFD) is a more recently developed neuropathology test in which a punch skin biopsy is taken to identify small fiber neuropathies by analyzing the nerve fibers of the skin. This test is becoming available in select labs as well as many universities; it replaces the traditional nerve biopsy test as less invasive.\n\nPulmonary pathology is a subspecialty of anatomic (and especially surgical) pathology that deals with diagnosis and characterization of neoplastic and non-neoplastic diseases of the lungs and thoracic pleura. Diagnostic specimens are often obtained via bronchoscopic transbronchial biopsy, CT-guided percutaneous biopsy, or video-assisted thoracic surgery. These tests can be necessary to diagnose between infection, inflammation, or fibrotic conditions.\nRenal pathology is a subspecialty of anatomic pathology that deals with the diagnosis and characterization of disease of the kidneys. In a medical setting, renal pathologists work closely with nephrologists and transplant surgeons, who typically obtain diagnostic specimens via percutaneous renal biopsy. The renal pathologist must synthesize findings from traditional microscope histology, electron microscopy, and immunofluorescence to obtain a definitive diagnosis. Medical renal diseases may affect the glomerulus, the tubules and interstitium, the vessels, or a combination of these compartments.\n\nSurgical pathology is one of the primary areas of practice for most anatomical pathologists. Surgical pathology involves the gross and microscopic examination of surgical specimens, as well as biopsies submitted by surgeons and non-surgeons such as general internists, medical subspecialists, dermatologists, and interventional radiologists. Often an excised tissue sample is the best and most definitive evidence of disease (or lack thereof) in cases where tissue is surgically removed from a patient. These determinations are usually accomplished by a combination of gross (i.e., macroscopic) and histologic (i.e., microscopic) examination of the tissue, and may involve evaluations of molecular properties of the tissue by immunohistochemistry or other laboratory tests.\nThere are two major types of specimens submitted for surgical pathology analysis: biopsies and surgical resections. A biopsy is a small piece of tissue removed primarily for surgical pathology analysis, most often in order to render a definitive diagnosis. Types of biopsies include core biopsies, which are obtained through the use of large-bore needles, sometimes under the guidance of radiological techniques such as ultrasound, CT scan, or magnetic resonance imaging. Incisional biopsies are obtained through diagnostic surgical procedures that remove part of a suspicious lesion, whereas excisional biopsies remove the entire lesion, and are similar to therapeutic surgical resections. Excisional biopsies of skin lesions and gastrointestinal polyps are very common. The pathologist's interpretation of a biopsy is critical to establishing the diagnosis of a benign or malignant tumor, and can differentiate between different types and grades of cancer, as well as determining the activity of specific molecular pathways in the tumor. Surgical resection specimens are obtained by the therapeutic surgical removal of an entire diseased area or organ (and occasionally multiple organs). These procedures are often intended as definitive surgical treatment of a disease in which the diagnosis is already known or strongly suspected, but pathological analysis of these specimens remains important in confirming the previous diagnosis.\n\nClinical pathology is a medical specialty that is concerned with the diagnosis of disease based on the laboratory analysis of bodily fluids such as blood and urine, as well as tissues, using the tools of chemistry, clinical microbiology, hematology and molecular pathology. Clinical pathologists work in close collaboration with medical technologists, hospital administrations, and referring physicians. Clinical pathologists learn to administer a number of visual and microscopic tests and an especially large variety of tests of the biophysical properties of tissue samples involving Automated analysers and cultures. Sometimes the general term \"laboratory medicine specialist\" is used to refer to those working in clinical pathology, including medical doctors, Ph.D.s and doctors of pharmacology. Immunopathology, the study of an organism's immune response to infection, is sometimes considered to fall within the domain of clinical pathology.\n\nHematopathology is the study of diseases of blood cells (including constituents such as white blood cells, red blood cells, and platelets) and the tissues, and organs comprising \nthe hematopoietic system. The term hematopoietic system refers to tissues and organs that produce and/or primarily host hematopoietic cells and includes bone marrow, the lymph nodes, thymus, spleen, and other lymphoid tissues. In the United States, hematopathology is a board certified subspecialty (licensed under the American Board of Pathology) practiced by those physicians who have completed a general pathology residency (anatomic, clinical, or combined) and an additional year of fellowship training in hematology. The hematopathologist reviews biopsies of lymph nodes, bone marrows and other tissues involved by an infiltrate of cells of the hematopoietic system. In addition, the hematopathologist may be in charge of flow cytometric and/or molecular hematopathology studies.\n\nImmunopathology is a branch of clinical pathology that deals with an organism’s immune response to a certain disease. When a foreign antigen enters the body, there is either an antigen specific or nonspecific response to it. These responses are the immune system fighting off the foreign antigens, whether they are deadly or not. Immunopathology could refer to how the foreign antigens cause the immune system to have a response or problems that can arise from an organism’s own immune response on itself. There are certain problems or faults in the immune system that can lead to more serious illness or disease. These diseases can come from one of the following problems. The first would be Hypersensitivity reactions, where there would be a stronger immune response than normal. There are four different types (type one, two, three and four), all with varying types and degrees of an immune response. The problems that arise from each type vary from small allergic reactions to more serious illnesses such as tuberculosis or arthritis. The second kind of complication in the immune system is Autoimmunity, where the immune system would attack itself rather than the antigen. Inflammation is a prime example of autoimmunity, as the immune cells used are self-reactive. A few examples of autoimmune diseases are Type 1 diabetes, Addison’s disease and Celiac disease. The third and final type of complication with the immune system is Immunodeficiency, where the immune system lacks the ability to fight off a certain disease. The immune system’s ability to combat it is either hindered or completely absent. The two types are Primary Immunodeficiency, where the immune system is either missing a key component or does not function properly, and Secondary Immunodeficiency, where disease is obtained from an outside source, like radiation or heat, and therefore cannot function properly. Diseases that can cause immunodeficiency include HIV, AIDS and leukemia. In all vertebrates, there are two different kinds of immune responses: Innate and Adaptive immunity. Innate immunity is used to fight off non-changing antigens and is therefore considered nonspecific. It is usually a more immediate response than the adaptive immune system, usually responding within minutes to hours. It is composed of physical blockades such as the skin, but also contains nonspecific immune cells such as dendritic cells, macrophages, T Cells, and basophils. The second for of immunity is Adaptive immunity. This form of immunity requires recognition of the foreign antigen before a response is produced. Once the antigen is recognized, a specific response is produced in order to destroy the specific antigen. Because of this idea, adaptive immunity is considered to be specific immunity. A key part of adaptive immunity that separates it from innate is the use of memory to combat the antigen in the future. When the antigen is originally introduced, the organism does not have any receptors for the antigen so it must generate them from the first time the antigen is present. The immune system then builds a memory of that antigen, which enables it to recognize the antigen quicker in the future and be able to combat it quicker and more efficiently. The more the system is exposed to the antigen, the quicker it will build up its responsiveness.\n\nRadiation Pathology is study of the interaction between human tissues and radiation, as long as the problems and diseases that can arise from the use of radiation. When human tissue is exposed to radiation, it can be genetically altered and deformed; in turn, this could lead to a variety of illnesses that could be minor or deadly.\n\nMolecular pathology is focused upon the study and diagnosis of disease through the examination of molecules within organs, tissues or bodily fluids. Molecular pathology is multidisciplinary by nature and shares some aspects of practice with both anatomic pathology and clinical pathology, molecular biology, biochemistry, proteomics and genetics. It is often applied in a context that is as much scientific as directly medical and encompasses the development of molecular and genetic approaches to the diagnosis and classification of human diseases, the design and validation of predictive biomarkers for treatment response and disease progression, and the susceptibility of individuals of different genetic constitution to particular disorders. The crossover between molecular pathology and epidemiology is represented by a related field \"molecular pathological epidemiology\".\n\nOral and Maxillofacial Pathology is one of nine dental specialties recognized by the American Dental Association, and is sometimes considered a specialty of both dentistry and pathology. Oral Pathologists must complete three years of post doctoral training in an accredited program and subsequently obtain diplomate status from the American Board of Oral and Maxillofacial Pathology. The specialty focuses on the diagnosis, clinical management and investigation of diseases that affect the oral cavity and surrounding maxillofacial structures including but not limited to odontogenic, infectious, epithelial, salivary gland, bone and soft tissue pathologies. It also significantly intersects with the field of dental pathology. Although concerned with a broad variety of diseases of the oral cavity, they have roles distinct from otorhinolaryngologists (\"ear, nose, and throat\" specialists), and speech pathologists, the latter of which helps diagnose many neurological or neuromuscular conditions relevant to speech phonology or swallowing. Owing to the availability of the oral cavity to non-invasive examination, many conditions in the study of oral disease can be diagnosed, or at least suspected, from gross examination, but biopsies, cell smears, and other tissue analysis remain important diagnostic tools in oral pathology.\n\nIndividual nations vary some in the medical licensing required of pathologists. In the United States, pathologists are physicians (D.O. or M.D.) that have completed a four-year undergraduate program, four years of medical school training, and three to four years of postgraduate training in the form of a pathology residency. Training may be within two primary specialties, as recognized by the American Board of Pathology: anatomical Pathology and clinical Pathology, each of which requires separate board certification. The American Osteopathic Board of Pathology also recognizes four primary specialties: anatomic pathology, dermatopathology, forensic pathology, and laboratory medicine. Pathologists may pursue specialised fellowship training within one or more subspecialties of either anatomical or clinical pathology. Some of these subspecialties permit additional board certification, while others do not.\nIn the United Kingdom, pathologists are physicians licensed by the UK General Medical Council. The training to become a pathologist is under the oversight of the Royal College of Pathologists. After four to six years of undergraduate medical study, trainees proceed to a two-year foundation program. Full-time training in histopathology currently lasts between five and five and a half years and includes specialist training in surgical pathology, cytopathology, and autopsy pathology. It is also possible to take a Royal College of Pathologists diploma in forensic pathology, dermatopathology, or cytopathology, recognising additional specialist training and expertise and to get specialist accreditation in forensic pathology, pediatric pathology, and neuropathology. All postgraduate medical training and education in the UK is overseen by the General Medical Council.\n\nIn France, Pathology is separate in two distinct specialties, anatomical pathology and clinical pathology. Residencies for both lasts four years. Residency in anatomical pathology is open to physicians only, while clinical pathology is open to both physicians and pharmacists. At the end of the second year of clinical pathology residency, residents can choose between general clinical pathology and a specialization in one of the disciplines, but they can not practice anatomical pathology, nor can anatomical pathology residents practice clinical pathology.\n\nThough separate fields in terms of medical practice, a number of areas of inquiry in medicine and\nmedical science either overlap greatly with general pathology, work in tandem with it, or contribute significantly to the understanding of the pathology of a given disease or its course in an individual. As a significant portion of all general pathology practice is concerned with cancer, the practice of oncology is deeply tied to, and dependent upon, the work of both anatomical and clinical pathologists. Biopsy, resection and blood tests are all examples of pathology work that is essential for the diagnoses of many kinds of cancer and for the staging of cancerous masses. In a similar fashion, the tissue and blood analysis techniques of general pathology are of central significance to the investigation of serious infectious disease and as such inform significantly upon the fields of epidemiology, etiology, immunology, and parasitology. General pathology methods are of great importance to biomedical research into disease, wherein they are sometimes referred to as \"experimental\" or \"investigative\" pathology.\n\nMedical imaging is the generating of visual representations of the interior of a body for clinical analysis and medical intervention. Medical imaging reveals details of internal physiology that help medical professionals plan appropriate treatments for tissue infection and trauma. Medical imaging is also central in supplying the biometric data necessary to establish baseline features of anatomy and physiology so as to increase the accuracy with which early or fine-detail abnormalities are detected. These diagnostic techniques are often performed in combination with general pathology procedures and are themselves often essential to developing new understanding of the pathogenesis of a given disease and tracking the progress of disease in specific medical cases. Examples of important subdivisions in medical imaging include radiology (which uses the imaging technologies of X-ray radiography) magnetic resonance imaging, medical ultrasonography (or ultrasound), endoscopy, elastography, tactile imaging, thermography, medical photography, nuclear medicine and functional imaging techniques such as positron emission tomography. Though they do not strictly relay images, readings from diagnostics tests involving electroencephalography, magnetoencephalography, and electrocardiography often give hints as to the state and function of certain tissues in the brain and heart respectively.\n\nPsychopathology is the study of mental illness, particularly of severe disorders. Informed heavily by both psychology and neurology, its purpose is to classify mental illness, elucidate its underlying causes, and guide clinical psychiatric treatment accordingly. Although diagnosis and classification of mental norms and disorders is largely the purview of psychiatry—the results of which are guidelines such as the Diagnostic and Statistical Manual of Mental Disorders, which attempt to classify mental disease mostly on behavioural evidence, though not without controversy—the field is also heavily, and increasingly, informed upon by neuroscience and other of the biological cognitive sciences. Mental or social disorders or behaviours seen as generally unhealthy or excessive in a given individual, to the point where they cause harm or severe disruption to the sufferer's lifestyle, are often called \"pathological\" (e.g., pathological gambling or pathological liar).\n\nAlthough the vast majority of lab work and research in pathology concerns the development of disease in humans, pathology is of significance throughout the biological sciences. Two main catch-all fields exist to represent most complex organisms capable of serving as host to a pathogen or other form of disease: veterinary pathology (concerned with all non-human species of kingdom of Animalia) and phytopathology, which studies disease in plants.\n\nVeterinary pathology covers a vast array of species, but with a significantly smaller number of practitioners, so understanding of disease in non-human animals, especially as regards veterinary practice, varies considerably by species. Nonetheless, significant amounts of pathology research are conducted on animals, for two primary reasons: 1) The origins of diseases are typically zoonotic in nature, and many infectious pathogens have animal vectors and, as such, understanding the mechanisms of action for these pathogens in non-human hosts is essential to the understanding and application of epidemiology and 2) those animals that share physiological and genetic traits with humans can be used as surrogates for the study of the disease and potential treatments as well as the effects of various synthetic products. For this reason, as well as their roles as livestock and companion animals, mammals generally have the largest body of research in veterinary pathology. Animal testing remains a controversial practice, even in cases where it is used to research treatment for human disease. As in human medical pathology, the practice of veterinary pathology is customarily divided into the two main fields of anatomical and clinical pathology.\n\nAlthough the pathogens and their mechanics differ greatly from those of animals, plants are subject to a wide variety of diseases, including those caused by fungi, oomycetes, bacteria, viruses, viroids, virus-like organisms, phytoplasmas, protozoa, nematodes and parasitic plants. Damage caused by insects, mites, vertebrate, and other small herbivores is not considered a part of the domain of plant pathology. The field is deeply connected to plant disease epidemiology and the horticulture of species that are of high importance to the human diet or other uses.\n\n\n\n", "id": "48791", "title": "Pathology"}
{"url": "https://en.wikipedia.org/wiki?curid=29462590", "text": "Mechanobiology\n\nMechanobiology is an emerging field of science at the interface of biology and engineering that focuses on how physical forces and changes in the mechanical properties of cells and tissues contribute to development, cell differentiation, physiology, and disease. A major challenge in the field is understanding mechanotransduction—the molecular mechanisms by which cells sense and respond to mechanical signals.\nWhile medicine has typically looked for the genetic and biochemical basis of disease, advances in mechanobiology suggest that changes in cell mechanics, extracellular matrix structure, or mechanotransduction may contribute to the development of many diseases, including atherosclerosis, fibrosis, asthma, osteoporosis, heart failure, and cancer. There is also a strong mechanical basis for many generalized medical disabilities, such as lower back pain, foot and postural injury, deformity, and irritable bowel syndrome.\n\nThe effectiveness of many of the mechanical therapies already in clinical use shows how important physical forces can be in physiological control. For example, pulmonary surfactant promotes lung development in premature infants; modifying the tidal volumes of mechanical ventilators reduces morbidity and death in patients with acute lung injury; expandable stents physically prevent coronary artery constriction; tissue expanders increase the skin area available for reconstructive surgery; and surgical tension application devices are used for bone fracture healing, orthodontics, cosmetic breast expansion and closure of non-healing wounds. \n\nInsights into the mechanical basis of tissue regulation may also lead to development of improved medical devices, biomaterials, and engineered tissues for tissue repair and reconstruction.\n\nStretch-activated ion channels, caveolae, integrins, cadherins, growth factor receptors, myosin motors, cytoskeletal filaments, nuclei, extracellular matrix, and numerous other molecular structures and signaling molecules have been shown to contribute to cellular mechanotransduction. In addition, endogenous cell-generated traction forces contribute significantly to these responses by modulating tensional prestress within cells, tissues, and organs that govern their mechanical stability, as well as mechanical signal transmission from the macroscale to the nanoscale.\n\nIt is claimed all cells are mechanosensitive. Collectively cells respond to perturbations to their local mechanical environment resulting in tissue-level observations. For example, at the tissue-level an artery will either thicken or thin in response to changes in blood pressure above or below the healthy levels. That is, in the case of increased blood pressure (Hypertension) individual arterial cells experience greater circumferential stress (or tension). In order to alleviate this tension, they produce growth factors, which in turn stimulates proliferation. The net result is increased arterial wall thickness but the stress levels in the artery are restored to the normal levels.\n\nUsing mankind as a macroscopic example, in closed chain function, ground reactive forces, a dynamic architecture and a dynamic equilibrium of forces around joint axes impact the posture to produce tissue stress. This tissue stress can be both beneficial or harmful. Since gravity, hard, unyielding ground surfaces and other factors such as activity level, body weight and health state impact each of us differently there is no one plan of care that will work for every individual. This results in a lifetime of adaptation of tissues via Wolff's and Davis' Laws of Bone and Soft Tissue respectively that can unless compensated and/or corrected lead to breakdown, injury and reduced quality of life on a case to case basis.\n\nAs a further example, the foot has an inherited functional shape which when used will remodel and adapt in predictable manners. Theoretically programs can be establishing for prevention, performance enhancement and quality of life upgrading in addition to the treatment of pathology and pain. These interventions, some day, will cause positive remodeling of bone and soft tissue that will extend and possibly improve the mechanobiological timeline of mankind.\n\n\n\n", "id": "29462590", "title": "Mechanobiology"}
{"url": "https://en.wikipedia.org/wiki?curid=34413", "text": "Zoology\n\nZoology () or animal biology is the branch of biology that studies the animal kingdom, including the structure, embryology, evolution, classification, habits, and distribution of all animals, both living and extinct, and how they interact with their ecosystems. The term is derived from Ancient Greek ζῷον, \"zōion\", i.e. \"animal\" and λόγος, \"logos\", i.e. \"knowledge, study\".\n\nThe history of zoology traces the study of the animal kingdom from ancient to modern times. Although the concept of \"zoology\" as a single coherent field arose much later, the zoological sciences emerged from natural history reaching back to the works of Aristotle and Galen in the ancient Greco-Roman world. This ancient work was further developed in the Middle Ages by Muslim physicians and scholars such as Albertus Magnus. During the Renaissance and early modern period, zoological thought was revolutionized in Europe by a renewed interest in empiricism and the discovery of many novel organisms. Prominent in this movement were Vesalius and William Harvey, who used experimentation and careful observation in physiology, and naturalists such as Carl Linnaeus and Buffon who began to classify the diversity of life and the fossil record, as well as the development and behavior of organisms. Microscopy revealed the previously unknown world of microorganisms, laying the groundwork for cell theory. The growing importance of natural theology, partly a response to the rise of mechanical philosophy, encouraged the growth of natural history (although it entrenched the argument from design).\n\nOver the 18th and 19th centuries, zoology became an increasingly professional scientific discipline. Explorer-naturalists such as Alexander von Humboldt investigated the interaction between organisms and their environment, and the ways this relationship depends on geography, laying the foundations for biogeography, ecology and ethology. Naturalists began to reject essentialism and consider the importance of extinction and the mutability of species. Cell theory provided a new perspective on the fundamental basis of life.\n\nThese developments, as well as the results from embryology and paleontology, were synthesized in Charles Darwin's theory of evolution by natural selection. In 1859, Darwin placed the theory of organic evolution on a new footing, by his discovery of a process by which organic evolution can occur, and provided observational evidence that it had done so.\n\nDarwin gave a new direction to morphology and physiology, by uniting them in a common biological theory: the theory of organic evolution. The result was a reconstruction of the classification of animals upon a genealogical basis, fresh investigation of the development of animals, and early attempts to determine their genetic relationships. The end of the 19th century saw the fall of spontaneous generation and the rise of the germ theory of disease, though the mechanism of inheritance remained a mystery. In the early 20th century, the rediscovery of Mendel's work led to the rapid development of genetics, and by the 1930s the combination of population genetics and natural selection in the modern synthesis created evolutionary biology.\n\nCell biology studies the structural and physiological properties of cells, including their behavior, interactions, and environment. This is done on both the microscopic and molecular levels, for single-celled organisms such as bacteria as well as the specialized cells in multicellular organisms such as humans. Understanding the structure and function of cells is fundamental to all of the biological sciences. The similarities and differences between cell types are particularly relevant to molecular biology.\n\nAnatomy considers the forms of macroscopic structures such as organs and organ systems. It focuses on how organs and organ systems work together in the bodies of humans and animals, in addition to how they work independently. Anatomy and cell biology are two studies that are closely related, and can be categorized under \"structural\" studies.\n\nPhysiology studies the mechanical, physical, and biochemical processes of living organisms by attempting to understand how all of the structures function as a whole. The theme of \"structure to function\" is central to biology. Physiological studies have traditionally been divided into plant physiology and animal physiology, but some principles of physiology are universal, no matter what particular organism is being studied. For example, what is learned about the physiology of yeast cells can also apply to human cells. The field of animal physiology extends the tools and methods of human physiology to non-human species. Physiology studies how for example nervous, immune, endocrine, respiratory, and circulatory systems, function and interact.\n\nEvolutionary research is concerned with the origin and descent of species, as well as their change over time, and includes scientists from many taxonomically oriented disciplines. For example, it generally involves scientists who have special training in particular organisms such as mammalogy, ornithology, herpetology, or entomology, but use those organisms as systems to answer general questions about evolution.\n\nEvolutionary biology is partly based on paleontology, which uses the fossil record to answer questions about the mode and tempo of evolution, and partly on the developments in areas such as population genetics and evolutionary theory. Following the development of DNA fingerprinting techniques in the late 20th century, the application of these techniques in zoology has increased the understanding of animal populations. In the 1980s, developmental biology re-entered evolutionary biology from its initial exclusion from the modern synthesis through the study of evolutionary developmental biology. Related fields often considered part of evolutionary biology are phylogenetics, systematics, and taxonomy.\n\nScientific classification in zoology, is a method by which zoologists group and categorize organisms by biological type, such as genus or species. Biological classification is a form of scientific taxonomy. Modern biological classification has its root in the work of Carl Linnaeus, who grouped species according to shared physical characteristics. These groupings have since been revised to improve consistency with the Darwinian principle of common descent. Molecular phylogenetics, which uses DNA sequences as data, has driven many recent revisions and is likely to continue to do so. Biological classification belongs to the science of zoological systematics.\nMany scientists now consider the five-kingdom system outdated. Modern alternative classification systems generally start with the three-domain system: Archaea (originally Archaebacteria); Bacteria (originally Eubacteria); Eukaryota (including protists, fungi, plants, and animals) These domains reflect whether the cells have nuclei or not, as well as differences in the chemical composition of the cell exteriors.\n\nFurther, each kingdom is broken down recursively until each species is separately classified. The order is:\nDomain; kingdom; phylum; class; order; family; genus; species. The scientific name of an organism is generated from its genus and species. For example, humans are listed as \"Homo sapiens\". \"Homo\" is the genus, and \"sapiens\" the specific epithet, both of them combined make up the species name. When writing the scientific name of an organism, it is proper to capitalize the first letter in the genus and put all of the specific epithet in lowercase. Additionally, the entire term may be italicized or underlined.\n\nThe dominant classification system is called the Linnaean taxonomy. It includes ranks and binomial nomenclature. The classification, taxonomy, and nomenclature of zoological organisms is administered by the International Code of Zoological Nomenclature. A merging draft, BioCode, was published in 1997 in an attempt to standardize nomenclature, but has yet to be formally adopted.\n\nEthology is the scientific and objective study of animal behavior under natural conditions, as opposed to behaviourism, which focuses on behavioral response studies in a laboratory setting. Ethologists have been particularly concerned with the evolution of behavior and the understanding of behavior in terms of the theory of natural selection. In one sense, the first modern ethologist was Charles Darwin, whose book, \"The Expression of the Emotions in Man and Animals,\" influenced many future ethologists.\n\nBiogeography studies the spatial distribution of organisms on the Earth, focusing on topics like plate tectonics, climate change, dispersal and migration, and cladistics. The creation of this study is widely accredited to Alfred Russel Wallace, a British biologist who had some of his work jointly published with Charles Darwin.\n\nAlthough the study of animal life is ancient, its scientific incarnation is relatively modern. This mirrors the transition from natural history to biology at the start of the 19th century. Since Hunter and Cuvier, comparative anatomical study has been associated with morphography, shaping the modern areas of zoological investigation: anatomy, physiology, histology, embryology, teratology and ethology. Modern zoology first arose in German and British universities. In Britain, Thomas Henry Huxley was a prominent figure. His ideas were centered on the morphology of animals. Many consider him the greatest comparative anatomist of the latter half of the 19th century. Similar to Hunter, his courses were composed of lectures and laboratory practical classes in contrast to the previous format of lectures only.\n\nGradually zoology expanded beyond Huxley's comparative anatomy to include the following sub-disciplines:\n\nRelated fields:\n\n\n", "id": "34413", "title": "Zoology"}
{"url": "https://en.wikipedia.org/wiki?curid=2787", "text": "Astrobiology\n\nAstrobiology is the study of the origin, evolution, distribution, and future of life in the universe: extraterrestrial life and life on Earth. Astrobiology addresses the question of whether life exists beyond Earth, and how humans can detect it if it does. The term exobiology is similar but more specific—it covers the search for life beyond Earth, and the effects of extraterrestrial environments on living things.\n\nAstrobiology makes use of physics, chemistry, astronomy, biology, molecular biology, ecology, planetary science, geography, and geology to investigate the possibility of life on other worlds and help recognize biospheres that might be different from that on Earth. The origin and early evolution of life is an inseparable part of the discipline of astrobiology. Astrobiology concerns itself with interpretation of existing scientific data, and although speculation is entertained to give context, astrobiology concerns itself primarily with hypotheses that fit firmly into existing scientific theories. Given more detailed and reliable data from other parts of the universe, the roots of astrobiology itself—physics, chemistry and biology—may have their theoretical bases challenged.\n\nThis interdisciplinary field encompasses research on the origin and evolution of planetary systems, origins of organic compounds in space, rock-water-carbon interactions, abiogenesis on Earth, planetary habitability, research on biosignatures for life detection, and studies on the potential for life to adapt to challenges on Earth and in outer space.\n\nThe chemistry of life may have begun shortly after the Big Bang, 13.8 billion years ago, during a habitable epoch when the Universe was only 10–17 million years old. According to the panspermia hypothesis, microscopic life—distributed by meteoroids, asteroids and other small Solar System bodies—may exist throughout the universe. According to research published in August 2015, very large galaxies may be more favorable to the creation and development of habitable planets than such smaller galaxies as the Milky Way. Nonetheless, Earth is the only place in the universe humans know to harbor life. Estimates of habitable zones around other stars, sometimes referred to as \"Goldilocks zones,\" along with the discovery of hundreds of extrasolar planets and new insights into extreme habitats here on Earth, suggest that there may be many more habitable places in the universe than considered possible until very recently.\n\nCurrent studies on the planet Mars by the \"Curiosity\" and \"Opportunity\" rovers are searching for evidence of ancient life as well as plains related to ancient rivers or lakes that may have been habitable. The search for evidence of habitability, taphonomy (related to fossils), and organic molecules on the planet Mars is now a primary NASA and ESA objective.\n\nThe term was first proposed by the Russian (Soviet) astronomer Gavriil Tikhov in 1953. \"Astrobiology\" is etymologically derived from the Greek , \"astron\", \"constellation, star\"; , \"bios\", \"life\"; and , \"-logia\", \"study\". The synonyms of astrobiology are diverse; however, the synonyms were structured in relation to the most important sciences implied in its development: astronomy and biology. A close synonym is \"exobiology\" from the Greek , \"external\"; Βίος, \"bios\", \"life\"; and λογία, -logia, \"study\". The term exobiology was coined by molecular biologist Joshua Lederberg. Exobiology is considered to have a narrow scope limited to search of life external to Earth, whereas subject area of astrobiology is wider and investigates the link between life and the universe, which includes the search for extraterrestrial life, but also includes the study of life on Earth, its origin, evolution and limits. \n\nAnother term used in the past is xenobiology, (\"biology of the foreigners\") a word used in 1954 by science fiction writer Robert Heinlein in his work The Star Beast. The term xenobiology is now used in a more specialized sense, to mean \"biology based on foreign chemistry\", whether of extraterrestrial or terrestrial (possibly synthetic) origin. Since alternate chemistry analogs to some life-processes have been created in the laboratory, xenobiology is now considered as an extant subject.\n\nWhile it is an emerging and developing field, the question of whether life exists elsewhere in the universe is a verifiable hypothesis and thus a valid line of scientific inquiry. Though once considered outside the mainstream of scientific inquiry, astrobiology has become a formalized field of study. Planetary scientist David Grinspoon calls astrobiology a field of natural philosophy, grounding speculation on the unknown, in known scientific theory. NASA's interest in exobiology first began with the development of the U.S. Space Program. In 1959, NASA funded its first exobiology project, and in 1960, NASA founded an Exobiology Program, which is now one of four main elements of NASA's current Astrobiology Program. In 1971, NASA funded the search for extraterrestrial intelligence (SETI) to search radio frequencies of the electromagnetic spectrum for interstellar communications transmitted by extraterrestrial life outside the Solar System. NASA's Viking missions to Mars, launched in 1976, included three biology experiments designed to look for metabolism of present life on Mars.\n\nAdvancements in the fields of astrobiology, observational astronomy and discovery of large varieties of extremophiles with extraordinary capability to thrive in the harshest environments on Earth, have led to speculation that life may possibly be thriving on many of the extraterrestrial bodies in the universe. A particular focus of current astrobiology research is the search for life on Mars due to this planet's proximity to Earth and geological history. There is a growing body of evidence to suggest that Mars has previously had a considerable amount of water on its surface, water being considered an essential precursor to the development of carbon-based life.\n\nMissions specifically designed to search for current life on Mars were the Viking program and Beagle 2 probes. The Viking results were inconclusive, and Beagle 2 failed minutes after landing. A future mission with a strong astrobiology role would have been the Jupiter Icy Moons Orbiter, designed to study the frozen moons of Jupiter—some of which may have liquid water—had it not been cancelled. In late 2008, the Phoenix lander probed the environment for past and present planetary habitability of microbial life on Mars, and researched the history of water there.\n\nIn November 2011, NASA launched the Mars Science Laboratory mission carrying the \"Curiosity\" rover, which landed on Mars at Gale Crater in August 2012. The \"Curiosity\" rover is currently probing the environment for past and present planetary habitability of microbial life on Mars. On 9 December 2013, NASA reported that, based on evidence from \"Curiosity\" studying Aeolis Palus, Gale Crater contained an ancient freshwater lake which could have been a hospitable environment for microbial life.\n\nThe European Space Agency is currently collaborating with the Russian Federal Space Agency (Roscosmos) and developing the ExoMars astrobiology rover, which is to be launched in 2018. Meanwhile, NASA is developing the Mars 2020 astrobiology rover and sample cacher for a later return to Earth.\n\nWhen looking for life on other planets like Earth, some simplifying assumptions are useful to reduce the size of the task of the astrobiologist. One is the informed assumption that the vast majority of life forms in our galaxy are based on carbon chemistries, as are all life forms on Earth. Carbon is well known for the unusually wide variety of molecules that can be formed around it. Carbon is the fourth most abundant element in the universe and the energy required to make or break a bond is at just the appropriate level for building molecules which are not only stable, but also reactive. The fact that carbon atoms bond readily to other carbon atoms allows for the building of extremely long and complex molecules.\n\nThe presence of liquid water is an assumed requirement, as it is a common molecule and provides an excellent environment for the formation of complicated carbon-based molecules that could eventually lead to the emergence of life. Some researchers posit environments of water-ammonia mixtures as possible solvents for hypothetical types of biochemistry.\n\nA third assumption is to focus on planets orbiting Sun-like stars for increased probabilities of planetary habitability. Very large stars have relatively short lifetimes, meaning that life might not have time to emerge on planets orbiting them. Very small stars provide so little heat and warmth that only planets in very close orbits around them would not be frozen solid, and in such close orbits these planets would be tidally \"locked\" to the star. The long lifetimes of red dwarfs could allow the development of habitable environments on planets with thick atmospheres. This is significant, as red dwarfs are extremely common. (See Habitability of red dwarf systems).\n\nSince Earth is the only planet known to harbor life, there is no evident way to know if any of these simplifying assumptions are correct.\n\nResearch on communication with extraterrestrial intelligence (CETI) focuses on composing and deciphering messages that could theoretically be understood by another technological civilization. Communication attempts by humans have included broadcasting mathematical languages, pictorial systems such as the Arecibo message and computational approaches to detecting and deciphering 'natural' language communication. The SETI program, for example, uses both radio telescopes and optical telescopes to search for deliberate signals from an extraterrestrial intelligence.\n\nWhile some high-profile scientists, such as Carl Sagan, have advocated the transmission of messages, scientist Stephen Hawking has warned against it, suggesting that aliens might simply raid Earth for its resources and then move on.\n\nMost astronomy-related astrobiology research falls into the category of extrasolar planet (exoplanet) detection, the hypothesis being that if life arose on Earth, then it could also arise on other planets with similar characteristics. To that end, a number of instruments designed to detect Earth-sized exoplanets have been considered, most notably NASA's Terrestrial Planet Finder (TPF) and ESA's Darwin programs, both of which have been cancelled. NASA launched the \"Kepler\" mission in March 2009, and the French Space Agency launched the COROT space mission in 2006. There are also several less ambitious ground-based efforts underway.\n\nThe goal of these missions is not only to detect Earth-sized planets, but also to directly detect light from the planet so that it may be studied spectroscopically. By examining planetary spectra, it would be possible to determine the basic composition of an extrasolar planet's atmosphere and/or surface. Given this knowledge, it may be possible to assess the likelihood of life being found on that planet. A NASA research group, the Virtual Planet Laboratory, is using computer modeling to generate a wide variety of virtual planets to see what they would look like if viewed by TPF or Darwin. It is hoped that once these missions come online, their spectra can be cross-checked with these virtual planetary spectra for features that might indicate the presence of life.\n\nAn estimate for the number of planets with intelligent \"communicative\" extraterrestrial life can be gleaned from the Drake equation, essentially an equation expressing the probability of intelligent life as the product of factors such as the fraction of planets that might be habitable and the fraction of planets on which life might arise:\nwhere:\n\nHowever, whilst the rationale behind the equation is sound, it is unlikely that the equation will be constrained to reasonable limits of error any time soon. The problem with the formula is that it is not usable to generate or support hypotheses because it contains factors that can never be verified. The first term, R*, number of stars, is generally constrained within a few orders of magnitude. The second and third terms, \"f\", stars with planets and \"f\", planets with habitable conditions, are being evaluated for the star's neighborhood. Drake originally formulated the equation merely as an agenda for discussion at the Green Bank conference, but some applications of the formula had been taken literally and related to simplistic or pseudoscientific arguments. Another associated topic is the Fermi paradox, which suggests that if intelligent life is common in the universe, then there should be obvious signs of it.\n\nAnother active research area in astrobiology is planetary system formation. It has been suggested that the peculiarities of the Solar System (for example, the presence of Jupiter as a protective shield) may have greatly increased the probability of intelligent life arising on our planet.\n\nBiology cannot state that a process or phenomenon, by being mathematically possible, has to exist forcibly in an extraterrestrial body. Biologists specify what is speculative and what is not.\n\nUntil the 1970s, life was thought to be entirely dependent on energy from the Sun. Plants on Earth's surface capture energy from sunlight to photosynthesize sugars from carbon dioxide and water, releasing oxygen in the process that is then consumed by oxygen-respiring organisms, passing their energy up the food chain. Even life in the ocean depths, where sunlight cannot reach, was thought to obtain its nourishment either from consuming organic detritus rained down from the surface waters or from eating animals that did. The world's ability to support life was thought to depend on its access to sunlight. However, in 1977, during an exploratory dive to the Galapagos Rift in the deep-sea exploration submersible \"Alvin\", scientists discovered colonies of giant tube worms, clams, crustaceans, mussels, and other assorted creatures clustered around undersea volcanic features known as black smokers. These creatures thrive despite having no access to sunlight, and it was soon discovered that they comprise an entirely independent ecosystem. Although most of these multicellular lifeforms need dissolved oxygen (produced by oxygenic photosynthesis) for their aerobic cellular respiration and thus are not completely independent from sunlight by themselves, the basis for their food chain is a form of bacterium that derives its energy from oxidization of reactive chemicals, such as hydrogen or hydrogen sulfide, that bubble up from the Earth's interior. Other lifeforms entirely decoupled from the energy from sunlight are green sulphur bacteria which are capturing geothermal light for anoxygenic photosynthesis or bacteria running chemolithoautotrophy based on the radioactive decay of uranium. This chemosynthesis revolutionized the study of biology and astrobiology by revealing that life need not be sun-dependent; it only requires water and an energy gradient in order to exist.\n\nExtremophiles, organisms able to survive in extreme environments, are a core research element for astrobiologists. Such organisms include biota which are able to survive several kilometers below the ocean's surface near hydrothermal vents and microbes that thrive in highly acidic environments. It is now known that extremophiles thrive in ice, boiling water, acid, alkali, the water core of nuclear reactors, salt crystals, toxic waste and in a range of other extreme habitats that were previously thought to be inhospitable for life. This opened up a new avenue in astrobiology by massively expanding the number of possible extraterrestrial habitats. Characterization of these organisms, their environments and their evolutionary pathways, is considered a crucial component to understanding how life might evolve elsewhere in the universe. For example, some organisms able to withstand exposure to the vacuum and radiation of outer space include the lichen fungi \"Rhizocarpon geographicum\" and \"Xanthoria elegans\", the bacterium \"Bacillus safensis\", \"Deinococcus radiodurans\", \"Bacillus subtilis\", yeast \"Saccharomyces cerevisiae\", seeds from \"Arabidopsis thaliana\" ('mouse-ear cress'), as well as the invertebrate animal Tardigrade. While Tardigrades are not considered true extremophiles, they are considered extremotolerant microorganisms that have contributed to the field of Astrobiology. Their extreme radiation tolerance and presence of DNA protection proteins may provide answers as to whether life can survive away from the protection of the Earth’s atmosphere.\n\nJupiter's moon, Europa, and Saturn's moon, Enceladus, are now considered the most likely locations for extant extraterrestrial life in the Solar System due to their subsurface water oceans where radiogenic and tidal heating enables liquid water to exist.\n\nThe origin of life, known as abiogenesis, distinct from the evolution of life, is another ongoing field of research. Oparin and Haldane postulated that the conditions on the early Earth were conducive to the formation of organic compounds from inorganic elements and thus to the formation of many of the chemicals common to all forms of life we see today. The study of this process, known as prebiotic chemistry, has made some progress, but it is still unclear whether or not life could have formed in such a manner on Earth. The alternative hypothesis of panspermia is that the first elements of life may have formed on another planet with even more favorable conditions (or even in interstellar space, asteroids, etc.) and then have been carried over to Earth—the panspermia hypothesis.\n\nThe cosmic dust permeating the universe contains complex organic compounds (\"amorphous organic solids with a mixed aromatic-aliphatic structure\") that could be created naturally, and rapidly, by stars. Further, a scientist suggested that these compounds may have been related to the development of life on Earth and said that, \"If this is the case, life on Earth may have had an easier time getting started as these organics can serve as basic ingredients for life.\" In September 2012, NASA scientists reported that polycyclic aromatic hydrocarbons (PAHs), subjected to interstellar medium conditions, are transformed through hydrogenation, oxygenation and hydroxylation, to more complex organics – \"a step along the path toward amino acids and nucleotides, the raw materials of proteins and DNA, respectively\".\n\nMore than 20% of the carbon in the universe may be associated with PAHs, possible starting materials for the formation of life. PAHs seem to have been formed shortly after the Big Bang, are widespread throughout the universe, and are associated with new stars and exoplanets.\n\nAstroecology concerns the interactions of life with space environments and resources, in planets, asteroids and comets. On a larger scale, astroecology concerns resources for life about stars in the galaxy through the cosmological future. Astroecology attempts to quantify future life in space, addressing this area of astrobiology.\n\nExperimental astroecology investigates resources in planetary soils, using actual space materials in meteorites. The results suggest that Martian and carbonaceous chondrite materials can support bacteria, algae and plant (asparagus, potato) cultures, with high soil fertilities. The results support that life could have survived in early aqueous asteroids and on similar materials imported to Earth by dust, comets and meteorites, and that such asteroid materials can be used as soil for future space colonies.\n\nOn the largest scale, cosmoecology concerns life in the universe over cosmological times. The main sources of energy may be red giant stars and white and red dwarf stars, sustaining life for 10 years. Astroecologists suggest that their mathematical models may quantify the potential amounts of future life in space, allowing a comparable expansion in biodiversity, potentially leading to diverse intelligent life forms.\n\nAstrogeology is a planetary science discipline concerned with the geology of celestial bodies such as the planets and their moons, asteroids, comets, and meteorites. The information gathered by this discipline allows the measure of a planet's or a natural satellite's potential to develop and sustain life, or planetary habitability.\n\nAn additional discipline of astrogeology is geochemistry, which involves study of the chemical composition of the Earth and other planets, chemical processes and reactions that govern the composition of rocks and soils, the cycles of matter and energy and their interaction with the hydrosphere and the atmosphere of the planet. Specializations include cosmochemistry, biochemistry and organic geochemistry.\n\nThe fossil record provides the oldest known evidence for life on Earth. By examining the fossil evidence, paleontologists are able to better understand the types of organisms that arose on the early Earth. Some regions on Earth, such as the Pilbara in Western Australia and the McMurdo Dry Valleys of Antarctica, are also considered to be geological analogs to regions of Mars, and as such, might be able to provide clues on how to search for past life on Mars.\n\nThe various organic functional groups, composed of hydrogen, oxygen, nitrogen, phosphorus, sulfur, and a host of metals, such as iron, magnesium, and zinc, provide the enormous diversity of chemical reactions necessarily catalyzed by a living organism. Silicon, in contrast, interacts with only a few other atoms, and the large silicon molecules are monotonous compared with the combinatorial universe of organic macromolecules. Indeed, it seems likely that the basic building blocks of life anywhere will be similar those on Earth, in the generality if not in the detail. Although terrestrial life and life that might arise independently of Earth are expected to use many similar, if not identical, building blocks, they also are expected to have some biochemical qualities that are unique. If life has had a comparable impact elsewhere in the Solar System, the relative abundances of chemicals key for its survival – whatever they may be – could betray its presence. Whatever extraterrestrial life may be, its tendency to chemically alter its environment might just give it away.\n\nPeople have long speculated about the possibility of life in settings other than Earth, however, speculation on the nature of life elsewhere often has paid little heed to constraints imposed by the nature of biochemistry. The likelihood that life throughout the universe is probably carbon-based is suggested by the fact that carbon is one of the most abundant of the higher elements. Only two of the natural atoms, carbon and silicon, are known to serve as the backbones of molecules sufficiently large to carry biological information. As the structural basis for life, one of carbon's important features is that unlike silicon, it can readily engage in the formation of chemical bonds with many other atoms, thereby allowing for the chemical versatility required to conduct the reactions of biological metabolism and propagation.\n\nThought on where in the Solar System life might occur, was limited historically by the understanding that life relies ultimately on light and warmth from the Sun and, therefore, is restricted to the surfaces of planets. The three most likely candidates for life in the Solar System are the planet Mars, the Jovian moon Europa, and Saturn's moons Titan, and Enceladus.\n\nMars, Enceladus and Europa are considered likely candidates in the search for life primarily because they may have underground liquid water, a molecule essential for life as we know it for its use as a solvent in cells. Water on Mars is found frozen in its polar ice caps, and newly carved gullies recently observed on Mars suggest that liquid water may exist, at least transiently, on the planet's surface. At the Martian low temperatures and low pressure, liquid water is likely to be highly saline. As for Europa, liquid water likely exists beneath the moon's icy outer crust. This water may be warmed to a liquid state by volcanic vents on the ocean floor, but the primary source of heat is probably tidal heating. On 11 December 2013, NASA reported the detection of \"clay-like minerals\" (specifically, phyllosilicates), often associated with organic materials, on the icy crust of Europa. The presence of the minerals may have been the result of a collision with an asteroid or comet according to the scientists.\n\nAnother planetary body that could potentially sustain extraterrestrial life is Saturn's largest moon, Titan. Titan has been described as having conditions similar to those of early Earth. On its surface, scientists have discovered the first liquid lakes outside Earth, but these lakes seem to be composed of ethane and/or methane, not water. Some scientists think it possible that these liquid hydrocarbons might take the place of water in living cells different from those on Earth. After Cassini data was studied, it was reported on March 2008 that Titan may also have an underground ocean composed of liquid water and ammonia. Additionally, Saturn's moon Enceladus may have an ocean below its icy surface and, according to NASA scientists in May 2011, \"is emerging as the most habitable spot beyond Earth in the Solar System for life as we know it\".\n\nMeasuring the ratio of hydrogen and methane levels on Mars may help determine the likelihood of life on Mars. According to the scientists, \"...low H/CH ratios (less than approximately 40) indicate that life is likely present and active.\" Other scientists have recently reported methods of detecting hydrogen and methane in extraterrestrial atmospheres.\n\nComplex organic compounds of life, including uracil, cytosine and thymine, have been formed in a laboratory under outer space conditions, using starting chemicals such as pyrimidine, found in meteorites. Pyrimidine, like polycyclic aromatic hydrocarbons (PAHs), is the most carbon-rich chemical found in the universe.\n\nThe Rare Earth hypothesis postulates that multicellular life forms found on Earth may actually be more of a rarity than scientists assume. It provides a possible answer to the Fermi paradox which suggests, \"If extraterrestrial aliens are common, why aren't they obvious?\" It is apparently in opposition to the principle of mediocrity, assumed by famed astronomers Frank Drake, Carl Sagan, and others. The Principle of Mediocrity suggests that life on Earth is not exceptional, but rather that life is more than likely to be found on innumerable other worlds.\n\nThe anthropic principle states that fundamental laws of the universe work specifically in a way that life would be possible. The anthropic principle supports the Rare Earth Hypothesis by arguing the overall elements that are needed to support life on Earth are so fine-tuned that it is nearly impossible for another just like it to exist by random chance.\n\nThe systematic search for possible life outside Earth is a valid multidisciplinary scientific endeavor. However, hypotheses and predictions as to its existence and origin vary widely, and at the present, the development of hypotheses firmly grounded on science may be considered astrobiology's most concrete practical application. It has been proposed that viruses are likely to be encountered on other life-bearing planets.\n\n, no evidence of extraterrestrial life has been identified. Examination of the Allan Hills 84001 meteorite, which was recovered in Antarctica in 1984 and originated from Mars, is thought by David McKay, as well as few other scientists, to contain microfossils of extraterrestrial origin; this interpretation is controversial.\n\nYamato 000593, the second largest meteorite from Mars, was found on Earth in 2000. At a microscopic level, spheres are found in the meteorite that are rich in carbon compared to surrounding areas that lack such spheres. The carbon-rich spheres may have been formed by biotic activity according to some NASA scientists.\n\nOn 5 March 2011, Richard B. Hoover, a scientist with the Marshall Space Flight Center, speculated on the finding of alleged microfossils similar to cyanobacteria in CI1 carbonaceous meteorites in the fringe \"Journal of Cosmology\", a story widely reported on by mainstream media. However, NASA formally distanced itself from Hoover's claim. According to American astrophysicist Neil deGrasse Tyson: \"At the moment, life on Earth is the only known life in the universe, but there are compelling arguments to suggest we are not alone.\"\n\n\nOn 17 March 2013, researchers reported that microbial life forms thrive in the Mariana Trench, the deepest spot on the Earth. Other researchers reported that microbes thrive inside rocks up to below the sea floor under of ocean off the coast of the northwestern United States. According to one of the researchers, \"You can find microbes everywhere—they're extremely adaptable to conditions, and survive wherever they are.\" These finds expand the potential habitability of certain niches of other planets.\n\nIn 2004, the spectral signature of methane () was detected in the Martian atmosphere by both Earth-based telescopes as well as by the Mars Express orbiter. Because of solar radiation and cosmic radiation, methane is predicted to disappear from the Martian atmosphere within several years, so the gas must be actively replenished in order to maintain the present concentration. The \"Curiosity\" rover will perform precision measurements of oxygen and carbon isotope ratios in carbon dioxide (CO) and methane (CH) in the atmosphere of Mars in order to distinguish between a geochemical and a biological origin.\n\nIt is possible that some exoplanets may have moons with solid surfaces or liquid oceans that are hospitable. Most of the planets so far discovered outside the Solar System are hot gas giants thought to be inhospitable to life, so it is not yet known whether the Solar System, with a warm, rocky, metal-rich inner planet such as Earth, is of an aberrant composition. Improved detection methods and increased observation time will undoubtedly discover more planetary systems, and possibly some more like ours. For example, NASA's Kepler Mission seeks to discover Earth-sized planets around other stars by measuring minute changes in the star's light curve as the planet passes between the star and the spacecraft. Progress in infrared astronomy and submillimeter astronomy has revealed the constituents of other star systems.\n\n\nEfforts to answer questions such as the abundance of potentially habitable planets in habitable zones and chemical precursors have had much success. Numerous extrasolar planets have been detected using the wobble method and transit method, showing that planets around other stars are more numerous than previously postulated. The first Earth-sized extrasolar planet to be discovered within its star's habitable zone is Gliese 581 c.\n\nResearch into the environmental limits of life and the workings of extreme ecosystems is ongoing, enabling researchers to better predict what planetary environments might be most likely to harbor life. Missions such as the \"Phoenix\" lander, Mars Science Laboratory, ExoMars, Mars 2020 rover to Mars, and the \"Cassini\" probe to Saturn's moons aim to further explore the possibilities of life on other planets in the Solar System.\n\nThe two Viking landers each carried four types of biological experiments to the surface of Mars in the late 1970s. These were the only Mars landers to carry out experiments looking specifically for metabolism by current microbial life on Mars. The landers used a robotic arm to collect soil samples into sealed test containers on the craft. The two landers were identical, so the same tests were carried out at two places on Mars' surface; Viking 1 near the equator and Viking 2 further north. The result was inconclusive, and is still disputed by some scientists.\n\n\"Beagle 2\" was an unsuccessful British Mars lander that formed part of the European Space Agency's 2003 Mars Express mission. Its primary purpose was to search for signs of life on Mars, past or present. Although it landed safely, it was unable to correctly deploy its solar panels and telecom antenna.\n\nEXPOSE is a multi-user facility mounted in 2008 outside the International Space Station dedicated to astrobiology. EXPOSE was developed by the European Space Agency (ESA) for long-term spaceflights that allow exposure of organic chemicals and biological samples to outer space in low Earth orbit.\n\nThe Mars Science Laboratory (MSL) mission landed the \"Curiosity\" rover that is currently in operation on Mars. It was launched 26 November 2011, and landed at Gale Crater on 6 August 2012. Mission objectives are to help assess Mars' habitability and in doing so, determine whether Mars is or has ever been able to support life, collect data for a future human mission, study Martian geology, its climate, and further assess the role that water, an essential ingredient for life as we know it, played in forming minerals on Mars.\n\nThe \"Tanpopo\" mission is an orbital astrobiology experiment investigating the potential interplanetary transfer of life, organic compounds, and possible terrestrial particles in the low Earth orbit. The purpose is to assess the panspermia hypothesis and the possibility of natural interplanetary transport of microbial life as well as prebiotic organic compounds. Early mission results show evidence that some clumps of microorganism can survive for at least one year in space. This may support the idea that clumps greater than 0.5 millimeters of microorganisms could be one way for life to spread from planet to planet.\n\n\n\"ExoMars rover\" is a robotic mission to Mars to search for possible biosignatures of Martian life, past or present. This astrobiological mission is currently under development by the European Space Agency (ESA) in partnership with the Russian Federal Space Agency (Roscosmos); it is planned for a 2018 launch.\n\n\"Mars 2020\" rover mission is under development by NASA for a launch in 2020. It will investigate environments on Mars relevant to astrobiology, investigate its surface geological processes and history, including the assessment of its past habitability and potential for preservation of biosignatures and biomolecules within accessible geological materials. The Science Definition Team is proposing the rover collect and package at least 31 samples of rock cores and soil for a later mission to bring back for more definitive analysis in laboratories on Earth. The rover could make measurements and technology demonstrations to help designers of a human expedition understand any hazards posed by Martian dust and demonstrate how to collect carbon dioxide (CO), which could be a resource for making molecular oxygen (O) and rocket fuel.\n\n\"Europa Clipper\" is a mission planned by NASA for a 2025 launch that will conduct detailed reconnaissance of Jupiter's moon Europa and will investigate whether the icy moon could harbor conditions suitable for life. It will also aid in the selection of future landing sites.\n\n\"Icebreaker Life\" is a lander mission that proposed for NASA's Discovery Program for the 2021 launch opportunity, but it was not selected for development. It would have had a stationary lander that would be a near copy of the successful 2008 \"Phoenix\" and it would have carried an upgraded astrobiology scientific payload, including a 1-meter-long core drill to sample ice-cemented ground in the northern plains to conduct a search for organic molecules and evidence of current or past life on Mars. One of the key goals of the \"Icebreaker Life\" mission is to test the hypothesis that the ice-rich ground in the polar regions has significant concentrations of organics due to protection by the ice from oxidants and radiation.\n\n\"Journey to Enceladus and Titan\" (\"JET\") is an astrobiology mission concept to assess the habitability potential of Saturn's moons Enceladus and Titan by means an orbiter.\n\n\"Enceladus Life Finder\" (\"ELF\") is a proposed astrobiology mission concept for a space probe intended to assess the habitability of the internal aquatic ocean of Enceladus, Saturn's sixth-largest moon.\n\n\"Life Investigation For Enceladus\" (\"LIFE\") is a proposed astrobiology sample-return mission concept. The spacecraft would enter into Saturn orbit and enable multiple flybys through Enceladus' icy plumes to collect icy plume particles and volatiles and return them to Earth on a capsule. The spacecraft may sample Enceladus' plumes, the E ring of Saturn, and the upper atmosphere of Titan.\n\n\"Oceanus\" is an orbiter proposed in 2017 for the New Frontiers mission #4. It would travel to the moon of Saturn, Titan, to assess its habitability. \"Oceanus\" objectives are to reveal Titan's organic chemistry, geology, gravity, topography, collect 3D reconnaissance data, catalog the organics and determine where they may interact with liquid water.\n\n\"Explorer of Enceladus and Titan\" (ET) is an orbiter mission concept that would investigate the evolution and habitability of the Saturnian satellites Enceladus and Titan and was proposed in 2017 by the European Space Agency.\n\n\n\n", "id": "2787", "title": "Astrobiology"}
{"url": "https://en.wikipedia.org/wiki?curid=1663537", "text": "Allometry\n\nAllometry is the study of the relationship of body size to shape, anatomy, physiology and finally behaviour, first outlined by Otto Snell in 1892, by D'Arcy Thompson in 1917 in \"On Growth and Form\" and by Julian Huxley in 1932.\n\nAllometry is a well-known study, particularly in statistical shape analysis for its theoretical developments, as well as in biology for practical applications to the differential growth rates of the parts of a living organism's body. One application is in the study of various insect species (e.g., Hercules beetles), where a small change in overall body size can lead to an enormous and disproportionate increase in the dimensions of appendages such as legs, antennae, or horns The relationship between the two measured quantities is often expressed as a power law equation which expresses a remarkable scale symmetry:\n\nor in a logarithmic form:\n\nwhere formula_3 is the scaling exponent of the law. Methods for estimating this exponent from data can use type-2 regressions, such as major axis regression or reduced major axis regression, as these account for the variation in both variables, contrary to least squares regression, which does not account for error variance in the independent variable (e.g., log body mass). Other methods include measurement-error models and a particular kind of principal component analysis.\n\nAllometry often studies shape differences in terms of ratios of the objects' dimensions. Two objects of different size, but common shape, will have their dimensions in the same ratio. Take, for example, a biological object that grows as it matures. Its size changes with age, but the shapes are similar. Studies of ontogenetic allometry often use lizards or snakes as model organisms both because they lack parental care after birth or hatching and because they exhibit a large range of body sizes between the juvenile and adult stage. Lizards often exhibit allometric changes during their ontogeny.\n\nIn addition to studies that focus on growth, allometry also examines shape variation among individuals of a given age (and sex), which is referred to as static allometry. Comparisons of species are used to examine interspecific or evolutionary allometry (see also: Phylogenetic comparative methods).\n\nIsometric scaling happens when proportional relationships are preserved as size changes during growth or over evolutionary time. An example is found in frogs — aside from a brief period during the few weeks after metamorphosis, frogs grow isometrically. Therefore, a frog whose legs are as long as its body will retain that relationship throughout its life, even if the frog itself increases in size tremendously.\n\nIsometric scaling is governed by the square-cube law. An organism which doubles in length isometrically will find that the surface area available to it will increase fourfold, while its volume and mass will increase by a factor of eight. This can present problems for organisms. In the case of above, the animal now has eight times the biologically active tissue to support, but the surface area of its respiratory organs has only increased fourfold, creating a mismatch between scaling and physical demands. Similarly, the organism in the above example now has eight times the mass to support on its legs, but the strength of its bones and muscles is dependent upon their cross-sectional area, which has only increased fourfold. Therefore, this hypothetical organism would experience twice the bone and muscle loads of its smaller version. This mismatch can be avoided either by being \"overbuilt\" when small or by changing proportions during growth, called allometry.\n\nIsometric scaling is often used as a null hypothesis in scaling studies, with 'deviations from isometry' considered evidence of physiological factors forcing allometric growth.\n\nAllometric scaling is any change that deviates from isometry. A classic example discussed by Galileo in his \"Dialogues Concerning Two New Sciences\" is the skeleton of mammals. The skeletal structure becomes much stronger and more robust relative to the size of the body as the body size increases. Allometry is often expressed in terms of a scaling exponent based on body mass, or body length (Snout-vent length, total length etc.). A perfectly isometrically scaling organism would see all volume-based properties change proportionally to the body mass, all surface area-based properties change with mass to the power 2/3, and all length-based properties change with mass to the 1/3 power. If, after statistical analyses, for example, a volume-based property was found to scale to mass to the 0.9 power, then this would be called \"negative allometry\", as the values are smaller than predicted by isometry. Conversely, if a surface area-based property scales to mass to the 0.8 power, the values are higher than predicted by isometry and the organism is said to show \"positive allometry\". One example of positive allometry occurs among species of monitor lizards (family Varanidae), in which the limbs are relatively longer in larger-bodied species. The same is true for some fish, e.g. the muskellunge, the weight of which grows with about the power of 3.325 of its length. A muskellunge will weigh about , while a muskellunge will weigh about , as 33% longer while more than double the weight.\n\nTo determine whether isometry or allometry is present, an expected relationship between variables needs to be determined to compare data to. This is important in determining if the scaling relationship in a dataset deviates from an expected relationship (such as those that follow isometry). The use of tools such as dimensional analysis is very helpful in determining expected slope. This ‘expected’ slope, as it is known, is essential for detecting allometry because scaling variables are comparisons to other things. Saying that mass scales with a slope of 5 in relation to length doesn’t have much meaning unless knowing the isometric slope is 3, meaning in this case, the mass is increasing extremely fast. For example, different sized frogs should be able to jump the same distance according to the geometric similarity model proposed by Hill 1950 and interpreted by Wilson 2000, but in actuality larger frogs do jump longer distances. \nDimensional analysis is extremely useful for balancing units in an equation or in this case, determining expected slope.\n\nA few dimensional examples follow (M=Mass, L=Length, V=Volume, which is also L cubed because a volume is merely length cubed):\n\nTo find the expected slope for the relationship between mass and the characteristic length of an animal (see figure), the units of mass (M=L, because mass is a volume; volumes are lengths cubed) from the Y-axis are divided by the units of the X-axis (in this case, L). The expected slope on a double-logarithmic plot of L/ L in this case is 3 (log(L)/log(L)=3). This is the slope of a straight line, but most data gathered in science do not fall neatly in a straight line, so data transformations are useful. \nIt is also important to keep in mind what is being compared in the data. Comparing a characteristic such as head length to head width might yield different results from comparing head length to body length. That is, different characteristics may scale differently.\n\nA common way to analyze data such as those collected in scaling is to use log-transformation. There are two reasons for log transformation - a biological reason and a statistical reason. Biologically, log-log transformation places numbers into a geometric domain so that proportional deviations are represented consistently, independent of the scale and units of measurement. In biology this is appropriate because many biological phenomena (e.g. growth, reproduction, metabolism, sensation) are fundamentally multiplicative. Statistically, it is beneficial to transform both axes using logarithms and then perform a linear regression. This will normalize the data set and make it easier to analyze trends using the slope of the line. Before analyzing data though, it is important to have a predicted slope of the line to compare the analysis to.\n\nAfter data are log-transformed and linearly regressed, comparisons can then use least squares regression with 95% confidence intervals or reduced major axis analysis. Sometimes the two analyses can yield different results, but often they do not. If the expected slope is outside the confidence intervals, then there is allometry present. If mass in this imaginary animal scaled with a slope of 5 and this was a statistically significant value, then mass would scale very fast in this animal versus the expected value. It would scale with positive allometry. If the expected slope were 3 and in reality in a certain organism mass scaled with 1 (assuming this slope is statistically significant), then it would be negatively allometric.\n\nAnother example: Force is dependent on the cross-sectional area of muscle (CSA), which is L. If comparing force to a length, then the expected slope is 2. \nAlternatively, this analysis may be accomplished with a power regression. Plot the relationship between the data onto a graph. Fit this to a power curve (depending on the stats program, this can be done multiple ways), and it will give an equation with the form: \"y\"=\"Zx\", where \"n\" is the number. That “number” is the relationship between the data points. The downside, to this form of analysis, is that it makes it a little more difficult to do statistical analyses.\n\nMany physiological and biochemical processes (such as heart rate, respiration rate or the maximum reproduction rate) show scaling, mostly associated with the ratio between surface area and mass (or volume) of the animal. The metabolic rate of an individual animal is also subject to scaling.\n\nIn plotting an animal's basal metabolic rate (BMR) against the animal's own body mass, a logarithmic straight line is obtained, indicating a power-law dependence. Overall metabolic rate in animals is generally accepted to show negative allometry, scaling to mass to a power ≈ 0.75, known as Kleiber's law, 1932. This means that larger-bodied species (e.g., elephants) have lower mass-specific metabolic rates and lower heart rates, as compared with smaller-bodied species (e.g., mice). The straight line generated from a double logarithmic scale of metabolic rate in relation to body mass is known as the \"mouse-to-elephant curve\". These relationships of metabolic rates, times, and internal structure have been explained as, \"an elephant is approximately a blown-up gorilla, which is itself a blown-up mouse.\"\n\nMax Kleiber contributed the following allometric equation for relating the BMR to the body mass of an animal. Statistical analysis of the intercept did not vary from 70 and the slope was not varied from 0.75, thus:\nwhere formula_5 is body mass, and metabolic rate is measured in kcal per day.\n\nConsequently, the body mass itself can explain the majority of the variation in the BMR. After the body mass effect, the taxonomy of the animal plays the next most significant role in the scaling of the BMR. The further speculation that environmental conditions play a role in BMR can only be properly investigated once the role of taxonomy is established. The challenge with this lies in the fact that a shared environment also indicates a common evolutionary history and thus a close taxonomic relationship. There are strides currently in research to overcome these hurdles; for example, an analysis in muroid rodents, the mouse, hamster, and vole type, took into account taxonomy. Results revealed the hamster (warm dry habitat) had lowest BMR and the mouse (warm wet dense habitat) had the highest BMR. Larger organs could explain the high BMR groups, along with their higher daily energy needs. Analyses such as these demonstrate the physiological adaptations to environmental changes that animals undergo.\n\nEnergy metabolism is subjected to the scaling of an animal and can be overcome by an individual's body design. The metabolic scope for an animal is the ratio of resting and maximum rate of metabolism for that particular species as determined by oxygen consumption.\nOxygen consumption V and maximum oxygen consumption VO2 max. Oxygen consumption in species that differ in body size and organ system dimensions show a similarity in their charted V distributions indicating that, despite the complexity of their systems, there is a power law dependence of similarity; therefore, universal patterns are observed in diverse animal taxonomy.\n\nAcross a broad range of species, allometric relations are not necessarily linear on a log-log scale. For example, the maximal running speeds of mammals show a complicated relationship with body mass, and the fastest sprinters are of intermediate body size.\n\nThe muscle characteristics of animals are similar in a wide range of animal sizes, though muscle sizes and shapes can and often do vary depending on environmental constraints placed on them. The muscle tissue itself maintains its contractile characteristics and does not vary depending on the size of the animal. Physiological scaling in muscles affects the number of muscle fibers and their intrinsic speed to determine the maximum power and efficiency of movement in a given animal. The speed of muscle recruitment varies roughly in inverse proportion to the cube root of the animal’s weight, such as the intrinsic frequency of the sparrow’s flight muscle compared to that of a stork’s.\n\nFor inter-species allometric relations related to such ecological variables as maximal reproduction rate, attempts have been made to explain scaling within the context of dynamic energy budget theory and the metabolic theory of ecology. However, such ideas have been less successful.\n\nAllometry has been used to study patterns in locomotive principles across a broad range of species. Such research has been done in pursuit of a better understanding of animal locomotion, including the factors that different gaits seek to optimize. Allometric trends observed in extant animals have even been combined with evolutionary algorithms to form realistic hypotheses concerning the locomotive patterns of extinct species. These studies have been made possible by the remarkable similarities among disparate species’ locomotive kinematics and dynamics, “despite differences in morphology and size”.\n\nAllometric study of locomotion involves the analysis of the relative sizes, masses, and limb structures of similarly shaped animals and how these features affect their movements at different speeds. Patterns are identified based on dimensionless Froude numbers, which incorporate measures of animals’ leg lengths, speed or stride frequency, and weight.\n\nAlexander incorporates Froude-number analysis into his “dynamic similarity hypothesis” of gait patterns. Dynamically similar gaits are those between which there are constant coefficients that can relate linear dimensions, time intervals, and forces. In other words, given a mathematical description of gait A and these three coefficients, one could produce gait B, and vice versa. The hypothesis itself is as follows: “animals of different sizes tend to move in dynamically similar fashion whenever the ratio of their speed allows it.” While the dynamic similarity hypothesis may not be a truly unifying principle of animal gait patterns, it is a remarkably accurate heuristic.\n\nIt has also been shown that living organisms of all shapes and sizes utilize spring mechanisms in their locomotive systems, probably in order to minimize the energy cost of locomotion. The allometric study of these systems has fostered a better understanding of why spring mechanisms are so common, how limb compliance varies with body size and speed, and how these mechanisms affect general limb kinematics and dynamics.\n\n\nThe physiological effect of drugs and other substances in many cases scales allometrically.\n\nWest, Brown, and Enquist in 1997 derived a hydrodynamic theory to explain the universal fact that metabolic rate scales as the ¾ power with body weight. They also showed why lifespan scales as the +¼ power and heart rate as the -¼ power. Blood flow (+¾) and resistance (-¾) scale in the same way, leading to blood pressure being constant across species.\n\nHu and Hayton in 2001 discussed whether the basal metabolic rate scale is a ⅔ or ¾ power of body mass. The exponent of ¾ might be used for substances that are eliminated mainly by metabolism, or by metabolism and excretion combined, while ⅔ might apply for drugs that are eliminated mainly by renal excretion.\n\nAn online allometric scaler of drug doses based on the above work is available.\n\nThe US Food and Drug Administration (FDA) published guidance in 2005 giving a flow chart that presents the decisions and calculations used to generate the maximum recommended starting dose in drug clinical trials from animal data.\n\n\"This technical section is not well-written and needs editing\".\n\nThe mass and density of an organism have a large effect on the organism's locomotion through a fluid. For example, a tiny organisms uses flagella and can effectively move through a fluid it is suspended in. Then on the other scale a blue whale that is much more massive and dense in comparison with the viscosity of the fluid, compared to a bacterium in the same medium. The way in which the fluid interacts with the external boundaries of the organism is important with locomotion through the fluid. For streamlined swimmers the resistance or drag determines the performance of the organism. This drag or resistance can be seen in two distinct flow patterns. There is Laminar Flow where the fluid is relatively uninterrupted after the organism moves through it. Turbulent flow is the opposite, where the fluid moves roughly around an organisms that creates vortices that absorb energy from the propulsion or momentum of the organism. Scaling also affects locomotion through a fluid because of the energy needed to propel an organism and to keep up velocity through momentum. The rate of oxygen consumption per gram body size decreases consistently with increasing body size. (Knut Schmidt-Nielson 2004)\n\nIn general, smaller, more streamlined organisms create laminar flow (\"R\" < 0.5x106), whereas larger, less streamlined organisms produce turbulent flow (\"R\" > 2.0×106). Also, increase in velocity (V) increases turbulence, which can be proved using the Reynolds equation. In nature however, organisms such as a 6‘-6” dolphin moving at 15 knots does not have the appropriate Reynolds numbers for laminar flow \"R\" = 107, but exhibit it in nature. Mr. G.A Steven observed and documented dolphins moving at 15 knots alongside his ship leaving a single trail of light when phosphorescent activity in the sea was high. The factors that contribute are:\n\nThe resistance to the motion of an approximately stream-lined solid through a fluid can be expressed by the formula: \"C\"(total surface)\"V\"2/2 V = velocity\n\nNotable Reynolds numbers:\n\nScaling also has an effect on the performance of organisms in fluid. This is extremely important for marine mammals and other marine organisms that rely on atmospheric oxygen to survive and carry out respiration. This can affect how fast an organism can propel itself efficiently and more importantly how long it can dive, or how long and how deep an organism can stay underwater. Heart mass and lung volume are important in determining how scaling can affect metabolic function and efficiency. Aquatic mammals, like other mammals, have the same size heart proportional to their bodies.\n\nMammals have a heart that is about 0.6% of the total body mass across the board from a small mouse to a large Blue Whale. It can be expressed as: Heart Weight = 0.006Mb1.0, where Mb is the body mass of the individual. Lung volume is also directly related to body mass in mammals (slope = 1.02). The lung has a volume of 63 ml for every kg of body mass. In addition, the tidal volume at rest in an individual is 1/10 the lung volume. Also respiration costs with respect to oxygen consumption is scaled in the order of Mb.75. This shows that mammals, regardless of size, have the same size respiratory and cardiovascular systems and it turn have the same amount of blood: About 5.5% of body mass. This means that for a similarly designed marine mammals, the larger the individual the more efficiently they can travel compared to a smaller individual. It takes the same effort to move one body length whether the individual is one meter or ten meters. This can explain why large whales can migrate far distance in the oceans and not stop for rest. It is metabolically less expensive to be larger in body size. This goes for terrestrial and flying animals as well. In fact, for an organism to move any distance, regardless of type from elephants to centipedes, smaller animals consume more oxygen per unit body mass than larger ones. This metabolic advantage that larger animals have makes it possible for larger marine mammals to dive for longer durations of time than their smaller counterparts. That the heart rate is lower means that larger animals can carry more blood, which carries more oxygen. Then in conjuncture with the fact that mammals reparation costs scales in the order of Mb.75 shows how an advantage can be had in having a larger body mass. More simply, a larger whale can hold more oxygen and at the same time demand less metabolically than a smaller whale.\n\nTraveling long distances and deep dives are a combination of good stamina and also moving an efficient speed and in an efficient way to create laminar flow, reducing drag and turbulence. In sea water as the fluid, it traveling long distances in large mammals, such as whales, is facilitated by their neutral buoyancy and have their mass completely supported by the density of the sea water. On land, animals have to expend a portion of their energy during locomotion to fight the effects of gravity.\n\nFlying organisms such as birds are also considered moving through a fluid. In scaling birds of similar shape, it has also been seen that larger individuals have less metabolic cost per kg than smaller species, which would be expected because it holds true for every other form of animal. Birds also have a variance in wing beat frequency. Even with the compensation of larger wings per unit body mass, larger birds also have a slower wing beat frequency, which allows larger birds to fly at higher altitudes, longer distances, and faster absolute speeds than smaller birds. Because of the dynamics of lift-based locomotion and the fluid dynamics, birds have a U-shaped curve for metabolic cost and velocity. Because flight, in air as the fluid, is metabolically more costly at the lowest and the highest velocities. On the other end, small organisms such as insects can make gain advantage from the viscosity of the fluid (air) that they are moving in. A wing-beat timed perfectly can effectively uptake energy from the previous stroke. (Dickinson 2000) This form of wake capture allows an organism to recycle energy from the fluid or vortices within that fluid created by the organism itself. This same sort of wake capture occurs in aquatic organisms as well, and for organisms of all sizes. This dynamic of fluid locomotion allows smaller organisms to gain advantage because the effect on them from the fluid is much greater because of their relatively smaller size.\nAllometric engineering is a method for manipulating allometric relationships within or among groups.\n\nArguing that there are a number of analogous concepts and mechanisms between cities and biological entities, Bettencourt et al. showed a number of scaling relationships between observable properties of a city and the city size. GDP, \"supercreative\" employment, number of inventors, crime, spread of disease, and even pedestrian walking speeds scale with city population.\n\nSome examples of allometric laws:\n\nMany factors go into the determination of body mass and size for a given animal. These factors often affect body size on an evolutionary scale, but conditions such as availability of food and habitat size can act much more quickly on a species. Other examples include the following:\n\n\n\n", "id": "1663537", "title": "Allometry"}
{"url": "https://en.wikipedia.org/wiki?curid=20874", "text": "Mycology\n\nMycology is the branch of biology concerned with the study of fungi, including their genetic and biochemical properties, their taxonomy and their use to humans as a source for tinder, medicine, food, and entheogens, as well as their dangers, such as toxicity or infection. \n\nA biologist specializing in mycology is called a mycologist.\nMycology branches into the field of phytopathology, the study of plant diseases, and the two other disciplines that remain closely related because the vast majority of \"plant\" pathogens are fungi.\n\nHistorically, mycology was a branch of botany because, although fungi are evolutionarily more closely related to animals than to plants, this was not recognized until a few decades ago. Pioneer \"mycologists\" included Elias Magnus Fries, Christian Hendrik Persoon, Anton de Bary, and Lewis David von Schweinitz.\n\nMany fungi produce toxins, antibiotics, and other secondary metabolites. For example, the cosmopolitan (worldwide) genus \"Fusarium\" and their toxins associated with fatal outbreaks of alimentary toxic aleukia in humans were extensively studied by Abraham Joffe.\n\nFungi are fundamental for life on earth in their roles as symbionts, e.g. in the form of mycorrhizae, insect symbionts, and lichens. Many fungi are able to break down complex organic biomolecules such as lignin, the more durable component of wood, and pollutants such as xenobiotics, petroleum, and polycyclic aromatic hydrocarbons. By decomposing these molecules, fungi play a critical role in the global carbon cycle.\n\nFungi and other organisms traditionally recognized as fungi, such as oomycetes and myxomycetes (slime molds), often are economically and socially important, as some cause diseases of animals (such as histoplasmosis) as well as plants (such as Dutch elm disease and Rice blast).\n\nApart from pathogenic fungi, many fungal species are very important in controlling the plant diseases caused by different pathogens. For example, species of the filamentous fungal genus \"Trichoderma\" considered as one of the most important biological control agents as an alternative to chemical based products for effective crop diseases management.\n\nField meetings to find interesting species of fungi are known as 'forays', after the first such meeting organized by the Woolhope Naturalists' Field Club in 1868 and entitled \"A foray among the funguses\"[\"sic\"].\n\nSome fungi can cause disease in humans or other organisms. The study of pathogenic fungi is referred to as medical mycology.\n\nIt is presumed that humans started collecting mushrooms as food in prehistoric times. Mushrooms were first written about in the works of Euripides (480-406 B.C.). The Greek philosopher Theophrastos of Eresos (371-288 B.C.) was perhaps the first to try to systematically classify plants; mushrooms were considered to be plants missing certain organs. It was later Pliny the Elder (23–79 A.D.), who wrote about truffles in his encyclopedia \"Naturalis historia\". The word \"mycology\" comes from the Greek: μύκης (\"mukēs\"), meaning \"fungus\" and the suffix (\"-logia\"), meaning \"study\".\n\nThe Middle Ages saw little advancement in the body of knowledge about fungi. Rather, the invention of the printing press allowed some authors to disseminate superstitions and misconceptions about the fungi that had been perpetuated by the classical authors.\n\nThe start of the modern age of mycology begins with Pier Antonio Micheli's 1737 publication of \"Nova plantarum genera\". Published in Florence, this seminal work laid the foundations for the systematic classification of grasses, mosses and fungi. The term \"mycology\" and the complementary \"mycologist\" were first used in 1836 by M.J. Berkeley.\n\nFor centuries, certain mushrooms have been documented as a folk medicine in China, Japan, and Russia. Although the use of mushrooms in folk medicine is centered largely on the Asian continent, people in other parts of the world like the Middle East, Poland, and Belarus have been documented using mushrooms for medicinal purposes. Certain mushrooms, especially polypores like reishi were thought to be able to benefit a wide variety of health ailments. Medicinal mushroom research in the United States is currently active, with studies taking place at City of Hope National Medical Center, as well as the Memorial Sloan–Kettering Cancer Center.\n\nCurrent research focuses on mushrooms that may have hypoglycemic activity, anti-cancer activity, anti-pathogenic activity, and immune system-enhancing activity. Recent research has found that the oyster mushroom naturally contains the cholesterol-lowering drug lovastatin, mushrooms produce large amounts of vitamin D when exposed to ultraviolet (UV) light, and that certain fungi may be a future source of taxol. To date, penicillin, lovastatin, ciclosporin, griseofulvin, cephalosporin, LSD-25, and statins are the most famous pharmaceuticals that have been isolated from the fifth kingdom of life.\n\n\n", "id": "20874", "title": "Mycology"}
{"url": "https://en.wikipedia.org/wiki?curid=172384", "text": "Lichenology\n\nLichenology is the branch of mycology that studies the lichens, symbiotic organisms made up of an intimate symbiotic association of a microscopic alga (or a cyanobacterium) with a filamentous fungus.\n\nStudy of lichens draws knowledge from several disciplines: mycology, phycology, microbiology and botany. Scholars of lichenology are known as lichenologists.\n\nThe taxonomy of lichens was first intensively investigated by the Swedish botanist Erik Acharius (1757–1819), who is therefore sometimes named the \"father of lichenology\". Acharius was a student of Carl Linnaeus. Some of his more important works on the subject, which marked the beginning of lichenology as a discipline, are:\n\n\nLater lichenologists include the American scientists Vernon Ahmadjian and Edward Tuckerman and the Russian evolutionary biologist Konstantin Merezhkovsky, as well as amateurs such as Louisa Collings.\n\nLichens as a group have received less attention in classical treatises on botany than other groups although the relationship between humans and some species has been documented from early times. Several species have appeared in the works of Dioscorides, Pliny the Elder and Theophrastus although the studies are not very deep. During the first centuries of the modern age they were usually put forward as examples of spontaneous generation and their reproductive mechanisms were totally ignored. For centuries naturalists had included lichens in diverse groups until in the early 18th century a French researcher Joseph Pitton de Tournefort in his \"Institutiones Rei Herbariae\" grouped them into their own genus. He adopted the Latin term lichen, which had already been used by Pliny who had imported it from Theophrastus but up until then this term had not been widely employed. The original meaning of the Greek word λειχήν (leichen) was moss that in its turn derives from the Greek verb λείχω (liekho) to suck because of the great ability of these organisms to absorb water. In its original use the term signified mosses, liverworts as well as lichens. Some forty years later Dillenius in his \"Historia Muscorum\" made the first division of the group created by Tournefort separating the sub-families Usnea, Coralloides and Lichens in response to the morphological characteristics of the lichen thallus.\n\nAfter the revolution in taxonomy brought in by Linnaeus and his new system of classification lichens are retained in the Plant Kingdom forming a single group Lichen with eight divisions within the group according to the morphology of the thallus.\n\nOver the years research is shedding new light into the nature of these organisms still classified as plants. A controversial issue surrounding lichens since the early 19th century is their reproduction. In these years a group of researchers faithful to the tenets of Linnaeus considered that lichens reproduced sexually and had sexual reproductive organs, as in other plants, independent of whether asexual reproduction also occurred. Other researchers only considered asexual reproduction by means of Propagules.\n\nAgainst this background appeared the Swedish botanist Erik Acharius disciple of Linnaeus, who is today considered the father of lichenology, starting the taxonomy of lichens with his pioneering study of Swedish lichens in Lichenographiae Suecicae Prodromus of 1798 or in his Synopsis Methodica Lichenum, Sistens omnes hujus Ordinis Naturalis of 1814. These studies and classifications are the cornerstone of subsequent investigations. In these early years of structuring the new discipline various works of outstanding scientific importance appeared such as Lichenographia Europaea Reformata published in 1831 by Elias Fries or Enumeratio Critico Lichenum Europaeorum 1850 by Ludwig Schaerer in Germany.\n\nScientific publications settled many unknown facts about lichens. In the French publication Annales des Sciences Naturelles in an article of 1852 \"Memorie pour servir a l'Histoire des Lichens Organographique et Physiologique\" by Edmond Tulasne, the reproductive organs or apothecia of lichens was identified.\n\nThese new discoveries were becoming increasingly contradictory for scientists. The apothecium reproductive organ being unique to fungi but absent in other photosynthetic organisms. With improvements in microscopy, algae were identified in the lichen structure, which heightened the contradictions. At first the presence of algae was taken as being due to contamination due to collection of samples in damp conditions and they were not considered as being in a symbiotic relation with the fungal part of the thallus. That the algae continued to multiply showed that they were not mere contaminants.\n\nIt was Anton de Bary a German mycologist who specialised in phytopathology who first suggested in 1865 that lichens were merely the result of parasitism of various fungi of the ascomycetes group by nostoc type algae and others. Successive studies such as those carried out by Andrei Famintsyn and Baranetzky in 1867 showed no dependence of the algal component upon the lichen thallus and that the algal component could live independently of the thallus. It was in 1869 that Simon Schwendener demonstrated that all lichens were the result of fungal attack on the cells of algal cells and that all these algae also exist free in nature. This researcher was the first to recognise the dual nature of lichens as a result of the capture of the algal component by the fungal component. In 1873 Jean-Baptiste Edouard Bornet concluded form studying many different lichen species that the relationship between fungi and algae was purely symbiotic. It was also established that algae could associate with many different fungi to form different lichen phenotypes. \n\nIn 1909 the Russian lichenologist Konstantin Mereschkowski presented a research paper \"The Theory of two Plasms as the basis of Symbiogenesis, A new study on the Origin of Organisms\", which aims to explain a new theory of Symbiogenesis by lichens and other organisms as evidenced by his earlier work \"Nature and Origin of Chromatophores in the Plant Kingdom\". These new ideas can be studied today under the title of the Theory of Endosymbiosis.\n\nDespite the above studies the dual nature of lichens remained no more than a theory until in 1939 the Swiss researcher Eugen A Thomas was able to reproduce in the laboratory the phenotype of the lichen \"Cladonia pyxidata\" by combining its two identified components.\n\nDuring the 20th century botany and mycology were still attempting to solve the two main problems surrounding lichens. On the one hand the definition of lichens and the relationship between the two symbionts and the taxonomic position of these organisms within the plant and fungal kingdoms. There appeared numerous renowned researchers within the field of lichenology Henry Nicollon des Abbayes, William Alfred Weber, Antonina Georgievna Borissova, Irwin M. Brodo, George Albert Llano.\n\nLichenology has found applications beyond biology itself in the field of geology in a technique known as lichenometry where the age of an exposed surface can be found by studying the age of lichens growing on them. Age dating in this way can be absolute or relative because the growth of these organisms can be arrested under various conditions. The technique provides an average age of the older individual lichens providing a minimum age of the medium being studied. Lichenometry relies upon the fact that the maximum diameter of the largest thallus of an epilithic lichen growing on a substrate is directly proportional to the time from first exposure of the area to the environment as seen in studies by Roland Beschel in 1950 and is especially useful in areas exposed for less than 1000 years. Growth is greatest in the first 20 to 100 years with 15–50 mm growth per year and less in the following years with average growth of 2–4 mm per year.\n\nThe difficulty of giving a definition applicable to every known lichen has been debated since lichenologists first recognised the dual nature of lichens. In 1982 the International Association for Lichenology convened a meeting to adopt a single definition of lichen drawing on the proposals of a committee. The chairman of this committee was the renowned researcher Vernon Ahmadjian. The definition finally adopted was that lichen could be considered as the association between a fungus and a photosynthetic symbiont resulting in a thallus of specific structure.\n\nSuch a simple a priori definition soon brought criticism from various lichenologists and there soon emerged reviews and suggestions for amendments. For example, David L. Hawksworth considered the definition imperfect because it is impossible to determine which one thallus is of a specific structure since thalli changed depending upon the substrate and conditions in which they developed. This researcher represents one of the main trends among lichenologists who consider it impossible to give a single definition to lichens since they are a unique type of organism.\n\nToday studies in lichenology are not restricted to the description and taxonomy of lichens but have application in various scientific fields. Especially important are studies on environmental quality that are made through the interaction of lichens with their environment. Lichen is extremely sensitive to various air pollutants, especially to sulphur dioxide, which causes acid rain and prevents water absorption. \n\nAlthough several species of lichen have been used in traditional medicine it was not until the early 20th century that modern science became interested in them. The discovery of various substances with antibacterial action in lichen thalli was essential for scientists to become aware of the possible importance of these organisms to medicine From the 1940s there appeared various works by the noted microbiologist Rufus Paul Burkholder who demonstrated antibacterial action of lichens of the genus \"Usnea\" against \"Bacillus subtilis\" and \"Sarcina lutea\". Studies showed that the substance that inhibited growth of bacteria was usnic acid. Something similar occurred with the substance Ramelina synthesised by the lichen \"Ramalina reticulata\", nevertheless, these substances proved ineffective against Gram negative bacteria such as \"Escherichia coli\" and \"Pseudomonas\". With these investigations the number of antibacterial substances and possible drug targets known to be produced by lichens increased ergosterol, usnic acid etc.\n\nInterest in the potential of substances synthesised by lichens increased with the end of World War II along with the growing interest in all antibiotic substances. In 1947 antibacterial action was identified in extracts of \"Cetraria islandica\" and the compounds identified as responsible for bacterial inhibition were shown to be d-protolichosteric acid and d-1-usnic acid. Further investigations have identified novel antibacterial substances, Alectosarmentin or Atranorin.\n\nAntibacterial action of substances produced by lichens is related to their ability to disrupt bacterial proteins with a subsequent loss of bacterial metabolic capacity. This is possible due to the action of lichen phenolics such as usnic acid derivatives.\n\nFrom the 1950s the lichen product usnic acid was the object of most antitumour research. These studies revealed some in vitro antitumour activity by substances identified in two common lichens \"Peltigera leucophlebia\" and \"Collema flaccidum\".\n\nRecent work in the field of applied biochemistry has shown some antiviral activity with some lichen substances. In 1989 K Hirabayashi presented his investigations on inhibitory lichen polysaccharides in HIV infection.\n\n\n\n", "id": "172384", "title": "Lichenology"}
{"url": "https://en.wikipedia.org/wiki?curid=38710316", "text": "Quantitative biology\n\nQuantitative biology is an umbrella term encompassing the use of mathematical, statistical or computational techniques to study life and living organisms. The central theme and goal of quantitative biology is the creation of predictive models based on fundamental principles governing living systems.\n\nThe subfields of biology that employ quantitative approaches include:\n", "id": "38710316", "title": "Quantitative biology"}
{"url": "https://en.wikipedia.org/wiki?curid=9819436", "text": "Actionbioscience\n\nAction Bioscience is a non-commercial, educational web site sponsored by the American Institute of Biological Sciences (AIBS) . It was created to promote bioscience literacy and bring attention to seven bioscience issues of critical current importance: 1) Biodiversity, 2) Environment, 3) Genomics, 4) Biotechnology, 5) Evolution, 6) New Frontiers, and 7) Science Education\n\nThe organization seeks to:\n\n\nTo meet these goals, the web site provides articles by scientists, science educators, and science students on issues related to seven bioscience challenges: environment, biodiversity, genomics, biotechnology, evolution, new frontiers in science, and bioscience education. In addition, the web site provides educators with original lessons and resources to enhance bioscience teaching. Up-to-date external links are provided at the bottom of each article to help the reader \"learn more\" about or \"get involved\" in the issue. Selected articles are translated into Spanish.\n\nIt won an award as one of the best sci-tech web sites by Scientific American.\n", "id": "9819436", "title": "Actionbioscience"}
{"url": "https://en.wikipedia.org/wiki?curid=4972643", "text": "AP Biology\n\nIn the United States, Advanced Placement Biology (commonly abbreviated to AP Biology or AP Bio), is a course and examination offered by the College Board to high school students as an opportunity to earn placement credit for a college-level biology course. For the 2012–2013 school year, the College Board unveiled a new curriculum with a greater focus on \"scientific practices.\"\n\nThis course is offered to highly motivated students who wish to pursue an interest in the life sciences. The College Board recommends successful completion of high school biology and high school chemistry before commencing AP Biology, although the actual prerequisites vary from school to school and from state to state. Many schools, for example, require no background in biology to take the course.\n\nTopics covered by this course include:\nIn addition to the standard biology topics above, students are required to be familiar with a set of 12 specific biology labs, as well as general lab procedure.\n\nThe makeup of the AP Biology exam is based on the following percentages of three topics:\n\n\nThe AP test for this course consists of two sections. Section I, administered over a period of 90 minutes, consists of 63 multiple-choice questions and 6 mathematics-related fill in questions. Students are allowed to use a four-function, scientific, or graphing calculator. Calculators are generally needed for this mathematics portion, and are allowed on the test day. Section II, which also lasts 90 minutes, consists of 2 essay prompts to be answered comprehensively by the student, as well as 6 shorter essays that require concise answers.\n\nIn the 2007 administration, 144,796 students took the exam from 8,486 schools. In the 2008 administration, the exam grades were recalibrated, resulting in a substantial decrease in the top scores and increase in the bottom scores. In the 2009 and 2010 administrations, 159,580 and 172,512 students took the test, respectively. The 2013 revised test showed a marked decrease in the number of 5s and 1s received.\n\nMost colleges award credit in an introductory biology course for a score of 3 or higher. Higher tier schools generally only accept a score of 4 or 5. \nThe grade distributions since 2008 are shown below:\n\n\n", "id": "4972643", "title": "AP Biology"}
{"url": "https://en.wikipedia.org/wiki?curid=29698161", "text": "Edinburgh BioQuarter\n\nThe Edinburgh BioQuarter is a bioscience community based near the Royal Infirmary of Edinburgh and the Queen's Medical Research Centre in Edinburgh, close to the Roslin Institute for Animal Biology. It is Scotland's key initiative in the development of its life sciences industry, which employs more than 30,000 people in 600 companies.\n\nIn 2007, Scottish Enterprise completed a series of land deals which cleared a 55-acre site for development adjacent to the existing Royal Infimary of Edinburgh and the creation of the BioQuarter was announced. In early 2010, Dr Mike Capaldi joined BioQuarter as Commercialisation Director and the development of an executive team and business creation programme were launched.\n\nThe key partners in the Edinburgh BioQuarter initiative are Scottish Enterprise/Scottish Development International, the University of Edinburgh and NHS Lothian.\n\nFrom 2011, it will be joined at this location by the Scottish Centre for Regenerative Medicine, giving the BioQuarter one of the highest concentrations of research classed officially by the government as \"internationally leading\" (four-star, according to the 2008 Research Assessment Exercise).\n\nEdinburgh BioQuarter currently co-locates an 1000+-bed teaching hospital, the Royal Infirmary of Edinburgh, with the Queen’s Medical Research Institute (ranked #1 in the UK for clinical medical research, according to the 2008 research assessment exercise) and the Clinical Research Imaging Centre, or CRIC, opened by HM the Duke of Edinburgh in late 2010.\n\nOver the last three years, Edinburgh has attracted 53% of all Wellcome Trust grants in Scotland and 62% of Medical Research Council (MRC) funding in Scotland, making it a major centre of research income.\n\nIn November 2011, a new bioincubator building will come on stream offering of office and laboratory space, as well as the new Scottish Centre for Regenerative Medicine (SCRM), headed by Professor Sir Ian Wilmut, creator of \"Dolly the Sheep.\" This new centre will apply Scotland’s world-leading stem cell research to new therapies for conditions such as Parkinson's disease, multiple sclerosis and other conditions.\n\nIn September 2010, author JK Rowling endowed research at the BioQuarter with a £10 million gift to create the Anne Rowling Centre for Multiple Sclerosis.\n\nBy 2016, the new Royal Hospital for Children and Young People will be located at the BioQuarter, together with the Scottish Mind and Body Institute, a cross-disciplinary centre aimed at finding cures or therapies for neurological conditions.\n\nPart of BioQuarter’s purpose is to create new companies based on medical research being undertaken in the NHS and at the University of Edinburgh. For this purpose it has created an innovation competition, offering prizes worth £45,000 to researchers in all disciplines at the University of Edinburgh and NHS Lothian. The first round of these prizes will be awarded in March 2011.\n\nCompanies currently located at or associated with the BioQuarter community include:\n\n\nFurther companies are expected to join the BioQuarter community over the next twelve months as the process of company formation gathers pace. In addition, licensing deals and other partnerships with major international pharmaceutical companies and equipment manufacturers will add to the number of organisations housed at the BioQuarter.\n\n\n", "id": "29698161", "title": "Edinburgh BioQuarter"}
{"url": "https://en.wikipedia.org/wiki?curid=1459978", "text": "Fetal pig\n\nFetal pigs are unborn pigs used in elementary as well as advanced biology classes as objects for dissection. Pigs, as a mammalian species, provide a good specimen for the study of physiological systems and processes due to the similarities between many pig and human organs.\n\nAlong with frogs and earthworms, fetal pigs are among the most common animals used in classroom dissection. There are several reasons for this, the main reason being that pigs, like humans, are mammals. Shared traits include common hair, mammary glands, live birth, similar organ systems, metabolic levels, and basic body form. They also allow for the study of fetal circulation, which differs from that of an adult. Secondly, fetal pigs are easy to obtain because they are by-products of the pork industry. Fetal pigs are the unborn piglets of sows that were killed by the meat packing industry. These pigs are not bred and killed for this purpose, but are extracted from the deceased sow’s uterus. Fetal pigs not used in classroom dissections are often used in fertilizer or simply discarded. Thirdly, fetal pigs are cheap, which is an essential component for dissection use by schools. They can be ordered for a little more than $25 at biological product companies. Fourthly, fetal pigs are easy to dissect because of their soft tissue and incompletely developed bones that are still made of cartilage. In addition, they are relatively large with well-developed organs that are easily visible. As long as the pork industry exists, fetal pigs will be relatively abundant, making them the prime choice for classroom dissections.\n\nSeveral peer-reviewed comparative studies have concluded that the educational outcomes of students who are taught basic and advanced biomedical concepts and skills using non-animal methods are equivalent or superior to those of their peers who use animal-based laboratories such as animal dissection.\n\nA systematic review concluded that students taught using non-animal methods demonstrated “superior understanding of complex biological processes, increased learning efficiency, and increased examination results.” It also reported that students’ confidence and satisfaction increased as did their preparedness for laboratories and their information-retrieval and communication abilities.\n\nThree studies at universities across the United States found that students who modeled body systems out of clay were significantly better at identifying the constituent parts of human anatomy than their classmates who performed animal dissection.\n\nAnother study found that students preferred using clay modeling over animal dissection and performed just as well as their cohorts who dissected animals.\n\nThe size of the fetal pig depends on the time allowed for the mother to gestate:\n\nNo studies have found significant data regarding the mother swine’s diet and fetal pig survival rate. However, there is a correlation between a mother pig having a nutritious diet containing proteins, vitamins and minerals during gestation period and the survival rate of piglets. The correlation, however, is not statistically different. Weight is also not a factor of survival rate because a healthier diet does not lead to a heavier offspring or a greater chance of live birth.\n\nThe placenta is used as a means of transferring nutrients from the mother to the fetus. The efficiency at which nutrients are transferred dictates the health and growth of the fetus. FRP, or fetal weight: placental weight ratio, was commonly used to determine placental efficiency. However, increasing FPR does not prove to increase litter size. Instead, a more accurate way of determining fetus growth is through certain characteristics of the placental lining. The placenta is made of a folded trophoblast/endometrial epithelial bilayer. The width and length of the placenta folds are positively related and increase as gestation progresses.\n\nThe width of the placental folds decreases until day 85 of gestation. From here, the width increases with gestation and is at its largest around day 105. The rate at which these folds increase is negatively related to fetus size. Thus, greater fold widths will be seen in smaller fetuses. Although increasing placental fold width does increase the interaction between fetus and mother, nutrient exchange is not most efficient in smaller fetal pigs, as would be expected. Many other factors, including depth of placental folds, are also responsible for these interactions.\n\nThe prenatal development of the fetus includes all the tissue and organ development. Within hours of mating, the sperm and egg undergo fertilization in the oviduct and three days later the egg moves into the uterus. The cells begin to specialize by day six, and attach themselves to the uterus lining by day eleven. From fertilization to day 18, the endoderm, ectoderm and mesoderm have been forming inside the embryo, and are completely formed by day 18, the same day the placenta forms. The endoderm transforms into the lungs, trachea, thyroid gland, and digestive tract of the fetus. The ectoderm has a greater role in the development of the fetus. It forms into the skin, nervous system, enamel of the teeth, lining of the intestine, mammary and sweat glands, hoofs, and hair. The mesoderm forms the major organ components that help keep the fetus alive. It forms the muscles and connective tissues of the body, blood vessels and cells, the skeleton, kidneys, adrenal glands, heart, and the reproductive organs. By day 20, most of the major organs are visible, and the last half of gestation focuses greatly on increasing the size of the fetuses.\n\nThe development of the lymphatic system and the formation of blood circulation occur at different stages of fetal pig development. The first lymphatic organ to become present is the thymus. Lymphocyte builds up in the spleen on the 70th day. By day 77, the thymus is already completely developed and is distinguishable from other organs. Also, follicles are present on the tongue and intestines on day 77. On the 84th day, Periarteriolar lymphoid sheaths appear in the fetal pig. By this time, the liver and bone marrow are active and functional.\n\nStudies have shown that litter size, the amount of floor space during the growing period, and the number of pigs the gilt, or female pig, is placed with while growing affect the reproduction rates of the gilts. Data from a study in 1976 by Nelson and Robinson showed that gilts from a small litter size ovulated more than the gilts from the larger litters. The study suggests stress plays a role in impacting the reproduction. The amount of floor space has been shown to impact the time it takes gilts to reach puberty. An adequate amount of floor space allowed the higher percentage of gilts to reach puberty sooner than those gilts who had less floor space. The gilts placed in smaller groups bore one more pig per litter than gilts in larger groups. Still, the environment in which the fetal gilt develops is significant to the reproductive and physiological development.\nAuthor: Brandon Anderson\n\nFetal pigs are often preserved in formaldehyde, a carcinogenic substance. A 1980 study found that exposure to formaldehyde could possibly cause nasal cancer in rats, leading to research on whether this was possible in humans or not. In 1995 it was concluded by the International Agency for Research on Cancer (IARC) that formaldehyde is a carcinogen for humans.\nIn Albania, fetal pigs are preserved by means of alcohol, and are used as an ingredient in Raki, a local moonshine liquor.\n\nThe anatomy of a fetal pig is similar to that of the adult pig in various aspects. Systems that are similar include the nervous, skeletal, respiratory (neglecting the under developed diaphragm), and muscular. Other important body systems have significant differences from the adult pig.\n\nThere are only a few differences between the circulatory system of an adult pig and a fetal pig, besides from the umbilical arteries and vein. There is a shunt between the wall of the right and left atrium called the foramen ovale. This allows blood to pass directly from the right to left atrium. There is also the ductus arterius which allows blood from the right atrium to be diverted to the aortic arch. Both of these shunts close a few minutes after birth.\n\nThe monogastric digestive system of the fetal pig harbors many similarities with many other mammals. The fetal pig's digestive organs are well developed before birth, although it does not ingest food. These organs include the esophagus, stomach, small and large intestines. Mesenteries serve to connect the organs of the fetal pig together. In order for digestion to occur, the fetal pig would have to ingest food. Instead, it gains much needed nutrition from the mother pig via the umbilical cord. In the adult pig, food will follow the general flow through the esophagus, which can be located behind the tracheae. From the oral cavity, the esophagus leads to the stomach, small intestine, and large intestine. Other organs developing during fetal pig development such as the gallbladder, pancreas and spleen are all critical in contributing to the overall flow of the digestive system because they contain digestive enzymes that will perform chemical digestion of food. After food is digested and nutrients are absorbed, the food follows through the large intestine and solid wastes are excreted through the anus. In the fetal pig however, the metabolic wastes are sent back to the mother through the umbilical cord where the mother excretes the wastes. Other remaining wastes remain in the fetal pig until birth. Then\n\nThe oral cavity of the fetal pig begins developing before birth. The tongue's taste buds, located in the enlarged papillae, facilitate food handling after birth. These taste buds develop during fetal development. Adult pigs have up to 15,000 taste buds, a much larger number than the average human tongue, which has 9,000.\n\nThe dental anatomy of the fetal pig shows differences from adult pigs. The fetal pig develops primary teeth (which are later replaced with permanent teeth). Some may erupt during fetal stage, which is why some of the fetuses show evidence of teeth. Depending on the age of the fetal pig, it is natural to see eruptions of third incisor and canine in the fetal pig. Because the fetal pigs were still in the mother’s uterus, teeth will still form which supports reasons for hollow unerupted teeth that may be seen. Similar to human dental anatomy, the overall dental anatomy of the pig consists of incisors, canines, pre-molars, and molars. Piglets can have 28 teeth total and adult pigs can have 44 teeth total.\n\nThe fetal pig's urogenital system is similar to the adult pig's system with the exception of the reproductive organs. The fetal pig urinary tract is relatively developed and easy to locate during dissection. The kidneys are located behind the abdominal organs and are partially embedded into the dorsal body wall by the spine. The ureters carry the urine to the urinary bladder, the large sack-like organ by the umbilical artery and vein, to the urethra. From there, the urine can be excreted.\n\nIf the fetal pig is a female, there will be a fleshy protrusion ventral near the anus called the genital papilla. The female's internal reproductive system is located below the kidneys. The two sac-like organs attached to the coil-like fallopian tubes are the ovaries. The uterus, which becomes the vagina, is located where the fallopian tubes meet. This system can be difficult to find as it is small as well as extremely dorsal and posterior to the other systems.\n\nMale fetal pigs have an urogenital opening located behind the umbilical cord. The swelling behind the hind legs of the fetal pig is the scrotum. The male's internal reproductive system has two scrotal sacs, which depending on the age of the fetal pig may or may not have developed testes. The epididymis coil on the testes connects to the vas deferens. The vas deferens crosses over the ureter and enters the urethra, which then connects to the penis located just posterior to the skin. Similar to the female system, it may be difficult to identify all parts.\n\n", "id": "1459978", "title": "Fetal pig"}
{"url": "https://en.wikipedia.org/wiki?curid=23615662", "text": "Canadian Stem Cell Foundation\n\nThe Canadian Stem Cell Foundation is an independent, non-profit organization established in 2008 and situated in Ottawa, Ontario. Stem Cell science is a Canadian innovation through the discovery of stem cells by Drs. James Till and Ernest McCulloch. It is globally known as the leading organization for stem cell research and support in the study of treatments and cures for diseases such as cancer, diabetes, blindness and stroke.\n\nTheir first strategy was created in 2013 to determine the concerns and actions required to develop an innovation that can advance stem cell research and clinics. The Canadian Stem Cell Foundation's goals are to invest a strategy for new treatments, sustainable healthcare, therapies and beneficial products. Their goals are beyond their capacity, such as \"using cells to treat respiratory heart diseases, restore lost vision, create a source of insulin-producing cells to treat diabetes, repair damaged spinal cords, reverse the effect of MS, Crohn's disease and other autoimmune disorders, reduce the ravages of Parkinson's disease and reverse tumour formation in the brain, breast and other solid tissues.\" Their other goals are to bring together scientists, institutions, health charities, industry partners, regulators, funders and philanthropists in a universal vision in the developments of stem cell science research and have public and private sectors support in the funding for stem cell research in the long-term.\n\nThere are many organizations involved such as the Stem Cell Network, Health Charities Coalition of Canada, Ontario Stem Cell Initiative, Centre for Commercialization of Regenerative Medicine, Ontario Bioscience Innovation Organization, and Cell CAN Regenerative Medicine and Cell Therapy Network.\n\nTo follow updates regarding \"The Canadian Stem Cell Strategy,\" visit the site: http://www.stemcellfoundation.ca/en/blog/categories/listings/strategy-updates\n\nThe Stem Cell Charter is an appeal launched in September, regarding the support for stem cell research. It is an interactive document that seeks concern for humanity with reference to stem cell science. It was created and written by, Bartha Maria Knoppers of McGill University, with a team of stem cell scientists, patients, ethicists and laypeople. This document is accessible to many individuals around the world, it is not a document for scientists, government officials or physicians, but for all. The charter includes five principles such as, \"responsible science, protection of citizens, intellectual freedom, transparency and integrity.\" The document's goal is to bring people together with the belief that stem cell science has the potential to conquer disease and advance medicine. Moreover, many significant scientists have endorsed the Charter such as, Mick Bhatia, Tim Caulfield, John Dick, Connie Eaves, Norman Iscove, Gordon Keller, Derek van der Kooy, Freda Miller, Andras Nagy, Janer Rossant, Michael Rudnicki, Guy Sauvageau, Jim Till and Sam Weiss.\n\nOver 3000 individuals have signed the charter, any individual can sign the charter by visiting the link: http://www.stemcellfoundation.ca/en/act/charter?view=default\n\nCanadian Stem Cell Foundation(2013) \"Canadian Stem Cell Foundation\" http://www.stemcellfoundation.ca/en/\n", "id": "23615662", "title": "Canadian Stem Cell Foundation"}
{"url": "https://en.wikipedia.org/wiki?curid=15919867", "text": "Francisco J. Ayala School of Biological Sciences\n\nThe Francisco J. Ayala School of Biological Sciences is one of the academic units of the University of California, Irvine (UCI). The school is divided into four departments: Developmental and Cell Biology, Ecology and Evolutionary Biology, Molecular Biology and Biochemistry, and Neurobiology and Behavior. With over 3,700 students it is in the top four largest schools in the university.\nIt is consistently ranked in the top one hundred in \"U.S. News & World Report’s\" yearly list of best graduate schools.\n\nThe Francisco J. Ayala School of Biological Sciences first opened in 1965 at the University of California, Irvine and was one of the first schools founded when the University campus opened. The school's founding Dean, Edward A. Steinhaus, had four founding department chairs and started out with 17 professors.\n\nOn March 12, 2014, the School was officially renamed after UCI professor and donor Francisco J. Ayala by Chancellor Michael V. Drake.\n", "id": "15919867", "title": "Francisco J. Ayala School of Biological Sciences"}
{"url": "https://en.wikipedia.org/wiki?curid=42036843", "text": "Biological illustration\n\nBiological illustration is the use of technical illustration to visually communicate the structure and specific details of biological subjects of study. This can be used to demonstrate anatomy, explain biological functions or interactions, direct surgical procedures, distinguish species, and other applications. The scope of biological illustration can range from the whole organism level to microscopic.\n\nTypes of biological illustrations include:\n\nHistorically, biological illustrations have been in use since the beginning of man's exploration and attempts to understand the world around him. The paleolithic cave paintings were so detailed that we can even recognize species and breeds of many of the depicted animals today. For example, in the Chauvet-Pont-d'Arc Cave (circa 30,000 BC), at least 13 different species have been identified. In one prehistoric cave (circa 15,000 BC), there is a drawing of a mammoth with a darkened area where the heart should be. If this is indeed the intention of the illustration, it would be the world's first anatomical illustration.\n\nIn the Alexandrian era (356 – 323 BC), the Greek physician Herophilus, now known as the father of anatomy, performed public dissections and recorded his findings. In the 1st century AD, Pedanius Dioscorides compiled the \"De Materia Medica\", a collection of medicinal information and recipes, containing illustrations of about 600 plants in all.\n\nDuring the Renaissance, artist and scientist Leonardo DaVinci famously sketched his observations from human dissections, as well as his studies of plants and the flight of birds. In the mid-16th century, the physician Andreas Vesalius compiled and published the \"De humani corporis fabrica\", a collection of textbooks on human anatomy superior to any illustrations that had been produced until that point. In the early 1600s, the explorer Étienne de Flacourt documented his travels to Madagascar, and illustrated the unique fauna there, setting a precedent for future explorers as world travel became a more feasible reality.\n\nDuring his five-year voyage of the HMS Beagle, Charles Darwin wrote and illustrated \"The Voyage of the Beagle\", which was published in 1839. In the beginning of the 20th century, one of the most prolific biological illustrator, Ernst Haeckel, discovered, described, and named thousands of new species, and his published work, \"Kunstformen der Natur\", contained hundreds of prints of various organisms, many of which were first described by Haeckel himself.\n\nBiological illustrations can be found in use in history and anatomy textbooks, nature guides, natural history museums, scientific magazines and journals, botanical gardens, zoos and aquariums, surgical training manuals, and many more applications. Biological illustration can be pursued as a degree in the undergraduate, graduate, and technical college levels. Preparation for a biological illustration career can include a background of art or science, or a combination of both. Skills development in biological illustration can involve two-dimensional art, animation, graphic design, and sculpture (such as necessary in custom prosthetics).\n\nIt is possible to work in biological illustration without a specific degree, but a degree will significantly enhance an illustrator's employment opportunities. Job applications can be submitted to scientific researchers, publishers of scientific manuscripts, research institutions, museums, scientific foundations, commercial book publishers or university presses, individual authors, hospitals and medical training centers, local and state government offices, park services, environmental control offices, special government committees, printers and commercial publishing houses. Employment opportunities in the biological illustration profession are fairly limited, full-time jobs are not often available, and many experienced illustrators are self-employed, on short-term contracts, or work in science communication careers with few illustration duties. Many illustrators prefer the flexibility of their own working arrangements, but this is only possible when they are well established in the field and capable of locating work when needed. Many freelance illustrators supplement their salary with commercial illustration and graphic design projects, as is common in many art careers.\n\nBiological illustration has traditionally employed the techniques of using carbon dust, color pencil, stipple pen and ink, lithography, watercolor and gouache, however digital illustration has recently become more important in the field. Every professional scientific illustration begins with multiple rough sketches. Many details must be discussed between the artist and scientist before a final drawing can be completed, and additional preliminary drawings must be prepared in order to work out aesthetic details.\n\nPen and ink (often a flex nib fountain pen) line illustrations are clean, crisp, clear, and inexpensive to produce, making them ideal for biological illustrations. Ink drawings are typically made on a heavy drawing paper, such as Bristol board.\n\nDigital illustration can be done using a monitor or drawing tablet, computer software such as Adobe Illustrator or Photoshop, or it can be used in post-production after the illustration has been drawn by hand.\n\n", "id": "42036843", "title": "Biological illustration"}
{"url": "https://en.wikipedia.org/wiki?curid=49032506", "text": "SensUs\n\nSensUs is the yearly international student competition on molecular biosensors for healthcare applications. Teams of students at international universities design and build biosensing systems, and demonstrate and present their prototypes to an audience during a public final contest event. The competition aims to stimulate education and innovation in the field of biosensing, with a worldwide scope. The long term aim is to improve the quality of life of patients.\n\nEach year, SensUs defines a theme that represents a current healthcare problem in society. A biomarker is selected and the participating teams have 9 months to develop a biosensor that can measure the selected biomarker. The participating teams have the same functional end goal, but develop creative solutions and unique biosensor prototypes. The prototypes are shown, shared and discussed at the annual SensUs event. \nSensUs is an international platform for learning and innovation in the field of molecular biosensors for healthcare applications. \n\nSensUs 2016 was the first edition of the competition. The aim was to develop biosensors for the measurement of creatinine in blood plasma, which is a key biomarker for Kidney failure. The student teams originated from five universities: University of Leuven, Imperial College London, Uppsala University, Technical University of Denmark, and Eindhoven University of Technology. The teams had worked during 9 months to develop biosensors in their own universities, and thereafter all came to the final contest event on the 9 and 10 of September 2016 at Eindhoven University of Technology. Here they could win four awards; the Analytical Performance award (AP), the Creativity Award (Cr), the Translational Potential award (TP) and the Public Inspiration award (PI). Photos and videos from SensUs 2016 can be found here.\n\nSensUs 2017 was the second edition of the yearly competition. The finals took place on the 8 and 9 of September 2017. Ten student teams from three continents built a biosensor to detect NT-proBNP in blood plasma, which is an important biomarker for Heart failure.\n\nDuring the event the teams showed their biosensors to the public, measured NT-proBNP live and presented pitches on technology and on translation into society. \nThe teams were judged by an international multidisciplinary jury, who conferred awards for Analytical Performance (AP), Creativity (Cr), and Translational Potential (TP). \n\nAbout 500 people visited the event at Eindhoven University of Technology. The event was broadcast on the SensUs Digital platform, including livestreams, live data, Q&A, photos and videos and voting functionality. The cumulative visitor count on SensUs digital was 5000 from 74 countries, of which 1500 people casted their votes for the Public Inspiration award (PI). Photos of SensUs 2017 can be found here.\n\nSensUs 2018 will be the third edition of the SensUs competition. The theme is antibiotic testing for improved healthcare. The biomarker that will be measured by the teams is vancomycin, an important antibiotic drug. Participants are the ten universities of SensUs 2017 and three new universities, from Canada (Université de Montréal), China (Zhejiang University) and Spain (UPC-BarcelonaTech). \nThe finals of SensUs 2018 will take place on the 7 and 8 of September 2018 in Eindhoven, the Netherlands.\n\n", "id": "49032506", "title": "SensUs"}
{"url": "https://en.wikipedia.org/wiki?curid=18322812", "text": "SAT Subject Test in Biology E/M\n\nThe SAT Subject Test in Biology is the name of a one-hour multiple choice test given on biology by The College Board. A student chooses whether to take the test depending upon college entrance requirements for the schools in which the student is planning to apply. Until 1994, the SAT Subject Tests were known as Achievement Tests; and from 1995 until January 2005, they were known as SAT IIs. Of all SAT subject tests, the Biology E/M test is the only SAT II that allows the test taker a choice between the ecological or molecular tests. A set of 60 questions is taken by all test takers for Biology and a choice of 20 questions is allowed between either the E or M tests. This test is graded on a scale between 200 and 800. The average for Molecular is 630 while Ecological is 591.\n\nThis test has 80 multiple-choice questions that are to be answered in one hour. All questions have five answer choices. Students receive one point for each correct answer, lose ¼ of a point for each incorrect answer, and receive 0 points for questions left blank. The student's score is based entirely on his or her performance in answering the multiple-choice questions.\n\nThe questions cover a broad range of topics in general biology. There are more specific questions related respectively on ecological concepts (such as population studies and general Ecology) on the E test and molecular concepts such as DNA structure, translation, and biochemistry on the M test.\n\nThe College Board suggests a year-long course in biology at the college preparatory level, as well as a one-year course in algebra, and lab experience as preparation for the test. The test requires understanding of biological data and concepts, science-related terms, and the ability to effectively synthesize and interpret data from charts, maps, and other visual media. However, most questions from this test are derived from, or are similar to, the pre-2012 AP Biology multiple choice questions. By taking an AP class or a class with similar rigor, one's chances at doing well on this test should improve.\n\n", "id": "18322812", "title": "SAT Subject Test in Biology E/M"}
{"url": "https://en.wikipedia.org/wiki?curid=342094", "text": "List of gene families\n\nThis is a list of gene families or gene complexes, that is sets of genes which occur across a number of different species which often serve similar biological functions. These gene families typically encode functionally related proteins, and sometimes the term gene families is a shorthand for the sets of proteins that the genes encode. They may or may not be physically adjacent on the same chromosome.\n\n\n\n\n\n\n\n", "id": "342094", "title": "List of gene families"}
{"url": "https://en.wikipedia.org/wiki?curid=1936485", "text": "Serotype\n\nA serotype or serovar is a distinct variation within a species of bacteria or virus or among immune cells of different individuals. These microorganisms, viruses, or cells are classified together based on their cell surface antigens, allowing the epidemiologic classification of organisms to the sub-species level. A group of serovars with common antigens is called a serogroup or sometimes \"serocomplex\".\n\nSerotyping often plays an essential role in determining species and subspecies. The \"Salmonella\" genus of bacteria, for example, has been determined to have over 2600 serotypes, including \"Salmonella enterica\" serovar Typhimurium, \"S. enterica\" serovar Typhi, and \"S. enterica\" serovar Dublin. \"Vibrio cholerae\", the species of bacteria that causes cholera, has over 200 serotypes, based on cell antigens. Only two of them have been observed to produce the potent enterotoxin that results in cholera: 0:1 and 0:139.\n\nSerotypes were discovered by the American microbiologist Rebecca Lancefield in 1933.\n\nThe immune system is capable of discerning a cell as being 'self' or 'non-self' according to that cell's serotype. In humans, that serotype is largely determined by human leukocyte antigen (HLA), the human version of the major histocompatibility complex. Cells determined to be non-self are usually recognized by the immune system as foreign, causing an immune response, such as hemagglutination. Serotypes differ widely between individuals; therefore, if cells from one human (or animal) are introduced into another random human, those cells are often determined to be non-self because they do not match the self-serotype. For this reason, transplants between genetically non-identical humans often induce a problematic immune response in the recipient, leading to transplant rejection. In some situations this effect can be reduced by serotyping both recipient and potential donors to determine the closest HLA match.\n\nThe Kauffman–White classification scheme is the basis for naming the manifold serovars of \"Salmonella\". To date, more than 2600 different serotypes have been identified. A \"Salmonella\" serotype is determined by the unique combination of reactions of cell surface antigens. The \"O\" antigen is determined by the outermost portion of the Lipopolysaccharide (LPS) and the \"H\" antigen is based on the flagellar (protein) antigens. \nThere are two species of \"Salmonella\": \"Salmonella bongori\" and \"Salmonella enterica\". \"Salmonella enterica\" can be subdivided into six subspecies.\nThe process to identify the serovar of the bacterium consists of finding the formula of cell antigens which represent the variations of the bacteria. First of all, the use of the agglutination method on slides is required in order to get the antigen formula. The agglutination between the antigen and the antibody is made with a specific antisera, which reacts with the antigen to produce a mass. The antigen O is tested with a bacterial suspension from an agar plate whereas the antigen H is tested with a bacterial suspension from a broth culture. The scheme is used to classify the serovar depending on its antigen formula obtained with the reaction of the agglutination.\n\n\n", "id": "1936485", "title": "Serotype"}
{"url": "https://en.wikipedia.org/wiki?curid=711718", "text": "Evolutionary taxonomy\n\nEvolutionary taxonomy, evolutionary systematics or Darwinian classification is a branch of biological classification that seeks to classify organisms using a combination of phylogenetic relationship (shared descent), progenitor-descendant relationship (serial descent), and degree of evolutionary change. This type of taxonomy may consider whole taxa rather than single species, so that groups of species can be inferred as giving rise to new groups. The concept found its most well-known form in the modern evolutionary synthesis of the early 1940s.\n\nEvolutionary taxonomy differs from strict pre-Darwinian Linnaean taxonomy (producing orderly lists only), in that it builds evolutionary trees. While in phylogenetic nomenclature each taxon must consist of a single ancestral node and all its descendants, evolutionary taxonomy allows for groups to be excluded from their parent taxa (e.g. dinosaurs are not considered to \"include\" birds, but to have \"given rise\" to them), thus permitting paraphyletic taxa.\n\nEvolutionary taxonomy arose as a result of the influence of the theory of evolution on Linnaean taxonomy. The idea of translating Linnaean taxonomy into a sort of dendrogram of the Animal and Plant Kingdoms was formulated toward the end of the 18th century, well before Charles Darwin's book \"On the Origin of Species\" was published. The first to suggest that organisms had common descent was Pierre-Louis Moreau de Maupertuis in his 1751 \"Essai de Cosmologie\", Transmutation of species entered wider scientific circles with Erasmus Darwin's 1796 Zoönomia and Jean-Baptiste Lamarck's 1809 \"Philosophie Zoologique\". The idea was popularised in the English-speaking world by the speculative but widely read \"Vestiges of the Natural History of Creation\", published anonymously by Robert Chambers in 1844.\n\nFollowing the appearance of \"On the Origin of Species\", Tree of Life representations became popular in scientific works. In \"On the Origin of Species\", the ancestor remained largely a hypothetical species; Darwin was primarily occupied with showing the principle, carefully refraining from speculating on relationships between living or fossil organisms and using theoretical examples only. In contrast, Chambers had proposed specific hypotheses, the evolution of placental mammals from marsupials, for example.\n\nFollowing Darwin's publication, Thomas Henry Huxley used the fossils of \"Archaeopteryx\" and \"Hesperornis\" to argue that the birds are descendants of the dinosaurs. Thus, a group of extant animals could be tied to a fossil group. The resulting description, that of dinosaurs \"giving rise to\" or being \"the ancestors of\" birds, exhibits the essential hallmark of evolutionary taxonomic thinking.\n\nThe past three decades have seen a dramatic increase in the use of DNA sequences for reconstructing phylogeny and a parallel shift in emphasis from evolutionary taxonomy towards Hennig’s ‘phylogenetic systematics’.\n\nToday, with the advent of modern genomics, scientists in every branch of biology make use of molecular phylogeny to guide their research. One common method is multiple sequence alignment.\n\nCavalier-Smith, G. G. Simpson and Ernst Mayr are some representative evolutionary taxonomists.\n\nEfforts in combining modern methods of cladistics, phylogenetics, and DNA analysis with classical views of taxonomy have recently appeared. Certain authors have found that phylogenetic analysis is acceptable scientifically as long as paraphyly at least for certain groups is allowable. Such a stance is promoted in papers by Tod F. Stuessy and others. A particularly strict form of evolutionary systematics has been presented by Richard H. Zander in a number of papers, but summarized in his \"Framework for Post-Phylogenetic Systematics\".\n\nBriefly, Zander's pluralistic systematics is thus: A method that cannot falsify a hypothesis is as unscientific as a hypothesis that cannot be falsified. Cladistics generates only trees of shared ancestry, not serial ancestry. Taxa evolving seriatim cannot be dealt with by analyzing shared ancestry with cladistic methods. Hypotheses such as adaptive radiation from a single ancestral taxon cannot be falsified with cladistics. Cladistics offers a way to cluster by trait transformations but no evolutionary tree can be entirely dichotomous. Phylogenetics posits shared ancestral taxa as causal agents for dichotomies yet there is no evidence for the existence of such taxa. Molecular systematics uses DNA sequence data for tracking evolutionary changes, thus paraphyly and sometimes phylogenetic polyphyly signal ancestor-descendant transformations at the taxon level, but otherwise molecular phylogenetics makes no provision for extinct paraphyly. Additional transformational analysis is needed to infer serial descent.\n\nThe Besseyan cactus or commagram is the best evolutionary tree for showing both shared and serial ancestry. First, a cladogram or natural key is generated. Generalized ancestral taxa are identified and specialized descendant taxa are noted as coming off the lineage with a line of one color representing the progenitor through time. A Besseyan cactus or commagram is then devised that represents both shared and serial ancestry. Progenitor taxa may have one or more descendant taxa. Support measures in terms of Bayes factors may be given, following Zander's method of transformational analysis using decibans.\n\nCladistic analysis groups taxa by shared traits but incorporates a dichotomous branching model borrowed from phenetics. It is essentially a simplified dichotomous natural key, although reversals are tolerated. The problem, of course, is that evolution is not necessarily dichotomous. An ancestral taxon generating two or more descendants requires a longer, less parsimonious tree. This is why a tree based solely on shared traits is not called an evolutionary tree but merely a cladistic tree. This tree reflects to a large extent evolutionary relationships through trait transformations but ignores relationships made by species-level transformation of extant taxa.\n\nPhylogenetics attempts to inject a serial element by postulating ad hoc, undemonstrable shared ancestors at each node of a cladistic tree. There are in number, for a fully dichotomous cladogram, one less invisible shared ancestor than the number of terminal taxa. We get, then, in effect a dichotomous natural key with an invisible shared ancestor generating each couplet. This cannot imply a process-based explanation without justification of the dichotomy, and supposition of the shared ancestors as causes. The cladistic form of analysis of evolutionary relationships cannot falsify any genuine evolutionary scenario incorporating serial transformation, according to Zander.\n\nZander has detailed methods for generating support measures for molecular serial descent and for morphological serial descent using Bayes factors through deciban addition.\n\nAs more and more fossil groups were found and recognized in the late 19th and early 20th century, palaeontologists worked to understand the history of animals through the ages by linking together known groups. The Tree of life was slowly being mapped out, with fossil groups taking up their position in the tree as understanding increased.\n\nThese groups still retained their formal Linnaean taxonomic ranks. Some of them are paraphyletic in that, although every organism in the group is linked to a common ancestor by an unbroken chain of intermediate ancestors within the group, some other descendants of that ancestor lie outside the group. The evolution and distribution of the various taxa through time is commonly shown as a \"spindle diagram\" (often called a \"Romerogram\" after the American palaeontologist Alfred Romer) where various spindles branch off from each other, with each spindle representing a taxon. The width of the spindles are meant to imply the abundance (often number of families) plotted against time.\n\nVertebrate palaeontology had mapped out the evolutionary sequence of vertebrates as currently understood fairly well by the closing of the 19th century, followed by a reasonable understanding of the evolutionary sequence of the plant kingdom by the early 20th century. The tying together of the various trees into a grand Tree of Life only really became possible with advancements in microbiology and biochemistry in the period between the World Wars.\n\nThe two approaches, evolutionary taxonomy and the phylogenetic systematics derived from Willi Hennig, differ in the use of the word \"monophyletic\". For evolutionary systematicists, \"monophyletic\" means only that a group is derived from a single common ancestor. In phylogenetic nomenclature, there is an added caveat that the ancestral species and all descendants should be included in the group. The term \"holophyletic\" has been proposed for the latter meaning. As an example, amphibians are monophyletic under evolutionary taxonomy, since they have arisen from fishes only once. Under phylogenetic taxonomy, amphibians do not constitute a monophyletic group in that the amniotes (reptiles, birds and mammals) have evolved from an amphibian ancestor and yet are not considered amphibians. Such paraphyletic groups are rejected in phylogenetic nomenclature, but are considered a signal of serial descent by evolutionary taxonomists.\n", "id": "711718", "title": "Evolutionary taxonomy"}
{"url": "https://en.wikipedia.org/wiki?curid=28734047", "text": "Animalia Paradoxa\n\nThese 10 taxa appear in the 1st to 5th editions:\n\nThe above 10 taxa and the 4 taxa following were in the 2nd (1740) edition and the 4th and 5th editions (total 14 entries):\n", "id": "28734047", "title": "Animalia Paradoxa"}
{"url": "https://en.wikipedia.org/wiki?curid=6198887", "text": "ARKive\n\nARKive is a global initiative with the mission of \"promoting the conservation of the world's threatened species, through the power of wildlife imagery\", which it does by locating and gathering films, photographs and audio recordings of the world's species into a centralised digital archive. Its current priority is the completion of audio-visual profiles for the c. 17,000 species on the IUCN Red List of Threatened Species.\n\nThe project is an initiative of Wildscreen, a UK-registered educational charity, based in Bristol. The technical platform was created by Hewlett Packard, as part of the HP Labs' Digital Media Systems research programme.\n\nARKive has the backing of leading conservation organisations, including BirdLife International, Conservation International, International Union for Conservation of Nature (IUCN), The United Nations' World Conservation Monitoring Centre (UNEP-WCMC), and the World Wide Fund for Nature (WWF), as well as leading academic and research institutions, such as the Natural History Museum; Royal Botanic Gardens, Kew; and the Smithsonian Institution. It is a member of the Institutional Council of the Encyclopedia of Life.\n\nTwo ARKive layers for Google Earth, featuring endangered species and species in the Gulf of Mexico have been produced by Google Earth Outreach. The first of these was launched in April 2008 by Wildscreen's Patron, Sir David Attenborough.\n\nThe project formally was launched on 20 May 2003 by its patron, the UK-based natural history presenter, Sir David Attenborough, a long-standing colleague and friend of its chief instigator, the late Christopher Parsons, a former Head of the BBC Natural History Unit. Parsons never lived to see the fruition of the project, succumbing to cancer in November 2002 at the age of 70.\n\nParsons identified a need to provide a centralised safe haven for wildlife films and photographs after discovering that many such records are held in scattered, non-indexed, collections, often with little or no public access, and sometimes in conditions that could lead to loss or damage. He believed the records could be a powerful force in building environmental awareness by bringing scientific names to life. He also saw their preservation as an important educational resource and conservation tool, not least because extinction rates and habitat destruction could mean that images and sounds might be the only legacy of some species’ existence.\n\nHis vision of a permanent, accessible, refuge for audio-visual wildlife material won almost immediate support from many of the world’s major broadcasters, including the BBC, Granada, international state broadcasting corporations and \"National Geographic\" magazine; leading film and photographic libraries, international conservation organisations and academic institutes such as Cornell University.\n\nThe initial feasibility study for creating ARKive was carried out in the late 1980s by conservationist John Burton, but at the time the costs of the technology needed were too far too high, and so it was over a decade later, after the technology had caught up with Christopher Parson's vision (and the costs dropped), that the project was able to get off the ground.\n\nAfter capital development funds of £2m were secured from the Heritage Lottery Fund in 1997 and New Opportunities Fund in 2000, work on building ARKive began as part of the UK's Millennium celebrations, using advanced computerised storage and retrieval technology devised for the project by Hewlett-Packard., with an initial capacity of up to 74 terabytes of data, using redundant hardware and multiple copies of media stored at multiple sites. Media is digitised to the highest available quality without compression and encoded to open standards.\n\nA prototype site was online as early as April 1999. There were several design iterations before the formal launch.\n\nBy the launch date, the project team had researched, catalogued, copied, described and authenticated image, sound and fact files of 1,000 animals, plants and fungi, many of them critically endangered. More multi-media profiles are added every month, starting with British flora and fauna and with species included on the Red List – that is, species that are believed to be closest to extinction, according to research by the World Conservation Union. By January 2006, the database had grown to 2,000 species, 15,000 still images and more than 50 hours of video. By 2010, over 5,500 donors had contributed 70,000 film clips and photos of more than 12,000 species.\n\nThe site was Sunday Times website of the year for 2005. It was a 2010 Webby Award honoree for its outstanding calibre of work, in the 'Education' category, and a 2010 Association of Educational Publishers 'Distinguished Achievement Award' winner, in the category for websites for 9-12 year olds.\n\n\n", "id": "6198887", "title": "ARKive"}
{"url": "https://en.wikipedia.org/wiki?curid=164897", "text": "Three-domain system\n\nThe three-domain system is a biological classification introduced by Carl Woese \"et al.\" in 1977 that divides cellular life forms into archaea, bacteria, and eukaryote domains. In particular, it emphasizes the separation of prokaryotes into two groups, originally called \"Eubacteria\" (now \"Bacteria\") and \"Archaebacteria\" (now \"Archaea\"). Woese argued that, on the basis of differences in 16S rRNA genes, these two groups and the eukaryotes each arose separately from an ancestor with poorly developed genetic machinery, often called a progenote. To reflect these primary lines of descent, he treated each as a domain, divided into several different kingdoms. Woese initially used the term \"kingdom\" to refer to the three primary phylogenic groupings, and this nomenclature was widely used until the term \"domain\" was adopted in 1990.\n\nParts of the three-domain theory have been challenged by scientists such as Radhey Gupta, who argues that the primary division within prokaryotes should be between those surrounded by a single membrane, and those with two membranes.\n\nThe three-domain system adds a level of classification (the domains) \"above\" the kingdoms present in the previously used five- or six-kingdom systems. This classification system recognizes the fundamental divide between the two prokaryotic groups, insofar as archaea appear to be more closely related to eukaryotes than they are to other prokaryotic bacteria. The current system has the following listed kingdoms in the three domains:\n\nDomain Archaea – prokaryotic, no nuclear membrane, distinct biochemistry and RNA markers from bacteria, possess unique ancient evolutionary history for which they are considered some of the oldest species of organisms on Earth; traditionally classified as archaebacteria; often characterized by living in extreme environments. Some examples of archaeal organisms are methanogens which produce the gas methane, halophiles which live in very salty water, and thermoacidophiles which thrive in acidic high temperature water.\n\nDomain Bacteria – prokaryotic, consists of prokaryotic cells possessing primarily \"diacyl glycerol diester lipids\" in their membranes and bacterial rRNA, no nuclear membrane, traditionally classified as bacteria. Most of the known pathogenic prokaryotic organisms belong to bacteria (see for exceptions), and are currently studied more extensively than Archaea. Some examples of bacteria include Cyanobacteria photosynthesizing bacteria that are related to the chloroplasts of eukaryotic plants and algae, Spirochaetes – Gram-negative bacteria that include those causing syphilis and Lyme disease, and Actinobacteria --Gram-positive bacteria including \"Bifidobacterium animalis\" which is present in the human large intestine.\n\nDomain Eukarya – eukaryotes, organisms that contain a membrane-bound nucleus. An inexhaustive list of eukaryotic organisms includes:\n\nEach of the three cell types tends to fit into recurring specialties or roles. Bacteria tend to be the most prolific reproducers, at least in moderate environments. Archaeans tend to adapt quickly to extreme environments, such as high temperatures, high acids, high sulfur, etc. This includes adapting to use a wide variety of food sources. Eukaryotes are the most flexible with regard to forming cooperative colonies, such as in multi-cellular organisms, including humans. In fact, the structure of a Eukaryote is likely to have derived from a joining of different cell types, forming organelles.\n\nParakaryon myojinensis (\"incertae sedis\") is a single-celled organism known by a unique example. \"[T]his organism appears to be a life form distinct from prokaryotes and eukaryotes\", with features of both.\n\nParts of the three-domain theory have been challenged by scientists including Ernst Mayr, Thomas Cavalier-Smith and Radhey Gupta. In particular, Gupta argues that the primary division within prokaryotes should be among those surrounded by a single membrane (monoderm), including gram-positive bacteria and archaebacteria, and those with an inner and outer cell membrane (diderm), including gram-negative bacteria. He claims that sequences of features and phylogenies from some highly conserved proteins are inconsistent with the three-domain theory, and that it should be abandoned despite its widespread acceptance.\n\nRecent work has proposed that Eukarya may have actually branched off from the domain Archea. According to Spang et al. Lokiarchaeota forms a monophyletic group with eukaryotes in phylogenomic analyses. The associated genomes also encode an expanded repertoire of eukaryotic signature proteins that are suggestive of sophisticated membrane remodelling capabilities. These data suggest a two-domain system as opposed to the classical three-domain system.\n", "id": "164897", "title": "Three-domain system"}
{"url": "https://en.wikipedia.org/wiki?curid=10705831", "text": "Isochore (genetics)\n\nIn genetics, an isochore is a large region of DNA (greater than 300 kb) with a high degree uniformity in guanine (G) and cytosine (C): G-\nC and C-G (collectively GC content).\n\nBernardi and colleagues first uncovered the compositional non-uniformity within vertebrate genomes using thermal melting and density gradient\ncentrifugation. The DNA\nfragments extracted by the gradient centrifugation were later termed \"isochores\", which was subsequently defined as \"very long (much greater than 200 KB) DNA segments\" that \"are fairly homogeneous in base composition and belong to a small number of major classes\ndistinguished by differences in guanine-cytosine (GC) content\". Subsequently, the isochores \"grew\"\nand were claimed to be \">300 kb in size.\" The theory proposed that isochore’s\ncomposition varied markedly between \"warm-blooded\" (homeotherm) vertebrates and \"cold-blooded\" (poikilotherm) vertebrates and later became\nknown as the isochore theory.\n\nThe isochore theory purported that the genome of \"warm-blooded\" vertebrates (mammals and birds) are mosaics of long isochoric regions of\nalternating GC-poor and GC-rich composition, as opposed to the genome of \"cold-blooded\" vertebrates (fishes and amphibians) that were supposed to\nlack GC-rich isochores. These findings were explained by the thermodynamic stability hypothesis, attributing genomic structure to body temperature. GC-rich isochores were purported to be a form\nof adaptation to environmental pressures, as an increase in genomic GC-content could protect DNA, RNA, and proteins from degradation by heat.\nDespite its attractive simplicity, the thermodynamic stability hypothesis has been repeatedly shown to be in error \n. Many\nauthors showed the absence of a relationship between temperature and GC-content in vertebrates, while others showed the existence of GC-rich domains in \"cold-blooded\"\nvertebrates such as crocodiles, amphibians, and fish.\n\nThe isochore theory was the first to identify the nonuniformity of nucleotide composition within vertebrate genomes and predict that the genome\nof \"warm-blooded\" vertebrates such as mammals and birds are mosaic of isochores (Bernardi et al. 1985). The human genome, for example, was\ndescribed as a mosaic of alternating low and high GC content isochores belonging to five compositional families, L1, L2, H1, H2, and H3, whose\ncorresponding ranges of GC contents were said to be <38%, 38%-42%, 42%-47%, 47%-52%, and >52%, respectively.\n\nThe main predictions of the isochore theory are that: \n\nTwo opposite explanations that endeavored to explain the formations of isochores were vigorously debated\nas part of the neutralist-selectionist controversy. The first view was that isochores reflect variable mutation processes among genomic regions consistent with the neutral model. Alternatively,\nisochores were posited as a result of natural selection for certain compositional environment required by certain genes. Several hypotheses derive from the\nselectionist view, such as the thermodynamic stability hypothesis and the biased gene conversion\nhypothesis. Thus far, none of the theories\nprovides a comprehensive explanation to the genome structure, and the topic is still under debate.\n\nThe isochore theory became one of the most useful theories in molecular evolution for many\nyears. It was the first and most comprehensive attempt to explain the long-range compositional heterogeneity of vertebrate genomes within an\nevolutionary framework. Despite the interest in the early years in the isochore model, in recent years, the theory’s methodology, terminology,\nand predictions have been challenged.\n\nBecause this theory was proposed in the past century before complete genomes were sequences, it could not be fully tested for nearly 30 years. In\nthe beginning of the 21st century, when the first genomes were made available it was clear that isochores do not exist in the human genome \nnor in other mammalian genomes. When failed to find isochores,\nmany attacked the very existence of isochores. The most important predictor of\nisochores, GC3 was shown to have no predictable power to the GC content of nearby genomic\nregions, refuting findings from over 30 years of research, which were the basis for many isochore studies. Isochore-originators replied that the\nterm was misinterpreted as isochores are not \"homogeneous\" but rather \"fairly homogeneous regions\" with \"a heterogeneous nature (especially) of GC-rich regions at the 5 kb scale\", which only added to the already\ngrowing confusion. The reason for this ongoing frustration was the ambiguous definition of isochores as \"long\" and \"homogeneous\",\nallowed some researchers to discover \"isochores\" and others to dismiss them, although both camps used the same data.\n\nThe unfortunate side effect of this controversy was an \"arms race\" in which isochores are frequently redefined and relabeled following\nconflicting findings that failed to reveal \"mosaic of isochores.\" The\nunfortunate outcomes of this controversy and the following terminological-methodological mud were the loss of interest in isochores by the\nscientific community. When the most important core-concept in isochoric literature, the thermodynamic stability hypothesis, was rejected, the\ntheory lost its appeal. Even today, there is no clear definition to isochores nor is there an algorithm that detects isochores. Isochores are detected manually by visual inspection of GC content curves\n, however because this approach lacks scientific merit and\nis difficult to replicate by independent groups, the findings remain disputed.\n\nAs the study of isochores was \"de facto\" abandoned by most scientists, an alternative theory was\nproposed to describe the compositional organization of genomes in accordance with the most recent genomic studies. The Compositional Domain Model depicts genomes as a medley of short and long homogeneous and nonhomogeneous domains. The theory defines \"compositional domains\" as genomic regions with distinct GC-contents as determined by a computational segmentation algorithm. The homogeneity of compositional domains is compared to that of the chromosome on which they reside using the F-test, which separated them into compositionally homogeneous domains and compositionally nonhomogeneous domains based on the outcome of test. Compositionally homogeneous domains that are sufficiently long (≥ 300 kb) are termed isochores or isochoric domains. These terms are in accordance with the literature as they provide clear distinction between isochoric- and nonisochoric-domains.\n\nA comprehensive study of the human genome unraveled a genomic organization where two-thirds of the genome is a mixture of many short compositionally homogeneous domains and relatively few long ones. The remaining portion of the genome is composed of nonhomogeneous domains. In\nterms of coverage, only 1% of the total number of compositionally homogeneous domains could be considered \"isochores\" which covered less than 20% of the genome.\n\nSince its inception the theory received wide attention and was extensively used to explain findings emerging from over dozen new genome\nsequencing studies.\nopen, such as which evolutionary forces shaped the structure of compositional domain and how do they differ between different species?\n", "id": "10705831", "title": "Isochore (genetics)"}
{"url": "https://en.wikipedia.org/wiki?curid=2783668", "text": "Infraspecific name\n\nIn botany, an infraspecific name is the scientific name for any taxon below the rank of species, i.e. an infraspecific taxon. (A \"taxon\", plural \"taxa\", is a group of organisms to be given a particular name.) The scientific names of botanical taxa are regulated by the \"International Code of Nomenclature for algae, fungi, and plants\" (ICN). This specifies a 'three part name' for infraspecific taxa, plus a 'connecting term' to indicate the rank of the name. An example of such a name is \"Astrophytum myriostigma\" subvar. \"glabrum\", the name of a subvariety of the species \"Astrophytum myriostigma\" (bishop's hat cactus).\n\nNames below the rank of species of cultivated kinds of plants and of animals are regulated by different codes of nomenclature and are formed somewhat differently.\n\nArticle 24 of the ICN describes how infraspecific names are constructed. The order of the three parts of an infraspecific name is:\nIt is customary to italicize all three parts of such a name, but not the connecting term. For example:\n\n\nThe recommended abbreviations for ranks below species are, for example:\n\nLike specific epithets, infraspecific epithets cannot be used in isolation as names. Thus the name of a particular species of \"Acanthocalycium\" is \"Acanthocalycium klimpelianum\", which can be abbreviated to \"A. klimpelianum\" where the context makes the genus clear. The species cannot be referred to as just \"klimpelianum\". In the same way, the name of a particular variety of \"Acanthocalycium klimpelianum\" is \"Acanthocalycium klimpelianum\" var. \"macranthum\", which can be abbreviated to \"A. k.\" var. \"macranthum\" where the context makes the species clear. The variety cannot be referred to as just \"macranthum\".\n\nSometimes more than three parts will be given; strictly speaking, this is not a name, but a classification. The ICN gives the example of \"Saxifraga aizoon\" var. \"aizoon\" subvar. \"brevifolia\" f. \"multicaulis\" subf. \"surculosa\"; the name of the subform would be \"Saxifraga aizoon\" subf. \"surculosa\".\n\nFor a proposed infraspecific name to be legitimate it must be in accordance with all the rules of the ICN. Only some of the main points are described here.\n\nA key concept in botanical names is that of a type. In many cases the type will be a particular preserved specimen stored in a herbarium, although there are other kinds of type. Like other names, an infraspecific name is attached to a type. Whether a plant should be given a particular infraspecific name can then be decided by comparing it to the type.\n\nThere is no requirement for a species to be divided into infraspecific taxa, of whatever rank; in other words, a species does not have to have subspecies, varieties, forms, etc. However, if infraspecific ranks are created, then the name of the type of the species must repeat the specific epithet as its infraspecific epithet. The type acquires this name automatically as soon as any infraspecific rank is created. As an example, consider \"Poa secunda\" , whose type specimen is in the Wisconsin State Herbarium.\n\nThe same epithet can be used again within a species, at whatever level, only if the names with the re-used epithet are attached to the same type. Thus there can be a form called \"Poa secunda\" f. \"juncifolia\" as well as the subspecies \"Poa secunda\" subsp. \"juncifolia\" if, and only if, the type specimen of \"Poa secunda\" f. \"juncifolia\" is the same as the type specimen of \"Poa secunda\" subsp. \"juncifolia\" (in other words, if there is a single type specimen whose classification is \"Poa secunda\" subsp. \"juncifolia\" f. \"juncifolia\").\n\nIf two infraspecific taxa which have different types are accidentally given the same epithet, then a homonym has been created. The earliest published name is the legitimate one and the other must be changed.\n\nWhen indicating authors for infraspecific names, it is possible to show either just the author(s) of the final, infraspecific epithet, or the authors of both the specific and the infraspecific epithets. Examples:\n\n\nIn zoological nomenclature, names of taxa below species rank are formed somewhat differently, using a trinomen or 'trinomial name'. No connecting term is required as there is only one rank below species, the subspecies.\n\nThe \"ICN\" does not regulate the names of cultivated plants, of cultivars, i.e. plants specifically created for use in agriculture or horticulture. Such names are regulated by the \"International Code of Nomenclature for Cultivated Plants\" (ICNCP).\n\nAlthough logically below the rank of species (and hence \"infraspecific\"), a cultivar name may be attached to any scientific name at the genus level or below. The minimum requirement is to specify a genus name. For example, \"Achillea\" 'Cerise Queen' is a cultivar; \"Pinus nigra\" 'Arnold Sentinel' is a cultivar of the species \"P. nigra\" (which is propagated vegetatively, by cloning).\n\n", "id": "2783668", "title": "Infraspecific name"}
{"url": "https://en.wikipedia.org/wiki?curid=1478634", "text": "Systema Naturae\n\nThe tenth edition of this book (1758) is considered the starting point of zoological nomenclature. In 1766–1768 Linnaeus published the much enhanced 12th edition, the last under his authorship. Another again enhanced work in the same style and entitled \"\"\" was published by Johann Friedrich Gmelin between 1788 and 1793. Since at least the early 1900s zoologists commonly recognized this as the last edition belonging to this series. It was also officially regarded by the ICZN in Opinion 296 (26 Oct 1954) as the 13th edition of \"Systema Naturae\".\n\nLinnaeus (later known as \"Carl von Linné\", after his ennoblement in 1761) published the first edition of ' in the year 1735, during his stay in the Netherlands. As was customary for the scientific literature of its day, the book was published in Latin. In it, he outlined his ideas for the hierarchical classification of the natural world, dividing it into the animal kingdom ('), the plant kingdom ('), and the \"mineral kingdom\" (').\n\nLinnaeus's \"Systema Naturae\" lists only about 10,000 species of organisms, of which about 6,000 are plants and 4,236 are animals. According to the historian of botany William T. Stearn, \"Even in 1753 he believed that the number of species of plants in the whole world would hardly reach 10,000; in his whole career he named about 7,700 species of flowering plants.\"\n\nLinnaeus developed his classification of the plant kingdom in an attempt to describe and understand the natural world as a reflection of the logic of God's creation. His sexual system, where species with the same number of stamens were treated in the same group, was convenient but in his view artificial. Linnaeus believed in God's creation, and that there were no deeper relationships to be expressed. He is frequently quoted to have said. \"God created, Linnaeus organized\". The classification of animals was more natural. For instance, humans were for the first time placed together with other primates, as Anthropomorpha.\n\nAs a result of the popularity of the work, and the number of new specimens sent to him from around the world, Linnaeus kept publishing new and ever-expanding editions of his work. It grew from eleven very large pages in the first edition (1735) to 2,400 pages in the 12th edition (1766–1768). Also, as the work progressed, he made changes: in the first edition, whales were classified as fishes, following the work of Linnaeus' friend and \"father of ichthyology\" Peter Artedi; in the 10th edition, published in 1758, whales were moved into the mammal class. In this same edition, he introduced two-part names (see binomen) for animal species, something that he had done for plant species (see binary name) in the 1753 publication of \"\". The system eventually developed into modern Linnaean taxonomy, a hierarchically organized biological classification.\n\nAfter Linnaeus' health declined in the early 1770s, publication of editions of \"Systema Naturae\" went in two directions. Another Swedish scientist, Johan Andreas Murray issued the \"Regnum Vegetabile\" section separately in 1774 as the \"Systema Vegetabilium\", rather confusingly labelled the 13th edition. Meanwhile, a 13th edition of the entire \"Systema\" appeared in parts between 1788 and 1793. It was as the \"Systema Vegetabilium\" that Linnaeus' work became widely known in England following translation from the Latin by the Lichfield Botanical Society, as \"A System of Vegetables\" (1783–1785).\n\nIn his ', Linnaeus established three kingdoms, namely ', ' and '. This approach, the Animal, Vegetable and Mineral Kingdoms, survives until today in the popular mind, notably in the form of parlour games: \"Is it animal, vegetable or mineral?\". The classification was based on five levels: kingdom, class, order, genus, and species. While species and genus was seen as God-given (or \"natural\"), the three higher levels were seen by Linnaeus as constructs. The concept behind the set ranks being applied to all groups was to make a system that was easy to remember and navigate, a task which most say he succeeded in.\nLinnaeus's work had a huge impact on science; it was indispensable as a foundation for biological nomenclature, now regulated by the Nomenclature Codes. Two of his works, the first edition of the ' (1753) for plants and the 10th edition of the \"Systema Naturæ\" (1758), are accepted among the starting points of nomenclature. Most of his names for species and genera were published at very early dates, thus take priority over those of other, later works. In zoology there is one exception, which is a monograph on Swedish spiders, ', published by Carl Clerck in 1757, so the names established there take priority over the Linnean names. However, his impact on science was not because of the value of his taxonomy. His talent for attracting skillful young students and sending them abroad to collect made his work far more influential than that of his contemporaries. At the close of the 18th century, his system had effectively become the standard system for biological classification.\n\nOnly in the Animal Kingdom is the higher taxonomy of Linnaeus still more or less recognizable and some of these names are still in use, but usually not quite for the same groups as used by Linnaeus. He divided the Animal Kingdom into six classes; in the tenth edition (1758), these were:\n\n\nThe orders and classes of plants, according to his ', were never intended to represent natural groups (as opposed to his ' in his \") but only for use in identification. They were used in that sense well into the 19th century.\n\nThe Linnaean classes for plants, in the Sexual System, were:\n\nLinnaeus's taxonomy of minerals has long since fallen out of use. In the 10th edition, 1758, of the \"\", the Linnaean classes were:\n\n\nThe Gmelin thirteenth (\"decima tertia\") edition of 1788–1793, is likely to be confused with another thirteenth edition prepared by Johan Andreas Murray in 1774, and then subsequently in further editions, all under a revised name of \"Systema Vegetabilium\".\n\nThe dates of publication for Gmelin's edition were the following: \n\n\n", "id": "1478634", "title": "Systema Naturae"}
{"url": "https://en.wikipedia.org/wiki?curid=6781871", "text": "Cline (biology)\n\nIn biology and ecology, an ecocline or simply cline (from \"to possess or exhibit gradient, to lean\") is an ecotone in which a series of biocommunities display a continuous gradient. The term was coined by the English evolutionary biologist Julian Huxley in 1938.\n\nMore technically, clines consist of ecotypes or forms of species that exhibit gradual phenotypic and/or genetic differences over a geographical area, typically as a result of environmental heterogeneity.\nGenetically, clines result from the change of allele frequencies within the gene pool of the group of taxa in question. Clines may manifest in time and/or space.\n\nIn ecology, spatial clines have led to gradient analysis where the abundance and distribution of organisms is rendered by sinusoidal curves on the plane. From these curves can be extracted that populations occupy zones of maximum and minimum presence, according to their special needs and tolerances imposed by their environment.\nTypically, a well-marked cline does not allow for a delineation of subspecies, as it is then impossible, by definition, to draw any further clear dividing lines between populations. In population genetics, a cline could include a spectrum of subspecies, as allele and haplotype frequencies tend to vary over a larger space; moreover, in evolution, genetic lineage sorting usually lags behind the establishment of locally differentiated phenotypes. Regardless, in neither case will such a variation yield different species, as long as the populations, though geographically spread, can interbreed one with another. \n\nRing species are a distinct type of cline where the geographical distribution in question is circular in shape, so that the two ends of the cline overlap with one another, giving two adjacent populations that rarely interbreed due to the cumulative effect of the many changes in phenotype along the cline. The populations elsewhere along the cline interbreed with their geographically adjacent populations as in a standard cline. Ring species present an interesting problem for those who seek to divide the living world into discrete species, as chain species are closely related to speciation (in this case, parapatric).\n\nIn the case of \"Larus\" gulls, the habitats of the end populations even overlap, which introduces questions as to what constitutes a species: nowhere along the cline can a line be drawn between the populations, but they are unable to interbreed.\nHowever a recent study (Liebers et al., 2004) has provided genetic evidence that this example is far more complicated than presented here, and likely does not constitute a typical ring species.\n", "id": "6781871", "title": "Cline (biology)"}
{"url": "https://en.wikipedia.org/wiki?curid=1056866", "text": "GC-content\n\nIn molecular biology and genetics, GC-content (or guanine-cytosine content) is the percentage of nitrogenous bases on a DNA or RNA molecule that are either guanine or cytosine (from a possibility of four different ones, also including adenine and thymine in DNA and adenine and uracil in RNA). This may refer to a certain fragment of DNA or RNA, or that of the whole genome. When it refers to a fragment of the genetic material, it may denote the GC-content of section of a gene (domain), single gene, group of genes (or gene clusters), or even a non-coding region. G (guanine) and C (cytosine) undergo a specific hydrogen bonding, whereas A (adenine) bonds specifically with T (thymine, in DNA) or U (uracil, in RNA).\n\nThe GC pair is bound by three hydrogen bonds, while AT and AU pairs are bound by two hydrogen bonds. To emphasize this difference in the number of hydrogen bonds, the base pairings can be represented as respectively G≡C versus A=T and A=U. DNA with low GC-content is less stable than DNA with high GC-content; however, the hydrogen bonds themselves do not have a particularly significant impact on stabilization, the stabilization is due mainly to interactions of base stacking.\nIn spite of the higher thermostability conferred to the genetic material, it has been observed that at least some bacteria species with DNA of high GC-content undergo autolysis more readily, thereby reducing the longevity of the cell \"per se\". Due to the thermostability given to the genetic materials in high GC organisms, it was commonly believed that the GC content played a necessary role in adaptation temperatures, a hypothesis that was refuted in 2001.\nHowever, it has been shown that there is a strong correlation between the prokaryotic optimal growth at higher temperatures and the GC content of structured RNAs (such as ribosomal RNA, transfer RNA, and many other non-coding RNAs). The AU base pairs are less stable than the GC base pairs previously attributed to GC bonds containing 3 hydrogen bonds and AU having only 2 hydrogen bonds, making high-GC-content RNA structures more resistant to the effects of high temperatures.\nMore recently, it has been proved that the most stabilizing factor of thermal stability of double stranded nucleic acids is actually due to the base stackings of adjacent bases, rather than the number of hydrogen bonds between the bases.There is more favorable stacking energy for G:C pairs because of the relative positions of exocyclic groups than in the A:U pairs. Additionally, there is a correlation between the order in which the bases stack and thermal stability.\n\nIn PCR experiments, the GC-content of primers are used to predict their annealing temperature to the template DNA. A higher GC-content level indicates a relatively higher melting temperature.\n\nGC content is usually expressed as a percentage value, but sometimes as a ratio (called G+C ratio or GC-ratio). GC-content percentage is calculated as\n\nwhereas the AT/GC ratio is calculated as \n\nThe GC-content percentages as well as GC-ratio can be measured by several means, but one of the simplest methods is to measure what is called the melting temperature of the DNA double helix using spectrophotometry. The absorbance of DNA at a wavelength of 260 nm increases fairly sharply when the double-stranded DNA separates into two single strands when sufficiently heated. The most commonly used protocol for determining GC ratios uses flow cytometry for large number of samples.\n\nIn alternative manner, if the DNA or RNA molecule under investigation has been sequenced then the GC-content can be accurately calculated by simple arithmetic or by using the free online GC calculator.\n\nGC ratios within a genome is found to be markedly variable. These variations in GC ratio within the genomes of more complex organisms result in a mosaic-like formation with islet regions called isochores. This results in the variations in staining intensity in the chromosomes. GC-rich isochores include in them many protein coding genes, and thus determination of ratio of these specific regions contributes in mapping gene-rich regions of the genome.\n\nWithin a long region of genomic sequence, genes are often characterised by having a higher GC-content in contrast to the background GC-content for the entire genome. Evidence of GC ratio with that of length of the coding region of a gene has shown that the length of the coding sequence is directly proportional to higher G+C content. This has been pointed to the fact that the stop codon has a bias towards A and T nucleotides, and, thus, the shorter the sequence the higher the AT bias.\n\nGC content is found to be variable with different organisms, the process of which is envisaged to be contributed to by variation in selection, mutational bias, and biased recombination-associated DNA repair. The species problem in prokaryotic taxonomy has led to various suggestions in classifying bacteria, and the \"ad hoc committee on reconciliation of approaches to bacterial systematics\" has recommended use of GC ratios in higher level hierarchical classification. For example, the Actinobacteria are characterised as \"high GC-content bacteria\". In \"Streptomyces coelicolor\" A3(2), GC content is 72%. The GC-content of Yeast (\"Saccharomyces cerevisiae\") is 38%, and that of another common model organism, thale cress (\"Arabidopsis thaliana\"), is 36%. Because of the nature of the genetic code, it is virtually impossible for an organism to have a genome with a GC-content approaching either 0% or 100%. A species with an extremely low GC-content is \"Plasmodium falciparum\" (GC% = ~20%), and it is usually common to refer to such examples as being AT-rich instead of GC-poor.\n\n\n", "id": "1056866", "title": "GC-content"}
{"url": "https://en.wikipedia.org/wiki?curid=1694221", "text": "Crown eukaryotes\n\nCrown eukaryotes are an artificial group of eukaryotic organisms found at the top of molecular phylogenetic trees including both eukaryotes and prokaryotes. They were originally thought to represent a late step of eukaryotic evolution (somewhat similar to a crown group) because they include multicellular and macroscopic lifeforms that represent the majority of the biomass of the planet while accounting for less than 1% of the genetic diversity. However, they are in fact the result of an artificial clustering of eukaryotic organisms with slowly evolving gene sequences. They are thus not a crown that excludes simpler eukaryotes, but correspond roughly to the initial radiation of eukaryotes. All eukaryotic lineages branching below the \"crown\" in phylogenetic trees are misplaced because of the long branch attraction phenomenon.\n\n\n", "id": "1694221", "title": "Crown eukaryotes"}
{"url": "https://en.wikipedia.org/wiki?curid=4721036", "text": "Circumscriptional name\n\nIn biological classification, circumscriptional names are taxon names that are not ruled by ICZN and are defined by the particular set of members included. Circumscriptional names are used mainly for taxa above family-group level (e. g. order or class), but can be also used for taxa of any ranks, as well as for rank-less taxa.\n\nNon-typified names other than those of the genus- or species-group constitute the majority of generally accepted names of taxa higher than superfamily. The ICZN regulates names of taxa up to family group rank (i. e. superfamily). There are no generally accepted rules of naming higher taxa (orders, classes, phyla, etc.). Under the approach of circumscription-based (circumscriptional) nomenclatures, a circumscriptional name is associated with a certain circumscription of a taxon without regard of its rank or position. Circumscriptional nomenclature, parallel with another rank-free nomenclature — hierarchical one, is used in cladoendesis.\n\nSome authors advocate introducing a mandatory standardized typified nomenclature of higher taxa. They suggest all names of higher taxa to be derived in the same manner as family-group names, i.e. by modifying names of type genera with endings to reflect the rank. There is no consensus on what such higher rank endings should be. A number of established practices exist as to the use of typified names of higher taxa, depending on animal group.\n\n\n\n", "id": "4721036", "title": "Circumscriptional name"}
{"url": "https://en.wikipedia.org/wiki?curid=35590090", "text": "Compositional domain\n\nA compositional domain in genetics is a region of DNA with a distinct guanine (G) and cytosine (C) G-C and C-G content (collectively GC content). The homogeneity of compositional domains is compared to that of the chromosome on which they reside. As such, compositional domains can be homogeneous or nonhomogeneous domains. Compositionally homogeneous domains that are sufficiently long (= 300 kb) are termed isochores or isochoric domains.\n\nThe compositional domain model was proposed as an alternative to the isochoric model. The isochore model was proposed by Bernardi and colleagues to explain the observed non-uniformity of genomic fragments in the genome. However, recent sequencing of complete genomic data refuted the isochoric model. Its main predictions were:\n\nThe compositional domain model describes the genome as a mosaic of short and long homogeneous and nonhomogeneous domains. The composition and organization of the domains were shaped by different evolutionary processes that either fused or broke down the domains. This genomic organization model was confirmed in many new genomic studies of cow, honeybee, sea urchin, body louse, \"Nasonia\", beetle, and ant genomes. The human genome was described as consisting of a mixture of compositionally nonhomogeneous domains with numerous short compositionally homogeneous domains and relatively few long ones.\n\n", "id": "35590090", "title": "Compositional domain"}
{"url": "https://en.wikipedia.org/wiki?curid=5176156", "text": "Conserved name\n\nA conserved name or nomen conservandum (plural nomina conservanda, abbreviated as nom. cons.) is a scientific name that has specific nomenclatural protection. \"Nomen conservandum\" is a Latin term, meaning \"a name to be conserved\". The terms are often used interchangeably, such as by the International Code of Nomenclature for algae, fungi, and plants (ICN), while the International Code of Zoological Nomenclature favours \"conserved name\". The process for conserving botanical names is different from that for zoological names. Under the botanical code, names may also be \"suppressed\", nomen rejiciendum (plural nomina rejicienda or nomina utique rejicienda, abbreviated as nom. rej.), or rejected in favour of a particular conserved name, and combinations based on a suppressed name are also listed as \"nom. rej\".\n\nIn botanical nomenclature, conservation is a nomenclatural procedure governed by Art. 14 of the ICN. Its purpose is \nConservation is possible only for names at the rank of family, genus or species.\n\nIt may effect a change in original spelling, type, or (most commonly) priority.\n\nBesides conservation/rejection of names of certain ranks (Art. 14), the ICN also offers the option of outright rejection of a name (nomen utique rejiciendum) also called suppressed name under Article 56, another way of creating a \"nomen rejiciendum\" that cannot be used anymore. Outright rejection is possible for a name at any rank.\n\nRejection (suppression) of individual names is distinct from suppression of works (opera utique oppressa) under article 34, which allows for listing certain taxonomic ranks in certain publications which are considered not to include any validly published names.\n\nConflicting conserved names are treated according to the normal rules of priority. Separate proposals (informally referred to as \"superconservation\" proposals) may be made to protect a conserved name that would be overtaken by another. However, conservation has different consequences depending on the type of name that is conserved:\n\nConserved and rejected names (and suppressed names) are listed in the appendices to the ICN. As of the 2012 (Melbourne) edition, a separate volume holds the bulk of the appendices (except appendix I, on names of hybrids). The substance of the second volume is generated from a database which also holds a history of published proposals and their outcomes, the binding decisions on whether a name is validly published (article 38.4) and on whether it is a homonym (article 53.5). The database can be queried online.\n\n\nIn the course of time there have been different standards for the majority required for a decision. However, for decades the Nomenclature Section has required a 60% majority for an inclusion in the \"Code\", and the Committees have followed this example, in 1996 adopting a 60% majority for a decision.\n\nFor zoology, the term \"conserved name\", rather than \"nomen conservandum\", is used in the \"International Code of Zoological Nomenclature\", although informally both terms are used interchangeably.\n\nIn the glossary of the \"International Code of Zoological Nomenclature\" (the code for names of animals, one of several nomenclature codes), this definition is given:\n\nThis is a more generalized definition than the one for \"nomen protectum\", which is specifically a conserved name that is either a junior synonym or homonym that is in use because the senior synonym or homonym has been made a \"nomen oblitum\" (\"forgotten name\").\n\nAn example of a conserved name is the dinosaur genus name \"Pachycephalosaurus\", which was formally described in 1943. Later, \"Tylosteus\" (which was formally described in 1872) was found to be the same genus as \"Pachycephalosaurus\" (a synonym). By the usual rules, the genus \"Tylosteus\" has precedence and would normally be the correct name. But the International Commission on Zoological Nomenclature (ICZN) ruled that the name \"Pachycephalosaurus\" was to be given precedence and treated as the valid name, because it was in more common use and better known to scientists.\n\nThe ICZN's procedural details are different from those in botany, but the basic operating principle is the same, with petitions submitted to the commission for review.\n\n\n", "id": "5176156", "title": "Conserved name"}
{"url": "https://en.wikipedia.org/wiki?curid=37237272", "text": "Eocyte hypothesis\n\nThe Eocyte hypothesis is a biological classification that indicates eukaryotes emerged within the prokaryotic Crenarchaeota (formerly known as eocytes), a phylum within the archaea. This hypothesis was originally proposed by James A. Lake and colleagues in 1984 based on the discovery that the shapes of ribosomes in the Crenarchaeota and eukaryotes are more similar to each other than to either bacteria or the second major kingdom of archaea, the Euryarchaeota.\n\nThe eocyte hypothesis gained considerable attention after its introduction due to the interest in determining the origin of the eukaryotic cell. This hypothesis has primarily been in contrast with the three-domain system introduced by Carl Woese in 1977. Additional evidence supporting the eocyte hypothesis was published in the 1980s, but despite fairly unequivocal evidence, support waned in favor of the three-domain system.\n\nWith advancements in genomics, the eocyte hypothesis experienced a revival beginning in the mid-2000s. As more archaeal genomes were sequenced, numerous genes coding for eukaryotic traits have been discovered in various archaean phyla, seemingly providing support for the eocyte hypothesis. In addition to a Crenarchaeal origin of eukaryotes, some studies have suggested that eukaryotes may also have originated in the Thaumarchaeota. A superphylum - TACK - has been proposed that includes the Thaumarchaeota, Crenarchaeota, and other groups of archaea, so that this superphylum may be related to the origin of eukaryotes.\n\nAs a result of metagenomic analysis of material found nearby hydrothermal vents, another superphylum -- Asgard -- has been named and proposed to be more closely related to the original eukaryote and a sister group to TACK more recently. \nThe eocyte tree root may be located in the RNA World, that is the root organism may have been a Ribocyte (aka Ribocell). For cellular DNA and DNA handling an \"out of virus\" scenarie has been proposed, i. e. string genetic information in DNA may have been an invention performed by viruses later handed over to Ribicytes twice transforming them as into bacteria as into archaea. \nAll these findings do not change the eocyte tree as given here in principle, but zoom into a higher resolution of it.\n", "id": "37237272", "title": "Eocyte hypothesis"}
{"url": "https://en.wikipedia.org/wiki?curid=2133291", "text": "Pathovar\n\nA pathovar is a bacterial strain or set of strains with the same or similar characteristics, that is differentiated at infrasubspecific level from other strains of the same species or subspecies on the basis of distinctive pathogenicity to one or more plant hosts.\n\nPathovars are named as a ternary or quaternary addition to the species binomial name, for example the bacterium that causes citrus canker \"Xanthomonas axonopodis\", has several pathovars with different host ranges, \"X. axonopodis\" pv. \"citri\" is one of them; the abbreviation 'pv.' means pathovar.\n\nThe type strains of pathovars are pathotypes, which are distinguished from the types (holotype, neotype, etc.) of the species to which the pathovar belongs. \n\n", "id": "2133291", "title": "Pathovar"}
{"url": "https://en.wikipedia.org/wiki?curid=5685431", "text": "Ichnotaxon\n\nAn ichnotaxon (plural ichnotaxa) is defined by the International Code of Zoological Nomenclature as \"a taxon based on the fossilized work of an organism\", that is, the non-human equivalent of an artifact.\n\nIchnotaxa are names used to identify and distinguish morphologically distinctive ichnofossils, more commonly known as trace fossils. They are assigned genus and species ranks by ichnologists, much like organisms in Linnaean taxonomy. These are known as ichnogenera and ichnospecies, respectively. \"Ichnogenus\" and \"ichnospecies\" are commonly abbreviated as \"igen.\" and \"isp.\". The binomial names of ichnospecies and their genera are to be written in italics.\n\nMost researchers classify trace fossils only as far as the ichnogenus rank, based upon trace fossils that resemble each other in morphology but have subtle differences. Some authors have constructed detailed hierarchies up to ichnosuperclass, recognizing such fine detail as to identify ichnosuperorder and ichnoinfraclass, but such attempts are controversial.\n\n\"Ichnotaxa\" comes from the Greek ίχνος, \"ichnos\" meaning \"track\" and ταξις, \"taxis\" meaning \"ordering\".\n\nDue to the chaotic nature of trace fossil classification, several ichnogenera hold names normally affiliated with animal body fossils or plant fossils. For example, many ichnogenera are named with the suffix \"-phycus\" due to misidentification as algae.\n\nEdward Hitchcock was the first to use the now common \"-ichnus\" suffix in 1858, with \"Cochlichnus\".\n\nDue to trace fossils' history of being difficult to classify, there have been several attempts to enforce consistency in the naming of ichnotaxa.\n\nIn 1961, the International Commission on Zoological Nomenclature ruled that most trace fossil taxa named after 1930 would be no longer available.\n\n\n", "id": "5685431", "title": "Ichnotaxon"}
{"url": "https://en.wikipedia.org/wiki?curid=40547081", "text": "Kew Rule\n\nThe Kew Rule was used by some authors to determine the application of synonymous names in botanical nomenclature up to about 1906, but was and still is contrary to codes of botanical nomenclature including the International Code of Nomenclature for algae, fungi, and plants. Index Kewensis, a publication that aimed to list all botanical names for seed plants at the ranks of species and genus, used the Kew Rule until its \"Supplement IV\" was published in 1913 (prepared 1906–1910).\n\nThe Kew Rule applied rules of priority in a more flexible way, so that when transferring a species to a new genus, there was no requirement to retain the epithet of the original species name, and future priority of the new name was counted from the time the species was transferred to the new genus. The effect has been summarized as \"nomenclature used by an established monographer or in a major publication should be adopted\". This is contrary to the modern article 11.4 of the Code of Nomenclature.\n\nThe first discussion in print of what was to become known as the Kew Rule appears to have occurred in 1877 between Henry Trimen and Alphonse Pyramus de Candolle. Trimen did not think it was reasonable for older names discovered in the literature to destabilize the nomenclature that had been well accepted:Probably all botanists are agreed that it is very desirable to retain when possible old specific names, but some of the best authors do not certainly consider themselves bound by any generally accepted rule in this matter. Still less will they be inclined to allow that a writer is at liberty, as M. de Candolle thinks, to reject the specific appellations made by an author whose genera are accepted, in favour of older ones in other genera. It will appear to such that to do this is to needlessly create in each case another synonym.\n\nThe first botanical code of nomenclature that declared itself to be binding was the 1906 \"Règles internationales de la nomenclature botanique adoptées par le Congres International de Botanique de Vienne 1905\" that followed from the 1905 International Botanical Congress. The Kew Rule was outlawed by this code.\n\nThe end of the Kew Rule brought about considerable upheaval in botanical nomenclature. Many new species names were coined to resurrect older epithets, for example, in 1917 Willis Jepson wrote:\n\"\"The plant so long known as \"Brodiaea grandiflora\" Smith ... [was] first published as \"Hookera coronaria\" Salisbury (1806). The correct name, then, is \"Brodiaea coronaria\" Jepson, \"n. comb.\"\"\n\nNames that had previously been conserved to improve the stability of well-known plant names often now no longer required conservation, and other names that had been formed using the Kew Rule and had become well known, were illegitimate. The entire previous list of conserved and rejected names was consequently replaced in 1959 with a reworked list.\n\nPreviously overlooked botanical literature has continued to yield new examples of forgotten older names for more than 100 years since the Kew Rule was banished from the International Code of Nomenclature.\n", "id": "40547081", "title": "Kew Rule"}
{"url": "https://en.wikipedia.org/wiki?curid=7308831", "text": "Nomen novum\n\nIn biological nomenclature, a nomen novum (Latin for \"new name\"), new replacement name (or replacement name, new substitute name, substitute name) is a technical term. It indicates a scientific name that is created specifically to replace another scientific name, but only when this other name can not be used for technical, nomenclatural reasons (for example because it is a homonym: it is spelled the same as an existing, older name); it does not apply when a name is changed for taxonomic reasons (representing a change in scientific insight). It is frequently abbreviated, \"e.g.\" \"nomen nov.\", \"nom. nov.\", or more explicitly by the rank specified, \"e.g.\" \"superordo nov.\"\n\nIn zoology establishing a new replacement name is a nomenclatural act and it must be expressly proposed to substitute a previously established and available name.\n\nOften, the older name cannot be used because another animal was described earlier with exactly the same name. For example, Lindholm discovered in 1913 that a generic name \"Jelskia\" established by Bourguignat in 1877 for a European freshwater snail could not be used because another author Taczanowski had proposed the same name in 1871 for a spider. So Lindholm proposed a new replacement name \"Borysthenia\". This is an objective synonym of \"Jelskia\" Bourguignat, 1877, because he has the same type species, and is used today as \"Borysthenia\".\n\nAlso for names of species new replacement names are often necessary. New replacement names have been proposed since more than 100 years ago. In 1859 Bourguignat saw that the name \"Bulimus cinereus\" Mortillet, 1851 for an Italian snail could not be used because Reeve had proposed exactly the same name in 1848 for a completely different Bolivian snail. Since it was understood even then that the older name has always priority, Bourguignat proposed a new replacement name \"Bulimus psarolenus\", and also added a note why this was necessary. The Italian snail is known until today under the name \"Solatopupa psarolena\" (Bourguignat, 1859).\n\nA new replacement name must obey certain rules, not all of these are well known.\n\nNot every author who proposes a name for a species that already has another name, establishes a new replacement name. An author who writes \"The name of the insect species with the green wings shall be named X, this is the one that the other author has named Y\", does not establish a new replacement name (but a regular new name).\n\nThe International Code of Zoological Nomenclature prescribes that for a new replacement name, an expressed statement must be given by the author, which means an explicit statement concerning the process of replacing the previous name. It is not necessary to employ the term \"nomen novum\", but something must be expressed concerning the act of substituting a name. Implicit evidence (\"everybody knows why the author used that new name\") is not allowed at this occasion. Many zoologists do not know that this expressed statement is necessary, and therefore a variety of names are regarded as having been established as new replacement names (often including names that were mentioned without any description, which is fundamentally contrary to the rules).\n\nThe author who proposes a new replacement name must state exactly which name shall be replaced. It is not possible to mention three available synonyms at once, to be replaced. Usually the author explains why the new replacement name is needed.\n\nSometimes we read \"the species cannot keep this old name \"P. brasiliensis\", because it does not live in Brazil, so I propose a new name \"P. angolana\"\". Even though this would not justify a new replacement name under the Code's rules, the author believed that a new name was necessary and gave an expressed statement concerning the act of replacing. So the name \"P. angolana\" was made available at this occasion, and is an objective synonym of \"P. brasiliensis\".\n\nA new replacement name can only be used for a taxon if the name that it replaces cannot be used, as in the example above with the snail and the spider, or in the other example with the Italian and the Bolivian snail. The animal from Angola must keep its name \"brasiliensis\", because this is the older name.\n\nNew replacement names do not occur very frequently, but they are not extremely rare. About 1% of the currently used zoological names might be new replacement names. There are no exact statistics covering all animal groups. In 2200 names of species and 350 names of genera in European non-marine molluscs, which might be a representative group of animals, 0.7% of the specific and 3.4% of the generic names were correctly established as new replacement names (and a further 0.7% of the specific and 1.7% of the generic names have incorrectly been regarded as new replacement names by some authors).\n\n\n", "id": "7308831", "title": "Nomen novum"}
{"url": "https://en.wikipedia.org/wiki?curid=2438185", "text": "Sister group\n\nA sister group or sister taxon is a phylogenetic term denoting the closest relatives of another given unit in an evolutionary tree. The expression is most easily illustrated by a cladogram: A, B, and C each represent a taxon:\n\nThe sister group to A is B; conversely, the sister group to B is A. Groups A and B, together with all other descendants of their most recent common ancestor, form the clade AB. The sister group to clade AB is C.\n\nThe whole clade ABC is itself a subtree of a larger tree, which offers yet more sister group branches that are related but farther removed from the leaf nodes, such as A, B, and C.\n\nIn cladistic standards, A, B, and C may represent specimens, species, taxon-groups, etc. If they represent species, the term \"sister species\" is sometimes used.\n\nThe term \"sister group\" is used in phylogenetic analysis, and only groups identified in the analysis are labeled as sister groups. An example is in birds, whose sister group is commonly cited as the crocodiles, but that is true only when dealing with extant taxa. The bird family tree is rooted in the dinosaurs, making for a number of extinct groups branching off before coming to the last common ancestor of birds and crocodiles. Thus, the term sister group must be seen as a relative term, with the caveat that the sister group is the closest relative only among the groups/species/specimens that are included in the analysis.\n", "id": "2438185", "title": "Sister group"}
{"url": "https://en.wikipedia.org/wiki?curid=27813", "text": "Systematics\n\nBiological systematics is the study of the diversification of living forms, both past and present, and the relationships among living things through time. Relationships are visualized as evolutionary trees (synonyms: cladograms, phylogenetic trees, phylogenies). Phylogenies have two components: branching order (showing group relationships) and branch length (showing amount of evolution). Phylogenetic trees of species and higher taxa are used to study the evolution of traits (e.g., anatomical or molecular characteristics) and the distribution of organisms (biogeography). Systematics, in other words, is used to understand the evolutionary history of life on Earth.\n\nIn the study of biological systematics, researchers use the different branches to further understand the relationships between differing organisms. These branches are used to determine the applications and uses for modern day systematics.\n\nBiological systematics classifies species by using three specific branches. \"Numerical systematics\", or \"biometry\", uses biological statistics to identify and classify animals. \"Biochemical systematics\" classifies and identifies animals based on the analysis of the material that makes up the living part of a cell—such as the nucleus, organelles, and cytoplasm. \"Experimental systematics\" identifies and classifies animals based on the evolutionary units that comprise a species, as well as their importance in evolution itself. Factors such as mutations, genetic divergence, and hybridization all are considered evolutionary units.\n\nWith the specific branches, researchers are able to determine the applications and uses for modern-day systematics. These applications include: \n\nJohn Lindley provided an early definition of systematics in 1830, although he wrote of \"systematic botany\" rather than using the term \"systematics\".\n\nIn 1970 Michener \"et al.\" defined \"systematic biology\" and \"taxonomy\" (terms that are often confused and used interchangeably) in relationship to one another as follows:\nSystematic biology (hereafter called simply systematics) is the field that (a) provides scientific names for organisms, (b) describes them, (c) preserves collections of them, (d) provides classifications for the organisms, keys for their identification, and data on their distributions, (e) investigates their evolutionary histories, and (f) considers their environmental adaptations. This is a field with a long history that in recent years has experienced a notable renaissance, principally with respect to theoretical content. Part of the theoretical material has to do with evolutionary areas (topics e and f above), the rest relates especially to the problem of classification. Taxonomy is that part of Systematics concerned with topics (a) to (d) above.\nTaxonomy, systematic biology, systematics, biosystematics, scientific classification, biological classification, phylogenetics: At various times in history, all these words have had overlapping, related meanings. However, in modern usage, they can all be considered synonyms of each other.\n\nFor example, Webster's 9th New Collegiate Dictionary of 1987 treats \"classification\", \"taxonomy\", and \"systematics\" as synonyms. According to this work, the terms originated in 1790, c. 1828, and in 1888 respectively. Some claim systematics alone deals specifically with relationships through time, and that it can be synonymous with phylogenetics, broadly dealing with the inferred hierarchy of organisms. This means it would be a subset of taxonomy as it is sometimes regarded, but the inverse is claimed by others.\n\nEuropeans tend to use the terms \"systematics\" and \"biosystematics\" for the study of biodiversity as a whole, whereas North Americans tend to use \"taxonomy\" more frequently. However, taxonomy, and in particular alpha taxonomy, is more specifically the identification, description, and naming (i.e. nomenclature) of organisms,\nwhile \"classification\" focuses on placing organisms within hierarchical groups that show their relationships to other organisms. All of these biological disciplines can deal with both extinct and extant organisms.\n\nSystematics uses taxonomy as a primary tool in understanding, as nothing about an organism's relationships with other living things can be understood without it first being properly studied and described in sufficient detail to identify and classify it correctly. Scientific classifications are aids in recording and reporting information to other scientists and to laymen. The systematist, a scientist who specializes in systematics, must, therefore, be able to use existing classification systems, or at least know them well enough to skilfully justify not using them.\n\nPhenetics was an attempt to determine the relationships of organisms through a measure of overall similarity, making no distinction between plesiomorphies (shared ancestral traits) and apomorphies (derived traits). From the late-20th century onwards, it was superseded by cladistics, which rejects plesiomorphies in attempting to resolve the phylogeny of Earth's various organisms through time. systematists generally make extensive use of molecular biology and of computer programs to study organisms.\n\nTaxonomic characters are the taxonomic attributes that can be used to provide the evidence from which relationships (the phylogeny) between taxa are inferred. Kinds of taxonomic characters:\n\n\n\n", "id": "27813", "title": "Systematics"}
{"url": "https://en.wikipedia.org/wiki?curid=28676576", "text": "Svenska Spindlar\n\nThe book or (Swedish and Latin, respectively, for \"Swedish spiders\") was one of the major works of the Swedish arachnologist and entomologist Carl Alexander Clerck and appeared in Stockholm in the year 1757. It was the first comprehensive book on the spiders of Sweden and one of the first regional monographs of a group of animals worldwide. The full title of the work was \" – \", (\"Swedish spiders into their main genera separated, and as sixty and a few particular species described and with illuminated figures illustrated\") and included 162 pages of text (eight pages were unpaginated) and 6 colour plates. It was published in Swedish, with a Latin translation printed in a slightly smaller font below the Swedish text.\n\nClerck described in detail 67 species of Swedish spiders, and for the first time in a zoological work consistently applied binomial nomenclature as proposed by Carl Linnaeus and used for the first time for botanical names in his 1753 work \"Species Plantarum\", and which he presented in 1758 in the 10th edition of his work \"Systema Naturae\" for more than 4,000 animal species.\n\n\"Svenska Spindlar\" is the first zoological work to make systematic use of binomial nomenclature, and the only pre-Linnaean source to be recognised as a taxonomic authority for such names.\n\nClerck explained in the last (9th of the 2nd part) chapter of his work that in contrast to previous authors he used the term \"spider\" in the strict sense, for animals possessing eight eyes and separated prosoma and opisthosoma, and that his concept of this group of animals did not include Opiliones (because they had two eyes and a broadly joined prosoma and opisthosoma) and other groups of arachnids.\n\nFor all spiders Clerck used a single generic name (\"Araneus\"), to which was added a specific name which consisted of only one word. Each species was presented in the Swedish text with their Latin scientific names, followed by detailed information containing the exact dates when he had found the animals, and a detailed description of eyes, legs and body. The differences between the sexes were also described. Each species was illustrated in impressively accurate drawings printed on coloured copper plates which were bound at the end of the volume.\n\nBecause of the exceptionally thorough treatment of the spider species, the scientific names proposed by Clerck (which were adopted by Carl Linnaeus in his \"Systema Naturae\" in 1758 with only minor modifications) had traditionally been recognized by arachnologists as binomial and available. In 1959 the ICZN Commission decided that Clerck's work should be available for zoological nomenclature, but the International Code of Zoological Nomenclature did not mention Clerck's work. Only after 1999 was this officially recognized in the Code. This means that in case of doubt the spelling of a spider name as from Clerck's 1757 work has priority over that proposed by Linnaeus in 1758 (an example is \"Araneus\" instead of \"Aranea\"), and that Clerck's spiders were the first animals in modern zoology to have obtained an available scientific name in the Linnean system.\n\nIn the late 1800s, Clerck's 1757 work was commonly accepted as the first application of binomial nomenclature to spiders. In 1959 the ICZN Commission ruled that the date 1758 should be used for Clerck's names, this date 1758 was repeated to apply to Clerck's names in the 4th edition of the International Code of Zoological Nomenclature in 1999.\n\nIn a complete binomial name with author and year, the year corresponds to the year of publication of the original source. Since 2000, the ICZN Code includes an exception of this very basic rule. From the beginning on the new provision in the Code has been misunderstood by many researchers who believed that by setting the date for Clerck's work to 1758 (overriding its true date 1757) and the date for \"Systema Naturae\" to 1 January 1758, the priority was changed. In 2007, a case was even brought before the Commission because the researchers were no longer sure whether the generic name should be \"Araneus\" Clerck or \"Aranea\" Linnaeus. In their judgement the year 1758 for Clerck's \"Svenska Spindlar\" could be interpreted in a way that the Linnean work from 1 January 1758 should have priority. In 2009 the Commission saw itself forced to repeat once more, although this was already explicit in the Code's Article 3.1, that the name \"Araneus\" established by Clerck shall have priority and be used for the genus.\n\n\"Svenska Spindlar\" lists the following 67 species of spider; their current identities follow Platnick (2000–2010).\nChapter 2 (Araneidae, Tetragnathidae)\n\nChapter 3 (Theridiidae, Nesticidae, Linyphiidae)\n\nChapter 4 (Agelenidae, Clubionidae)\n\nChapter 5 (Lycosidae, Pisauridae)\n\nChapter 6 (Salticidae)\n\nChapter 7 (Thomisidae, Philodromidae, Sparassidae)\n\nChapter 8 (Cybaeidae)\n", "id": "28676576", "title": "Svenska Spindlar"}
{"url": "https://en.wikipedia.org/wiki?curid=23963", "text": "Phenetics\n\nIn biology, phenetics ( - to appear) , also known as taximetrics, is an attempt to classify organisms based on overall similarity, usually in morphology or other observable traits, regardless of their phylogeny or evolutionary relation. It is closely related to numerical taxonomy which is concerned with the use of numerical methods for taxonomic classification. Many people contributed to the development of phenetics, but the most influential were Peter Sneath and Robert R. Sokal. Their books are still primary references for this sub-discipline, although now out of print.\n\nPhenetics has largely been superseded by cladistics for research into evolutionary relationships among species. However, certain phenetic methods, such as neighbor-joining, have found their way into phylogenetics, as a reasonable approximation of phylogeny when more advanced methods (such as Bayesian inference) are too computationally expensive.\n\nPhenetic techniques include various forms of clustering and ordination. These are sophisticated ways of reducing the variation displayed by organisms to a manageable level. In practice this means measuring dozens of variables, and then presenting them as two- or three-dimensional graphs. Much of the technical challenge in phenetics revolves around balancing the loss of information in such a reduction against the ease of interpreting the resulting graphs.\n\nThe method can be traced back to 1763 and Michel Adanson (in his \"Familles des plantes\") because of two shared basic principles — overall similarity and equal weighting — and modern pheneticists are sometimes called neo-Adansonians.\n\nPhenetic analyses are unrooted, that is, they do not distinguish between plesiomorphies, traits that are inherited from an ancestor, and apomorphies, traits that evolved anew in one or several lineages. A common problem with phenetic analysis is that basal evolutionary grades, which retain many plesiomorphies compared to more advanced lineages, appear to be monophyletic. Phenetic analyses are also liable to be misled by convergent evolution and adaptive radiation. Cladistic methods have attempted to solve those problems.\n\nConsider for example songbirds. These can be divided into two groups - Corvida, which retains ancient characters in phenotype and genotype, and Passerida, which has more modern traits. But only the latter are a group of closest relatives; the former are numerous independent and ancient lineages which are about as distantly related to each other as each single one of them is to the Passerida. In a phenetic analysis, the large degree of overall similarity found among the Corvida will make them appear to be monophyletic too, but their shared traits were present in the ancestors of \"all\" songbirds already. It is the loss of these ancestral traits rather than their presence that signifies which songbirds are more closely related to each other than to other songbirds. However, the requirement that taxa be monophyletic - rather than paraphyletic as in the case of the Corvida - is itself part of the cladistic view of Taxonomy, not necessarily followed to an absolute degree by other schools.\n\nThe two methodologies are not mutually exclusive. There is no reason why, e.g., species identified using phenetics cannot subsequently be subjected to cladistic analysis, to determine their evolutionary relationships. Phenetic methods can also be superior to cladistics when only the \"distinctness\" of related taxa is important, as the computational requirements are lower.\n\nThe history of pheneticism and cladism as rival taxonomic systems is analysed in David Hull's 1988 book \"Science as a Process\".\n\nTraditionally there was a great deal of heated debate between pheneticists and cladists, as both methods were initially proposed to resolve evolutionary relationships. Perhaps the \"high-water mark\" of phenetics were the DNA-DNA hybridization studies by Charles G. Sibley, Jon E. Ahlquist and Burt L. Monroe Jr., from which resulted the 1990 Sibley-Ahlquist taxonomy for birds. Highly controversial at its time, some of its findings (e.g. the Galloanserae) have been vindicated, while others (e.g. the all-inclusive \"Ciconiiformes\" or the \"Corvida\") have been rejected. However, with computers growing increasingly powerful and widespread, more refined cladistic algorithms became available and could put the suggestions of Willi Hennig to the test; as it turned out, the results of cladistic analyses turned out to be superior to those of phenetic methods - at least when it came to resolving phylogenies.\n\nMany systematists continue to use phenetic methods, particularly in addressing species-level questions. While a major goal of taxonomy remains describing the 'tree of life' - the evolutionary path connecting all species - in fieldwork one needs to be able to separate one taxon from another. Classifying diverse groups of closely related organisms that differ very subtly is difficult using a cladistic approach. Phenetics provides numerical tools for examining overall patterns of variation, allowing researchers to identify discrete groups that can be classified as species.\n\nModern applications of phenetics are common in botany, and some examples can be found in most issues of the journal \"Systematic Botany\". Indeed, due to the effects of horizontal gene transfer, polyploid complexes and other peculiarities of plant genomics, phenetic techniques in botany - though less informative altogether - may, in these special cases, be less prone to errors compared with cladistic analysis of DNA sequences.\n\nIn addition, many of the techniques developed by phenetic taxonomists have been adopted and extended by community ecologists, due to a similar need to deal with large amounts of data.\n\n", "id": "23963", "title": "Phenetics"}
{"url": "https://en.wikipedia.org/wiki?curid=4635563", "text": "Quinarian system\n\nThe Quinarian system was a method of zoological classification which had a brief period of popularity in the mid 19th century, especially among British naturalists. It was largely developed by the entomologist William Sharp MacLeay in 1819. The system was further promoted in the works of Nicholas Aylward Vigors, William John Swainson and Johann Jakob Kaup. Swainson's work on ornithology gave wide publicity to the idea. The system had opponents even before the publication of Charles Darwin's \"On the Origin of Species\" (1859 ) which paved the way for evolutionary trees.\n\nQuinarianism gets its name from the emphasis on the number five: it proposed that all taxa are divisible into five subgroups, and if fewer than five subgroups were known, quinarians believed that a missing subgroup remained to be found.\n\nPresumably this arose as a chance observation of some accidental analogies between different groups, but it was erected into a guiding principle by the quinarians. It became increasingly elaborate, proposing that each group of five classes could be arranged in a circle, with those closer together having greater affinities. Typically they were depicted with relatively advanced groups at the top, and supposedly degenerate forms towards the bottom. Each circle could touch or overlap with adjacent circles; the equivalent overlapping of actual groups in nature was called osculation.\n\nAnother aspect of the system was the identification of \"analogies\" across groups:\nQuinarianism was not widely popular outside the United Kingdom (some followers like William Hincks persisted in Canada); it became unfashionable by the 1840s, during which time more complex \"maps\" were made by Hugh Edwin Strickland and Alfred Russel Wallace. Strickland and others specifically rejected the use of relations of \"analogy\" in constructing natural classifications. These systems were eventually discarded in favour of principles of genuinely natural classification, namely based on evolutionary relationship.\n", "id": "4635563", "title": "Quinarian system"}
{"url": "https://en.wikipedia.org/wiki?curid=3182598", "text": "Soft-bodied organism\n\nSoft-bodied organisms are animals that lack skeletons, a group roughly corresponding to the group Vermes as proposed by Carl von Linné. All animals have muscles but, since muscles can only pull, never push, a number of animals have developed hard parts that the muscles can pull on, commonly called skeletons. Such skeletons may be internal, as in vertebrates, or external, like in arthropods. However, a large number of animals groups do very well without hard parts. This include animals like earthworms, jellyfish, tapeworms, squids and an enormous variety of animals from almost every part of the kingdom Animalia.\n\nMost soft-bodied animals are small, but they do make up the majority of the animal biomass. If we were to weigh up all animals on Earth with hard parts against soft-bodied ones, estimates indicate that the biomass of soft-bodied animals would be at least twice that of animals with hard parts, quite possibly much larger. Particularly the roundworms are extremely numerous. The nematodologist Nathan Cobb described the ubiquitous presence of nematodes on Earth as follows:\n\n\"In short, if all the matter in the universe except the nematodes were swept away, our world would still be dimly recognizable, and if, as disembodied spirits, we could then investigate it, we should find its mountains, hills, vales, rivers, lakes, and oceans represented by a film of nematodes. The location of towns would be decipherable, since for every massing of human beings there would be a corresponding massing of certain nematodes. Trees would still stand in ghostly rows representing our streets and highways. The location of the various plants and animals would still be decipherable, and, had we sufficient knowledge, in many cases even their species could be determined by an examination of their erstwhile nematode parasites.\"\n\nNot being a true phylogenetic group, soft-bodied organism vary enormously in anatomy. Cnidarians and flatworms have a single opening to the gut and a diffuse nerve system. The roundworms, annelids, molluscs, the various lophoporate phyla and non-vertebrate chordates have a tubular gut open at both ends. While the majority of the soft-bodied animals typically don't have any kind of skeleton, some do, mainly in the form of stiff cuticulas (roundworms, water bears) or hydrostatic skeletons (annelids).\n\nWhile lack of a skeleton typically restricts the body size of soft-bodied animals on land, marine representatives can grow to very large sizes. The heaviest soft-bodied organisms are likely the giant squids, with maximum weight estimated at for females, while arctic lion's mane jellyfish mat reach comparable sizes. The longest animal on record is also thought to be a soft-bodied organism, a long thread-like bootlace worm, \"Lineus longissimus\" found on a Scottish beach 1864. Siphonophores may grow to considerable sizes too, though they are colonial organisms, and each single animal is small. Most soft-bodied organisms are as small or smaller, even microscopic. The various organisms grouped as mesozoans and the curious Placozoa are typically composed of just a few hundred cells.\n\nThe lack of hard parts in Soft-bodied organisms makes them extremely rare in the fossil record. Accordingly, the evolutionary history of many of the soft-bodied groups are poorly known. The first major find of fossil soft-bodied animals was from the Burgess Shale in Canada. Today, several sites with Burgess Shale type preservation are known, but the history of many groups of soft-bodied animals is still poorly understood.\n", "id": "3182598", "title": "Soft-bodied organism"}
{"url": "https://en.wikipedia.org/wiki?curid=227053", "text": "Superorganism\n\nA superorganism or supraorganism (the latter is less frequently used but more etymologically correct) is a group of interacting organisms of the same species. A community of synergetically interacting organisms of different species is called a holobiont.\n\nThe term superorganism is used most often to describe a social unit of eusocial animals, where division of labour is highly specialised and where individuals are not able to survive by themselves for extended periods. Ants are the best-known example of such a superorganism. A superorganism can be defined as \"a collection of agents which can act in concert to produce phenomena governed by the collective\", phenomena being any activity \"the hive wants\" such as ants collecting food and avoiding predators, or bees choosing a new nest site. Superorganisms tend to exhibit homeostasis, power law scaling, persistent disequilibrium and emergent behaviours.\n\nThe term was coined in 1789 by James Hutton, the \"Father of Geology\", to refer to Earth in the context of geophysiology. The Gaia hypothesis of James Lovelock, and Lynn Margulis as well as the work of Hutton, Vladimir Vernadsky and Guy Murchie, have suggested that the biosphere itself can be considered a superorganism, although this has been disputed. This view relates to systems theory and the dynamics of a complex system.\n\nThe concept of a superorganism raises the question of what is to be considered an individual. Toby Tyrrell's critique of the Gaia hypothesis argues that Earth's climate system does not resemble an animal's physiological system. Planetary biospheres are not tightly regulated in the same way that animal bodies are: \"planets, unlike animals, are not products of evolution. Therefore we are entitled to be highly skeptical (or even outright dismissive) about whether to expect something akin to a \"superorganism\"\". He concludes that \"the superorganism analogy is unwarranted\". However, as Gaia is another systemic level of integration in Nature, precisely has properties that cannot be inferred from their components.\n\nSome scientists have suggested that individual human beings can be thought of as \"superorganisms\"; as a typical human digestive system contains 10 to 10 microorganisms whose collective genome, the microbiome studied by the Human Microbiome Project, contains at least 100 times as many genes as the human genome itself. Salvucci wrote that superorganism is another level of integration that it is observed in nature. These levels include the genomic, the organismal and the ecological levels. The genomic structure of organism reveals the fundamental role of integration and gene shuffling along evolution.\n\nThe nineteenth century thinker Herbert Spencer coined the term \"super-organic\" to focus on social organization (the first chapter of his \"Principles of Sociology\" is entitled \"Super-organic Evolution\"), though this was apparently a distinction between the organic and the social, \"not\" an identity: Spencer explored the holistic nature of society as a social organism while distinguishing the ways in which society did not behave like an organism. For Spencer, the super-organic was an emergent property of interacting organisms, that is, human beings. And, as has been argued by D. C. Phillips, there is a \"difference between emergence and reductionism\".\n\nThe economist Carl Menger expanded upon the evolutionary nature of much social growth, but without ever abandoning methodological individualism. Many social institutions arose, Menger argued, not as \"the result of socially teleological causes, but the unintended result of innumerable efforts of economic subjects pursuing 'individual' interests\".\n\nSpencer and Menger both argued that because it is individuals who choose and act, any social whole should be considered less than an organism, though Menger emphasized this more emphatically. Spencer used the organistic idea to engage in extended analysis of social structure, conceding that it was primarily an analogy. So, for Spencer, the idea of the super-organic best designated a distinct level of social reality above that of biology and psychology, and not a one-to-one identity with an organism. Nevertheless, Spencer maintained that \"every organism of appreciable size is a society\", which has suggested to some that the issue may be terminological.\n\nThe term \"superorganic\" was adopted by the anthropologist Alfred L. Kroeber in 1917. Social aspects of the superorganism concept are analysed in Marshall (2002). Finally, recent work in social psychology has offered the superorganism metaphor as a unifying framework to understand diverse aspects of human sociality, such as religion, conformity, and social identity processes.\n\nSuperorganisms are important in cybernetics, particularly biocybernetics. They exhibit a form of \"distributed intelligence\", a system in which many individual agents with limited intelligence and information are able to pool resources to accomplish a goal beyond the capabilities of the individuals. Existence of such behavior in organisms has many implications for military and management applications, and is being actively researched.\n\n\n\n", "id": "227053", "title": "Superorganism"}
{"url": "https://en.wikipedia.org/wiki?curid=31032602", "text": "Taxon in disguise\n\nIn bacteriology, a taxon in disguise is a species, genus or higher unit of biological classification whose evolutionary history reveals has evolved from another unit of similar or lower rank, making the parent unit paraphyletic. This happens when rapid evolution makes a new species appear radically different from the ancestral group, so that it is not (initially) recognised as belonging to the parent phylogenetic group, leaving the latter an evolutionary grade.\n\nWhile the term is from bacteriology, parallel examples are found throughout the tree of life. E.g. four-footed animals have evolved from piscine ancestors, yet are not generally considered fish. The four footed animals can thus be said to be \"fish in disguise\". In many cases, the paraphyly can be resolved by re-classifying the taxon in question under the parent group, but in bacteriology renaming groups may have serious consequences as it may cause confusion over the identity of pathogens, and is generally avoided for some groups.\n\nThe bacterial genus \"Shigella\" is the cause of bacillary dysentery, a potentially severe infection that claim the lives of over a million people annually. The genus (\"S. dysenteriae\", \"S. flexneri\", \"S. boydii\", \"S. sonnei\") have evolved from the common intestinal bacterium \"Escherichia coli\", rendering that species parephyletic. \"E. coli\" itself can also cause serious dysentery, but the difference in the genetic makeup between the \"E. coli\" and \"Shigella\" causes different medical conditions and symptoms.\n\n\"Escherichia coli\" is a badly classified species, since some strains share only 20% of their genome. Being so diverse it should be given a higher taxonomic ranking. However, due to the medical conditions associated both with \"E. coli\" itself and with \"Shigella\" the current classification will not be changed, to avoid confusion in medical context. \"Shigella\" will thus remain \"\"E. coli\" in disguise\".\n\nIn a similar way, the \"Bacillus\" species of the \"B. cereus\"-group (\"B. anthracis\", \"B. cereus\", \"B . thuringiensis\", \"B. mycoides\", \"B. pseudomycoides\", \"B. weihenstephanensis\" and \"B. medusa\") have 99-100% similar 16S rRNA sequence (97% is a commonly cited adequate species cut-off) and should be considered a single species. Some of the members of the group appear to have arisen from other \"Bacillus\" strains by acquisition of a protein coding plasmid and the group may thus be polyphyletic. For medical reasons (anthrax \"etc.\"), the current arrangement of separate species remain.\n\n", "id": "31032602", "title": "Taxon in disguise"}
{"url": "https://en.wikipedia.org/wiki?curid=199556", "text": "Taxon\n\nIn biology, a taxon (plural taxa; back-formation from \"taxonomy\") is a group of one or more populations of an organism or organisms seen by taxonomists to form a unit. Although neither is required, a taxon is usually known by a particular name and given a particular ranking, especially if and when it is accepted or becomes established. It is not uncommon, however, for taxonomists to remain at odds over what belongs to a taxon and the criteria used for inclusion. If a taxon is given a formal scientific name, its use is then governed by one of the nomenclature codes specifying which scientific name is correct for a particular grouping.\n\nAlthough preceded by Linnaeus's system in \"Systema Naturae\" (10th edition, 1758) and unpublished work by Bernard and Antoine Laurent de Jussieu, the notion of a unit-based \"natural system\" of biological classification was first made widely available in 1805 through the publication, as the introduction to the third edition of Jean-Baptiste Lamarck's \"Flore françoise\", of Augustin Pyramus de Candolle's \"Principes élémentaires de botanique\", an exposition of a system for the \"natural classification\" of plants. Since then, systematists have striven to construct an accurate classification encompassing the diversity of life; today, a \"good\" or \"useful\" taxon is commonly taken to be one that reflects evolutionary relationships. \n\nMany modern systematists, such as advocates of phylogenetic nomenclature, use cladistic methods that require taxa to be monophyletic (all descendants of some ancestor). Their basic unit, therefore, is the clade rather than the taxon. Similarly, among those contemporary taxonomists working with the traditional Linnean (binomial) nomenclature, few propose taxa they know to be paraphyletic. An example of a well-established taxon that is not also a clade is the class Reptilia, the reptiles; birds are descendants of reptiles but are not included in the Reptilia.\n\nThe term \"taxon\" was first used in 1926 by for animal groups. For plants, it was proposed by Herman Johannes Lam in 1948, and it was adopted at the VII International Botanical Congress, held in 1950.\n\nThe Glossary of the \"International Code of Zoological Nomenclature\" (1999) defines a \n\nA taxon can be assigned a taxonomic rank, usually (but not necessarily) when it is given a formal name.\n\n\"Phylum\" applies formally to any biological domain, but traditionally it was always used for animals, whereas \"Division\" was traditionally often used for plants, fungi, etc.\n\nA prefix is used to indicate a ranking of lesser importance. The prefix \"super-\" indicates a rank above, the prefix \"sub-\" indicates a rank below. In zoology the prefix \"infra-\" indicates a rank below \"sub-\". For instance, among the additional ranks of class are superclass, subclass and infraclass.\n\nRank is relative, and restricted to a particular systematic schema. For example, liverworts have been grouped, in various systems of classification, as a family, order, class, or division (phylum). The use of a narrow set of ranks is challenged by users of cladistics; for example, the mere 10 ranks traditionally used between animal families (governed by the ICZN) and animal phyla (usually the highest relevant rank in taxonomic work) often cannot adequately represent the evolutionary history as more about a lineage's phylogeny becomes known. In addition, the class rank is quite often not an evolutionary but a phenetic or paraphyletic group and as opposed to those ranks governed by the ICZN (family-level, genus-level and species-level taxa), can usually not be made monophyletic by exchanging the taxa contained therein. This has given rise to phylogenetic taxonomy and the ongoing development of the \"PhyloCode\", which has been proposed as a new alternative to replace Linnean classification and govern the application of names to clades. Many cladists do not see any need to depart from traditional nomenclature as governed by the ICZN, ICN, etc.\n\n", "id": "199556", "title": "Taxon"}
{"url": "https://en.wikipedia.org/wiki?curid=9365268", "text": "Web-based taxonomy\n\nWeb-based taxonomy is the effort by taxonomists to use the World Wide Web in order to create unified, consensus taxonomies of life on Earth.\n\nIn his 2002 paper on the subject, H. Charles J. Godfray called for the creation of Web-based organisations to collect all the accumulated literature on a taxonomic group into a centralized knowledge base and make this data available through the Web as a unified taxonomy, so that it can be more easily examined and revised. Such a platform would be owned and maintained by a taxonomic working group, governed by an editor or an editorial board. An example of such a platform is FishBase.\n\nThe notion of Web-based consensus taxonomies remains controversial because, as two Australian researchers pointed out, taxonomic names are not fixed but hypotheses, and therefore in constant change.\n\n\n", "id": "9365268", "title": "Web-based taxonomy"}
{"url": "https://en.wikipedia.org/wiki?curid=5962524", "text": "Trace fossil classification\n\nTrace fossils are classified in various ways for different purposes. Traces can be classified taxonomically (by morphology), ethologically (by behavior), and toponomically, that is, according to their relationship to the surrounding sedimentary layers. Except in the rare cases where the original maker of a trace fossil can be identified with confidence, phylogenetic classification of trace fossils is an unreasonable proposition.\n\nThe taxonomic classification of trace fossils parallels the taxonomic classification of organisms under the International Code of Zoological Nomenclature. In trace fossil nomenclature a Latin binomial name is used, just as in animal and plant taxonomy, with a genus and specific epithet. However, the binomial names are not linked to an organism, but rather just a trace fossil. This is due to the rarity of association between a trace fossil and a specific organism or group of organisms. Trace fossils are therefore included in an \"ichnotaxon\" separate from Linnaean taxonomy. When referring to trace fossils, the terms \"ichnogenus\" and \"ichnospecies\" parallel genus and species respectively.\n\nThe most promising cases of phylogenetic classification are those in which similar trace fossils show details complex enough to deduce the makers, such as bryozoan borings, large trilobite trace fossils such as \"Cruziana\", and vertebrate footprints. However, most trace fossils lack sufficiently complex details to allow such classification.\n\nAdolf Seilacher was the first to propose a broadly accepted ethological basis for trace fossil classification. He recognized that most trace fossils are created by animals in one of five main behavioural activities, and named them accordingly:\n\n\nSince the inception of behavioural categorization, several other ethological classes have been suggested and accepted, as follows:\n\n\nOver the years several other behavioural groups have been proposed, but in general they have been quickly discarded by the ichnological community. Some of the failed proposals are listed below, with a brief description.\n\n\nFixichnia is perhaps the group with the most weight as a candidate for the next accepted ethological class, being not fully described by any of the eleven currently accepted categories. There is also potential for the three plant traces (cecidoichnia, corrosichnia and sphenoichnia) to gain recognition in coming years, with little attention having been paid to them since their proposal.\n\nAnother way to classify trace fossils is to look at their relation to the sediment of origin. Martinsson has provided the most widely accepted of such systems, identifying four distinct classes for traces to be separated in this regard:\n\n\nOther classifications have been proposed, but none stray far from the above.\n\nEarly paleontologists originally classified many burrow fossils as the remains of marine algae, as is apparent in ichnogenera named with the \"-phycus\" suffix. Alfred Gabriel Nathorst and Joseph F. James both controversially challenged this incorrect classification, suggesting the reinterpretation of many \"algae\" as marine invertebrate trace fossils.\n\nSeveral attempts to classify trace fossils have been made throughout the history of paleontology. In 1844, Edward Hitchcock proposed two orders: \"Apodichnites\", including footless trails, and \"Polypodichnites\", including trails of organisms with more than four feet.\n\n\n", "id": "5962524", "title": "Trace fossil classification"}
{"url": "https://en.wikipedia.org/wiki?curid=44432370", "text": "Woeseian revolution\n\nThe Woeseian revolution was the progression of the phylogenetic tree of life from two main divisions, known as the Prokarya and Eukarya, into three domains now classified as Bacteria, Archaea, and Eukaryotes. The discovery of the new domain stemmed from the work of biophysicist Carl Woese in 1997 from a principle of evolutionary biology designated as Woese's dogma. It states that the evolution of ribosomal RNA (rRNA) was a necessary precursor to the evolution of modern life forms. Although the three-domain system has been widely accepted, the initial introduction of Woese’s discovery received criticism from the scientific community.\n\nThe basis of phylogenetics was limited by the technology of the time, which led to a greater dependence on phenotypic classification prior to advances that would allow for molecular methods of organization. This was a major reason why the dichotomy of all living things, being either animal or plant in nature, was deemed as an acceptable theory. Without truly understanding the genetic implication of each organismal classification in phylogenies via nucleic acid sequencing of shared molecular material, the phylogenetic tree of life and other such phylogenies would no doubt be incorrect. Woese’s advances in molecular sequencing and phylogenetic organization allowed for a better understanding of the three domains of life - the Bacteria, Archaea, and Eukaryotes. In regards to their varying types of shared rRNA, that of the small subunit rRNA was deemed as the best molecules to sequence in order to distinguish phylogenetic relationships because of its relatively small size, ease of isolation, and universal distribution.\n\nThis reorganization caused for an initial push back, being accepted nearly a decade after its publication. Possible factors that led to the initial criticism of the discovery include Woese’s oligonucleotide cataloging of which he was one of “only two or three people in the world” to be able to execute this method let alone read the films. Also, Woese’s background was in physics whereas most of the research was being done in the realm of microbiologists.\n", "id": "44432370", "title": "Woeseian revolution"}
{"url": "https://en.wikipedia.org/wiki?curid=36587327", "text": "Klepton\n\nIn biology, a klepton (abbr. kl.) and synklepton (abbr sk.) is a species that requires input from another biological taxon (normally from a species which is closely related to the kleptonic species) to complete their reproductive cycle. Specific types of kleptons are \"zygokleptons\", which reproduce by zygogenesis; \"gynokleptons\" which reproduce by gynogenesis, and \"tychokleptons\", which reproduce by a combination of both systems. The term is derived from the Greek, \"kleptein\", \"to steal\".\n\nKleptogenic reproduction results in three potential outcomes. A unisexual female may simply activate cell division in the egg through the presence of a male's sperm without incorporating any of his genetic material—this results in the production of clonal offspring. The female may also incorporate the male's sperm into her egg, but can do so without excising any of her genetic material. This results in increased ploidy levels that range from triploid to pentaploid in wild individuals. Finally, the female also has the option of replacing some of her genetic material with that of the male's, resulting in a \"hybrid\" of sorts without increasing ploidy.\n\nIn the wild, five species of \"Ambystoma\" salamanders contribute to a unisexual complex that reproduces via a combination of gynogenesis and kleptogenesis: \"A. tigrinum\", \"A. barbouri\", \"A. texanum\", \"A. jeffersonium\", and \"A. laterale\". Over twenty genomic combinations have been found in nature, ranging from \"LLJ\" individuals (two \"A. laterale\" and an \"A. jeffersonium\" genome) to \"LJTi\" individuals (an \"A. laterale\", \"A. jeffersonium\", and an \"A. tigrinum\" genome). Every combination, however, contains the genetic information from the \"A. laterale\" species, and analysis of mitochondrial DNA has indicated that these unisexual species most likely diverged from an \"A. barbouri\" individual some 5 million years ago, making them the oldest unisexual vertebrate species on Earth \nThe fact that these salamanders have persisted for so long is remarkable, as it contradicts the notion that a majority of asexual lineages arise when the conditions are right and quickly disappear. It has been argued that this persistence is very much due to the aforementioned \"genome replacement\" strategy that accompanies kleptogenic reproduction—replacing a portion of the maternal genome with paternal DNA in offspring has allowed unisexual individuals to \"refresh\" their genetic material through time. This facet of kleptogenesis was recently ascertained from genetic research that indicates there is no ancestral \"A. laterale\" genome that is maintained from one unisexual to the next, and that there is not a specific \"L\" genome that is found more often than others. \"L\" genetic material found in these salamanders has also not evolved to be substantially unique from sexual genomes.\n\nOther studied species exhibiting the property include the water frogs \"Pelophylax\".\n\n", "id": "36587327", "title": "Klepton"}
{"url": "https://en.wikipedia.org/wiki?curid=94320", "text": "Virus classification\n\nVirus classification is the process of naming viruses and placing them into a taxonomic system. Similar to the classification systems used for cellular organisms, virus classification is the subject of ongoing debate and proposals. This is mainly due to the pseudo-living nature of viruses, which is to say they are non-living particles with some chemical characteristics similar to those of life, or non-cellular life. As such, they do not fit neatly into the established biological classification system in place for cellular organisms.\n\nViruses are mainly classified by phenotypic characteristics, such as morphology, nucleic acid type, mode of replication, host organisms, and the type of disease they cause. Currently, two main schemes are used for the classification of viruses: the International Committee on Taxonomy of Viruses (ICTV) system and Baltimore classification system, which places viruses into one of seven groups. Accompanying this broad method of classification are specific naming conventions and further classification guidelines set out by the ICTV.\n\nA catalogue of all the world's viruses has been proposed; some related preliminary efforts have been accomplished.\n\nSpecies form the basis for any biological classification system. The ICTV had adopted the principle that a virus species is a polythetic class of viruses that constitutes a replicating lineage and occupies a particular ecological niche. In July 2013, the ICTV definition of species changed to state: \"A species is a monophyletic group of viruses whose properties can be distinguished from those of other species by multiple criteria.\"\n\nThe International Committee on Taxonomy of Viruses began to devise and implement rules for the naming and classification of viruses early in the 1970s, an effort that continues to the present. The ICTV is the only body charged by the International Union of Microbiological Societies with the task of developing, refining, and maintaining a universal virus taxonomy. \n\nThe system shares many features with the classification system of cellular organisms, such as taxon structure. However, this system of nomenclature differs from other taxonomic codes on several points. A minor point is that names of orders and families are italicized, unlike in the International Code of Nomenclature for algae, fungi, and plants and International Code of Zoological Nomenclature.\n\nViral classification starts at the level of order and continues as follows, with the taxon suffixes given in italics:\n\nSpecies names often take the form of \"[Disease] virus\", particularly for higher plants and animals.\n\nThe establishment of an order is based on the inference that the virus families it contains have most likely evolved from a common ancestor. The majority of virus families remain unplaced. As of 2012, seven orders, 96 families, 22 subfamilies, 420 genera, and 2,618 species of viruses have been defined by the ICTV. The orders are the \"Caudovirales, Herpesvirales, Ligamenvirales, Mononegavirales, Nidovirales, Picornavirales, Bunyavirales\" and \"Tymovirales\". These orders span viruses with varying host ranges. The \"Ligamenvirales\" (Group I) , infecting archaea, and \"Bunyavirales\" (Group V) are the most recent additions to the classification system.\n\n\"Caudovirales\" are tailed dsDNA (group I) bacteriophages.\n\n\"Herpesvirales\" contain large eukaryotic dsDNA viruses.\n\n\"Ligamenvirales\" contains linear, dsDNA (group I) archaean viruses.\n\n\"Mononegavirales\" include nonsegmented (-) strand ssRNA (Group V) plant and animal viruses.\n\n\"Nidovirales\" are composed of (+) strand ssRNA (Group IV) viruses with vertebrate hosts.\n\n\"Picornavirales\" contains small (+) strand ssRNA viruses that infect a variety of plant, insect and animal hosts.\n\n\"Tymovirales\" contain monopartite (+) ssRNA viruses that infect plants.\n\n\"Bunyavirales\" contain tripartite (-) ssRNA viruses (Group V).\n\nOther variations occur between the orders: \"Nidovirales\", for example, are isolated for their differentiation in expressing structural and nonstructural proteins separately.\n\nIt has been suggested that similarity in virion assembly and structure observed for certain viral groups infecting hosts from different domains of life (e.g., bacterial tectiviruses and eukaryotic adenoviruses or prokaryotic Caudovirales and eukaryotic herpesviruses) reflects an evolutionary relationship between these viruses. Therefore, structural relationship between viruses has been suggested to be used as a basis for defining higher-level taxa - structure-based viral lineages - that could complement the existing ICTV classification scheme.\nBaltimore classification (first defined in 1971) is a classification system that places viruses into one of seven groups depending on a combination of their nucleic acid (DNA or RNA), strandedness (single-stranded or double-stranded), Sense, and method of replication. Named after David Baltimore, a Nobel Prize-winning biologist, these groups are designated by Roman numerals. Other classifications are determined by the disease caused by the virus or its morphology, neither of which are satisfactory due to different viruses either causing the same disease or looking very similar. In addition, viral structures are often difficult to determine under the microscope. Classifying viruses according to their genome means that those in a given category will all behave in a similar fashion, offering some indication of how to proceed with further research. Viruses can be placed in one of the seven following groups:\n\n\n\n\nHolmes (1948) used Carl Linnaeus's system of binomial nomenclature to classify viruses into 3 groups under one order, Virales. They are placed as follows:\n\n\nThe LHT System of Virus Classification is based on chemical and physical characters like nucleic acid (DNA or RNA), Symmetry (Helical or Icosahedral or Complex), presence of envelope, diameter of capsid, number of capsomers. This classification was approved by the Provisional Committee on Nomenclature of Virus (PNVC) of the International Association of Microbiological Societies (1962). It is as follows:\n\nThe following agents are smaller than viruses but have only some of their properties.\n\n\nSatellites depend on co-infection of a host cell with a helper virus for productive multiplication. Their nucleic acids have substantially distinct nucleotide sequences from either their helper virus or host. When a satellite subviral agent encodes the coat protein in which it is encapsulated, it is then called a satellite virus.\n\nPrions, named for their description as \"\"pr\"oteinaceous and \"in\"fectious particles\", lack any detectable (as of 2002) nucleic acids or virus-like particles. They resist inactivation procedures that normally affect nucleic acids.\n\n\n", "id": "94320", "title": "Virus classification"}
{"url": "https://en.wikipedia.org/wiki?curid=48664569", "text": "Cormus\n\nCormus (\"cormi\") – from ancient Greek, κορμός - \"kormόs\" = trunk of a tree with the boughs cut – is an organism made up of a number of individuals, such as, for example, would be formed by a process of budding from a parent stalk where the buds remain attached.\n\n", "id": "48664569", "title": "Cormus"}
{"url": "https://en.wikipedia.org/wiki?curid=10761675", "text": "Form classification\n\nForm classification is the classification of organisms based on their morphology, which does not necessarily reflect their biological relationships. Form classification, generally restricted to palaeontology, reflects uncertainty; the goal of science is to move \"form taxa\" to biological taxa whose affinity is known.\n\nForm taxonomy is restricted to fossils that preserve too few characters for a conclusive taxonomic definition or assessment of their biological affinity, but whose study is made easier if a binomial name is available by which to identify them. The term \"form classification\" is preferred to \"form taxonomy\"; taxonomy suggests that the classification implies a biological affinity, whereas form classification is about giving a name to a group of morphologically-similar organisms that may not be related.\n\nForm taxa are groupings that are based on common overall forms. Early attempts at classification of labyrinthodonts was based on skull shape (the heavily armoured skulls often being the only preserved part). The amount of convergent evolution in the many groups lead to a number of polyphyletic taxa. Such groups are united by a common mode of life, often one that is generalist, in consequence acquiring generally similar body shapes by convergent evolution. Ediacaran biota — whether they are the precursors of the Cambrian explosion of the fossil record, or are unrelated to any modern phylum — can currently only be grouped in \"form taxa\". Other examples include the seabirds and the \"Graculavidae\". The latter were initially described as the earliest family of Neornithes but are nowadays recognized to unite a number of unrelated early neornithine lineages, several of which probably later gave rise to the \"seabird\" form taxon of today.\n\nA \"parataxon\" (not to be confused with parataxonomy), or \"sciotaxon\" (Gr. \"shadow taxon\"), is a classification based on incomplete data: for instance, the larval stage of an organism that cannot be matched up with an adult. It reflects a paucity of data that makes biological classification impossible. A sciotaxon is defined as a taxon thought to be equivalent to a true taxon (orthotaxon), but whose identity cannot be established because the two candidate taxa are preserved in different ways and thus cannot be compared directly.\n\nIn paleobotany, two terms were formerly used in the codes of nomenclature, \"form genera\" and \"organ genera\", to mean groups of fossils of a particular part of a plant, such as a leaf or seed, whose parent plant is not known because the fossils were preserved unattached to the parent plant. A later term \"morphotaxa\" also allows for differences in preservational state. These three terms have been replaced as of 2011 by provisions for \"fossil-taxa\" that are more similar to the provisions for other types of plants.\n\nNames given to organ genera could only be applied to the organs in question, and could not be extended to the entire organism. Fossil-taxon names can cover several parts of an organism, or several preservational states, but do not compete for priority with any names for the same organism that are based on a non-fossil type.\n\nThe part of the plant was often, but not universally, indicated by the use of a suffix in the generic name:\n\n\"Form taxon\" can more casually be used to describe a wastebasket taxon: either a taxon that is not a natural (monophyletic) group but united by shared plesiomorphies, or a presumably artificial group of organisms whose true relationships are not known, being obscured by ecomorphological similarity. Well-known form taxa of this kind include \"ducks\", \"fish\", \"reptiles\" and \"worms\".\n\n", "id": "10761675", "title": "Form classification"}
{"url": "https://en.wikipedia.org/wiki?curid=9019997", "text": "Indigenous (ecology)\n\nIn biogeography, a species is defined as indigenous to a given region or ecosystem if its presence in that region is the result of only natural process, with no human intervention. The term is equivalent to \"native\" in less scientific usage. Every wild organism (as opposed to a domesticated organism) has its own natural range of distribution in which it is regarded as indigenous. Outside this native range, a species may be introduced by human activity; it is then referred to as an \"introduced species\" within the regions where it was anthropogenically introduced.\n\nAn indigenous species is not necessarily endemic. In biology and ecology, endemic means \"exclusively\" native to the biota of a specific place. An indigenous species may occur in areas other than the one under consideration.\n\nThe terms \"endemic\" and \"indigenous\" do not imply that an organism necessarily originated or evolved from where it is found.\n\n\n", "id": "9019997", "title": "Indigenous (ecology)"}
{"url": "https://en.wikipedia.org/wiki?curid=4585070", "text": "Non-cellular life\n\nNon-cellular life is life that exists without a cellular structure for at least part of its life cycle. Historically, most (descriptive) definitions of life postulated that a living organism must be composed of one or more cells, but this is no longer considered necessary, and modern criteria allow for forms of life based on other structural arrangements.\n\nThe primary candidates for non-cellular life are viruses. A minority of biologists consider viruses to be living organisms, but most do not. Their primary objection is that no known viruses are capable of autopoiesis, which means they cannot reproduce themselves: they must rely on cells to copy them. However, the recent discovery of giant viruses that possess genes for part of the required translation machinery has raised the prospect that they may have had extinct ancestors that could evolve and replicate independently. Most biologists agree that such an ancestor would be a \"bona fide\" non-cellular lifeform, but its existence and characteristics are still uncertain.\n\nEngineers sometimes use the term \"artificial life\" to refer to software and robots inspired by biological processes, but these do not satisfy any biological definition of life.\n\nThe nature of viruses was unclear for many years following their discovery as pathogens. They were described as poisons or toxins at first, then as \"infectious proteins\", but with advances in microbiology it became clear that they also possessed genetic material, a defined structure, and the ability to spontaneously assemble from their constituent parts. This spurred extensive debate as to whether they should be regarded as fundamentally organic or inorganic — as very small biological organisms or very large biochemical molecules — and since the 1950s many scientists have thought of viruses as existing at the border between chemistry and life; a gray area between living and nonliving.\n\nThe recent discovery of giant viruses (aka giruses, nucleocytoplasmic large DNA viruses, NCLDVs) has reignited this debate, since they are not only physically larger than previously known viruses, but also possess much more extensive genomes, including genes coding for aminoacyl tRNA synthetases, key proteins involved in translation, which were previously thought to be exclusive to cellular organisms. This raises the prospect that the giant viruses may have had extinct ancestors (or even undiscovered ones) capable of engaging in life processes (such as evolution and replication) independent of cells, and that modern viruses lost those abilities secondarily. The viral lineage including this ancestor would be ancient and may have originated alongside the earliest archaea or before the LUCA. If such a virus is discovered, or its past existence supported by further genetic evidence, most biologists agree that it would constitute a \"bona fide\" lifeform, and its descendants (at least the giant viruses, and possibly including all known viruses) could be phylogenetically classified in a fourth domain of life. Discovery of the pandoraviruses, with genomes are even larger than the other giant viruses and exhibiting particularly low homology with the three existing domains, further discredited the traditional view that viruses simply \"pick-pocketed\" all of their genes from cellular organisms, and further supported the \"complex ancestors\" hypothesis.\n\nOngoing research is being conducted in this area, using techniques such as phylogenetic bracketing on the giant viruses to infer characteristics of their proposed progenitor.\n\nFurthermore, Viral replication and self-assembly has implications for the study of the origin of life, as it lends further credence to the hypothesis that life could have started as self-assembling organic molecules.\n\nViroids are the smallest infectious pathogens known, consisting solely of short strands of circular, single-stranded RNA without protein coats. They are mostly plant pathogens, some of which are of commercial importance. Viroid genomes are extremely small in size, ranging from 246 to 467 nucleobases. In comparison, the genome of the smallest known viruses capable of causing an infection by themselves are around 2,000 nucleobases in size. Viroids are the first known representatives of a new biological realm of sub-viral pathogens.\n\nViroid RNA does not code for any protein. Its replication mechanism hijacks RNA polymerase II, a host cell enzyme normally associated with synthesis of messenger RNA from DNA, which instead catalyzes \"rolling circle\" synthesis of new RNA using the viroid's RNA as a template. Some viroids are ribozymes, having catalytic properties which allow self-cleavage and ligation of unit-size genomes from larger replication intermediates.\n\nViroids attained significance beyond plant virology since one possible explanation of their origin is that they represent “living relics” from a hypothetical, ancient, and non-cellular RNA world before the evolution of DNA or protein. This view was first proposed in the 1980s, and regained popularity in the 2010s to explain crucial intermediate steps in the evolution of life from inanimate matter (Abiogenesis).\n\nIn discussing the taxonomic domains of life, the terms \"Acytota\" or \"Aphanobionta\" are occasionally used as the name of a viral kingdom, domain, or empire. The corresponding cellular life name would be Cytota. Non-cellular organisms and cellular life would be the two top-level subdivisions of life, whereby life as a whole would be known as organisms, Naturae, or Vitae. The taxon Cytota would include three top-level subdivisions of its own, the domains Bacteria, Archaea, and Eukarya.\n\n", "id": "4585070", "title": "Non-cellular life"}
{"url": "https://en.wikipedia.org/wiki?curid=30463", "text": "Taxonomy (biology)\n\nTaxonomy () is the science of defining and naming groups of biological organisms on the basis of shared characteristics. Organisms are grouped together into taxa (singular: taxon) and these groups are given a taxonomic rank; groups of a given rank can be aggregated to form a super-group of higher rank, thus creating a taxonomic hierarchy. The principal ranks in modern use are domain, kingdom, phylum (division is sometimes used in botany in place of phylum), class, order, family, genus and species. The Swedish botanist Carl Linnaeus is regarded as the father of taxonomy, as he developed a system known as Linnaean taxonomy for categorization of organisms and binomial nomenclature for naming organisms.\n\nWith the advent of such fields of study as phylogenetics, cladistics, and systematics, the Linnaean system has progressed to a system of modern biological classification based on the evolutionary relationships between organisms, both living and extinct.\n\nThe exact definition of taxonomy varies from source to source, but the core of the discipline remains: the conception, naming, and classification of groups of organisms. As points of reference, recent definitions of taxonomy are presented below:\n\n\nThe varied definitions either place taxonomy as a sub-area of systematics (definition 2), invert that relationship (definition 6), or appear to consider the two terms synonymous. There is some disagreement as to whether biological nomenclature is considered a part of taxonomy (definitions 1 and 2), or a part of systematics outside taxonomy. For example, definition 6 is paired with the following definition of systematics that places nomenclature outside taxonomy:\n\n\nA whole set of terms including taxonomy, systematic biology, systematics, biosystematics, scientific classification, biological classification, and phylogenetics have at times had overlapping meanings – sometimes the same, sometimes slightly different, but always related and intersecting. The broadest meaning of \"taxonomy\" is used here. The term itself was introduced in 1813 by de Candolle, in his \"Théorie élémentaire de la botanique\".\n\nThe term \"alpha taxonomy\" is primarily used today to refer to the discipline of finding, describing, and naming taxa, particularly species. In earlier literature, the term had a different meaning, referring to morphological taxonomy, and the products of research through the end of the 19th century.\n\nWilliam Bertram Turrill introduced the term \"alpha taxonomy\" in a series of papers published in 1935 and 1937 in which he discussed the philosophy and possible future directions of the discipline of taxonomy. … there is an increasing desire amongst taxonomists to consider their problems from wider viewpoints, to investigate the possibilities of closer co-operation with their cytological, ecological and genetical colleagues and to acknowledge that some revision or expansion, perhaps of a drastic nature, of their aims and methods, may be desirable … Turrill (1935) has suggested that while accepting the older invaluable taxonomy, based on structure, and conveniently designated \"alpha\", it is possible to glimpse a far-distant taxonomy built upon as wide a basis of morphological and physiological facts as possible, and one in which \"place is found for all observational and experimental data relating, even if indirectly, to the constitution, subdivision, origin, and behaviour of species and other taxonomic groups\". Ideals can, it may be said, never be completely realized. They have, however, a great value of acting as permanent stimulants, and if we have some, even vague, ideal of an \"omega\" taxonomy we may progress a little way down the Greek alphabet. Some of us please ourselves by thinking we are now groping in a \"beta\" taxonomy.\n\nTurrill thus explicitly excludes from alpha taxonomy various areas of study that he includes within taxonomy as a whole, such as ecology, physiology, genetics, and cytology. He further excludes phylogenetic reconstruction from alpha taxonomy (pp. 365–366).\n\nLater authors have used the term in a different sense, to mean the delimitation of species (not subspecies or taxa of other ranks), using whatever investigative techniques are available, and including sophisticated computational or laboratory techniques. Thus, Ernst Mayr in 1968 defined beta taxonomy as the classification of ranks higher than species.An understanding of the biological meaning of variation and of the evolutionary origin of groups of related species is even more important for the second stage of taxonomic activity, the sorting of species into groups of relatives (\"taxa\") and their arrangement in a hierarchy of higher categories. This activity is what the term classification denotes; it is also referred to as beta taxonomy.\n\nHow species should be defined in a particular group of organisms gives rise to practical and theoretical problems that are referred to as the species problem. The scientific work of deciding how to define species has been called microtaxonomy. By extension, macrotaxonomy is the study of groups at higher taxonomic ranks, from subgenus and above only, than species.\n\nWhile some descriptions of taxonomic history attempt to date taxonomy to ancient civilizations, a truly scientific attempt to classify organisms did not occur until the 18th century. Earlier works were primarily descriptive and focused on plants that were useful in agriculture or medicine. There are a number of stages in this scientific thinking. Early taxonomy was based on arbitrary criteria, the so-called \"artificial systems\", including Linnaeus's system of sexual classification. Later came systems based on a more complete consideration of the characteristics of taxa, referred to as \"natural systems\", such as those of de Jussieu (1789), de Candolle (1813) and Bentham and Hooker (1862–1863). These were pre-evolutionary in thinking. The publication of Charles Darwin's \"On the Origin of Species\" (1859) led to new ways of thinking about classification based on evolutionary relationships. This was the concept of phyletic systems, from 1883 onwards. This approach was typified by those of Eichler (1883) and Engler (1886–1892). The advent of molecular genetics and statistical methodology allowed the creation of the modern era of \"phylogenetic systems\" based on cladistics, rather than morphology alone.\n\nNaming and classifying our surroundings has probably been taking place as long as mankind has been able to communicate. It would always have been important to know the names of poisonous and edible plants and animals in order to communicate this information to other members of the family or group. Medicinal plant illustrations show up in Egyptian wall paintings from c. 1500 BC, indicating that the uses of different species were understood and that a basic taxonomy was in place.\n\nIn Canto 3, chapter 10 of \"Bhagavata Purana\" 6 types of trees are recognised, by name:\n\nOrganisms were first classified by Aristotle (Greece, 384–322 BC) during his stay on the Island of Lesbos. He classified beings by their parts, or in modern terms \"attributes\", such as having live birth, having four legs, laying eggs, having blood, or being warm-bodied. He divided all living things into two groups: plants and animals. Some of his groups of animals, such as \"Anhaima\" (animals without blood, translated as invertebrates) and \"Enhaima\" (animals with blood, roughly the vertebrates), as well as groups like the sharks and cetaceans, are still commonly used today. His student Theophrastus (Greece, 370–285 BC) carried on this tradition, mentioning some 500 plants and their uses in his \"Historia Plantarum\". Again, several plant groups currently still recognized can be traced back to Theophrastus, such as \"Cornus\", \"Crocus\", and \"Narcissus\".\n\nTaxonomy in the Middle Ages was largely based on the Aristotelian system, with additions concerning the philosophical and existential order of creatures. This included concepts such as the Great chain of being in the Western scholastic tradition, again deriving ultimately from Aristotle. Aristotelian system did not classify plants or fungi, due to the lack of microscope at the time, as his ideas were based on arranging the complete world in a single continuum, as per the \"scala naturae\" (the Natural Ladder). This, as well, was taken into consideration in the Great chain of being. Advances were made by scholars such as Procopius, Timotheos of Gaza, Demetrios Pepagomenos, and Thomas Aquinas. Medieval thinkers used abstract philosophical and logical categorizations more suited to abstract philosophy than to pragmatic taxonomy.\n\nDuring the Renaissance, the Age of Reason, and the Enlightenment, categorizing organisms became more prevalent,\nand taxonomic works became ambitious enough to replace the ancient texts. This is sometimes credited to the development of sophisticated optical lenses, which allowed the morphology of organisms to be studied in much greater detail. One of the earliest authors to take advantage of this leap in technology was the Italian physician Andrea Cesalpino (1519–1603), who has been called \"the first taxonomist\". His magnum opus \"De Plantis\" came out in 1583, and described more than 1500 plant species. Two large plant families that he first recognized are still in use today: the Asteraceae and Brassicaceae. Then in the 17th century John Ray (England, 1627–1705) wrote many important taxonomic works. Arguably his greatest accomplishment was \"Methodus Plantarum Nova\" (1682), in which he published details of over 18,000 plant species. At the time, his classifications were perhaps the most complex yet produced by any taxonomist, as he based his taxa on many combined characters. The next major taxonomic works were produced by Joseph Pitton de Tournefort (France, 1656–1708). His work from 1700, \"Institutiones Rei Herbariae\", included more than 9000 species in 698 genera, which directly influenced Linnaeus, as it was the text he used as a young student.\n\nThe Swedish botanist Carl Linnaeus (1707–1778) ushered in a new era of taxonomy. With his major works \"Systema Naturae\" 1st Edition in 1735, \"Species Plantarum\" in 1753, and \"Systema Naturae\" 10th Edition, he revolutionized modern taxonomy. His works implemented a standardized binomial naming system for animal and plant species, which proved to be an elegant solution to a chaotic and disorganized taxonomic literature. He not only introduced the standard of class, order, genus, and species, but also made it possible to identify plants and animals from his book, by using the smaller parts of the flower. Thus the Linnaean system was born, and is still used in essentially the same way today as it was in the 18th century. Currently, plant and animal taxonomists regard Linnaeus' work as the \"starting point\" for valid names (at 1753 and 1758 respectively). Names published before these dates are referred to as \"pre-Linnaean\", and not considered valid (with the exception of spiders published in \"Svenska Spindlar\"). Even taxonomic names published by Linnaeus himself before these dates are considered pre-Linnaean.\n\nWhereas Linnaeus classified for ease of identification, the idea of the Linnaean taxonomy as translating into a sort of dendrogram of the Animal- and Plant Kingdoms was formulated toward the end of the 18th century, well before \"On the Origin of Species\" was published. Among early works exploring the idea of a transmutation of species were Erasmus Darwin's 1796 \"Zoönomia\" and Jean-Baptiste Lamarck's \"Philosophie Zoologique\" of 1809. The idea was popularised in the Anglophone world by the speculative but widely read \"Vestiges of the Natural History of Creation\", published anonymously by Robert Chambers in 1844.\n\nWith Darwin's theory, a general acceptance quickly appeared that a classification should reflect the Darwinian principle of common descent. Tree of life representations became popular in scientific works, with known fossil groups incorporated. One of the first modern groups tied to fossil ancestors was birds. Using the then newly discovered fossils of \"Archaeopteryx\" and \"Hesperornis\", Thomas Henry Huxley pronounced that they had evolved from dinosaurs, a group formally named by Richard Owen in 1842. The resulting description, that of dinosaurs \"giving rise to\" or being \"the ancestors of\" birds, is the essential hallmark of evolutionary taxonomic thinking. As more and more fossil groups were found and recognized in the late 19th and early 20th centuries, palaeontologists worked to understand the history of animals through the ages by linking together known groups. \n\nThe cladistic method (or cladism) has emerged since the 1960s. In 1958, Julian Huxley used the term clade. Later, in 1960, Cain and Harrison introduced the term cladistic. The salient feature is arranging taxa in a hierarchical evolutionary tree, ignoring ranks. A taxon is called monophyletic, if it includes all the descendants of an ancestral form. Groups that have descendant groups removed from them (e.g. dinosaurs, with birds as offspring group) are termed paraphyletic, while groups representing more than one branch from the tree of life are called polyphyletic. The \"International Code of Phylogenetic Nomenclature\" or \"PhyloCode\" is intended to regulate the formal naming of clades. Linnaean ranks will be optional under the \"PhyloCode\", which is intended to coexist with the current, rank-based codes.\n\nWell before Linnaeus, plants and animals were considered separate Kingdoms. Linnaeus used this as the top rank, dividing the physical world into the plant, animal and mineral kingdoms. As advances in microscopy made classification of microorganisms possible, the number of kingdoms increased, five and six-kingdom systems being the most common.\n\nDomains are a relatively new grouping. First proposed in 1977, Carl Woese's three-domain system was not generally accepted until later. One main characteristic of the three-domain method is the separation of Archaea and Bacteria, previously grouped into the single kingdom Bacteria (a kingdom also sometimes called Monera), with the Eukaryota for all organisms whose cells contain a nucleus. A small number of scientists include a sixth kingdom, Archaea, but do not accept the domain method.\n\nThomas Cavalier-Smith, who has published extensively on the classification of protists, has recently proposed that the Neomura, the clade that groups together the Archaea and Eucarya, would have evolved from Bacteria, more precisely from Actinobacteria. His 2004 classification treated the archaeobacteria as part of a subkingdom of the Kingdom Bacteria, i.e. he rejected the three-domain system entirely. Stefan Luketa in 2012 proposed a five \"dominion\" system, adding Prionobiota (acellular and without nucleic acid) and Virusobiota (acellular but with nucleic acid) to the traditional three domains.\nPartial classifications exist for many individual groups of organisms and are revised and replaced as new information becomes available, however comprehensive treatments of most or all life are rarer; two recent examples are that of Adl et al., 2012, which covers eukaryotes only with an emphasis on protists, and Ruggiero et al., 2015, covering both eukaryotes and prokaryotes to the rank of Order, although both exclude fossil representatives.\n\nBiological taxonomy is a sub-discipline of biology, and is generally practiced by biologists known as \"taxonomists\", though enthusiastic naturalists are also frequently involved in the publication of new taxa. Because taxonomy aims to describe and organize life, the work conducted by taxonomists is essential for the study of biodiversity and the resulting field of conservation biology.\n\nBiological classification is a critical component of the taxonomic process. As a result, it informs the user as to what the relatives of the taxon are hypothesized to be. Biological classification uses taxonomic ranks, including among others (in order from most inclusive to least inclusive): Domain, Kingdom, Phylum, Class, Order, Family, Genus, and Species.\n\nThe \"definition\" of a taxon is encapsulated by its description or its diagnosis or by both combined. There are no set rules governing the definition of taxa, but the naming and publication of new taxa is governed by sets of rules. In zoology, the nomenclature for the more commonly used ranks (superfamily to subspecies), is regulated by the \"International Code of Zoological Nomenclature\" (\"ICZN Code\"). In the fields of botany, phycology, and mycology, the naming of taxa is governed by the \"International Code of Nomenclature for algae, fungi, and plants\" (\"ICN\").\n\nThe initial description of a taxon involves five main requirements:\n\n\nHowever, often much more information is included, like the geographic range of the taxon, ecological notes, chemistry, behavior, etc. How researchers arrive at their taxa varies: depending on the available data, and resources, methods vary from simple quantitative or qualitative comparisons of striking features, to elaborate computer analyses of large amounts of DNA sequence data.\n\nAn \"authority\" may be placed after a scientific name. The authority is the name of the scientist or scientists who first validly published the name. For example, in 1758 Linnaeus gave the Asian elephant the scientific name \"Elephas maximus\", so the name is sometimes written as \"\"Elephas maximus\" Linnaeus, 1758\". The names of authors are frequently abbreviated: the abbreviation \"L., for Linnaeus,\" is commonly used. In botany, there is, in fact, a regulated list of standard abbreviations (see list of botanists by author abbreviation). The system for assigning authorities differs slightly between botany and zoology. However, it is standard that if a species' name or placement has been changed since the original description, the original authority's name is placed in parentheses.\n\nIn phenetics, also known as taximetrics, or numerical taxonomy, organisms are classified based on overall similarity, regardless of their phylogeny or evolutionary relationships. It results in a measure of evolutionary \"distance\" between taxa. Phenetic methods have become relatively rare in modern times, largely superseded by cladistic analyses, as phenetic methods do not distinguish plesiomorphic from apomorphic traits. However, certain phenetic methods, such as neighbor joining, have found their way into cladistics, as a reasonable approximation of phylogeny when more advanced methods (such as Bayesian inference) are too computationally expensive.\n\nModern taxonomy uses database technologies to search and catalogue classifications and their documentation. While there is no commonly used database, there are comprehensive databases such as the \"Catalogue of Life\", which attempts to list every documented species. The catalogue listed 1.64 million species for all kingdoms as of April 2016, claiming coverage of more than three quarters of the estimated species known to modern science.\n\n\n", "id": "30463", "title": "Taxonomy (biology)"}
{"url": "https://en.wikipedia.org/wiki?curid=25466113", "text": "Cormophyte\n\nCormophytes are the \"plants differentiated into roots, shoots and leaves, and well adapted for life on land, comprising pteridophytes and the Spermatophyta.\" \nThese plants differ from thallophytes, whose body is referred to as the thallus, i.e. a simple body not differentiated into leaf and stem, as of lichens, multicellular algae and some liverworts.\n", "id": "25466113", "title": "Cormophyte"}
{"url": "https://en.wikipedia.org/wiki?curid=56209785", "text": "List of taxa with candidatus status\n\nThis is a list of taxa with candidatus status.\n\n\n\n\n\n\nUnless otherwise noted (♦), these entries are from LPSN.\n\n\n", "id": "56209785", "title": "List of taxa with candidatus status"}
{"url": "https://en.wikipedia.org/wiki?curid=1651963", "text": "Biomedical cybernetics\n\nBiomedical cybernetics investigates signal processing, decision making and control structures in living organisms. Applications of this research field are in biology, ecology and health sciences.\n\n\n\n\n\n", "id": "1651963", "title": "Biomedical cybernetics"}
{"url": "https://en.wikipedia.org/wiki?curid=16554664", "text": "Living systems\n\nLiving systems are open self-organizing life forms that interact with their environment. These systems are maintained by flows of information, energy and matter.\n\nSome scientists have proposed in the last few decades that a general living systems theory is required to explain the nature of life. Such a general theory, arising out of the ecological and biological sciences, attempts to map general principles for how all living systems work. Instead of examining phenomena by attempting to break things down into components, a general living systems theory explores phenomena in terms of dynamic patterns of the relationships of organisms with their environment.\n\nLiving systems theory is a general theory about the existence of all living systems, their structure, interaction, behavior and development. This work is created by James Grier Miller, which was intended to formalize the concept of life. According to Miller's original conception as spelled out in his magnum opus \"Living Systems\", a \"living system\" must contain each of twenty \"critical subsystems\", which are defined by their functions and visible in numerous systems, from simple cells to organisms, countries, and societies. In \"Living Systems\" Miller provides a detailed look at a number of systems in order of increasing size, and identifies his subsystems in each. \nMiller considers living systems as a subset of all systems. Below the level of living systems, he defines space and time, matter and energy, information and entropy, levels of organization, and physical and conceptual factors, and above living systems ecological, planetary and solar systems, galaxies, etc.\n\nLiving systems according to Parent (1996) are by definition \"open self-organizing systems that have the special characteristics of life and interact with their environment. This takes place by means of information and material-energy exchanges. Living systems can be as simple as a single cell or as complex as a supranational organization such as the European Union. Regardless of their complexity, they each depend upon the same essential twenty subsystems (or processes) in order to survive and to continue the propagation of their species or types beyond a single generation\".\n\nMiller said that systems exist at eight \"nested\" hierarchical levels: cell, organ, organism, group, organization, community, society, and supranational system. At each level, a system invariably comprises twenty critical subsystems, which process matter–energy or information except for the first two, which process both matter–energy and information: reproducer and boundary.\n\nThe processors of matter–energy are: \n\nThe processors of information are:\n\nJames Grier Miller in 1978 wrote a 1,102-page volume to present his living systems theory. He constructed a general theory of living systems by focusing on concrete systems—nonrandom accumulations of matter–energy in physical space–time organized into interacting, interrelated subsystems or components. Slightly revising the original model a dozen years later, he distinguished eight \"nested\" hierarchical levels in such complex structures. Each level is \"nested\" in the sense that each higher level contains the next lower level in a nested fashion.\n\nHis central thesis is that the systems in existence at all eight levels are open systems composed of twenty critical subsystems that process inputs, throughputs, and outputs of various forms of matter–energy and information. Two of these subsystems—reproducer and boundary—process both matter–energy and information. Eight of them process only matter–energy. The other ten process information only. \nAll nature is a continuum. The endless complexity of life is organized into patterns which repeat themselves—theme and variations—at each level of system. These similarities and differences are proper concerns for science. From the ceaseless streaming of protoplasm to the many-vectored activities of supranational systems, there are continuous flows through living systems as they maintain their highly organized steady states.\nSeppänen (1998) says that Miller applied general systems theory on a broad scale to describe all aspects of living systems.\n\nMiller's theory posits that the mutual interrelationship of the components of a system extends across the hierarchical levels. Examples: Cells and organs of a living system thrive on the food the organism obtains from its suprasystem; the member countries of a supranational system reap the benefits accrued from the communal activities to which each one contributes. Miller says that his eclectic theory \"ties together past discoveries from many disciplines and provides an outline into which new findings can be fitted\".\n\nMiller says the concepts of space, time, matter, energy, and information are essential to his theory because the living systems exist in space and are made of matter and energy organized by information. Miller's theory of living systems employs two sorts of spaces: physical or geographical space, and conceptual or abstracted spaces. Time is the fundamental \"fourth dimension\" of the physical space–time continuum/spiral. Matter is anything that has mass and occupies physical space. Mass and energy are equivalent as one can be converted into the other. Information refers to the degrees of freedom that exist in a given situation to choose among signals, symbols, messages, or patterns to be transmitted.\n\nOther relevant concepts are system, structure, process, type, level, echelon, suprasystem, subsystem, transmissions, and steady state. A system can be conceptual, concrete or abstracted. The structure of a system is the arrangement of the subsystems and their components in three-dimensional space at any point of time. Process, which can be reversible or irreversible, refers to change over time of matter–energy or information in a system. Type defines living systems with similar characteristics. Level is the position in a hierarchy of systems. Many complex living systems, at various levels, are organized into two or more echelons. The suprasystem of any living system is the next higher system in which it is a subsystem or component. The totality of all the structures in a system which carry out a particular process is a subsystem. Transmissions are inputs and outputs in concrete systems. Because living systems are open systems, with continually altering fluxes of matter–energy and information, many of their equilibria are dynamic—situations identified as steady states or flux equilibria.\n\nMiller identifies the comparable matter–energy and information processing critical subsystems. Elaborating on the eight hierarchical levels, he defines society, which constitutes the seventh hierarchy, as \"a large, living, concrete system with [community] and lower am levels of living systems as subsystems and components\". Society may include small, primitive, totipotential communities; ancient city–states, and kingdoms; as well as modern nation–states and empires that are not supranational systems. Miller provides general descriptions of each of the subsystems that fit all eight levels.\n\nA supranational system, in Miller's view, \"is composed of two or more societies, some or all of whose processes are under the control of a decider that is superordinate to their highest echelons\". However, he contends that no supranational system with all its twenty subsystems under control of its decider exists today. The absence of a supranational decider precludes the existence of a concrete supranational system. Miller says that studying a supranational system is problematical because its subsystems\n...tend to consist of few components besides the decoder. These systems do little matter-energy processing. The power of component societies [nations] today is almost always greater than the power of supranational deciders. Traditionally, theory at this level has been based upon intuition and study of history rather than data collection. Some quantitative research is now being done, and construction of global-system models and simulations is currently burgeoning.\n\nAt the supranational system level, Miller's emphasis is on international organizations, associations, and groups comprising representatives of societies (nation–states). Miller identifies the subsystems at this level to suit this emphasis. Thus, for example, the reproducer is \"any multipurpose supranational system which creates a single purpose supranational organization\" (p. 914); and the boundary is the \"supranational forces, usually located on or near supranational borders, which defend, guard, or police them\" (p. 914).\n\nNot just those specialized in international communication, but all communication science scholars could pay particular attention to the major contributions of living systems theory (LST) to social systems approaches that Bailey has pointed out:\n\nBailey says that LST, perhaps the \"most integrative\" social systems theory, has made many more contributions that may be easily overlooked, such as: providing a detailed analysis of types of systems; making a distinction between concrete and abstracted systems; discussion of physical space and time; placing emphasis on information processing; providing an analysis of entropy; recognition of totipotential systems, and partipotential systems; providing an innovative approach to the structure–process issue; and introducing the concept of joint subsystem—a subsystem that belongs to two systems simultaneously; of dispersal—lateral, outward, upward, and downward; of inclusion—inclusion of something from the environment that is not part of the system; of artifact—an animal-made or human-made inclusion; of adjustment process, which combats stress in a system; and of critical subsystems, which carry out processes that all living systems need to survive.\n\nLST's analysis of the twenty interacting subsystems, Bailey adds, clearly distinguishing between matter–energy-processing and information-processing, as well as LST's analysis of the eight interrelated system levels, enables us to understand how social systems are linked to biological systems. LST also analyzes the irregularities or \"organizational pathologies\" of systems functioning (e.g., system stress and strain, feedback irregularities, information–input overload). It explicates the role of entropy in social research while it equates negentropy with information and order. It emphasizes both structure and process, as well as their interrelations.\n\nIt omits the analysis of subjective phenomena, and it overemphasizes concrete Q-analysis (correlation of objects) to the virtual exclusion of R-analysis (correlation of variables). By asserting that societies (ranging from totipotential communities to nation-states and non-supranational systems) have greater control over their subsystem components than supranational systems have, it dodges the issue of transnational power over the contemporary social systems. Miller's supranational system bears no resemblance to the modern world-system that Immanuel Wallerstein (1974) described, although both of them were looking at the same living (dissipative) structure.\n\n\n\n", "id": "16554664", "title": "Living systems"}
{"url": "https://en.wikipedia.org/wiki?curid=18789195", "text": "The Seven Pillars of Life\n\nThe Seven Pillars of Life are the essential principles of life described by Daniel E. Koshland in 2002 in order to create a universal definition of life. One stated goal of this universal definition is to aid in understanding and identifying artificial and extraterrestrial life. The seven pillars are Program, Improvisation, Compartmentalization, Energy, Regeneration, Adaptability, and Seclusion. These can be abbreviated as PICERAS.\n\nKoshland defines \"Program\" as an \"organized plan that describes both the ingredients themselves and the kinetics of the interactions among ingredients as the living system persists through time.\" In natural life as it is known on Earth, the program operates through the mechanisms of nucleic acids and amino acids, but the concept of program can apply to other imagined or undiscovered mechanisms.\n\n\"Improvisation\" refers to the living system's ability to change its program in response to the larger environment in which it exists. An example of improvisation on earth is natural selection.\n\n\"Compartmentalization\" refers to the separation of spaces in the living system that allow for separate environments for necessary chemical processes. Compartmentalization is necessary to protect the concentration of the ingredients for a reaction from outside environments.\n\nBecause living systems involve net movement in terms of chemical movement or body movement, and lose energy in those movements through entropy, energy is required for a living system to exist. The main source of energy on Earth is the sun, but other sources of energy exist for life on Earth, such as hydrogen gas or methane, used in chemosynthesis.\n\n\"Regeneration\" in a living system refers to the general compensation for losses and degradation in the various components and processes in the system. This covers the thermodynamic loss in chemical reactions, the wear and tear of larger parts, and the larger decline of components of the system in ageing. Living systems replace these losses by importing molecules from the outside environment, synthesizing new molecules and components, or creating new generations to start the system over again.\n\n\"Adaptability\" is the ability of a living system to respond to needs, dangers, or changes. It is distinguished from improvisation because the response is timely and does not involve a change of the program. Adaptability occurs from a molecular level to a behavioral level through feedback and feedforward systems. For example, an animal seeing a predator will respond to the danger with hormonal changes and escape behavior.\n\n\"Seclusion\" is the separation of chemical pathways and the specificity of the effect of molecules, so that processes can function separately within the living system. In organisms on Earth, proteins aid in seclusion because of their individualized structure that are specific for their function, so that they can efficiently act without affecting separate functions.\n\nY. N. Zhuravlev and V. A. Avetisov have analyzed Koshland's seven pillars from the context of primordial life and, though calling the concept \"elegant,\" point out that the pillars of compartmentalization, program, and seclusion don't apply well to the non-differentiated earliest life.\n\n", "id": "18789195", "title": "The Seven Pillars of Life"}
{"url": "https://en.wikipedia.org/wiki?curid=44262036", "text": "Coleridge's theory of life\n\nRomanticism grew largely out of an attempt to understand not just inert nature, but also vital nature. Romantic works in the realm of art and Romantic medicine were a response to the general failure of the application of method of inertial science to reveal the foundational laws and operant principles of vital nature. German romantic science and medicine sought to understand the nature of the life principle identified by John Hunter as distinct from matter itself via Johan Friedrich Blumenbach's \"Bildungstrieb\" and Romantic medicine's \"Lebenskraft\", as well as Röschlaub's development of the Brunonian system of medicine system of John Brown, in his \"excitation theory\" of life (German:\"Erregbarkeit theorie\"), working also with Schelling's \"Naturphilosophie\", the work of Goethe regarding morphology, and the first dynamic conception of physiology of Richard Saumarez. But it is in Samuel Taylor Coleridge that we find the question of life and vital nature most intensely and comprehensively examined, particularly in his Hints towards the Formation of a more Comprehensive Theory of Life (1818), providing the foundation for Romantic philosophy, science and medicine. The work is key to understanding the relationship of Romantic literature and science.\n\nThe Enlightenment had developed a philosophy and science supported by formidable twin pillars: the first the Cartesian split of mind and matter, the second Newtonian physics, with its conquest of inert nature, both of which focused the mind's gaze on things or objects. For Cartesian philosophy, life existed on the side of matter, not mind; and for the physical sciences, the method that had been so productive for revealing the secrets of inert nature, should be equally productive in examining vital nature. The initial attempt to seek the cause and principle of life in matter was challenged by John Hunter, who held that the principle of life was not to be found nor confined within matter, but existed independently of matter itself, and informed or animated it, that is, he implied, it was the unifying or antecedent cause of the things or what Aristotelean philosophy termed \"natura naturata\" (the outer appearances of nature).\nThis reduction of the question of life to matter, and the corollary, that the method of the inertial sciences was the way to understand the very phenomenon of life, that is, its very nature and essence as a power (\"natura naturans\"), not as manifestations through sense-perceptible appearances (\"natura naturata\"), also reduced the individual to a material-mechanical 'thing' and seemed to render human freedom an untenable concept. It was this that Romanticism challenged, seeking instead to find an approach to the essence of nature as being also vital not simply inert, through a systematic method involving not just physics, but physiology (living functions). For Coleridge, quantitative analysis was anti-realist and needed to be grounded in qualitative analysis ('-ologies') (as was the case with Goethe's approach).\nAt the same time, the Romantics had to deal with the idealistic view that life was a 'somewhat' outside of things, such that the things themselves lost any real existence, a stream coming through Hume and Kant, and also infusing the German natural philosophical stream, German idealism, and in particular \"naturphilosophie\", eventuating scientifically in the doctrine of 'vitalism'. For the Romantics, life is independent of and antecedent to nature, but also infused and suspended in nature, not apart from it, As David Bohm expresses it in more modern terms \"In nature nothing remains constant…everything comes from other things and gives rise to other things. This principle is…at the foundation of the possibility of our understanding nature in a rational way.\"\n\nAnd as Coleridge explained, 'this antecedent unity, or cause and principle of each union, it has since the time of Bacon and Kepler been customary to call a law.\" And as law, \"we derive from it a progressive insight into the necessity and generation of the phenomena of which it is the law.\"\n\nColeridge's was the dominant mind on many issues involving the philosophy and science in his time, as John Stuart Mill acknowledged, along with others since who have studied the history of Romanticism.\n\nFor Coleridge, as for many of his romantic contemporaries, the idea that matter itself can begat life only dealt with the various changes in the arrangement of particles and did not explain life itself as a principle or power that lay behind the material manifestations, \"natura naturans\" or \"the productive power suspended and, as it were, quenched in the product\" Until this were addressed, according to Coleridge, \"we have not yet attained to a science of nature.\"\n\nThis productive power is above sense experience, but not above nature herself, that is, supersensible, but not supernatural, and, thus, not 'occult' as was the case with vitalism. Vitalism failed to distinguish between spirit and nature, and then within nature, between the visible appearances and the invisible, yet very real and not simply hypothesized notion, essence or motivating principle (\"natura naturans\"). Even Newton spoke of things invisible in themselves (though not in their manifestations), such as force, though Comte, the thorough materialist, complained of the use of such terms as the 'force of gravity' as being relics of animism.\n\nMatter was not a 'datum' or thing in and of itself, but rather a product or effect, and for Coleridge, looking at life in its broadest sense, it was the product of a polarity of forces and energies, but derived from a unity which is itself a power, not an abstract or nominal concept, that is Life, and this polar nature of forces within the power of Life is the very law or 'Idea' (in the Platonic sense) of Creation.\n\nFor Coleridge the essence of the universe is motion and motion is driven by a dynamic polarity of forces that is both inherent in the world as potential and acting inherently in all manifestations. This polarity is the very dynamic that acts throughout all of nature, including into the more particular form of 'life biological', as well as of mind and consciousness.\n\nAnd this polarity is dynamic, that is real, though not visible, and not simply logical or abstract. Thus, the polarity results in manifestations that are real, as the opposite powers are not contradictory, but counteracting and inter-penetrating.\n\nThus, then, Life itself is not a thing—a self-subsistent hypostasis—but an act and process...\n\nAnd in that sense Coleridge re-phrases the question \"What is Life?\" to \"What is not Life that really is?\"\n\nThis dynamic polar essence of nature in all its functions and manifestations is a universal law in the order of the law of gravity and other physical laws of inert nature. And, critically, this dynamic polarity of constituent powers of life at all levels is not outside or above nature, but is within nature (\"natura naturans\"), not as a part of the visible product, but as the ulterior natural functions that produce such products or things.\n\nIt is these functions that provided the bridge being sought by Romantic science and medicine, in particular by Andreas Röschlaub and the Brunonian system of medicine, between the inertial science of inert nature (physics) and the vital science of vital nature (physiology) and its therapeutic application or physic (the domain of the physician).\n\nColeridge was influenced by German philosophy, in particular Kant, Fichte and Schelling (Naturphilosophie), as well as the physiology of Blumenbach and the dynamic excitation theory of life of the Brunonian system. He sought a path that was neither the mystical tendency of the earlier vitalists nor the materialistic reductionist approach to natural science, but a dynamic one.\n\nColeridge's challenge was to describe something that was dynamic neither in mystical terms not materialistic ones, but via analogy, drawing from the examples of inertial science. As one writer explains, he uses the examples of electricity, magnetism and gravity not because they are like life, but because they offer a way of understanding powers, forces and energies, which lie at the heart of life. And using these analogies, Coleridge seeks to demonstrate that life is not a material force, but a product of relations amongst forces. Life is not linear and static, but dynamic process of self-regulation and Emergent evolution that results in increasing complexity and individuation. This spiral, upward movement (cf. Goethe's ideas) creates a force for organization that unifies, and is most intense and powerful in that which is most complex and most individual - the self-regulating, enlightened, developed individual mind. But at the same time, this process of life increases interdependence (like the law of comparative advantage in economics) and associational powers of the mind. Thus, he is not talking about an isolated, individual subjective mind, but about the evolution of a higher level of consciousness and thought at the core of the process of life.\n\nAnd the direction of this motion is towards increasing individuation, that is the creation of specific, individual units of things. At the same time, given the dynamic polarity of the world, there must always be an equal and opposite tendency, in this case, that of connection. So, a given of our experience is that man is both an individual, tending in each life and in history generally to greater and greater individualization, and a social creature seeking interaction and connection. It is the dynamic interplay between the individuation and connecting forces that leads to higher and higher individuation.\n\nColeridge makes a further distinction between mathematics and life, the latter being productive or creative, that is, living, and the former ideal. Thus, the mathematical approach that works so well with inert nature, is not suitable for vital nature.\n\nThe counteraction then of the two assumed forces does not depend on their meeting from opposite directions; the power which acts in them is indestructible; it is therefore inexhaustibly re-ebullient; and as something must be the result of these two forces, both alike infinite, and both alike indestructible; and as rest or neutralization cannot be this result; no other conception is possible, but that the product must be a tertium aliquid, [a third thing] or finite generation. Consequently this conception is necessary. Now this tertium aliquid can be no other than an inter-penetration of the counteracting powers, partaking of both… Consequently the 'constituent powers', that have given rise to a body, may then reappear in it as its function: \"a Power, acting in and by its Product or Representative to a predetermined purpose is a Function...the first product of its energy is the thing itself: \"ipsa se posuit et iam facta est ens positum\". Still, however, its productive energy is not exhausted in this product, but overflows, or is effluent, as the specific forces, properties, faculties, of the product. It reappears, in short, as the function of the body...The vital functions are consequents of the \"Vis Vitae Principium Vitale\", and presuppose the Organs, as the Functionaries.\n\nLife, that is, the essential polarity in unity (multeity in unity) in Coleridge’s sense also has a four beat cycle, different from the arid dialectics of abstraction - namely the tension of the polar forces themselves, the charge of their synthesis, the discharge of their product (indifference) and the resting state of this new form (predominance). The product is not a neutralization, but a new form of the essential forces, these forces remaining within, though now as the functions of the form.\n\nTo make it adequate, we must substitute the idea of positive production for that of rest, or mere neutralization. To the fancy alone it is the null-point, or zero, but to the reason it is the punctum saliens, and the power itself in its eminence.\n\nThis dynamic polarity that is Life is expressed at different levels. At its most basic it is Space-Time, with its product - motion. The interplay of both gives us either a line or a circle, and then there are different degrees possible within a given form or “predominance” of forces. Geometry is not conceivable except as the dynamic interplay of space (periphery) and time (point). Space, time and motion are also geometrically represented by width, length (breadth) and depth. And this correspondence is repeated throughout the scale of Life.\n\nMatter, then, is the product of the dynamic forces - repulsion (centrifugal), and attraction (centripetal); it is not itself a productive power. It is also the mass of a given body.\n\nColeridge’s understanding of life is contrasted with the materialist view which is essentially reduced to defining life as that which is the opposite of not-life, or that which resists death, that is, that which is life.\n\nThe problem for Coleridge and the Romantics was that the intellect, 'left to itself' as Bacon stated, was capable of apprehending only the outer forms of nature (natura naturata) and not the inmost, living functions (natura naturans) giving rise to these forms. Thus, effects can only be 'explained' in terms of other effects, not causes. It takes a different capacity to 'see' these living functions, which is an imaginative activity. For Coleridge, there is an innate, primitive or 'primary' imagination that configures invisibly sense-experience into perception, but a rational perception, that is, one raised into consciousness and awareness and then rationally presentable, requires a higher level, what he termed 'secondary imagination', which is able to connect with the thing being experienced, penetrate to its essence in terms of the living dynamics upholding its outer form, and then present the phenomena as and within its natural law, and further, using reason, develop the various principles of its operation.\n\nThis cognitive capacity involved what Coleridge termed the 'inmost sense' or what Goethe termed the Gemüt. It also involved the reactivation of the old Greek noetic capacity, and the ability to 'see' or produce the theory (Greek \"theoria\" from the verb 'to see') dynamic polarities, or natural Laws, the dynamic transcendent (foundational) entities that Plato termed 'Ideas' (\"eidos\").\n\nSince \"natura naturata\" is sustained by \"natura naturans\", and the creative power of \"natura naturans\" is one of a kind with the human mind, itself creative, then there must be a correspondence or connection between the mind and the things we perceive, such that we can overcome the apparent separation between the object and the representation in the mind of the object that came to bedevil Enlightenment thought (Hume, Kant). As one commentator noted \"to speak at all of the unity of intelligence and nature is of course flatly to contradict Descartes.\"\n\nFor Coleridge the power of life lies in every seed as a potential to be unfolded as a result of interaction with the environment (heat, light, air, moisture, etc.), an insight which allowed him to see in the Brunonian system a dynamic polarity in excitation theory.\nColeridge also saw that there was a progressive movement through time and space of life or the law of polarity, from the level of physics (space and time) and the mineral or inert nature (law of gravity, operating through forces of attraction and repulsion), up to man, with his law of resonance in terms of his innate desire to be himself (force of individuation) and to also connect with like-minded (force of connection), as Goethe expressed in his novel \"Elective Affinities\" (\"Wahlverwandschaften\") as well as in his own life's experience.\nEvolution occurred because the original polarity of creation, the very 'Law of Creation', itself gives birth to subsequent polarities, as each pole is itself a unity that can be further polarized (what Wilhelm Reich later termed 'orgonomic functionalism' and what at the biological level constitutes physiology), an insight that would later be taken up by the concept of emergent evolution, including the emergence of mind and consciousness.\n\nAnd that this is so, is also an intimate and shared experience of all humans, as is set out in Reid's Common Sense philosophy. As Coleridge states\n\nThat nature evolves towards a purpose, and that is the unfolding of the human mind and consciousness in all its levels and degrees, is not teleological but a function of the very nature of the law of polarity or creation itself, namely that of increasing individuation of an original unity, what Coleridge termed 'multeity in unity'. As he states, \"without assigning to nature as nature, a conscious purpose\" we must still \"distinguish her agency from a blind and lifeless mechanism.\"\n\nWhile man contains and is subject to the various laws of nature, man as a self-conscious being is also the summa of a process of creation leading to greater mind and consciousness, that is, a creative capacity of imagination. Instead of being a creature of circumstance, man is the creator of them, or at least has that potential.\n\n", "id": "44262036", "title": "Coleridge's theory of life"}
{"url": "https://en.wikipedia.org/wiki?curid=3447756", "text": "Carbon-based life\n\nCarbon is a key component of all known life on Earth. Complex molecules are made up of carbon bonded with other elements, especially oxygen, hydrogen and nitrogen. Carbon is abundant on Earth. It is also lightweight and relatively small in size, making it easier for enzymes to manipulate carbon\nmolecules. It is frequently assumed in astrobiology that if life exists somewhere else in the universe, it will also be carbon-based. Critics refer to this assumption as \"carbon chauvinism\".\n\n\"What we normally think of as 'life' is based on chains of carbon atoms, with a few other atoms, such as nitrogen or phosphorus\", per Stephen Hawking in a 2008 lecture, \"carbon [...] has the richest chemistry.\" \nThe most important characteristics of carbon as a basis for the chemistry of life are, that it has four valence bonds, and that the energy required to make or break a bond is at an appropriate level for building molecules, which are stable and reactive. Carbon atoms bond readily to other carbon atoms; this allows the building of arbitrarily long complex molecules and polymers.\n\nThere are not many other elements which even appear to be promising candidates for supporting life, for example, processes such as metabolism. The most frequently suggested alternative is silicon. Silicon is in the same group in the Periodic Table of elements, and has four valence bonds, and bonds to itself, generally in the form of crystal lattices rather than long chains. It is considerably more electropositive than carbon. Silicon compounds do not readily recombine into different permutations in a manner that would plausibly support lifelike processes.\n\nThe most notable groups of chemicals used in the processes of living organisms include:\n\n\nSilicon has been a theme of non-carbon-based-life since it is somewhat similar to carbon in its chemical characteristics. \nIn cinematic and literary science fiction, when man-made machines cross from non-living to living, this new form would be an example of non-carbon-based life. Since the advent of the microprocessor in the late 1960s, these machines are often classed as \"silicon-based life\". Another example of \"silicon-based life\" is the episode \"The Devil in the Dark\" from , in which a living rock creature's biochemistry is based on silicon. Also in The X-Files episode \"Firewalker\" where silicon based organism was discovered in a volcano.\n\nIn the movie adaptation of Arthur C. Clarke's \"2010\" (1984) a character argues, \"Whether we are based on carbon or on silicon makes no fundamental difference; we should each be treated with appropriate respect\". This quote may be the basis of Steve Jobs's quip when he introduced Carbon within MacOS X, \"Carbon. All life forms will be based on it.\"\n\n\n", "id": "3447756", "title": "Carbon-based life"}
