{"url": "https://en.wikipedia.org/wiki?curid=25428989", "text": "Homeotic selector gene\n\nHomeotic selector genes confer segment identity in \"Drosophila\". They encode homeodomain proteins which interact with Hox and other homeotic genes to initiate segment-specific gene regulation. Homeodomain proteins are transcription factors that share a DNA-binding domain called the homeodomain. Changes in the expression and function of homeotic genes are responsible for the changes in the morphology of the limbs of arthropods as well as in the axial skeletons of vertebrates. Mutations in homeotic selector genes do not lead to elimination of a segment or pattern, but instead cause the segment to develop incorrectly.\n\nThe homeotic selector genes were discovered through the genetic analysis of Drosophila over 80 years ago. Unusual disturbances were found in the organization of the adult fly, resulting in misplaced limbs, such as legs developing where antennae usually develop or an extra pair of wings developing where halteres should be. This discovery provided a glimpse to understanding how each segment acquires its individual identity.\n\nThe first homeotic gene cluster, the bithorax complex, was discovered by Edward B. Lewis in 1978. Similar mutations in the complex were found to cluster together, leading Lewis to propose that these homeotic genes arose through a duplication mechanism which would conserve the clusters through evolution.\nThe independent discoveries of the homeobox in the 1983 by Walter Gehring's laboratory at the University of Basel, Switzerland, and Thomas Kaufman's laboratory at Indiana University confirmed Lewis's theory.\n\nCollinearity is found between the order of the genes on the chromosome and the order in which the genes are expressed along the anteroposterior axis of the embryo. For example, the lab gene is found in the 3' position in the Antennapedia complex, and is expressed in the most anterior head region of the embryo. At the same time, the Abd-B gene is located at the 5' position of the Bithorax complex, and expressed in the most posterior region of the embryo. This suggests that the genes may be activated through a graded process, in which the action is gradually spread along the chromosome. Although the significance of colinearity is still not understood, it is thought to have an important role, due to its conservation in arthropods, and vertebrates including humans.\n\nHomeotic selector genes encode regulatory DNA-binding proteins which are all related through a highly conserved DNA binding sequences called the homeobox (from which the \"Hox Complex\" name is derived from). Although each all of the DNA-binding complexes are conserved, each para-segment still has an individual identity. The proteins do not bind directly to the DNA, rather, they interact with other regulatory proteins which are already bound to DNA-binding complexes. Different interactions determine which DNA binding sites are recognized and subsequently activated or repressed. Homeotic selector proteins combine in different combinations with regulatory proteins to give each parasegment its identity. \nCertain signals set up the spatial pattern of expression of the Hox complex early in development. The Hox complex acts like a stamp, giving cells in each segment a long term positional value. The cell memory of a given positional value depends on two inputs, the first being the ability of many Hox proteins to autoactivate their own transcription, and the second derived from two large groups of transcriptional regulators: The Polycomb group and the Trithorax group. A defect in either of these regulators results in a pattern which is initially correct but is not maintained at later embryonic stages.\nThe Polycomb and Trithorax regulators act in opposite ways. The Trithorax group maintains Hox transcription after transcription is already activated. The Polycomb group forms stable complexes that bind to the chromatin of Hox genes, and keep it in a repressed state at sites where Hox genes are not active.\n\nHomologs of the Homeotic selector gene are found in a variety of species, varying from cnidarians to nematodes, to mammals. These genes are grouped similarly to the Hox complex found in insects. The mouse has four complexes, HoxA, HoxB, HoxC, and HoxD, each on different chromosomes. Individual genes in each complex correspond to specific members of the Drosophila genome. The mammalian Hox genes can function in Drosophila as partial replacements for the Drosophila Hox genes. Each of the four mammalian Hox complexes has a rough counterpart in the insect complex.\n\nThe theory behind this evolutionary conservation stems from the belief that some common ancestor of worms, flies, and vertebrates had a single primordial homeotic selector gene, an ancestral Hox complex, that went through repeated duplication to form a series of tandem genes. In Drosophila, this ancestral Hox complex split into two separate complexes: Antennapedia and Bithorax. In mammals, the whole complex repeatedly duplicated resulting in four Hox complexes. This theory has some faults, including that some individual genes have been duplicated while others have been lost.\n\nChanges in homeotic gene expression contributes to the diversity. The Drosophila genome holds its eight homeotic genes in two complexes. The Invertebrate genome contains 8-10 of is homeotic genes in only one complex, while Vertebrates have duplicated the Hox complex and have four clusters. Changes in the expression and functionality of individual genes result in various morphology as seen in arthropods. The diversity found between the five groups of arthropods is a result of their modular architecture. The arthropods are composed of a series of repeating body segments that can be modified in a limitless number of ways. While some segments may carry antenna, others can be modified to carry wings. Crustaceans have different morphology within the group due to different patterns of Ubx expression in isopods and brachiopods. Similar to brachiopods, isopods have swimming limbs on the second through eighth thoracic segments, however the limbs on the first thoracic segment are smaller than the others, and are used as feeding limbs. The different pattern of Ubx expression correlates with these modifications, possibly a result of an acquired mutation that allows the Ubx enhancers to no longer mediate expression in the first thoracic segment.\n\nBrachiopods: Src expression is limited to the head region in brachipods and helps in the development of feeding appendages. Ubx is expressed in the thorax where it controls the development of swimming limbs.\n\nIsopods: Src expression is detected in both the head and the first thoracic segment (T1) in isopods and as a result, the swimming limb in T1 is transformed into a feeding appendage (the maxillipped). The posterior expansion of Src is possible by the loss of Ubx expression in T1 because Ubx normally represses Src expression.\n\nEvery insect has six legs, one pair found on each of the three thoracic segments while other arthropods have a variable number of limbs. This change in morphology is due to functional changes in the Ubx regulatory protein. Ubx and abd-A repress the expression of Distal-less, Dll, a gene responsible for the development of limbs. In the Drosophila embryo, Ubx is expressed at high levels in the metathorax and anterior abdominal segments; abd-A is expressed in the posterior abdominal segments. In combination, these two genes do not allow Dll to function in the first seven abdominal segments. However, Ubx is expressed in the metathorax and does not interfere with the Dll expression because Dll is activated before Ubx is expressed.\n\nIn crustaceans, there are high levels of both Ubx and DII in all 11 thoracic segments. The expression of DII promotes the development of swimming limbs. The Ubx protein does not repress DII in crustaceans because Ubx is functionally different in insects and crustaceans.\n", "id": "25428989", "title": "Homeotic selector gene"}
{"url": "https://en.wikipedia.org/wiki?curid=612341", "text": "Haploidisation\n\nHaploidisation is the process of halving the chromosomal content of a cell, creating a haploid cell. Within the normal reproductive cycle, haploidisation is one of the major functional consequences of meiosis, the other being a process of chromosomal crossover that mingles the genetic content of the parental chromosomes. Usually, haploidisation creates a monoploid cell from a diploid progenitor, or it can involve halving of a polyploid cell, for example to make a diploid potato plant from a tetraploid lineage of potato plants.\n\nIf haploidisation is not followed by fertilisation, the result is a haploid lineage of cells. For example, experimental haploidisation may be used to recover a strain of haploid \"Dictyostelium\" from a diploid strain. It sometimes occurs naturally in plants when meiotically reduced cells (usually egg cells) develop by parthenogenesis.\n\nHaploidisation was one of the procedures used by Japanese researchers to produce Kaguya, a fatherless mouse; two haploids were then combined to make the diploid mouse. \n\nHaploidisation commitment is a checkpoint in meiosis which follows the successful completion of premeiotic DNA replication and recombination commitment.\n\n", "id": "612341", "title": "Haploidisation"}
{"url": "https://en.wikipedia.org/wiki?curid=37913847", "text": "Reciprocal silencing\n\nReciprocal silencing, a genetic phenomenon that primarily occurs in plants, refers to the pattern of redundant genes being silenced following a polyploid event. Polyploidy (wholesale genome duplication) is common in plants and constitutes an important method of speciation. When a polyploid species arises, its genome contains homeologs, duplicated chromosomes with equivalent genetic information. However silencing of redundant genes occurs rapidly in new polyploids through genetic and epigenetic means. This primarily occurs because redundancy allows one of the two genes present for each locus to be silenced without affecting the phenotype of the organism, and thus mutations that eliminate gene expression are much less likely to be deleterious or lethal. This allows mutations that would be lethal in diploid populations to accumulate in polyploids. Reciprocal silencing refers to the specific pattern of silencing where equivalent loci in are both silenced and expressed in a reciprocal manner. This phenomenon is observed on two distinct scales.\n\nAllopolyploids are species whose increased complement of genetic material is the result of hybridization of two closely related species. Thus homeologous chromosomes in allopolyploids are equivalent, but not identical. These differences mean that the precise pattern of silencing and expression can have important phenotypic effects. Reciprocal silencing on the population level refers to the case where two populations are each descended from the same allopolyploid. In one population, one of the two equivalent locci (A) is expressed while the other (B) has been silenced, while in the other population the reciprocal pattern occurs, with B being expressed and A silenced. It is important to note that this refers to equivalent loci, specific locations within the genome, rather than the entire homeologous chromosome.\n\nReciprocal silencing on the population level has been proposed as a means of allopatric speciation following a polyploid event. Allopatric speciation occurs when two populations of the same species become spatially separated and accumulate enough genetic differences to lose the ability to interbreed. As redundant genes are silenced in allopolyploids there is the potential for rapid genetic differences to accumulate through reciprocal silencing. These differences can lead to the loss of ability to interbreed between separated populations at a faster rate than other methods of speciation, given the relative speed with which genes are silenced following a polyploid event. Faster still, redundant genes can be silenced through epigenetic means, although the importance of this phenomenon is not fully understood.\n\nReciprocal silencing on the tissue level refers to the same pattern of silencing and expression of homeologous loci. However in this case, the differences in silencing and expression occur between two types of tissue within the same individual, rather than in individuals of different populations. This is an example of neofunctionalization, a process where duplicated genes that were once at equivalent loci evolve to carry out two separate functions. Since different tissues require different genes to be expressed, reciprocal silencing can occur between tissues. Importantly, while the pattern of gene expression is the same as in the population case, the genetic means by which this pattern is achieved are very different. While silencing mutations are thought to be the main source of reciprocal silencing \nat the population level, at the tissue level only epigenetic factors are in play, since expressible copies of both homeologous loci must exist in all cells in an individual if different tissues express different homeologs.\n\n", "id": "37913847", "title": "Reciprocal silencing"}
{"url": "https://en.wikipedia.org/wiki?curid=37890211", "text": "Synthesis-dependent strand annealing\n\nIn genetics, the initial processes involved in repair of a double-strand break by synthesis-dependent strand annealing (SDSA) are identical to those in the double Holliday junction model, and have been most extensively studied in yeast species \"Saccharomyces cerevisiae\". Following a double-stranded break, a protein complex (MRX) binds to either end of the break, working with DNA nucleases to carry out resection, resulting in 5' end digest to produce 3' overhangs of single-stranded DNA (see Figure). These overhangs are then bound to form a nucleoprotein filament, which can then locate DNA sequences similar to one of the 3' overhangs, initiating a single-stranded strand invasion into the DNA duplex containing these sequences. Once strand invasion has occurred, a displacement loop, or D-loop, is formed, at which point either SDSA or a double Holliday junction occurs.\n\nHomologous recombination via the SDSA pathway occurs in both mitotic and meiotic cells as an important mechanism of non-crossover recombination, and was first suggested as a model in 1976, acquiring its current name in 1994. As the double Holliday junction model was the first posited in order to explain this phenomenon, various versions of the SDSA model were later proposed to explain heteroduplex DNA configurations that did not match predictions of the double Holliday junction model. Studies in \"S. cerevisiae\" found that non-crossover products appear earlier than double Holliday junctions or crossover products, which challenged the previous notion that both crossover and non-crossover products are produced by double Holliday junctions.\n\nIn the SDSA model, repair of double-stranded breaks occurs without the formation of a double Holliday junction, such that the two processes of homologous recombination are identical until just after D-loop formation. In yeast, the D-loop is formed by strand invasion with the help of proteins Rad51 and Rad52, and is then acted on by DNA helicase Srs2 to prevent formation of the double Holliday junction in order for the SDSA pathway to occur. The invading 3' strand is thus extended along the recipient homologous DNA duplex by DNA polymerase in the 5' to 3' direction, such that the D-loop physically translocates – a process referred to as bubble migration DNA synthesis. The resulting single Holliday junction then slides down the DNA duplex in the same direction in a process called branch migration, displacing the extended strand from the template strand. This displaced strand pops up to form a 3' overhang in the original double-stranded break duplex, which can then anneal to the opposite end of the original break through complementary base pairing. Thus DNA synthesis fills in gaps left over from annealing, and extends both ends of the still present single stranded DNA break, ligating all remaining gaps to produce recombinant non-crossover DNA.\n\nSDSA is unique in that D-loop translocation results in conservative rather than semiconservative replication, as the first extended strand is displaced from its template strand, leaving the homologous duplex intact. Therefore, although SDSA produces non-crossover products because flanking markers of heteroduplex DNA are not exchanged, gene conversion does occur, wherein nonreciprocal genetic transfer takes place between two homologous sequences.\n\nAssembly of a nucleoprotein filament comprising single-stranded DNA (ssDNA) and the RecA homolog, Rad51, is a key step necessary for homology search during recombination. In the budding yeast \"Saccharomyces cerevisiae\", Srs2 translocase dismantles Rad51 filaments during meiosis. By directly interacting with Rad51, Srs2 dislodges Rad51 from nucleoprotein filaments thereby inhibiting Rad51-dependent formation of joint molecules and D-loop structures. This dismantling activity is specific for Rad51 since Srs2 does not dismantle DMC1 (a meiosis-specific Rad51 homolog), Rad52 (a Rad 51 mediator) or replication protein A (RPA, a single-stranded DNA binding protein). Srs2 promotes the non-crossover SDSA pathway, apparently by regulating RAD51 binding during strand exchange.\n\nThe Sgs1(BLM) helicase is an ortholog of the human Bloom syndrome protein. It appears to be a central regulator of most of the recombination events that occur during \"S. cerevisiae\" meiosis. Sgs1(BLM) may disassemble D-loop structures analogous to early strand invasion intermediates and thus promote NCO formation by SDSA. The Sgs1 helicase forms a conserved complex with the topoisomerase III (Top3)-RMI1 heterodimer (that catalyzes DNA single strand passage). This complex, called STR (for its three components), promotes early formation of NCO recombinants by SDSA during meiosis.\n\nAs reviewed by Uringa et al. the RTEL1 helicase is required to regulate recombination during meiosis in the worm \"Caenorhabditis elegans\". RTEL1 is a key protein in repair of DSBs. It disrupts D-loops and promotes NCO outcomes through SDSA.\n\nThe number of DSBs created during meiosis can substantially exceed the number of final CO events. In the plant \"Arabidopsis thaliana\", only about 4% of DSBs are repaired by CO recombination, suggesting that most DSBs are repaired by NCO recombination. Data based on tetrad analysis from several species of fungi show that only a minority (on average about 34%) of recombination events during meiosis are COs (see Whitehouse, Tables 19 and 38 for summaries of data from \"S. cerevisiae\", \"Podospora anserina\", \"Sordaria fimicola\" and \"Sordaria brevicollis\"). In the fruit fly \"D. melanogaster\" during meiosis in females there is at least a 3:1 ratio of NCOs to COs. These observations indicate that the majority of recombination events during meiosis are NCOs, and suggest that SDSA is the principal pathway for recombination during meiosis. The major function of SDSA during meiosis presumably is to repair DNA damages, particularly DSBs, in the genomes to be passed on to gametes (see Genetic recombination).\n", "id": "37890211", "title": "Synthesis-dependent strand annealing"}
{"url": "https://en.wikipedia.org/wiki?curid=28474959", "text": "Exon skipping\n\nIn molecular biology, exon skipping is a form of RNA splicing used to cause cells to “skip” over faulty or misaligned sections of genetic code, leading to a truncated but still functional protein despite the genetic mutation.\n\nExon skipping is used to restore the reading frame within a gene. Genes are the genetic instructions for creating a protein, and are composed of introns and exons. Exons are the sections of DNA that contain the instruction set for generating a protein; they are interspersed with non-coding regions called introns. The introns are later removed before the protein is made, leaving only the coding exon regions.\n\nSplicing naturally occurs in pre-mRNA when introns are being removed to form mature-mRNA that consists solely of exons. Starting in the late 1990s, scientists realized they could take advantage of this naturally occurring cellular splicing to downplay genetic mutations into less harmful ones.\n\nThe mechanism behind exon skipping is a mutation specific antisense oligonucleotide (AON). An antisense oligonucleotide is a synthesized short nucleic acid polymer, typically fifty or fewer base pairs in length that will bind to the mutation site in the pre-messenger RNA, to induce exon skipping. The AON binds to the mutated exon, so that when the gene is then translated from the mature mRNA, it is “skipped” over, thus restoring the disrupted reading frame. This allows for the generation of an internally deleted, but largely functional protein.\n\nSome mutations require exon skipping at multiple sites, sometimes adjacent to one another, in order to restore the reading frame. Multiple exon skipping has successfully been carried out using a combination of AONs that target multiple exons.\n\nExon skipping is being heavily researched for the treatment of Duchenne muscular dystrophy (DMD), where the muscular protein dystrophin is prematurely truncated, which leads to a non-functioning protein. Successful treatment by way of exon skipping could lead to a mostly functional dystrophin protein, and create a phenotype similar to the less severe Becker muscular dystrophy (BMD).\n\nIn the case of Duchenne muscular dystrophy, the protein that becomes compromised is dystrophin. The dystrophin protein has two essential functional domains that flank a central rod domain consisting of repetitive and partially dispensable segments. Dystrophin’s function is to maintain muscle fiber stability during contraction by linking the extra cellular matrix to the cytoskeleton. Mutations that disrupt the open reading frame within dystrophin create prematurely truncated proteins that are unable to perform their job. Such mutations lead to muscle fiber damage, replacement of muscle tissue by fat and fibrotic tissue, and premature death typically occurring in the early twenties of DMD patients. Comparatively, mutations that do not upset the open reading frame, lead to a dystrophin protein that is internally deleted and shorter than normal, but still partially functional. Such mutations are associated with the much milder Becker muscular dystrophy. Mildly affected BMD patients carrying deletions that involve over two thirds of the central rod domain have been described, suggesting that this domain is largely dispensable.\n\nDystrophin can maintain a large degree of functionality so long as the essential terminal domains are unaffected, and exon skipping only occurs within the central rod domain. Given these parameters, exon skipping can be used to restore an open reading frame by inducing a deletion of one or several exons within the central rod domain, and thus converting a DMD phenotype into a BMD phenotype. \n\nThe genetic mutation that leads to Becker muscular dystrophy is an in-frame deletion. This means that, out of the 79 exons that code for dystrophin, one or several in the middle may be removed, without affecting the exons that follow the deletion. This allows for a shorter-than-normal dystrophin protein that maintains a degree of functionality. In Duchenne muscular dystrophy, the genetic mutation is out-of-frame. Out-of-frame mutations cause a premature stop in protein generation - the ribosome is unable to “read” the RNA past the point of initial error - leading to a severely shortened and completely non-functional dystrophin protein.\n\nThe goal of exon skipping is to manipulate the splicing pattern so that an out-of-frame mutation becomes an in-frame mutation, thus changing a severe DMD mutation into a less harmful in-frame BMD mutation.\n\nGenetic testing, usually from blood samples, can be used to determine the precise nature and location of the DMD mutation in the dystrophin gene. It is known that these mutations cluster in areas known as the 'hot spot' regions — primarily in exons 45–53 and to a lesser extent exons 2–20. As the majority of DMD mutations occur in these 'hot spot' regions, a treatment which causes these exons to be skipped could be used to treat up to 50% of DMD patients.\n\n", "id": "28474959", "title": "Exon skipping"}
{"url": "https://en.wikipedia.org/wiki?curid=1188046", "text": "Nutrigenomics\n\nNutrigenomics is a branch of nutritional genomics and is the study of the effects of foods and food constituents on gene expression. This means that nutrigenomics is research focusing on identifying and understanding molecular-level interaction between nutrients and other dietary bioactives with the genome. Nutrigenomics has also been described by the influence of genetic variation on nutrition, by correlating gene expression or SNPs with a nutrient's absorption, metabolism, elimination or biological effects. By doing so, nutrigenomics aims to enhance rational means to optimise nutrition with respect to the subject's genotype.\n\nBy determining the mechanism of the effects of nutrients or the effects of a nutritional regime, nutrigenomics tries to define the causality or relationship between these specific nutrients and specific nutrient regimes (diets) on human health. Nutrigenomics has been associated with the idea of personalized nutrition based on genotype. While there is hope that nutrigenomics will ultimately enable such personalised dietary advice, it is a science still in its infancy and its contribution to public health over the next decade is thought to be major. Whilst nutrigenomics is aimed at developing an understanding of how the whole body responds to a food via systems biology, research into the effect of a single gene/single food compound relationships is known as nutrigenetics.\n\nIn a \"Nature Reviews Genetics\" paper, nutrigenomics is defined an emerging field of research that expands upon the existing field of nutritional science using genomic based data.  Certain advances in the field such as microarrays, and high throughput sequencing allow for expansive analysis of the genome and in-vivo experiments in knockout mice are major sources of genomic based data. This type of genomic data collection can be applied to view the effects that certain nutrients or foods may have on large portions or different locales of the genome rather than one specific location.\n\nNutrigenomics is also defined as a field that examines \"effect of nutrients on genome, proteome, metabolome and explains the relationship between these specific nutrients and nutrient-regimes on human health\". In other words, a nutrigenomics approach is a holistic one that examines the effect of nutrients at all levels, from gene expression to metabolic pathways.\n\nNutritional science originally emerged as a field that studied individuals lacking certain nutrients and the subsequent effects, such as the disease scurvy which results from a lack of vitamin C. As other diseases closely related to diet,(but not deficiency) such as Obesity, became more prevalent, nutritional science expanded to cover these topics as well.  Nutritional research typically focuses on preventative measure, trying to identify what nutrients or foods will raise or lower risks of diseases and damage to the human body.\n\nNutrigenomics emerged as a possible way to fix gaps in the current field of nutritional science. The development of technology to analyze the genome such as different types of sequencing and different microarrays suggest a new way to reinforce current theories or hypotheses. Existing information from genetic research directs emerging research in nutrigenomics. Individuals within the same population or even the same family have genetic variability. There is a lack of consistent relationships between certain foods and nutrients and increased disease risk, most likely due to this type of variation.  Nutrigenomics is highly personalized because it looks at biomarkers within each individual. One group of researchers suggest that current technology can be used to build an ideal diet/intake of certain nutrients, or a 'nutriome.' A 'nutriome' would ensure proper function of all pathways involved in genome maintenance.\n\nResearch has already provided evidence identifying potential genetic origins of metabolic disorders or compromised phenotypes. Disorders that scientists previously thought to be heritable, can be identified as genetic disorders with set pathological effects. For example, Prader-Willi syndrome, a disease whose most distinguishing factor is insatiable appetite, has been specifically linked to an epigenetic pattern in which the paternal copy in the chromosomal region is erroneously deleted, and the maternal loci is inactivated by over methylation. Yet, although certain disorders may be linked to certain single nucleotide polymorphisms (SNPs) or other localized patterns, variation within a population may yield many more polymorphisms. Each may have a negligible effect by itself, yet the cumulative effects may be significant.Now, with advances that have been made, these small changes and additive effects are possible to study.  Small epigenetic changes such as methylation patterns or phosphorylation can be determined.\n\nCell signaling is an important component of regulation of gene expression and metabolism, relying on both internal and external signals to ensure the body is maintaining homeostasis. Individual nutrients can each be considered signals, with the summation of their effects being the diet. The effort of nutrigenomics is to identify this \"dietary signature\", or pattern of effects ranging from effects at the cellular level to entire body systems. However it is often hard to monitor the diet of an individual, and current protocols should be improved. The desired outcome from this type of research is to identify genetic factors for chronic diseases and conditions, whether it be a certain gene itself or an epigenetic marker, and how foods influence it. Nutrigenomics looks mainly to be a way of identifying individuals predisposed for conditions and preventing onset.  First, genes with regulation influenced must be identified, and then more focused studies may emerge.\n\nIn addition, nutrigenomics also looks to identify certain compounds that are bioactive, and other foods that are of particular benefit to health. This knowledge can be personalized to produce specific diet plans and functional foods to both prevent predisposed conditions and maximize health.\n\nAging of cells occur because of the accumulation of excess free radicals formed due to the lack of proper nutrition to the cells and external factors like UV rays, pollution, stress, food, etc. DNA analysis is instrumental in identifying the right concoction of nutrients needed to eliminate the excess free radicals present in the cell.\n\nThe science of nutrigenomics studies the interaction between dietary components of food and genes. Scientific advances have now made it possible to apply nutrigenomics in the field of anti ageing and customize nutritional solutions in the form of supplements to meet the optimal nutrition required by the body to prevent aging of cells by the formation of excess free radicals.\n\nObesity is one of the most widely studied topics in nutrigenomics. Due to genetic variations among individuals, each person could respond to diet differently. By exploring the interaction between dietary pattern and genetic factors, nutrigenomics aim to suggest prevention measures and/ or treatment to obesity via personal nutrition.\n\nThere are studies suggesting genetic factors account for a fair proportion of inter-individual BMI (Body Mass Index). Among different types of genetic variation between humans, SNPs are suggested to be the most important marker for the study of nutrigenomics. \nMultiple studies have found association between SNPs and obesity. One of the most well known obesity associating gene is the FTO gene. Among studied individuals, it was found that those with AA genotype showed a higher BMI compared those with TT genotype when having high fat or low carbohydrate dietary intake. \nThe APO B SNP rs512535 is another obesity related variation. It was found that the A/G heterozygous genotype was found to have association with obesity (in terms of BMI and waist circumference). The same study also found that for individuals with habitual high fat diet (>35% of energy intaken), individuals with GG homozygotes genotype showed higher BMI compared to AA allele carriers. However, this difference is not found in low fat consuming group (<35% of energy intaken).\nBesides the FTO genes and APO B, SNPs in various genes such as MC4R, SH2B1, MTCH2, SEC16B etc. have been found to be associated with obesity. Although many of these genetic variations are found in populations all over the world, there are also variations unique to certain races or populations.\n\nNutrigenomics may be able to supplement current oncology. There is a wealth of information about processes that occur within genome maintenance that prevent cell abnormalities linked to cancer and certain nutrients that play a role as cofactors. Genome damage caused by micronutrient deficiency may be just as severe as damage owed to exposure to certain environmental carcinogens. If these micronutrients can be identified, with concrete evidence, the risk for cancer in some individuals could be significantly reduced. One such micronutrient may be folate. In one experiment, folate was given to cells in different concentrations and those with less folate exhibited as much damage to their chromosomes as they would have exhibited with a heavy amount of radiation. \nLikewise, dietary supplementation using the cinnamon-derived food factor cinnamaldehyde prevented colon cancer in a chemical exposure-induced mouse model of the human disease, an effect dependent on expression of the cytoprotective transcription factor Nrf2. \nNutrigenomics can be used to develop new, alternative treatments that target the altered cancer cell metabolism. The alternative way of energy production in cancer cell metabolism, the Warburg effect, in which glycolysis and lactic acid fermentation are the main means of energy production opposed to oxidative reduction. Certain nutrients may provide ways to starve or inhibit this type of metabolism. Polyunsaturated fatty acids (PUFA) which affect gene expression related to inflammation and other nutrients that have displayed potential in repressing cancer cell metabolism. Another practical application of nutrigenomics to cancer may be identifying nutrient that is a cofactor of a compromised pathway where consuming a surplus of could potentially reduce the compromised pathway’s negative consequences. A nutrigenomics approach could provide a safe, holistic model to mitigate tumor growth in place of existing cancer treatments that often have harsh side effects and are not always effective.\n\nCompanies across the Globe are currently involved in Nutrigenomics. Solutions such as genetic diet plans and exercise schedule have helped sportsmen perform even better. In India sportsmen like Sushil Kumar have taken advantage of Nutrigenomics. Today, even film stars have started following such plans. In the movie Dangal, Aamir Khan got his genes tested. Across the globe, there are companies who are not only working towards personalized nutrition but also personalized medicine. Few companies like DNAfit in London, 23andMe in the United States, Genecorp in India and HiMyDna in HongKong.\n\nTo put nutrigenomics into practice, genetic testing is required as the test results act as the reference for diagnosis. Genetic testing has been met with many concerns surrounding ethics and regulations. These concerns inherently become a part of, if not augmented by Nutrigenomics, a field that looks to provide highly personalized information.\n\nOne of the major concerns regarding genetic tests would be privacy issue. To perform any type of genetic testing, consent is need directly by the individual who provides the sample. However, if an individual has results that indirectly tie family members to it, by identifying information about a genetic predisposition or condition, information about that family member has been inadvertently revealed. Thus, this type of genetic testing would require consent from a network of individuals. For some sets of the population such as mentally impaired adult or children, it is not possible to obtain direct consent.'The best interest' of the patient must be determined by close family members, care takers and professionals, leaving room for discrepancy. Tissue samples obtained from patients, particularly those who are deceased are also a source of controversy. There is no established ethical code to suggest if data from these patients should be allowed to be published, or if they should remain only as sources of validation for lab techniques. There also exists no regulation for releasing information about heritable condition to family members. The stances on how to approach these situations are arbitrary and regulation provides few guidelines to direct them.\n\nAs the subject is recently commercialized by companies which sell direct to customer (DTC) genetic tests, as well as being applied by related professionals (such as dietetic practitioners), there has been increased awareness in the use of this information.\n\nNutrigenomics is still a new field. There are no set guidelines on how to interpret data from genetic testing. Without a validated way to produce accurate results, there exist concern about how valid results produced are. The Government Accountability Office (GAO) attempted to check the validity of numerous DTC tests by sending out information and samples of sham identities. The information they received was varied and not medically verified, and two companies tried to market general supplements as 'individualized'. The GAO study was also rudimentary, without taking into concern that differing environmental factors may affect results.\n\nOne suggestion to try and minimize fraud is to channel distribution of genetic testing to healthcare professionals. American College of Medical Genetics(ACMG) has taken a stance that healthcare professionals should be involved for proper implementation of information from genetic testing.  Healthcare professionals are not necessarily qualified to properly interpret and distribute this information as it is not currently required that they have an in-depth knowledge of genetics. There are a sheer 45 genetic residencies in the US, with a low number of individuals who have completed training per year.  Practitioners often focus on acute medical conditions and do not spend much of their time making health recommendations to each patient. It is suggested that nutritionists and genetic counselors may be the best choice to ensure proper distribution of genetic tests' results.\n\nOne of the major concerns regarding genetic tests would be privacy issue. There are concerns on who has the right to have access to test results. Abuse of these tests could result in discrimination. For example, genetic information might be used by insurance companies to risk rate their clients or assess how likely their clients are to be costly.. Other examples of privacy concerns include disclosure to the workplace that may led to discrimination in employment. Social concerns exist as certain conditions may be stigmatized by the general population.\n\n\n", "id": "1188046", "title": "Nutrigenomics"}
{"url": "https://en.wikipedia.org/wiki?curid=30967636", "text": "Genome evolution\n\nGenome evolution is the process by which a genome changes in structure (sequence) or size over time. The study of genome evolution involves multiple fields such as structural analysis of the genome, the study of genomic parasites, gene and ancient genome duplications, polyploidy, and comparative genomics. Genome evolution is a constantly changing and evolving field due to the steadily growing number of sequenced genomes, both prokaryotic and eukaryotic, available to the scientific community and the public at large.\nSince the first sequenced genomes became available in the late 1970s, scientists have been using comparative genomics to study the differences and similarities between various genomes. Genome sequencing has progressed over time to include more and more complex genomes including the eventual sequencing of the entire human genome in 2001. By comparing genomes of both close relatives and distant ancestors the stark differences and similarities between species began to emerge as well as the mechanisms by which genomes are able to evolve over time.\n\nProkaryotic genomes have two main mechanisms of evolution: mutation and horizontal gene transfer. A third mechanism, sexual reproduction, prominent in eukaryotes, is not found in bacteria although prokaryotes can acquire novel genetic material through the process of bacterial conjugation in which both plasmids and whole chromosomes can be passed between organisms. An often cited example of this process is the transfer of antibiotic resistance utilizing plasmid DNA. Another mechanism of genome evolution is provided by transduction whereby bacteriophages introduce new DNA into a bacterial genome.\n\nGenome evolution in bacteria is well understood because of the thousands of completely sequenced bacterial genomes available. Genetic changes may lead to both increases or decreases of genomic complexity due to adaptive genome streamlining and purifying selection. In general, free-living bacteria have evolved larger genomes with more genes so they can adapt more easily to changing environmental conditions. By contrast, most parasitic bacteria have reduced genomes as their hosts supply many if not most nutrients, so that their genome does not need to encode for enzymes that produce these nutrients themselves.\n\nEukaryotic genomes are generally larger than that of the prokaryotes. While the \"E. coli\" genome is roughly 4.6Mb in length, in comparison the Human genome is much larger with a size of approximately 3.2Gb. The eukaryotic genome is linear and can be composed of multiple chromosomes, packaged in the nucleus of the cell. The non-coding portions of the gene, known as introns, which are largely not present in prokaryotes, are removed by RNA splicing before translation of the protein can occur. Eukaryotic genomes evolve over time through many mechanisms including sexual reproduction which introduces much greater genetic diversity to the offspring than the prokaryotic process of replication in which the offspring are theoretically genetic clones of the parental cell.\n\nGenome size is usually measured in base pairs (or bases in single-stranded DNA or RNA). The C-value is another measure of genome size. Research on prokaryotic genomes shows that there is a significant positive correlation between the C-value of prokaryotes and the amount of genes that compose the genome. This indicates that gene number is the main factor influencing the size of the prokaryotic genome. In eukaryotic organisms, there is a paradox observed, namely that the number of genes that make up the genome does not correlate with genome size. In other words, the genome size is much larger than would be expected given the total number of protein coding genes.\n\nGenome size can increase by duplication, insertion, or polyploidization. Recombination can lead to both DNA loss or gain. Genomes can also shrink because of deletions. A famous example for such gene decay is the genome of \"Mycobacterium leprae\", the causative agent of leprosy. \"M. leprae\" has lost many once-functional genes over time due to the formation of pseudogenes. This is evident in looking at its closest ancestor \"Mycobacterium tuberculosis\". \"M. leprae\" lives and replicates inside of a host and due to this arrangement it does not have a need for many of the genes it once carried which allowed it to live and prosper outside the host. Thus over time these genes have lost their function through mechanisms such as mutation causing them to become pseudogenes. It is beneficial to an organism to rid itself of non-essential genes because it makes replicating its DNA much faster and requires less energy.\n\nAn example of increasing genome size over time is seen in filamentous plant pathogens. These plant pathogen genomes have been growing larger over the years due to repeat-driven expansion. The repeat-rich regions contain genes coding for host interaction proteins. With the addition of more and more repeats to these regions the plants increase the possibility of developing new virulence factors through mutation and other forms of genetic recombination. In this way it is beneficial for these plant pathogens to have larger genomes.\n\nGene duplication is the process by which a region of DNA coding for a gene is duplicated. This can occur as the result of an error in recombination or through a retrotransposition event. Duplicate genes are often immune to the selective pressure under which genes normally exist. As a result, a large number of mutations may accumulate in the duplicate gene code. This may render the gene non-functional or in some cases confer some benefit to the organism.\n\nSimilar to gene duplication, whole genome duplication is the process by which an organism’s entire genetic information is copied, once or multiple times which is known as polyploidy. This may provide an evolutionary benefit to the organism by supplying it with multiple copies of a gene thus creating a greater possibility of functional and selectively favored genes. In 1997, Wolfe & Shields gave evidence for an ancient duplication of the \"Saccharomyces cerevisiae\" (Yeast) genome. It was initially noted that this yeast genome contained many individual gene duplications. Wolfe & Shields hypothesized that this was actually the result of an entire genome duplication in the yeast’s distant evolutionary history. They found 32 pairs of homologous chromosomal regions, accounting for over half of the yeast's genome. They also noted that although homologs were present, they were often located on different chromosomes. Based on these observations, they determined that \"Saccharomyces cerevisiae\" underwent a whole genome duplication soon after its evolutionary split from \"Kluyveromyces\", a genus of ascomycetous yeasts. Over time, many of the duplicate genes were deleted and rendered non-functional. A number of chromosomal rearrangements broke the original duplicate chromosomes into the current manifestation of homologous chromosomal regions. This idea was further solidified in looking at the genome of yeast's close relative \"Ashbya gossypii\". Whole genome duplication is common in fungi as well as plant species. An example of extreme genome duplication is represented by the Common Cordgrass (\"Spartina anglica\") which is a dodecaploid, meaning that it contains 12 sets of chromosomes, in stark contrast to the human diploid structure in which each individual has only two sets of 23 chromosomes.\n\nTransposable elements are regions of DNA that can be inserted into the genetic code through one of two mechanisms. These mechanisms work similarly to \"cut-and-paste\" and \"copy-and-paste\" functionalities in word processing programs. The \"cut-and-paste\" mechanism works by excising DNA from one place in the genome and inserting itself into another location in the code. The \"copy-and-paste\" mechanism works by making a genetic copy or copies of a specific region of DNA and inserting these copies elsewhere in the code. The most common transposable element in the human genome is the Alu sequence, which is present in the genome over one million times.\n\nSpontaneous mutations often occur which can cause various changes in the genome. Mutations can either change the identity of one or more nucleotides, or result in the addition or deletion of one or more nucleotide bases. Such changes can lead to a frameshift mutation, causing the entire code to be read in a different order from the original, often resulting in a protein becoming non-functional. A mutation in a promoter region, enhancer region or transcription factor binding region can also result in either a loss of function, or an up or downregulation in the transcription of the gene targeted by these regulatory elements. Mutations are constantly occurring in an organism's genome and can cause either a negative effect, positive effect or neutral effect (no effect at all).\n\nOften a result of spontaneous mutation, pseudogenes are dysfunctional genes derived from previously functional gene relatives. There are many mechanisms by which a functional gene can become a pseudogene including the deletion or insertion of one or multiple nucleotides. This can result in a shift of reading frame, causing the gene to no longer code for the expected protein, introduce a premature stop codon or a mutation in the promoter region. Often cited examples of pseudogenes within the human genome include the once functional olfactory gene families. Over time, many olfactory genes in the human genome became pseudogenes and were no longer able to produce functional proteins, explaining the poor sense of smell humans possess in comparison to their mammalian relatives.\n\nExon shuffling is a mechanism by which new genes are created. This can occur when two or more exons from different genes are combined together or when exons are duplicated. Exon shuffling results in new genes by altering the current intron-exon structure. This can occur by any of the following processes: transposon mediated shuffling, sexual recombination or non-homologous recombination (also called illegitimate recombination). Exon shuffling may introduce new genes into the genome that can be either selected against and deleted or selectively favored and conserved.\n\nMany species exhibit genome reduction when subsets of their genes are not needed anymore. This typically happens when organisms adapt to a parasitic life style, e.g. when their nutrients are supplied by a host. As a consequence, they lose the genes needed to produce these nutrients. In many cases, there are both free living and parasitic species that can be compared and their lost genes identified. Good examples are the genomes of \"Mycobacterium tuberculosis\" and \"Mycobacterium leprae\", the latter of which has a dramatically reduced genome.\n\nAnother beautiful example are endosymbiont species. For instance, \"Polynucleobacter necessarius\" was first described as a cytoplasmic endosymbiont of the ciliate \"Euplotes aediculatus\". The latter species dies soon after being cured of the endosymbiont. In the few cases in which \"P. necessarius\" is not present, a different and rarer bacterium apparently supplies the same function. No attempt to grow symbiotic \"P. necessarius\" outside their hosts has yet been successful, strongly suggesting that the relationship is obligate for both partners. Yet, closely related free-living relatives of P. necessarius have been identified. The endosymbionts have a significantly reduced genome when compared to their free-living relatives (1.56 Mbp vs. 2.16 Mbp).\n\nA major question of evolutionary biology is how genomes change to create new species. Speciation requires changes in behavior, morphology, physiology, or metabolism (or combinations thereof). The evolution of genomes during speciation has been studied only very recently with the availability of next-generation sequencing technologies. For instance, cichlid fish in African lakes differ both morphologically and in their behavior. The genomes of 5 species have revealed that both the sequences but also the expression pattern of many genes has quickly changed over a relatively short period of time (100,000 to several million years). Notably, 20% of duplicate gene pairs have gained a completely new tissue-specific expression pattern, indicating that these genes also obtained new functions. Given that gene expression is driven by short regulatory sequences, this demonstrates that relatively few mutations are required to drive speciation. The cichlid genomes also showed increased evolutionary rates in microRNAs which are involved in gene expression.\n\nMutations can lead to changed gene function or, probably more often, to changed gene expression patterns. In fact, a study on 12 animal species provided strong evidence that tissue-specific gene expression was largely conserved between orthologs in different species. However, paralogs within the same species often have a different expression pattern. That is, after duplication of genes they often change their expression pattern, for instance by getting expressed in another tissue and thereby adopting new roles.\n\nThe genetic code is made up of sequences of four nucleotide bases: Adenine, Guanine, Cytosine and Thymine, commonly referred to as A,G,C,and T. The GC-content is the percentage of G & C bases within a genome. GC-content varies greatly between different organisms. Gene coding regions have been shown to have a higher GC-content and the longer the gene is, the greater the percentage of G and C bases that are present. A higher GC-content confers a benefit because a Guanine-Cytosine bond is made up of three hydrogen bonds while an Adenine-Thymine bond is made up of only two. Thus the three hydrogen bonds give greater stability to the DNA strand. So, it is not surprising that important genes often have a higher GC-content than other parts of an organism's genome. For this reason, many species living at very high temperatures such as the ecosystems surrounding hydrothermal vents, have a very high GC-content. High GC-content is also seen in regulatory sequences such as promoters which signal the start of a gene. Many promoters contain CpG islands, areas of the genome where a cytosine nucleotide occurs next to a guanine nucleotide at a greater proportion. It has also been shown that a broad distribution of GC-content between species within a genus shows a more ancient ancestry. Since the species have had more time to evolve, their GC-content has diverged further apart.\n\nAmino acids are made up of three base long codons and both Glycine and Alanine are characterized by codons with Guanine-Cytosine bonds at the first two codon base positions. This GC bond gives more stability to the DNA structure. It has been hypothesized that as the first organisms evolved in a high-heat and pressure environment they needed the stability of these GC bonds in their genetic code.\n\nNovel genes can arise from non-coding DNA. For instance, Levine and colleagues reported the origin of five new genes in the \"D. melanogaster\" genome from noncoding DNA. Subsequently, de novo origin of genes has been also shown in other organisms such as yeast, rice and humans. For instance, Wu et al. (2011) reported 60 putative de novo human-specific genes all of which are short consisting of a single exon (except one).\n", "id": "30967636", "title": "Genome evolution"}
{"url": "https://en.wikipedia.org/wiki?curid=37388686", "text": "Minimal genome\n\nThe concept of minimal genome assumes that genomes can be reduced to a bare minimum, given that they contain many non-essential genes of limited or situational importance to the organism. Therefore, if a collection of all the essential genes were put together, a minimum genome could be created artificially in a stable environment. By adding more genes, the creation of an organism of desired properties is possible. The concept of minimal genome arose from the observations that many genes do not appear to be necessary for survival. In order to create a new organism a scientist must determine the minimal set of genes required for metabolism and replication. This can be achieved by experimental and computational analysis of the biochemical pathways needed to carry out basic metabolism and reproduction. A good model for a minimal genome is \"Mycoplasma genitalium\" due to its very small genome size. Most genes that are used by this organism are usually considered essential for survival; based on this concept a minimal set of 256 genes has been proposed.\n\nMany naturally occurring bacteria have reduced genomes even though they may not be reduced to the bare minimum. Although these genomes are thus not \"minimal\", they are good models for genome reduction and thus \"minimal genomes\". Genome reduction occurs most commonly in endosymbiotic, parasitic or pathogenic bacteria that live in their hosts. The host provides most of the nutrients such bacteria require, hence the bacteria do not need the genes for producing such compounds themselves. Examples include species of \"Buchnera\", \"Chlamydia\", \"Treponema\", \"Mycoplasma\", and many others. One of the most reduced genomes in free-living bacteria has been found in \"Pelagibacter ubique\" which encodes 1,354 proteins. \"Mycoplasma genitalium\" has been used as a prime model for minimal genomes. It is a human urogenital pathogen which has the smallest genome of size 580 kb and it consists of only 482 protein-coding genes.\n\nViruses have the smallest genomes in nature. For instance, bacteriophage MS2 consists of only 3569 nucleotides (single-stranded RNA) and encodes just four proteins. Similarly, among eukaryotic viruses, porcine circoviruses are among the smallest. They encode only 2-3 open reading frames.\n\nIt was a collaborative effort by the National Aeronautics and Space Administration (NASA) and the two scientists: Morowitz and Tourtellote that this concept arose. NASA in the 1960s was searching for extraterrestrial life forms, assuming that if they existed they may be simple creatures. While Morowitz, to attract people’s attention published about mycoplasmas as being the smallest and simplest self-replicating creatures. The two grouped together and came up with an idea to assemble a living cell from the components of mycoplasmas. Since, mycoplasmas are built with a minimum set of organelles such as: a plasma membrane, ribosomes and a circular double stranded DNA; it was selected as the best candidate for cell reassembly. Morowitz’ major idea was to define the entire machinery of mycoplasmas cell in molecular level. He announced that an international effort would help him accomplish this main objective.\n\nThis entire process was hard work, meanwhile even when papers were being published on the construction of minimal genome; by the 1980s Richard Herrmann’s laboratory had successfully managed to fully sequence and genetically characterize the 800kb genome of M. pneumoniae. That small of a genome itself took close to three years of hard work. Later in the 1995s another laboratory located in Maryland the Institute for Genomic Research (TIGR) collaborated with the teams of Johns Hopkins and University of North Carolina. Their organism for genome sequencing was Mycoplasma genitalium consisting of only 580 kb genome, the sequencing of which was done in 6 months.\n\nThe sequencing data revealed many interesting facts about M. genitalium such as discovery of some conserved genes, which ultimately helped in defining essentiality to life, of a minimal self- replicating cell. By far M. genitalium has become the prime candidate for minimal genome project. In fact these organisms are closest to minimal genome capable of self-replicating.\n\nMinimal set of essential genes are typically found by selective inactivation or deletions of genes and then testing the effect of each under a given set conditions. The discovery of essential genes have been done by the J. Craig Venter institute, they claim \"M. genitalium\" consists of 381 essential genes.\n\nThe next venture that the J.Craig Venter institute landed upon was creating a synthetic organism named Mycoplasma laboratorium, through minimal set genes of M. genitalium. This project opens new doors for synthetic biology because this impressive creation is being built upon by bringing together chemical synthesis and recombination cloning methodology.\n\nReconstruction of a minimal genome is possible by using the knowledge of existing\ngenomes via which the sets of genes, essential for living can also be determined. Once\nthe set of essential genetic elements are known, one can proceed to define the key\npathways and core-players by modeling simulations and wet lab genome engineering.\nThe two organisms upon which the ‘minimal gene set for cellular life’ was\napplied were: \"Haemophilus influenzae\", and \"M. genitalium\". A list of orthologous proteins were compiled in hope that it would contain protein necessary for cell survival, as orthologous analysis determines how two organisms evolved and shed away any non-essential genes. Since, \"H. influenza\" and \"M. genitalium\" are Gram negative and Gram positive bacteria and due to their vast evolution it was expected that these organisms would be enriched genes that were of universal importance. However, 244\ndetected orthologs discovered contained no parasitism-specific proteins. The conclusion of this analysis was that similar biochemical functions might be performed by non-\northologous proteins. Even when biochemical pathways of these two organisms were\nmapped, several pathways were present but many were incomplete. Proteins determined\nto be common between the two organisms were non-orthologous to each other.\nMuch of the research mainly focuses on the ancestral genome and less on the minimal\ngenome. Studies of these existing genomes have helped determine that orthologous genes\nfound in these two species are not necessarily essential for survival, in fact non-orthologous genes were found to be more important. Also, it was determined that in order for proteins to share same functions they do not need to have same sequence or common three dimensional folds. Distinguishing between orthologs and paralogs and detecting displacements of orthologs have been quiet beneficial in reconstructing evolution and determining the minimal gene set required for a cellular life. Instead, of conducting a strict orthology study, comparing groups of orthologs and occurrence in most clades instead of every species helped encounter genes lost or displaced. Only genomes that have been completely sequenced have enabled in studying orthologs among group of organisms. Without a fully sequenced genome it would not be possible to determine the essential minimal gene set required for survival.\n\nJ. Craig Venter Institute (JCVI) conducted a study to find all the essential genes of \"M. genitalium\" through global transposon mutagenesis. As a result they found that 382 out of 482 protein coding genes were essential. Genes encoding proteins of unknown function constitute 28% of the essential protein coding genes set. Before conducting this study the JCVI had performed another study on the non-essential genes, genes not required for growth, of \"M.genitalium\", where they reported the use of transposon mutagenesis. Despite figuring out the non-essential genes, it is not confirmed that the products that these genes make have any important biological functions. It was only through gene essentiality studies of bacteria that JCVI have been able to compose a hypothetical minimal gene sets.\n\n In JCVI's 1999 study among the two organisms, \"M. genitalium\" and \"Mycoplasma pneumoniae\" they mapped around 2,200 transposon insertion sites and identified 130 putative non-essentials genes in \"M. genitalium\" protein coding genes or \"M. pneumoniae\" orthologs of \"M. genitalium\" genes. In their experiment they grew a set of Tn4001 transformed cells for many weeks and isolated the genomic DNA from these mixture of mutants. Amplicons were sequenced to detect the transposon insertion sites in mycoplasma genomes. Genes that contained the transposon insertions were hypothetical proteins or proteins considered non-essential.\n\nThe same study of 1999 was later expanded and the updated results were then published in 2005.\n\nSome of the disruptive genes though to be essential were isoleucyl and tyrosyl-tRNA synthetases (MG345 and MG455), DNA replication gene \"dnaA\" (MG469), and DNA polymerase III subunit a (MG261). The way they improved this study was by isolating and characterizing \"M. genitalium\" Tn4001 insertions in each colony one by one. The individual analyses of each colony showed more results and estimates of essential genes necessary for life. The key improvement they made in this study was isolating and characterizing individual transposon mutants. Previously, they isolated many colonies containing a mixture of mutants. The filter cloning approach helped in separating the mixtures of mutants.\n\nNow, they claim completely different sets of non-essential genes. The 130 non-essential genes claimed at first have now reduced to 67. Of the remaining 63 genes 26 genes were only disrupted in \"M. pneumoniae\" which means that some \"M. genitalium\" orthologs of non-essential \"M. pneumoniae\" genes were actually essential.\n\nThey have now fully identified almost all of the non-essential genes in \"M. genitalium\", the number of gene disruptions based on colonies analyzed reached a plateau as function and they claim a total of 100 non-essential genes out of the 482 protein coding genes in \"M. genitalium\"\n\nThe ultimate result of this project has now come down to constructing a synthetic organism, \"Mycoplasma laboratorium\" based on the 387 protein coding region and 43 structural RNA genes found in \"M. genitalium\".\n\nThis project is currently still going on and it might possibly become the very first life form created by humans. It is quite likely that this line of research may lead to creating a bacterium that could further be engineered to produce fuels, make medicines, take some action on global warming, and make antibiotics.\n\nIn May 2010 the JCVI successfully created a “synthetic life” form, which will enable them to dissect a genetic instruction set of a bacterial cell and see how it really works. The synthetic life form was constructed by replacing the DNA of an existing bacterium and replacing it with DNA that was artificially designed and constructed.\n\nA number of projects have attempted to identify the essential genes of a species. This number should approximate the \"minimal genome\". The following table contains a list of such minimal genome projects (including the various techniques used). \n\nFor more information please refer also to section 'Minimal genome project' at 'Mycoplasma laboratorium'.\n\nThe number of essential genes is different for each organism. In fact, each organism has a different number of essential genes, depending on which strain (or individual) is tested. In addition, the number depends on the conditions under which an organism is tested. In several bacteria (or other microbes such as yeast) all or most genes have been deleted individually to determine which genes are \"essential\" for survival. Such tests are usually carried out on rich media which contain all nutrients. However, if all nutrients are provided, genes required for the synthesis of nutrients are not \"essential\". When cells are grown on minimal media, many more genes are essential as they may be needed to synthesize such nutrients (e.g. vitamins). The numbers provided in the following table typically have been collected using rich media (but consult original references for details).\n\nThe number of essential genes were collected from the Database of Essential Genes (DEG), except for B. subtilis, where the data comes from Genome News Network \nThe organisms listed in this table have been systematically tested for essential genes.\nFor more information about minimal genome Please refer also to section 'Other Genera' at 'Mycoplasma laboratorium'.\n\nMay 20, 2010- Researchers at the JCVI have successfully created a synthetic bacterial cell that is capable of replicating itself. The team has synthesized a 1.08 million base pair chromosome of a modified \"Mycoplasma mycoides\". The synthetic cell is called: \"Mycoplasma mycoides\" JCVI-syn1.0. One of the remarkable thing about this cell is that its DNA was built in the computer and then brought to life in the laboratory. The original proteins and biological materials of the converted cell use the new artificial DNA to generate daughter cells. These daughter cells are of synthetic origin and capable of further replication. This proves that genomes can be designed on computers. The steps they applied to build this was first they simulated a model of this genome computationally, they identified DNA via watermarks; next, they chemically produced this genome in the laboratory and finally, transplanted this genome into a recipient cell to produce a synthetic cell solely controlled by this synthetic genome.\n\nJust the first half of the project has taken 15 years to complete and there is still more to come. The team designed an accurate, digitized genome of \"M. mycoides\". A total of 1,078 cassettes all 1,080 base pair long were built. These cassettes were designed in a way that the end of each DNA cassette overlapped by 80 base pairs. The whole assembled genome was transplanted in yeast cells and grown as yeast artificial chromosome. This synthetic cell will be now able to show scientists how truly a cell works.\n\nNow that they have synthetic cells growing in their laboratory, the JCVI group can focus on their ultimate goal of synthesizing a minimal cell containing just the essential genes necessary for life.\n\nFuture direction: Based on JCVI’s progress in the field of synthetic biology, it is possible that in near future scientists will be able to propagate \"M. genitalium’s\" genome in the form of naked DNA, into recipient mycoplasmas cells and replace their original genome with a synthetic genome. Since, mycoplasmas have no cell wall, the transfer of a naked DNA into their cell is possible. The only requirement now is the technique to include the synthetic genome of \"M. genitalium\" into mycoplasma cells. To some extent this has become possible, the first replicating synthetic cell has already been developed by the JCVI and they are now on to creating their first synthetic life, consisting of minimal number of essential genes. This new breakthrough in synthetic biology will certainly bring in a new approach to understand biology; and this redesigning and prototyping genomes will later become beneficial to biotechnology companies, enabling them to produce synthetic microbes that produce new, cheaper and better bio-products.\n\nUses of Minimal genome:\n", "id": "37388686", "title": "Minimal genome"}
{"url": "https://en.wikipedia.org/wiki?curid=38190921", "text": "Family aggregation\n\nFamily aggregation, also known as familial aggregation, is the clustering of certain traits, behaviours, or disorders within a given family. Family aggregation may arise because of genetic or environmental similarities.\n\nThe data from the family aggregation studies have been extensively studied to determine the mode of inheritance of schizophrenia. Studies to date have shown that when numerous families are studied, simple modes of inheritance are not statistically supported. The majority of studies analyzing for the mode of inheritance have concluded that a multifactorial threshold mode is most likely.\n\nThe most consistent and dramatic evidence of family influences on cardiovascular disease (CVD) is family aggregation of physiological factors. In several studies the parent-child and sibling-sibling correlations of blood pressure are approximately .24. Genetic determination of blood pressure is strong, but does not explain all of the variance.\n\nFamilial Parkinson's disease (PD) exists but is infrequent. Early investigations failed to show substantial family aggregation for PD.\n\n", "id": "38190921", "title": "Family aggregation"}
{"url": "https://en.wikipedia.org/wiki?curid=14029", "text": "Histone\n\nIn biology, histones are highly alkaline proteins found in eukaryotic cell nuclei that package and order the DNA into structural units called nucleosomes. They are the chief protein components of chromatin, acting as spools around which DNA winds, and playing a role in gene regulation. Without histones, the unwound DNA in chromosomes would be very long (a length to width ratio of more than 10 million to 1 in human DNA). For example, each human diploid cell (containing 23 pairs of chromosomes) has about 1.8 meters of DNA; wound on the histones, the dipliod cell has about 90 micrometers (0.09 mm) of chromatin. When the diploid cells are duplicated and condensed during mitosis, the result is about 120 micrometers of chromosomes.\n\nFive major families of histones exist: H1/H5, H2A, H2B, H3, and H4. Histones H2A, H2B, H3 and H4 are known as the core histones, while histones H1/H5 are known as the linker histones.\n\nThe core histones all exist as dimers, which are similar in that they all possess the histone fold domain; three alpha helices linked by two loops. It is this helical structure that allows for interaction between distinct dimers, particularly in a head-tail fashion (also called the handshake motif). The resulting four distinct dimers then come together to form one octameric nucleosome core, approximately 63 Angstroms in diameter (a solenoid (DNA)-like particle). Around 146 base pairs (bp) of DNA wrap around this core particle 1.65 times in a left-handed super-helical turn to give a particle of around 100 Angstroms across. The linker histone H1 binds the nucleosome at the entry and exit sites of the DNA, thus locking the DNA into place and allowing the formation of higher order structure. The most basic such formation is the 10 nm fiber or beads on a string conformation. This involves the wrapping of DNA around nucleosomes with approximately 50 base pairs of DNA separating each pair of nucleosomes (also referred to as linker DNA). Higher-order structures include the 30 nm fiber (forming an irregular zigzag) and 100 nm fiber, these being the structures found in normal cells. During mitosis and meiosis, the condensed chromosomes are assembled through interactions between nucleosomes and other regulatory proteins.\n\nHistones are subdivided into canonical replication-dependent histones that are expressed during the S-phase of cell cycle and replication-independent histone variants, expressed during the whole cell cycle. In animals, genes encoding canonical histones are typically clustered along the chromosome, lack introns and use a stem loop structure at the 3’ end instead of a polyA tail. Genes encoding histone variants are usually not clustered, have introns and their mRNAs are regulated with polyA tails. Complex multicellular organisms typically have a higher number of histone variants providing a variety of different functions. Recent data are accumulating about the roles of diverse histone variants highlighting the functional links between variants and the delicate regulation of organism development. Histone variants from different organisms, their classification and variant specific features can be found in \"HistoneDB 2.0 - Variants\" database.\n\nThe following is a list of human histone proteins:\n\nThe nucleosome core is formed of two H2A-H2B dimers and a H3-H4 tetramer, forming two nearly symmetrical halves by tertiary structure (C2 symmetry; one macromolecule is the mirror image of the other). The H2A-H2B dimers and H3-H4 tetramer also show pseudodyad symmetry. The 4 'core' histones (H2A, H2B, H3 and H4) are relatively similar in structure and are highly conserved through evolution, all featuring a 'helix turn helix turn helix' motif (DNA-binding protein motif that recognize specific DNA sequence). They also share the feature of long 'tails' on one end of the amino acid structure - this being the location of post-translational modification (see below).\n\nIt has been proposed that histone proteins are evolutionarily related to the helical part of the extended AAA+ ATPase domain, the C-domain, and to the N-terminal substrate recognition domain of Clp/Hsp100 proteins. Despite the differences in their topology, these three folds share a homologous helix-strand-helix (HSH) motif.\n\nUsing an electron paramagnetic resonance spin-labeling technique, British researchers measured the distances between the spools around which eukaryotic cells wind their DNA. They determined the spacings range from 59 to 70 Å.\n\nIn all, histones make five types of interactions with DNA:\n\nThe highly basic nature of histones, aside from facilitating DNA-histone interactions, contributes to their water solubility.\n\nHistones are subject to post translational modification by enzymes primarily on their N-terminal tails, but also in their globular domains. Such modifications include methylation, citrullination, acetylation, phosphorylation, SUMOylation, ubiquitination, and ADP-ribosylation. This affects their function of gene regulation.\n\nIn general, genes that are active have less bound histone, while inactive genes are highly associated with histones during interphase. It also appears that the structure of histones has been evolutionarily conserved, as any deleterious mutations would be severely maladaptive. All histones have a highly positively charged N-terminus with many lysine and arginine residues.\n\nHistones were discovered in 1884 by Albrecht Kossel. The word \"histone\" dates from the late 19th century and is from the German word \"\"Histon\"\", a word itself of uncertain origin - perhaps from the Greek \"histanai\" or \"histos\". Until the early 1990s, histones were dismissed by most as inert packing material for eukaryotic nuclear DNA, a view based in part on the models of Mark Ptashne and others, who believed that transcription was activated by protein-DNA and protein-protein interactions on largely naked DNA templates, as is the case in bacteria.\n\nDuring the 1980s, Yahli Lorch and Roger Kornberg showed that a nucleosome on a core promoter prevents the initiation of transcription in vitro, and Michael Grunstein demonstrated that histones repress transcription in vivo, leading to the idea of the nucleosome as a general gene repressor. Relief from repression is believed to involve both histone modification and the action of chromatin-remodeling complexes. Vincent Allfrey and Alfred Mirsky earlier proposed a role of histone modification in transcriptional activation, regarded as a molecular manifestation of epigenetics. Michael Grunstein and David Allis found support for this proposal, in the importance of histone acetylation for transcription in yeast and the activity of the transcriptional activator Gcn5 as a histone acetyltransferase.\n\nThe discovery of the H5 histone appears to date back to the 1970s, and it is now considered an isoform of Histone H1.\n\nHistones are found in the nuclei of eukaryotic cells, and in certain Archaea, namely Thermoproteales and Euryarchaea, but not in bacteria. The unicellular algae known as dinoflagellates were previously thought to be the only eukaryotes that completely lack histones, however, later studies showed that their DNA still encodes histone genes.\n\nArchaeal histones may well resemble the evolutionary precursors to eukaryotic histones. Histone proteins are among the most highly conserved proteins in eukaryotes, emphasizing their important role in the biology of the nucleus. In contrast mature sperm cells largely use protamines to package their genomic DNA, most likely because this allows them to achieve an even higher packaging ratio.\n\nCore histones are highly conserved proteins; that is, there are very few differences among the amino acid sequences of the histone proteins of different species.\n\nThere are some \"variant\" forms in some of the major classes. They share amino acid sequence homology and core structural similarity to a specific class of major histones but also have their own feature that is distinct from the major histones. These \"minor histones\" usually carry out specific functions of the chromatin metabolism. For example, histone H3-like CenpA is associated with only the centromere region of the chromosome. Histone H2A variant H2A.Z is associated with the promoters of actively transcribed genes and also involved in the prevention of the spread of silent heterochromatin. Furthermore, H2A.Z has roles in chromatin for genome stability. Another H2A variant H2A.X is phosphorylated at S139 in regions around double-strand breaks and marks the region undergoing DNA repair. Histone H3.3 is associated with the body of actively transcribed genes.\n\nHistones act as spools around which DNA winds. This enables the compaction necessary to fit the large genomes of eukaryotes inside cell nuclei: the compacted molecule is 40,000 times shorter than an unpacked molecule.\n\nHistones undergo posttranslational modifications that alter their interaction with DNA and nuclear proteins. The H3 and H4 histones have long tails protruding from the nucleosome, which can be covalently modified at several places. Modifications of the tail include methylation, acetylation, phosphorylation, ubiquitination, SUMOylation, citrullination, and ADP-ribosylation. The core of the histones H2A and H2B can also be modified. Combinations of modifications are thought to constitute a code, the so-called \"histone code\". Histone modifications act in diverse biological processes such as gene regulation, DNA repair, chromosome condensation (mitosis) and spermatogenesis (meiosis).\n\nThe common nomenclature of histone modifications is:\n\nSo H3K4me1 denotes the monomethylation of the 4th residue (a lysine) from the start (i.e., the N-terminal) of the H3 protein.\n\nA huge catalogue of histone modifications have been described, but a functional understanding of most is still lacking. Collectively, it is thought that histone modifications may underlie a histone code, whereby combinations of histone modifications have specific meanings. However, most functional data concerns individual prominent histone modifications that are biochemically amenable to detailed study.\n\nThe addition of one, two, or three methyl groups to lysine has little effect on the chemistry of the histone; methylation leaves the charge of the lysine intact and adds a minimal number of atoms so steric interactions are mostly unaffected. However, proteins containing Tudor, chromo or PHD domains, amongst others, can recognise lysine methylation with exquisite sensitivity and differentiate mono, di and tri-methyl lysine, to the extent that, for some lysines (e.g.: H4K20) mono, di and tri-methylation appear to have different meanings. Because of this, lysine methylation tends to be a very informative mark and dominates the known histone modification functions.\nWhat was said above of the chemistry of lysine methylation also applies to arginine methylation, and some protein domains—e.g., Tudor domains—can be specific for methyl arginine instead of methyl lysine. Arginine is known to be mono- or di-methylated, and methylation can be symmetric or asymmetric, potentially with different meanings.\n\nEnzymes called peptidylarginine deiminases (PADs) hydrolyze the imine group of arginines and attach a keto group, so that there is one less positive charge on the amino acid residue. This process has been involved in the activation of gene expression by making the modified histones less tightly bound to DNA and thus making the chromatin more accessible. PADs can also produce the opposite effect by removing or inhibiting mono-methylation of arginine residues on histones and thus antagonizing the positive effect arginine methylation has on transcriptional activity.\n\nAddition of an acetyl group has a major chemical effect on lysine as it neutralises the positive charge. This reduces electrostatic attraction between the histone and the negatively charged DNA backbone, loosening the chromatin structure; highly acetylated histones form more accessible chromatin and tend to be associated with active transcription. Lysine acetylation appears to be less precise in meaning than methylation, in that histone acetyltransferases tend to act on more than one lysine; presumably this reflects the need to alter multiple lysines to have a significant effect on chromatin structure. The modification includes H3K27ac.\nAddition of a negatively charged phosphate group can lead to major changes in protein structure, leading to the well-characterised role of phosphorylation in controlling protein function. It is not clear what structural implications histone phosphorylation has, but histone phosphorylation has clear functions as a post-translational modification, and binding domains such as BRCT have been characterised.\n\nMost well-studied histone modifications are involved in control of transcription.\n\nTwo histone modifications are particularly associated with active transcription:\n\nThree histone modifications are particularly associated with repressed genes:\n\nAnalysis of histone modifications in embryonic stem cells (and other stem cells) revealed many gene promoters carrying both H3K4Me3 and H3K27Me3, in other words these promoters display both activating and repressing marks simultaneously. This peculiar combination of modifications marks genes that are poised for transcription; they are not required in stem cells, but are rapidly required after differentiation into some lineages. Once the cell starts to differentiate, these bivalent promoters are resolved to either active or repressive states depending on the chosen lineage.\n\nMarking sites of DNA damage is an important function for histone modifications. It also protects DNA from getting destroyed by ultraviolet radiation of sun.\n\nH3K36me3 has the ability to recruit the MSH2-MSH6 (hMutSα) complex of the DNA mismatch repair pathway. Consistenty, regions of the human genome with high levels of H3K36me3 accumulate less somatic mutations due to mismatch repair activity.\n\n\n\nThe first step of chromatin structure duplication is the synthesis of histone proteins: H1, H2A, H2B, H3, H4. These proteins are synthesized during S phase of the cell cycle. There are different mechanisms which contribute to the increase of histone synthesis. \n\nYeast carry one or two copies of each histone gene, which are not clustered but rather scattered throughout chromosomes. Histone gene transcription is controlled by multiple gene regulatory proteins such as transcription factors which bind to histone promoter regions. In budding yeast, the candidate gene for activation of histone gene expression is SBF. SBF is a transcription factor that is activated in late G1 phase, when it dissociates from its repressor Whi5. This occurs when Whi5 is phosphorylated by Cdc8 which is a G1/S Cdk. Suppression of histone gene expression outside of S phases is dependent on Hir proteins which form inactive chromatin structure at the locus of histone genes, causing transcriptional activators to be blocked.\n\nIn metazoans the increase in the rate of histone synthesis is due to the increase in processing of pre-mRNA to its mature form as well as decrease in mRNA degradation; this results in an increase of active mRNA for translation of histone proteins. The mechanism for mRNA activation has been found to be the removal of a segment of the 3’ end of the mRNA strand, and is dependent on association with stem-loop binding protein (SLBP). SLBP also stabilizes histone mRNAs during S phase by blocking degradation by the 3’hExo nuclease. SLBP levels are controlled by cell-cycle proteins, causing SLBP to accumulate as cells enter S phase and degrade as cells leave S phase. SLBP are marked for degradation by phosphorylation at two threonine residues by cyclin dependent kinases, possibly cyclin A/ cdk2, at the end of S phase. Metazoans also have multiple copies of histone genes clustered on chromosomes which are localized in structures called Cajal bodies as determined by genome-wide chromosome conformation capture analysis (4C-Seq). \n\nNuclear protein Ataxia-Telangiectasia (NPAT), also known as nuclear protein coactivator of histone transcription, is a transcription factor which activates histone gene transcription on chromosomes 1 and 6 of human cells. NPAT is also a substrate of cyclin E-Cdk2, which is required for the transition between G1 phase and S phase. NPAT activates histone gene expression only after it has been phosphorylated by the G1/S-Cdk cyclin E-Cdk2 in early S phase. This shows an important regulatory link between cell-cycle control and histone synthesis.\n\n\n", "id": "14029", "title": "Histone"}
{"url": "https://en.wikipedia.org/wiki?curid=32761768", "text": "Scaffold/matrix attachment region\n\nThe term S/MAR (scaffold/matrix attachment region), otherwise called SAR (scaffold-attachment region), or MAR (matrix-associated region), are sequences in the DNA of eukaryotic chromosomes where the nuclear matrix attaches. As architectural DNA components that organize the genome of eukaryotes into functional units within the cell nucleus, S/MARs mediate structural organization of the chromatin within the nucleus. These elements constitute anchor points of the DNA for the chromatin scaffold and serve to organize the chromatin into structural domains. Studies on individual genes led to the conclusion that the dynamic and complex organization of the chromatin mediated by S/MAR elements plays an important role in the regulation of gene expression.\n\nIt has been known for many years that a polymer meshwork, a so-called \"nuclear matrix\" or \"nuclear-scaffold\" is an essential component of eukaryotic nuclei. This nuclear skeleton acts as a dynamic support for many specialized events concerning the readout a spread of genetic information (see below).\n\nS/MARs map to non-random locations in the genome. They occur at the flanks of transcribed regions, in 5´-introns, and also at gene breakpoint cluster regions (BCRs). Being association points for common nuclear structural proteins S/MARs are required for authentic and efficient chromosomal replication and transcription, for recombination and chromosome condensation. S/MARs do not have an obvious consensus sequence. Although prototype elements consist of AT-rich regions several hundred base pairs in length, the overall base composition is definitely not the primary determinant of their activity. Instead, their function requires a pattern of \"AT-patches\" that confer the propensity for local strand unpairing under torsional strain.\n\nBioinformatics approaches support the idea that, by these properties, S/MARs not only separate a given transcriptional unit (chromatin domain) from its neighbors, but also provide platforms for the assembly of factors enabling transcriptional events within a given domain. An increased propensity to separate the DNA strands (the so-called 'stress induced duplex destabilization' potential, SIDD) can serve the formation of secondary structures such as cruciforms or slippage structures, which are recognizable features for a number of enzymes (DNAses, topoisomerases, poly(ADP-ribosyl) polymerases and enzymes of the histone-acetylation and DNA-methylation apparatus). S/MARs have been classified as either being constitutive (acting as permanent domain boundaries in all cell types) or facultative (cell type- and activity-related) depending on their dynamic properties.\n\nWhile the number of S/MARs in the human genome has been estimated to approach 64,000 (chromatin domains) plus an additional 10,000 (replication foci), in 2007 still only a minor fraction (559 for all eukaryotes) had met the standard criteria for an annotation in the S/MARt database SMARtDB.\n\nCurrent views of the nuclear matrix envision it as a dynamic entity, which changes its properties along the requirements of the cell nucleus—much the same as the cytoskeleton adapts its structure and function to external signals. In retrospect it is of note that the discovery of S/MARs has two major routes:\nSubsequent work demonstrated both the constitutive (SAR-like) and the facultative (MAR-like) function of the elements depending on the context. Whereas constitutive S/MARs were found to be associated with a DNase I hypersensitive site in 'all' cell types (whether or not the enclosed domain was transcribed), DNAse I hypersensitivity of the facultative type depended on the transcriptional status. The major difference between these two functional types of S/MARs is their size: the constitutive elements may extend over several kilobasepairs whereas facultative ones are at the lower size limit around 300 base pairs.\n\nThe figure shows our present understanding of these properties and it incorporates the following findings:\n\nRecently, Tetko has found a strong correlation of intragenic S/MARs with spatiotemporal expression of genes in \"Arabidopsis thaliana\". On a genome scale, pronounced tissue- and organ-specific and developmental expression patterns of S/MAR-containing genes have been detected. Notably, transcription factor genes contain a significant higher portion of S/MARs. The pronounced difference in expression characteristics of S/MAR-containing genes emphasizes their functional importance and the importance of structural chromosomal characteristics for gene regulation in plants as well as within other eukaryotes.\n", "id": "32761768", "title": "Scaffold/matrix attachment region"}
{"url": "https://en.wikipedia.org/wiki?curid=12989754", "text": "GAL4/UAS system\n\nThe GAL4-UAS system is a biochemical method used to study gene expression and function in organisms such as the fruit fly. It has also been adapted to study receptor chemical-binding functions in vitro in cell culture. It was developed by Hitoshi Kakidani and Mark Ptashne, and Nicholas Webster and Pierre Chambon in 1988, then adapted by Andrea Brand and Norbert Perrimon in 1993 and is considered a powerful technique for studying the expression of genes. The system has two parts: the Gal4 gene, encoding the yeast transcription activator protein Gal4, and the UAS (Upstream Activation Sequence), an enhancer to which GAL4 specifically binds to activate gene transcription.\n\nThe Gal4 system allows separation of the problems of defining which cells express a gene or protein and what the experimenter wants to do with this knowledge. Geneticists have created genetic varieties of model organisms (typically fruit flies), called \"GAL4 lines\", each of which expresses GAL4 in some subset of the animal's tissues. For example, some lines might express GAL4 only in muscle cells, or only in nerves, or only in the antennae, and so on. For fruit flies in particular, there are tens of thousands of such lines, with the most useful expressing GAL4 in only a very specific subset of the animal—perhaps, for example, only those neurons that connect two specific compartments of the fly's brain. The presence of GAL4, by itself, in these cells has little or no effect, since GAL4's main effect is to bind to a UAS region, and most cells have no (or innocuous) UAS regions.\n\nSince Gal4 by itself is not visible, and has little effect on cells, the other necessary part of this system are the \"reporter lines\". These are strains of flies with the special UAS region next to a desired gene. These genetic instructions occur in every cell of the animal, but in most cells nothing happens since that cell is not producing GAL4. In the cells that \"are\" producing GAL4, however, the UAS is activated, the gene next to it is turned on, and it starts producing its resulting protein. This may report to the investigator which cells are expressing GAL4, hence the term \"reporter line\", but genes intended to manipulate the cell behavior are often used as well.\n\nTypical reporter genes include:\n\nFor example, scientists can first visualize a class of neurons by choosing a fly from a GAL4 line that expresses GAL4 in the desired set of neurons, and crossing it with a reporter line that express GFP. In the offspring, the desired subset of cells will make GAL4, and in these cells the GAL4 will bind to the UAS, and enable the production of GFP. So the desired subset of cells will now fluoresce green and can be followed with a fluorescence microscope. Next, to figure out what these cells might do, the experimenter might express channelrhodopsin in each of these cells, by crossing the same GAL4 line with a channelrhodopsin reporter line. In the offspring the selected cells, and only those cells, will contain channelrhodopsin and can be triggered by a bright light. Now the scientist can trigger these particular cells at will, and examine the resulting behavior to see what these cells might do.\n\nGAL4 is a modular protein consisting broadly of a DNA-binding domain and an activation domain. The UAS to which GAL4 binds is CGG-N-CCG, where N can be any base. Although GAL4 is a yeast protein not normally present in other organisms it has been shown to work as a transcription activator in a variety of organisms such as \"Drosophila\", and human cells, highlighting that the same mechanisms for gene expression have been conserved over the course of evolution.\n\nFor study in \"Drosophila\", the GAL4 gene is placed under the control of a native gene promoter, or driver gene, while the UAS controls expression of a target gene. GAL4 is then only expressed in cells where the driver gene is usually active. In turn, GAL4 should only activate gene transcription where a UAS has been introduced. For example, by fusing a gene encoding a visible marker like GFP (Green Fluorescent Protein) the expression pattern of the driver genes can be determined. GAL4 and the UAS are very useful for studying gene expression in \"Drosophila\" as they are not normally present and their expression does not interfere with other processes in the cell. For example, GAL4/UAS-regulated transgenes in \"Drosophila\" have been used to alter glial expression to produce arrhythmic behavior in a known rhythmic circadian output called pigment dispersing factor (PDF). However, some research has indicated that over-expression of GAL4 in \"Drosophila\" can have side-effects, probably relating to immune and stress responses to what is essentially an alien protein.\n\nThe GAL4-UAS system has also been employed to study gene expression in organisms besides \"Drosophila\" such as the African clawed frog \"Xenopus\" and zebrafish.\n\nThe GAL4/UAS system is also utilized in Two-Hybrid Screening, a method of identifying interactions between two proteins or a protein with DNA.\n\nGAL4 expression can be made even more specific by means of \"intersectional strategies\". These can combine two different GAL4 lines—say, A and B—in a way that GAL4 is only expressed in the cells that are in line A but not line B, or those that are in both lines A and B. When combined with intrinsically sparse GAL4 lines, this offers very specific selection, often limited to a single cell type. The disadvantage is that at least three independent insertion sites are required, so the lines must use different and independent insertion sites, and creating the desired final organisms needs more than a single cross. This is a very active field of research, and there are many such intersectional strategies, of which two are discussed below.\n\nOne way to create GAL4 expression in the cells that are in line A but not line B, requires line A to be made to express GAL4, and line B made to express GAL80, which is a GAL4 inhibitor. Therefore, only the cells that are in A but not B will have active GAL4, which can then drive the reporter gene.\n\nTo express GAL4 in only the cells contained in both A and B, a technique called \"split-GAL4\" can be used. Line A is made to express half of the GAL4 protein, which is inactive by itself. Similarly, line B is made to express the other half of GAL4, also inactive by itself. Only the cells that are in both lines make both halves, which self-assemble by leucine zipper into GAL4 and activate the reporter gene.\n", "id": "12989754", "title": "GAL4/UAS system"}
{"url": "https://en.wikipedia.org/wiki?curid=5457540", "text": "Ka/Ks ratio\n\nIn genetics, the K/K ratio is used to estimate the balance between neutral mutations, purifying selection and beneficial mutations acting on a set of homologous protein-coding genes. It is calculated as the ratio of the number of nonsynonymous substitutions per non-synonymous site (K), in a given period of time, to the number of synonymous substitutions per synonymous site (K), in the same period. The latter are assumed to be neutral, so that the ratio indicates the net balance between deleterious and beneficial mutations. Values of K/K significantly above 1 are unlikely to occur without at least some of the mutations being advantageous. If beneficial mutations are assumed to make little contribution, then K estimates the degree of evolutionary constraint.\n\nThe ratio is also known as ω or \"d\"/\"d\".\n\nEvolution acts on genes that code for proteins. The genetic code is written in DNA sequences as codons, groups of three nucleotides. Each codon represents a single amino acid in a protein chain. However, there are more codons (64) than amino acids found in proteins (20), so many codons are effectively synonyms. For example, the DNA codons TTT and TTC both code for the amino acid Phenylalanine, so a change from the third T to C makes no difference to the resulting protein. On the other hand, the codon GAG codes for Glutamic acid while the codon GTG codes for Valine, so a change from the middle A to T does change the resulting protein, for better or (more likely) worse, so the change is not a synonym. These changes are illustrated in the tables below.\n\nThe K/K ratio measures the relative rates of synonymous and nonsynonymous substitutions at a particular site.\n\nMethods for estimating K and K use a sequence alignment of two or more nucleotide sequences of homologous genes that code for proteins (rather than being genetic switches, controlling development or the rate of activity of other genes). Methods can be classified into three groups: approximate methods, maximum-likelihood methods, and counting methods. However, unless the sequences to be compared are distantly related (in which case maximum-likelihood methods prevail), the class of method used makes a minimal impact on the results obtained; more important are the assumptions implicit in the chosen method.\n\nApproximate methods involve three basic steps:\n\nThese steps, particularly the latter, require simplistic assumptions to be made if they are to be achieved computationally; for reasons discussed later, it is impossible to exactly determine the number of multiple substitutions.\n\nThe maximum-likelihood approach uses probability theory to complete all three steps simultaneously. It estimates critical parameters, including the divergence between sequences and the transition/transversion ratio, by deducing the most likely values to produce the input data.\n\nIn order to quantify the number of substitutions, one may reconstruct the ancestral sequence and record the inferred changes at sites (straight counting – likely to provide an underestimate); fitting the substitution rates at sites into predetermined categories (Bayesian approach; poor for small data sets); and generating an individual substitution rate for each codon (computationally expensive). Given enough data, all three of these approaches will tend to the same result.\n\nThe K/K ratio is used to infer the direction and magnitude of natural selection acting on protein coding genes. A ratio greater than 1 implies positive or Darwinian selection (driving change); less than 1 implies purifying or stabilizing selection (acting against change); and a ratio of exactly 1 indicates neutral (i.e. no) selection. However, a combination of positive and purifying selection at different points within the gene or at different times along its evolution may cancel each other out, giving an average value that may be lower than, equal to, or higher than 1.\n\nOf course, it is necessary to perform a statistical analysis to determine whether a result is significantly different from 1, or whether any apparent difference may occur as a result of a limited data set. The appropriate statistical test for an approximate method involves approximating dN − dS with a normal approximation, and determining whether 0 falls within the central region of the approximation. More sophisticated likelihood techniques can be used to analyse the results of a Maximum Likelihood analysis, by performing a chi-squared test to distinguish between a null model (K/K = 1) and the observed results.\n\nThe K/K ratio is a more powerful test of the neutral model of evolution than many others available in population genetics as it requires fewer assumptions.\n\nThere is often a systematic bias in the frequency at which various nucleotides are swapped, as certain mutations are more probable than others. For instance, some lineages may swap C to T more frequently than they swap C to A. In the case of the amino acid Asparagine, which is coded by the codons AAT or AAC, a high C->T exchange rate will increase the proportion of synonymous substitutions at this codon, whereas a high C→A exchange rate will increase the rate of non-synonymous substitutions. Because it is rather common for transitions (T↔C & A↔G) to be favoured over transversions (other changes), models must account for the possibility of non-homogeneous rates of exchange. Some simpler approximate methods, such as those of Miyata & Yasunaga and Nei & Gojobori, neglect to take these into account, which generates a faster computational time at the expense of accuracy; these methods will systematically overestimate N and underestimate S.\n\nFurther, there may be a bias in which certain codons are preferred in a gene, as a certain combination of codons may improve translational efficiency.\n\nIn addition, as time progresses, it is possible for a site to undergo multiple modifications. For instance, a codon may switch from AAA→AAC→AAT→AAA. There is no way of detecting multiple substitutions at a single site, thus the estimate of the number of substitutions is always an underestimate. In addition, in the example above two non-synonymous and one synonymous substitution occurred at the third site; however, because substitutions restored the original sequence, there is no evidence of any substitution. As the divergence time between two sequences increases, so too does the amount of multiple substitutions. Thus \"long branches\" in a dN/dS analysis can lead to underestimates of both dN and dS, and the longer the branch, the harder it is to correct for the introduced noise. Of course, the ancestral sequence is usually unknown, and two lineages being compared will have been evolving in parallel since their last common ancestor. This effect can be mitigated by constructing the ancestral sequence; the accuracy of this sequence is enhanced by having a large number of sequences descended from that common ancestor to constrain its sequence by phylogenetic methods.\n\nMethods that account for biases in codon usage and transition/transversion rates are substantially more reliable than those that do not.\n\nAlthough the K/K ratio is a good indicator of selective pressure at the sequence level, evolutionary change can often take place in the regulatory region of a gene which affects the level, timing or location of gene expression. K/K analysis will not detect such change. It will only calculate selective pressure within protein coding regions. In addition, selection that does not cause differences at an amino acid level—for instance, balancing selection—cannot be detected by these techniques.\n\nAnother issue is that heterogeneity within a gene can make a result hard to interpret. For example, if K/K = 1, it could be due to relaxed selection, or to a chimera of positive and purifying selection at the locus. A solution to this limitation would be to apply K/K analysis across many species at individual codons.\n\nThe K/K method requires a rather strong signal in order to detect selection. \nIn order to detect selection between lineages, then the selection, averaged over all sites in the sequence, must produce a K/K greater than one—quite a feat if regions of the gene are strongly conserved.\nIn order to detect selection at specific sites, then the K/K ratio must be greater than one when averaged over all included \"lineages\" at that site—implying that the site must be under selective pressure in all sampled lineages. \nThis limitation can be moderated by allowing the K/K rate to take multiple values across sites and across lineages; the inclusion of more lineages also increases the power of a sites-based approach.\n\nFurther, the method lacks the capability to distinguish between positive and negative nonsynonymous substitutions. Some amino acids are chemically similar to one another, whereas other substitutions may place an amino acid with wildly different properties to its precursor. In most situations, a smaller chemical change is more likely to allow the protein to continue to function, and a large chemical change is likely to disrupt the chemical structure and cause the protein to malfunction. However, incorporating this into a model is not straightforward as the relationship between a nucleotide substitution and the effects of the modified chemical properties is very difficult to determine.\n\nAn additional concern is that the effects of time must be incorporated into an analysis, if the lineages being compared are closely related; this is because it can take a number of generations for natural selection to \"weed out\" deleterious mutations from a population, especially if their effect on fitness is weak. This limits the usefulness of the K/K ratio for comparing closely related populations.\n\nAdditional information can be gleaned by determining the K/K ratio at specific codons within a gene sequence. For instance, the frequency-tuning region of an opsin may be under enhanced selective pressure when a species colonises and adapts to new environment, whereas the region responsible for initializing a nerve signal may be under purifying selection. In order to detect such effects, one would ideally calculate the K/K ratio at each site. However this is computationally expensive and in practise, a number of K/K classes are established, and each site is shoehorned into the best-fitting class.\n\nThe first step in identifying whether positive selection acts on sites is to compare a test where the K/K ratio is constrained to be < 1 in all sites to one where it may take any value, and see if permitting K/K to exceed 1 in some sites improves the fit of the model. If this is the case, then sites fitting into the class where K/K > 1 are candidates to be experiencing positive selection. This form of test can either identify sites that further laboratory research can examine to determine possible selective pressure; or, sites believed to have functional significance can be assigned into different K/K classes before the model is run.\n\n\n\n", "id": "5457540", "title": "Ka/Ks ratio"}
{"url": "https://en.wikipedia.org/wiki?curid=38669910", "text": "DNase I hypersensitive site\n\nIn genetics, DNase I hypersensitive sites (DHSs) are regions of chromatin that are sensitive to cleavage by the DNase I enzyme. In these specific regions of the genome, chromatin has lost its condensed structure, exposing the DNA and making it accessible. This raises the availability of DNA to degradation by enzymes, such as DNase I. These accessible chromatin zones are functionally related to transcriptional activity, since this remodeled state is necessary for the binding of proteins such as transcription factors.\n\nSince the discovery of DHSs 30 years ago, they have been used as markers of regulatory DNA regions. These regions have been shown to map many types of cis-regulatory elements including promoters, enhancers, insulators, silencers and locus control regions. A high-throughput measure of these regions is available through DNase-Seq.\n\nThe ENCODE project proposes to map all of the DHSs in the human genome with the intention of cataloging human regulatory DNA.\n\nDHSs mark transcriptionally active regions of the genome, where there will be cellular selectivity. So, they used 125 different human cell types. This way, using the massive sequencing technique, they obtained the DHSs profiles of every cellular type. Through an analysis of the data, they identified almost 2.9 million distinct DHSs. 34% were specific to each cell type, and only a small minority (3,692) were detected in all cell types. Also, it was confirmed that only 5% of DHSs were found in TSS (Transcriptional Start Site) regions. The remaining 95% represented distal DHSs, divided in a uniform way between intronic and intergenic regions. The data gives an idea of the great complexity regulating the genetic expression in the human genome and the quantity of elements that control this regulation.\n\nThe high-resolution mapping of DHSs in the model plant Arabidopsis thaliana has been reported. Total 38,290 and 41,193 DHSs in leaf and flower tissues have been identified, respectively.\n\nThe study of DHS profiles combined with other techniques allows analysis of regulatory DNA in humans:\n\n\nThe data obtained were validated with the chromosome conformation capture carbon copy (5C) technique. This technique is based in the physical association that exists between the promoter and the enhancers, determining the regions of chromatin that enter in contact in the promoter/enhancer connections.\n\nIt was confirmed that the majority of promoters were related with more than one enhancer, which indicates the existence of a complicated network of regulation for the immense majority of genes. Surprisingly, they also found that approximately half of the enhancers were found to be associated with more than one promoter. This discovery shows that the human cis-regulatory system is much more complicated than initially thought.\n\nThe number of distal cis-regulatory elements connected to a promoter is related to the quantitative average of the regulation complexity of a gene. In this way, it was determined that human genes with more interactions with distal DHSs, and with at least one more complex regulation, corresponded with those genes with functions in the immune system. This indicates that the complexly of cellular and environmental signals processed by the immune system is directly encoded in the cis-regulatory architecture of its constituent genes.\n\n", "id": "38669910", "title": "DNase I hypersensitive site"}
{"url": "https://en.wikipedia.org/wiki?curid=38674471", "text": "Enhancer RNA\n\nEnhancer RNAs (eRNAs) represent a class of relatively short non-coding RNA molecules (50-2000 nucleotides) transcribed from the DNA sequence of enhancer regions. They were first detected in 2010 through the use of genome-wide techniques such as RNA-seq and ChIP-seq. eRNAs can be subdivided into two main classes: 1D eRNAs and 2D eRNAs, which differ primarily in terms of their size, polyadenylation state, and transcriptional directionality. The expression of a given eRNA seems to correlate with the activity of its corresponding enhancer in a context-dependent fashion. Increasing evidence suggests that eRNAs actively play a role in transcriptional regulation in cis and in trans, and while their mechanisms of action remain unclear, a few models have been proposed.\n\nEnhancers as sites of extragenic transcription were initially discovered in genome-wide studies that identified enhancers as common regions of RNA polymerase II (RNA pol II) binding and non-coding RNA transcription. The level of RNA pol II-enhancer interaction and RNA transcript formation were found to be highly variable among these initial studies. Using explicit chromatin signature peaks, a significant proportion (~70%) of extragenic RNA Pol II transcription start sites were found to overlap enhancer sites in murine macrophages. Out of 12,000 neuronal enhancers in the mouse genome, almost 25% of the sites were found to bind RNA Pol II and generate transcripts. These eRNAs, unlike messenger RNAs (mRNAs), lacked modification by polyadenylation, were generally short and non-coding, and were bidirectionally transcribed. Later studies revealed the transcription of another type of eRNAs, generated through unidirectional transcription, that were longer and contained a poly A tail. Furthermore, eRNA levels were correlated with mRNA levels of nearby genes, suggesting the potential regulatory and functional role of these non-coding enhancer RNA molecules.\n\neRNAs are transcribed from DNA sequences upstream and downstream of extragenic enhancer regions. Previously, several model enhancers have demonstrated the capability to directly recruit RNA Pol II and general transcription factors and form the pre-initiation complex (PIC) prior to the transcription start site at the promoter of genes. In certain cell types, activated enhancers have demonstrated the ability to both recruit RNA Pol II and also provide a template for active transcription of their local sequences.\n\nDepending on the directionality of transcription, enhancer regions generate two different types of non-coding transcripts, 1D-eRNAs and 2D-eRNAs. The nature of the pre-initiation complex and specific transcription factors recruited to the enhancer may control the type of eRNAs generated. After transcription, the majority of eRNAs remain in the nucleus. In general, eRNAs are very unstable and actively degraded by the nuclear exosome. Not all enhancers are transcribed, with non-transcribed enhancers greatly outnumbering the transcribed ones in the order of magnitude of dozens of thousands in every given cell type.\n\nIn most cases, unidirectional transcription of enhancer regions generates long (>4kb) and polyadenylated eRNAs. Enhancers that generate polyA+ eRNAs have a lower H3K4me1/me3 ratio in their chromatin signature than 2D-eRNAs. PolyA+ eRNAs are distinct from long multiexonic poly transcripts (meRNAs) that are generated by transcription initiation at intragenic enhancers. These long non-coding RNAs, which accurately reflect the host gene’s structure except for the alternative first exon, display poor coding potential. As a result, polyA+ 1D-eRNAs may represent a mixed group of true enhancer-templated RNAs and multiexonic RNAs.\n\nBidirectional transcription at enhancer sites generates comparatively shorter (0.5-2kb) and non-polyadenylated eRNAs. Enhancers that generate polyA- eRNAs have a chromatin signature with a higher H3K4me1/me3 ratio than 1D-eRNAs. In general, enhancer transcription and production of bidirectional eRNAs demonstrate a strong correlation of enhancer activity on gene transcription.\n\nThe notions that not all enhancers are transcribed at the same time and that eRNA transcription correlates with enhancer-specific activity support the idea that individual eRNAs carry distinct and relevant biological functions. However, there is still no consensus on the functional significance of eRNAs. Furthermore, eRNAs can easily be degraded through exosomes and nonsense-mediated decay, which limits their potential as important transcriptional regulators. To date, four main models of eRNA function have been proposed, each supported by different lines of experimental evidence.\n\nSince multiple studies have shown that RNA Pol II can be found at a very large number of extragenic regions, it is possible that eRNAs simply represent the product of random “leaky” transcription and carry no functional significance. The non-specific activity of RNA Pol II would therefore allow extragenic transcriptional noise at sites where chromatin is already in an open and transcriptionally competent state. This would explain even tissue-specific eRNA expression as open sites are tissue-specific as well.\n\nRNA Pol II-mediated gene transcription induces a local opening of chromatin state through the recruitment of histone acetyltransferases and other histone modifiers that promote euchromatin formation. It was proposed that the presence of these enzymes could also induce an opening of chromatin at enhancer regions, which are usually present at distant locations but can be recruited to target genes through looping of DNA. In this model, eRNAs are therefore expressed in response to RNA Pol II transcription and therefore carry no biological function.\n\nWhile the two previous models implied that eRNAs were not functionally relevant, this mechanism states that eRNAs are functional molecules that exhibit cis activity. In this model, eRNAs can locally recruit regulatory proteins at their own site of synthesis. Supporting this hypothesis, transcripts originating from enhancers upstream of the Cyclin D1 gene are thought to serve as adaptors for the recruitment of histone acetyltransferases. It was found that depletion of these eRNAs led to Cyclin D1 transcriptional silencing.\n\nThe last model involves transcriptional regulation by eRNAs at distant chromosomal locations. Through the differential recruitment of protein complexes, eRNAs can affect the transcriptional competency of specific loci. Evf-2 represents a good example of such trans regulatory eRNA as it can induce the expression of Dlx2, which in turn can increase the activity of the Dlx5 and Dlx6 enhancers. It must be noted that trans-acting eRNAs might also be working in cis, and vice versa.\n\nThe detection of eRNAs is fairly recent (2010) and has been made possible through the use of genome-wide investigation techniques such as RNA sequencing (RNA-seq) and chromatin immunoprecipitation-sequencing (ChIP-seq). RNA-seq permits the direct identification of eRNAs by matching the detected transcript to the corresponding enhancer sequence through bioinformatic analyses. ChIP-seq represents a less direct way to assess enhancer transcription but can also provide crucial information as specific chromatin marks are associated with active enhancers. Although some data remain controversial, the consensus in the literature is that the best combination of histone post-translational modifications at active enhancers is made of H2AZ, H3K27ac, and a high ratio of H3K4me1 over H3K4me3. ChIP experiments can also be conducted with antibodies that recognize RNA Pol II, which can be found at sites of active transcription. The experimental detection of eRNAs is complicated by their low endogenous stability conferred by exosome degradation and nonsense-mediated decay. Nonetheless, the fact that eRNAs tend to be expressed from active enhancers might make their detection a useful tool to distinguish between active and inactive enhancers.\n\nEvidence that eRNAs cause downstream effects on the efficiency of enhancer activation and gene transcription suggests its functional capabilities and potential importance.\nThe transcription factor p53 has been demonstrated to bind enhancer regions and generate eRNAs in a p53-dependent manner. In cancer, p53 plays a central role in tumor suppression as mutations of the gene are shown to appear in 50% of tumors. These p53-bound enhancer regions (p53BERs) are shown to interact with multiple local and distal gene targets involved in cell proliferation and survival. Furthermore, eRNAs generated by the activation of p53BERs are shown to be required for efficient transcription of the p53 target genes, indicating the likely important regulatory role of eRNAs in tumor suppression and cancer.\n\nVariations in enhancers have been implicated in human disease but a therapeutic approach to manipulate enhancer activity is currently not possible. With the emergence of eRNAs as important components in enhancer activity, powerful therapeutic tools such as RNAi may provide promising routes to target disruption of gene expression.\n\n", "id": "38674471", "title": "Enhancer RNA"}
{"url": "https://en.wikipedia.org/wiki?curid=38734340", "text": "Chaperone code\n\nThe Chaperone code has been identified as one of the main regulatory mechanisms underlying cell function in biology. While the genetic code specifies how DNA makes proteins, while the histone code rules genomic transactions, the chaperone code stipulates how proteins produce a functional proteome.\nThe chaperone code refers to the combinatorial array of posttranslational modifications - i.e. phosphorylation, acetylation, ubiquitination, methylation, etc - that target molecular chaperones to modulate their activity. Molecular chaperones are proteins specialized in folding and unfolding of the other cellular proteins and assembly and dismantling of protein complexes, thereby orchestrating the dynamic organization of the proteome. As a consequence, a limited number of chaperones must be able to act on a very large number of substrates in a highly regulated manner.\n\nThe chaperone code concept posits that combinations of posttranslational modifications at the surface of chaperones, including phosphorylation, acetylation, methylation, ubiquitination, etc, control protein folding/unfolding and protein complex assembly/disassembly by stipulating substrate specificity, activity, subcellular localization and co-factor binding. This conclusion emerges from the analysis of nearly two hundred reports in the literature, including a key article published in 2013 reporting on the discovery of a novel family of methyltransferases that preferentially target and regulate molecular chaperones. Because posttranslational modifications are marks that can be added and removed rapidly, they provide an efficient mechanism to explain the plasticity observed in proteome organization during cell growth and development.\nA large number of diseases, including degenerative neuromuscular disorders and cancer, are associated with dysfunction of molecular chaperones. Decrypting and reprogramming the chaperone code represents a gigantic initiative that generates new hopes for the development of therapeutics for degenerative diseases.\n", "id": "38734340", "title": "Chaperone code"}
{"url": "https://en.wikipedia.org/wiki?curid=38760393", "text": "Haplotype estimation\n\nIn genetics, haplotype estimation (also known as \"phasing\") refers to the process of statistical estimation of haplotypes from genotype data. The most common situation arises when genotypes are collected at a set of polymorphic sites from a group of individuals. For example in human genetics, genome-wide association studies collect genotypes in thousands of individuals at between 200,000-5,000,000 SNPs using microarrays. Haplotype estimation methods are used in the analysis of these datasets and allow genotype imputation of alleles from reference databases such as the HapMap Project and the 1000 Genomes Project.\n\nGenotypes measure the unordered combination of alleles at each site, whereas haplotypes are the two sequence of alleles that have been inherited together from the individuals parents. When there are formula_1 heterozygous genotypes present in an individual's set of genotypes, there will be formula_2 possible pairs of haplotypes that could underlie the genotypes. For example, when formula_3, we have the following haplotypes: AA/TT, AT/TA, TA/AT, and TT/AA. If there are missing genotypes then the number of possible haplotype pairs increases.\n\nMany statistical methods that have been proposed for estimation of haplotypes. Some of the earliest approaches used a simple multinomial model in which each possible haplotype consistent with the sample was given an unknown frequency parameter and these parameters were estimated with an Expectation–maximization algorithm. These approaches were only able to handle small numbers of sites at once, although sequential versions were later developed, specifically the SNPHAP method.\n\nThe most accurate and widely used methods for haplotype estimation utilize some form of hidden Markov model (HMM) to carry out inference. For a long time PHASE was the most accurate method. PHASE was the first method to utilize ideas from coalescent theory concerning the joint distribution of haplotypes. This method used a Gibbs sampling approach in which each individuals haplotypes were updated conditional upon the current estimates of haplotypes from all other samples. Approximations to the distribution of a haplotype conditional upon a set of other haplotypes were used for the conditional distributions of the Gibbs sampler. PHASE was used to estimate the haplotypes from the HapMap Project. PHASE was limited by its speed and was not applicable to datasets from genome-wide association studies.\n\nThe fastPHASE and BEAGLE methods introduced haplotype cluster models applicable to GWAS-sized datasets. Subsequently the IMPUTE2 and MaCH methods were introduced that were similar to the PHASE approach but much faster. These methods iteratively update the haplotype estimates of each sample conditional upon a subset of K haplotype estimates of other samples. IMPUTE2 introduced the idea of carefully choosing which subset of haplotypes to condition on to improve accuracy. Accuracy increases with K but with quadratic formula_4 computational complexity.\n\nThe SHAPEIT1 method made a major advance by introducing a linear formula_5 complexity method that operates only on the space of haplotypes consistent with an individual’s genotypes. The HAPI-UR method subsequently proposed a very similar method. SHAPEIT2 combines the best features of SHAPEIT1 and IMPUTE2 to improve efficiency and accuracy.\n\n", "id": "38760393", "title": "Haplotype estimation"}
{"url": "https://en.wikipedia.org/wiki?curid=7376681", "text": "Phene\n\nA phene is an individual genetically determined characteristic or trait which can be possessed by an organism, such as eye colour, height, behavior, tooth shape or any other observable characteristic.\n\nThe term 'phene' was evidently coined as an obvious parallel construct to 'gene'. Phene is to Phenotype as Gene is to Genotype, and Similarly Phene is to Phenome as Gene is to Genome. More specifically, a Phene is an abstract concept describing a particular characteristic which can be possessed by an organism. Whereas Phenotype refers to a collection of Phenes possessed by a particular organism, and Phenome refers to the entire set of Phenes that exist within an organism or species.\n\nGenome wide association studies use \"phenes\" or \"traits\" (symptoms) to distinguish groups in the human population. These groups are then employed to identify associations with genetic alleles that are more common in the symptomatic group than in the asymptomatic control group. Allen et al. report that with respect to Schizophrenia \"Research in molecular genetics has focused on detecting multiple genes of small effect\" This indicates the importance of discovering individual traits or \"phenes\" that are governed by single genes. Schizophrenia or bipolar disorder may be described as a phenotype but how many individual traits or \"phenes\" contribute to these phenotypes? Very large genome wide association studies have not found many significant gene linkages. On the contrary the results of these studies implicate a large number of gene alleles that have a very small effect (phene).\n\nIt is important to note that the word phenotype was originally used to refer to both the trait/character itself (e.g. the blue eyes phenotype) and the set of traits/characteristics possessed by the organism (clair's eye-colour phenotype is blue). While this definition is still used in many places, the lack of distinction can make indepth explanations confusing and thus use of the term Phene becomes necessary. Indeed, it is extremely difficult to determine precisely what the fundamental building blocks of a phenome are. Since the term \"phenotype\" has been used to describe traits and syndromes and population characteristics it is not helpful in the collective search for specific traits that could be a consequence of a single gene or gene-environmental interaction. Phene has emerged as a candidate building block for the phenome.\n\nGenes give rise to phenes. Genes are the biochemical instructions encoding what an organism \"can\" be, while phenes are what the organism \"is\". In general it takes a combination of particular genes, environmental influences and random variation to give rise to any one phene in an organism. Both phenes and genes are subject to evolution. However, if one defines \"genes\" as \"DNA sequences encoding polypeptides\", they are not directly accessible to natural selection; the associated phenes are. Note that some, e.g. Richard Dawkins, have used a wider definition of \"gene\" than the one used in genetics on occasion, extending it to any DNA sequence with a function.\n\nDue to the distinct chemical and physical properties of the nucleotides in the DNA and some mutations being \"silent\" (that is, not altering gene expression), the DNA primary sequence may also be a phene. For example, A-T and C-G base pairs are differently resistant to heat (see also DNA-DNA hybridization). In a thermophilic microorganism, \"silent\" mutations may have an effect on DNA stability and thus survival. While being subject to evolution, natural selection affects the primary sequence directly in this case, with or without it being expressed.\n\nConsider, for example, a mutation that makes a zygote abort development as a young embryo. This mutation, obviously, will not spread, as it is quickly fatal. It is not the mutated nucleotide that is selected against, but the fact that \"due to\" this mutation, the phene (a key enzyme or developmental factor for example) does not get expressed.\n\nCompare a (fictional) kind of mutation that breaks the DNA strand in a crucial position and defies all attempts to repair it, leading to cell death. Here, the mutated and unmutated DNA sequences would be phenes themselves; it is the changed primary sequence itself which by failing would cause death, not the corresponding polypeptide.\n\nSee also Dawkin's concept of extended phenotype.\n\nThe term has been widely adopted by the academic community and appears in scientific literature. A quick keyword search of titles and abstracts containing \"phene\" at PubMed returns many articles.\nIt is a valuable concept in the genomic era where \"phenes\" or \"traits\" (symptoms) are used to distinguish groups with genetic disorders.\n\n\"Phene\" is used as to refer to relevant phenotypic traits in the OMIA (Online Mendelian Inheritance in Animals) database. One of the objectives of the OMIA is to match genotypes to phenotypes. Lenffer et al. (2006) describe the OMIA as a \"comparative biology resource\" \"(The) OMIA is a comprehensive resource of phenotypic information on heritable animal traits and genes in a strongly comparative context, relating traits to genes where possible. OMIA is modelled on and is complementary to Online Mendelian Inheritance in Man (OMIM).\" The term \"phene\" is equated with \"trait\".\n\n\n", "id": "7376681", "title": "Phene"}
{"url": "https://en.wikipedia.org/wiki?curid=38632380", "text": "GC skew\n\nGC skew is when the nucleotides Guanine and Cytosine are over- or under-abundant in a particular region of DNA or RNA. In equilibrium conditions (without mutational or selective pressure and with nucleotides randomly distributed within the genome) there is an equal frequency of the four DNA bases (Adenine, Guanine, Thymine, and Cytosine) on both single strands of a DNA molecule. However, in most prokaryotes (e.g. \"E. coli\") and some archaea (e.g. \"Sulfolobus solfataricus\"), nucleotide compositions are asymmetric between the leading strand and the lagging strand: the leading strand contains more Guanine (G) and Thymine (T), whereas the lagging strand contains more Adenine (A) and Cytosine (C). This phenomenon is referred to as GC and AT skew. It is represented mathematically as follows:\n\nGC skew = (G - C)/(G + C) \nAT skew = (A - T)/(A + T)\n\nErwin Chargaff's work in 1950 demonstrated that, in DNA, the bases guanine and cytosine were found in equal abundance, and the bases adenine and thymine were found in equal abundance. However, there was no equality between the amount of one pair versus the other. Chargaff’s finding is referred to as Chargaff's rule or parity rule 1. Three years later Watson and Crick used this fact during their derivation the structure of DNA, their double helix model.\n\nA natural result of parity rule 1, at the state of equilibrium, in which there is no mutation and/or selection biases in any of the two DNA strands, is that when there is an equal substitution rate, the complementary nucleotides on each strand have equal amounts of a given base and its complement. In other words, in each DNA strand the frequency of occurrence of T is equal to A and the frequency of occurrence of G is equal to C because the substitution rate is presumably equal. This phenomenon is referred to as parity rule 2. Hence, the second parity rule only exists, when there is no mutation or substitution.\n\nAny deviation from parity rule 2 will result in asymmetric base composition that discriminates the leading from the lagging strand. This asymmetry is referred to as GC or AT skew.\n\nThere is a richness of guanine over cytosine and thymine over adenine in the leading strand and vice versa for the lagging strand. The nucleotide composition skew spectra ranges from -1, which corresponds to G = 0 or A = 0, to +1, which corresponds to T= 0 or C = 0. Therefore, positive GC skew represents richness of G over C and the negative GC skew represents richness of C over G. As a result, one expects to see a positive GC skew and negative AT skew in the leading strand, and a negative GC skew and a positive AT skew in the lagging strand. GC or AT skew changes sign at the boundaries of the two replichores, which correspond to DNA replication origin or terminus. Originally, this asymmetric nucleotide composition was explained as a different mechanism used in DNA replication between the leading strand and lagging strand. DNA replication is semi-conservative and an asymmetric process itself. This asymmetry is due the formation of the replication fork and its division into nascent leading and lagging strands. The leading strand is synthesized continuously and in juxtapose to the leading strand; the lagging strand is replicated through short fragments of polynucleotide (Okazaki fragments) in a 5' to 3' direction.\n\nThere are three major approaches to calculate and graphically demonstrate GC skew and its properties.\n\nThe first approach is GC and AT Asymmetry. Jean R. Lobry was the first to illustrate the nucleotide composition asymmetry throughout the genome of three bacterium: \"E. coli\", \"Bacillus subtilis\", and \"haemophilus influenzae\" by using GC and AT bias. This is the most common and traditional way to quantitatively evaluate base composition asymmetry. The original formulas at the time were not named skew, but rather deviation from [A] = [T] or [C] = [G]:\n\ndeviation from [A] = [T] as (A - T)/(A + T)\n\ndeviation from [C] = [G] as (C - G)/(C + G)\n\nwhere A, T, G, and C represent the frequency of occurrence of the equivalent base in a particular sequence in a defined length. A window sliding strategy is used to calculate deviation from C through the genome. In these plots, a positive deviation from C corresponds to lagging strand and negative deviation from C corresponds to leading strand. Furthermore, the site where the deviation sign switches corresponds to origin or terminal. The x-axis represents the chromosome locations plotted 5' to 3' and y-axis represents the deviation value. The major weakness of this method is its window-size dependent property. Therefore, choosing an adequate window size greatly affects the outcome of the plot. Other techniques should be combined with deviation in order to identify and locate the origin of the DNA replication with greater accuracy.\n\nThe second approach is referred to as cumulative GC skew (CGC skew). This method still uses the sliding window strategy but it takes advantage of the sum of the adjacent windows from an arbitrary start. In this scheme, the entire genome is usually plotted 5' to 3' using an arbitrary start and arbitrary strand. In the cumulative GC skew plot, the peaks corresponds to the switch points (terminus or origin).\n\nIn contrast to Lobry's earlier paper, recent implementations of GC skew flips the original definition, redefining it to be:\n\nGC skew = (G - C)/(G + C).\n\nWith the flipped definition of GC skew, the maximum value of the cumulative skew corresponds to the terminal, and the minimum value corresponds to the origin of replication.\n\nThe final approach is the Z curve. Unlike the previous methods, this method do not uses the sliding window strategy and is thought to perform better as to finding the origin of replication. In this method each base’s cumulative frequency with respect to the base at the beginning of the sequence is investigated. Z curve uses a three-dimensional representation with the following parameters:\n\nXn = (An + Gn) – (Cn + Tn)\n\nYn = (An + Cn) – (Gn + Tn)\n\nZn = (An + Tn) – (Cn + Gn)\n\nWhere n = 0, 1, 2, …, N, Xn represents the excess of purine over pyrimidine, Yn denotes excess of keto over amino, and Zn shows the relationship between the weak and strong hydrogen bonds. X and Y components can alone detect the replication origin and asymmetric composition of the strands.\nA combination of these methods should be used for prediction of replication origin and terminal, in order to compensate for their weakness.\n\nThere is lack of consensus in scientific community with regard to the mechanism underlining the bias in nucleotide composition within each DNA strand. There are two major schools of thought that explain the mechanism behind the strand specific nucleotide composition in bacteria.\n\nThe first one describes a bias and an asymmetric mutational pressure on each DNA strand during replication and transcription. Due to the asymmetric nature of the replication process, an unequal mutational frequency and DNA repair efficiency during the replication process can introduce more mutations in one strand as compared to the other. Furthermore, the time used for replication between the two strands varies and may lead to asymmetric mutational pressure between leading and lagging strand. In addition to mutations during DNA replication, transcriptional mutations can create strand specific nucleotide composition skew. Deamination of cytosine and ultimately mutation of cytosine to thymine in one DNA strand can increase the relative number of guanine and thymine to cytosine and adenine. In most bacteria majority of the genes are encoded in the leading strand. For instance, the leading strand in \"Bacillus\" \"subtilis\" encodes 75% of the genes. In addition an excess of deamination and conversion of cytosine to thymine in the coding strand compared to the non-coding strand has been reported. One possible explanation is that the non-transcribed strand (coding strand) is single stranded during the transcription process; therefore, it is more vulnerable to deamination compared to the transcribed strand (non-coding strand). Another explanation is that the deamination repair activity during transcription does not occur on the coding strand. Only the transcribed strand benefits from these deamination repair events.\n\nThe second school of thought describes the mechanism of GC and AT skew as resulting from differential selective pressure between the leading and lagging strands. Examination of the prokaryotic genome shows a preference in third codon position for G over C and T over A. This discrimination creates an asymmetric nucleotide composition, if the coding strand is unequally distributed between the leading and lagging strands, as in the case for bacteria. In addition, the highly transcribed genes, such as ribosomal proteins, have been shown to be located mostly on the leading strand in bacteria. Therefore, a bias in the third-position codon choice of G over C can lead to GC skew. Additionally, some signal sequences are rich in guanine and thymine, such as chi sequences, and these sequences might have a higher frequency of occurrence in one strand compared to the other.\n\nBoth mutational and selective pressure can independently introduce asymmetry in DNA strands. However the combination and cumulative effect of both mechanisms is the most plausible explanation for GC and AT skew.\n\nIn the majority of prokaryotes there is a richness of G over C and T over A in the leading strand and vice versa for the lagging strand. However, it has been reported that there is positive AT skew in the leading strand in some prokaryotes, such as the phylum Firmicutes. Firmicutes demonstrate an atypical AT skew. This unique nucleotide composition is thought to be due to selection pressure of adenine over thymine in the coding region. This biased selection avoids the formation of stop codons and use of metabolically expensive amino acids. The coding regions are mostly distributed on the leading strand; therefore, A over T richness is observed in Firmicutes.\n\nThe GC skew is proven to be useful as the indicator of the DNA leading strand, lagging strand, replication origin, and replication terminal. Most prokaryotes and archaea contain only one DNA replication origin. The GC skew is positive and negative in the leading strand and in the lagging strand respectively; therefore, it is expected to see a switch in GC skew sign just at the point of DNA replication origin and terminus. GC skew can also be used to study the strand biases and mechanism related to them by calculating the excess of one base over its complementary base in different milieus. Method such as GC skew, CGC skew, and Z-curve are tools that can provide opportunity to better investigate the mechanism of DNA replication in different organisms.\n\n", "id": "38632380", "title": "GC skew"}
{"url": "https://en.wikipedia.org/wiki?curid=38429723", "text": "Genetics nursing\n\nGenetics nursing is a nursing specialty that focuses on providing genetic healthcare to patients.\n\nThe integration of genetics into nursing began in the 1980s and has been a slow but important process in improving the quality of healthcare for patients receiving genetic and genomic based care from nurses. Modeling the United Kingdom, the United States critically established a set of essential competencies as a set of guidelines for registered nurses. Through the process of consensus the essential competencies were created by the Steering Committee, and provided the minimalist competency and scope of practice for registered nurses delivering genetic healthcare to patients.\n\nThe Nursing Code of Ethics and other ethical foundations were established for field of genetics nursing to provide regulations when ethical issues develop.\n\nAdopted from the early Christians in 30 AD, the term nurse was created from the Latin origin nutrire, which means to nurture or nourish. Establishing nursing as one of the oldest forms of healthcare and continues to be a growing field of medicine. Genetics, which is the study of inherited traits and their variation is a much more recent field of medicine. The experiments and theories of Gregor Mendel in the mid-19th century helped to introduce the field of genetics into medicine. Genomics is a subset of genetics that compares and analyzes genomes and how the genes interact with one another. Both genetics and genomics help to reveal how closely related we are to each other and to other species. This scientific study is ongoing and strives to interpret health, illness, disease risk, and treatment response.The progress in genetics and genomics is applicable to the entire spectrum of health care and all health professionals and as such to the entire nursing profession. Genetics and genomics are important to healthcare because it provides information in the diagnosis, treatment, and prevention of diseases and illnesses. Even though genetics has been a growing field of medicine since the mid-19th century, the process of integrating genomics into the nursing curriculum, National Council Licensure Examinations, continuing education, and certification was not highlighted until the 1980s. Genetics and genomics are fundamental to the nursing practice because the basis of genetics can recognize individuals at risk for certain illnesses and diseases, identify the risks of certain disease or illnesses when conceiving children, facilitate drug dosage or selection for certain illnesses or specific patients, and genetics promotes benefits in treatment of particular ailments.\n\nHowever, it took twenty more years until the Health Recourses and Services Administration (HRSA) stressed the significance of incorporating genetics into nursing education. After HRSA’s proposal, there was minor advancement and the development that was established contained a lot of inconsistency. The progress of integration continued to be slow and limited. By fall of 2005, only 30% of academic nursing programs contained a curriculum thread in genetics and genomics. One of the leading factors in the limited progress of genetics integration is the relevance to all nursing practice is not fully appreciated by many, and genetics is also seen by many nurses to be a subspecialty. Also state boards of nursing do not require competency in genomics and genetics as part of licensure and genetics and genomics are not considered in the evaluations of accrediting bodies. The extremely large size and variation of the nursing workforce provides an extra challenge in the many existing barriers needed to be overcome for genetics to be implemented. Some of the first successful training of genetic practices in the nursing workforce can be seen in the United Kingdom. The main aspect of the U.K.’s strategy was simplicity. They achieved this by constructing seven essential competencies that were applied to the entire nursing profession. In 2003, the U.K. National Health Service created the NHS National Genetics Education and Development Centre. The main functions of these programs were to enhance genetics education and to dispense materials and resources for educators of all genetic professions. The United States mirrored the efforts and ideas established in the U.K. and adopted similar methods and competencies.The U.S National Human Genome Research Institute (NHGRI) and the National Cancer Institute (NCI) of the National Institutes of Health (NIGH) united to initiate strategies, training programs, committees, and define the competencies.\n\nThe genetic and genomic competencies are important to the practice of all nurses regardless of academic preparation, practice setting, role, or specialty. The competencies are significant because they establish a foundation and set of guidelines for the nursing workforce on administering the minimal amount of genetic and genomic based healthcare. Since the competencies would only reflect the minimalist amount of genetic and genomic based healthcare, they were specifically drawn up to focus on the scope of practice for registered nurses. This was done because a registered nurse is a general level of practice for nursing and requires that one has graduated from a college or university nursing program and has passed the NCLEX. The NCLEX is a national licensing exam that signifies minimal competency in practicing nursing if passed.\n\nTo begin the development of the competencies, the initial strategy of the U.S National Human Genome Research Institute (NHGRI) and the National Cancer Institute (NCI) of the National Institutes of Health (NIGH) established the Steering Committee. The Steering Committee was composed of nurse leaders from a variety of professional nursing agencies, academic settings, and organizations. Two of the major nursing leaders, Jean Jenkins, RN, PhD, FAAN and Kathleen Calzone, RN, MSN, APNG, FAAN were chosen as the Co-Chairs of the committee. The committee's fundamental function was to generate a mechanism for establishing competencies by recognizing, examining, and comparing existing published competencies. The published competencies that were being examined targeted all health care professionals, specifically those practicing genetics, nurses with bachelor's degrees, and advanced practice nurses. After the published competencies were reviewed carefully, the developing of the essential competencies was produced in four phases called the process of consensus.\n\nDuring phase I of the process of consensus, a subset of the committee was created to synthesize competencies from the documents under review that would apply to all registered nurses; then the steering committee reviewed, modified, and approved the recommended competencies. In 2005, nurse representatives of the National Coalition for Health Professional Education in Genetics (NCHPEG) also reviewed the proposed competencies and made modifications. Throughout phase II, the American Nursing Association (ANA) published the competencies during a meeting in 2006 and requested judgment, thoughts, and comments from the public, specifically targeting the insight from the nursing community. The 10 comments that were received were recorded and evaluated and the majority of them showed support. Phase III consisted of establishing consensus on the final draft of the essential competencies by the Steering Committee and the Consensus Panel, which is also made up of a variety of nursing leaders in different organizations and settings. The Steering Committee also constructed strategies for integrating genetic and genomic information into education and practice such as the NCLEX exam, accreditation programs, certification processes, and nursing curriculum. In March 2006, phase IV occurred and consisted of endorsing the final document by the Nursing Organizations Alliance member organizations.\n\nThe essential competencies consists of two domains: professional responsibilities and professional practice. Under the professional responsibilities domain, all professional activities by registered nurses are required to fall within the confines of the \"Nursing: Scope and Standards of Practice\" produced by the American Nurses Association (ANA).\n\nAlso, competent nursing practice now requires the incorporation of genetic and genomic knowledge and skills in order to: \n\nThe competencies for the registered nurse, under the professional practice domain, includes: nursing assessment, which is the application and integration of genetic and genomic knowledge, identification, referral activities, and provision of education, care, and support.\n\nNursing Assessment for the registered nurse includes:\nIdentification for the registered nurse includes: \nReferral Activities for the registered nurse includes:\nProvision of support, care, and education for the registered nurse includes:\n\nEthics pertain to the ‘’rightness’’ and ‘’wrongness’’ of human actions, motives, and conduct. Complicated ethical issues in areas such as justice, privacy, and autonomy, tend to follow both the field of genetics and the field of nursing. Ethical problems and dilemmas arise daily in healthcare settings for both the patient and health care provider. For example, all patients and individuals have the right to receive equal health care regardless of gender, religious beliefs, status, or race. A Code of Ethics for Nursing was created by the American Nurses Association (ANA), which provides rules, regulations, and guidelines to follow when making a decision that is ethical based. These regulations were mainly established to help provide equal healthcare, protect the rights, safety, and privacy of the patient, and to hold nurses accountable for their actions and choices. Genetics can create ethical issues in nursing for a variety of different situations. Many scenarios, questions, and debates have been encountered such as what individuals can receive genetic testing or information? Who owns or controls the information received from the genetic test and how can the owner use that information? However, the code of ethics does not address genetics or genomics specifically, so ethical foundations were also established to help guide genetics into health care. The foundations provide a set of guidelines to understand and manage an ethical issue if one should arise, and to assist in the translation of genetics into the healthcare environment.\n", "id": "38429723", "title": "Genetics nursing"}
{"url": "https://en.wikipedia.org/wiki?curid=14752773", "text": "Boveri–Sutton chromosome theory\n\nThe Boveri–Sutton chromosome theory (also known as the chromosome theory of inheritance or the Sutton–Boveri theory) is a fundamental unifying theory of genetics which identifies chromosomes as the carriers of genetic material. It correctly explains the mechanism underlying the laws of Mendelian inheritance by identifying chromosomes with the paired factors (particles) required by Mendel's laws. It also states that chromosomes are linear structures with genes located at specific sites called loci along them.\n\nIt states simply that chromosomes, which are seen in all dividing cells and pass from one generation to the next, are the basis for all genetic inheritance.\nOver a period of time random mutation\ncreates changes in DNA sequence of gene\n\nThe chromosome theory of inheritance is credited to papers by Walter Sutton in 1902 and 1903, as well as to independent work by Theodor Boveri during roughly the same period. Boveri was studying sea urchins, in which he found that all the chromosomes had to be present for proper embryonic development to take place. Sutton's work with grasshoppers showed that chromosomes occur in matched pairs of maternal and paternal chromosomes which separate during meiosis and \"may constitute the physical basis of the Mendelian law of heredity\".\n\nThis groundbreaking work led E.B. Wilson in his classic text to name the chromosome theory of inheritance the \"Sutton-Boveri Theory\". Wilson was close to both men, since the young Sutton was his student and the prominent Boveri was his friend (in fact, Wilson dedicated the afore-mentioned book to Boveri). Although the naming precedence is now often reversed to \"Boveri-Sutton\", there are some who argue that Boveri didn't actually articulate the theory until 1904.\n\nThe proposal that chromosomes carried the factors of Mendelian inheritance was initially controversial, but in 1913 it gained strong support when Eleanor Carothers documented definitive evidence of independent assortment of chromosomes in a species of grasshopper. Debate continued, however, until 1915 when Thomas Hunt Morgan's work on inheritance and genetic linkage in the fruit fly \"Drosophila melanogaster\" provided incontrovertible evidence for the proposal. The unifying theory stated that inheritance patterns may be generally explained by assuming that genes are located in specific sites on chromosomes.\n\n", "id": "14752773", "title": "Boveri–Sutton chromosome theory"}
{"url": "https://en.wikipedia.org/wiki?curid=38898126", "text": "Chromosome instability\n\nChromosomal instability (CIN) is a type of genomic instability in which chromosomes are unstable, such that either whole chromosomes or parts of chromosomes are duplicated or deleted. The unequal distribution of DNA to daughter cells upon mitosis results in a failure to maintain euploidy (the correct number of chromosomes) leading to aneuploidy (incorrect number of chromosomes). In other words, the daughter cells do not have the same number of chromosomes as the cell they originated from.\n\nThese changes have been studied in solid tumors, which may or may not be cancerous. CIN is a common occurrence in solid and haematological cancers, especially colorectal cancer. Although many tumours show chromosomal abnormalities, CIN is characterised by an increased rate of these errors.\n\n\nNumerical CIN is a high rate of either gain or loss of whole chromosomes; causing aneuploidy. Normal cells make errors in chromosome segregation in 1% of cell divisions, whereas cells with CIN make these errors approximately 20% of cell divisions. Because aneuploidy is a common feature in tumour cells, the presence of aneuploidy in cells does not necessarily mean CIN is present; a high rate of errors is definitive of CIN. One way of differentiating aneuploidy without CIN and CIN-induced aneuploidy is that CIN causes widely variable (heterogeneous) chromosomal aberrations; whereas when CIN is not the causal factor, chromosomal alterations are often more clonal.\n\nStructural CIN is different in that rather than whole chromosomes, fragments of chromosomes may be duplicated or deleted. The rearrangement of parts of chromosomes (translocations) and amplifications or deletions within a chromosome may also occur in structural CIN.\n\nCIN often results in aneuploidy. There are three ways that aneuploidy can occur. It can occur due to loss of a whole chromosome, gain of a whole chromosome or rearrangement of partial chromosomes known as gross chromosomal rearrangements (GCR). All of these are hallmarks of some cancers. Segmental aneuploidy can occur due to deletions, amplifications or translocations, which arise from breaks in DNA, while loss and gain of whole chromosomes is often due to errors during mitosis.\n\nChromosomes consist of the DNA sequence, and the proteins (such as histones) that are responsible for its packaging into chromosomes. Therefore, when referring to chromosome instability, epigenetic changes can also come into play. Genes on the other hand, refer only to the DNA sequence (hereditary unit) and it is not necessary that they will be expressed once epigenetic factors are taken into account. Disorders such as chromosome instability can be inherited via genes, or acquired later in life due to environmental exposure. One way that Chromosome Instability can be acquired is by exposure to ionizing radiation. Radiation is known to cause DNA damage, which can cause errors in cell replication, which may result in chromosomal instability. Chromosomal instability can in turn cause cancer. \nHowever, chromosomal instability syndromes such as Bloom syndrome, ataxia telangiectasia and Fanconi anaemia are inherited and are considered to be genetic diseases. These disorders are associated with tumor genesis, but often have a phenotype on the individuals as well. The genes that control chromosome instability are known as chromosome instability genes and they control pathways such as mitosis, DNA replication, repair and modification. They also control transcription, and process nuclear transport.\n\nThe research associated with chromosomal instability is associated with solid tumors, which are tumors that refer to a solid mass of cancer cells that grow in organ systems and can occur anywhere in the body. These tumors are opposed to liquid tumors, which occur in the blood, bone marrow, and lymph nodes.\n\nAlthough chromosome instability has long been proposed to promote tumor progression, recent studies suggest that chromosome instability can either promote or suppress tumor progression. The difference between the two are related to the amount of chromosomal instability taking place, as a small rate of chromosomal instability leads to tumor progression, or in other words cancer, while a large rate of chromosomal instability is often lethal to cancer. This is due to the fact that a large rate of chromosomal instability is detrimental to the survival mechanisms of the cell, and the cancer cell cannot replicate and dies (apoptosis). Therefore, the relationship between chromosomal instability and cancer can also be used to assist with diagnosis of malignant vs. benign tumors.\n\nA majority of human solid malignant tumors is characterized by chromosomal instability, and have gain or loss of whole chromosomes or fractions of chromosomes. For example, the majority of colorectal and other solid cancers have chromosomal instability (CIN). This shows that chromosomal instability can be responsible for the development of solid cancers. However, genetic alterations in a tumor do not necessarily indicate that the tumor is genetically unstable, as ‘genomic instability’ refers to various instability phenotypes, including the chromosome instability phenotype \n\nThe role of CIN in carcinogenesis has been heavily debated. While some argue the canonical theory of oncogene activation and tumor suppressor gene inactivation, such as Robert Weinberg, some have argued that CIN may play a major role in the origin of cancer cells, since CIN confers a mutator phenotype that enables a cell to accumulate large number of mutations at the same time. Scientists active in this debate include Christoph Lengauer, Kenneth W. Kinzler, Keith R. Loeb, Lawrence A. Loeb, Bert Vogelstein and Peter Duesberg.\n\nChromosomal instability can be diagnosed using analytical techniques at the cellular level. Often used to diagnose CIN is cytogenetics flow cytometry, Comparative genomic hybridization and Polymerase Chain Reaction. Karyotyping, and fluorescence in situ hybridization (FISH) are other techniques that can be used. In Comparative genomic hybridization, since the DNA is extracted from large cell populations it is likely that several gains and losses will be identified. \nKaryotyping is used for Fanconi Anemia, based on 73-hour whole-blood cultures, which are then stained with Giemsa. Following staining they are observed for microscopically visible chromatid-type aberrations \n\n", "id": "38898126", "title": "Chromosome instability"}
{"url": "https://en.wikipedia.org/wiki?curid=198948", "text": "Coding region\n\nThe coding region of a gene, also known as the coding sequence or CDS (from coding DNA sequence), is that portion of a gene's DNA or RNA, composed of exons, that codes for protein. The region is bounded nearer the 5' end by a start codon and nearer the 3' end with a stop codon. The coding region in mRNA is bounded by the five prime untranslated region (5'-UTR) and the three prime untranslated region (3'-UTR), which are also parts of the exons. The CDS is that portion of an mRNA transcript that is translated by a ribosome.\n\nThe coding region of an organism is the sum total of the organism's genome that is composed of gene coding regions.\n\nA cDNA sequence is derived from the transcript by reverse transcription, but in this case it also contains the 5' and 3' UTRs, which are not part of the CDS (they are transcribed, but not translated). Due to this, a CDS will -almost- always start with an AUG codon and stop at one of the three STOP codons (UAA,UGA,UAG). These stop codons are called the stop signal.\n\nWhile identification of open reading frames within a DNA sequence is straightforward, identifying coding sequences is not, because the cell translates only a subset of all open reading frames to proteins.\nCurrently CDS prediction uses sampling and sequencing of mRNA from cells, although there is still the problem of determining which parts of a given mRNA are actually translated to protein. CDS prediction is a subset of gene prediction, the latter also including prediction of DNA sequences that code not only for protein but also for other functional elements such as RNA genes and regulatory sequences.\n\n", "id": "198948", "title": "Coding region"}
{"url": "https://en.wikipedia.org/wiki?curid=39482626", "text": "Transpoviron\n\nA transpoviron is a mobile genetic element found in the genomes of giant DNA viruses.\n\nThey are linear DNA elements of approximately 7 kilobases that encompass six to eight protein encoding genes. Two of these genes are homologous to virophage genes. Transpovirons encode a superfamily 1 helicase, which encompasses an inactivated family B DNA polymerase domain. Homologs of this unique polymerase-helicase fusion protein are widespread in Polinton-Like Viruses (PLV). Based on the phylogenetic analysis of the helicase domain it has been concluded that transpovirons have evolved from PLV via the loss of several genes including those encoding the morphogenetic module proteins. \n", "id": "39482626", "title": "Transpoviron"}
{"url": "https://en.wikipedia.org/wiki?curid=37470192", "text": "Nullomers\n\nNullomers are short sequences of DNA base pairs that do not occur in the genome of a species (commonly humans), even though they are theoretically possible. Nullomers must be under a selective pressure - for example, they may be toxic to the cell. Some nullomers have been shown to be useful to treat leukemia, breast, and prostate cancer. They are not useful in healthy cells because normal cells adapt and become immune to them. Nullomers are also being developed for use as DNA tags to prevent cross contamination when analyzing crime scene material.\n\nNullomers are naturally available but potentially unused sequences of DNA. Determining these \"forbidden\" sequences can improve the understanding of the basic rules that govern sequence evolution. Sequencing the entire genome has shown that there is a high level of non-uniformity in genomic sequences. When a codon is artificially substituted for a synonymous codon, it often results in a lethal change and cell death. This is believed to be due to ribosomal stalling and early termination of protein synthesis. For example, both AGA and CGA code for arginine in bacteria; however, bacteria almost never use AGA, and when substituted it proves lethal. Such codon biases have been seen in all species, and are examples of constraints on sequence evolution. Other sequences may have selective pressure; for example, GG-rich sequences are used as sacrificial sinks for oxidative damage because oxidizing agents are attracted to regions with GG-rich sequences and then induce strand breakage.\n\nNullomers have been used as an approach to drug discovery and development. Nullomer peptides were screened for anti-cancer action. Absent sequences have short polyarginine tails added to increase solubility and uptake into the cell, producing peptides called PolyArgNulloPs. One successful sequence, RRRRRNWMWC, was demonstrated to have lethal effects in breast and prostate cancer. It damaged mitochondria by increasing ROS production, which reduced ATP production, leading to cell growth inhibition and cell death. Normal cells show a decreased sensitivity to PolyArgNulloPs over time.\n\nAccidental transfer of biological material containing DNA can produce misleading results. This is a particularly important consideration in forensic and crime labs, where mistakes can cause an innocent person to be convicted of a crime. There was no way to detect if a reference sample was mislabeled as evidence or if a forensic sample is contaminated, but a nullomer barcode can be added to reference samples to distinguish them from evidence on analysis. Tagging can be carried out during sample collection without affecting genotype or quantification results. Impregnated filter paper with various nullomers can be used to soak up and store DNA samples from a crime scene, making the technology simple and effective. Tagging with nullomers can be detected—even when diluted to a million-fold and spilled on evidence, these tags are still clearly detected. Tagging in this way supports National Research Council's recommendations on quality control to reduce fraud and mistakes.\n", "id": "37470192", "title": "Nullomers"}
{"url": "https://en.wikipedia.org/wiki?curid=39506967", "text": "Education in personalized medicine\n\nPersonalized medicine involves medical treatments based on the characteristics of individual patients, including their medical history, family history, and genetics. Although personal genetic information is becoming increasingly important in healthcare, there is a lack of sufficient education in medical genetics among physicians and the general public. For example, pharmacogenomics (genetic factors influencing drug response) is practiced worldwide by only a limited number of pharmacists, although most pharmacy colleges in the United States now include it in their curriculum. It is also increasingly common for genetic testing to be offered directly to consumers, who subsequently seek out educational materials and bring their results to their doctors. Issues involving genetic testing also invariably lead to ethical and legal concerns, such as the potential for inadvertent effects on family members, increased insurance rates, or increased psychological stress.\n\nAs of 2009, the majority of primary care physicians did not have adequate training in genetics or genomics. Although medical school curricula typically include medical genetics, fewer than half offer a standalone course, and the emphasis on practical applications is weak.\n\nIn the United States, Stanford University was the first medical school in the United States to offer a course teaching the interpretation of genetic data. Students were able to study their own genotypes, determined using commercially available genotyping platforms (23andMe or Navigenics). Although there was skepticism that this would improve educational outcomes, a survey later showed that this had increased students’ enthusiasm for the subject. A similar class is offered at Mount Sinai School of Medicine, launched in 2012, in which students have the option of analyzing their entire genome sequence instead of only their genotype.\n\nIn 2010, the University of California, Berkeley offered entering students a genetic test for SNPs affecting alcohol, lactose, and folate metabolism. The goal was “to spark discussion during orientation on how genetic testing works, the results of the students' tests and their decisions on whether or not to participate.” However, criticism of the program led to an informational hearing by the California State Committee on Higher Education, and a bill was introduced by Chris Norby to prevent California state universities from genetically testing their students. The California Department of Public Health concluded that the program constituted clinical testing, and the university released only aggregate information instead of personal results.\n\nAs described in the preceding section, in some courses on personalized medicine, students have been able to study their personal genetic information. For the Stanford course, which was designed for graduate and medical students, a review was conducted in 2009-2010 by a “joint genotyping task force” including research and clinical faculty, biomedical ethicists, genetic counselors, and legal counsel. The recommendations adopted included: making genetic testing optional (with instructors blinded to the choice of the students), strict data confidentiality (only aggregate data was made available during discussions), incorporation of lectures discussing issues related to personal genotyping, and availability of genetic counseling to students if necessary.\n\n\n", "id": "39506967", "title": "Education in personalized medicine"}
{"url": "https://en.wikipedia.org/wiki?curid=38922521", "text": "Normalized chromosome value\n\nNormalized chromosome value (NCV) is a mathematical calculation for comparing each chromosome under tested in cell free DNA (cfDNA) for detecting genetic disorder of the fetus. NCV calculation removes variation within and between sequencing runs to optimize test precision.\n", "id": "38922521", "title": "Normalized chromosome value"}
{"url": "https://en.wikipedia.org/wiki?curid=39255709", "text": "SoxC group\n\nSoxC group is group C of Sry-related HMG box proteins transcription factors. SoxC genes play an important role in determining the cell fate of neuronal mesenchymal progenitor cells in many developmental processes.\n\nIn \"Drosophila melanogaster\" (fly), \"Caenorhabditis elegans\" (worm), and other lower animals SoxC is made up of only one member, but humans, mice and most other vertebrates have three members of the SoxC group. The three are Sox4, Sox11, and Sox12. These three are extremely similar to one another, more so than other proteins, but they are all highly distinct in the way that they bind DNA and active transcription \"in vitro\" with different affinity. The three found in humans and other vertebrates are single-exon genes. The SoxC proteins have 2 domains, the first an Sry-related HMG box DNA binding domain that is located near the N-terminal and the second a trans-activation domain, known as TAD, located near the C-terminal. Although these are transcription factors, to this date there is no evidence of post-translational modifications on SoxC members, but they can work cooperatively with other proteins though, such as transcription factors Brn2 and Brn1.\n\nAll of the SoxC proteins share 67% identity and 94% similarity in the 33 residues of the C-terminal domain. All SoxC genes show 84% identity and 95% similarity to one another in the HMG box, which is just slightly more highly conserved than the C-terminal domain. The SoxC proteins only have between 45% to 67% identity to Sry and the other Sox proteins. The SoxC genes found in the \"Drosophila melanogaster\" (fly), \"Caenorhabditis elegans\" (worm), and other lower animals are closely linked to those found in humans and other vertebrates. The SoxC genes are highly conserved through vertebrate evolution and are similar enough to those in invertebrates and other lower animal species to speculate that these are based off an evolutionary necessity from before vertebrates and invertebrates separated through evolution.\n\nIf these SoxC proteins were missing during developmental stages it would cause widespread problems through the body of mice, and in many cases death. These have not been studied in humans though, because no occurrences have been linked to congential malformations of any of these proteins. In mice however, mice embryos without Sox4 die of heart defects, but mice newborns without Sox11 do live but have widespread defects. Mice without Sox12 are viable and show no outward signs of malformation. Some of the common human malformations are also seen in mice with mutated SoxC, such as cleft palate or heart outflow tract malformation. There is no evidence of the correlation between SoxC mutations in humans and these malformations, but there is speculation.\n\nSox4 facilitates differentiation of lymphocytes, osteoblasts, pancreatic beta cells and along with Sox11 promotes neural differentiation. In recent years, the belief has raised that SoxC genes may lead to tumor prognosis at elevated levels. Increased expression of Sox11 and Sox4 are seen in numerous tumors and cancers and it is possible that the tumors differ depending on the circumstance and primary transformation mechanism.\n", "id": "39255709", "title": "SoxC group"}
{"url": "https://en.wikipedia.org/wiki?curid=22249817", "text": "Genetic admixture\n\nGenetic admixture occurs when two or more previously isolated populations begin interbreeding. Admixture results in the introduction of new genetic lineages into a population. It has been known to slow local adaptation by introducing foreign, unadapted genotypes (known as gene swamping). It also prevents speciation by homogenizing populations.\n\nGenetic admixture often occurs when a geographic barrier separating populations, such as a river or isthmus, is removed or when anthropogenic activities result in movement of populations (for example invasive species).\n\nOne example of genetic admixture resulting from the introduction of an invasive species is provided by the Cuban brown anole. Several isolated populations of this species exist in the native range of Cuba. However, in the introduced range of Florida, these populations freely interbreed, forming an admixed population.\n\nAnother example of a genetic admixture involves a sudden collapse of a natural barrier leading to hybridizations between closely related rival species, such as the gray wolves and the coyotes from the northeastern to the Atlantic regions of North America as well as some parts of the southern US. While wolves and coyotes are closely related and both share a common ancestry, they do not normally interbreed due to the natural hostility between the two species that are known to view each other as competitors. However, in eastern Canada, possible human impacts and persecutions resulting with the decline of the gray wolf populations may have led to the remnants seeking potential mates in coyote population that migrated into the east. The modern day coywolves native to eastern Canada and the northeastern regions of the US are descendants from the hybrids originating in this genetic admixture.\n\nAdmixture mapping is a method of gene mapping that uses a population of mixed ancestry (an admixed population) to find the genetic loci that contribute to differences in diseases or other phenotypes found between the different ancestral populations. The method is best applied to populations with recent admixture from two populations that were previously genetically isolated for tens of thousands of years, such as African Americans (admixture of African and European populations). The method attempts to correlate the degree of ancestry near a genetic locus with the phenotype or disease of interest. Genetic markers that differ in frequency between the ancestral populations are needed across the genome.\n\nAdmixture mapping is based on the assumption that differences in disease rates or phenotypes are due in part to differences in the frequencies of disease-causing or phenotype-causing genetic variants between populations. In an admixed population, these causal variants occur more frequently on chromosomal segments inherited from one or another ancestral population. The first admixture scans were published in 2005 and since then genetic contributors to a variety of disease and trait differences have been mapped. These include hypertension, multiple sclerosis, BMI, and prostate cancer in African Americans. By 2010, high-density mapping panels had been constructed for African Americans, Latino/Hispanics, and Uyghurs.\n\n\n", "id": "22249817", "title": "Genetic admixture"}
{"url": "https://en.wikipedia.org/wiki?curid=39654121", "text": "DNA-directed RNA interference\n\nDNA-directed RNA interference (ddRNAi) is a gene-silencing technique that utilizes DNA constructs to activate an animal cell’s endogenous RNA interference (RNAi) pathways. DNA constructs are designed to express self-complementary double-stranded RNAs, typically short-hairpin RNAs (shRNA), that once processed bring about silencing of a target gene or genes. Any RNA, including endogenous mRNAs or viral RNAs, can be silenced by designing constructs to express double-stranded RNA complementary to the desired mRNA target.\n\nThis mechanism has great potential as a novel therapeutic to silence disease-causing genes. Proof-of-concept has been demonstrated across a range of disease models, including viral diseases such as HIV, hepatitis B or hepatitis C, or diseases associated with altered expression of endogenous genes such as drug-resistant lung cancer, neuropathic pain, advanced cancer and retinitis pigmentosa.\n\nAs seen in Figure 1, a ddRNAi construct encoding an shRNA is packaged into a delivery vector or reagent tailored to target specific cells. Inside the cell, the DNA is transported to the nucleus where transcription machinery continually manufactures the encoded RNAs. The shRNA molecules are then processed by endogenous systems and enter the RNAi pathway and silence the desired genes.\n\nUnlike small interfering RNA (siRNA) therapeutics that turn over within a cell and consequently only silence genes transiently, DNA constructs are continually transcribed, replenishing the cellular ‘dose’ of shRNA, thereby enabling long-term silencing of targeted genes. The ddRNAi mechanism, therefore, offers the potential for ongoing clinical benefit with reduced medical intervention.\n\nFigure 2 illustrates the most common type of ddRNAi DNA construct, which is designed to express a shRNA. This consists of a promoter sequence, driving expression of sense and antisense sequences separated by a loop sequence, followed by a transcriptional terminator. The antisense species processed from the shRNA can bind to the target RNA and specify its degradation. shRNA constructs typically encode sense and antisense sequences of 20 – 30 nucleotides. Flexibility in construct design is possible: for example, the positions of sense and antisense sequences can be reversed, and other modifications and additions can alter intracellular shRNA processing. Moreover, a variety of promoter loop and terminator sequences can be used.\n\nA particularly useful variant is a multi-cassette (Figure 2b). Designed to express two or more shRNAs, they can target multiple sequences for degradation simultaneously. This is a particularly useful strategy for targeting viruses. Natural sequence variations can render a single shRNA-target site unrecognizable preventing RNA degradation. Multi-cassette constructs that target multiple sites within the same viral RNA circumvent this issue.\n\nDelivery of ddRNAi DNA constructs is simplified by the existence of a number of clinically-approved and well-characterized gene therapy vectors developed for the purpose. Delivery is a major challenge for RNAi-based therapeutics with new modifications and reagents continually being developed to optimize target cell delivery. Two broad strategies to facilitate delivery of DNA constructs to the desired cells are available: these use either viral vectors or one of a number of classes of transfection reagents.\n\n\" In vivo \" delivery of ddRNAi constructs has been demonstrated using a range of vectors and reagents with different routes of administration (ROA).\n\nddRNAi constructs have also been successfully delivered into host cells ex vivo, and then transplanted back into the host.\n\nFor example, in a Phase I clinical trial at the City of Hope National Medical Center, California, US, four HIV-positive patients with non-Hodgkin’s lymphoma were successfully treated with autologous hematopoietic progenitor cells pre-transduced ex vivo with ddRNAi constructs using lentiviral vectors. This construct was designed to express three therapeutic RNAs, one of which was a shRNA, thereby combating HIV replication in three different ways: \n\nOngoing expression of the shRNA has been confirmed in T cells, monocytes and B cells more than one year after transplantation.\n\nNervana is an investigational ddRNAi construct that knocks down the expression of protein kinase C gamma (PKCγ) known to be associated with neuropathic pain and morphine tolerance.\n\nTwo conserved PKCγ sequences found across all key model species and humans have been identified, and both single and double DNA cassettes designed. In vitro, expression of PKCγ was silenced by 80%. When similar ddRNAi constructs were delivered intrathecally using a lentiviral vector, pain relief in a neuropathic-rat model was demonstrated.\n\nThe development of resistance to chemotherapies such as paclitaxel and cisplatin in non-small-cell lung cancer (NSCLC) is strongly associated with over expression of beta III tubulin. Investigations by the Children’s Cancer Institute Australia (University of NSW, Lowy Cancer Research Centre) demonstrated that beta III-tubulin knockdown by ddRNAi delayed tumor growth and increased chemosensitivity in mouse models.\n\nTribetarna is a triple DNA cassette expressing three shRNA molecules that each separately target beta III tubulin and strongly inhibit its expression. Studies in an orthotopic-mouse model, where the construct is delivered by a modified polyethylenimine vector, jetPEI, that targets lung tissue are in progress.\n\nThe hepatitis B virus (HBV) genome encodes its own DNA polymerase for replication. Biomics Biotechnologies has evaluated around 5000 siRNA sequences of this gene for effective knockdown; five sequences were chosen for further investigation and shown to have potent silencing activity when converted into shRNA expression cassettes. A multi-cassette construct, Hepbarna, is under preclinical development for delivery by an adeno-associated virus 8 (AAV-8) liver-targeting vector.\n\nClassified as an orphan disease, there is currently no therapy for OPMD, caused by a mutation in the poly(A) binding protein nuclear 1 (PABPN1) gene. Silencing the mutant gene using ddRNAi offers a potential therapeutic approach.\n\nBesides the ex vivo approach by the City of Hope National Medical Center discussed above, the Center for Infection and Immunity Amsterdam (CINIMA), University of Amsterdam, the Netherlands, is extensively researching the composition of multi-cassette DNA constructs to tackle HIV.\n\nAs with all gene therapies, a number of safety and toxicity issues need to be evaluated during the development of ddRNAi therapeutics:\n\nOncogene activation by viral insertion: Some gene therapy vectors integrate into the host genome, thereby acting as insertional mutagens. This was a particular issue with early retroviral vectors where insertions adjacent to oncogenes resulted in the development of lymphoid tumors. AAV vectors are considered a low risk for host-genome integration, as adeno-associated virus infection has not been associated with the induction of cancers in humans despite widespread prevalence across the general population. Moreover, extensive clinical use of AAV vectors has provided no evidence of carcinogenicity. While lentiviral vectors do integrate into the genome they do not appear to show a propensity to activate oncogene expression.\n\nImmune response to gene therapy vectors: An immunological response to an adenoviral vector resulted in the death of a patient in an early human trial. Careful monitoring of potential toxicities in preclinical testing and analyses of pre-existing antibodies to gene therapy vectors in patients minimizes such risks.\n\nInnate immune response: siRNAs have been shown to activate immune responses through interaction with Toll-like receptors leading to interferon responses. These receptors reside on the cells surface and so ddRNAi constructs – delivered directly into intracellular space – are not expected to induce this response.\n\nToxic effects due to over-expression of shRNAs: High level expression of shRNAs has been shown to be toxic. Strategies to minimize levels of shRNA expression or promote precise processing of shRNAs can overcome this problem.\n\nOff-target effects: Unintended silencing of genes that share sequence homology with expressed shRNAs can theoretically occur. Careful selection of shRNA sequences and thorough preclinical testing of constructs can circumvent this issue.\n\n", "id": "39654121", "title": "DNA-directed RNA interference"}
{"url": "https://en.wikipedia.org/wiki?curid=12266", "text": "Genetics\n\nGenetics is the study of genes, genetic variation, and heredity in living organisms. It is generally considered a field of biology, but intersects frequently with many other life sciences and is strongly linked with the study of information systems.\n\nThe father of genetics is Gregor Mendel, a late 19th-century scientist and Augustinian friar. Mendel studied \"trait inheritance\", patterns in the way traits are handed down from parents to offspring. He observed that organisms (pea plants) inherit traits by way of discrete \"units of inheritance\". This term, still used today, is a somewhat ambiguous definition of what is referred to as a gene.\n\nTrait inheritance and molecular inheritance mechanisms of genes are still primary principles of genetics in the 21st century, but modern genetics has expanded beyond inheritance to studying the function and behavior of genes. Gene structure and function, variation, and distribution are studied within the context of the cell, the organism (e.g. dominance), and within the context of a population. Genetics has given rise to a number of subfields, including epigenetics and population genetics. Organisms studied within the broad field span the domain of life, including bacteria, plants, animals, and humans.\n\nGenetic processes work in combination with an organism's environment and experiences to influence development and behavior, often referred to as nature versus nurture. The intracellular or extracellular environment of a cell or organism may switch gene transcription on or off. A classic example is two seeds of genetically identical corn, one placed in a temperate climate and one in an arid climate. While the average height of the two corn stalks may be genetically determined to be equal, the one in the arid climate only grows to half the height of the one in the temperate climate due to lack of water and nutrients in its environment.\n\nThe word \"genetics\" stems from the ancient Greek ' meaning \"genitive\"/\"generative\", which in turn derives from ' meaning \"origin\".\n\nThe observation that living things inherit traits from their parents has been used since prehistoric times to improve crop plants and animals through selective breeding. The modern science of genetics, seeking to understand this process, began with the work of the Augustinian friar Gregor Mendel in the mid-19th century.\n\nPrior to Mendel, Imre Festetics, a Hungarian noble, who lived in Kőszeg before Mendel, was the first who used the word \"genetics.\" He described several rules of genetic inheritance in his work \"The genetic law of the Nature\" (Die genetische Gesätze der Natur, 1819). His second law is the same as what Mendel published. In his third law, he developed the basic principles of mutation (he can be considered a forerunner of Hugo de Vries).\n\nOther theories of inheritance preceded Mendel's work. A popular theory during the 19th century, and implied by Charles Darwin's 1859 \"On the Origin of Species\", was blending inheritance: the idea that individuals inherit a smooth blend of traits from their parents. Mendel's work provided examples where traits were definitely not blended after hybridization, showing that traits are produced by combinations of distinct genes rather than a continuous blend. Blending of traits in the progeny is now explained by the action of multiple genes with quantitative effects. Another theory that had some support at that time was the inheritance of acquired characteristics: the belief that individuals inherit traits strengthened by their parents. This theory (commonly associated with Jean-Baptiste Lamarck) is now known to be wrong—the experiences of individuals do not affect the genes they pass to their children, although evidence in the field of epigenetics has revived some aspects of Lamarck's theory. Other theories included the pangenesis of Charles Darwin (which had both acquired and inherited aspects) and Francis Galton's reformulation of pangenesis as both particulate and inherited.\n\nModern genetics started with Mendel's studies of the nature of inheritance in plants. In his paper \"\"Versuche über Pflanzenhybriden\"\" (\"Experiments on Plant Hybridization\"), presented in 1865 to the \"Naturforschender Verein\" (Society for Research in Nature) in Brünn, Mendel traced the inheritance patterns of certain traits in pea plants and described them mathematically. Although this pattern of inheritance could only be observed for a few traits, Mendel's work suggested that heredity was particulate, not acquired, and that the inheritance patterns of many traits could be explained through simple rules and ratios.\n\nThe importance of Mendel's work did not gain wide understanding until 1900, after his death, when Hugo de Vries and other scientists rediscovered his research. William Bateson, a proponent of Mendel's work, coined the word \"genetics\" in 1905 (the adjective \"genetic\", derived from the Greek word \"genesis\"—γένεσις, \"origin\", predates the noun and was first used in a biological sense in 1860). Bateson both acted as a mentor and was aided significantly by the work of female scientists from Newnham College at Cambridge, specifically the work of Becky Saunders, Nora Darwin Barlow, and Muriel Wheldale Onslow. Bateson popularized the usage of the word \"genetics\" to describe the study of inheritance in his inaugural address to the Third International Conference on Plant Hybridization in London in 1906.\n\nAfter the rediscovery of Mendel's work, scientists tried to determine which molecules in the cell were responsible for inheritance. In 1911, Thomas Hunt Morgan argued that genes are on chromosomes, based on observations of a sex-linked white eye mutation in fruit flies. In 1913, his student Alfred Sturtevant used the phenomenon of genetic linkage to show that genes are arranged linearly on the chromosome.\n\nAlthough genes were known to exist on chromosomes, chromosomes are composed of both protein and DNA, and scientists did not know which of the two is responsible for inheritance. In 1928, Frederick Griffith discovered the phenomenon of transformation (see Griffith's experiment): dead bacteria could transfer genetic material to \"transform\" other still-living bacteria. Sixteen years later, in 1944, the Avery–MacLeod–McCarty experiment identified DNA as the molecule responsible for transformation. The role of the nucleus as the repository of genetic information in eukaryotes had been established by Hämmerling in 1943 in his work on the single celled alga \"Acetabularia\". The Hershey–Chase experiment in 1952 confirmed that DNA (rather than protein) is the genetic material of the viruses that infect bacteria, providing further evidence that DNA is the molecule responsible for inheritance.\n\nJames Watson and Francis Crick determined the structure of DNA in 1953, using the X-ray crystallography work of Rosalind Franklin and Maurice Wilkins that indicated DNA has a helical structure (i.e., shaped like a corkscrew). Their double-helix model had two strands of DNA with the nucleotides pointing inward, each matching a complementary nucleotide on the other strand to form what look like rungs on a twisted ladder. This structure showed that genetic information exists in the sequence of nucleotides on each strand of DNA. The structure also suggested a simple method for replication: if the strands are separated, new partner strands can be reconstructed for each based on the sequence of the old strand. This property is what gives DNA its semi-conservative nature where one strand of new DNA is from an original parent strand.\n\nAlthough the structure of DNA showed how inheritance works, it was still not known how DNA influences the behavior of cells. In the following years, scientists tried to understand how DNA controls the process of protein production. It was discovered that the cell uses DNA as a template to create matching messenger RNA, molecules with nucleotides very similar to DNA. The nucleotide sequence of a messenger RNA is used to create an amino acid sequence in protein; this translation between nucleotide sequences and amino acid sequences is known as the genetic code.\n\nWith the newfound molecular understanding of inheritance came an explosion of research. A notable theory arose from Tomoko Ohta in 1973 with her amendment to the neutral theory of molecular evolution through publishing the nearly neutral theory of molecular evolution. In this theory, Ohta stressed the importance of natural selection and the environment to the rate at which genetic evolution occurs. One important development was chain-termination DNA sequencing in 1977 by Frederick Sanger. This technology allows scientists to read the nucleotide sequence of a DNA molecule. In 1983, Kary Banks Mullis developed the polymerase chain reaction, providing a quick way to isolate and amplify a specific section of DNA from a mixture. The efforts of the Human Genome Project, Department of Energy, NIH, and parallel private efforts by Celera Genomics led to the sequencing of the human genome in 2003.\n\nAt its most fundamental level, inheritance in organisms occurs by passing discrete heritable units, called genes, from parents to offspring. This property was first observed by Gregor Mendel, who studied the segregation of heritable traits in pea plants. In his experiments studying the trait for flower color, Mendel observed that the flowers of each pea plant were either purple or white—but never an intermediate between the two colors. These different, discrete versions of the same gene are called alleles.\n\nIn the case of the pea, which is a diploid species, each individual plant has two copies of each gene, one copy inherited from each parent. Many species, including humans, have this pattern of inheritance. Diploid organisms with two copies of the same allele of a given gene are called homozygous at that gene locus, while organisms with two different alleles of a given gene are called heterozygous.\n\nThe set of alleles for a given organism is called its genotype, while the observable traits of the organism are called its phenotype. When organisms are heterozygous at a gene, often one allele is called dominant as its qualities dominate the phenotype of the organism, while the other allele is called recessive as its qualities recede and are not observed. Some alleles do not have complete dominance and instead have incomplete dominance by expressing an intermediate phenotype, or codominance by expressing both alleles at once.\n\nWhen a pair of organisms reproduce sexually, their offspring randomly inherit one of the two alleles from each parent. These observations of discrete inheritance and the segregation of alleles are collectively known as Mendel's first law or the Law of Segregation.\n\nGeneticists use diagrams and symbols to describe inheritance. A gene is represented by one or a few letters. Often a \"+\" symbol is used to mark the usual, non-mutant allele for a gene.\n\nIn fertilization and breeding experiments (and especially when discussing Mendel's laws) the parents are referred to as the \"P\" generation and the offspring as the \"F1\" (first filial) generation. When the F1 offspring mate with each other, the offspring are called the \"F2\" (second filial) generation. One of the common diagrams used to predict the result of cross-breeding is the Punnett square.\n\nWhen studying human genetic diseases, geneticists often use pedigree charts to represent the inheritance of traits. These charts map the inheritance of a trait in a family tree.\n\nOrganisms have thousands of genes, and in sexually reproducing organisms these genes generally assort independently of each other. This means that the inheritance of an allele for yellow or green pea color is unrelated to the inheritance of alleles for white or purple flowers. This phenomenon, known as \"Mendel's second law\" or the \"law of independent assortment,\" means that the alleles of different genes get shuffled between parents to form offspring with many different combinations. (Some genes do not assort independently, demonstrating genetic linkage, a topic discussed later in this article.)\n\nOften different genes can interact in a way that influences the same trait. In the Blue-eyed Mary (\"Omphalodes verna\"), for example, there exists a gene with alleles that determine the color of flowers: blue or magenta. Another gene, however, controls whether the flowers have color at all or are white. When a plant has two copies of this white allele, its flowers are white—regardless of whether the first gene has blue or magenta alleles. This interaction between genes is called epistasis, with the second gene epistatic to the first.\n\nMany traits are not discrete features (e.g. purple or white flowers) but are instead continuous features (e.g. human height and skin color). These complex traits are products of many genes. The influence of these genes is mediated, to varying degrees, by the environment an organism has experienced. The degree to which an organism's genes contribute to a complex trait is called heritability. Measurement of the heritability of a trait is relative—in a more variable environment, the environment has a bigger influence on the total variation of the trait. For example, human height is a trait with complex causes. It has a heritability of 89% in the United States. In Nigeria, however, where people experience a more variable access to good nutrition and health care, height has a heritability of only 62%.\n\nThe molecular basis for genes is deoxyribonucleic acid (DNA). DNA is composed of a chain of nucleotides, of which there are four types: adenine (A), cytosine (C), guanine (G), and thymine (T). Genetic information exists in the sequence of these nucleotides, and genes exist as stretches of sequence along the DNA chain. Viruses are the only exception to this rule—sometimes viruses use the very similar molecule RNA instead of DNA as their genetic material. Viruses cannot reproduce without a host and are unaffected by many genetic processes, so tend not to be considered living organisms.\n\nDNA normally exists as a double-stranded molecule, coiled into the shape of a double helix. Each nucleotide in DNA preferentially pairs with its partner nucleotide on the opposite strand: A pairs with T, and C pairs with G. Thus, in its two-stranded form, each strand effectively contains all necessary information, redundant with its partner strand. This structure of DNA is the physical basis for inheritance: DNA replication duplicates the genetic information by splitting the strands and using each strand as a template for synthesis of a new partner strand.\n\nGenes are arranged linearly along long chains of DNA base-pair sequences. In bacteria, each cell usually contains a single circular genophore, while eukaryotic organisms (such as plants and animals) have their DNA arranged in multiple linear chromosomes. These DNA strands are often extremely long; the largest human chromosome, for example, is about 247 million base pairs in length. The DNA of a chromosome is associated with structural proteins that organize, compact, and control access to the DNA, forming a material called chromatin; in eukaryotes, chromatin is usually composed of nucleosomes, segments of DNA wound around cores of histone proteins. The full set of hereditary material in an organism (usually the combined DNA sequences of all chromosomes) is called the genome.\n\nWhile haploid organisms have only one copy of each chromosome, most animals and many plants are diploid, containing two of each chromosome and thus two copies of every gene. The two alleles for a gene are located on identical loci of the two homologous chromosomes, each allele inherited from a different parent.\n\nMany species have so-called sex chromosomes that determine the gender of each organism. In humans and many other animals, the Y chromosome contains the gene that triggers the development of the specifically male characteristics. In evolution, this chromosome has lost most of its content and also most of its genes, while the X chromosome is similar to the other chromosomes and contains many genes. The X and Y chromosomes form a strongly heterogeneous pair.\n\nWhen cells divide, their full genome is copied and each daughter cell inherits one copy. This process, called mitosis, is the simplest form of reproduction and is the basis for asexual reproduction. Asexual reproduction can also occur in multicellular organisms, producing offspring that inherit their genome from a single parent. Offspring that are genetically identical to their parents are called clones.\n\nEukaryotic organisms often use sexual reproduction to generate offspring that contain a mixture of genetic material inherited from two different parents. The process of sexual reproduction alternates between forms that contain single copies of the genome (haploid) and double copies (diploid). Haploid cells fuse and combine genetic material to create a diploid cell with paired chromosomes. Diploid organisms form haploids by dividing, without replicating their DNA, to create daughter cells that randomly inherit one of each pair of chromosomes. Most animals and many plants are diploid for most of their lifespan, with the haploid form reduced to single cell gametes such as sperm or eggs.\n\nAlthough they do not use the haploid/diploid method of sexual reproduction, bacteria have many methods of acquiring new genetic information. Some bacteria can undergo conjugation, transferring a small circular piece of DNA to another bacterium. Bacteria can also take up raw DNA fragments found in the environment and integrate them into their genomes, a phenomenon known as transformation. These processes result in horizontal gene transfer, transmitting fragments of genetic information between organisms that would be otherwise unrelated.\n\nThe diploid nature of chromosomes allows for genes on different chromosomes to assort independently or be separated from their homologous pair during sexual reproduction wherein haploid gametes are formed. In this way new combinations of genes can occur in the offspring of a mating pair. Genes on the same chromosome would theoretically never recombine. However, they do, via the cellular process of chromosomal crossover. During crossover, chromosomes exchange stretches of DNA, effectively shuffling the gene alleles between the chromosomes. This process of chromosomal crossover generally occurs during meiosis, a series of cell divisions that creates haploid cells.\n\nThe first cytological demonstration of crossing over was performed by Harriet Creighton and Barbara McClintock in 1931. Their research and experiments on corn provided cytological evidence for the genetic theory that linked genes on paired chromosomes do in fact exchange places from one homolog to the other.\n\nThe probability of chromosomal crossover occurring between two given points on the chromosome is related to the distance between the points. For an arbitrarily long distance, the probability of crossover is high enough that the inheritance of the genes is effectively uncorrelated. For genes that are closer together, however, the lower probability of crossover means that the genes demonstrate genetic linkage; alleles for the two genes tend to be inherited together. The amounts of linkage between a series of genes can be combined to form a linear linkage map that roughly describes the arrangement of the genes along the chromosome.\n\nGenes generally express their functional effect through the production of proteins, which are complex molecules responsible for most functions in the cell. Proteins are made up of one or more polypeptide chains, each of which is composed of a sequence of amino acids, and the DNA sequence of a gene (through an RNA intermediate) is used to produce a specific amino acid sequence. This process begins with the production of an RNA molecule with a sequence matching the gene's DNA sequence, a process called transcription.\n\nThis messenger RNA molecule is then used to produce a corresponding amino acid sequence through a process called translation. Each group of three nucleotides in the sequence, called a codon, corresponds either to one of the twenty possible amino acids in a protein or an instruction to end the amino acid sequence; this correspondence is called the genetic code. The flow of information is unidirectional: information is transferred from nucleotide sequences into the amino acid sequence of proteins, but it never transfers from protein back into the sequence of DNA—a phenomenon Francis Crick called the central dogma of molecular biology.\n\nThe specific sequence of amino acids results in a unique three-dimensional structure for that protein, and the three-dimensional structures of proteins are related to their functions. Some are simple structural molecules, like the fibers formed by the protein collagen. Proteins can bind to other proteins and simple molecules, sometimes acting as enzymes by facilitating chemical reactions within the bound molecules (without changing the structure of the protein itself). Protein structure is dynamic; the protein hemoglobin bends into slightly different forms as it facilitates the capture, transport, and release of oxygen molecules within mammalian blood.\n\nA single nucleotide difference within DNA can cause a change in the amino acid sequence of a protein. Because protein structures are the result of their amino acid sequences, some changes can dramatically change the properties of a protein by destabilizing the structure or changing the surface of the protein in a way that changes its interaction with other proteins and molecules. For example, sickle-cell anemia is a human genetic disease that results from a single base difference within the coding region for the β-globin section of hemoglobin, causing a single amino acid change that changes hemoglobin's physical properties. Sickle-cell versions of hemoglobin stick to themselves, stacking to form fibers that distort the shape of red blood cells carrying the protein. These sickle-shaped cells no longer flow smoothly through blood vessels, having a tendency to clog or degrade, causing the medical problems associated with this disease.\n\nSome DNA sequences are transcribed into RNA but are not translated into protein products—such RNA molecules are called non-coding RNA. In some cases, these products fold into structures which are involved in critical cell functions (e.g. ribosomal RNA and transfer RNA). RNA can also have regulatory effects through hybridization interactions with other RNA molecules (e.g. microRNA).\n\nAlthough genes contain all the information an organism uses to function, the environment plays an important role in determining the ultimate phenotypes an organism displays. The phrase \"nature and nurture\" refers to this complementary relationship. The phenotype of an organism depends on the interaction of genes and the environment. An interesting example is the coat coloration of the Siamese cat. In this case, the body temperature of the cat plays the role of the environment. The cat's genes code for dark hair, thus the hair-producing cells in the cat make cellular proteins resulting in dark hair. But these dark hair-producing proteins are sensitive to temperature (i.e. have a mutation causing temperature-sensitivity) and denature in higher-temperature environments, failing to produce dark-hair pigment in areas where the cat has a higher body temperature. In a low-temperature environment, however, the protein's structure is stable and produces dark-hair pigment normally. The protein remains functional in areas of skin that are coldersuch as its legs, ears, tail and faceso the cat has dark-hair at its extremities.\n\nEnvironment plays a major role in effects of the human genetic disease phenylketonuria. The mutation that causes phenylketonuria disrupts the ability of the body to break down the amino acid phenylalanine, causing a toxic build-up of an intermediate molecule that, in turn, causes severe symptoms of progressive intellectual disability and seizures. However, if someone with the phenylketonuria mutation follows a strict diet that avoids this amino acid, they remain normal and healthy.\n\nA common method for determining how genes and environment (\"nature and nurture\") contribute to a phenotype involves studying identical and fraternal twins, or other siblings of multiple births. Because identical siblings come from the same zygote, they are genetically the same. Fraternal twins are as genetically different from one another as normal siblings. By comparing how often a certain disorder occurs in a pair of identical twins to how often it occurs in a pair of fraternal twins, scientists can determine whether that disorder is caused by genetic or postnatal environmental factors – whether it has \"nature\" or \"nurture\" causes. One famous example involved the study of the Genain quadruplets, who were identical quadruplets all diagnosed with schizophrenia.\nHowever such tests cannot separate genetic factors from environmental factors affecting fetal development.\n\nThe genome of a given organism contains thousands of genes, but not all these genes need to be active at any given moment. A gene is expressed when it is being transcribed into mRNA and there exist many cellular methods of controlling the expression of genes such that proteins are produced only when needed by the cell. Transcription factors are regulatory proteins that bind to DNA, either promoting or inhibiting the transcription of a gene. Within the genome of \"Escherichia coli\" bacteria, for example, there exists a series of genes necessary for the synthesis of the amino acid tryptophan. However, when tryptophan is already available to the cell, these genes for tryptophan synthesis are no longer needed. The presence of tryptophan directly affects the activity of the genes—tryptophan molecules bind to the tryptophan repressor (a transcription factor), changing the repressor's structure such that the repressor binds to the genes. The tryptophan repressor blocks the transcription and expression of the genes, thereby creating negative feedback regulation of the tryptophan synthesis process.\n\nDifferences in gene expression are especially clear within multicellular organisms, where cells all contain the same genome but have very different structures and behaviors due to the expression of different sets of genes. All the cells in a multicellular organism derive from a single cell, differentiating into variant cell types in response to external and intercellular signals and gradually establishing different patterns of gene expression to create different behaviors. As no single gene is responsible for the development of structures within multicellular organisms, these patterns arise from the complex interactions between many cells.\n\nWithin eukaryotes, there exist structural features of chromatin that influence the transcription of genes, often in the form of modifications to DNA and chromatin that are stably inherited by daughter cells. These features are called \"epigenetic\" because they exist \"on top\" of the DNA sequence and retain inheritance from one cell generation to the next. Because of epigenetic features, different cell types grown within the same medium can retain very different properties. Although epigenetic features are generally dynamic over the course of development, some, like the phenomenon of paramutation, have multigenerational inheritance and exist as rare exceptions to the general rule of DNA as the basis for inheritance.\n\nDuring the process of DNA replication, errors occasionally occur in the polymerization of the second strand. These errors, called mutations, can affect the phenotype of an organism, especially if they occur within the protein coding sequence of a gene. Error rates are usually very low—1 error in every 10–100 million bases—due to the \"proofreading\" ability of DNA polymerases. Processes that increase the rate of changes in DNA are called mutagenic: mutagenic chemicals promote errors in DNA replication, often by interfering with the structure of base-pairing, while UV radiation induces mutations by causing damage to the DNA structure. Chemical damage to DNA occurs naturally as well and cells use DNA repair mechanisms to repair mismatches and breaks. The repair does not, however, always restore the original sequence.\n\nIn organisms that use chromosomal crossover to exchange DNA and recombine genes, errors in alignment during meiosis can also cause mutations. Errors in crossover are especially likely when similar sequences cause partner chromosomes to adopt a mistaken alignment; this makes some regions in genomes more prone to mutating in this way. These errors create large structural changes in DNA sequenceduplications, inversions, deletions of entire regionsor the accidental exchange of whole parts of sequences between different chromosomes (chromosomal translocation).\n\nMutations alter an organism's genotype and occasionally this causes different phenotypes to appear. Most mutations have little effect on an organism's phenotype, health, or reproductive fitness. Mutations that do have an effect are usually detrimental, but occasionally some can be beneficial. Studies in the fly \"Drosophila melanogaster\" suggest that if a mutation changes a protein produced by a gene, about 70 percent of these mutations will be harmful with the remainder being either neutral or weakly beneficial.\n\nPopulation genetics studies the distribution of genetic differences within populations and how these distributions change over time. Changes in the frequency of an allele in a population are mainly influenced by natural selection, where a given allele provides a selective or reproductive advantage to the organism, as well as other factors such as mutation, genetic drift, genetic draft, artificial selection and migration.\n\nOver many generations, the genomes of organisms can change significantly, resulting in evolution. In the process called adaptation, selection for beneficial mutations can cause a species to evolve into forms better able to survive in their environment. New species are formed through the process of speciation, often caused by geographical separations that prevent populations from exchanging genes with each other.\n\nBy comparing the homology between different species' genomes, it is possible to calculate the evolutionary distance between them and when they may have diverged. Genetic comparisons are generally considered a more accurate method of characterizing the relatedness between species than the comparison of phenotypic characteristics. The evolutionary distances between species can be used to form evolutionary trees; these trees represent the common descent and divergence of species over time, although they do not show the transfer of genetic material between unrelated species (known as horizontal gene transfer and most common in bacteria).\n\nAlthough geneticists originally studied inheritance in a wide range of organisms, researchers began to specialize in studying the genetics of a particular subset of organisms. The fact that significant research already existed for a given organism would encourage new researchers to choose it for further study, and so eventually a few model organisms became the basis for most genetics research. Common research topics in model organism genetics include the study of gene regulation and the involvement of genes in development and cancer.\n\nOrganisms were chosen, in part, for convenience—short generation times and easy genetic manipulation made some organisms popular genetics research tools. Widely used model organisms include the gut bacterium \"Escherichia coli\", the plant \"Arabidopsis thaliana\", baker's yeast (\"Saccharomyces cerevisiae\"), the nematode \"Caenorhabditis elegans\", the common fruit fly (\"Drosophila melanogaster\"), and the common house mouse (\"Mus musculus\").\n\nMedical genetics seeks to understand how genetic variation relates to human health and disease. When searching for an unknown gene that may be involved in a disease, researchers commonly use genetic linkage and genetic pedigree charts to find the location on the genome associated with the disease. At the population level, researchers take advantage of Mendelian randomization to look for locations in the genome that are associated with diseases, a method especially useful for multigenic traits not clearly defined by a single gene. Once a candidate gene is found, further research is often done on the corresponding (or homologous) genes of model organisms. In addition to studying genetic diseases, the increased availability of genotyping methods has led to the field of pharmacogenetics: the study of how genotype can affect drug responses.\n\nIndividuals differ in their inherited tendency to develop cancer, and cancer is a genetic disease. The process of cancer development in the body is a combination of events. Mutations occasionally occur within cells in the body as they divide. Although these mutations will not be inherited by any offspring, they can affect the behavior of cells, sometimes causing them to grow and divide more frequently. There are biological mechanisms that attempt to stop this process; signals are given to inappropriately dividing cells that should trigger cell death, but sometimes additional mutations occur that cause cells to ignore these messages. An internal process of natural selection occurs within the body and eventually mutations accumulate within cells to promote their own growth, creating a cancerous tumor that grows and invades various tissues of the body.\n\nNormally, a cell divides only in response to signals called growth factors and stops growing once in contact with surrounding cells and in response to growth-inhibitory signals. It usually then divides a limited number of times and dies, staying within the epithelium where it is unable to migrate to other organs. To become a cancer cell, a cell has to accumulate mutations in a number of genes (three to seven) that allow it to bypass this regulation: it no longer needs growth factors to divide, continues growing when making contact to neighbor cells, ignores inhibitory signals, keeps growing indefinitely and is immortal, escapes from the epithelium and ultimately may be able to escape from the primary tumor, cross the endothelium of a blood vessel, be transported by the bloodstream and colonize a new organ, forming deadly metastasis. Although there are some genetic predispositions in a small fraction of cancers, the major fraction is due to a set of new genetic mutations that originally appear and accumulate in one or a small number of cells that will divide to form the tumor and are not transmitted to the progeny (somatic mutations). The most frequent mutations are a loss of function of p53 protein, a tumor suppressor, or in the p53 pathway, and gain of function mutations in the Ras proteins, or in other oncogenes.\n\nDNA can be manipulated in the laboratory. Restriction enzymes are commonly used enzymes that cut DNA at specific sequences, producing predictable fragments of DNA. DNA fragments can be visualized through use of gel electrophoresis, which separates fragments according to their length.\n\nThe use of ligation enzymes allows DNA fragments to be connected. By binding (\"ligating\") fragments of DNA together from different sources, researchers can create recombinant DNA, the DNA often associated with genetically modified organisms. Recombinant DNA is commonly used in the context of plasmids: short circular DNA molecules with a few genes on them. In the process known as molecular cloning, researchers can amplify the DNA fragments by inserting plasmids into bacteria and then culturing them on plates of agar (to isolate clones of bacteria cells – \"cloning\" can also refer to the various means of creating cloned (\"clonal\") organisms).\n\nDNA can also be amplified using a procedure called the polymerase chain reaction (PCR). By using specific short sequences of DNA, PCR can isolate and exponentially amplify a targeted region of DNA. Because it can amplify from extremely small amounts of DNA, PCR is also often used to detect the presence of specific DNA sequences.\n\nDNA sequencing, one of the most fundamental technologies developed to study genetics, allows researchers to determine the sequence of nucleotides in DNA fragments. The technique of chain-termination sequencing, developed in 1977 by a team led by Frederick Sanger, is still routinely used to sequence DNA fragments. Using this technology, researchers have been able to study the molecular sequences associated with many human diseases.\n\nAs sequencing has become less expensive, researchers have sequenced the genomes of many organisms using a process called genome assembly, which utilizes computational tools to stitch together sequences from many different fragments. These technologies were used to sequence the human genome in the Human Genome Project completed in 2003. New high-throughput sequencing technologies are dramatically lowering the cost of DNA sequencing, with many researchers hoping to bring the cost of resequencing a human genome down to a thousand dollars.\n\nNext-generation sequencing (or high-throughput sequencing) came about due to the ever-increasing demand for low-cost sequencing. These sequencing technologies allow the production of potentially millions of sequences concurrently. The large amount of sequence data available has created the field of genomics, research that uses computational tools to search for and analyze patterns in the full genomes of organisms. Genomics can also be considered a subfield of bioinformatics, which uses computational approaches to analyze large sets of biological data. A common problem to these fields of research is how to manage and share data that deals with human subject and personally identifiable information. See also genomics data sharing.\n\nOn 19 March 2015, a group of leading biologists urged a worldwide ban on clinical use of methods, particularly the use of CRISPR and zinc finger, to edit the human genome in a way that can be inherited. In April 2015, Chinese researchers reported results of basic research to edit the DNA of non-viable human embryos using CRISPR.\n\n", "id": "12266", "title": "Genetics"}
{"url": "https://en.wikipedia.org/wiki?curid=39555516", "text": "TwinsUK\n\nTwinsUK, also known as TwinsUK Registry, is the biggest UK adult registry of twins in the United Kingdom, ages 16 to 98 to study the genetic and environmental aetiology of age related complex traits and diseases. Established in 1993, it is based at King's College London with an intent of aiding genetic research. The registry is used to connect researchers to volunteers.\n\nIt is the largest registry of twin adults in the UK. As of 2013, it hosted information on more than 13,000 volunteer twins, with approximately 50% having completed a baseline comprehensive assessment and more than 70% having completed a detailed health questionnaire. As of 2013, the cohort was predominately female (83%) and middle-aged or older, with about an equal division of monozygotic and dizygotic twins.\n\n", "id": "39555516", "title": "TwinsUK"}
{"url": "https://en.wikipedia.org/wiki?curid=39848784", "text": "German Resource Center for Genome Research\n\nThe German Resource Center for Genome Research (RZPD, Resourcenzentrum Primärdatenbank) was a service center for gen and genome research in Berlin-Charlottenburg and Heidelberg.\n\nIn 1987 Hans Lehrach and Günther Zehetner founded the \"Reference Library / Primary Database (RLDB)\" at the Imperial Cancer Research Fund (ICRF) in London. The RLDB distributed macroarrays generated at the ICRF to research groups worldwide. The macroarrays consisted of clones of genomic and cDNA libraries transferred to 22 × 22 cm nylon filters with the help of robots which have been developed by Zehetner at the ICRF. Experimental results done with the standardized materials were reported back by researches and collected in the \"Primary Database\".\n\nIn early 1995 the RLDB was moved to Berlin to the Max Planck Institute for Molecular Genetics, where it continued its work.\n\nIn the summer of 1995, the Resource Centre was established as part of the German Human Genome Project (DHGP) by Hans Lehrach (Max Planck Institute for Molecular Genetics (MPIMG), Berlin-Dahlem), Günther Zehetner (MPIMG, until July 2000 Scientific Director of the Berlin Resource Center) and Annemarie Poustka (German Cancer Research Center (DKFZ), Heidelberg). As \"Resource Center/Primary Database\" (RZPD) it took over the work of the RLDB. It was funded by the Federal Ministry of Education and Research (BMBF) in order to provide the materials free of charge to the members of the DHGP, while they were available to other researchers at cost prize. The RZPD became the largest database for genetic clones worldwide and was associated with many of the major national and international gene and genome research projects.\n\nThe RZPD was one of only two distributors in Europe for IMAGE cDNA clones and also constructed its own plasmid, cosmid, YAC and cDNA libraries. Each clone was stored in a well of a 96 or 384 well microtiter plate, which were kept, while not used for spotting, in deep freezers at -80 °C.\n\nOn 1 July 2000 the RZPD was converted into a Non-Profit-Gesellschaft mit beschränkter Haftung (gGmbH). This company with limited liability was supported by three of the most important research companies in Germany: the Max Planck Society, the German Cancer Research Center and the Max Delbrück Center for Molecular Medicine (MDC Berlin-Buch). As scientific company members served Hans Lehrach (MPIMG), Annemarie Poustka (DKFZ), Jens Reich (MDC) Matthias Uhlen (Karolinska Institute, Stockholm) and Harald zur Hausen (DKFZ).\n\nOn 31 July 2007 the non-profit company was disbanded. A large part of the existing services was taken over by two new companies. \"ImaGenes\" (a start-up company by the former managing directors of the RZPD) and \"ATLAS Biolabs GmbH\", which was co-founded by Lehrach with the former department heads of the RZPD, Uwe Radelof und Bernd Drescher.\n\nThe scope of services was constantly expanded and included the provision of genomic cell clones and cDNA databases, cell cultures, microarrays, expression profiles, Affymetrix services and PCR offers. Data collected in the \"Primary Database\" were made available to compare experimental results by researches using the globally standardized material provided by the center.\n\nThe RLDB office in London was located in the main ICRF building at Lincoln's Inn Fields, Holborn and later moved to bigger premises in the adjacent Royal College of Surgeons building.\n\nIn Berlin the RZPD was first located at the Max Planck Institute for Molecular Genetics in Berlin-Dahlem and then moved to a nearby site at the Harnack road (which today is the site of the Max Planck Institute for the History of Science) before the center relocated to its final location at the former building of the \"Empress Auguste Viktoria Infant Home\" in Heubnerweg in Berlin-Charlottenburg. The Neo-baroque building was constructed in the years 1907–1909 under the leadership of architect and Berlin City Commissioner for City Planning Ludwig Hoffmann established, while the planning was significantly influenced by the architect Alfred Messel. The small Heidelberg component of the center was located within the DKFZ.\n", "id": "39848784", "title": "German Resource Center for Genome Research"}
{"url": "https://en.wikipedia.org/wiki?curid=40160407", "text": "Essential gene\n\nEssential genes are those genes of an organism that are thought to be critical for its survival. However, being \"essential\" is highly dependent on the circumstances in which an organism lives. For instance, a gene required to digest starch is only essential if starch is the only source of energy. Recently, systematic attempts have been made to identify those genes that are absolutely required to maintain life, provided that all nutrients are available. Such experiments have led to the conclusion that the absolutely required number of genes for bacteria is on the order of about 250-300. These essential genes encode proteins to maintain a central metabolism, replicate DNA, translate genes into proteins, maintain a basic cellular structure, and mediate transport processes into and out of the cell. Most genes are not essential but convey selective advantages and increased fitness.\n\nTwo main strategies have been employed to identify essential genes on a genome-wide basis: directed deletion of genes and random mutagenesis using transposons. In the first case, individual genes (or ORFs) are completely deleted from the genome in a systematic way. In transposon-mediated mutagenesis transposons are randomly inserted in as many positions in a genome as possible, aiming to inactivate the targeted genes (see figure below). Insertion mutants that are still able to survive or grow are not in essential genes. A summary of such screens is shown in the table.\n\nTable 1. Essential genes in bacteria. Mutagenesis: \"targeted\" mutants are gene deletions; \"random\" mutants are transposon insertions. Methods: \"Clones\" indicate single gene deletions, \"population\" indicates whole population mutagenesis, e.g. using transposons. Essential genes from population screens include genes essential for fitness (see text). ORFs: number of all open reading frames in that genome. Notes: (a) mutant collection available; (b) direct essentiality screening method (e.g. via antisense RNA) that does not provide information about nonessential genes. (c) Only partial dataset available. (d) Includes predicted gene essentiality and data compilation from published single-gene essentiality studies. (e) Project in progress. (f) Deduced by comparison of the two gene essentiality datasets obtained independently in the \"P. aeruginosa \"strains PA14 and PAO1. (g) The original result of 271 essential genes has been corrected to 261, with 31 genes that were thought to be essential being in fact non-essential whereas 20 novel essential genes have been described since then. (h) Counting genes with essential domains and those that lead to growth-defects when disrupted as essential, and those who lead to growth-advantage when disrupted as non-essential. (i) Involved a fully saturated mutant library of 14 replicates, with 84.3% of possible insertion sites with at least one transposon insertion.\n\nIn \"Saccharomyces cerevisiae\" (budding yeast) 15-20% of all genes are essential. In \"Schizosaccharomyces pombe\" (fission yeast) 4,836 heterozygous deletions covering 98.4% of the 4,914 protein coding open reading frames have been constructed. 1,260 of these deletions turned out to be essential.\n\nSimilar screens are more difficult to carry out in other multicellular organisms, including mammals (as a model for humans), due to technical reasons, and their results are less clear. However, various methods have been developed for the nematode worm \"C. elegans\", the fruit fly, and zebrafish (see table). A recent study of 900 mouse genes concluded that 42% of them were essential although the selected genes were not representative.\n\nGene knockout experiments are not possible or at least not ethical in humans. However, natural mutations have led to the identification of mutations that lead to early embryonic or later death. Note that many genes in humans are not absolutely essential for survival but can cause severe disease when mutated. Such mutations are catalogued in the Online Mendelian Inheritance in Man (OMIM) database. In a computational analysis of genetic variation and mutations in 2,472 human orthologs of known essential genes in the mouse, Georgi et al. found strong, purifying selection and comparatively reduced levels of sequence variation, indicating that these human genes are essential too.\n\nWhile it may be difficult to prove that a gene is essential in humans, it can be demonstrated that a gene is not essential or not even causing disease. For instance, sequencing the genomes of 2,636 Icelandic citizens and the genotyping of 101,584 additional subjects found 8,041 individuals who had 1 gene completely knocked out (i.e. these people were homozygous for a non-functional gene). Of the 8,041 individuals with complete knock-outs, 6,885 were estimated to be homozygotes, 1,249 were estimated to be compound heterozygotes (i.e. they had both alleles of a gene knocked out but the two alleles had different mutations). In these individuals, a total of 1,171 of the 19,135 human (RefSeq) genes (6.1%) were completely knocked out. It was concluded that these 1,171 genes are \"non-essential\" in humans — at least no associated diseases were reported. Similarly, the exome sequences of 3222 British Pakistani-heritage adults with high parental relatedness revealed 1111 rare-variant homozygous genotypes with predicted loss of gene function (LOF = knockouts) in 781 genes. This study found an average of 140 predicted LOF genotypes (per subject), including 16 rare (minor allele frequency <1%) heterozygotes, 0.34 rare homozygotes, 83.2 common heterozygotes and 40.6 common homozygotes. Nearly all rare homozygous LOF genotypes were found within autozygous segments (94.9%). Even though most of these individuals had no obvious health issue arising from their defective genes, it is possible that minor health issues may be found upon more detailed examination.\n\nA summary of essentiality screens is shown in the table below (mostly based on the Database of Essential Genes.\nScreens for essential genes have been carried out in a few viruses. For instance, human cytomegalovirus (CMV) was found to have 41 essential, 88 nonessential, and 27 augmenting ORFs (150 total ORFs). Most essential and augmenting genes are located in the central region, and nonessential genes generally cluster near the ends of the viral genome.\n\nTscharke and Dobson (2015) compiled a comprehensive survey of essential genes in Vaccinia Virus and assigned roles to each of the 223 ORFs of the Western Reserve (WR) strain and 207 ORFs of the Copenhagen strain, assessing their role in replication in cell culture. According to their definition, a gene is considered essential (i.e. has a role in cell culture) if its deletion results in a decrease in virus titre of greater than 10-fold in either a single or multiple step growth curve. All genes involved in wrapped virion production, actin tail formation, and extracellular virion release were also considered as essential. Genes that influence plaque size, but not replication were defined as non-essential. By this definition 93 genes are required for Vaccinia Virus replication in cell culture, while 108 and 94 ORFs, from WR and Copenhagen respectively, are non-essential. Vaccinia viruses with deletions at either end of the genome behaved as expected, exhibiting only mild or host range defects. In contrast, combining deletions at both ends of the genome for VACV strain WR caused a devastating growth defect on all cell lines tested. This demonstrates that single gene deletions are not sufficient to assess the essentiality of genes and that more genes are essential in Vaccinia virus than originally thought.\n\nOne of the bacteriophages screened for essential genes includes mycobacteriophage Giles. At least 35 of the 78 predicted Giles genes (45%) are non-essential for lytic growth. 20 genes were found to be essential. A major problem with phage genes is that a majority of their genes remain functionally unknown, hence their role is difficult to assess. A screen of \"Salmonella enterica\" phage SPN3US revealed 13 essential genes although it remains a bit obscure how many genes were really tested.\n\nMost genes are neither absolutely essential nor absolutely non-essential. Ideally their contribution to cell or organismal growth needs to be measured quantitatively, e.g. by determining how much growth rate is reduced in a mutant compared to \"wild-type\" (which may have been chosen arbitrarily from a population). For instance, a particular gene deletion may reduce growth rate (or fertility rate or other characters) to 90% of the wild-type.\n\nTwo genes are synthetic lethal if neither one is essential but when both are mutated the double-mutant is lethal. Some studies have estimated that the number of synthetic lethal genes may be on the order of 45% of all genes.\n\nMany genes are essential only under certain circumstances. For instance, if the amino acid lysine is supplied to a cell any gene that is required to make lysine is non-essential. However, when there is no lysine supplied, genes encoding enzymes for lysine biosynthesis become essential, as no protein synthesis is possible without lysine.\n\n\"Streptococcus pneumoniae\" appears to require 147 genes for growth and survival in saliva, more than the 113-133 that have been found in previous studies.\n\nThe deletion of a gene may result in death or in a block of cell division. While the latter case may implicate \"survival\" for some time, without cell division the cell may still die eventually. Similarly, instead of blocked cell division a cell may have reduced growth or metabolism ranging from nearly undetectable to almost normal. Thus, there is gradient from \"essential\" to completely non-essential, again depending on the condition. Some authors have thus distinguished between genes \"essential for survival\" and \"essential for fitness\".\n\nThe role of genetic background. Similar to environmental conditions, the genetic background can determine the essentiality of a gene: a gene may be essential in one individual but not another, given his or her genetic background. Gene duplications are one possible explanation (see below).\n\nMetabolic dependency. Genes involved in certain biosynthetic pathways, such as amino acid synthesis, can become non-essential if one or more amino acids are supplied by another organism. This is the main reason why many parasitic or endosymbiontic bacteria lost many genes (e.g. \"Chlamydia\"). Such genes may be essential but only present in the host organism. For instance, \"Chlamydia trachomatis\" cannot synthesize purine and pyrimidine nucleotides \"de novo\", so these bacteria are dependent on the nucleotide biosynthetic genes of the host.\n\nMany genes are duplicated within a genome. Such duplications (paralogs) often render essential genes non-essential because the duplicate can replace the original copy. For instance, the gene encoding the enzyme aspartokinase is essential in \"E. coli\". By contrast, the \"Bacillus subtilis\" genome contains three copies of this gene, none of which is essential on its own. However, a triple-deletion of all three genes is lethal. In such cases, the essentiality of a gene or a group of paralogs can often be predicted based on the essentiality of an essential single gene in a different species. In yeast, few of the essential genes are duplicated within the genome: 8.5% of the non-essential genes, but only 1% of the essential genes have a homologue in the yeast genome.\n\nIn the worm \"C. elegans\", non-essential genes are highly over-represented among duplicates, possibly because duplication of essential genes causes overexpression of these genes. Woods et al. found that non-essential genes are more often successfully duplicated (fixed) and lost compared to essential genes. By contrast, essential genes are less often duplicated but upon successful duplication are maintained over longer periods.\n\nIn bacteria, essential genes appear to be more conserved than nonessential genes but the correlation is not very strong. For instance, only 34% of the \"B. subtilis\" essential genes have reliable orthologs in all Firmicutes and 61% of the \"E. coli\" essential genes have reliable orthologs in all Gamma-proteobacteria. Fang et al. (2005) defined persistent genes as the genes present in more than 85% of the genomes of the clade. They found 475 and 611 of such genes for \"B. subtilis\" and \"E. coli\", respectively. Furthermore, they classified genes into five classes according to persistence and essentiality: persistent genes, essential genes, persistent nonessential (PNE) genes (276 in \"B. subtilis\", 409 in \"E. coli\"), essential nonpersistent (ENP) genes (73 in \"B. subtilis\", 33 in \"E. coli\"), and nonpersistent nonessential (NPNE) genes (3,558 in \"B. subtilis\", 3,525 in \"E. coli\"). Fang et al. found 257 persistent genes, which exist both in \"B. subtilis\" (for the Firmicutes) and \"E. coli\" (for the Gamma-proteobacteria). Among these, 144 (respectively 139) were previously identified as essential in \"B. subtilis\" (respectively \"E. coli\") and 25 (respectively 18) of the 257 genes are not present in the 475 \"B. subtilis\" (respectively 611 \"E. coli)\" persistent genes. All the other members of the pool are PNE genes.\n\nIn eukaryotes, 83% of the one-to-one orthologs between \"Schizosaccharomyces pombe\" and \"Saccharomyces cerevisiae\" have conserved essentiality, that is, they are nonessential in both species or essential in both species. The remaining 17% of genes are nonessential in one species and essential in the other. This is quite remarkable, given that \"S. pombe\" is separated from \"S. cerevisiae\" by approximately 400 million years of evolution.\n\nIn general, highly conserved and thus older genes (i.e. genes with earlier phylogenetic origin) are more likely to be essential than younger genes - even if they have been duplicated.\n\nThe experimental study of essential genes is limited by the fact that, by definition, inactivation of an essential gene is lethal to the organism. Therefore they can not be simply deleted or mutated to analyze the resulting phenotypes (a common technique in genetics).\n\nThere are, however, some circumstances in which essential genes can be manipulated. In diploid organisms, only a single functional copy of some essential genes may be needed (haplosufficiency), with the heterozygote displaying an instructive phenotype. Some essential genes can tolerate mutations that are deleterious, but not wholly lethal, since they do not completely abolish the gene's function.\n\nComputational analysis can reveal many properties of proteins without analyzing them experimentally, e.g. by looking at homologous proteins, function, structure etc. (see also below, \"Predicting essential genes\"). The products of essential genes can also be studied when expressed in other organisms, or when purified and studied \"in vitro\".\n\nConditionally essential genes are easier to study. Temperature-sensitive variants of essential genes have been identified which encode products that lose function at high temperatures, and so only show a phenotype at increased temperature.\n\nIf screens for essential genes are repeated in independent laboratories, they often result in different gene lists. For instance, screens in \"E. coli\" have yielded from ~300 to ~600 essential genes (see Table 1). Such differences are even more pronounced when different bacterial strains are used (see Figure 2). A common explanation is that the experimental conditions are different or that the nature of the mutation may be different (e.g. a complete gene deletion vs. a transposon mutant). Transposon screens in particular are hard to reproduce, given that a transposon can insert at many positions within a gene. Insertions towards the 3' end of an essential gene may not have a lethal phenotype (or no phenotype at all) and thus may not be recognized as such. This can lead to erroneous annotations (here: false negatives).\n\nComparison of CRISPR/cas9 and RNAi screens. Screens to identify essential genes in the human chronic myelogenous leukemia cell line K562 with these two methods showed only limited overlap. At a 10% false positive rate there were ~4,500 genes identified in the Cas9 screen versus ~3,100 in the shRNA screen, with only ~1,200 genes identified in both.\n\nDifferent organisms have different essential genes. For instance, \"Bacillus subtilis\" has 271 essential genes. About one-half (150) of the orthologous genes in \"E. coli\" are also essential. Another 67 genes that are essential in \"E. coli\" are not essential in \"B. subtilis\", while 86 \"E. coli\" essential genes have no \"B. subtilis\" ortholog.\n\nIn \"Mycoplasma genitalium\" at least 18 genes are essential that are not essential in \"M. bovis.\"\n\nEssential genes can be predicted computationally. However, most methods use experimental data (\"training sets\") to some extent. Chen et al. determined four criteria to select training sets for such predictions: (1) essential genes in the selected training set should be reliable; (2) the growth conditions in which essential genes are defined should be consistent in training and prediction sets; (3) species used as training set should be closely related to the target organism; and (4) organisms used as training and prediction sets should exhibit similar phenotypes or lifestyles. They also found that the size of the training set should be at least 10% of the total genes to yield accurate predictions. Some approaches for predicting essential genes are:\n\nComparative genomics. Shortly after the first genomes (of \"Haemophilus influenzae\" and \"Mycoplasma genitalium\") became available, Mushegian et al. tried to predict the number of essential genes based on common genes in these two species. It was surmised that only essential genes should be conserved over the long evolutionary distance that separated the two bacteria. This study identified approximately 250 candidate essential genes. As more genomes became available the number of predicted essential genes kept shrinking because more genomes shared fewer and fewer genes. As a consequence, it was concluded that the universal conserved core consists of less than 40 genes. However, this set of conserved genes is not identical to the set of essential genes as different species rely on different essential genes.\n\nA similar approach has been used to infer essential genes from the pan-genome of \"Brucella\" species. 42 complete \"Brucella\" genomes and a total of 132,143 protein-coding genes were used to predict 1252 potential essential genes, derived from the core genome by comparison with a prokaryote database of essential genes.\n\nHua et al. used Machine Learning to predict essential genes in 25 bacterial species\".\"\n\nHurst index. Liu et al. (2015) used the Hurst exponent, a characteristic parameter to describe long-range correlation in DNA to predict essential genes. In 31 out of 33 bacterial genomes the significance levels of the Hurst exponents of the essential genes were significantly higher than for the corresponding full-gene-set, whereas the significance levels of the Hurst exponents of the nonessential genes remained unchanged or increased only slightly.\n\nMinimal genomes. It was also thought that essential genes could be inferred from minimal genomes which supposedly contain only essential genes. The problem here is that the smallest genomes belong to parasitic (or symbiontic) species which can survive with a reduced gene set as they obtain many nutrients from their hosts. For instance, one of the smallest genomes is that of \"Hodgkinia cicadicola\", a symbiont of cicadas, containing only 144 Kb of DNA encoding only 188 genes. Like other symbionts, \"Hodgkinia\" receives many of its nutrients from its host, so its genes do not need to be essential.\n\nMetabolic modelling. Essential genes may be also predicted in completely sequenced genomes by metabolic reconstruction, that is, by reconstructing the complete metabolism from the gene content and then identifying those genes and pathways that have been found to be essential in other species. However, this method can be compromised by proteins of unknown function. In addition, many organisms have backup or alternative pathways which have to be taken into account (see figure 1). Metabolic modeling was also used by Basler (2015) to develop a method to predict essential metabolic genes. Flux balance analysis, a method of metabolic modeling, has recently been used to predict essential genes in clear cell renal cell carcinoma metabolism.\n\nGenes of unknown function. Surprisingly, a significant number of essential genes has no known function. For instance, among the 385 essential candidates in \"M. genitalium\", no function could be ascribed to 95 genes even though this number had been reduced to 75 by 2011.\n\nZUPLS. Song et al. presented a novel method to predict essential genes that only uses the Z-curve and other sequence-based features. Such features can be calculated readily from the DNA/amino acid sequences. However, the reliability of this method remains a bit obscure.\n\nEssential gene prediction servers. Guo et al. (2015) have developed three online services to predict essential genes in bacterial genomes. These freely available tools are applicable for single gene sequences without annotated functions, single genes with definite names, and complete genomes of bacterial strains.\n\nAlthough most essential genes encode proteins, many essential proteins consist of a single domain. This fact has been used to identify essential protein domains. Goodacre et al. have identified hundreds of essential domains of unknown function (eDUFs). Lu et al. presented a similar approach and identified 3,450 domains that are essential in at least one microbial species.\n\n\n\nLong, Jason Lu (Editor) 2015, Gene Essentiality - Methods and Protocols. Springer Protocols / Methods in Molecular Biology 1279, Humana Press, 248 pp., , (eBook), DOI 10.1007/978-1-4939-2398-4\n", "id": "40160407", "title": "Essential gene"}
{"url": "https://en.wikipedia.org/wiki?curid=38887508", "text": "Aicardi–Goutières syndrome\n\nAicardi–Goutières syndrome (AGS), which is completely distinct from the similarly named Aicardi syndrome, is a rare, usually early onset childhood, inflammatory disorder most typically affecting the brain and the skin (neurodevelopmental disorder). The majority of affected individuals experience significant intellectual and physical problems, although this is not always the case. The clinical features of AGS can mimic those of \"in utero\" acquired infection, and some characteristics of the condition also overlap with the autoimmune disease systemic lupus erythematosus (SLE). Following an original description of eight cases in 1984, the condition was first referred to as 'Aicardi–Goutières syndrome' (AGS) in 1992, and the first international meeting on AGS was held in Pavia, Italy, in 2001.\n\nAGS can occur due to mutations in any one of a number of different genes, of which seven have been identified to date, namely: TREX1, RNASEH2A, RNASEH2B, RNASEH2C (which together encode the Ribonuclease H2 enzyme complex), SAMHD1, ADAR1, and IFIH1 (coding for MDA5). This neurological disease occurs in all populations worldwide, although it is almost certainly under-diagnosed. To date (2014) at least 400 cases of AGS are known.\n\nThe initial description of AGS suggested that the disease was always severe, and was associated with unremitting neurological decline, resulting in death in childhood. As more cases have been identified, it has become apparent that this is not necessarily the case, with many patients now considered to demonstrate an apparently stable clinical picture, alive in their 4th decade. Moreover, rare individuals with pathogenic mutations in the AGS-related genes can be minimally affected (perhaps only with chilblains) and are in mainstream education, and even affected siblings within a family can show marked differences in severity.\n\nIn about ten percent of cases, AGS presents at or soon after birth (i.e. in the neonatal period). This presentation of the disease is characterized by microcephaly, neonatal seizures, poor feeding, jitteriness, cerebral calcifications (accumulation of calcium deposits in the brain), white matter abnormalities, and cerebral atrophy; thus indicating that the disease process became active before birth i.e. \"in utero\". These infants can have hepatosplenomegaly and thrombocytopaenia, very much like cases of transplacental viral infection. About one third of such early presenting cases, most frequently in association with mutations in \"TREX1\", die in early childhood.\n\nOtherwise the majority of AGS cases present in early infancy, sometimes after an apparently normal period of development. During the first few months after birth, these children develop features of an encephalopathy with irritability, persistent crying, feeding difficulties, an intermittent fever (without obvious infection), and abnormal neurology with disturbed tone, dystonia, an exaggerated startle response, and sometimes seizures.\nGlaucoma can be present at birth, or develop later. Many children retain apparently normal vision, although a significant number are cortically blind. Hearing is almost invariably normal. Over time, up to 40% of patients develop so-called chilblain lesions, most typically on the toes and fingers and occasionally also involving the ears. They are usually worse in the winter.\n\nAGS is a genetically heterogeneous disease resulting from mutations in any of seven genes encoding: a 3' repair exonuclease with preferential activity on single stranded DNA (TREX1); any of the three components of the ribonuclease H2 endonuclease complex acting on ribonucleotides in RNA:DNA hybrids (RNASEH2A, RNASEH2B, RNASEH2C); a SAM domain and HD domain containing protein which functions as a deoxynucleoside triphosphate triphosphohydrolase (SAMHD1); an enzyme catalysing the hydrolytic deamination of adenosine to inosine in double-stranded RNA (ADAR1); and the cytosolic double-stranded RNA receptor (MDA5, also known as \"IFIH1\"). Mutations in the gene OCLN on chromosome 5q13.2, which is thought to cause band-like calcification in the brain, have been discovered in affected individuals and categorized as BLCPMG which often associated with AGS. In most cases, except for IFIH1- and rare cases of TREX1- and ADAR1-related disease, these mutations follow an autosomal recessive inheritance pattern (and thus the parents of an affected child face a 1 in 4 risk of having a further child similarly affected at every conception).\n\nAGS can be divided into subtypes based on the gene in which the causative mutation occurs. A survey of 374 patients with an AGS diagnosis reported that the most frequent mutations occurred in RNASEH2B.\n\nAGS-associated mutations have been found to show incomplete penetrance in some cases, with children in the same family with the same mutations showing markedly different neurological and developmental outcomes. Clinical features and disease course vary somewhat by genotype, with TREX1 associated with likely \"in utero\" onset and high mortality rate, and RNASEH2B mutations associated with slightly milder neurological impairments, lower interferon activity, and longer lifespan.\n\nType I interferon activity was originally described over 50 years ago as a soluble factor produced by cells treated with inactivated, non-replicating viruses that blocked subsequent infection with live virus. Although the rapid induction and amplification of the type I interferon system is highly adaptive in terms of virus eradication, aberrant stimulation or unregulated control of the system could lead to inappropriate and / or excessive interferon output.\n\nStudies of the AGS-related proteins TREX1, the RNase H2 complex, SAMHD1 and ADAR1, suggest that an inappropriate accumulation of self-derived nucleic acids can induce type I interferon signaling. The findings of \"IFIH1\" mutations in the similar context implicates the aberrant sensing of nucleic acids as a cause of immune upregulation.\n\nWhat is the source of the nucleic acid inducing the immune disturbance in AGS? Intriguingly, it has been shown that TREX1 can metabolise reverse-transcribed HIV-1 DNA and that single-stranded DNA derived from endogenous retroelements accumulates in Trex1-deficient cells; however, the upregulation of retroelements in TREX1-null cells has recently been disputed. Similarly, another AGS-related gene product SAMHD1 also presents strong potency against activity of multiple non-LTR retroelements, which is independent from SAMHD1's famous dNTPase activity.\n Laboratory: normal metabolic and infective screening. An increase in the number of white cells (particularly lymphocytes) in the CSF, and high levels of interferon-alpha activity and neopterin in the CSF are important clues - however, these features are not always present. More recently, a persistent elevation of mRNA levels of interferon-stimulated gene transcripts have been recorded in the peripheral blood of almost all cases of AGS with mutations in \"TREX1\", \"RNASEH2A\", \"RNASEH2C\", \"SAMHD1\", \"ADAR1\" and \"IFIH1\", and in 75% of patients with mutations in \"RNASEH2B\". These results are irrespective of age. Thus, this interferon signature appears to be a very good marker of disease.\n\nGenetics: pathogenic mutations in any of the seven genes known to be involved in AGS.\n\nAt the moment there are no therapies specifically targeting the underlying cause of AGS. Current treatments address the symptoms, which can be varied both in scope and severity. Many patients benefit from tube-feeding. Drugs can be administered to help with seizures / epilepsy. The treatment of chilblains remains problematic, but particularly involves keeping the feet / hands warm. Physical therapy, including the use of splints can help to prevent contractures and surgery is sometimes required. Botox (botulinium toxin) has sometimes caused severe immune reactions in some AGS patients, and the high risk of possible further brain damage must be considered before giving Botox. Occupational therapy can help with development, and the use of technology (e.g. Assistive Communication Devices) can facilitate communication. Patients should be regularly screened for treatable conditions, most particularly glaucoma and endocrine problems (especially hypothyroidism). The risk versus benefit of giving immunizations also must be considered, as some AGS patients have high immune responses or flares that cause further brain damage from immunizations but other patients have no problems with immunizations; on the other hand, AGS patients have died from illnesses that can be immunized against, so the family must consider the risk vs. benefit of each immunization vs. risk of the actual virus if they choose not to immunize. As of 2017, there are current drug trials being conducted that may lead to drug treatments for AGS.\n\nIn 1984, Jean Aicardi and Francoise Goutières described eight children from five families presenting with a severe early onset encephalopathy, which was characterized by calcification of the basal ganglia, abnormalities of the cerebral white matter and diffuse brain atrophy. An excess of white cells, chiefly lymphocytes, was found in the cerebrospinal fluid (CSF), thus indicating an inflammatory condition. During the first year of life, these children developed microcephaly, spasticity and dystonia. Some of the parents of the children were genetically related to each other, and the children were both male and female, which suggested that the disease was inherited as an autosomal recessive genetic trait.\n\nIn 1988, Pierre Lebon and his colleagues identified the additional feature of raised levels of interferon-alpha in patient CSF in the absence of infection. This observation supported the suggestion that AGS was an inflammatory disease, as did the later finding of increased levels of the inflammatory marker neopterin in CSF, and the demonstration that more than 90% of individuals with a genetic diagnosis of AGS, tested at any age, demonstrate an upregulation of interferon-induced gene transcripts - a so-called interferon signature.\n\nAll cases of Cree encephalitis (an early-onset progressive encephalopathy in a Cree First Nations community in Canada), and many cases previously described as pseudo-TORCH syndrome, (toxoplasmosis, rubella, cytomegalovirus, and herpes simplex virus), initially considered to be separate disorders, were later found to be the same as AGS (although other causes of, genetically distinct, ‘pseudo-TORCH’ phenotypes exist).\n", "id": "38887508", "title": "Aicardi–Goutières syndrome"}
{"url": "https://en.wikipedia.org/wiki?curid=28211767", "text": "Non-allelic homologous recombination\n\nNon-allelic homologous recombination (NAHR) is a form of homologous recombination that occurs between two lengths of DNA that have high sequence similarity, but are not alleles.\n\nIt usually occurs between sequences of DNA that have been previously duplicated through evolution, and therefore have low copy repeats (LCRs). These repeat elements typically range from 10–300 kb in length and share 95-97% sequence identity. During meiosis or mitosis, LCRs can misalign and subsequent crossing-over can result in genetic rearrangement. When non-allelic homologous recombination occurs between different LCRs, deletions or further duplications of the DNA can occur. This can give rise to rare genetic disorders, caused by the loss or increased copy number of genes within the deleted or duplicated region. It can also contribute to the copy number variation seen in some gene clusters.\n\nAs LCRs are often found in \"hotspots\" in the human genome, some chromosomal regions are particularly prone to NAHR. Recurrent rearrangements are nucleotide sequence variations found in multiple individuals, sharing a common size and location of break points. Therefore, multiple patients may manifest with similar deletions or duplications, resulting in the description of genetic syndromes. Examples of these include NF1 microdeletion syndrome, 17q21.3 recurrent microdeletion syndrome or 3q29 microdeletion syndrome.\n\n", "id": "28211767", "title": "Non-allelic homologous recombination"}
{"url": "https://en.wikipedia.org/wiki?curid=40707846", "text": "Designer crossbreed\n\nA designer crossbreed or designer breed is a crossbred animal that has purebred parents, usually registered with a breed registry, but from two different breeds. These animals are the result of a deliberate decision to create a specific crossbred animal. Less often, the animal may have more than two pure breeds in its ancestry, but unlike a mutt or a mongrel, its entire pedigree is known to descend from specific known animals. While the term is best-known when applied to certain dog crossbreeds, other animals such as cattle, horses, birds and cats may also be bred in this fashion. Some crossbred breeders start a freestanding breed registry to record designer crossbreds, other crossbreds may be included in an \"appendix\" to an existing purebred registry. either form of registration may be the first step in recording and tracking pedigrees in order to develop a new breed.\n\nThe purpose of creating designer crossbreds is usually one or more of the following reasons: \n\nBreeders of designer crossbreds borrow the technical language from hybrid plant breeding: A first generation, 50-50 crossbred is an F1 cross. Subsequent generations may see a purebred animal crossed back on a crossbred, creating a 75/25 cross, or a BC1 or F1b \"backcross.\" The breeding of two crossbreds of the same combination of breeds, creating a F2 cross, an animal that is still a 50-50 cross, but it is the second filial generation of the combination. A F2 cross bred to an F2 cross creates a F3 cross. Similarly, a F2 animal bred to an F1 animal creates a F2b backcross. F3 crosses and greater are called \"multi-generational\" crosses. In dog breeding, three generations of reliable documented breeding can be considered a \"breed\" rather than a crossbreed.\n\nThere are disadvantages to creating designer crossbreds, notably the potential that the cross will be of inferior quality or that it will not produce as consistent a result as would breeding purebred animals. For example, the poodle is a frequent breed used in creation of designer crossbreds, due to its nonshedding coat, but that trait does not always breed true when it is part of a designer cross. Also, because breeders of crossbred animals may be less careful about genetic testing and weeding out undesirable traits, certain deleterious dominant genes may still be passed on to a crossbred offspring. In an F2 cross, recessive genetic traits may also return if the parent animals were both carriers of an undesired trait.\n\n\n", "id": "40707846", "title": "Designer crossbreed"}
{"url": "https://en.wikipedia.org/wiki?curid=40798491", "text": "Aminoacylase 1 deficiency\n\nAminoacylase 1 deficiency is a rare inborn error of metabolism. To date only 21 cases have been described.\n\nThe clinical picture is heterogeneous and includes motor delay, seizures, moderate to severe mental retardation, absent speech, growth delay, muscular hypotonia and autistic features.\n\nThis disorder in inherited in an autosomal recessive fashion.\n\nAminoacylase 1 (ACY1: EC 3.5.14) is a zinc binding enzyme which hydrolyzes N-acetyl amino acids into the free amino acid and acetic acid. Of the N-actyl amino hydrolyzing enzymes, aminoacylase 1 is the most common.\n\nThe ACY1 gene is located on the short arm of chromosome 3 (3p21.2).\n\nThere is a specific pattern of N-acetyl amino acid excretion in the urine. The diagnosis can be confirmed by sequencing of the aminoacylase 1 gene.\n\nThis disorder was first reported in 2005.\n", "id": "40798491", "title": "Aminoacylase 1 deficiency"}
{"url": "https://en.wikipedia.org/wiki?curid=40841348", "text": "Computational and Statistical Genetics\n\nThe interdisciplinary research field of Computational and Statistical Genetics uses the latest approaches in genomics, quantitative genetics, computational sciences, bioinformatics and statistics to develop and apply computationally efficient and statistically robust methods to sort through increasingly rich and massive genome wide data sets to identify complex genetic patterns, gene functionalities and interactions, disease and phenotype associations involving the genomes of various organisms. This field is also often referred to as computational genomics. This is an important discipline within the umbrella field computational biology.\n\nDuring the last two decades, there has been a great interest in understanding the genetic and genomic\nmakeup of various species, including humans primarily aided by the different genome sequencing technologies to read the genomes that has been rapidly developing. However, these technologies are still limited, and computational and statistical methods are a must to detect and process errors and put together the pieces of partial information from the sequencing and genotyping technologies.\n\nA haplotype is defined the sequence of nucleotides (A,G,T,C) along a single chromosome. In humans, we have 23 pairs of chromosomes. Another example is maize which is also a diploid with 10 pairs of chromosomes. However, with current technology, it is difficult to separate the two chromosomes within a pair and the assays produce the combined haplotype, called the genotype information at each nucleotide.\nThe objective of haplotype phasing is to find the phase of the two haplotypes given the combined genotype information. Knowledge of the haplotypes is extremely important and not only gives us a complete picture of an individuals genome, but also aids other computational genomic processes such as Imputation among many significant biological motivations.\n\nFor diploid organisms such as humans and maize, each organism has two copies of a chromosome - one each from the two parents. The two copies are highly similar to each other. A haplotype is the sequence of nucleotides in a chromosome. the haplotype phasing problem is focused on the nucleotides where the two homologous chromosomes differ. Computationally, for a genomic region with K differing nucleotide sites, there are 2^K - 1 possible haplotypes, so the phasing problem focuses on efficiently finding the most probable haplotypes given an observed genotype. For more information, see Haplotype.\n\nAlthough the genome of a higher organism (eukaryotes) contains millions of single nucleotide polymorphisms (SNPs), genotyping arrays are pre- determined to detect only a handful of such markers. The missing markers are predicted using imputation analysis. Imputation of un-genotyped markers has now become an essential part of genetic and genomic studies. It utilizes the knowledge of linkage disequilibrium (LD) from haplotypes in a known reference panel (for example, HapMap and the 1000 Genomes Projects) to predict genotypes at the missing or un-genotyped markers. The process allows the scientists to accurately perform analysis of both the genotyped polymorphic markers and the un-genotyped markers that are predicted computationally. It has been shown that downstream studies benefit a lot from imputation analysis in the form of improved the power to detect disease-associated loci. Another crucial contribution of imputation is that it also facilitates combining genetic and genomic studies that used different genotyping platforms for their experiments. For example. although 415 million common and rare genetic variants exist in the human genome,the current genotyping arrays such as Affymetrix and Illumina microarrays can only assay up to 2.5 million SNPs. Therefore, imputation analysis is an important research direction and it is important to identify methods and platforms to impute high quality genotype data using existing genotypes and reference panels from publicly available resources, such as the International HapMap Project and the 1000 Genomes Project. For humans, the analysis has successfully generated predicted genotypes in many races including Europeans and African Americans. For other species such as plants, imputation analysis is an ongoing process using reference panels such as in maize.\n\nA number of different methods exist for genotype imputation. The three most widely used imputation methods are - Mach, Impute and Beagle. All three methods utilize hidden markov models as the underlying basis for estimating the distribution of the haplotype frequencies. Mach and Impute2 are more computationally intensive compared with Beagle. Both Impute and Mach are based on different implementations of the product of the conditionals or PAC model. Beagle groups the reference panel haplotypes into clusters at each SNP to form localized haplotype-cluster model that allows it to dynamically vary the number of clusters at each SNP making it computationally faster than Mach and Impute2.\n\nFor more information, see Imputation (genetics).\n\nOver the past few years, genome-wide association studies (GWAS) have become a powerful tool for investigating the genetic basis of common diseases and has improved our understanding of the genetic basis of many complex traits. Traditional single SNP (single-nucleotide polymorphism) GWAS is the most commonly used method to find trait associated DNA sequence variants - associations between variants and one or more phenotypes of interest are investigated by studying individuals with different phenotypes and examining their genotypes at the position of each SNP individually. The SNPs for which one variant is statistically more common in individuals belonging to one phenotypic group are then reported as being associated with the phenotype. However, most complex common diseases involve small population-level contributions from multiple genomic loci. To detect such small effects as genome-wide significant, traditional GWAS rely on increased sample size e.g. to detect an effect which accounts for 0.1% of total variance, traditional GWAS needs to sample almost 30,000 individuals. Although the development of high throughput SNP genotyping technologies has lowered the cost and improved the efficiency of genotyping. Performing such a large scale study still costs considerable money and time. Recently, association analysis methods utilizing gene-based tests have been proposed that are based on the fact that variations in protein-coding and adjacent regulatory regions are more likely to have functional relevance. These methods have the advantage that they can account for multiple independent functional variants within a gene, with the potential to greatly increase the power to identify disease/trait associated genes. Also, imputation of ungenotyped markers using known reference panels(e.g. HapMap and the 1000 Genomes Project) predicts genotypes at the missing or untyped markers thereby allowing one to accurately evaluate the evidence for association at genetic markers that are not directly genotyped (in\naddition to the typed markers) and has been shown to improve the power of GWAS to detect disease\nassociated loci.\n\nFor more information, see Genome-wide association study\n\nIn this era of large amount of genetic and genomic data, accurate representation and identification of statistical interactions in biological/genetic/genomic data constitutes a vital basis for designing interventions and curative solutions for many complex diseases. Variations in human genome have been long known to make us susceptible to many diseases. We are hurtling towards the era of personal genomics and personalized medicine that require accurate predictions of disease risk posed by predisposing genetic factors. Computational and statistical methods for identifying these genetic variations, and building these into intelligent models for diseaseassociation and interaction analysis studies genome-wide are a dire necessity across many disease areas. The principal challenges are: (1) most complex diseases involve small or weak contributions from multiple genetic factors that explain only a minuscule fraction of the population variation attributed to genetic factors. (2) Biological data is inherently extremely noisy, so the underlying complexities of biological systems (such as linkage disequilibrium and genetic heterogeneity) need to be incorporated into the statistical models for disease association studies. The chances of developing many common diseases such as cancer, autoimmune diseases and cardiovascular diseases involves complex interactions between multiple genes and several endogenous and exogenous environmental agents or covariates. Many previous disease association studies could not produce significant results because of the lack of incorporation of statistical interactions in their mathematical models explaining the disease outcome. Consequently much of the genetic risks underlying several diseases and disorders remain unknown. Computational methods such as to model and identify the genetic/genomic variations underlying disease risks has a great potential to improve prediction of disease outcomes, understand the interactions and design better therapeutic methods based on them.\n", "id": "40841348", "title": "Computational and Statistical Genetics"}
{"url": "https://en.wikipedia.org/wiki?curid=344879", "text": "Cytogenetics\n\nCytogenetics is a branch of genetics that is concerned with how the chromosomes relate to cell behaviour, particularly to their behaviour during mitosis and meiosis. Techniques used include karyotyping, analysis of G-banded chromosomes, other cytogenetic banding techniques, as well as molecular cytogenetics such as fluorescent \"in situ\" hybridization (FISH) and comparative genomic hybridization (CGH).\n\nChromosomes were first observed in plant cells by Karl Wilhelm von Nägeli in 1842. Their behavior in animal (salamander) cells was described by Walther Flemming, the discoverer of mitosis, in 1882. The name was coined by another German anatomist, von Waldeyer in 1888.\n\nThe next stage took place after the development of genetics in the early 20th century, when it was appreciated that the set of chromosomes (the karyotype) was the carrier of the genes. Levitsky seems to have been the first to define the karyotype as the phenotypic appearance of the somatic chromosomes, in contrast to their genic contents. Investigation into the human karyotype took many years to settle the most basic question: how many chromosomes does a normal diploid human cell contain? In 1912, Hans von Winiwarter reported 47 chromosomes in spermatogonia and 48 in oogonia, concluding an XX/XO sex determination mechanism. Painter in 1922 was not certain whether the diploid number of man was 46 or 48, at first favoring 46. He revised his opinion later from 46 to 48, and he correctly insisted on man having an XX/XY system. Considering their techniques, these results were quite remarkable.\n\nIn science books, the number of human chromosomes remained at 48 for over thirty years. New techniques were needed to correct this error. Joe Hin Tjio working in Albert Levan's lab was responsible for finding the approach:\n\nIt took until 1956 until it became generally accepted that the karyotype of man included only 46 chromosomes. The great apes have 48 chromosomes. Human chromosome 2 was formed by a merger of ancestral chromosomes, reducing the number.\n\nBarbara McClintock began her career as a maize cytogeneticist. In 1931, McClintock and Harriet Creighton demonstrated that cytological recombination of marked chromosomes correlated with recombination of genetic traits (genes). McClintock, while at the Carnegie Institution, continued previous studies on the mechanisms of chromosome breakage and fusion flare in maize. She identified a particular chromosome breakage event that always occurred at the same locus on maize chromosome 9, which she named the \"\"Ds\"\" or \"dissociation\" locus. McClintock continued her career in cytogenetics studying the mechanics and inheritance of broken and ring (circular) chromosomes of maize. During her cytogenetic work, McClintock discovered transposons, a find which eventually led to her Nobel Prize in 1983.\n\nIn the 1930s, Dobzhansky and his coworkers collected \"Drosophila pseudoobscura\" and \"D. persimilis\" from wild populations in California and neighboring states. Using Painter's technique they studied the polytene chromosomes and discovered that the wild populations were polymorphic for chromosomal inversions. All the flies look alike whatever inversions they carry: this is an example of a cryptic polymorphism.\n\nEvidence rapidly accumulated to show that natural selection was responsible. Using a method invented by L'Heretier and Teissier, Dobzhansky bred populations in \"population cages\", which enabled feeding, breeding and sampling whilst preventing escape. This had the benefit of eliminating migration as a possible explanation of the results. Stocks containing inversions at a known initial frequency can be maintained in controlled conditions. It was found that the various chromosome types do not fluctuate at random, as they would if selectively neutral, but adjust to certain frequencies at which they become stabilised. By the time Dobzhansky published the third edition of his book in 1951 he was persuaded that the chromosome morphs were being maintained in the population by the selective advantage of the heterozygotes, as with most polymorphisms.\n\nThe lily is a favored organism for the cytological examination of meiosis since the chromosomes are large and each morphological stage of meiosis can be easily identified microscopically. Hotta et al. presented evidence for a common pattern of DNA nicking and repair synthesis in male meiotic cells of lilies and rodents during the zygotene–pachytene stages of meiosis when crossing over was presumed to occur. The presence of a common pattern between organisms as phylogenetically distant as lily and mouse led the authors to conclude that the organization for meiotic crossing-over in at least higher eukaryotes is probably universal in distribution.\n\nIn the event of procedures which allowed easy enumeration of chromosomes, discoveries were quickly made related to aberrant chromosomes or chromosome number. In some congenital disorders, such as Down syndrome, cytogenetics revealed the nature of the chromosomal defect: a \"simple\" trisomy. Abnormalities arising from nondisjunction events can cause cells with aneuploidy (additions or deletions of entire chromosomes) in one of the parents or in the fetus. In 1959, Lejeune discovered patients with Down syndrome had an extra copy of chromosome 21. Down syndrome is also referred to as trisomy 21.\n\nOther numerical abnormalities discovered include sex chromosome abnormalities. A female with only one X chromosome has Turner syndrome, whereas an additional X chromosome in a male, resulting in 47 total chromosomes, has Klinefelter Syndrome. Many other sex chromosome combinations are compatible with live birth including XXX, XYY, and XXXX. The ability for mammals to tolerate aneuploidies in the sex chromosomes arises from the ability to inactivate them, which is required in normal females to compensate for having two copies of the chromosome. Not all genes on the X chromosome are inactivated, which is why there is a phenotypic effect seen in individuals with extra X chromosomes.\n\nTrisomy 13 was associated with Patau Syndrome and trisomy 18 with Edwards Syndrome.\n\nIn 1960, Peter Nowell and David Hungerford discovered a small chromosome in the white blood cells of patients with Chronic myelogenous leukemia (CML). This abnormal chromosome was dubbed the Philadelphia chromosome - as both scientists were doing their research in Philadelphia, Pennsylvania. Thirteen years later, with the development of more advanced techniques, the abnormal chromosome was shown by Janet Rowley to be the result of a translocation of chromosomes 9 and 22. Identification of the Philadelphia chromosome by cytogenetics is diagnostic for CML.\n\nIn the late 1960s, Torbjörn Caspersson developed a quinicrine fluorescent staining technique (Q-banding) which revealed unique banding patterns for each chromosome pair. This allowed chromosome pairs of otherwise equal size to be differentiated by distinct horizontal banding patterns. Banding patterns are now used to elucidate the breakpoints and constituent chromosomes involved in chromosome translocations. Deletions and inversions within an individual chromosome can also be identified and described more precisely using standardized banding nomenclature. G-banding (utilizing trypsin and Giemsa/ Wright stain) was concurrently developed in the early 1970s and allows visualization of banding patterns using a bright field microscope.\n\nDiagrams identifying the chromosomes based on the banding patterns are known as \"idiograms\". These maps became the basis for both prenatal and oncological fields to quickly move cytogenetics into the clinical lab where karyotyping allowed scientists to look for chromosomal alterations. Techniques were expanded to allow for culture of free amniocytes recovered from amniotic fluid, and elongation techniques for all culture types that allow for higher-resolution banding.\n\nIn the 1980s, advances were made in molecular cytogenetics. While radioisotope-labeled probes had been hybridized with DNA since 1969, movement was now made in using fluorescent labeled probes. Hybridizing them to chromosomal preparations using existing techniques came to be known as fluorescence \"in situ\" hybridization (FISH). This change significantly increased the usage of probing techniques as fluorescent labeled probes are safer. Further advances in micromanipulation and examination of chromosomes led to the technique of chromosome microdissection whereby aberrations in chromosomal structure could be isolated, cloned and studied in ever greater detail.\n\nThe routine chromosome analysis (Karyotyping) refers to analysis of metaphase chromosomes which have been banded using trypsin followed by Giemsa, Leishmanns, or a mixture of the two. This creates unique banding patterns on the chromosomes. The molecular mechanism and reason for these patterns is unknown, although it likely related to replication timing and chromatin packing.\n\nSeveral chromosome-banding techniques are used in cytogenetics laboratories. Quinacrine banding (Q-banding) was the first staining method used to produce specific banding patterns. This method requires a fluorescence microscope and is no longer as widely used as Giemsa banding (G-banding). Reverse banding, or R-banding, requires heat treatment and reverses the usual black-and-white pattern that is seen in G-bands and Q-bands. This method is particularly helpful for staining the distal ends of chromosomes. Other staining techniques include C-banding and nucleolar organizing region stains (NOR stains). These latter methods specifically stain certain portions of the chromosome. C-banding stains the constitutive heterochromatin, which usually lies near the centromere, and NOR staining highlights the satellites and stalks of acrocentric chromosomes.\nHigh-resolution banding involves the staining of chromosomes during prophase or early metaphase (prometaphase), before they reach maximal condensation. Because prophase and prometaphase chromosomes are more extended than metaphase chromosomes, the number of bands observable for all chromosomes increases from about 300 to 450 to as many as 800. This allows the detection of less obvious abnormalities usually not seen with conventional banding.\n\nCells from bone marrow, blood, amniotic fluid, cord blood, tumor, and tissues (including skin, umbilical cord, chorionic villi, liver, and many other organs) can be cultured using standard cell culture techniques in order to increase their number. A mitotic inhibitor (colchicine, colcemid) is then added to the culture. This stops cell division at mitosis which allows an increased yield of mitotic cells for analysis. The cells are then centrifuged and media and mitotic inhibitor are removed, and replaced with a hypotonic solution. This causes the white blood cells or fibroblasts to swell so that the chromosomes will spread when added to a slide as well as lyses the red blood cells. After the cells have been allowed to sit in hypotonic solution, Carnoy's fixative (3:1 methanol to glacial acetic acid) is added. This kills the cells and hardens the nuclei of the remaining white blood cells. The cells are generally fixed repeatedly to remove any debris or remaining red blood cells. The cell suspension is then dropped onto specimen slides. After aging the slides in an oven or waiting a few days they are ready for banding and analysis.\n\nAnalysis of banded chromosomes is done at a microscope by a clinical laboratory specialist in cytogenetics (CLSp(CG)). Generally 20 cells are analyzed which is enough to rule out mosaicism to an acceptable level. The results are summarized and given to a board-certified cytogeneticist for review, and to write an interpretation taking into account the patients previous history and other clinical findings. The results are then given out reported in an \"International System for Human Cytogenetic Nomenclature 2009\" (ISCN2009).\n\nFluorescent in situ hybridization (FISH) refers to using fluorescently labeled probe to hybridize to cytogenetic cell preparations.\n\nIn addition to standard preparations FISH can also be performed on:\n\n\"This section refers to preparation of standard cytogenetic preparations\"\n\nThe slide is aged using a salt solution usually consisting of 2X SSC (salt, sodium citrate). The slides are then dehydrated in ethanol, and the probe mixture is added. The sample DNA and the probe DNA are then co-denatured using a heated plate and allowed to re-anneal for at least 4 hours. The slides are then washed to remove excess unbound probe, and counterstained with 4',6-Diamidino-2-phenylindole (DAPI) or propidium iodide.\n\nAnalysis of FISH specimens is done by fluorescence microscopy by a clinical laboratory specialist in cytogenetics. For oncology generally a large number of interphase cells are scored in order to rule out low-level residual disease, generally between 200 and 1,000 cells are counted and scored. For congenital problems usually 20 metaphase cells are scored.\n\nAdvances now focus on molecular cytogenetics including automated systems for counting the results of standard FISH preparations and techniques for virtual karyotyping, such as comparative genomic hybridization arrays, CGH and Single nucleotide polymorphism arrays.\n\n\n", "id": "344879", "title": "Cytogenetics"}
{"url": "https://en.wikipedia.org/wiki?curid=5504842", "text": "Sense (molecular biology)\n\nIn molecular biology and genetics, the sense of nucleic acid molecules (often DNA or RNA) is the nature of their roles and their complementary molecules' nucleic acid units' roles in specifying amino acids. Depending on the context within molecular biology, sense may have slightly different meanings.\n\nMolecular biologists call a single strand of DNA, sense (or positive (+)) if an RNA version of the same sequence is translated or translatable into protein. Its complementary strand is called antisense (or negative (-) sense). Sometimes the phrases coding strand (for sense) and template strand (for antisense) are encountered; however, protein coding and non-coding RNAs can be transcribed from the sense strand. Additionally, the terms \"sense\" and \"antisense\" are relative to the RNA transcript in question and not to the entire DNA strand as a whole. In other words, either DNA strand can serve as the sense or antisense strand for a particular RNA transcript. In some cases, RNA transcripts can be transcribed in both directions (i.e. on either strand) from a common promoter region, or be transcribed from within introns on either strand (see \"ambisense\" below).\n\nThe two complementary strands of double-stranded DNA (dsDNA) are usually differentiated as the \"sense\" strand and the \"antisense\" strand. The DNA sense strand looks like the messenger RNA (mRNA) and can be used to read the expected protein code; for example, ATG in the sense DNA may correspond to an AUG codon in the mRNA, encoding the amino acid methionine. However, the DNA sense strand itself is not used to make protein by the cell. It is the DNA antisense strand which serves as the source for the protein code, because, with bases complementary to the DNA sense strand, it is used as a template for the mRNA. Since transcription results in an RNA product complementary to the DNA template strand, the mRNA is complementary to the DNA antisense strand. The mRNA is what is used for translation (protein synthesis).Hence, a base triplet 3'-TAC-5' in the DNA antisense strand can be used as a template which will result in an 5'-AUG-3' base triplet in mRNA (AUG is the codon for methionine, the start codon). The DNA sense strand will have the triplet ATG, which looks just like AUG but will not be used to make methionine because it will not be used to make mRNA. The DNA sense strand is called a \"sense\" strand not because it will be used to make protein (it won't be), but because it has a sequence that looks like the protein codon sequence.\n\nIn biology and research, short antisense molecules can interact with complementary strands of nucleic acids, modifying expression of genes. See the section on \"antisense oligonucleotides\" below.\n\nDNA strand 1: antisense strand (transcribed to)→ RNA strand (sense)\n\nDNA strand 2: sense strand\n\nSome regions within a double strand of DNA code for genes, which are usually instructions specifying the order of amino acids in a protein along with regulatory sequences, splicing sites, noncoding introns, and other complicating details. For a cell to use this information, one strand of the DNA serves as a template for the synthesis of a complementary strand of RNA. The template DNA strand is called the transcribed strand with antisense sequence and the mRNA transcript is said to be sense sequence (the complement of antisense). Because the DNA is double-stranded, the strand complementary to the antisense sequence is called the non-transcribed strand and has the same sense sequence as the mRNA transcript (though T bases in DNA are substituted with U bases in RNA).\n\n\"A note on the confusion between \"sense\" and \"antisense\" strands:\" The strand names actually depend on which direction you are writing the sequence that contains the information for proteins (the \"sense\" information), not on which strand is on the top or bottom (that is arbitrary). The only real biological information that is important for labeling strands is the location of the 5' phosphate group and the 3' hydroxyl group because these ends determine the direction of transcription and translation. A sequence 5' CGCTAT 3' is equivalent to a sequence written 3' TATCGC 5' as long as the 5' and 3' ends are noted. If the ends are not labeled, convention is to assume that the sequence is written from left to right in the 5' to 3' direction. Watson strand refers to 5' to 3' top strand (5' → 3'), whereas Crick strand refers to 5' to 3' bottom strand (3' ← 5'). Both Watson and Crick strands can be either sense or antisense strands depending on the gene whose sequences are displayed in the genome sequence database. For example, YEL021W, an alias of URA3 gene used in NCBI database, defines that this gene is located on the 21st open reading frame (ORF) from the centromere of the left arm (L) of Yeast (Y) chromosome number V (E), and that the expression coding strand is Watson strand (W). YKL074C defines the 74th ORF to the left of the centromere of chromosome XI and denotes coding strand from the Crick strand (C). Another confusing term referring to \"Plus\" and \"Minus\" strand is also widely used. Whether the strand is sense (positive) or antisense (negative), the default query sequence in NCBI BLAST alignment is \"Plus\" strand.\n\nA single-stranded genome that contains both positive-sense and negative-sense is said to be ambisense.\nBunyaviruses have 3 single-stranded RNA (ssRNA) fragments containing both positive-sense and negative-sense sections; arenaviruses are also ssRNA viruses with an ambisense genome, as they have 2 fragments that are mainly negative-sense except for part of the 5' ends of the large and small segments of their genome.\n\nAntisense RNA is an RNA transcript that is complementary to endogenous mRNA. In other words, it is a non-coding strand complementary to the coding sequence of RNA; this is similar to negative-sense viral RNA. Introducing a transgene coding for antisense RNA is a technique used to block expression of a gene of interest. Radioactively-labelled antisense RNA can be used to show the level of transcription of genes in various cell types. Some alternative antisense structural types are being experimentally applied as antisense therapy, with at least one antisense therapy approved for use in humans. \n\nWhen mRNA forms a duplex with a complementary antisense RNA sequence, translation is blocked. This process is related to RNA interference.\n\nAntisense nucleic acid molecules have been used experimentally to bind to mRNA and prevent expression of specific genes. Antisense therapies are also in development; in the USA, the Food and Drug Administration (FDA) has approved phosphorothioate antisense oligos fomivirsen (Vitravene) and mipomersen (Kynamro) for human therapeutic use.\n\nCells can produce antisense RNA molecules naturally, called microRNA, which interact with complementary mRNA molecules and inhibit their expression.\n\nIn virology, the genome of an RNA virus can be said to be either positive-sense, also known as a \"plus-strand\", or negative-sense, also known as a \"minus-strand\". In most cases, the terms \"sense\" and \"strand\" are used interchangeably, making such terms as \"positive-strand\" equivalent to \"positive-sense\", and \"plus-strand\" equivalent to \"plus-sense\". Whether a virus genome is positive-sense or negative-sense can be used as a basis for classifying viruses.\n\nPositive-sense (5' to 3') viral RNA signifies that a particular viral RNA sequence may be directly translated into the desired viral proteins. Therefore, in positive-sense RNA viruses, the viral RNA genome can be considered viral mRNA, and can be immediately translated by the host cell. Unlike negative-sense RNA, positive-sense RNA is of the same sense as mRNA. Some viruses (e.g., Coronaviridae) have positive-sense genomes that can act as mRNA and be used directly to synthesize proteins without the help of a complementary RNA intermediate. Because of this, these viruses do not need to have an RNA polymerase packaged into the virion.\n\nNegative-sense (3' to 5') viral RNA is complementary to the viral mRNA and thus from it a positive-sense RNA must be produced by an RNA-dependent RNA polymerase prior to translation. Negative-sense RNA (like DNA) has a nucleotide sequence complementary to the mRNA that it encodes. Like DNA, this RNA cannot be translated into protein directly. Instead, it must first be transcribed into a positive-sense RNA that acts as an mRNA. Some viruses (Influenza, for example) have negative-sense genomes and so must carry an RNA polymerase inside the virion.\n\nGene silencing can be achieved by introducing into cells a short \"antisense oligonucleotide\" that is complementary to an RNA target. This experiment was first done by Zamecnik and Stephenson in 1978 and continues to be a useful approach, both for laboratory experiments and potentially for clinical applications (antisense therapy).\n\nIf the antisense oligonucleotide contains a stretch of DNA or a DNA mimic (phosphorothioate DNA, 2'F-ANA, or others) it can recruit RNase H to degrade the target RNA. This makes the mechanism of gene silencing catalytic. Double-stranded RNA can also act as a catalytic, enzyme-dependent antisense agent through the RNAi/siRNA pathway, involving target mRNA recognition through sense-antisense strand pairing followed by target mRNA degradation by the RNA-induced silencing complex (RISC). The R1 plasmid hok/sok system provides yet another example of an enzyme-dependent antisense regulation process through enzymatic degradation of the resulting RNA duplex.\n\nOther antisense mechanisms are not enzyme-dependent, but involve steric blocking of their target RNA (e.g. to prevent translation or induce alternative splicing). Steric blocking antisense mechanisms often use oligonucleotides that are heavily modified. Since there is no need for RNase H recognition, this can include chemistries such as 2'-O-alkyl, peptide nucleic acid (PNA), locked nucleic acid (LNA), and Morpholino oligomers.\n\n", "id": "5504842", "title": "Sense (molecular biology)"}
{"url": "https://en.wikipedia.org/wiki?curid=3633250", "text": "Meiotic drive\n\nMeiotic drive is a type of intragenomic conflict, whereby one or more loci within a genome will effect a manipulation of the meiotic process in such a way as to favor the transmission of one or more alleles over another, regardless of its phenotypic expression. More simply, meiotic drive is when one copy of a gene is passed on to offspring more than the expected 50% of the time. \n\nAccording to Buckler et al., \"Meiotic drive is the subversion of meiosis so that particular genes are preferentially transmitted to the progeny. Meiotic drive generally causes the preferential segregation of small regions of the genome\".\n\nA recent study by John Didion and Fernando Pardo-Manuel de Villena found evidence of a gene in mice (r2d2 – responder to meiotic drive 2) that is passed on more than 50% of the time. Gregor Mendel's First and Second Laws (the law of segregation and the law of independent assortment) tell us that there is a random chance of each allele being passed on to offspring, but selfish genes seem to break these laws.\n\nFixed allele\n", "id": "3633250", "title": "Meiotic drive"}
{"url": "https://en.wikipedia.org/wiki?curid=41185140", "text": "Phenotypic integration\n\nPhenotypic Integration is the term used to describe when multiple functionally-related traits are correlated with each other. Complex phenotypes often require multiple traits working together in order to function properly. Phenotypic integration is significant because it provides an explanation as to how phenotypes are sustained by relationships between traits. Every aspect of an organism is created so that the role it plays overall is performed harmoniously with all its other parts. Every organism's phenotype is integrated, organized, and a functional whole. Integration is also associated with functional modules. Modules are complex character units that are tightly associated, such as a flower. It is hypothesized that organisms with high correlations between traits in a module have the most efficient functions. The fitness of a particular value for one phenotypic trait frequently depends on the value of the other phenotypic traits, making it important for those traits evolve together. One trait can have a direct effect on fitness, and it has been shown that the correlations among traits can also change fitness, causing these correlations to be adaptive, rather than solely genetic. Integration can be involved in multiple aspects of life, not just at the genetic level, but during development, or simply at a functional level. Integration can be caused by genetic, developmental, environmental, or physiological relationships among characters. Environmental conditions can alter or cause integration, i.e. they may be plastic. Correlational selection, a form of natural selection can also produce integration. At the genetic level, integration can be caused by pleiotropy, close linkage, or linkage disequilibrium among unlinked genes. At the developmental level it can be due to cell-cell signaling such as in the development of the ectopic eyes in Drosophila. It is believed that the patterns of genetic covariance helped distinguish certain species. It can create variation among certain phenotypes, and can facilitate efficiency. This is significant because integration may play a huge role in phenotypic evolution. Phenotypic integration and its evolution can not only create large amounts of variety among phenotypes which can cause variation among species. For example, the color patterns on Garter snakes range widely and are caused by the covariance among multiple phenotypes.\n\nShortly after the structure of DNA was uncovered, Olson and Miller (1958) wrote the first book regarding the topic of phenotypic integration. The term integration was first used by Olson and Miller to describe correlations among characters that are influenced by selection. Following Olson and Miller, botanical studies on coherence between characters were done spanning over many years. Its first expansion was in the construction of a morphological integration genetic model constructed by Lande (1980). However, the term \"Phenotypic Integration\" was first coined by Pigliucci and Preston, in their book, \"Phenotypic Integration\", which helped elucidate the observed laws of correlation and some theoretical issues regarding the topic.\n\nPhenotypic Integration can be favorable or unfavorable with respect to natural selection. It has been shown that certain combinations of correlated traits can be unfavorable to an organism. In an ontogenetic study of laboratory rats, certain covariances among developmental characters which produced differing functions in the skull and limb were less favorable than another set that contributed to skull and limb structure. The most common form of selection on phenotypic integration is correlational selection. Correlational selection is a form of natural selection that favors certain combinations of traits (phenotypic integration). It can promote both genetic correlations and high levels of genetic variation. It has even been found that correlational selection may be the most common form of natural selection. Occasionally, this form of selection will favor a group of traits at the expense of others and if it does favor a particular set of traits it will include the most used traits whose functional effectiveness is essential for their ability to work together, and whose successful interaction is needed for the fitness of the individual. Phenotypic integration may be the adaptive product of correlational selection. An example of natural selection favoring integration is in the color patterns and escape mechanisms of the Garter snake, \"Thamnophis Ordinoides\". Another example is in plants that have highly-specific pollinators, natural selection favors plants that have highly specialized flowering to pair with the specific pollinators, and therefore high floral integration.\n\nIntegration can be found at the genetic level due to gene linkage. Gene linkage involves multiple genes being inherited together during meiosis because they are close to each other on the same chromosome. Alleles at different loci can be inherited together if they are tightly linked. Large genetic correlations can only be upheld if the loci that influence different characters are tightly linked, or if high levels of inbreeding in the population occur. Even if selection favors the correlations, it will not be maintained unless those conditions are met. Selection will favor tight linkage because it is maintained better. Poorly linked genetic correlations will not last. Transposition allows the loci at different locations on the chromosome to move so that they can become close to each other and be inherited together. This is significant to understanding the relationship between phenotypic integration and evolution because it is one of the mechanisms of how multiple traits that are connected to each other to evolve and change together. For instance, the Papilio dardanus butterflies come in three different forms, each mimicking a different distasteful butterfly species. Multiple loci contribute to these different forms, and a butterfly with alleles for form A at one locus and B at another locus would have poor fitness. However, the multiple loci are tightly linked, so they are inherited together as a single allele. Through transposition, these multiple loci ended up close to each other. Mutations among these linked genes are the nonadaptive fuel which can create evolution. Evolution may also occur because the integration may have an adaptive advantage in a particular environment for an organism. It is also important to recognize that not only can the traits be inherited together, but inherited separately and selected together. Another important example of phenotypic integration evolving over time is the relationship between the neurocranium and the brain. Over the last 150 million years the number of bones in the brain has decreased while the size of the brain in mammals has changed. Integration between the brain and the skull has evolved over this time period to reduce the number of bones in the cranium, while increasing the size of the brain. This relationship between correlated traits has played an important role in the evolution of mammalian cranium structure and brain size. Finally, development is another crucial cause of phenotypic integration that has evolved over time. Cell-signaling pathways which utilize integration in the form of complex interactions among specific cells in the pathway are crucial to proper development in many organisms. The interactions among the cells in the pathway, and the interaction of the pathways with other pathways have evolved over time to create complex structures.\n\nRecent studies on fruit have shown that phenotypic integration was high in fleshy fruits that were consumed by birds. Traits among the fruits consumed by birds were more integrated than the traits in fruits consumed solely by mammals. This illustrates major groups of frugivores can affect the covariance among certain fruit traits differently and so influence fruit diversification. Diversity caused by phenotypic integration is significant because it provides another explanation as to why there are so many different species found today.\n\nAposematism in poison frogs has also shown that phenotypic integration may be involved. Aposematism is the use of warning colors to deter predators because it often conveys the organism being poisonous, and this study found that diet specialization, and chemical defense are integrated and help affect aposematism. \n\nIn another study regarding the relationship of sexual ornaments and phenotypic integration, there seems to be a paradox where sexual traits are expected to be both less integrated for greater expression, and more integrated to better indicate physiological quality. However, in the case of the house finch, the female house finches select for males based on their likelihood to be a good parent. The females base their choice of male parental behaviors on the elaboration of the male’s sexual ornamentation. Thus, female choice favors hormonally controlled integration of male sexual behaviors and male sexual ornamentation.\n\nPhylogenetically consistent patterns of phenotypic integration have also been recently reported in leaves, floral morphology, and dry fruits as well as in the morphology of some animal organs.\n\nUnderstanding phenotypic integration will continue as more research and understanding is done with regards to genetic, developmental, and physiological mechanisms, and learn more about the relationship of selection and complex phenotypes. Research of this topic can even be beneficial to modern biomedicine.\n", "id": "41185140", "title": "Phenotypic integration"}
{"url": "https://en.wikipedia.org/wiki?curid=37844765", "text": "Nik operon\n\nThe \"nik\" operon is an operon required for uptake of nickel ions. It is present in many bacteria, but has been extensively studied in \"Helicobacter pylori\". Nickel is an essential nutrient for many microorganisms, where it participates in a variety of cellular processes. Excessive levels of nickel ions in cell can be fatal to the cell. Nickel ion concentration in the cell is regulated through the \"nik\" operon.\n\nThe \"nik\" operon consists of six genes. The first five genes \"nikABCDE\" encode components of a typical ABC transport system and the last gene \"nikR\" encodes a DNA-binding protein that represses transcription of \"nikABCDE\" when sufficient Ni is present. The \"nikR\" gene is located 5 bp downstream of the end of \"nikE\", transcribed in the same direction as \"nikABCDE\". The following table summarizes the structure of the \"nik\" operon:\n\nRegulation of expression of the \"nikR\" gene is achieved by two promoters. The first is through the FNR regulon. The FNR controlled regulation of \"nikABCDE–nikR\" occurs at a FNR box located upstream of \"nikA\" at a putative NikR binding site. The second promoter element regulating \"nikR\" expression occurs 51 bp upstream of the \"nikR\" transcription start site and results in low-level constitutive expression. There is also evidence that \"nikR\" expression is partially autoregulated.\n\nNi is taken up into prokaryotic cells by one of two types of high-affinity transport systems. The first method involves ABC-type transporters (discussed in this article) and the second mechanism makes use of transition-metal permeases (such as HoxN of \"Ralstonia eutropha\"). The ABC-type transporter system consists of five proteins, NikA–E, that carry out the ATP-dependent transport of Ni. NikA is a soluble, periplasmic, Ni-binding protein; NikB and NikC form a transmembrane pore for passage of Ni; and NikD and NikE hydrolyze ATP and couple this energy to Ni-transport. When Ni is available in excess, NikR protein represses transcription of \"nikABCDE\".\n\nUsing profile-based sequence database searches, NikR was shown to be a member of the ribbon-helix-helix (RHH) family of transcription factors. It has been demonstrated that the N-terminal domain of NikR is responsible for binding to DNA and that it only binds in presence of Ni. NikR has two sites for binding to Ni ions. Binding of Ni at concentrations that allow full occupancy of only the high-affinity sites is sufficient for operator binding, but the affinity for the operator is increased 1000-fold and the operator footprints are larger when both nickel-binding sites are occupied. These results, combined with estimates of intracellular Ni and NikR concentrations, lead to the conclusion that NikR is able to sense Ni and regulate the \"nik\" operon expression over a range of intracellular Ni concentrations from as low as one to as high as 10,000 molecules per cell.\n\n", "id": "37844765", "title": "Nik operon"}
{"url": "https://en.wikipedia.org/wiki?curid=623866", "text": "Quantitative trait locus\n\nA quantitative trait locus (QTL) is a section of DNA (the locus) which correlates with variation in a phenotype (the quantitative trait). Usually the QTL is linked to, or contains, the genes which control that phenotype. QTLs are mapped by identifying which molecular markers (such as SNPs or AFLPs) correlate with an observed trait. This is often an early step in identifying and sequencing the actual genes that cause the trait variation.\n\nA quantitative trait locus (QTL) is a region of DNA which is associated with a particular phenotypic trait, which varies in degree and which can be attributed to polygenic effects, i.e., the product of two or more genes, and their environment. These QTLs are often found on different chromosomes. The number of QTLs which explain variation in the phenotypic trait indicates the genetic architecture of a trait. It may indicate that plant height is controlled by many genes of small effect, or by a few genes of large effect.\n\nTypically, QTLs underlie continuous traits (those traits which vary continuously, e.g. height) as opposed to discrete traits (traits that have two or several character values, e.g. red hair in humans, a recessive trait, or smooth vs. wrinkled peas used by Mendel in his experiments).\n\nMoreover, a single phenotypic trait is usually determined by many genes. Consequently, many QTLs are associated with a single trait.\nAnother use of QTLs is to identify candidate genes underlying a trait. Once a region of DNA is identified as contributing to a phenotype, it can be sequenced. The DNA sequence of any genes in this region can then be compared to a database of DNA for genes whose function is already known.\n\nMendelian inheritance was rediscovered at the beginning of the 20th century, and as Mendel's ideas spread geneticists began to connect Mendel's rules of inheritance of single factors to Darwinian evolution. For early geneticists, it was not immediately clear that the smooth variation in traits like body size (i.e., Incomplete Dominance) was caused by the inheritance of single genetic factors. Although Darwin himself observed that inbred features of fancy pigeons were inherited in accordance with Mendel's laws (although Darwin didn't actually know about Mendel's ideas when he made the observation), it was not obvious that these features selected by fancy pigeon breeders can similarly explain quantitative variation in nature.\n\nAn early attempt by William Ernest Castle to unify the laws of Mendelian inheritance with Darwin's theory of speciation invoked the idea that species become distinct from one another as one species or the other acquires a novel Mendelian factor. Castle's conclusion was based on the observation that novel traits that could be studied in the lab and that show Mendelian inheritance patterns reflect a large deviation from the wild type, and Castle believed that acquisition of such features is the basis of \"discontinuous variation\" that characterizes speciation. Interestingly, Darwin discussed the inheritance of similar mutant features but did not invoke them as a requirement of speciation. Instead Darwin used the emergence of such features in breeding populations as evidence that mutation can occur at random within breeding populations, which is a central premise of his model of selection in nature. Later in his career, Castle would refine his model for speciation to allow for small variation to contribute to speciation over time. He also was able to demonstrate this point by selectively breeding laboratory populations of rats to obtain a hooded phenotype over several generations.\n\nCastle's was perhaps the first attempt made in the scientific literature to direct evolution by artificial selection of a trait with continuous underlying variation, however the practice had previously been widely employed in the development of agriculture to obtain livestock or plants with favorable features from populations that show quantitative variation in traits like body size or grain yield.\n\nCastle's work was among the first to attempt to unify the recently rediscovered laws of Mendelian inheritance with Darwin's theory of evolution. Still, it would be almost thirty years until the theoretical framework for evolution of complex traits would be widely formalized. In an early summary of the theory of evolution of continuous variation, Sewall Wright, a graduate student who trained under Castle, summarized contemporary thinking about the genetic basis of quantitative natural variation: \"As genetic studies continued, ever smaller differences were found to mendelize, and any character, sufficiently investigated, turned out to be affected by many factors.\" Wright and others formalized population genetics theory that had been worked out over the preceding 30 years explaining how such traits can be inherited and create stably breeding populations with unique characteristics. Quantitative trait genetics today leverages Wright's observations about the statistical relationship between genotype and phenotype in families and populations to understand how certain genetic features can affect variation in natural and derived populations.\n\nPolygenic inheritance refers to inheritance of a phenotypic characteristic (trait) that is attributable to two or more genes and can be measured quantitatively. Multifactorial inheritance refers to polygenic inheritance that also includes interactions with the environment. Unlike monogenic traits, polygenic traits do not follow patterns of Mendelian inheritance (discrete categories). Instead, their phenotypes typically vary along a continuous gradient depicted by a bell curve.\n\nAn example of a polygenic trait is human skin color variation. Several genes factor into determining a person's natural skin color, so modifying only one of those genes can change skin color slightly or in some cases, such as for SLC24A5, moderately. Many disorders with genetic components are polygenic, including autism, cancer, diabetes and numerous others. Most phenotypic characteristics are the result of the interaction of multiple genes.\n\nExamples of disease processes generally considered to be results of many contributing factors:\n\nCongenital malformation\nAdult onset diseases\n\nMultifactorially inherited diseases are said to constitute the majority of genetic disorders affecting humans which will result in hospitalization or special care of some kind.\n\nTraits controlled by the both environment and genetic factors.\nUsually, multifactorial traits outside of illness result in what we see as continuous characteristics in organisms, especially human organisms such as: height, skin color, and body mass. All of these phenotypes are complicated by a great deal of give-and-take between genes and environmental effects. The continuous distribution of traits such as height and skin color described above, reflects the action of genes that do not manifest typical patterns of dominance and recessiveness. Instead the contributions of each involved locus are thought to be additive. Writers have distinguished this kind of inheritance as \"polygenic\", or \"quantitative inheritance\".\n\nThus, due to the nature of polygenic traits, inheritance will not follow the same pattern as a simple monohybrid or dihybrid cross. Polygenic inheritance can be explained as Mendelian inheritance at many loci, resulting in a trait which is normally-distributed. If \"n\" is the number of involved loci, then the coefficients of the binomial expansion of (\"a\" + \"b\") will give the frequency of distribution of all \"n\" allele combinations. For a sufficiently high values of \"n\", this binomial distribution will begin to resemble a normal distribution. From this viewpoint, a disease state will become apparent at one of the tails of the distribution, past some threshold value. Disease states of increasing severity will be expected the further one goes past the threshold and away from the mean.\n\nA mutation resulting in a disease state is often recessive, so both alleles must be mutant in order for the disease to be expressed phenotypically. A disease or syndrome may also be the result of the expression of mutant alleles at more than one locus. When more than one gene is involved, with or without the presence of environmental triggers, we say that the disease is the result of multifactorial inheritance.\n\nThe more genes involved in the cross, the more the distribution of the genotypes will resemble a normal, or Gaussian distribution. This shows that multifactorial inheritance is polygenic, and genetic frequencies can be predicted by way of a polyhybrid Mendelian cross. Phenotypic frequencies are a different matter, especially if they are complicated by environmental factors.\n\nThe paradigm of polygenic inheritance as being used to define multifactorial disease has encountered much disagreement. Turnpenny (2004) discusses how simple polygenic inheritance cannot explain some diseases such as the onset of Type I diabetes mellitus, and that in cases such as these, not all genes are thought to make an equal contribution.\n\nThe assumption of polygenic inheritance is that all involved loci make an equal contribution to the symptoms of the disease. This should result in a normal curve distribution of genotypes. When it does not, the idea of polygenetic inheritance cannot be supported for that illness.\n\nThe above are well-known examples of diseases having both genetic and environmental components. Other examples involve atopic diseases such as eczema or dermatitis;spina bifida (open spine), and anencephaly (open skull).\n\nWhile schizophrenia is widely believed to be multifactorially genetic by biopsychiatrists, no characteristic genetic markers have been determined with any certainty.\n\nIf it is shown that the brothers and sisters of the patient have the disease, then there is a strong chance that the disease is genetic and that the patient will also be a genetic carrier. This is not quite enough as it also needs to be proven that the pattern of inheritance is non-Mendelian. This would require studying dozens, even hundreds of different family pedigrees before a conclusion of multifactorial inheritance is drawn. This often takes several years.\n\nIf multifactorial inheritance is indeed the case, then the chance of the patient contracting the disease is reduced only if cousins and more distant relatives have the disease. It must be stated that while multifactorially-inherited diseases tend to run in families, inheritance will not follow the same pattern as a simple monohybrid or dihybrid cross.\n\nIf a genetic cause is suspected and little else is known about the illness, then it remains to be seen exactly how many genes are involved in the phenotypic expression of the disease. Once that is determined, the question must be answered: if two people have the required genes, why are there differences in expression between them? Generally, what makes the two individuals different are likely to be environmental factors. Due to the involved nature of genetic investigations needed to determine such inheritance patterns, this is not usually the first avenue of investigation one would choose to determine etiology.\n\nFor organisms whose genomes are known, one might now try to exclude genes in the identified region whose function is known with some certainty not to be connected with the trait in question. If the genome is not available, it may be an option to sequence the identified region and determine the putative functions of genes by their similarity to genes with known function, usually in other genomes. This can be done using BLAST, an online tool that allows users to enter a primary sequence and search for similar sequences within the BLAST database of genes from various organisms. It is often not the actual gene underlying the phenotypic trait, but rather a region of DNA that is closely linked with the gene.\n\nAnother interest of statistical geneticists using QTL mapping is to determine the complexity of the genetic architecture underlying a phenotypic trait. For example, they may be interested in knowing whether a phenotype is shaped by many independent loci, or by a few loci, and do those loci interact. This can provide information on how the phenotype may be evolving.\n\nIn a recent development, classical QTL analyses were combined with gene expression profiling i.e. by DNA microarrays. Such expression QTLs (eQTLs) describe cis- and trans-controlling elements for the expression of often disease-associated genes. Observed epistatic effects have been found beneficial to identify the gene responsible by a cross-validation of genes within the interacting loci with metabolic pathway- and scientific literature databases.\n\nThe simplest method for QTL mapping is analysis of variance (ANOVA, sometimes called \"marker regression\") at the marker loci. In this method, in a backcross, one may calculate a t-statistic to compare the averages of the two marker genotype groups. For other types of\ncrosses (such as the intercross), where there are more than two possible genotypes, one uses a more general form of ANOVA, which provides a so-called F-statistic. The ANOVA approach for QTL mapping has three important weaknesses. First, we do not receive separate estimates of QTL location and QTL effect. QTL location is indicated only by looking at which markers give the greatest differences between genotype group averages, and the apparent QTL effect at a marker will be smaller than the true QTL effect as a result of recombination between the marker and the QTL. Second, we must discard individuals whose genotypes are missing at the marker. Third, when the markers are widely spaced, the QTL may be quite far from all markers, and so the power for QTL detection will decrease.\n\nLander and Botstein developed interval mapping, which overcomes the three disadvantages of analysis of variance at marker loci. Interval mapping is currently the most popular approach for QTL mapping in experimental crosses. The method makes use of a genetic map of the typed markers, and, like analysis of variance, assumes the presence of a single QTL. In interval mapping, each locus is considered one at a time and the logarithm of the odds ratio (LOD score) is calculated for the model that the given locus is a true QTL. The odds ratio is related to the Pearson correlation coefficient between the phenotype and the marker genotype for each individual in the experimental cross.\n\nThe term ‘interval mapping’ is used for estimating the position of a QTL within two markers (often indicated as ‘marker-bracket’). Interval mapping is originally based on the maximum likelihood but there are also very good approximations possible with simple regression.\n\nThe principle for QTL mapping is:\n1) The Likelihood can be calculated for a given set of parameters (particularly QTL effect and QTL position) given the observed data on phenotypes and marker genotypes.\n2) The estimates for the parameters are those where the likelihood are highest.\n3) A significance threshold can be established by permutation testing.\n\nConventional methods for the detection of quantitative trait loci (QTLs) are based on a comparison of single QTL models with a model assuming no QTL. For instance in the “interval mapping” method the likelihood for a single putative QTL is assessed at each location on the genome. However, QTLs located elsewhere on the genome can have an interfering effect. As a consequence, the power of detection may be compromised, and the estimates of locations and effects of QTLs may be biased (LANDER and BOTSTEIN 1989; KNAPP 1991). Even nonexisting so-called “ghost” QTLs may appear (HALEY and KNOTT 1992; MARTINEZ and CURNOW 1992). Therefore, it is obvious that multiple QTLs could be mapped more efficiently and more accurately by using multiple QTL models. One popular approach to handle QTL mapping where multiple QTL contribute to a trait is to iteratively scan the genome and add known QTL to the regression model as QTLs are identified. This method, termed composite interval mapping determine both the location and effects size of QTL more accurately than single-QTL approaches, especially in small mapping populations where the effect of correlation between genotypes in the mapping population may be problematic.\n\nIn this method, one performs interval mapping using a subset of marker loci as covariates. These markers serve as proxies for other QTLs to increase the resolution of interval mapping, by accounting for linked QTLs and reducing the residual variation. The key problem with CIM concerns the choice of suitable marker loci to serve as covariates; once these have been chosen, CIM turns the model selection problem into a single-dimensional scan. The choice\nof marker covariates has not been solved, however. Not surprisingly, the appropriate markers are those closest to the true QTLs, and so if one could find these, the QTL mapping problem\nwould be complete anyway.\n\nFamily based QTL mapping, or Family-pedigree based mapping (Linkage and association mapping), involves multiple families instead of a single family. Family based QTL mapping has been the only way for mapping of genes where experimental crosses are difficult to make. However, due to some advantages, now plant geneticists are attempting to incorporate some of the methods pioneered in human genetics. Using family-pedigree based approach has been discussed (Bink et al. 2008). Family-based linkage and association has been successfully implemented (Rosyara et al. 2009)\n\n\nEuphytica 2008, 161:85–96.\n\n", "id": "623866", "title": "Quantitative trait locus"}
{"url": "https://en.wikipedia.org/wiki?curid=37122597", "text": "Behavioral epigenetics\n\nBehavioral epigenetics is the field of study examining the role of epigenetics in shaping animal (including human) behaviour. It is an experimental science that seeks to explain how nurture shapes nature, where nature refers to biological heredity and nurture refers to virtually everything that occurs during the life-span (e.g., social-experience, diet and nutrition, and exposure to toxins). Behavioral epigenetics attempts to provide a framework for understanding how the expression of genes is influenced by experiences and the environment to produce individual differences in behaviour, cognition, personality, and mental health.\n\nEpigenetic gene regulation involves changes other than to the sequence of DNA and includes changes to histones (proteins around which DNA is wrapped) and DNA methylation. These epigenetic changes can influence the growth of neurons in the developing brain as well as modify activity of the neurons in the adult brain. Together, these epigenetic changes on neuron structure and function can have a marked influence on an organism's behavior.\n\nIn biology, and specifically genetics, epigenetics is the study of heritable changes in gene activity which are \"not\" caused by changes in the DNA sequence; the term can also be used to describe the study of stable, long-term alterations in the transcriptional potential of a cell that are not necessarily heritable.\n\nExamples of mechanisms that produce such changes are DNA methylation and histone modification, each of which alters how genes are expressed without altering the underlying DNA sequence. Gene expression can be controlled through the action of repressor proteins that attach to silencer regions of the DNA. \nDNA methylation turns a gene \"off\" – it results in the inability of genetic information to be read from DNA; removing the methyl tag can turn the gene back \"on\".\n\nEpigenetics has a strong influence on the development of an organism and can alter the expression of individual traits. Epigenetic changes occur not only in the developing fetus, but also in individuals throughout the human life-span. Because some epigenetic modifications can be passed from one generation to the next, subsequent generations may be affected by the epigenetic changes that took place in the parents.\n\nThe first documented example of epigenetics affecting behavior was provided by Michael Meaney and Moshe Szyf. While working at McGill University in Montréal in 2004, they discovered that the type and amount of nurturing a mother rat provides in the early weeks of the rat's infancy determines how that rat responds to stress later in life. This stress sensitivity was linked to a down-regulation in the expression of the glucocorticoid receptor in the brain. In turn, this down-regulation was found to be a consequence of the extent of methylation in the promoter region of the glucocorticoid receptor gene. Immediately after birth, Meaney and Szyf found that methyl groups repress the glucocorticoid receptor gene in all rat pups, making the gene unable to unwind from the histone in order to be transcribed, causing a decreased stress response. Nurturing behaviours from the mother rat were found to stimulate activation of stress signalling pathways that remove methyl groups from DNA. This releases the tightly wound gene, exposing it for transcription. The glucocorticoid gene is activated, resulting in lowered stress response. Rat pups that receive a less nurturing upbringing are more sensitive to stress throughout their life-span.\n\nThis pioneering work in rodents has been difficult to replicate in humans because of a general lack of availability human brain tissue for measurement of epigenetic changes.\n\nIn a small clinical study in humans published in 2008, epigenetic differences were linked to differences in risk-taking and reactions to stress in monozygotic twins. The study identified twins with different life paths, wherein one twin displayed risk-taking behaviours, and the other displayed risk-averse behaviours. Epigenetic differences in DNA methylation of the CpG islands proximal to the DLX1 gene correlated with the differing behavior. The authors of the twin study noted that despite the associations between epigenetic markers and differences personality traits, epigenetics cannot predict complex decision-making processes like career selection.\n\nAnimal and human studies have found correlations between poor care during infancy and epigenetic changes that correlate with long-term impairments that result from neglect.\n\nStudies in rats have shown correlations between maternal care in terms of the parental licking of offspring and epigenetic changes. A high level of licking results in a long-term reduction in stress response as measured behaviorally and biochemically in elements of the hypothalamic-pituitary-adrenal axis (HPA). Further, decreased DNA methylation of the glucocorticoid receptor gene were found in offspring that experienced a high level of licking; the glucorticoid receptor plays a key role in regulating the HPA. The opposite is found in offspring that experienced low levels of licking, and when pups are switched, the epigenetic changes are reversed. This research provides evidence for an underlying epigenetic mechanism. Further support comes from experiments with the same setup, using drugs that can increase or decrease methylation. Finally, epigenetic variations in parental care can be passed down from one generation to the next, from mother to female offspring. Female offspring who received increased parental care (i.e., high licking) became mothers who engaged in high licking and offspring who received less licking became mothers who engaged in less licking.\n\nIn humans, a small clinical research study showed the relationship between prenatal exposure to maternal mood and genetic expression resulting in increased reactivity to stress in offspring. Three groups of infants were examined: those born to mothers medicated for depression with serotonin reuptake inhibitors; those born to depressed mothers not being treated for depression; and those born to non-depressed mothers. Prenatal exposure to depressed/anxious mood was associated with increased DNA methylation at the glucocorticoid receptor gene and to increased HPA axis stress reactivity. The findings were independent of whether the mothers were being pharmaceutically treated for depression.\n\nRecent research has also shown the relationship of methylation of the maternal glucocorticoid receptor and maternal neural activity in response to mother-infant interactions on video. Longitudinal follow-up of those infants will be important to understand the impact of early caregiving in this high-risk population on child epigenetics and behavior.\n\nA 2010 review discusses the role of DNA methylation in memory formation and storage, but the precise mechanisms involving neuronal function, memory, and methylation reversal remain unclear.\n\nStudies in rodents have found that the environment exerts an influence on epigenetic changes related to cognition, in terms of learning and memory; environmental enrichment correlated with increased histone acetylation, and verification by administering histone deacetylase inhibitors induced sprouting of dendrites, an increased number of synapses, and reinstated learning behaviour and access to long-term memories. Research has also linked learning and long-term memory formation to reversible epigenetic changes in the hippocampus and cortex in animals with normal-functioning, non-damaged brains. In human studies, post-mortem brains from Alzheimer's patients show increased histone de-acetylase levels.\n\nEnvironmental and epigenetic influences seem to work together to increase the risk of addiction. For example, environmental stress has been shown to increase the risk of substance abuse. In an attempt to cope with stress, alcohol and drugs can be used as an escape. Once substance abuse commences, however, epigenetic alterations may further exacerbate the biological and behavioural changes associated with addiction.\n\nEven short-term substance abuse can produce long-lasting epigenetic changes in the brain of rodents, via DNA methylation and histone modification. Epigenetic modifications have been observed in studies on rodents involving ethanol, nicotine, cocaine, amphetamine, methamphetamine and opiates. Specifically, these epigenetic changes modify gene expression, which in turn increases the vulnerability of an individual to engage in repeated substance overdose in the future. In turn, increased substance abuse results in even greater epigenetic changes in various components of a rodent's reward system (e.g., in the nucleus accumbens). Hence, a cycle emerges whereby changes in the pleasure-reward areas contribute to the long-lasting neural and behavioural changes associated with the increased likelihood of addiction, the maintenance of addiction and relapse. In humans, alcohol consumption has been shown to produce epigenetic changes that contribute to the increased craving of alcohol. As such, epigenetic modifications may play a part in the progression from the controlled intake to the loss of control of alcohol consumption. These alterations may be long-term, as is evidenced in smokers who still possess nicotine-related epigenetic changes ten years after cessation. Therefore, epigenetic modifications may account for some of the behavioural changes generally associated with addiction. These include: repetitive habits that increase the risk of disease, and personal and social problems; need for immediate gratification; high rates of relapse following treatment; and, the feeling of loss of control.\n\nEvidence for related epigenetic changes has come from human studies involving alcohol, nicotine, and opiate abuse. Evidence for epigenetic changes stemming from amphetamine and cocaine abuse derives from animal studies. In animals, drug-related epigenetic changes in fathers have also been shown to negatively affect offspring in terms of poorer spatial working memory, decreased attention and decreased cerebral volume.\n\nEpigenetic changes may help to facilitate the development and maintenance of eating disorders via influences in the early environment and throughout the life-span. Pre-natal epigenetic changes due to maternal stress, behaviour and diet may later predispose offspring to persistent, increased anxiety and anxiety disorders. These anxiety issues can precipitate the onset of eating disorders and obesity, and persist even after recovery from the eating disorders.\n\nEpigenetic differences accumulating over the life-span may account for the incongruent differences in eating disorders observed in monozygotic twins. At puberty, sex hormones may exert epigenetic changes (via DNA methylation) on gene expression, thus accounting for higher rates of eating disorders in men as compared to women. Overall, epigenetics contribute to persistent, unregulated self-control behaviours related to the urge to binge.\n\nEpigenetic changes including hypomethylation of glutamatergic genes (i.e., NMDA-receptor-subunit gene NR3B and the promoter of the AMPA-receptor-subunit gene GRIA2) in the post-mortem human brains of schizophrenics are associated with increased levels of the neurotransmitter glutamate. Since glutamate is the most prevalent, fast, excitatory neurotransmitter, increased levels may result in the psychotic episodes related to schizophrenia. Interestingly, epigenetic changes affecting a greater number of genes have been detected in men with schizophrenia as compared to women with the illness.\n\nPopulation studies have established a strong association linking schizophrenia in children born to older fathers. Specifically, children born to fathers over the age of 35 years are up to three times more likely to develop schizophrenia. Epigenetic dysfunction in human male sperm cells, affecting numerous genes, have been shown to increase with age. This provides a possible explanation for increased rates of the disease in men. To this end, toxins (e.g., air pollutants) have been shown to increase epigenetic differentiation. Animals exposed to ambient air from steel mills and highways show drastic epigenetic changes that persist after removal from the exposure. Therefore, similar epigenetic changes in older human fathers are likely. Schizophrenia studies provide evidence that the nature versus nurture debate in the field of psychopathology should be re-evaluated to accommodate the concept that genes and the environment work in tandem. As such, many other environmental factors (e.g., nutritional deficiencies and cannabis use) have been proposed to increase the susceptibility of psychotic disorders like schizophrenia via epigenetics.\n\nEvidence for epigenetic modifications for bipolar disorder is unclear. One study found hypomethylation of a gene promoter of a prefrontal lobe enzyme (i.e., membrane-bound catechol-O-methyl transferase, or COMT) in post-mortem brain samples from individuals with bipolar disorder. COMT is an enzyme that metabolizes dopamine in the synapse. These findings suggest that the hypomethylation of the promoter results in over-expression of the enzyme. In turn, this results in increased degradation of dopamine levels in the brain. These findings provide evidence that epigenetic modification in the prefrontal lobe is a risk factor for bipolar disorder. However, a second study found no epigenetic differences in post-mortem brains from bipolar individuals.\n\nThe causes of major depressive disorder (MDD) are poorly understood from a neuroscience perspective. The epigenetic changes leading to changes in glucocorticoid receptor expression and its effect on the HPA stress system discussed above, have also been applied to attempts to understand MDD.\n\nMuch of the work in animal models has focused on the indirect downregulation of brain derived neurotrophic factor (BDNF) by over-activation of the stress axis. Studies in various rodent models of depression, often involving induction of stress, have found direct epigenetic modulation of BDNF as well.\n\nEpigenetics may be relevant to aspects of psychopathic behaviour through methylation and histone modification. These processes are heritable but can also be influenced by environmental factors such as smoking and abuse. Epigenetics may be one of the mechanisms through which the environment can impact the expression of the genome. Studies have also linked methylation of genes associated with nicotine and alcohol dependence in women, ADHD, and drug abuse. It is probable that epigenetic regulation as well as methylation profiling will play an increasingly important role in the study of the play between the environment and genetics of psychopaths.\n\nA study of the brains of 24 suicide completers, 12 of whom had a history of child abuse and 12 who did not, found decreased levels of glucocorticoid receptor in victims of child abuse and associated epigenetic changes.\n\nSeveral studies have indicated DNA cytosine methylation linked to the social behavior of insects, such as honeybees and ants. In honeybees, when nurse bee switched from her in-hive tasks to out foraging, cytosine methylation marks are changing. When a forager bee was reversed to do nurse duties, the cytosine methylation marks were also reversed. Knocking down the DNMT3 in the larvae changed the worker to queen-like phenotype. Queen and worker are two distinguish castes with different morphology, behavior, and physiology. Studies in DNMT3 silencing also indicated DNA methylation may regulate gene alternative splicing and pre-mRNA maturation.\n\nMany researchers contribute information to the Human Epigenome Consortium. The aim of future research is to reprogram epigenetic changes to help with addiction, mental illness, age related changes, memory decline, and other issues. However, the sheer volume of consortium-based data makes analysis difficult. Most studies also focus on one gene. In actuality, many genes and interactions between them likely contribute to individual differences in personality, behaviour and health. As social scientists often work with many variables, determining the number of affected genes also poses methodological challenges. More collaboration between medical researchers, geneticists and social scientists has been advocated to increase knowledge in this field of study.\nLimited access to human brain tissue poses a challenge to conducting human research. Not yet knowing if epigenetic changes in the blood and (non-brain) tissues parallel modifications in the brain, places even greater reliance on brain research. Although some epigenetic studies have translated findings from animals to humans, some researchers caution about the extrapolation of animal studies to humans. One view notes that when animal studies do not consider how the subcellular and cellular components, organs and the entire individual interact with the influences of the environment, results are too reductive to explain behaviour.\n\nSome researchers note that epigenetic perspectives will likely be incorporated into pharmacological treatments. Others caution that more research is necessary as drugs are known to modify the activity of multiple genes and may, therefore, cause serious side effects. However, the ultimate goal is to find patterns of epigenetic changes that can be targeted to treat mental illness, and reverse the effects of childhood stressors, for example. If such treatable patterns eventually become well-established, the inability to access brains in living humans to identify them poses an obstacle to pharmacological treatment. Future research may also focus on epigenetic changes that mediate the impact of psychotherapy on personality and behaviour.\n\nMost epigenetic research is correlational; it merely establishes associations. More experimental research is necessary to help establish causation. Lack of resources has also limited the number of intergenerational studies. Therefore, advancing longitudinal and multigenerational, experience-dependent studies will be critical to further understanding the role of epigenetics in psychology.\n\n\n\n", "id": "37122597", "title": "Behavioral epigenetics"}
{"url": "https://en.wikipedia.org/wiki?curid=41443123", "text": "Bx1 benzoxazin1\n\n\"Based on submission by Monika Frey to the MaizeGDB and the Maize Genetics Cooperation Newsletter.\"\n\nFunction Maize gene for first step in biosynthesis of benzoxazin, which aids in resistance to insect pests, pathogenic fungi and bacteria.\n\nFirst report Hamilton 1964, as a mutant sensitive to the herbicide atrazine, and lacking benzoxazinoids (less than 1% of non-mutant plants).\n\nMutations in the bx1 gene reduce the resistance to first generation European corn borer that is conferred by benzoxazinoids. Molecular characterization reveals that the BX1 protein is a homologue to the alpha-subunit of tryptophan synthase. The reference mutant allele has a deletion of about 900 bp, located at the 5'-terminus and comprising sequence upstream of the transcription start site and the first exon. A second mutant allele is given by a \"Mu\" transposon insertion in the fourth exon (Frey et al. 1997 ). Gene sequence diversity analysis has been performed for 281 inbred lines of maize, and the results suggest that bx1 is responsible for much of the natural variation in DIMBOA (a benzoxazinoid compound) synthesis (Butron et al. 2010).\n\nAB chromosome translocation analyses place on short arm of chromosome 4 (4S; Simcox and Weber 1985 ). There is close linkage to other genes in the benzoxazinoid synthesis pathway [\"bx2, bx3, bx4, bx5\" Frey et al. 1995, 1997 ). Gene \"bx1\" is 2490 bp from \"bx2\" (Frey et al. 1997 ); between \"umc123\" and \"agrc94\" on 4S (Melanson et al. 1997 ). Mapping probes: SSR p-umc1022 (Sharopova et al. 2002 ); Overgo (physical map probe) PCO06449 (Gardiner et al. 2004 ).\n\nMutants are viable, but may be distinguished from normal plants by FeCl3 staining: plants able to synthesize benzoxinoids have pale blue color when crushed and treated with FeCl3 solutions (Hamilton 1964, Simcox 1993 )\n\nCatalyzes the first step in the synthesis of DIMBOA, forming indole from indole-3-glycerol phosphate. The enzyme is called indole-3-glycerol phosphate lyase, chloroplast, EC 4.1.2.8 and is located in the chloroplast. The X-ray structure of BX1 protein has been resolved and compared with bacterial TSA (tryptophan synthase alpha subunit, Kulik et al. 2005). Three homologs of the BX1 protein occur in maize. One is encoded by the gene \"tsa1\", \"tryptophan synthase alpha1\"(Frey et al. 1997, Melanson et al. 1997 ), on chromosome 7, another by \"igl1\", \"indole-3-glycerol phosphate1\"(Frey et al. 1997, on chromosome 1, and another by \"tsah1\", 'TSA like\" and located near the \"bx1\" gene (Frey et al. 1997. ).\n\n", "id": "41443123", "title": "Bx1 benzoxazin1"}
{"url": "https://en.wikipedia.org/wiki?curid=41442860", "text": "Benzoxazinone biosynthesis\n\nThe biosynthesis of benzoxazinone, a cyclic hydroxamate and a natural insecticide, has been well-characterized in maize and related grass species (Frey et al. 1997). In maize, genes in the pathway are named using the symbol \"bx\". Maize Bx-genes are tightly linked, a feature that has been considered uncommon for plant genes of a biosynthetic pathways. Especially notable are genes encoding the different enzymatic functions BX1, BX2 and BX8 and which are found within about 50 kilobases (Frey et al. 1995, 1997 ) Results from wheat and rye indicate that the cluster is an ancient feature (Nomura et al. 2003). In wheat the cluster is split into two parts. The wheat genes Bx1 and Bx2 are located in close proximity on chromosome 4 and wheat Bx3, Bx4 and Bx5 map to the short arm of chromosome 5; an additional Bx3 copy was detected on the long arm of chromosome 5B (Nomura et al. 2003). Recently, additional biosynthetic clusters have been detected in other plants for other biosynthetic pathways and this organization might be common in plants (Osbourn 2010).\n\nThe bx1 gene encodes a protein, BX1, that forms indol from indol-3-glycerol phosphate in the plastid. It is the first step in the pathway and determines much of the natural variation in levels of DIMBOA in maize. The next steps in the pathway occur in the endoplasmic reticulum, also referred to as the microsomes in cell fractionation experiments, and are carried by proteins encoded by genes bx2, bx3, bx4, and bx5.\n", "id": "41442860", "title": "Benzoxazinone biosynthesis"}
{"url": "https://en.wikipedia.org/wiki?curid=41502227", "text": "Ac/Ds Activator/Dissociation Transposable Element\n\nThe \"Ac\"/\"Ds\" transposable element system was the first transposable element system recognized in maize.The \"Ac\" \"Activator\" element is autonomous, whereas the \"Ds\" \"Dissociation\" element requires an \"Activator\" element to transpose. \"Ac\" was initially discovered as enabling a \"Ds\" element to break chromosomes. Both \"Ac\" and \"Ds\" can also insert into genes, causing mutants that may revert to normal on excision of the element. The phenotypic consequence of \"Ac\"/\"Ds\" transposable element includes mosaic colors in kernels and leaves in maize.\n\nIts discovery was based on studying its genetic behavior, i.e., \"jumping genes\" in maize and published by Barbara McClintock, leading to her 1983 Nobel Prize in Medicine. The Ac/Ds transposable elements were first isolated and sequenced By Federoff et al. 1983 using insertions of \"Ac\" and \"Ds\" into the well-studied Waxy(Wx1) gene. The elements have been shown to function in other plants, including tobacco (Baker et al. 1987 ), Arabidopsis (Van SLuys 1987) and rice (Murai et al. 1991).\n\nGenomic analysis of maize show that these elements, which share terminal 11 bp imperfect inverted repeat sequences, have much sequence heterogeneity, both in length and content. They also include a class of DNA elements that do not transpose in the presence of the Ac element (Du et al. 2011). The chromosome breaking property has been shown to come from pairs of closely positioned elements, reviewed by Huang and Dooner 2008.\n\nResearchers use mutant phenotypes to discover gene functions. Ac/Ds prefer to transpose to nearby genes, affording a way to mutagenize those regions of the genome, and by subsequent genetic crosses, remove the Ac that causes new mutants and instability of a Ds mutant. Collections of Ac/Ds elements that cover the genome and are useful for generating mutants. Application of Ac/Ds toolkits has also been applied to other species like arabidopsis, yeast, and even zebrafish.\n", "id": "41502227", "title": "Ac/Ds Activator/Dissociation Transposable Element"}
{"url": "https://en.wikipedia.org/wiki?curid=41529879", "text": "Arginine catabolic mobile element\n\nThe arginine catabolic mobile element (ACME) is a mobile genetic element of \"Staphylococcus\" bacterial species. This genetic element provides for several immune modulating functions, including resistance to polyamines which serve as a non-specific immune response both on intact skin and following the inflammatory response in wound healing. Diverse ACME are present in several species of \"Staphylococcus\", including \"Staphylococcus epidermidis\".\n\nACME are not common among antibiotic sensitive \"S. aureus\" (MSSA). The elements for the most prominent MRSA ACME appear to have assembled recently in \"S. epidermidis\" into the speG-positive ACME which was transferred to virulent \"S. aureus\" during the evolution of the epidemic USA300 MRSA strain. This broadened the ability of \"S. aureus\" to colonize sites other than a specific part of the nose. This strain is able to persist on intact skin and is spread rapidly person-to-person. As a result, the speG-positive ACME is a particularly important element of MRSA pathogenesis.\n\n", "id": "41529879", "title": "Arginine catabolic mobile element"}
{"url": "https://en.wikipedia.org/wiki?curid=41399630", "text": "Diversity panel\n\nA diversity panel is a collection of genetic material or individual samples taken from a diverse population of a certain species.\n\nDiversity panels exist for human populations, mouse and other organisms.\n\nResearchers in the area of genetics often use diversity panels in order to reveal genotypes that are linked to certain traits, such as in QTL mapping.\n\n", "id": "41399630", "title": "Diversity panel"}
{"url": "https://en.wikipedia.org/wiki?curid=1795277", "text": "Insertion (genetics)\n\nIn genetics, an insertion (also called an insertion mutation) is the addition of one or more nucleotide base pairs into a DNA sequence. This can often happen in Microsatellite regions due to the DNA polymerase slippage. Insertions can be anywhere in size from one base pair incorrectly inserted into a DNA sequence to a section of one chromosome inserted into another. The merchandise of the smallest singlehandedly base insertion mutations is believed to be through case-pair separation between the template and primer strands hollowed by non-neighbor base stacking, which can occur locally within the DNA polymerase active site. On a chromosome level, an \"insertion\" refers to the insertion of a larger sequence into a chromosome. This can happen due to unequal crossover during meiosis.\n\nN region addition is the addition of non-coded nucleotides during recombination by terminal deoxynucleotidyl transferase.\n\nP nucleotide insertion is the insertion of palindromic sequences encoded by the ends of the recombining gene segments.\n\nTrinucleotide repeats are classified as insertion mutations and sometimes as a separate class of mutations.\n\nInsertions can be particularly hazardous if they occur in an exon, the amino acid coding region of a gene. A frameshift mutation, an alteration in the normal reading frame of a gene, results if the number of inserted nucleotides is not divisible by three, i.e., the number of nucleotides per codon. Frameshift mutations will alter all the amino acids encoded by the gene following the mutation. Usually, insertions and the subsequent frameshift mutation will cause the active translation of the gene to encounter a premature stop codon, resulting in an end to translation and the production of a truncated protein. Transcripts carrying the frameshift mutation may also be degraded through Nonsense-mediated decay during translation, thus not resulting in any protein product. If translated, the truncated proteins frequently are unable to function properly or at all and can possibly result in any number of genetic disorders depending on the gene in which the insertion occurs. Methods to detect DNA sequencing errors were developed.\n\nIn-frame insertions occur when the reading frame is not altered as a result of the insertion; the number of inserted nucleotides is divisible by three. The reading frame remains intact after the insertion and translation will most likely run to completion if the inserted nucleotides do not code for a stop codon. However, because of the inserted nucleotides, the finished protein will contain, depending on the size of the insertion, multiple new amino acids that may affect the function of the protein.\n\n", "id": "1795277", "title": "Insertion (genetics)"}
{"url": "https://en.wikipedia.org/wiki?curid=1547778", "text": "Identity by descent\n\nA DNA segment is identical by state (IBS) in two or more individuals if they have identical nucleotide sequences in this segment. An IBS segment is identical by descent (IBD) (note: the acronym IBD is also used for another concept in population genetics, isolation by distance) in two or more individuals if they have inherited it from a common ancestor without recombination, that is, the segment has the same ancestral origin in these individuals. DNA segments that are IBD are IBS per definition, but segments that are not IBD can still be IBS due to the same mutations in different individuals or recombinations that do not alter the segment.\n\nAll individuals in a finite population are related if traced back long enough and will, therefore, share segments of their genomes IBD. During meiosis segments of IBD are broken up by recombination. Therefore, the expected length of an IBD segment depends on the number of generations since the most recent common ancestor at the locus of the segment. The length of IBD segments that result from a common ancestor \"n\" generations in the past (therefore involving 2\"n\" meiosis) is exponentially distributed with mean 1/(2\"n\") Morgans (M). The expected number of IBD segments decreases with the number of generations since the common ancestor at this locus. For a specific DNA segment, the probability of being IBD decreases as 2 since in each meiosis the probability of transmitting this segment is 1/2.\n\nIdentified IBD segments can be used for a wide range of purposes. As noted above the amount (length and number) of IBD sharing depends on the familial relationships between the tested individuals. Therefore, one application of IBD segment detection is to quantify relatedness. Measurement of relatedness can be used in forensic genetics, but can also increase information in genetic linkage mapping and help to decrease bias by undocumented relationships in standard association studies.\nAnother application of IBD is genotype imputation and haplotype phase inference. Long shared segments of IBD, which are broken up by short regions may be indicative for phasing errors.\n\nIBD mapping is similar to linkage analysis, but can be performed without a known pedigree on a cohort of unrelated individuals. IBD mapping can be seen as a new form of association analysis that increases the power to map genes or genomic regions containing multiple rare disease susceptibility variants.\n\nUsing simulated data, Browning and Thompson showed that IBD mapping has higher power than association testing when multiple rare variants within a gene contribute to disease susceptibility. Via IBD mapping, genome-wide significant regions in isolated populations as well as outbred populations were found while standard association tests failed. Houwen et al. used IBD sharing to identify the chromosomal location of a gene responsible for benign recurrent intrahepatic cholestasis in an isolated fishing population. Kenny et al. also used an isolated population to fine-map a signal found by a genome-wide association study (GWAS) of plasma plant sterol (PPS) levels, a surrogate measure of cholesterol absorption from the intestine. Francks et al. was able to identify a potential susceptibility locus for schizophrenia and bipolar disorder with genotype data of case-control samples. Lin et al. found a genome-wide significant linkage signal in a dataset of multiple sclerosis patients. Letouzé et al. used IBD mapping to look for founder mutations in cancer samples.\n\nDetection of natural selection in the human genome is also possible via detected IBD segments. Selection will usually tend to increase the number of IBD segments among individuals in a population. By scanning for regions with excess IBD sharing, regions in the human genome that have been under strong, very recent selection can be identified.\n\nIn addition to that, IBD segments can be useful for measuring and identifying other influences on population structure. Gusev et al. showed that IBD segments can be used with additional modeling to estimate demographic history including bottlenecks and admixture. Using similar models Palamara et al. and Carmi et al. reconstructed the demographic history of Ashkenazi Jewish and Kenyan Maasai individuals. Botigué et al. investigated differences in African ancestry among European populations. Ralph and Coop used IBD detection to quantify the common ancestry of different European populations and Gravel et al. similarly tried to draw conclusions of the genetic history of populations in the Americas. Ringbauer et al. utilized geographic structure of IBD segments to estimate dispersal within Eastern Europe during the last centuries. Using the 1000 Genomes data Hochreiter found differences in IBD sharing between African, Asian and European populations as well as IBD segments that are shared with ancient genomes like the Neanderthal or Denisova.\n\nPrograms for the detection of IBD segments in unrelated individuals:\n\n", "id": "1547778", "title": "Identity by descent"}
{"url": "https://en.wikipedia.org/wiki?curid=42069970", "text": "Neuronal lineage marker\n\nA Neuronal lineage marker is an endogenous tag that is expressed in different cells along neurogenesis and differentiated cells such as neurons. It allows detection and identification of cells by using different techniques. A neuronal lineage marker can be either DNA, mRNA or RNA expressed in a cell of interest. It can also be a protein tag, as a partial protein, a protein or an epitope that discriminates between different cell types or different states of a common cell. An ideal marker is specific to a given cell type in normal conditions and/or during injury. Cell markers are very valuable tools for examining the function of cells in normal conditions as well as during disease. The discovery of various proteins specific to certain cells led to the production of cell-type-specific antibodies that have been used to identify cells.\n\nThe techniques used for its detection can be immunohistochemistry, immunocytochemistry, methods that utilize transcriptional modulators and site-specific recombinases to label specific neuronal population, in situ hybridization or fluorescence in situ hybridization (FISH). A neuronal lineage marker can be a neuronal antigen that is recognized by an autoantibody for example Hu, which is highly restricted to neuronal nuclei. By immunohistochemistry, anti-Hu stains the nuclei of neurons. To localize mRNA in brain tissue, one can use a fragment of DNA or RNA as a neuronal lineage marker, a hybridization probe that detects the presence of nucleotide sequences that are complementary to the sequence in the probe. This technique is known as in situ hybridization. Its application have been carried out in all different tissues, but particularly useful in neuroscience. Using this technique, it is possible to locate gene expression to specific cell types in specific regions and observe how changes in this distribution occur throughout the development and correlate with the behavioral manipulations.\n\nAlthough immunohistochemistry is the staple methodology for identifying neuronal cell types, since it is relatively low in cost and a wide range of immunohistochemical markers are available to help distinguish the phenotype of cells in the brain, sometimes it is time-consuming to produce a good antibody. Therefore, one of the most convenient methods for the rapid assessment of the expression of a cloned ion channel could be in situ hybridization histochemistry.\n\nAfter cells are isolated from tissue or differentiated from pluripotent precursors, the resulting population needs to be characterized to confirm whether the target population has been obtained. Depending on the goal of a particular study, one can use neural stem cells markers, neural progenitor cell markers, neuron markers or PNS neuronal markers.\n\nThe study of the nervous system dates back to ancient Egypt but only in the ninetieth century it became more detailed. With the invention of the microscope and a technique of staining developed by Camillo Golgi, it was possible to study individual neurons. This scientist started to impregnate nervous tissue with metal, as silver. The reaction consists in fixing particles of silver chromate to the neurilemma, and resulted in a stark black deposit in the soma, axon and dendrites of the neuron. Thus, it was possible to identify different types of neurons, as Golgi Cell, Golgi I and Golgi II.\nIn 1885 there was a German medical researcher called Franz Nissl who developed another staining technique now known by Nissl staining. This technique is slightly different from Golgi staining since it stains the cell body and the endoplasmic reticulum.\nIn 1887, a Spanish scientist called Santiago Ramon y Cajal learned the staining technique with Golgi and started his famous work of neuroanatomy. With this technique he made an extensive study of several areas of the brain and in different species. He also described very precisely the purkinje cells, the chick cerebellum and the neuronal circuit of the rodent hippocampus.\nIn 1941 Dr. Albert Coons used for the first time a revolutionary technique that uses the principle of antibodies binding specifically to antigens in the tissues. He created an immunoflorescent technique for labelling the antibodies. This technique continues to be widely used in neuroscience studies for identifying different structures. The most important neural markers used nowadays are the GFAB, Nestin, NeuroD antibodies and others. For the past years there are still creating new neural markers for immunocytochemistry or/and immunohistochemistry.\nIn 1953 Heinrich Klüver invented a new staining technique called, Luxol Fast Blue stain or LFB, and with this technique it’s possible to detect demyelination in the central nervous system. Myelin sheath will be stained blue, but other structures will be stained as well.\nThe next revolutionary technique was invented in 1969 by an American scientist called Joseph G. Gall. This technique is called in situ Hybridization and it is used in a large variety of studies but mainly used in developmental biology. With this technique it is possible to mark some genes expressed in determined areas of the animal. In neurobiology, it's very useful for understanding the formation of the nervous system.\n\nThis is one of the most powerful techniques to mark cells. This method consists of hybridizing a labeled complementary DNA or RNA strand to a specific DNA or RNA in the tissue. By doing this hybridization we will be able to reveal the location of a specific mRNA, giving us information about the physiological process of organization, regulation and function of the genes.\n\nUsing this technique we can now know what are the genes and proteins that are behind a certain process, like the formation of the neural crest, or a specific behavior; and what is the location of that same genes. We can also see how changes in the distribution of these genes can affect the development of a tissue, and correlate it with behavioral manipulations. Some examples are the use of, digoxigenin- or fluorophore-conjugated oligo- nucleotide probes, for the detection of localized mRNAs in dendrites, spines, axons, and growth cones of cultured neurons; or digoxigenin-labeled RNA probes and fluorescence tyramide amplification for the detection of less abundant mRNAs localized to dendrites in vivo. These examples use FISH (Fluorescent in situ hybridization). With this technique we can understand the physiological processes and neurological diseases.\nImmunohistochemistry is a technique that uses antibodies with fluorescent staining tags that target a specific antigen present in a certain protein. This high specificity allows us to localize the peptidergic and classical transmitter compounds, their synthetic enzymes and other cell specific antigen in neuronal tissiue.\n\nAn example of the application of this technique in neuroscience is the immunolabeling of antigens like NGF-Inducible Large External glycoprotein (NILE-GF), choline acetyltransferase, parvalbumin, and neurofilament protein. All of these antigens are present in specific neuronal cell types. With these we can define anatomical circuits with a high degree of resolution, and understand the role of some proteins and cells in the nervous system, as well as the location of that same proteins and cells.\n\nAlthough this is a very potent technique there are some drawbacks. The procedure has a nonquantifiable nature and has the occurrence of both false positives and false negatives.\n\nImmunocytochemistry uses the same method that immunohistochemistry, but with the difference that this technique is used in isolated cells in culture, and the other is in tissues. The results are the same but with more resolution, once we are looking to one cell only.\n\nNeural stem cells are an example of somatic stem cell found in various tissues, both during development and in the adult. They have two fundamental characteristics: they are self-renewing and upon terminal division and differentiation, they can give rise to the full range of cells classes within the relevant tissue. Hence, a neural stem cell can give rise to another neural stem cell, or to any of the differentiated cell types found in the central and peripheral nervous systems (inhibitory and excitatory neurons, astrocytes and oligodendrocytes).\n\nThe standard method of isolating neural stem cells in vitro is with the neurosphere culture system, the method originally used to identify NSCs. After some proliferation, the cells are either induced to differentiate by withdrawing the mitogens or by exposing the cells to another factor that induces some of the cells to develop into different lineages. Cellular fates are analysed by staining with antibodies directed against antigens specific for astrocytes, oligodendrocytes, and neurons. In some cases, cells are plated at low density and monitored to determine if a single cell can give rise to the three phenotypes. Immunomagnetic cell separation strategies using antibodies directed against cell surface markers present on stem cells, progenitors and mature CNS cells have been applied to the study of NSCs. Other non-immunological methods have been used to identify populations of cells from normal and tumorigenic CNS tissues, which demonstrate some of the in vitro properties of stem cells, including high aldehyde dehydrogenase (ALDH) enzyme activity. ALDH cells from embryonic rat and mouse CNS have been isolated and shown to have the ability to generate neurospheres, neurons, astrocytes and oligodendrocytes in vitro, as well as neurons in vivo when transplanted into the adult mouse cerebral cortex. Once a stem cell divides asymmetrically, the more mature progenitor is born and migrates to regions of differentiation. As the progenitor migrates, it matures further until it reaches a site where it stops and either becomes quiescent or fully differentiates into a functioning cell. The major obstacle to identifying and discovering markers that define a stem cell is that the most primitive cells are probably in a quiescent state and do not express many unique antigens. Thus, as with other fields like haematopoiesis, a combination of positive and negative markers will be required to better define the central nervous system stem cell.\nNonetheless, changes in the expression levels of specific molecules can be used to indicate the presence of neural stem cells in studies focused on further differentiation toward specific neural lineages. Usual markers used for neural stem cells include Nestin and SOX2. Although Nestin it is expressed predominantly in stem cells of the central nervous system (CNS), its expression is absent from nearly all mature CNS cells, thus it is an efficient marker for neural stem cells. During neurogenesis, Sox2 is expressed throughout developing cells in the neural tube as well as in proliferating CNS progenitors, hence is thought to be centrally important for neural stem cell proliferation and differentiation. In addition to intracellular molecules, products are available to study proteins which are expressed at the cell surface, including ABCG2, FGF R4, and Frizzled-9.\n\nThe differentiation of neural stem cells is controlled, in a context-dependent manner, by intrinsic factors and extracellular signalling molecules that act as positive or negative regulators that can be used as markers.\n\nA neural progenitor cell is distinct from a neural stem cell since it is incapable of continuous self-renewal and usually has the capacity to give rise to only one class of differentiated progeny. They are tripotent cells which can give rise to neurons, astrocytes and oligodendrocytes. An oligodendroglial progenitor cell, for example, gives rise to oligodendrocytes until its mitotic capacity is exhausted.\nSome neural progenitor markers are capable of tracking cells as they undergo expansion and differentiation from rosettes to neurons. The neural rosette is the developmental signature of neuroprogenitors in cultures of differentiating embryonic stem cells; rosettes are radial arrangements of columnar cells that express many of the proteins expressed in neuroepithelial cells in the neural tube. It has been shown that cells within rosettes express multiple cell markers, including among others Nestin, NCAM and Musashi-1, a RNA-binding protein that is expressed in proliferating neural stems cells.\nNeuroepithelial progenitors (NEP) are responsible for neurogenesis in the neural tube and also give rise to two other types of neural progenitor cell, radial glia and basal progenitors. Radial glia are the dominant progenitor cell type in the developing brain whereas basal progenitors are specifically located at the subventricular zone (SVZ) in the developing telencephalon. Although functional studies of radial glia are increasing, it is difficult to distinguish them from neuroprogenitors and astrocytes. Like neuroprogenitors, radial glia express intermediate filament proteins nestin as well as the transcription factor PAX6 that is expressed in some neuroprogenitors in the ventral half of the neural tube. Radial glia also express proteins characteristic of astrocytes, including the widely used glial fibrillar acidic protein (GFAP), among others. Cytological markers that might be unique to radial glia include modified forms of nestin identified by the RC1 and RC2 antibodies that recognize the murine antigens.\n\nMarkers can detect neurons in different stages of development from nuclear, cytoplasmic, membrane or perisynaptic products present in neurons. It is also possible to label specifically cholinergic, dopaminergic, serotonergic, GABAergic or glutamatergic neurons. Pan neuron markers have multiple targets (somatic, nuclear, dendritic, spine and axonal proteins) and consequently label across all parts of the neuron. It is used to study neuronal morphology, although there are specific markers that label particular regions of the neuron.\n\nDoublecortin (DCX) is a microtubule-associated protein that is widely expressed in the soma and leading processes of migrating neurons and in the axons of differentiation neurons. Its expression is downregulated with maturation \n\nNeuron-specific Class III β-tubulin (TuJ1) is present in newly generated immature postmitotic neurons and differentiated neurons and in some mitotically active neuronal precursors.\n\nMicrotubule-associated protein 2 (MAP-2) is a cytoskeletal protein. Its expression is weak in neuronal precursors but it increases during neuron development process. In general, its expression is confined to neurons and reactive astrocytes.\n\nNeuron specific enolase (NSE), also called as gamma-enolase or enolase 2, is a cytosolic protein that is expressed in mature neurons. NSE levels increase along the neuron development reaching higher level in later stages. It can be expressed in glial cells during oligodendrocyte differentiation with the same levels that have been found in neuron culture, but is repressed when cells become mature. In pathological conditions was also reported that glial neoplasms and reactive glial cells expressed this marker.\n\nCalretinin is widely distributed in different neuronal populations of vertebrate retina, being a valuable marker for immature postmitotic neurons.\n\nNeuronal Nuclei antigen (NeuN) or Fox-3 is a nuclear protein present in postmitotic stage when start to differentiate into mature cells. It allows to detect all the neuronal cell types except Purkinje cells, olfactory bulb mitral cells, retinal photoreceptor and dopaminergic neurons in the substantia nigra.\n\nCalbindin is expressed by cerebellar Purkinje cells and granule cells of hippocampus. The reorganization and migration of calbindin-stained Purkinje neurons in rat cerebellum after peripheral nerve injury suggests that calbindin may be a marker for immature post-mitotic neurons, similar to calretinin.\n\nTyrosine hydroxylase (TH) is an enzyme involved in the synthesis of dopamine and norepinephrine. Generally, it is used as a marker for dopaminergic neurons, but it can also be found in some forebrain neurons which make norepinephrine (which is the product of dopamine and the enzyme dopamine β-hydroxylase).\n\nCholine Acetyltransferase (ChAT) is expressed in cholinergic neurons of both the CNS and PNS. In the CNS, ChAT is expressed in motor neurons and pre-ganglionic autonomic neurons of the spinal cord, a subset of neurons in the neostriatum and in the basal forebrain. On the other hand, in PNS it is present in a small group of sympathetic neurons and in all parasympathetic neurons.\n\nGABA is a mature neuronal marker expressed in GABAergic interneurons (inhibitor neurons which are generally interneurons in the brain). GAD65/67 are two enzymes involved in GABA synthesis by GABAergic interneurons.\n\nNeuronal lineage markers can be used in clinical research to identify diseased cells and/or in repair process. Since selective degeneration of functional neurons is associated with the pathogenesis of neurodegenerative disorders, such as degeneration of midbrain dopaminergic neurons in Parkinson's disease, forebrain cholinergic neurons in Alzheimer's disease and cortical GABAergic neurons in schizophrenia, markers of neuronal cell phenotype are of particular interest because of their utility in understanding pathology of clinical disease. There are two key markers in these studies: choline acetyltransferase and tyrosine hydroxylase. Choline acetyltransferase (ChAT) is an enzyme responsible for catalyzing the synthesis of acetylcholine, and is expressed in the majority of cholinergic neurons. Hence, ChAT immunoreactivity is used to detect cognitive decline in several neurodegenerative disorders.\nIn motor regions, sensory cortex and in the basal forebrain these immunolabeling has been applied to evaluate disruptions in cholinergic neurons of the ChAT fiber network and also for overall morphology.\nThe Tyrosine hydroxylase (TH) immunolabeling has been very useful for Parkinson's disease investigation. It is used to determine the quantity of dopaminergic cell loss in Parkinson’s patients.\n", "id": "42069970", "title": "Neuronal lineage marker"}
{"url": "https://en.wikipedia.org/wiki?curid=20232", "text": "Messenger RNA\n\nMessenger RNA (mRNA) is a large family of RNA molecules that convey genetic information from DNA to the ribosome, where they specify the amino acid sequence of the protein products of gene expression. Following transcription of primary transcript mRNA (known as pre-mRNA) by RNA polymerase, processed, mature mRNA is translated into a polymer of amino acids: a protein, as summarized in the central dogma of molecular biology.\n\nAs in DNA, mRNA genetic information is in the sequence of nucleotides, which are arranged into codons consisting of three base pairs each. Each codon encodes for a specific amino acid, except the stop codons, which terminate protein synthesis. This process of translation of codons into amino acids requires two other types of RNA: Transfer RNA (tRNA), that mediates recognition of the codon and provides the corresponding amino acid, and ribosomal RNA (rRNA), that is the central component of the ribosome's protein-manufacturing machinery.\n\nThe existence of mRNA was first suggested by Jacques Monod and François Jacob, and subsequently discovered by Jacob, Sydney Brenner and Matthew Meselson at the California Institute of Technology in 1961.\n\nIt should not be confused with mitochondrial DNA.\n\nThe brief existence of an mRNA molecule begins with transcription, and ultimately ends in degradation. During its life, an mRNA molecule may also be processed, edited, and transported prior to translation. Eukaryotic mRNA molecules often require extensive processing and transport, while prokaryotic mRNA molecules do not. A molecule of eukaryotic mRNA and the proteins surrounding it are together called a messenger RNP.\n\nTranscription is when RNA is made from DNA. During transcription, RNA polymerase makes a copy of a gene from the DNA to mRNA as needed. This process is similar in eukaryotes and prokaryotes. One notable difference, however, is that eukaryotic RNA polymerase associates with mRNA-processing enzymes during transcription so that processing can proceed quickly after the start of transcription. The short-lived, unprocessed or partially processed product is termed \"precursor mRNA\", or \"pre-mRNA\"; once completely processed, it is termed \"mature mRNA\".\n\nProcessing of mRNA differs greatly among eukaryotes, bacteria, and archea. Non-eukaryotic mRNA is, in essence, mature upon transcription and requires no processing, except in rare cases. Eukaryotic pre-mRNA, however, requires extensive processing.\n\nA \"5' cap\" (also termed an RNA cap, an RNA 7-methylguanosine cap, or an RNA mG cap) is a modified guanine nucleotide that has been added to the \"front\" or 5' end of a eukaryotic messenger RNA shortly after the start of transcription. The 5' cap consists of a terminal 7-methylguanosine residue that is linked through a 5'-5'-triphosphate bond to the first transcribed nucleotide. Its presence is critical for recognition by the ribosome and protection from RNases.\n\nCap addition is coupled to transcription, and occurs co-transcriptionally, such that each influences the other. Shortly after the start of transcription, the 5' end of the mRNA being synthesized is bound by a cap-synthesizing complex associated with RNA polymerase. This enzymatic complex catalyzes the chemical reactions that are required for mRNA capping. Synthesis proceeds as a multi-step biochemical reaction.\n\nIn some instances, an mRNA will be edited, changing the nucleotide composition of that mRNA. An example in humans is the apolipoprotein B mRNA, which is edited in some tissues, but not others. The editing creates an early stop codon, which, upon translation, produces a shorter protein.\n\nPolyadenylation is the covalent linkage of a polyadenylyl moiety to a messenger RNA molecule. In eukaryotic organisms most messenger RNA (mRNA) molecules are polyadenylated at the 3' end, but recent studies have shown that short stretches of uridine (oligouridylation) are also common. The poly(A) tail and the protein bound to it aid in protecting mRNA from degradation by exonucleases. Polyadenylation is also important for transcription termination, export of the mRNA from the nucleus, and translation. mRNA can also be polyadenylated in prokaryotic organisms, where poly(A) tails act to facilitate, rather than impede, exonucleolytic degradation.\n\nPolyadenylation occurs during and/or immediately after transcription of DNA into RNA. After transcription has been terminated, the mRNA chain is cleaved through the action of an endonuclease complex associated with RNA polymerase. After the mRNA has been cleaved, around 250 adenosine residues are added to the free 3' end at the cleavage site. This reaction is catalyzed by polyadenylate polymerase. Just as in alternative splicing, there can be more than one polyadenylation variant of an mRNA.\n\nPolyadenylation site mutations also occur. The primary RNA transcript of a gene is cleaved at the poly-A addition site, and 100-200 A’s are\nadded to the 3’ end of the RNA. If this site is altered, an abnormally long and unstable mRNA construct will be formed.\n\nAnother difference between eukaryotes and prokaryotes is mRNA transport. Because eukaryotic transcription and translation is compartmentally separated, eukaryotic mRNAs must be exported from the nucleus to the cytoplasm—a process that may be regulated by different signaling pathways. Mature mRNAs are recognized by their processed modifications and then exported through the nuclear pore by binding to the cap-binding proteins CBP20 and CBP80, as well as the transcription/export complex (TREX). Multiple mRNA export pathways have been identified in eukaryotes.\n\nIn spatially complex cells, some mRNAs are transported to particular subcellar destinations. In mature neurons, certain mRNA are transported from the soma to dendrites. One site of mRNA translation is at polyribosomes selectively localized beneath synapses. The mRNA for Arc/Arg3.1 is induced by synaptic activity and localizes selectively near active synapses based on signals generated by NMDA receptors. Other mRNAs also move into dendrites in response to external stimuli, such as β-actin mRNA. Upon export from the nucleus, actin mRNA associates with ZBP1 and the 40S subunit. The complex is bound by a motor protein and is transported to the target location (neurite extension) along the cytoskeleton. Eventually ZBP1 is phosphorylated by Src in order for translation to be initiated. In developing neurons, mRNAs are also transported into growing axons and especially growth cones. Many mRNAs are marked with so-called \"zip codes,\" which target their transport to a specific location.\n\nBecause prokaryotic mRNA does not need to be processed or transported, translation by the ribosome can begin immediately after the end of transcription. Therefore, it can be said that prokaryotic translation is \"coupled\" to transcription and occurs \"co-transcriptionally\".\n\nEukaryotic mRNA that has been processed and transported to the cytoplasm (i.e., mature mRNA) can then be translated by the ribosome. Translation may occur at ribosomes free-floating in the cytoplasm, or directed to the endoplasmic reticulum by the signal recognition particle. Therefore, unlike in prokaryotes, eukaryotic translation \"is not\" directly coupled to transcription.\n\nCoding regions are composed of codons, which are decoded and translated (in eukaryotes usually into one and in prokaryotes usually into several) into proteins by the ribosome. Coding regions begin with the start codon and end with a stop codon. In general, the start codon is an AUG triplet and the stop codon is UAA, UAG, or UGA. The coding regions tend to be stabilised by internal base pairs, this impedes degradation. In addition to being protein-coding, portions of coding regions may serve as regulatory sequences in the pre-mRNA as exonic splicing enhancers or exonic splicing silencers.\n\nUntranslated regions (UTRs) are sections of the mRNA before the start codon and after the stop codon that are not translated, termed the five prime untranslated region (5' UTR) and three prime untranslated region (3' UTR), respectively. These regions are transcribed with the coding region and thus are exonic as they are present in the mature mRNA. Several roles in gene expression have been attributed to the untranslated regions, including mRNA stability, mRNA localization, and translational efficiency. The ability of a UTR to perform these functions depends on the sequence of the UTR and can differ between mRNAs. Genetic variants in 3' UTR have also been implicated in disease susceptibility because of the change in RNA structure and protein translation.\n\nThe stability of mRNAs may be controlled by the 5' UTR and/or 3' UTR due to varying affinity for RNA degrading enzymes called ribonucleases and for ancillary proteins that can promote or inhibit RNA degradation. (See also, C-rich stability element.)\n\nTranslational efficiency, including sometimes the complete inhibition of translation, can be controlled by UTRs. Proteins that bind to either the 3' or 5' UTR may affect translation by influencing the ribosome's ability to bind to the mRNA. MicroRNAs bound to the 3' UTR also may affect translational efficiency or mRNA stability.\n\nCytoplasmic localization of mRNA is thought to be a function of the 3' UTR. Proteins that are needed in a particular region of the cell can also be translated there; in such a case, the 3' UTR may contain sequences that allow the transcript to be localized to this region for translation.\n\nSome of the elements contained in untranslated regions form a characteristic secondary structure when transcribed into RNA. These structural mRNA elements are involved in regulating the mRNA. Some, such as the SECIS element, are targets for proteins to bind. One class of mRNA element, the riboswitches, directly bind small molecules, changing their fold to modify levels of transcription or translation. In these cases, the mRNA regulates itself.\n\nThe 3' poly(A) tail is a long sequence of adenine nucleotides (often several hundred) added to the 3' end of the pre-mRNA. This tail promotes export from the nucleus and translation, and protects the mRNA from degradation.\n\nAn mRNA molecule is said to be monocistronic when it contains the genetic information to translate only a single protein chain (polypeptide). This is the case for most of the eukaryotic mRNAs. On the other hand, polycistronic mRNA carries several open reading frames (ORFs), each of which is translated into a polypeptide. These polypeptides usually have a related function (they often are the subunits composing a final complex protein) and their coding sequence is grouped and regulated together in a regulatory region, containing a promoter and an operator. Most of the mRNA found in bacteria and archaea is polycistronic, as is the human mitochondrial genome\n. Dicistronic or bicistronic mRNA encodes only two proteins.\n\nIn eukaryotes mRNA molecules form circular structures due to an interaction between the eIF4E and poly(A)-binding protein, which both bind to eIF4G, forming an mRNA-protein-mRNA bridge. Circularization is thought to promote cycling of ribosomes on the mRNA leading to time-efficient translation, and may also function to ensure only intact mRNA are translated (partially degraded mRNA characteristically have no m7G cap, or no poly-A tail).\n\nOther mechanisms for circularization exist, particularly in virus mRNA. Poliovirus mRNA uses a cloverleaf section towards its 5' end to bind PCBP2, which binds poly(A)-binding protein, forming the familiar mRNA-protein-mRNA circle. Barley yellow dwarf virus has binding between mRNA segments on its 5' end and 3' end (called kissing stem loops), circularizing the mRNA without any proteins involved.\n\nRNA virus genomes (the + strands of which are translated as mRNA) are also commonly circularized. During genome replication the circularization acts to enhance genome replication speeds, cycling viral RNA-dependent RNA polymerase much the same as the ribosome is hypothesized to cycle.\n\nDifferent mRNAs within the same cell have distinct lifetimes (stabilities). In bacterial cells, individual mRNAs can survive from seconds to more than an hour; in mammalian cells, mRNA lifetimes range from several minutes to days. The greater the stability of an mRNA the more protein may be produced from that mRNA. The limited lifetime of mRNA enables a cell to alter protein synthesis rapidly in response to its changing needs. There are many mechanisms that lead to the destruction of an mRNA, some of which are described below.\n\nIn general, in prokaryotes the lifetime of mRNA is much shorter than in eukaryotes. Prokaryotes degrade messages by using a combination of ribonucleases, including endonucleases, 3' exonucleases, and 5' exonucleases. In some instances, small RNA molecules (sRNA) tens to hundreds of nucleotides long can stimulate the degradation of specific mRNAs by base-pairing with complementary sequences and facilitating ribonuclease cleavage by RNase III. It was recently shown that bacteria also have a sort of 5' cap consisting of a triphosphate on the 5' end. Removal of two of the phosphates leaves a 5' monophosphate, causing the message to be destroyed by the exonuclease RNase J, which degrades 5' to 3'.\n\nInside eukaryotic cells, there is a balance between the processes of translation and mRNA decay. Messages that are being actively translated are bound by ribosomes, the eukaryotic initiation factors eIF-4E and eIF-4G, and poly(A)-binding protein. eIF-4E and eIF-4G block the decapping enzyme (DCP2), and poly(A)-binding protein blocks the exosome complex, protecting the ends of the message. The balance between translation and decay is reflected in the size and abundance of cytoplasmic structures known as P-bodies The poly(A) tail of the mRNA is shortened by specialized exonucleases that are targeted to specific messenger RNAs by a combination of cis-regulatory sequences on the RNA and trans-acting RNA-binding proteins. Poly(A) tail removal is thought to disrupt the circular structure of the message and destabilize the cap binding complex. The message is then subject to degradation by either the exosome complex or the decapping complex. In this way, translationally inactive messages can be destroyed quickly, while active messages remain intact. The mechanism by which translation stops and the message is handed-off to decay complexes is not understood in detail.\n\nThe presence of AU-rich elements in some mammalian mRNAs tends to destabilize those transcripts through the action of cellular proteins that bind these sequences and stimulate poly(A) tail removal. Loss of the poly(A) tail is thought to promote mRNA degradation by facilitating attack by both the exosome complex and the decapping complex. Rapid mRNA degradation via AU-rich elements is a critical mechanism for preventing the overproduction of potent cytokines such as tumor necrosis factor (TNF) and granulocyte-macrophage colony stimulating factor (GM-CSF). AU-rich elements also regulate the biosynthesis of proto-oncogenic transcription factors like c-Jun and c-Fos.\n\nEukaryotic messages are subject to surveillance by nonsense mediated decay (NMD), which checks for the presence of premature stop codons (nonsense codons) in the message. These can arise via incomplete splicing, V(D)J recombination in the adaptive immune system, mutations in DNA, transcription errors, leaky scanning by the ribosome causing a frame shift, and other causes. Detection of a premature stop codon triggers mRNA degradation by 5' decapping, 3' poly(A) tail removal, or endonucleolytic cleavage.\n\nIn metazoans, small interfering RNAs (siRNAs) processed by Dicer are incorporated into a complex known as the RNA-induced silencing complex or RISC. This complex contains an endonuclease that cleaves perfectly complementary messages to which the siRNA binds. The resulting mRNA fragments are then destroyed by exonucleases. siRNA is commonly used in laboratories to block the function of genes in cell culture. It is thought to be part of the innate immune system as a defense against double-stranded RNA viruses.\n\nMicroRNAs (miRNAs) are small RNAs that typically are partially complementary to sequences in metazoan messenger RNAs. Binding of a miRNA to a message can repress translation of that message and accelerate poly(A) tail removal, thereby hastening mRNA degradation. The mechanism of action of miRNAs is the subject of active research.\n\nThere are other ways by which messages can be degraded, including non-stop decay and silencing by Piwi-interacting RNA (piRNA), among others.\n\nFull length mRNA molecules have been proposed as therapeutics since the beginning of the biotech era but there was little traction until the 2010s, when Moderna Therapeutics was founded and managed to raise almost a billion dollars in venture funding in its first three years. \n\nTheoretically, the administered mRNA sequence can cause a cell to make a protein, which in turn could directly treat a disease or could function as a vaccine; more indirectly the protein could drive an endogenous stem cell to differentiate in a desired way.\n\nRibonucleic acid (RNA) is produced from deoxyribonucleic acid (DNA) in a process called transcription. RNA is primarily used as the code to make proteins and amino acids. Errors in protein or amino acid production can result in diseases such as cystic fibrosis or phenylketonuria. Therapies are now being developed that directly target RNA.\n\nThe primary challenges of RNA therapy center on delivering the RNA to directed cells, more even than determining what sequence to deliver. Naked RNA sequences will naturally degrade after preparation; they may trigger the body's immune system to attack them as an invader; and they are impermeable to the cell membrane. Once within the cell, they must then leave the cell's transport mechanism to take action within the cytoplasm, which houses the ribosomes that direct manufacture of proteins.\n\nAs opposed to targeting proteins with small chemicals or large compounds, drugs that target RNA hold the potential to treat numerous diseases resistant to conventional medicines. \n\n\n", "id": "20232", "title": "Messenger RNA"}
{"url": "https://en.wikipedia.org/wiki?curid=6003871", "text": "Tag SNP\n\nA tag SNP is a representative single nucleotide polymorphism (SNP) in a region of the genome with high linkage disequilibrium that represents a group of SNPs called a haplotype. It is possible to identify genetic variation and association to phenotypes without genotyping every SNP in a chromosomal region. This reduces the expense and time of mapping genome areas associated with disease, since it eliminates the need to study every individual SNP. Tag SNPs are useful in whole-genome SNP association studies in which hundreds of thousands of SNPs across the entire genome are genotyped.\n\nTwo loci are said to be in linkage equilibrium (LE) if their inheritance is an independent event. If the alleles at those loci are non-randomly inherited then we say that they are at linkage disequilibrium (LD). LD is most commonly caused by physical linkage of genes. When two genes are inherited on the same chromosome, depending on their distance and the likelihood of recombination between the loci they can be at high LD. However, LD can be also observed due to functional interactions where even genes from different chromosomes can jointly confer evolutionary selected phenotype or can affect the viability of potential offspring.\n\nIn families LD is highest because of the lowest numbers of recombination events (fewest number of meiosis events). This is especially true between inbred lines. In populations LD exists because of selection, physical closeness of the genes that causes low recombination rates or due to recent crossing or migration. On a population level, processes that influence linkage disequilibrium include genetic linkage, epistatic natural selection, rate of recombination, mutation, genetic drift, random mating, genetic hitchhiking and gene flow.\n\nWhen a group of SNPs are inherited together because of high LD there tends to be redundant information. The selection of a tag SNP as a representative of these groups reduces the amount of redundancy when analyzing parts of the genome associated with traits/diseases. The regions of the genome in high LD that harbor a specific set of SNPs that are inherited together are also known as haplotypes. Therefore tag SNPs are representative of all SNPs within a haplotype.\n\nThe selection of tag SNPs is dependent on the haplotypes present in the genome. Most sequencing technologies provide the genotypic information and not the haplotypes i.e. they provide information on the specific bases that are present but do not provide phasic information (at which specific chromosome each of the bases appear). Determination of haplotypes can be done through molecular methods (Allele Specific PCR, Somatic cell hybrids). These methods distinguish which allele is present at which chromosome by separating the chromosomes before genotyping. They can be very time-consuming and expensive, so statistical inference methods have been developed as a less expensive and automated option. These statistical-inference software packages utilize parsimony, maximum likelihood, and Bayesian algorithms to determine haplotypes. Disadvantage of statistical-inference is that a proportion of the inferred haplotypes could be wrong.\n\nWhen haplotypes are used for genome wide association studies, it is important to note the population being studied. Often different populations will have different patterns of LD. One example of differentiating patterns are African-descended populations vs. European and Asian-descended populations. Since humans originated in Africa and spread into Europe and then the Asian and American continents, the African populations are the most genetically diverse and have smaller regions of LD while European and Asian-descended populations have larger regions of LD due to founder effect. When LD patterns differ in populations, SNPs can become disassociated with each other due to the changes in haplotype blocks. This means that tag SNPs, as representatives of the haplotype blocks, are unique in populations and population differences should be taken into account when performing association studies.\n\nAlmost every trait has both genetic and environmental influence. Heritability is the proportion of phenotypic variance that is inherited from our ancestors. Association studies are used to determine the genetic influence on phenotypic presentation. Although mostly used for mapping diseases to genomic areas they can be used to map heritability of any phenotype like height, eye color etc.\n\nGenome-wide association studies (GWAS) use single-nucleotide polymorphisms (SNPs) to identify genetic associations with clinical conditions and phenotypic traits. They are hypothesis free and use a whole-genome approach to investigate traits by comparing large group of individuals that express a phenotype with a large group of people that don't. The ultimate goal of GWAS is to determine genetic risk factors that can be used to make predictions about who is at risk for a disease, what are the biological underpinnings of disease susceptibility and creating new prevention and treatment strategies. The National Human Genome Research Institute and the European Bioinformatics Institute publishes a Catalog of published genome-wide association studies that highlights statistically significant associations between hundreds of SNPs with a broad range of phenotypes.\n\nDue to the large number of possible SNP variants (more than 149 million as of June 2015 ) it is still very expensive to sequence all SNPs. That is why GWAS use customizable arrays (SNP chips) to genotype only a subset of the variants identified as tag snps. Most GWAS use products from the two primary genotyping platforms. The Affymetrix platform prints DNA probes on a glass or silicone chip that hybridize to specific alleles in the sample DNA. The Illumina platform uses bead-based technology, with longer DNA sequences and produces better specificity. Both platforms are able to genotype more than a million tag SNPs using either pre-made or custom DNA oligos.\n\nGenome-wide studies are predicated on the common disease-common variant (CD/CV) hypothesis which states that common disorders are influenced by common genetic variation. Effect size (penetrance) of the common variants needs to be smaller relative to those found in rare disorders. That means that the common SNP can explain only a small portion of the variance due to genetic factors and that common diseases are influenced by multiple common alleles of small effect size. Another hypothesis is that common diseases are caused by rare variants that are synthetically linked to common variants. In that case the signal produced from GWAS is an indirect (synthetic) association between one or more rare causal variants in linkage disequilibrium. It is important to recognize that this phenomenon is possible when selecting a group for tag SNPs. When a disease is found to be associated with a haplotype, some SNPs in that haplotype will have synthetic association with the disease. To pinpoint the causal SNPs we need a greater resolution in the selection of haplotype blocks. Since whole genome sequencing technologies are rapidly changing and becoming less expensive it is likely that they will replace the current genotyping technologies providing the resolution needed to pinpoint causal variants.\n\nBecause whole genome sequencing of individuals is still cost prohibitive, the international HapMap Project was constructed with a goal to map the human genome to haplotype groupings (haplotype blocks) that can describe common patterns of human genetic variation. By mapping the entire genome to haplotypes, tag SNPs can be identified to represent the haplotype blocks examined by genetic studies. An important factor to consider when planning a genetic study is the frequency and risk incurred by specific alleles. These factors can vary in different populations so the HapMap project used a variety of sequencing techniques to discover and catalog SNPs from different sets of populations. Initially he project sequenced individuals from Yoruba population of African origin (YRI), residents of Utah with western European ancestry (CEU), unrelated individuals from Tokyo, Japan (JPT) and unrelated Han Chinese individuals from Beijing, China (CHB). Recently their datasets have been expanded to include other populations (11 groups) \n\nSelection of maximum informative tag SNPs is an NP complete problem. However, algorithms can be devised to provide approximate solution within a margin of error. The criteria that are needed to define each tag SNP selection algorithm is the following:\n\n\nMethods for selecting features fall into two categories: filter methods and wrapper methods. Filter algorithms are general preprocessing algorithms that do not assume the use of a specific classification method. Wrapper algorithms, in contrast, “wrap” the feature selection around a specific classifier and select a subset of features based on the classifier’s accuracy using cross-validation.\n\nThe feature selection method suitable for selecting tag SNPs must have the following characteristics:\n\nSeveral algorithms have been proposed for selecting tag SNPs. The first approach was based on the measure of goodness of SNP sets and searched for SNP subsets that are small but attain high value of the defined measure. Examining every SNP subset to find good ones is computationally feasible only for small data sets.\nAnother approach uses principal component analysis (PCA) to find subsets of SNPs capturing majority of the data variance. A sliding windows method is employed to repeatedly apply PCA to short chromosomal regions. This reduces the data produced and also does not require exponential search time. Yet it is not feasible to apply the PCA method to large chromosomal data sets as it is computationally complex.\nThe most commonly used approach, block-based method, exploits the principle of linkage disequilibrium observed within haplotype blocks. Several algorithms have been devised to partition chromosomal regions into haplotype blocks which are based on haplotype diversity, LD, four-gamete test and information complexity and tag SNPs are selected from all SNPs that belong to that block. The main presumption in this algorithm is that the SNPs are biallelic. The main drawback is that the definition of blocks is not always straightforward. Even though there is a list of criteria for forming the haplotype blocks, there is no consensus on the same. Also, local correlations based selection of tag SNPs ignores inter-block correlations.\n\nUnlike the block-based approach, a block-free approach does not rely on the block structure. The SNP frequency and recombination rates are known to vary across the genome and some studies have reported LD distances much longer than the reported maximum block sizes. Setting a strict border for the neighborhood is not desired and the block-free approach looks for tag SNPs globally. There are several algorithms to perform this. In one algorithm, the non-tagging SNPs are represented as boolean functions of tag SNPs and set theory techniques are used to reduce search space. Another algorithm searches for subsets of markers that can come from non-consecutive blocks. Due to the marker neighborhood, the search space is reduced.\n\nWith the number of individuals genotyped and number of SNPs in databases growing, tag SNP selection takes too much time to compute. In order to improve the efficiency of the tag SNP selection method, the algorithm first ignores the SNPs being biallelic, and then compresses the length (SNP number) of the haplotype matrix by grouping the SNP sites with the same information. The SNP sites that partition the haplotypes into the same group are called redundant sites. The SNP sites which contain distinct information within a block are called non-redundant sites (NRS). In order to further compress the haplotype matrix, the algorithm needs to find the tag SNPs such that all haplotypes of the matrix can be distinguished. By using the idea of joint partition, an efficient tag SNPs selection algorithm is provided.\n\nDepending on how the tag SNPs are selected, different prediction methods have been used during the cross-validation process. Machine learning method was employed to predict the left-out haplotype. Another approach predicted the alleles of a non-tagging SNP n from the tag SNPs that had the highestcorrelation coefficient with n. If a single highly correlated tag SNP t is found, the alleles are assigned so their frequencies agree with the allele frequencies of t. When multiple tagging SNPs have the same (high) correlation coefficient with n, the common allele of n has advantage. It is easy to see that in this case the prediction method agrees well with the selection method, which uses PCA on the matrix of correlation coefficients between SNPs.\n\nThere are other ways to assess the accuracy of a tag SNP selection method. The accuracy can be evaluated by the quality measure R2, which is the measure of association between the true numbers of haplotype copies defined over the full set of SNPs and the predicted number of haplotype copies where the prediction is based on the subset of tagging SNPs. This measure assumes diploid data and explicit inference of haplotypes from genotypes.\n\nAnother assessment method due to Clayton is based on a measure of the diversity of haplotypes. The diversity is defined as the total number of differences in all pairwise comparison between haplotypes. The difference between a pair of haplotypes is the sum of differences over all the SNPs. The Clayton’s diversity measure can be used to define how well a set of tag SNPs differentiate different haplotypes. This measure is suitable only for haplotype blocks with limited haplotype diversity and it is not clear how to use it for large data sets consisting of multiple haplotype blocks.\n\nSome recent works evaluate tag SNPs selection algorithms based on how well the tagging SNPs can be used to predict non-tagging SNPs. The prediction accuracy is determined using cross-validation such as leave-one-out or hold out. In leave-one-out cross-validation, for each sequence in the data set, the algorithm is run on the rest of the data set to select a minimum set of tagging SNPs.\n\nTagger is a web tool available for evaluating and selecting tag SNPs from genotypic data such as the International HapMap Project. It utilizes pairwise methods and multimarker haplotype approaches. Users can upload HapMap genotype data or pedigree format and the linkage disequilibrium patterns will be calculated. Tagger options allow for the user to specify chromosomal landmarks, which indicate regions of interest in the genome for picking tag SNPs. The program then produces a list of tag SNPs and their statistical test values as well as a coverage report. It is developed by Paul de Bakker in the labs of David Altshuler and Mark Daly at the Center for Human Genetic Research of Massachusetts General Hospital and Harvard Medical School, at the Broad Institute.\n\nIn the freeware (free software) CLUSTAG and WCLUSTAG, there contain cluster and set-cover algorithms to obtain a set of tag SNPs that can represent all the known SNPs in a chromosomal region. The programs are implemented with Java, and they can run in Windows platform as well as the Unix environment. They are developed by SIO-IONG AO et al. in The University of Hong Kong.\n\n", "id": "6003871", "title": "Tag SNP"}
{"url": "https://en.wikipedia.org/wiki?curid=16153022", "text": "Expanded genetic code\n\nAn expanded genetic code is an artificially modified genetic code in which one or more specific codons have been re-allocated to encode an amino acid that is not among the 20 common naturally-encoded proteinogenic amino acids.\n\nThe key prerequisites to expand the genetic code are:\n\nExpanding the genetic code is an area of research of synthetic biology, an applied biological discipline whose goal is to engineer living systems for useful purposes. The genetic code expansion enriches the repertoire of useful tools available to science.\n\nIt is noteworthy that the genetic code for all organisms is basically the same, so that all living beings use the same ’genetic language’. In general, the introduction of new functional unnatural amino acids into proteins of living cells breaks the universality of the genetic language, which ideally leads to alternative life forms. Proteins are produced thanks to the translational system molecules, which decode the RNA messages into a string of amino acids. The translation of genetic information contained in messenger RNA (mRNA) into a protein is catalysed by ribosomes. Transfer RNAs (tRNA) are used as keys to decode the mRNA into its encoded polypeptide. The tRNA recognizes a specific three nucleotide codon in the mRNA with a complementary sequence called the anticodon on one of its loops. Each three nucleotide codon is translated into one of twenty naturally occurring amino acids. There is at least one tRNA for any codon, and sometimes multiple codons code for the same amino acid. Many tRNAs are compatible with several codons. An enzyme called an aminoacyl tRNA synthetase covalently attaches the amino acid to the appropriate tRNA. Most cells have a different synthetase for each amino acid (20 or more synthetases). On the other hand, some bacteria have fewer than 20 aminoacyl tRNA synthetases, and introduce the \"missing\" amino acid(s) by modification of a structurally related amino acid by an aminotransferase enzyme. A feature exploited in the expansion of the genetic code is the fact the aminoacyl tRNA synthetase often does not recognize the anticodon, but another part of the tRNA, meaning that if the anticodon were to be mutated the encoding of that amino acid would change to a new codon.\nIn the ribosome, the information in mRNA is translated into a specific amino acid when the mRNA codon matches with the complementary anticodon of a tRNA, and the attached amino acid is added onto a growing polypeptide chain. When it is released from the ribosome, the polypeptide chain folds into a functioning protein.\n\nIn order to incorporate a novel amino acid into the genetic code several changes are required. First, for successful translation of a novel amino acid, the codon to which the novel amino acid is assigned cannot already code for one of the 20 natural amino acids. Usually a nonsense codon (stop codon) or a four-base codon are used. Second, a novel pair of tRNA and aminoacyl tRNA synthetase are required, these are called the orthogonal set. The orthogonal set must not crosstalk with the endogenous tRNA and synthetase sets, while still being functionally compatible with the ribosome and other components of the translation apparatus. The active site of the synthetase is modified to accept only the novel amino acid. Most often, a library of mutant synthetases is screened for one which charges the tRNA with the desired amino acid. The synthetase is also modified to recognize only the orthogonal tRNA. The tRNA synthetase pair is often engineered in other bacteria or eukaryotic cells.\n\nIn this area of research, the 20 encoded proteinogenic amino acids are referred to as standard amino acids, or alternatively as natural or canonical amino acids, while the added amino acids are called non-standard amino acids (NSAAs), or unnatural amino acids (uAAs; term not used in papers dealing with natural non-proteinogenic amino acids, such as phosphoserine), or non-canonical amino acids.\n\nThe first element of the system is the amino acid that is added to the genetic code of a certain strain of organism.\n\nOver 71 different NSAAs have been added to different strains of \"E. coli\", yeast or mammalian cells. Due to technical details (easier chemical synthesis of NSAAs, less crosstalk and easier evolution of the aminoacyl-tRNA synthase), the NSAAs are generally larger than standard amino acids and most often have a phenylalanine core but with a large variety of different substitutents. These allow a large repertoire of new functions, such as labelling (see figure), as a fluorescent reporter (\"e.g.\" dansylalanine) or to produce translationally protein in \"E. coli\" with Eukaryotic post-translational modifications (\"e.g.\" phosphoserine, phosphothreonine, and phosphotyrosine).\n\nUnnatural amino acids incorporated into proteins include heavy atom containing amino acids to facilitate certain x-ray crystallographic studies; amino acids with novel steric/packing and electronic properties; photocrosslinking amino acids which can be used to probe protein-protein interactions in vitro or in vivo; keto, acetylene, azide, and boronate containing amino acids which can be used to selectively introduce a large number of biophysical probes, tags, and novel chemical functional groups into proteins \"in vitro\" or \"in vivo\"; redox active amino acids to probe and modulate electron transfer; photocaged and photoisomerizable amino acids to photoregulate biological processes; metal binding amino acids for catalysis and metal ion sensing; amino acids that contain fluorescent or infra-red active side chains to probe protein structure and dynamics; α-hydroxy acids and -amino acids as probes of backbone conformation and hydrogen bonding interactions; and sulfated amino acids and mimetics of phosphorylated amino acids as probes of posttranslational modifications.\n\nAvailability of the non-standard amino acid requires that the organism either import it from the medium or biosynthesised it.\nIn the first case, the unnatural amino acid is first synthesised chemically in optically pure -form. It is then added to the growth medium of the cell. Generally a library of compounds is tested to see which can be imported and incorporated, but often the various transport systems can handle unnatural amino acids with apolar side-chains.\nIn the second case, a biosynthetic paths need to be engineered. One example is an \"E. coli\" strain that biosynthesizes a novel, previously unnatural amino acid (p-aminophenylalanine) from basic carbon sources and includes this amino acid in its genetic code. Another example is the production of phosphoserine, which is a natural metabolite and as a consequence the pathway flux had to be altered to increase its production.\n\nAnother element of the system is a codon to allocate to the new amino acid.\n\nA major problem for the genetic code expansion is that there are no free codons. The genetic code has a nonrandom layout that shows tell-tale signs of various phases of primordial evolution, however, it has since frozen into place and is near-universally conserved.\nNevertheless, some codons are rarer than others. In fact, in \"E. coli\" (and all organisms) the codon usage is not equal, but presents several rare codons (see table), the rarest being the amber stop codon (UGA).\n\nThe possibility of reassigning codons was realized by Normanly \"et al.\" in 1990, when a viable mutant strain of \"E. coli\" read through the UAG (\"amber\") stop codon.\nThis was possible thanks to the rarity of this codon and the fact that release factor 1 alone makes the amber codon terminate translation. Later, in the Schultz lab, the tRNATyr/tyrosyl-tRNA synthetase (TyrRS) from \"Methanococcus jannaschii\", an archaebacterium, was used to introduce a tyrosine instead of STOP, the default value of the amber codon. This was possible because of the differences between the endogenous bacterial synthases and the orthologous archaeal synthase, which do not recognize each other. Subsequently, the group evolved the orthologonal tRNA/synthase pair to utilise the non-standard amino acid \"O\"-methyltyrosine. This was followed by the larger naphthylalanine and the photocrosslinking benzoylphenylalanine, which proved the potential utility of the system.\n\nThe amber codon is the least used codon in \"Escherichia coli\", but hijacking it results in a substantial loss of fitness. One study in fact found that there were at least 83 peptides majorly affected by the readthrough Additionally, the labelling was incomplete. As a consequence, several strains have been made to reduce the fitness cost, including the removal of all amber codons from the genome.\nIn most \"E. coli\" K-12 strains (viz. \"Escherichia coli\" (molecular biology) for strain pedigrees) there are 314 UAG stop codons. Consequently, a gargantuan amount of work has gone into the replacement of these. One approach pioneered by the group of Prof. George Church from Harvard, was dubbed MAGE in CAGE: this relied on a multiplex transformation and subsequent strain recombination to remove all UAG codons—the latter part presented a halting point in a first paper, but was overcome. This resulted in the \"E. coli\" strain C321.ΔA, which lacks all UAG codons and RF1. This allowed an experiment to be done with this strain to make it \"addicted\" to the amino acid biphenylalanine by evolving several key enzymes to require it structurally, therefore putting its expanded genetic code under positive selection.\n\nIn addition to the amber codon, rare sense codons have also been considered for use. The AGG codon codes for arginine, but a strain has been successfully modified to make it code for 6-\"N\"-allyloxycarbonyl-lysine.\nAnother candidate is the AUA codon, which is unusual in that its respective tRNA has to differentiate against AUG that codes for methionine (primordially, isoleucine, hence its location). In order to do this, the AUA tRNA has a special base, lysidine. The deletion of the synthase (\"tilS\") was possible thanks to the replacement of the native tRNA with that of \"Mycoplasma mobile\" (no lysidine). The reduced fitness is a first step towards pressuring the strain to loose all instances of AUA, allowing it to be used for genetic code expansion.\n\nOther approaches include the addition of extra base pairing or the use of orthologous ribosomes that accept in addition to the regular triplet genetic code, tRNAs with quadruple code. This allowed the simultaneous usage of two unnatural amino acids, \"p\"-azidophenylalanine (AzPhe) and N6-[(2-propynyloxy)carbonyl]lysine (CAK), which cross-link with each other by Huisgen cycloaddition.\n\nAnother key element is the tRNA/synthase pair.\n\nThe orthologous set of synthetase and tRNA can be mutated and screened through directed evolution to charge the tRNA with a different, even novel, amino acid. Mutations to the plasmid containing the pair can be introduced by error-prone PCR or through degenerate primers for the synthetase's active site.\nSelection involves multiple rounds of a two-step process, where the plasmid is transferred into cells expressing chloramphenicol acetyl transferase with a premature amber codon. In the presence of toxic chloramphenicol and the non-natural amino acid, the surviving cells will have overridden the amber codon using the orthogonal tRNA aminoacylated with either the standard amino acids or the non-natural one. To remove the former, the plasmid is inserted into cells with a barnase gene (toxic) with a premature amber codon but without the non-natural amino acid, removing all the orthogonal synthases that do not specifically recognize the non-natural amino acid.\nIn addition to the recoding of the tRNA to a different codon, they can be mutated to recognize a four-base codon, allowing additional free coding options.\nThe non-natural amino acid, as a result, introduces diverse physicochemical and biological properties in order to be used as a tool to explore protein structure and function or to create novel or enhanced protein for practical purposes.\nThe orthogonal pairs of synthetase and tRNA that work for one organism may not work for another, as the synthetase may mis-aminoacylate endogenous tRNAs or the tRNA be mis-aminoacylated itself by an endogenous synthetase. As a result, the sets created to date differ between organisms.\n\n\n\nSimilarly to orthogonal tRNAs and aminoacyl tRNA synthetases (aaRSs), orthogonal ribosomes have been engineered to work in parallel to the natural ribosomes. Orthogonal ribosomes ideally use different mRNA transcripts than their natural counterparts and ultimately should draw on a separate pool of tRNA as well. This should alleviate some of the loss of fitness which currently still arises from techniques such as Amber codon suppression. Additionally, orthogonal ribosomes can be mutated and optimized for particular tasks, like the recognition of quadruplet codons. Such an optimization is not possible, or highly disadvantageous for natural ribosomes.\n\nIn 2005 three sets of ribosomes were published, which did not recognize natural mRNA, but instead translated a separate pool of orthogonal mRNA (o-mRNA). This was achieved by changing the recognition sequence of the mRNA, the Shine-Dalgarno sequence, and the corresponding recognition sequence in the 16S rRNA of ribosomes, the so-called Anti-Shine-Darlgarno-Sequence. This way the base pairing, which is usually lost if either sequence is mutated, stays available. However the mutations in the 16S rRNA were not limited to the obviously base-pairing nucleotides of the classical Anti-Shine-Darlgarno sequence.\n\nIn 2007 the group of Jason W. Chin presented an orthogonal ribosome, which was optimized for Amber codon suppression. The 16S rRNA was mutated in such a way that it bound the release factor RF1 less strongly than the natural ribosome does. This ribosome did not eliminate the problem of lowered cell fitness caused by suppressed stop codons in natural proteins. However through the improved specificity it raised the yields of correctly synthesized target protein significantly (from ~20% to >60% percent for one amber codon to be suppressed and form <1% to >20% for two amber codons).\n\nIn 2010 the group of Jason W. Chin presented a further optimized version of the orthogonal ribosome. The Ribo-Q is a 16S rRNA optimized to recognize tRNAs, which have quadruplet anti-codons to recognize quadruplet codons, instead of the natural triplet codons. With this approach the number of possible codons rises from 64 to 256. Even accounting for a variety of stop codons, more than 200 different amino acids could potentially be encoded this way.\n\nThe orthogonal ribosomes described above all focus on optimizing the 16S rRNA. Thus far, this optimized 16S rRNA was combined with natural large-subunits to form orthogonal ribosomes. If the 23S rRNA, the main RNA-component of the large ribosomal subunit, is to be optimized as well, it had to be assured, that there was no crosstalk in the assembly of orthogonal and natural ribosomes (see figureX B). To ensure that optimized 23S rRNA would only form into ribosomes with the optimized 16S rRNA, the two rRNAs were combined into one transcript. By inserting the sequence for the 23S rRNA into a loop-region of the 16S rRNA sequence, both subunits still adopt functioning folds. Since the two rRNAs are linked and thus in constant proximity, they preferably bind each other, not other free floating ribosomal subunits.\n\nIn 2014 it was shown that by altering the peptidyl transferase center of the 23S rRNA, ribosomes could be created which draw on orthogonal pools of tRNA. The 3’ end of tRNAs is universally conserved to be CCA. The two cytidines base pair with two guanines the 23S rRNA to bind the tRNA to the ribosome. This interaction is required for translational fidelity. However, by co-mutating the binding nucleotides in such a way, that they can still base pair, the translational fidelity can be conserved. The 3’-end of the tRNA is mutated from CCA to CGA, while two cytidine nucleotides in the ribosomes A- and P-sites are mutated to guanidine. This leads to ribosomes which do not accept naturally occurring tRNAs as substrates and to tRNAs, which cannot be used as substrate by natural ribosomes. \nTo use such tRNAs effectively, they would have to be aminoacylated by specific, orthogonal aaRSs. Most naturally occurring aaRSs recognize the 3’-end of their corresponding tRNA. aaRSs for these 3’-mutated tRNAs are not available yet. Thus far, this system has only been shown to work in an in-vitro translation setting where the aminoacylation of the orthogonal tRNA was achieved using so called “flexizymes”. Flexizymes are ribozymes with tRNA-amino-aclylation activity.\n\nWith an expanded genetic code, the unnatural amino acid can be genetically directed to any chosen site in the protein of interest. The high efficiency and fidelity of this process allows a better control of the placement of the modification compared to modifying the protein post-translationally, which, in general, will target all amino acids of the same type, such as the thiol group of cysteine and the amino group of lysine. Also, an expanded genetic code allows modifications to be carried out \"in vivo\".\nThe ability to site-specifically direct lab-synthesized chemical moieties into proteins allows many types of studies that would otherwise be extremely difficult, such as:\n\nThe expansion of the genetic code is still in its infancy. Current methodology uses only one non-standard amino acid at the time, whereas ideally multiple could be used.\n\nOne way to achieve the encoding of multiple unnatural amino acids is by rewriting genome synthetically. In 2010, at the cost of $40 million an organism, \"Mycoplasma laboratorium\", was constructed that was controlled by a synthetic genome. Due to the larger genome size this is not possible with \"E. coli\", however several methods are being developed to overcome this, such as the fragmentation of the genome into separate linear chromosomes.\nIn addition to the elimination of the usage of rare codons, the specificity of the system needs to be increased as many tRNA recognise several codons\n\nAnother approach is to expand the number of nucleobases to increase the coding capacity.\n\nAn unnatural base pair (UBP) is a designed subunit (or nucleobase) of DNA which is created in a laboratory and does not occur in nature. A demonstration of UBPs were achieved \"in vitro\" by Ichiro Hirao's group at RIKEN institute in Japan. In 2002, they developed an unnatural base pair between 2-amino-8-(2-thienyl)purine (s) and pyridine-2-one (y) that functions \"in vitro\" in transcription and translation for the site-specific incorporation of non-standard amino acids into proteins. In 2006, they created 7-(2-thienyl)imidazo[4,5-b]pyridine (Ds) and pyrrole-2-carbaldehyde (Pa) as a third base pair for replication and transcription. Afterward, Ds and 4-[3-(6-aminohexanamido)-1-propynyl]-2-nitropyrrole (Px) was discovered as a high fidelity pair in PCR amplification. In 2013, they applied the Ds-Px pair to DNA aptamer generation by \"in vitro\" selection (SELEX) and demonstrated the genetic alphabet expansion significantly augment DNA aptamer affinities to target proteins.\n\nIn 2012, a group of American scientists led by Floyd Romesberg, a chemical biologist at the Scripps Research Institute in San Diego, California, published that his team designed an unnatural base pair (UBP). The two new artificial nucleotides or \"Unnatural Base Pair\" (UBP) were named d5SICS and dNaM. More technically, these artificial nucleotides bearing hydrophobic nucleobases, feature two fused aromatic rings that form a (d5SICS–dNaM) complex or base pair in DNA. In 2014 the same team from the Scripps Research Institute reported that they synthesized a stretch of circular DNA known as a plasmid containing natural T-A and C-G base pairs along with the best-performing UBP Romesberg's laboratory had designed, and inserted it into cells of the common bacterium \"E. coli\" that successfully replicated the unnatural base pairs through multiple generations. This is the first known example of a living organism passing along an expanded genetic code to subsequent generations. This was in part achieved by the addition of a supportive algal gene that expresses a nucleotide triphosphate transporter which efficiently imports the triphosphates of both d5SICSTP and dNaMTP into \"E. coli\" bacteria. Then, the natural bacterial replication pathways use them to accurately replicate the plasmid containing d5SICS–dNaM.\n\nThe successful incorporation of a third base pair into a living micro-organism is a significant breakthrough toward the goal of greatly expanding the number of amino acids which can be encoded by DNA, thereby expanding the potential for living organisms to produce novel proteins. The artificial strings of DNA do not encode for anything yet, but scientists speculate they could be designed to manufacture new proteins which could have industrial or pharmaceutical uses.\n\nIn May 2014, researchers announced that they had successfully introduced two new artificial nucleotides into bacterial DNA, and by including individual artificial nucleotides in the culture media, were able to passage the bacteria 24 times; they did not create mRNA or proteins able to use the artificial nucleotides.\n\nThere have been many studies that have produced protein with non-standard amino acids, but they do not alter the genetic code. These protein, called alloprotein, are made by incubating cells with an unnatural amino acid in the absence of a similar coded amino acid in order for the former to be incorporated into protein in place of the latter, for example -2-aminohexanoic acid (Ahx) for methionine (Met).\n\nThese studies rely on the natural promiscuous activity of the aminoacyl tRNA synthetase to add to its target tRNA an unnatural amino acid (i.e. analog) similar to the natural substrate, for example methionyl-tRNA synthase's mistaking isoleucine for methionine. In protein crystallography, for example, the addition of selenomethionine to the media of a culture of a methionine-auxotrophic strain results in proteins containing selenomethionine as opposed to methionine (\"viz.\" Multi-wavelength anomalous dispersion for reason). Another example is that photoleucine and photomethionine are added instead of leucine and methionine to cross-label protein.\nSimilarly, some tellurium-tolerant fungi can incorporate tellurocysteine and telluromethionine into their protein instead of cysteine and methionine.\nThe objective of expanding the genetic code is more radical as it does not replace an amino acid, but it adds one or more to the code. On the other hand, proteome-wide replacements are most efficiently performed by global amino acid substitutions. For example, global proteome-wide substitutions of natural amino acids with fluorinated analogs have been attempted in \"E. coli\" and \"B. subtilis\". A complete tryptophan substitution with thienopyrrole-alanine in response to 20899 UGG codons in \"E. coli\" was reported in 2015 by Budisa and Söll. Moreover, many biological phenomena, such as protein folding and stability, are based on synergistic effects at many positions in the protein sequence.\n\nIn this context, the SPI method generates recombinant protein variants or alloproteins directly by substitution of natural amino acids with unnatural counterparts. An amino acid auxotrophic expression host is supplemented with an amino acid analog during target protein expression. This approach avoids the pitfalls of suppression-based methods and it is superior to it in terms of efficiency, reproducibility and an extremely simple experimental setup. Numerous studies demonstrated how global substitution of canonical amino acids with various isosteric analogs caused minimal structural perturbations but dramatic changes in thermodynamic, folding, aggregation spectral properties and enzymatic activity.\n\nThe genetic code expansion described above is \"in vivo\". An alternative is the change of coding \"in vitro\" translation experiments. This requires the depletion of all tRNAs and the selective reintroduction of certain aminoacylated-tRNAs, some chemically aminoacylated.\n\nThere are several techniques to produce peptides chemically, generally it is by solid-phase protection chemistry. This means that any (protected) amino acid can be added into the nascent sequence.\n\nOn November 2017, a team from the Scripps Research Institute reported having constructed a semi-synthetic \"E. coli\" bacteria genome using six different nucleic acids (versus four found in nature). The two extra 'letters' form a third, unnatural base pair. The resulting organisms were able to thrive and synthesize proteins using \"unnatural amino acids\". The unnatural base pair used is dNaM–dTPT3. This unnatural base pair has been demonstrated previously, but this is the first report of transcription and translation of proteins using an unnatural base pair.\n\n", "id": "16153022", "title": "Expanded genetic code"}
{"url": "https://en.wikipedia.org/wiki?curid=42676080", "text": "SIR proteins\n\nSilent Information Regulator (SIR) proteins are involved in regulating gene expression and some SIR family members are conserved from yeast to humans. SIR proteins organize heterochromatin at telomeres, rDNA and at silent loci including, in yeast, the hidden mating type loci. SIR family genes encode catalytic and non-catalytic proteins that are involved in de-acetylation of histone tails and the subsequent condensation of chromatin around a SIR protein scaffold.\n\nSIR proteins have been identified in many screens, and have historically been known as SIR (silent information regulator), MAR (mating-type regulator), STE (sterile), CMT (change of mating type) or SSP (sterile suppressor) according to which screen led to their identification. Ultimately, the name SIR had the most staying power, because it most accurately describes the function of the encoded proteins.\n\nOne of the early yeast screens to identify SIR genes was performed by Anita Hopper and Benjamin Hall, who screened with mutagenesis for alleles that allow sporulation in a normally sporulation-deficient heterothallic α/α (\"ho/ho MATα/MATα\"). Their screen identified a mutation in a novel gene that was not linked to \"HO\" that allowed the α/α diploid to sporulate, as if it were an α/a diploid, and inferred that the mutation affected a change in mating type by an \"HO\"-independent mechanism. Later, it was discovered at the CMT allele identified by Hopper & Hall did not cause a mating type conversion at the MAT locus, but rather allowed the expression of cryptic mating type genes that are silenced in wild-type yeast. In their paper clarifying the mechanism of the CMT mutation, Haber and George acknowledge the contribution of Amar Klar, who presented his MAR mutant strains that had similar properties as the CMT mutants at the Cold Spring Harbor Laboratory yeast genetics meeting, which led Haber and George to consider the hypotheses that the \"cmt\" mutants may act by de-repressing silent information.\n\nIn the same year that Haber & George demonstrated that the \"cmt\" mutant restores sporulation by de-repressing hidden mating type loci, two other groups published screens for genes involved in the regulation of silent mating type cassettes. The first study, performed by Amar Klar, Seymour Fogel and Kathy Macleod, identified a mutation in a spontaneous a/a diploid that caused the products of sporulation to be haploids with an apparent diploid phenotype, as assayed by ability to mate. The authors reasoned that the mutation caused the de-repression of then-recently appreciated silent mating type loci HMa and HMα, which would allow an a/a diploid to sporulate and would cause haploid segregants inheriting the mutant allele to behave as a/α diploids despite being haploid. The authors named the mutation MAR for its apparent role in mating type regulation, and were able to map the mutation to chromosome IV, and determined that it was located 27.3 cM from a commonly used \"trp1\" marker.\n\nA few months later, Jasper Rine and Ira Herskowitz published a different screen for genes that affect the ability of yeast to mate, and ultimate discovered the gene family that they called SIR, a name that remains in the modern parlance. Unlike the Klar et al. screen that identified a mutant by its inability to mate, Rine & Herskowitz took a more directed approach towards discovering factors responsible for mating type silencing. Specifically, Rine & Herskowitz reasoned that a haploid yeast cell with a recessive mutation in matα1 could be complemented if the silent copy of MATα were de-repressed. Starting in a \"ho matα1\" haploid strain, Rine & Herskowitz screened mutants arising from mutagenesis and identified five mutants that restored a MATα phenotype in matα cells, but were not linked to the MAT locus and did not cause a gene conversion between the HMα locus and matα. These mutants, they reasoned, were specifically defective in silencing the cryptic mating type genes.\n\nEventually, all of the mutants resulting from the original Hopper & Hall screen as well as the later Rine & Herskowitz screen and the Klar et al. screen were characterized and mapped, and it was shown that the causative genes were the same. In fact, the genes that are now referred to as SIR1-4 have at one time been referred to as MAR, CMT or STE according to the screen that identified the mutants.\n\nAlthough Klar, Hartwell and Hopper identified mutations in SIR genes and applied other names to the genes before Rine performed his screen, the SIR name was eventually adopted because Rine eventually identified the most complete set of functionally related genes (SIR1-4), and because the work by Rine and Herskowitz most accurately described the function of the SIR family genes. Later it would be shown that in yeast and in higher organisms, SIR proteins are important for transcriptional regulation of many chromatin domains.\n\nIn budding yeast, SIR proteins are found at the silent mating type loci, telomeres, and at the rDNA locus. At the silent mating type loci and at the telomeres, SIR proteins participate in transcriptional silencing of genes within their domain of localization. At the rDNA locus, SIR proteins are thought to primarily be important for repressing recombination between rDNA repeats rather than for suppressing transcription.\n\nIn transcriptional silencing, SIR2,3,4 are required in stoichiometric amounts to silence genes in \"cis\". In yeast, SIR proteins bind sites on nucleosome tails and form a multimeric compound of SIR2,3,4 that condenses chromatin and is thought to physically occlude promoters in the silenced interval, preventing their interaction with transcription machinery. The establishment of SIR-reppressed heterochromatin domains is a complicated process that involves different subsets of proteins and regulatory proteins depending on the locus in the genome. At the silent mating type loci and at yeast telomeres, the transcription factors Abf1 (ARS binding factor) and Rap1 (repressor-activator protein) associate with specific nucleotide sequences in the silencers that flank heterochromatic regions. Rap1 contains a Sir3-binding domain that recruits SIR3 to the silencers. Once at the silencers, Sir3 recruits Sir4-Sir2 dimers to the chromatin nucleation site. Sir2 then deacetylates histone H3 and H4 tails, and free Sir3 binds the now-deacetylated lysine residues H4K16,79, and recruits additional Sir4-Sir2 dimers to promote the further spreading of the heterochromatin domain.\n\nOnce it has spread to cover a genomic locus, the SIR2,3,4 effectively prevents transcription from the region it occupies, in a process that is thought to depend on the physical occlusion of DNA by SIR proteins. Recently, it has been shown that certain promoters are capable of directing transcription inside regions that are otherwise silenced by SIR proteins. Specifically, if an inducible promoter is induced inside a silent chromatin domain, it can achieve ~200x increase in expression levels with little detectable change in covalent histone modifications.\n\nSIR proteins are conserved from yeast to humans, and lend their name to a class of mammalian histone deacetylases (Sirtuins, homologs of Sir2). Sirtuins have been implicated in myriad human traits including Alzheimers and diabetes, and have been proposed to regulate of lifespan.\n", "id": "42676080", "title": "SIR proteins"}
{"url": "https://en.wikipedia.org/wiki?curid=42700066", "text": "Cytodeme\n\nCytodeme first emerged as a printed word more than 60 years ago in a book by Heslop-Harrison. Discussing the \"Deme Terminology\" - he continued \"\"cytodeme, a population differing in some distinctive cytological feature from others.\"\" More precisely the cytodeme may be defined as \"the total assembly of all those individuals/organisms that use an identical specific suite of chromosomes to carry their genes\". In most cases the suite is composed of several pairs of homologous chromosomes with or without a pair of sex chromosomes. Since the only acceptable proof of the identity (homology) of chromosomes lies in their ability to pair fully from end to end during meiosis it follows that:\n\n\nAs a general rule for most species all of its members are of the same cytodeme excepting only the infrequent aneuploid aberrants. However some species are known to include several chromosome races which must necessarily belong to different cytodemes. Although it is not intrinsic in the formal definition it is a matter of commom observation that members of different cytodemes are essentially cross-incompatible and any hybrids that do arise are usually highly infertile.\n\nThe current surgance of interest in cytodemes stems from the realisation that membership of the cytodeme is not necessarily restricted to the members of one species: frequently two or more species are in fact of the same cytodeme. Although fully intercompatible and yielding fertile hybrids when they do cross these species never or rarely cross-breed in the natural environment because they are spatially isolated, geographically and/or ecologically. Typically the species involved are members of the same genus. There are now a substantial number of cases in which two or more species of the same cytodeme are clearly different and even in different genera. Thus all five diploid species recognised in the genus \"Brassicella\" and both species recognised in the genus \"Hutera\" (7spp. in total) are in the same cytodeme. Perhaps the most extreme case on record concerns the wild grass, teosinte, \"Euchlaena mexicana\" and the strikingly different Maize, or Indian Corn, \"Zea mays\", both 2n=20, fully interfertile and yielding fertile hybrids. There is perhaps a remote possibility that two such very different species could have evolved independently from distinct sources and converged in their chromosomal ideotype until they became members of the same cytodeme (including the capacity to cross-breed). It is more likely, though, that the cytodeme arose first complete with its suite of chromosomes and breeding patterns all intact and then, remaining constant in its fundamentals, it diversified into species sometimes so different as to merit generic distinction. Thus, in what may be termed the \"cytodeme adjunct\" to Darwinian theory, evolution becomes a two-stage process - first, the establishment of distinct cytodemes reproductively isolated both from one another and from all previously existing cytodemes; second, diversification within cytodeme to yield taxonomically recognisable (but not reproductively isolated) species.\n\nWhereas the second stage is Natural Selection as expounded by Darwin, the first stage is not necessarily Darwinian. In the special case of polyploidy it is known that the first stage is not Darwinian. Doubling the chromosome number of the sterile hybrid between two diploid cytodemes yields an allotetraploid which is both fertile within its bounds and essentially incompatible with all previous life forms.\n\nAs yet no mechanism is known which could account for the origin of new diploid cytodemes - nor is there any proof that there are any diploid cytodemes of recent origin. Indeed, the evidence is that cytodemes are remarkably long lived. Presumably the \"Platanus\" cytodeme existed on Pangaea before that land fragmented. The present day forms of \"Platanus\" - \"P. orientalis\" in Europe and \"P. occidentalis\" in America - are fully intercompatible when brought together artificially in Botanic Gardens yielding fertile hybrids like the London plane, establishing not only that they are in the same cytodeme as one another but also that they are both in the same cytodeme as their common ancestor of approximately 200 million years ago.\n\n", "id": "42700066", "title": "Cytodeme"}
{"url": "https://en.wikipedia.org/wiki?curid=14436317", "text": "Doubled haploidy\n\nA doubled haploid (DH) is a genotype formed when haploid cells undergo chromosome doubling. Artificial production of doubled haploids is important in plant breeding.\n\nHaploid cells are produced from pollen or egg cells or from other cells of the gametophyte, then by induced or spontaneous chromosome doubling, a doubled haploid cell is produced, which can be grown into a doubled haploid plant. If the original plant was diploid, the haploid cells are monoploid, and the term doubled monoploid may be used for the doubled haploids. Haploid organisms derived from tetraploids or hexaploids are sometimes called dihaploids (and the doubled dihaploids are, respectively, tetraploid or hexaploid).\n\nConventional inbreeding procedures take six generations to achieve approximately complete homozygosity, whereas doubled haploidy achieves it in one generation. Dihaploid plants derived from tetraploid crop plants may be important for breeding programs that involve diploid wild relatives of the crops.\n\nThe first report of the haploid plant was published by Blakeslee \"et al.\" (1922) in \"Datura stramonium\". Subsequently, haploids were reported in many other species. Guha and Maheshwari (1964) developed an anther culture technique for the production of haploids in the laboratory. Haploid production by wide crossing was reported in barley (Kasha and Kao, 1970) and tobacco (Burk \"et al.\", 1979). Tobacco, rapeseed, and barley are the most responsive species for doubled haploid production. Doubled haploid methodologies have now been applied to over 250 species.\n\nDoubled haploids can be produced \"in vivo\" or \"in vitro\". Haploid embryos are produced \"in vivo\" by parthenogenesis, pseudogamy, or chromosome elimination after wide crossing. The haploid embryo is rescued, cultured, and chromosome-doubling produces doubled haploids. The \"in vitro\" methods include gynogenesis (ovary and flower culture) and androgenesis (anther and microspore culture). Androgenesis is the preferred method. Another method of producing the haploids is wide crossing. In barley, haploids can be produced by wide crossing with the related species \"Hordeum bulbosum\"; fertilization is affected, but during the early stages of seed development the \"H. bulbosum\" chromosomes are eliminated leaving a haploid embryo. In tobacco (\"Nicotiana tabacum\"), wide crossing with \"Nicotiana africana\" is widely used. When \"N. africana\" is used to pollinate \"N. tabacum\", 0.25 to 1.42 percent of the progeny survive and can readily be identified as either F1 hybrids or maternal haploids. Although these percentages appear small, the vast yield of tiny seeds and the early death of most seedlings provide significant numbers of viable hybrids and haploids in relatively small soil containers. This method of interspecific pollination serves as a practical way of producing seed-derived haploids of \"N. tabacum\", either as an alternative method or complementary method to anther culture.\n\nIn DH method only two types of genotypes occur for a pair of alleles, A and a, with the frequency of ½ AA and ½ aa, while in diploid method three genotypes occur with the frequency of ¼ AA, ½ Aa, ¼ aa. Thus, if AA is desirable genotype, the probability of obtaining this genotype is higher in haploid method than in diploid method. If n loci are segregating, the probability of getting the desirable genotype is (1/2)n by the haploid method and (1/4)n by the diploid method. Hence the efficiency of the haploid method is high when the number of genes concerned is large.\nStudies were conducted comparing DH method and other conventional breeding methods and it was concluded that adoption of doubled haploidy does not lead to any bias of genotypes in populations, and random DHs were even found to be compatible to selected line produced by conventional pedigree method.\n\nMost of the economic traits are controlled by genes with small but cumulative effects. Although the potential of DH populations in quantitative genetics has been understood for some time, it was the advent of molecular marker maps that provided the impetus for their use in identifying loci controlling quantitative traits. As the quantitative trait loci (QTL) effects are small and highly influenced by environmental factors, accurate phenotyping with replicated trials is needed. This is possible with doubled haploidy organisms because of their true breeding nature and because they can conveniently be produced in large numbers. Using DH populations, 130 quantitative traits have been mapped in nine crop species. In total, 56 DH populations were used for QTL detection.\n\nIn backcross conversion, genes are introgressed from a donor cultivar or related species into a recipient elite line through repeated backcrossing. A problem in this procedure is being able to identify the lines carrying the trait of interest at each generation. The problem is particularly acute if the trait of interest is recessive, as it will be present only in a heterozygous condition after each backcross. The development of molecular markers provides an easier method of selection based on the genotype (marker) rather than the phenotype. Combined with doubled haploidy it becomes more effective. In marker assisted backcross conversion, a recipient parent is crossed with a donor line and the hybrid (F1) backcrossed to the recipient. The resulting generation (BC1) is backcrossed and the process repeated until the desired genotypes are produced. The combination of doubled haploidy and molecular marker provides the short cut. In the back cross generation one itself a genotype with the character of interest can be selected and converted into homozygous doubled haploid genotype. Chen \"et al.\" (1994) used marker assisted backcross conversion with doubled haploidy of BC1 individuals to select stripe rust resistant lines in barley.\n\nIn bulked segregant analysis, a population is screened for a trait of interest and the genotypes at the two extreme ends form two bulks. Then the two bulks are tested for the presence or absence of molecular markers. Since the bulks are supposed to contrast in the alleles that contribute positive and negative effects, any marker polymorphism between the two bulks indicates the linkage between the marker and trait of interest. BSA is dependent on accurate phenotyping and the DH population has particular advantage in that they are true breeding and can be tested repeatedly. DH populations are commonly used in bulked segregant analysis, which is a popular method in marker assisted breeding. This method has been applied mostly to rapeseed and barley.\n\nGenetic maps are very important to understand the structure and organization of genomes from which evolution patterns and syntenic relationships between species can be deduced. Genetic maps also provide a framework for the mapping of genes of interest and estimating the magnitude of their effects and aid our understanding of genotype/phenotype associations. DH populations have become standard resources in genetic mapping for species in which DHs are readily available. Doubled haploid populations are ideal for genetic mapping. It is possible to produce a genetic map within two years of the initial cross regardless of the species. Map construction is relatively easy using a DH population derived from a hybrid of two homozygous parents as the expected segregation ratio is simple, \"i.e.\" 1:1. DH populations have now been used to produce genetic maps of barley, rapeseed, rice, wheat, and pepper. DH populations played a major role in facilitating the generation of the molecular marker maps in eight crop species.\n\nGenetic ratios and mutation rates can be read directly from haploid populations. A small doubled haploid (DH) population was used to demonstrate that a dwarfing gene in barley is located chromosome 5H. In another study the segregation of a range of markers has been analyzed in barley.\n\nAlthough QTL analysis has generated a vast amount of information on gene locations and the magnitude of effects on many traits, the identification of the genes involved has remained elusive. This is due to poor resolution of QTL analysis. The solution for this problem would be production of recombinant chromosome substitution line, or stepped aligned recombinant inbred lines. Here, backcrossing is carried out until a desired level of recombination has occurred and genetic markers are used to detect desired recombinant chromosome substitution lines in the target region, which can be fixed by doubled haploidy. In rice, molecular markers have been found to be linked with major genes and QTLs for resistance to rice blast, bacterial blight, and sheath blight in a map produced from DH population.\n\nTraditional breeding methods are slow and take 10–15 years for cultivar development. Another disadvantage is inefficiency of selection in early generations because of heterozygosity.\nThese two disadvantages can be over come by DHs, and more elite crosses can be evaluated and selected within less time.\n\nUniformity is a general requirement of cultivated line in most species, which can be easily obtained through DH production. There are various ways in which DHs can be used in cultivar production. The DH lines themselves can be released as cultivars, they may be used as parents in hybrid cultivar production or more indirectly in the creation of breeders lines and in germplasm conservation. Barley has over 100 direct DH cultivars. According to published information there are currently around 300 DH derived cultivars in 12 species worldwide.\n\nThe relevance of DHs to plant breeding has increased markedly in recent years owing to the development of protocols for 25 species. Doubled haploidy already plays an important role in hybrid cultivar production of vegetables, and the potential for ornamental production is being vigorously examined. DHs are also being developed in the medicinal herb \"Valeriana officinalis\" to select lines with high pharmacological activity. Another interesting development is that fertile homozygous DH lines can be produced in species that have self-incompatibility systems.\n\nThe ability to produce homozygous lines after a single round recombination saves a lot of time for the plant breeders. Studies conclude that random DH’s are comparable to the selected lines in pedigree inbreeding. The other advantages include development of large number of homozygous lines, efficient genetic analysis and development of markers for useful traits in much less time. More specific benefits include the possibility of seed propagation as an alternative to vegetative multiplication in ornamentals, and in species such as trees in which long life cycles and inbreeding depression preclude traditional breeding methods, doubled haploidy provides new alternatives.\n\nThe main disadvantage with the DH population is that selection cannot be imposed on the population. But in conventional breeding selection can be practised for several generations: thereby desirable characters can be improved in the population.\n\nIn haploids produced from anther culture, it is observed that some plants are aneuploids and some are mixed haploid-diploid types. Another disadvantage associated with the double haploidy is the cost involved in establishing tissue culture and growth facilities. The over-usage of doubled haploidy may reduce genetic variation in breeding germplasm. Hence one has to take several factors into consideration before deploying doubled haploidy in breeding programmes.\n\nTechnological advances have now provided DH protocols for most plant genera. The number of species amenable to doubled haploidy has reached a staggering 250 in just a few decades. Response efficiency has also improved with gradual removal of species from recalcitrant category. Hence it will provide greater efficiency of plant breeding.\n\n\n", "id": "14436317", "title": "Doubled haploidy"}
{"url": "https://en.wikipedia.org/wiki?curid=42900649", "text": "Beanbag genetics\n\nBeanbag genetics is a conceptual model of genetics which was used by early Mendelians, who used to keep colored beans in bags as a way of tracking Mendelian ratios. To be able to understand beanbag genetics, the meaning of population has to change. Population is no longer a group of individuals in an area but are all the Alleles in an area that assort and segregate separately. All of the alleles become the gene pool. When using the beanbag approach, there are two ways that the gene pool can be viewed. The first is to view the gene pool as all the alleles that represent all the traits in the population be viewed at once. The second, the gene pool could only be alleles for a single trait in the population. Once the alleles are chosen for the gene pool they are selected for at random. Phrase was first coined by Ernst Mayr in criticizing the work of Ronald Fisher and J. B. S. Haldane who treated genes as independent entities to simplify their mathematical analysis of population genetics. Mayr created the name, beanbag, because all the alleles were thought of like beans in a beanbag. The beanbag full of beans would be considered the gene pool for the population.\n\nThe paper 'Perils of “industrial gene” and “beanbag genetics”' written in 2008 by Dr. Lakhotia, discusses that biotechnology needs to be reassessed before continuing down the path that it is going down now. Dr. Lakhotia believes that overall everything in our genome needs to be looked at rather than just coding regions. We do know that our genetic material forms and organic system that is built by multiple smaller units working together. Since we know that multiple smaller units are used, scientists need to look at more than just the structural gene. We should look at the entire genome because know that even in the non-coding regions of our DNA is involved in the regulatory network.\n\nThe debate of BeanBag Genetics has been occurring for many years now. One thing scientist have agreed on is that BeanBag genetics is too simplistic and needs to be built upon. Sir Ronald Fisher and Sewall Wright are two scientist that were once apart of this debate. Fisher's assumptions were that traits are determined by many loci, populations are well mixed, and mating is random. Wright's assumptions were his Shifting balance theory that included: Random Drift, Individual selection, and Interdeme Selection. As both argue their theories, people agree with Fisher's theory because it works and is a well developed theory. Other people agree with Wright because his theory is closer to the real world and provides an aesthetically pleasing view of evolution. Even though both theories have parts that are right they both are wrong as well. Fisher's theory does not explain the diversity of the world and Wright's theory is just a metaphor not a well developed theory. So how do they know which theory is correct? Even though both theories are right and wrong, they both see a part of a larger picture.\n\n\n", "id": "42900649", "title": "Beanbag genetics"}
{"url": "https://en.wikipedia.org/wiki?curid=42911981", "text": "Allelotype\n\nAllelotype describes the occurrence of an allele in a population. Specifically, it describes the frequency distribution of a given set of alleles in a population. Allelotype is important for the field of population genetics, particularly when studying complex or multifactorial disorders such as cancer. Determining tumor allelotypes increases the understanding of the underlying tumorigenesis and improves the prognosis in tumor patients.\n", "id": "42911981", "title": "Allelotype"}
{"url": "https://en.wikipedia.org/wiki?curid=42912705", "text": "Genocopy\n\nGenocopy is a trait that is a phenotypic copy of a genetic trait but is caused by a different genotype. A genotype at one locus that produces a phenotype that at some levels of resolution is indistinguishable from that produced by another genotype; two types of elliptocytosis that are genocopies of each other, but are distinguished by the fact that one is linked to the Rh blood group locus and the other is not. The way to distinguish a recessive genocopy from a phenotype caused by a different allele would be by carrying out a test cross, breeding the two together, if they F1 Hybrid segregates 1:2:1 then we can determine that it was a genocopy.\n\n", "id": "42912705", "title": "Genocopy"}
{"url": "https://en.wikipedia.org/wiki?curid=30101225", "text": "Sterility (physiology)\n\nSterility is the physiological inability to effect sexual reproduction in a living thing, members of whose kind have been produced sexually. Sterility has a wide range of causes. It may be an inherited trait, as in the mule; or it may be acquired from the environment, for example through physical injury or disease, or by exposure to radiation.\n\nSterility can be caused by different closely related species breeding and producing offspring, these animals are usually sterile due to different numbers of chromosomes from the two parents, causing an imbalance in the resulting offspring making it viable but not fertile, this is the case with the mule. Sterility can also be caused by chromosomal differences within the patient, these individuals tend to be known as a genetic mosaics. Loss of part of a chromosome can also cause sterility due to nondisjunction. Sterility can also be caused by selective breeding, where the trait you are selecting for is closely linked to genes involved in sex determination or fertility, for example goats breed to be polled (hornless), this results in a high number of intersex individuals among the offspring, which are typically sterile. \nXX male syndrome is another cause of sterility, this is where the sexual determining factor on the Y chromosome (SRY) is transferred to the X chromosome due to an unequal crossing over, this gene indicated what gender the individual should be and causes the development of testes, causing the individual to be phenotypically male but genotypically female, the resulting individual is (information needed).\n\nEconomic uses of sterility include:\n", "id": "30101225", "title": "Sterility (physiology)"}
{"url": "https://en.wikipedia.org/wiki?curid=42663845", "text": "Epigenetics of physical exercise\n\nEpigenetics of physical exercise is the study of epigenetic modifications resulting from physical exercise to the genome of cells. Epigenetic modifications are heritable alterations that are not due to changes in the sequence of nucleotides. Epigenetic modifications, such as histone modifications and DNA methylation, alter the accessibility to DNA and change chromatin structure, thereby regulating patterns of gene expression. Methylated histones can act as binding sites for certain transcription factors due to their bromodomains and chromodomains. Methylated histones can also prevent the binding of transcription factors by hiding the transcription factor's recognition site, which is usually found on the major groove of DNA. The methyl groups bound to the cytosine residues lie in the major groove of DNA, the same region most transcription factors use to read a DNA sequence. A common epigenetic tag found in DNA is the covalent attachment of a methyl group to the C5 position of the cytosine found in CpG dinucleotide sequences. CpG methylation is an important mechanism of transcriptional silencing. Methylation of CpG islands is shown to reduce gene expression by the formation of tightly condensed heterochromatin that is transcriptionally inactive. CpG sites in a gene are most commonly found in the promoter regions of a gene while also being present in non promoter regions. The CpG sites in non promoter regions tend to be constitutively methylated, causing transcription machinery to ignore them as possible promoters. The CpG site near promoter regions are mostly left unmethylated until a cell decides to methylate them and repress transcription. Methylation of CpGs in promoter regions result in the transcriptional silencing of a gene. Environmental factors including physical exercise have been shown to have a beneficial influence on epigenetic modifications.\n\nPhysical exercise leads to epigenetic modifications that can have beneficial effects in cancer patients. The effect of physical exercise on DNA methylation patterns leads to increased expression of genes associated with tumor suppression and decreased expression of oncogenes. Cancer cells have non-normal patterns of DNA methylation including hypermethylation in promoter regions for tumor-suppressing genes and hypomethylation in promoter regions of oncogenes. These epigenetic mutations in cancer cells cause the cell to grow and divide uncontrollably, resulting in tumorigenesis. Physical exercise has been shown to reduce and even reverse these epigenetic mutations, increasing expression levels of tumor-suppressing genes and decreasing expression levels of oncogenes.\n\nHypermethylation in the promoter regions of tumor suppressor genes is thought to help cause some forms of cancer. The hypermethylation in the promoter regions of the tumor suppressing genes APC and RASSF1A are common epigenetic markers for cancer. The APC gene functions to make sure cells divide properly and maintain a correct number of chromosomes after division has completed. The RASSF1A gene product interacts with the DNA repair protein XPA. Physical exercise has been shown to decrease and even reverse these promoter hypermethylation, lowering the risk of the development of cancer. Decreased hypermethylation patterns reveal a transcriptionally accessible promoter region, allowing for increased expression of the tumor suppressing genes.\n\nPhysical exercise increases levels of eustress, or good stress, on the body. This eustress stimulates epigenetic modifications affecting the DNA genome of cancer cells. Environmental conditions, such as eustress, strongly induces expression of the tumor suppressor TP53 gene by influencing epigenetic modifications to be made to the cancer cells genome. The TP53 gene codes for the p53 protein, a protein important in the apoptotic pathway of programmed cell death. The p53 protein is important for the regulation of cell growth and apoptosis, so hypermethylation of the TP53 promoter region are common markers associated with the development of cancer. Other than methylation patterns affecting expression of TP53, microRNAs and antisense RNAs control the levels of the p53 protein by regulating expression of the p53 coding TP53 gene.\n\nIn a study on the epigenetic effects of physical exercise on breast cancer in women, blood samples from breast cancer patients were collected before and after 6 months of moderate-intensity aerobic exercise. The test group experienced 129 minutes of exercise on average per week compared to the control group’s 21.8 minutes a week. The study found 43 genes having significant changes in DNA methylation. Of the 43 genes, 3 of the genes experiencing reduced methylation levels were directly correlated with increased survival of breast cancer. The gene L3MBTL1, a known tumor suppressor, had methylation levels decreased by 1.48% in the exercise group while the limited exercise control group experienced a 2.15% increase in methylation. The 1.48% decrease in methylation of L3MBTL1 resulted in greater expression of the tumor suppressor while the 2.15% increase in methylation experienced by the limited exercise control group led to a decrease in expression. The findings of the study showed patients who exercised regularly had lower methylation levels and higher gene expression of L3MBTL1. These patients also experienced a greater than 60% reduction in risk of breast cancer death compared to patients in the limited exercise group.\n\nEpigenetic mechanisms affected by physical exercise have also been seen to be involved in age-related processes. A major component of aging is significant loss of DNA methylation over time. Methyl deoxycytidine, which is a methylated cytosine on the 5’ carbon of a cytosine, is involved in the process of cell differentiation and maintenance. Cell differentiation involves methylation of different areas within the DNA of a cell, which can alter the transcription of genes. During cell differentiation, DNA methylation is important for establishing the identity and function of a cell because of its role in controlling gene expression. A recent study looking at genome DNA methylation of newborn infants and humans aged 100 years or older found that the older individuals had significantly decreased overall DNA methylation. As one ages, the amount of DNA methylation slowly begins to decrease.\n\nStudies have also looked at methyl deoxycytidine residues from tissues collected from rodents at various ages. These studies found that DNA methylation loss increased significantly as the rodent aged. Thus, aging is related to a significant loss in DNA methylation. However, this loss of DNA methylation appears to be slowed by physical exercise. Further studies have looked at the effects of physical exercise on DNA methylation and aging in humans. This found that genome wide DNA methylation in adult individuals who obtained thirty or more minutes of exercise a day had significantly more DNA methylation as compared to sedentary individuals. Thus, physical exercise can affect aging through slowing the rate of the loss of DNA methylation over time.\n\nAnother component of aging is the gradual shortening of telomeres located at the end of chromosomes. Telomeres are repetitive sequences located at the end of chromosomes whose purpose are to slow the process of shortening and cell damage which occurs after every cell division as well as stabilize the ends of DNA. Aging and age-related diseases are associated with the significant shortening of these sequences. The shrinking of telomeres occurs in somatic cells where telomerase, the enzyme in control of telomere lengthening, is not expressed.\n\nHowever, it has been seen that telomeres can transcribe non-coding RNA, or functional RNAs that do not get translated into protein. Research has demonstrated that some of the non-coding RNAs transcribed at telomeres are involved in heterochromatin formation and stability of the telomeres. These non-coding RNAs can be positively impacted by physical exercise. Notably, a study found that mice exposed to short-term running phases had increased non-coding RNA transcription at telomeres as compared to sedentary controls. This increase in non-coding RNA transcription aided telomere stability, making the exercise group's telomeres less likely to be as affected by aging over time. Through helping to increase telomere stability, physical exercise can have positive impacts on aging by helping to decreasing the shortening of telomeres.\n\nIn addition to restructuring the muscular and skeletal system to better handle mechanical stress, physical exercise also affects gene expression with respect to metabolism. The effects are widespread and can affect anything from muscle growth to aerobic stamina to diabetes and other metabolic disorders.\n\nIn general, even a small amount of exercise can induce hypomethylation of the whole genome within muscle cells. This means that many regulatory genes can be turned on for pathways like muscle repair and growth. The intensity of the exercise directly correlates to the amount of promoter demethylation, so more strenuous exercise activates more genes.\n\nMicroRNAs (miRNAs) interfere with mRNA that is present and render it unusable and therefore decrease the product of that mRNA. MiRNAs regulate many physiological processes, such as inflammation, angiogenesis (the creation of blood vessels), as well as ischemia (the restriction of blood flow within the vessels) prevention. Aerobic exercise reduces the overall number of various miRNAs within the skeletal muscle that produce negative effects. Stimuli that cause the body to enter an anabolic, or constructive, phase, such as resistance training as well as the correct diet, has also shown a reduction of miRNAs. This reduction may actually play a role in the growth of the muscle cell.\n\nClass IIa Histone deacetyltransferases (HDACs) are highly expressed within human skeletal muscles. Exercise helps to reduce their activity, especially at promoters, which affects gene expression. In mice, this regulation of HDAC5 has been shown to increase the amount of type I fibers in muscle. Type I fibers are slow twitch, endurance fibers. This data agrees with human data that says the amount of type I fibers is positively correlated with the maximal aerobic capacity.\n\nIt also suggested that the amount of type 1 fibers is correlated with a histone acetyltransferase (HAT) that is involved in osteoblast differentiation and bone formation.\n\nIndividuals with type II diabetes have hypermethylation of several genes within the muscle, like peroxisome proliferator-activated receptor gamma(PPAR-γ) and coactivator 1 alpha(PGC-1α). The hypermethylation of these genes decreases the expression of both mitochondrial DNA as well as PGC-1α mRNA. Exercise is a way to prevent and treat these effects by helping to hypomethylate PPAR-γ and PGC-1α. Additionally, exercise also increases expression of glucose transporter type 4 (GLUT4), which will also help with diabetes symptoms.\n\nWith further knowledge of epigenetic pathways, exercise will continue to show its benefits in all phases of life including but not limited to cancer prevention and treatment, aging, metabolism and metabolic disorders like diabetes.\n", "id": "42663845", "title": "Epigenetics of physical exercise"}
{"url": "https://en.wikipedia.org/wiki?curid=9894237", "text": "Gene nomenclature\n\nGene nomenclature is the scientific naming of genes, the units of heredity in living organisms. An international committee published recommendations for genetic symbols and nomenclature in 1957. The need to develop formal guidelines for human gene names and symbols was recognized in the 1960s and full guidelines were issued in 1979 (Edinburgh Human Genome Meeting). Several other genus-specific research communities (e.g., \"Drosophila\" fruit flies, \"Mus\" mice) have adopted nomenclature standards, as well, and have published them on the relevant model organism websites and in scientific journals, including the \"Trends in Genetics\" Genetic Nomenclature Guide. Scientists familiar with a particular gene family may work together to revise the nomenclature for the entire set of genes when new information becomes available. For many genes and their corresponding proteins, an assortment of alternate names is in use across the scientific literature and public biological databases, posing a challenge to effective organization and exchange of biological information. Standardization of nomenclature thus tries to achieve the benefits of vocabulary control and bibliographic control, although adherence is voluntary. The advent of the information age has brought gene ontology, which in some ways is a next step of gene nomenclature, because it aims to unify the representation of gene and gene product attributes across all species.\n\nGene nomenclature and protein nomenclature are not separate endeavors; they are aspects of the same whole. Any name or symbol used for a protein can potentially also be used for the gene that encodes it, and vice versa. But owing to the nature of how science has developed (with knowledge being uncovered bit by bit over decades), proteins and their corresponding genes have not always been discovered simultaneously (and not always physiologically understood when discovered), which is the largest reason why protein and gene names do not always match, or why scientists tend to favor one symbol or name for the protein and another for the gene. Another reason is that many of the mechanisms of life are the same or very similar across species, genera, orders, and phyla, so that a given protein may be produced in many kinds of organisms; and thus scientists naturally often use the same symbol and name for a given protein in one species (for example, mice) as in another species (for example, humans). Regarding the first duality (same symbol and name for gene or protein), the context usually makes the sense clear to scientific readers, and the nomenclatural systems also provide for some specificity by using italic for a symbol when the gene is meant and plain (roman) for when the protein is meant. Regarding the second duality (a given protein is endogenous in many kinds of organisms), the nomenclatural systems also provide for at least human-versus-nonhuman specificity by using different capitalization, although scientists often ignore this distinction, given that it is often biologically irrelevant.\n\nAlso owing to the nature of how scientific knowledge has unfolded, proteins and their corresponding genes often have several names and symbols that are synonymous. Some of the earlier ones may be deprecated in favor of newer ones, although such deprecation is voluntary. Some older names and symbols live on simply because they have been widely used in the scientific literature (including before the newer ones were coined) and are well established among users.\n\nLastly, some proteins and protein complexes are built from the products of several genes (each gene contributing a polypeptide subunit), which means that the protein or complex will not have the same name or symbol as any one gene.\n\nThe HUGO Gene Nomenclature Committee is responsible for providing human gene naming guidelines and approving new, unique human gene names and symbols (short identifiers typically created by abbreviating). For some nonhuman species, model organism databases serve as central repositories of guidelines and help resources, including advice from curators and nomenclature committees. In addition to species-specific databases, approved gene names and symbols for many species can be located in the National Center for Biotechnology Information's \"Entrez Gene\" database.\n\nThere are generally accepted rules and conventions used for naming genes in bacteria. Standards were proposed in 1966 by Demerec et al.\n\nEach bacterial gene is denoted by a mnemonic of three lower case letters which indicate the pathway or process in which the gene-product is involved, followed by a capital letter signifying the actual gene. In some cases, the gene letter may be followed by an allele number. All letters and numbers are underlined or italicised. For example, \"leuA\" is one of the genes of the leucine biosynthetic pathway, and \"leuA273\" is a particular allele of this gene.\n\nWhere the actual protein coded by the gene is known then it may become part of the basis of the mnemonic, thus:\n\nSome gene designations refer to a known general function:\n\nLoss of gene activity leads to a nutritional requirement (auxotrophy) not exhibited by the wildtype (prototrophy).\n\nAmino acids:\n\nSome pathways produce metabolites that are precursors of more than one pathway. Hence, loss of one of these enzymes will lead to a requirement for more than one amino acid. For example:\n\nNucleotides:\n\nVitamins:\n\nLoss of gene activity leads to loss of the ability to catabolise (use) the compound.\n\n\n\n\nIf the gene in question is the wildtype a superscript '+' sign is used:\n\nIf a gene is mutant, it is signified by a superscript '-':\n\nBy convention, if neither is used, it is considered to be mutant.\n\nThere are additional superscripts and subscripts which provide more information about the mutation:\n\n\nOther modifiers:\n\nWhen referring to the genotype (the gene) the mnemonic is italicized and not capitalised. When referring to the gene product or phenotype, the mnemonic is first-letter capitalised and not italicized (\"e.g.\" DnaA – the protein produced by the \"dnaA\" gene; LeuA – the phenotype of a \"leuA\" mutant; Amp – the ampicillin-resistance phenotype of the β-lactamase gene \"bla\").\n\nProtein names are the same as the gene names, but the protein names are not italicized, and the first letter is upper-case. E.g. the name of RNA polymerase is RpoB, and this protein is encoded by \"rpoB\" gene.\n\nThe research communities of vertebrate model organisms have adopted guidelines whereby genes in these species are given, whenever possible, the same names as their human orthologs. The use of prefixes on gene symbols to indicate species (e.g., \"Z\" for zebrafish) is discouraged. The recommended formatting of printed gene and protein symbols varies between species.\n\nVertebrate genes and proteins have names (typically strings of words) and symbols, which are short identifiers (typically 3 to 8 characters). For example, the gene cytotoxic T-lymphocyte-associated protein 4 has the HGNC symbol \"CTLA4\". These symbols are usually, but not always, coined by contraction or acronymic abbreviation of the name. They are pseudo-acronyms, however, in the sense that they are complete identifiers by themselves—short names, essentially. They are synonymous with (rather than standing for) the gene/protein name (or any of its aliases), regardless of whether the initial letters \"match\". For example, the symbol for the gene v-akt murine thymoma viral oncogene homolog 1, which is \"AKT1\", cannot be said to be an acronym for the name, and neither can any of its various synonyms, which include \"AKT\", \"PKB\", \"PRKBA\", and \"RAC\". Thus, the relationship of a gene symbol to the gene name is functionally the relationship of a nickname to a formal name (both are complete identifiers)—it is not the relationship of an acronym to its expansion. In this sense they are similar to the symbols for units of measurement in the SI system (such as km for the kilometre), in that they can be viewed as true logograms rather than just abbreviations. Sometimes the distinction is academic, but not always. Although it is not wrong to say that \"VEGFA\" is an acronym standing for \"vascular endothelial growth factor A\", just as it is not wrong that \"km\" is an abbreviation for \"kilometre\", there is more to the formality of symbols than those statements capture.\n\nThe root portion of the symbols for a gene family (such as the \"SERPIN\" root in \"SERPIN1\", \"SERPIN2\", \"SERPIN3\", and so on) is called a root symbol.\n\nThe HUGO Gene Nomenclature Committee is responsible for providing human gene naming guidelines and approving new, unique human gene names and symbols (short identifiers typically created by abbreviating). All human gene names and symbols can be searched online at the HGNC website, and the guidelines for their formation are available there. The guidelines for humans fit logically into the larger scope of vertebrates in general, and the HGNC's remit has recently expanded to assigning symbols to all vertebrate species without an existing nomenclature committee, to ensure that vertebrate genes are named in line with their human orthologs/paralogs. Human gene symbols generally are italicised, with all letters in uppercase (e.g., \"SHH\", for sonic hedgehog). Italics are not necessary in gene catalogs. Protein designations are the same as the gene symbol, but are not italicised, with all letters in uppercase (SHH). mRNAs and cDNAs use the same formatting conventions as the gene symbol. For naming families of genes, the HGNC recommends using a \"root symbol\" as the root for the various gene symbols. For example, for the peroxiredoxin family, \"PRDX\" is the root symbol, and the family members are \"PRDX1\", \"PRDX2\", \"PRDX3\", \"PRDX4\", \"PRDX5\", and \"PRDX6\".\n\nGene symbols generally are italicised, with only the first letter in uppercase and the remaining letters in lowercase (\"Shh\"). Italics are not required on web pages. Protein designations are the same as the gene symbol, but are not italicised and all are upper case (SHH).\n\nNomenclature generally follows the conventions of human nomenclature. Gene symbols generally are italicised, with all letters in uppercase (e.g., \"NLGN1\", for neuroligin1). Protein designations are the same as the gene symbol, but are not italicised; all letters are in uppercase (NLGN1). mRNAs and cDNAs use the same formatting conventions as the gene symbol.\n\nGene symbols are italicised and all letters are in lowercase (\"shh\"). Protein designations are the same as the gene symbol, are not italicised, and all letters are in uppercase (SHH).\n\nGene symbols are italicised and all letters are in lowercase (\"shh\"). Protein designations are the same as the gene symbol, are not italicised; the first letter is in uppercase and the remaining letters are in lowercase (Shh).\n\nGene symbols are italicised, with all letters in lowercase (\"shh\"). Protein designations are the same as the gene symbol, but are not italicised; the first letter is in uppercase and the remaining letters are in lowercase (Shh).\n\nA nearly universal rule in copyediting of articles for public health journals is that abbreviations and acronyms must be expanded at first use, to provide a glossing type of explanation. Typically no exceptions are permitted except for small lists of especially well known terms (such as \"DNA\" or \"HIV\"). Although readers with high subject-matter expertise do not need most of these expansions, those with intermediate or (especially) low expertise are appropriately served by them.\n\nOne complication that gene and protein symbols bring to this general rule is that they are not, accurately speaking, abbreviations or acronyms, despite the fact that many were originally coined via abbreviating or acronymic etymology. They are pseudoacronyms (as \"SAT\" and \"KFC\" also are) because they do not \"stand for\" any expansion. Rather, the relationship of a gene symbol to the gene name is functionally the relationship of a nickname to a formal name (both are complete identifiers)—it is not the relationship of an acronym to its expansion. In fact, many official gene symbol–gene name pairs do not even share their initial-letter sequences (although some do). Nevertheless, gene and protein symbols \"look just like\" abbreviations and acronyms, which presents the problem that \"failing\" to \"expand\" them (even though it is not actually a failure and there are no true expansions) creates the appearance of violating the spell-out-all-acronyms rule.\n\nOne common way of reconciling these two opposing forces is simply to exempt all gene and protein symbols from the glossing rule. This is certainly fast and easy to do, and in highly specialized journals, it is also justified because the entire target readership has high subject matter expertise. (Experts are not confused by the presence of symbols (whether known or novel) and they know where to look them up online for further details if needed.) But for journals with broader and more general target readerships, this action leaves the readers without any explanatory annotation and can leave them wondering what the apparent-abbreviation stands for and why it was not explained. Therefore, a good alternative solution is simply to put either the official gene name or a suitable short description (gene alias/other designation) in parentheses after the first use of the official gene/protein symbol. This meets both the formal requirement (the presence of a gloss) and the functional requirement (helping the reader to know what the symbol refers to). The same guideline applies to shorthand names for sequence variations; AMA says, \"In general medical publications, textual explanations should accompany the shorthand terms at first mention.\" Thus \"188del11\" is glossed as \"an 11-bp deletion at nucleotide 188.\" This corollary rule (which forms an adjunct to the spell-everything-out rule) often also follows the \"abbreviation-leading\" style of expansion that is becoming more prevalent in recent years. Traditionally, the abbreviation always followed the fully expanded form in parentheses at first use. This is still the general rule. But for certain classes of abbreviations or acronyms (such as clinical trial acronyms [e.g., \"ECOG\"<nowiki>]</nowiki> or standardized polychemotherapy regimens [e.g., \"CHOP\"<nowiki>]</nowiki>), this pattern may be reversed, because the short form is more widely used and the expansion is merely parenthetical to the discussion at hand. The same is true of gene/protein symbols.\n\nThe HUGO Gene Nomenclature Committee (HGNC) maintains an official symbol and name for each human gene, as well as a list of synonyms and previous symbols and names. For example, for \"AFF1\" (AF4/FMR2 family, member 1), previous symbols and names are \"MLLT2\" (\"myeloid/lymphoid or mixed-lineage leukemia (trithorax (Drosophila) homolog); translocated to, 2\") and \"PBM1\" (\"pre-B-cell monocytic leukemia partner 1\"), and synonyms are \"AF-4\" and \"AF4\". Authors of journal articles often use the latest official symbol and name, but just as often they use synonyms and previous symbols and names, which are well established by earlier use in the literature. AMA style is that \"authors should use the most up-to-date term\" and that \"in any discussion of a gene, it is recommended that the approved gene symbol be mentioned at some point, preferably in the title and abstract if relevant.\" Because copyeditors are not expected or allowed to rewrite the gene and protein nomenclature throughout a manuscript (except by rare express instructions on particular assignments), the middle ground in manuscripts using synonyms or older symbols is that the copyeditor will add a mention of the current official symbol at least as a parenthetical gloss at the first mention of the gene or protein, and query for confirmation.\n\nSome basic conventions, such as (1) that animal/human homolog (ortholog) pairs differ in letter case (title case and all caps, respectively) and (2) that the symbol is italicized when referring to the gene but nonitalic when referring to the protein, are often not followed by contributors to public health journals. Many journals have the copyeditors restyle the casing and formatting to the extent feasible, although in complex genetics discussions only subject-matter experts (SMEs) can effortlessly parse them all. One example that illustrates the potential for ambiguity among non-SMEs is that some official gene names have the word \"protein\" within them, so the phrase \"brain protein I3 (\"BRI3\")\" (referring to the gene) and \"brain protein I3 (BRI3)\" (referring to the protein) are both valid. The \"AMA Manual\" gives another example: both \"the TH gene\" and \"the \"TH\" gene\" can validly be parsed as correct (\"the gene for tyrosine hydroxylase\"), because the first mentions the alias (description) and the latter mentions the symbol. This seems confusing on the surface, although it is easier to understand when explained as follows: in this gene's case, as in many others, the alias (description) \"happens to use the same letter string\" that the symbol uses. (The matching of the letters is of course acronymic in origin and thus the phrase \"happens to\" implies more coincidence than is actually present; but phrasing it that way helps to make the explanation clearer.) There is no way for a non-SME to know this is the case for any particular letter string without looking up every gene from the manuscript in a database such as NCBI Gene, reviewing its symbol, name, and alias list, and doing some mental cross-referencing and double-checking (plus it helps to have biochemical knowledge). Most medical journals do not (in some cases cannot) pay for that level of fact-checking as part of their copyediting service level; therefore, it remains the author's responsibility. However, as pointed out earlier, many authors make little attempt to follow the letter case or italic guidelines; and regarding protein symbols, they often won't use the official symbol at all. For example, although the guidelines would call p53 protein \"TP53\" in humans or \"Tp53\" in rats, most authors call it \"p53\" in both (and even refuse to call it \"TP53\" if edits or queries try to), not least because of the biologic principle that many proteins are essentially or exactly the same molecules regardless of mammalian species. Regarding the gene, authors are usually willing to call it by its human-specific symbol and capitalization, \"TP53\", and may even do so without being prompted by a query. But the end result of all these factors is that the published literature often does not follow the nomenclature guidelines completely.\n\n", "id": "9894237", "title": "Gene nomenclature"}
{"url": "https://en.wikipedia.org/wiki?curid=39674185", "text": "Twin registry\n\nA twin registry is a database of information about both identical twins and fraternal twins, which is often maintained by an academic institution, such as a university, or by other research institutions.\n\nThe use of twins can improve the statistical power of a genetic study by reducing the amount of genetic and/or environmental variability. \"Identical twins\" (monozygotic (MZ) twins) share virtually all their genes with each other, and \"fraternal twins\" (dizygotic (DZ) twins), on average, share about 50% of their genes with each other (about the same amount of sharing as non-twin siblings). Both types of twin pairs in twin registries almost always share similar prenatal and early childhood environments as well. By determining what are called \"concordance\" rates for a disease or trait among identical and fraternal twin pairs, researchers can estimate whether contributing factors for that disease or trait are more likely to be hereditary, environmental, or some combination of these. A concordance rate is a statistical measure of probability - if one twin has a specific trait or disease, what is the probability that the other twin has (or will develop) that same trait or disease. In addition, with structural equation modeling and multivariate analyses of twin data, researchers can offer estimates of the extent to which allelic variants and environment may influence phenotypic traits.\n\nSome twin registries seek to cover all twins in an entire country, including Sweden, Denmark, Norway, Finland, Australia, Sri Lanka and the United Kingdom. The Swedish Twin Registry is the largest twin database in the world, with approximately 85,000 twin pairs. \n\nOther twin registries cover a more limited geographic scope and are maintained by researchers at academic institutions, such as the Michigan State University Twin Registry, a registry of twins produced by researchers at Michigan State University, the Washington State Twin Registry, a registry of twins produced by researchers at Washington State University, and the Minnesota Twin Registry project by researchers at the University of Minnesota related to the Minnesota Twin Family Study. The largest twin registry in the United States is the Mid-Atlantic Twin Registry (MATR) at Virginia Commonwealth University, which has more than 51,000 registered participants, with approximately 46,000 of these representing intact twin pairs.\n\nMany twin registries depend on the voluntary participation of twins – that is, participation in these twin registries is not compulsory, and twins must voluntarily elect whether or not to register with a twin registry (and later, whether to participate in research projects). This characteristic limitation of many twin registries leads to standard issues known as \"recruitment bias\" or \"volunteer bias\". Recruitment biases include an over-inclusion of twins who share similar characteristics, and over-inclusion of identical twins and female twins:\n\nTwin registries use a number of strategies to try to reduce the risk of recruitment bias. Some twin registries are \"mandatory\" - that is, for example, under the public health laws of Norway, all births of twins since 1967 have been recorded in a twin registry maintained by the Norwegian government. By comparison, enlisting with the Australian Twin Registry is voluntary. While the twin registry in Sri Lanka is based on volunteer twins, that twin registry has made extensive out-reach efforts, such as examining hospital birth records, and then making multiple follow-up efforts (including in-person visits) to find the twins and have them (or their parents) agree to be registered.\n", "id": "39674185", "title": "Twin registry"}
{"url": "https://en.wikipedia.org/wiki?curid=39379960", "text": "De-extinction\n\nDe-extinction, or resurrection biology, or species revivalism is the process of creating an organism, which is either a member of, or resembles an extinct species, or breeding population of such organisms. Cloning is the most widely proposed method, although selective breeding has also been proposed. Similar techniques have been applied to endangered species.\n\nThere is significant controversy over de-extinction. Critics assert that efforts would be better spent conserving existing species, that the habitat necessary for formerly extinct species to survive is too limited to warrant de-extinction, and that the evolutionary conservation benefits of these operations are questionable.\n\nCloning is one method discussed as an option for bringing extinct species back. Proponents include author Stewart Brand, and proposed species include the passenger pigeon and the woolly mammoth. De-extinction efforts are now underway to revive the passenger pigeon by extracting DNA fragments and taking skin samples from preserved specimens and, later, using band-tailed pigeons or rock pigeons as surrogate parents.\n\nA team of Russian and South Korean scientists are, as of April 2013, in the planning stages of cloning a woolly mammoth using an Asian elephant as a surrogate mother. Large amounts of well-preserved mammoth tissue have been found in Siberia. If the process can be completed, there are plans to introduce the mammoths to Pleistocene Park, a wildlife reserve in Siberia. (Evolutionary biologist Beth Shapiro points out that \"cloning\" is a specific technique which cannot be accomplished without a living cell, none of which are available for mammoths, but suggests genome editing might be feasible.)\n\nAlthough de-extinction efforts have not yet succeeded in producing viable offspring of a previously extinct species, the same process has been applied successfully to endangered species. The banteng is the second endangered species to be successfully cloned, and the first to survive for more than a week (the first was a gaur that died two days after being born). Scientists at Advanced Cell Technology in Worcester, Massachusetts, United States extracted DNA from banteng cells kept in the San Diego Zoo's \"Frozen Zoo\" facility, and transferred it into eggs from domestic cattle, a process called somatic cell nuclear transfer. Thirty hybrid embryos were created and sent to Trans Ova Genetics, which implanted the fertilized eggs in domestic cattle. Two were carried to term and delivered by Caesarian section. The first hybrid was born on April 1, 2003, and the second two days later. The second was euthanized, but the first survived and, as of September 2006, remained in good health at the San Diego Zoo.\n\nScientists from the University of Newcastle and the University of New South Wales, including Andrew French, Michael Mahony, Simon Clulow and Mike Archer reported in May 2013 the successful cloning of the extinct frog \"Rheobatrachus silus\" using the process of somatic cell nuclear transfer. The embryos developed for several days but died. In an important development the scientists from Newcastle reported associated technologies that provide a \"proof of concept\" for the proposal that frozen zoos (also referred to as genome banks and seed banks) are an effective mechanism to provide an insurance against species extinction and the loss of population genetic diversity. They connected the circle between de-extinction and the prevention of extinction for threatened animal species. The important advances were the capacity to successfully recover live frozen embryonic cells from animals that produce large yolky eggs (anamniotes such as fishes and amphibians) When this development is combined with somatic cell nuclear transfer (SCNT) it enables the genome to be recovered. The scientists point out that many embryonic cells can be frozen and when combined with frozen sperm storage enables the genetic diversity of populations to be stored. With groups of vertebrates such as the amphibians facing an extinction crisis they propose this as an effective means to prevent extinction while the causes of declines can be identified and remedied. The technical difference between frozen tissue samples commonly used for genetic studies (e.g. phylogenetic reconstruction) and those in a frozen zoo is the use of cryoprotectants and special freezing rates at the time of freezing and thawing.\n\nThe aurochs, which became extinct in 1627, could possibly be brought back by taking DNA samples from bone and teeth fragments in museums in order to obtain genetic material to recreate its DNA. Researchers would then compare the DNA to that of modern European cattle to determine which breeds still carry the creature's genes, and then undertake a selective breeding program to reverse the evolutionary process. The intention would be that with every passing generation, the cattle would more closely resemble the ancient aurochs.\n\nThe quagga, a subspecies of zebra which has been extinct since the 1880s, has been revived using selective breeding of zebras. Since the new animal is not genetically identical to the extinct subspecies, the new animal is called the Rau quagga.\n\n\"Chelonoidis elephantopus\", an extinct tortoise, originally discovered on the Galapagos Islands by Charles Darwin, has hopes of being revived through selective breeding. A group of scientists have collected over 1,700 blood samples from tortoises on Isabela Island during an expedition in 2012 having identified 80 tortoises with traces of the extinct species DNA.\n\nOpponents of de-extinction have claimed that efforts and resources used to resurrect extinct species could have been better used trying to conserve endangered species that might themselves become extinct.\n\nIt has also been noted that a resurrected species, while being genetically the same as previously living specimens, will not have the same behaviour as its predecessors. The first animal to be brought back will be raised by parents of a different species (the fetus's host), not the one that died out and thus have differing mothering techniques and other behaviors.\n\n\"Scientific American\", in an editorial condemning de-extinction, pointed out that the technologies involved could have secondary applications, specifically to help species on the verge of extinction regain their genetic diversity, for example the black-footed ferret or the northern white rhinoceros. It noted, however, that such research \"should be conducted under the mantle of preserving modern biodiversity rather than conjuring extinct species from the grave.\"\n\nOther scholars have published ethical concerns regarding de-extinction. In \"Conservation Biology\", Robert Sandler argues that introducing extinct species to environments may produce harm to modern species, as invasive species. Issues regarding scientific hubris, human and animal health, and the ecology of sensitive environments have been raised by the scientific community. Further research must be performed regarding de-extinction to investigate advantages and disadvantages to the technology. New technological practices must be examined to prevent environmental hazards.\n\nCounter arguments have been made, however, in regards to the benefits of bringing back extinct species. Harvard geneticist, George Church, gives an example of the positive effects of bringing back the extinct woolly mammoth would have on the environment. He explains that if the newly developed mammoth hybrids were to be placed in areas such as Siberia and Alaska, the outcome may reverse the damage that global warming has caused. He and his fellow researchers predict that mammoths would eat the dead grass allowing the sun to reach the spring grass; their weight would allow them to break through dense, insulating snow in order to let cold air reach the soil; and their characteristic of felling trees would increase the absorption of sunlight. If the theories are proven true, global warming could eventually be lessened.\n\n\n\n\n\n\n\n\n\n\n\n", "id": "39379960", "title": "De-extinction"}
{"url": "https://en.wikipedia.org/wiki?curid=42680555", "text": "List of intestinal stem cell marker genes\n\nThe following is a list of intestinal stem cell marker genes, including their name and known function.\n\nIn the adult intestine, the crypts of Lieberkühn are the niche for epithelial stem cells and contain all proliferative stem and progenitor cells. Differentiating cells exit the cell cycle and migrate out of the crypts and onto the surface epithelium of the intestine, where they perform their physiological role (e.g., nutrient absorption by enterocytes; mucous secretion by goblet cells) and are eventually shed into the lumen. Intestinal stem cells were first identified as such in the 1970s. Cheng and Leblond used autoradiography of phagosomes to track the fate of cells at the base of the crypts, and determined that slender cells interspersed among Paneth cells at the crypt base could give rise to all of the other cell types that constituted the intestinal epithelium. Due to their narrow shape and location, these cells were called crypt base columnar cells (CBCs). Potten and colleagues used a combination of DNA labeling and assessment of the response of the epithelium to high-dose radiation to identify label-retaining cells (LRCs) as putative stem cells, which were typically located around four cell positions above the bottom of the crypt, and were therefore also called \"+4 cells\". Later work suggested that these \"+4 cells\" may function as reserve or back-up stem cells, and further suggested that they divide slowly relative to the other progenitor cells in the crypt. Thus, these cells are also called quiescent stem cells.\n\nThe stem cell zone model states that the CBC stem cells reside in a stem-cell-permissive environment. These cycling stem cells regularly generate progeny, which subsequently exit the niche and pass through the “common origin of differentiation” around position +5, where they commit toward the various individual lineages. Progenitors mature as they migrate upward onto the villus. Maturing Paneth cell progenitors migrate downward, with the oldest Paneth cells residing at the very base of the crypt. In accordance with the stem cell zone model proposing that, during their upward migration, CBC stem cells would only gradually lose their self-renewal capacity, it was shown in vivo that transient amplifying cells can revert to Lgr5+ CBC stem cells after damage, presumably by direct contact with Paneth cells.\n\nMore recently modern genetics techniques, primarily using transgenic mice, have been used to identify genes that are specifically expressed or highly enriched in the intestinal stem cells. Below, a table of intestinal stem cell \"marker\" genes is given, along with a notation if this marks active of CBC stem cells, or quiescent/reserve/+4 stem cells.\n\nAdditional possible markers: \nCD24\nCD44v6\nActive beta-catenin\nPcdh8 21419747 \n", "id": "42680555", "title": "List of intestinal stem cell marker genes"}
{"url": "https://en.wikipedia.org/wiki?curid=43732972", "text": "Protospacer adjacent motif\n\nProtospacer adjacent motif (PAM) is a 2-6 base pair DNA sequence immediately following the DNA sequence targeted by the Cas9 nuclease in the CRISPR bacterial adaptive immune system. PAM is a component of the invading virus or plasmid, but is not a component of the bacterial CRISPR locus. Cas9 will not successfully bind to or cleave the target DNA sequence if it is not followed by the PAM sequence. PAM is an essential targeting component (not found in bacterial genome) which distinguishes bacterial self from non-self DNA, thereby preventing the CRISPR locus from being targeted and destroyed by nuclease.\n\nCRISPR loci in a bacterium contain \"spacers\" (viral DNA inserted into a CRISPR locus) that in type II adaptive immune systems were created from invading viral or plasmid DNA (called \"protospacers\"). On subsequent invasion, Cas9 nuclease attaches to tracrRNA:crRNA which guides Cas9 to the invading protospacer sequence. But Cas9 will not cleave the protospacer sequence unless there is an adjacent PAM sequence. The spacer in the bacterial CRISPR loci will not contain a PAM sequence, and will thus not be cut by the nuclease. But the protospacer in the invading virus or plasmid will contain the PAM sequence, and will thus be cleaved by the Cas9 nuclease. For editing genes, guideRNAs (gRNAs) are synthesized to perform the function of the tracrRNA:crRNA complex in recognizing gene sequences having a PAM sequence at the 3'-end.\n\nThe canonical PAM is the sequence 5'-NGG-3' where \"N\" is any nucleobase followed by two guanine (\"G\") nucleobases. Guide RNAs (gRNAs) can transport Cas9 to anywhere in the genome for gene editing, but no editing can occur at any site other than one at which Cas9 recognizes PAM. The canonical PAM is associated with the Cas9 nuclease of Streptococcus pyogenes (designated SpCas9), whereas different PAMs are associated with the Cas9 proteins of the bacteria Neisseria meningitidis, Treponema denticola, and Streptococcus thermophilus. 5'-NGA-3' can be a highly efficient non-canonical PAM for human cells, but efficiency varies with genome location. Attempts have been made to engineer Cas9s to recognize different PAMs to improve ability of CRISPR-Cas9 to do gene editing at any desired genome location.\n\nCas9 of Francisella novicida recognizes the canonical PAM sequence 5'-NGG-3', but has been engineered to recognize the PAM 5'-YG-3' (where \"Y\" is a pyrimidine), thus adding to the range of possible Cas9 targets. The Cpf1 nuclease of Francisella novicida recognizes the PAM 5'-TTTN-3' or 5'-YTN-3'.\n\nAside from CRISPR-Cas9 and CRISPR-Cpf1, there are doubtless many yet undiscovered nucleases and PAMs.\n\nCRISPR/C2c2 from the bacterium Leptotrichia shahii is RNA-guided CRISPR that targets RNA rather than DNA. PAM is not relevant for an RNA-targeting CRISPR, although a guanine flanking the target affects efficacy, and has been designated Protospacer Flanking Site (PFS).\n\nA technology called GUIDE-Seq has been devised to assay off-target cleavages produced by such gene editing. The PAM requirement can be exploited to specifically target single-nucleotide heterozygous mutations while exerting no aberrant effects on the wild-type alleles\n\n\n", "id": "43732972", "title": "Protospacer adjacent motif"}
{"url": "https://en.wikipedia.org/wiki?curid=23167397", "text": "GENCODE\n\nGENCODE is a scientific project in genome research and part of the ENCODE (ENCyclopedia Of DNA Elements) scale-up project.\n\nThe GENCODE consortium was initially formed as part of the pilot phase of the ENCODE project to identify and map all protein-coding genes within the ENCODE regions (approx. 1% of Human genome). Given the initial success of the project, GENCODE now aims to build an “Encyclopedia of genes and genes variants” by identifying all gene features in the human and mouse genome using a combination of computational analysis, manual annotation, and experimental validation, and annotating all evidence-based gene features in the entire human genome at a high accuracy.\n\nThe result will be a set of annotations including all protein-coding loci with alternatively transcribed variants, non-coding loci with transcript evidence, and pseudogenes.\n\nGENCODE is currently progressing towards its goals in Phase 2 of the project, which are:\nThe most recent release of the Human geneset annotations is Gencode 20, with a freeze date of April 2014. This release utilises the latest GRCh38 human reference genome assembly, and corresponds to Ensembl release 76.\nThe latest release for the mouse geneset annotations is Gencode M3, also with a freeze date of April 2014.\n\nSince September 2009, GENCODE has been the human gene set used by the Ensembl project and each new GENCODE release corresponds to an Ensembl release.\n\n2003 September\nThe National Human Genome Research Institute (NHGRI) launched a public research consortium named ENCODE, the Encyclopedia Of DNA Elements, in September 2003, to carry out a project to identify all functional elements in the human genome sequence. The project was designed with three phases - Pilot, Technology development and Production phase.\nThe pilot stage of the ENCODE project aimed to investigate in great depth, computationally and experimentally, 44 regions totaling 30 Mb of sequence representing approximately 1% of the human genome. As part of this stage, the GENCODE consortium was formed to identify and map all protein-coding genes within the ENCODE regions. It was envisaged that the results of the first two phases will be used to determine the best path forward for analysing the remaining 99% of the human genome in a cost-effective and comprehensive production phase.\n\n2005 April\nThe first release of the annotation of the 44 ENCODE regions was frozen on 29 April 2005 and was used in the first ENCODE Genome Annotation Assessment Project (E-GASP) workshop. GENCODE Release 1 contained 416 known loci, 26 novel (coding DNA sequence) CDS loci, 82 novel transcript loci, 78 putative loci, 104 processed pseudogenes and 66 unprocessed pseudogenes.\n\n2005 October\nA second version (release 02) was frozen on 14 October 2005, containing updates following discoveries from experimental validations using RACE and RT-PCR techniques. GENCODE Release 2 contained 411 known loci, 30 novel CDS loci, 81 novel transcript loci, 83 putative loci, 104 processed pseudogenes and 66 unprocessed pseudogenes.\n\n2007 June\nThe conclusions from the pilot project were published in June 2007. The findings highlighted the success of the pilot project to create a feasible platform and new technologies to characterise functional elements in the human genome, which paves the way for opening research into genome-wide studies.\n\n2007 October\nAfter a successful pilot phase on 1% of the genome, the Wellcome Trust Sanger Institute was awarded a grant from the US National Human Genome Research Institute (NHGRI) to carry out a scale-up of the GENCODE project for integrated annotation of gene features.\nThis new funding was part of NHGRI's endeavour to scale-up the ENCODE Project to a production phase on the entire genome along with additional pilot-scale studies.\n\n2012 September\nIn September 2012, The GENCODE consortium published a major paper discussing the results from a major release – GENCODE Release 7, which was frozen in December 2011. The GENCODE 7 release used a combination of manual gene annotation from the Human and Vertebrate Analysis and Annotation (HAVANA) group and full new release (Ensembl release 62) of the automatic gene annotation from Ensembl. At the time of release, GENCODE Release 7 had the most comprehensive annotation of long noncoding RNA (lncRNA) loci publicly available with the predominant transcript form consisting of two exons.\n\n2013 - 2017\nHaving been involved in successfully delivering the definitive annotation of functional elements in the human genome, the GENCODE group were awarded a second grant in 2013 in order to continue their human genome annotation work and expand GENCODE to include annotation of the mouse genome. It is envisaged that the mouse annotation data will allow comparative studies between the human and mouse genomes, to improve annotation quality in both genomes.\n\nThe key participants of the GENCODE project have remained relatively consistent throughout its various phases, with the Wellcome Trust Sanger Institute now leading the overall efforts of the project.\n\nA summary of key participating institutions of each phase is listed below:\n\nSince its inception, GENCODE has released 20 versions of the Human gene set annotations (excluding minor updates).\n\nThe key summary statistics of the most recent GENCODE Human gene set annotation (Release 20, April 2014 freeze, Ensembl 76), which is the first version that utilises the latest version of the Human Genome Assembly (GRCh38), is shown below: \nRefer to the GENCODE Statistics README and GENCODE biotypes page for more details on the classification of the above gene set.\n\nThrough advancements in sequencing technologies (such as RT-PCR-seq), increased coverage from manual annotations (HAVANA group), and improvements to automatic annotation algorithms using Ensembl, the accuracy and completeness of GENCODE annotations have been continuously refined through its iteration of releases.\n\nA comparison of key statistics from 3 major GENCODE releases is shown below. It is evident that although the coverage, in terms of total number of genes discovered, is steady increasing, the number of protein-coding genes has actually decreased. This is mostly attributed to new experimental evidence obtained using Cap Analysis Gene Expression (CAGE) clusters, annotated PolyA sites, and peptide hits.\nThe general process to create an annotation for GENCODE involves manual curation, different computational analysis and targeted experimental approaches. Putative loci can be verified by wet-lab experiments and computational predictions are analysed manually.\nCurrently, to ensure a set of annotation covers the complete genome rather than just the regions that have been manually annotated, a merged data set is created using manual annotations from HAVANA, together with automatic annotations from the Ensembl automatically annotated gene set. This process also adds unique full-length CDS predictions from the Ensembl protein coding set into manually annotated genes, to provide the most complete and up-to-date annotation of the genome possible.\n\nEnsembl transcripts are products of the Ensembl automatic gene annotation system (a collection of gene annotation pipelines), termed the Ensembl gene build. All Ensembl transcripts are based on experimental evidence and thus the automated pipeline relies on the mRNAs and protein sequences deposited into public databases from the scientific community. Moreover, Protein levels 1 and 2 from UniProt, untranslated regions (UTRs), long intergenic noncoding RNA (lincRNA) genes (annotated using a combination of cDNA sequences and regulatory data from the Ensembl project), short non-coding RNAs (annotated using the Ensembl ncRNA pipelines) are included.\n\nThe main approach to manual gene annotation is to annotate transcripts aligned to the genome and take the genomic sequences as the reference rather than the cDNAs. The finished genomic sequence is analyzed using a modified Ensembl pipeline, and BLAST results of cDNAs/ESTs and proteins, along with various ab initio predictions, can be analyzed manually in the annotation browser tool Otterlace. Thus, more alternative spliced variants can be predicted compared with cDNA annotation. Moreover, genomic annotation produces a more comprehensive analysis of pseudogenes.\nThere are several analysis groups in the GENCODE consortium that run pipelines that aid the manual annotators in producing models in unannotated regions, and to identify potential missed or incorrect manual annotation, including completely missing loci, missing alternative isoforms, incorrect splice sites and incorrect biotypes. These are fed back to the manual annotators using the AnnoTrack tracking system. Some of these pipelines use data from other ENCODE subgroups including RNASeq data, histone modification and CAGE and Ditag data. RNAseq data is an important new source of evidence, but generating complete gene models from it is a difficult problem. As part of GENCODE, a competition was run to assess the quality of predictions produced by various RNAseq prediction pipelines (Refer to RGASP below). To confirm uncertain models, GENCODE also has an experimental validation pipeline using RNA sequencing and RACE \n\nDuring the merge process, all HAVANA and Ensembl transcripts models are compared, first by clustering overlapped coding exons on a same strand, and then by pairwise comparisons of each exon in a cluster of transcripts. The module used to merge the gene set is HavanaAdder. Additional steps are required prior to running the HavanaAdder code (e.g. Ensembl health-checking system and queries against CCDS gene set and Ensembl´s cDNA alignments). If annotation described in external data sets is missing from the manual set, then this is stored in the AnnoTrack system to be reviewed.\n\nFor GENCODE 7, transcript models are assigned a high or low level of support based on a new method developed to score the quality of transcripts. This method relies on mRNA and EST alignments supplied by UCSC and Ensembl. The mRNA and EST alignments are compared to the GENCODE transcripts, and the transcripts are scored according to the alignment over its full length. A summary of support levels for each chromosome in GENCODE Release 7 is shown in the figure on the right. The annotations are partitioned into those produced by the automated process, manual method and the merged annotations, where both processes result in the same annotation.\n\nAmplification, sequencing, mapping and validation exon–exon junction\n\nDouble-stranded cDNA of eight human tissues (brain, heart, kidney, testis, liver, spleen, lung, and skeletal muscle) were generated with a cDNA amplification, and the purified DNA was directly used to generate a sequencing library with the ‘‘Genomic DNA sample prep kit’’ (Illumina). This library was subsequently sequenced on an Illumina Genome Analyzer 2 platform. Then, reads (35 or 75 nt) were mapped on to the reference human genome (hg19) and the predicted spliced amplicons with Bowtie software. Only uniquely mapping reads with no mismatch were considered to validate a splice site (transcript). Splice junctions were validated if a minimum of 10 reads with the following characteristics spanned the predicted splice junctions. For 35- and 75-nt-long reads, it required at least 4 and 8 nt on each side of the breakpoints (i.e., on each targeted exon), respectively.\n\nComparison of RefSeq, UCSC, AceView, and GENCODE transcripts\n\nTranscripts belonging to four different data sets (GENCODE, RefSeq, UCSC, and AceView) were compared to assess to which extent these data sets overlap. Releases compared were GENCODE 7, RefSeq and UCSC Genes freeze July 2011, and AceView 2010 release. The overlaps between different data set combinations were graphically represented as three-way Venn diagrams using the Vennerable R package and edited manually.\n\nPhyloCSF analysis\n\nPhyloCSF was used to identify potential novel coding genes in RNA-seq transcript models based on evolutionary signatures. For each transcript model generated from the Illumina HBM data using either Exonerate or Scripture, a mammalian alignment was generated by extracting the alignment of each exon from UCSC’s vertebrate alignments (which includes 33 placental mammals).\n\nAPPRIS (CNIO)\n\nAPPRIS is a system that deploys a range of computational methods to provide value to the annotations of the human genome. APPRIS also selects one of the CDS for each gene as the principal isoform. Moreover, it defines the principal variant by combining protein structural and functional information and information from the conservation of related species. The APPRIS server has been used in the context of the scale up of the ENCODE project to annotate the Human genome but APPRIS is being used for other species (e.g. mouse, rat and zebrafish). The pipeline is made up of separate modules that combine protein structure and function information and evolutionary evidence. Each module has been implemented as a separate web service.\n\nThe current GENCODE Human gene set version (GENCODE Release 20) includes annotation files (in GTF and GFF3 formats), FASTA files and METADATA files associated with the GENCODE annotation on all genomic regions (reference-chromosomes/patches/scaffolds/haplotypes). The annotation data is referred on reference chromosomes and stored in separated files which include: Gene annotation, PolyA features annotated by HAVANA, (Retrotransposed) pseudogenes predicted by the Yale & UCSC pipelines, but not by HAVANA, long non-coding RNAs, and tRNA structures predicted by tRNA-Scan.\nSome examples of the lines in the GTF format are shown below:\n\nThe columns within the GENCODE GTF file formats are described below.\n\nFormat description of GENCODE GTF file. TAB-separated standard GTF columns\n\nDescription of key-value pairs in 9th column of the GENCODE GTF file (format: key \"value\")\nEach gene in the GENCODE data set are classified into three levels according to their type of annotation:\n\nLevel 1 (verified loci):\nIncludes transcripts that have been manually annotated and experimentally validated by RT-PCR-seq, and pseudogenes that have been validated by three different methodologies.\n\nLevel 2 (manually annotated loci):\nHighlights transcripts that have been manually annotated by HAVANA only, and also includes transcripts that have been merged with models produced by the Ensembl automatic pipeline.\n\nLevel 3 (automatically annotated loci):\nIndicates transcripts and pseudogene predictions resulting from Ensembl’s automated annotation pipeline.\n\nGenes & transcripts are assigned the status ‘‘known,’’ ‘‘novel,’’ or ‘‘putative’’ depending on their presence in other major databases and the evidence used to build their component transcripts.\n\nKnown:\nRepresented in the HUGO Gene Nomenclature Committee (HGNC) database and RefSeq.\n\nNovel: \nNot currently represented in HGNC or RefSeq databases, but are well supported by either locus specific transcript evidence or evidence from a paralogous or orthologous locus.\n\nPutative:\nNot currently represented in HGNC or RefSeq databases, but are supported by shorter, more sparse transcript evidence.\n\nAlso, the GENCODE website contains a Genome Browser for human and mouse where you can reach any genomic region by giving the chromosome number and start-end position (e.g. 22:30,700,000..30,900,000), as well as by ENS transcript id (with/without version) , ENS gene id (with/without version) and gene name. The browser is powered by Biodalliance.\n\nThe definition of a \"gene\" has never been a trivial issue, with numerous definitions and notions proposed throughout the years since the discovery of the human genome. First, genes were conceived in the 1900s as discrete units of heredity, then it was thought as the blueprint for protein synthesis, and in more recent times, it was being defined as genetic code that is transcribed into RNA. Although the definition of a gene has evolved greatly over the last century, it has remained a challenging and controversial subject for many researchers. With the advent of the ENCODE/GENCODE project, even more problematic aspects of the definition have been uncovered, including alternative splicing (where a series of exons are separated by introns), intergenic transcriptions, and the complex patterns of dispersed regulation, together with non-genic conservation and the abundance of noncoding RNA genes. As GENCODE endeavours to build an encyclopaedia of genes and gene variants, these problems presented a mounting challenge for the GENCODE project to come up with an updated notion of a gene.\n\nPseudogenes have DNA sequences which are similar to functional protein-coding genes, however their transcripts are usually identified with a frameshift or deletion, and are generally annotated as a byproduct of protein-coding gene annotation in most genetic databases. However, recent analysis of retrotransposed pseudogenes have found some retransposed pseudogenes to be expressed and functional and to have major biological/regulatory impacts on human biology. To deal with the unknowns and complexities of pseudogenes, GENCODE has created a pseudogene ontology using a combination of automated, manual, and experimental methods to associate a variety of biological properties—such as sequence features, evolution, and potential biological functions to pseudogenes.\n\nThe Encyclopedia Of DNA Elements (ENCODE) is a public research consortium launched by the National Human Genome Research Institute (NHGRI), in September 2003 (Pilot phase). The goal of ENCODE is to build a comprehensive parts list of functional elements in the human genome, including elements that act at the protein and RNA levels, and regulatory elements that control cells and circumstances in which a gene is active. \nData analysis during the pilot phase (2003 - 2007) was coordinated by the Ensembl group, a joint project of EBI and the Wellcome Trust Sanger Institute. During the initial pilot and technology development phases of the project, 44 regions—approximately 1% of the human genome—were targeted for analysis using a variety of experimental and computational methods.\nAll data produced by ENCODE investigators and the results of ENCODE analysis projects from 2003 to 2012 are hosted in the UCSC Genome browser and database. ENCODE results from 2013 and later are freely available for download and analysis from the ENCODE Project Portal. To annotate all evidence-based gene features (genes, transcripts, coding sequences, etc.) in the entire human genome at a high accuracy, ENCODE consortium create the subproject GENCODE.\n\nThe Human Genome Project was an international research effort to determine the sequence of the human genome and identify the genes that it contains. The Project was coordinated by the National Institutes of Health and the U.S. Department of Energy. Additional contributors included universities across the United States and international partners in the United Kingdom, France, Germany, Japan, and China. The Human Genome Project formally began in 1990 and was completed in 2003, 2 years ahead of its original schedule.\nFollowing the release of the completed human genome sequence in April 2003, the scientific community intensified its efforts to mine the data for clues about how the body works in health and in disease. A basic requirement for this understanding of human biology is the ability to identify and characterize sequence-based functional elements through experimentation and computational analysis. In September 2003, the NHGRI introduced the ENCODE project to facilitate the identification and analysis of the complete set of functional elements in the human genome sequence.\n\nEnsembl is part of the GENCODE project, and it has played a critical role to provide automatic annotation on the human reference genome assembly and to merge this annotation with manual annotation from the HAVANA team. The gene set provided by Ensembl for human is the GENCODE gene set \n\nA key research area of the GENCODE project was to investigate the biological significance of long non-coding RNAs (lncRNA). To better understand the lncRNA expression in Humans, a sub project was created by GENCODE to develop custom microarray platforms capable of quantifying the transcripts in the GENCODE lncRNA annotation. A number of designs have been created using the Agilent Technologies eArray system, and these designs are available in a standard custom Agilent format.\n\nThe RNA-seq Genome Annotation Assessment Project (RGASP) project is designed to assess the effectiveness of various computational methods for high quality RNA-sequence data analysis. The primary goals of RGASP are to provide an unbiased evaluation for RNA-seq alignment, transcript characterisation (discovery, reconstruction and quantification) software, and to determine the feasibility of automated genome annotations based on transcriptome sequencing.\n\nRGASP is organised in a consortium framework modelled after the EGASP (ENCODE Genome Annotation Assessment Project) gene prediction workshop, and two rounds of workshops have been conducted to address different aspects of RNA-seq analysis as well as changing sequencing technologies and formats. One of the main discoveries from rounds 1 & 2 of the project was the importance of read alignment on the quality of gene predictions produced. Hence, a third round of RGASP workshop is currently being conducted (in 2014) to focus primarily on read mapping to the genome.\n\n\n", "id": "23167397", "title": "GENCODE"}
{"url": "https://en.wikipedia.org/wiki?curid=30625026", "text": "Consensus CDS Project\n\nThe Consensus Coding Sequence (CCDS) Project is a collaborative effort to maintain a dataset of protein-coding regions that are identically annotated on the human and mouse reference genome assemblies. The CCDS project tracks identical protein annotations on the reference mouse and human genomes with a stable identifier (CCDS ID), and ensures that they are consistently represented by the National Center for Biotechnology Information (NCBI), Ensembl, and UCSC Genome Browser. The integrity of the CCDS dataset is maintained through stringent quality assurance testing and on-going manual curation.\n\nBiological and biomedical research has come to rely on accurate and consistent annotation of genes and their products on genome assemblies. Reference annotations of genomes are available from various sources, each with their own independent goals and policies, which results in some annotation variation.\n\nThe CCDS project was established to identify a gold standard set of protein-coding gene annotations that are identically annotated on the human and mouse reference genome assemblies by the participating annotation groups. The CCDS gene sets that have been arrived at by consensus of the different partners now consist of over 18,000 human and over 20,000 mouse genes (see CCDS release history). The CCDS dataset is increasingly representing more alternative splicing events with each new release.\n\nParticipating annotation groups include:\nManual annotation is provided by:\n\n\"Consensus\" is defined as protein-coding regions that agree at the start codon, stop codon, and splice junctions, and for which the prediction meets quality assurance benchmarks. A combination of manual and automated genome annotations provided by (NCBI)\nand Ensembl (which incorporates manual HAVANA annotations) are compared to identify annotations with matching genomic coordinates.\n\nIn order to ensure that CDSs are of high quality, multiple quality assurance (QA) tests are performed (Table 1). All tests are performed following the annotation comparison step of each CCDS build and are independent of individual annotation group QA tests performed prior to the annotation comparison.\n\nAnnotations that fail QA tests undergo a round of manual checking that may improve results or reach a decision to reject annotation matches based on QA failure.\n\nThe CCDS database is unique in that the review process must be carried out by multiple collaborators, and agreement must be reached before any changes can be made. This is made possible with a collaborator coordination system that includes a work process flow and forums for analysis and discussion. The CCDS database operates an internal website that serves multiple purposes including curator communication, collaborator voting, providing special reports and tracking the status of CCDS representations. When a collaborating CCDS group member identifies a CCDS ID that may need review, a voting process is employed to decide on the final outcome.\n\nCoordinated manual curation is supported by a restricted-access website and a discussion e-mail list. CCDS curation guidelines were established to address specific conflicts that were observed at a higher frequency. Establishment of CCDS curation guidelines has helped to make the CCDS curation process more efficient by reducing the number of conflicting votes and time spent in discussion to reach a consensus agreement. A link to the CCDS curation guidelines can be found here.\n\nCuration policies established for the CCDS data set have been integrated in to the RefSeq and HAVANA annotation guidelines and thus, new annotations provided by both groups are more likely to be concordant and result in addition of a CCDS ID. These standards address specific problem areas, are not a comprehensive set of annotation guidelines, and do not restrict the annotation polices of any collaborating group. Examples include, standardized curation guidelines for selection of the initiation codon and interpretation of upstream ORFs and transcripts that are predicted to be candidates for nonsense-mediated decay. Curation occurs continuously, and any of the collaborating centers can flag a CCDS ID as a potential update or withdrawal.\n\nConflicting opinions are addressed by consulting with scientific experts or other annotation curation groups such as the HUGO Gene Nomenclature Committee (HGNC) and Mouse Genome Informatics (MGI). If a conflict cannot be resolved, then collaborators agree to withdraw the CCDS ID until more information becomes available.\n\nNonsense-mediated decay (NMD):\nNMD is the most powerful mRNA surveillance process. NMD eliminates defective mRNA before it can be translated into protein. This is important because if the defective mRNA is translated, the truncated protein may cause disease. Different mechanisms have been proposed to explain NMD; one being the exon junction complex (EJC) model. In this model, if the stop codon is >50 nt upstream of the last exon-exon junction, the transcript is assumed to be a NMD candidate. The CCDS collaborators use a conservative method, based on the EJC model, to screen mRNA transcripts. Any transcripts determined to be NMD candidates are excluded from the CCDS data set except in the following situations:\n\nPreviously, NMD candidate transcripts were considered to be protein coding transcripts by both RefSeq and HAVANA, and thereby, these NMD candidate transcripts were represented in the CCDS data set. The RefSeq group and the HAVANA project have subsequently revised their annotation policies.\n\nMultiple in-frame translation start sites:\nMultiple factors contribute to translation initiation, such as upstream open reading frames (uORFs), secondary structure and the sequence context around the translation initiation site. A common start site is defined within Kozak consensus sequence: (GCC) GCCACCAUGG in vertebrates. The sequence in brackets (GCC) is the motif with unknown biological impact. There are variations within Kozak consensus sequence, such as G or A is observed three nucleotides upstream (at position -3) of AUG. Bases between positions -3 and +4 of Kozak sequence have the most significant impact on translational efficiency. Hence, a sequence (A/G)NNAUGG is defined as a strong Kozak signal in the CCDS project.\n\nAccording to the scanning mechanism, the small ribosomal subunit can initiate translation from the first reached start codon. There are exceptions to the scanning model: \n\nAccording to the CCDS annotation guidelines, the longest ORF must be annotated except when there is experimental evidence that an internal start site is used to initiate translation. Additionally, other types of new data, such as ribosome profiling data, can be used to identify start codons. The CCDS data set records one translation initiation site per CCDS ID. Any alternative start sites may be used for translation and will be stated in a CCDS public note.\n\nUpstream open reading frames:\nAUG initiation codons located within transcript leaders are known as upstream AUGs (uAUGs). Sometimes, uAUGs are associated with uORFs . uORFs are found in approximately 50% of human and mouse transcripts. The existence of uORFs are another challenge for the CCDS data set. The scanning mechanism for translation initiation suggests that small ribosomal subunits (40S) bind at the 5’ end of a nascent mRNA transcript and scan for the first AUG start codon. It is possible that an uAUG is recognised first, and the corresponding uORF is then translated. The translated uORF could be a NMD candidate, although studies have shown that some uORFs can avoid NMD. The average size limit for uORFs that will escape NMD is approximately 35 amino acids. It also has been suggested that uORFs inhibit translation of the downstream gene by trapping a ribosome initiation complex and causing the ribosome to dissociate from the mRNA transcript before it reaches the protein-coding regions. Currently, no studies have reported the global impact of uORFs on translational regulation.\n\nThe current CCDS annotation guidelines allow the inclusion of mRNA transcripts containing uORFs if they meet the following two biological requirements:\n\nRead-through transcripts:\nRead-through transcripts are also known as conjoined genes or co-transcribed genes. Read-through transcripts are defined as transcripts combining at least part of one exon from each of two or more distinct known (partner) genes which lie on the same chromosome in the same orientation. The biological function of read-through transcripts and their corresponding protein molecules remain unknown. However, the definition of a read-through gene in the CCDS data set is that the individual partner genes must be distinct, and the read-through transcripts must share ≥ 1 exon (or ≥ 2 splice sites except in the case of a shared terminal exon) with each of the distinct shorter loci. Transcripts are not considered to be read-through transcripts in the following circumstances: \n\nQuality of reference genome sequence:\nAs the CCDS data set is built to represent genomic annotations of human and mouse, the quality problems with the human and mouse reference genome sequences become another challenge. Quality problems occur when the reference genome is misassembled. Thereby the misassembled genome may contain premature stop codons, frame-shift indels, or likely polymorphic pseudogenes. Once these quality problems are identified, the CCDS collaborators report the issues to the Genome Reference Consortium, which investigates and makes the necessary corrections.\n\nThe CCDS project is available from the NCBI CCDS data set page (here), which provides FTP download links and a query interface to acquire information about CCDS sequences and locations. CCDS reports can be obtained by using the query interface, which is located at the top of the CCDS data set page. Users can select various types of identifiers such as CCDS ID, gene ID, gene symbol, nucleotide ID and protein ID to search for specific CCDS information. The CCDS reports (Figure 1) are presented in a table format, providing links to specific resources, such as a history report, Entrez Gene or re-query the CCDS data set. The sequence identifiers table presents transcript information in VEGA, Ensembl and Blink. The chromosome location table includes the genomic coordinates for each individual exon of the specific coding sequence. This table also provides links to several different genome browsers, which allow you to visualise the structure of the coding region. Exact nucleotide sequence and protein sequence of the specific coding sequence are also displayed in the section of CCDS sequence data.\n\nThe CCDS dataset is an integral part of the GENCODE gene annotation project and it is used as a standard for high-quality coding exon definition in various research fields, including clinical studies, large-scale epigenomic studies, exome projects and exon array design. Due to the consensus annotation of CCDS exons by the independent annotation groups, exome projects in particular have regarded CCDS coding exons as reliable targets for downstream studies (e.g., for single nucleotide variant detection), and these exons have been used as coding region targets in commercially available exome kits.\n\nThe CCDS data set size has continued to increase with both the computational genome annotation updates, which integrate new data sets submitted to the International Nucleotide Sequence Database Collaboration (INSDC), and on ongoing curation activities that supplement or improve upon that annotation. Table 2 summarises the key statistics for each CCDS build where Public CCDS IDs are all those that were not under review or pending an update or withdrawal at the time of the current release date.\n\nThe complete set of release statistics can be found at the official CCDS website on their Releases & Statistics page.\n\nLong-term goals include the addition of attributes that indicate where transcript annotation is also identical (including the UTRs) and to indicate splice variants with different UTRs that have the same CCDS ID. It is also anticipated that as more complete and high-quality genome sequence data become available for other organisms, annotations from these organisms may be in scope for CCDS representation.\n\nThe CCDS set will become more complete as the independent curation groups agree on cases where they initially differ, as additional experimental validation of weakly supported genes occurs, and as automatic annotation methods continue to improve. Communication among the CCDS collaborating groups is ongoing and will resolve differences and identify refinements between CCDS update cycles. Human updates are expected to occur roughly every 6 months and mouse releases yearly.\n\n\n\n\n\n", "id": "30625026", "title": "Consensus CDS Project"}
{"url": "https://en.wikipedia.org/wiki?curid=42447237", "text": "Reverse Transcription Loop-mediated Isothermal Amplification\n\nReverse transcription loop-mediated isothermal amplification (RT-LAMP) is a technique for the amplification of RNA.\n\nWithin the last 10 years of its development, applications of the LAMP method in pathogenic microorganisms, genetically modified ingredients, tumor detection, and embryo sex identification have been widely used. This method was then improved by taking it a step further and combining it with a reverse transcription phase to allow for the detection of RNA. RT-LAMP is a one step nucleic acid amplification method that is used to diagnose infectious disease caused by bacteria or viruses. Although it has not been formally recognised by NAT, the method has been developed into many commercial kits that can be used for the identification of pathogens. The commonly used PCR method is able to generate millions of copies of the target strand. This process relies on thermal cycling, cycles of heating and cooling to facilitate the DNA replication. RT-LAMP does not require these cycles and is performed at a constant temperature between 60 and 65 °C. Similar to RT-PCR, RT-LAMP uses reverse transcriptase to synthesize complementary DNA (cDNA) from RNA sequences. This cDNA is then amplified using DNA polymerase, generating 10^9 copies per hour.\n\nRT-LAMP is used in the detection of viruses. This method can be very effective in detecting viruses with an RNA genome (Group II, IV, and V based on the Baltimorevirus classification system).\n\nFour specially designed primers recognize distinct target sequences on the template strand. The primers bind only to these sequences which allows for high specificity. Out of the 4 primers involved, two of them are “inner primers” (FIP and BIP) which are designed to synthesize new DNA strands. The outer primers (F3 and B3) anneal to the template strand and also generate new DNA. These primers are accompanied by DNA polymerase which aids in strand displacement and releases the newly formed DNA strands.\n\nThe BIP primer, accompanied by reverse transcriptase, initiates the process by binding to a target sequence on the 3’ end of the RNA template and synthesizing a copy DNA strand. The B3 primer binds to this side of the template strand as well, and with the help of DNA polymerase simultaneously creates a new cDNA strand while displacing the previously made copy. The double stranded DNA containing the template strand is no longer needed.\n\nThe single stranded copy now loops at the 3’ end as it binds to itself. The FIP primer binds to the 5’ end of this single strand and accompanied by DNA polymerase, synthesizes a complementary strand. The F3 primer, with DNA polymerase, binds to this end and generates a new double stranded DNA molecule while displacing the previously made single strand.\n\nThis new single strand that has been released will act as the starting point for the LAMP cycling amplification. The DNA has a dumbbell-like structure as the ends fold in and self anneal. This structure becomes a stem-loop when the FIP or BIP primer once again initiates DNA synthesis at one of the target sequence locations. This cycle can be started from either the forward or backward side of the strand using the appropriate primer. Once this cycle has begun, the strand undergoes self-primed DNA synthesis during the elongation stage of the amplification process. This amplification takes place in only an hour, under isothermal conditions between 60-65 °C.\n\nThis method is specifically advantageous because it can all be done quickly in one step.The sample is mixed with the primers, reverse transcriptase and DNA polymerase and the reaction takes place under a constant temperature. The required temperature is so low that the reaction can be completed using a simple hot water bath. There is no need for expensive thermocycling equipment that is necessary for other methods like PCR, which makes RT-LAMP very cost effective. In contrast with conventional PCR and real-time PCR assays, this method is much more efficient while still obtaining a high level of precision. This inexpensive and streamlined method can be more readily used in developing countries that do not have access to high tech laboratories. These areas are known for having a multitude of infectious diseases caused by various bacteria and viruses. The LAMP method is very useful for detection of these pathogens.\n\nA disadvantage of this method is generating the sequence specific primers. For each LAMP assay, primers must be specifically designed to be compatible with the target DNA. This can be difficult which discourages researchers from using the LAMP method in their work. There is however, a free software called Primer Explorer, developed by Fujitsu in Japan, which can aid in the selection of these primers.\n\nViruses infect host cells with their specific genetic information, which the cell then replicates, causing the host to become diseased. In an effort to identify which certain virus is present in a host, RT-LAMP is used to test for the specific sequence of the virus, made possible by comparing the sequences against a large external database of references.\n\nA primary example of the RT-LAMP was is an experiment to detect a new duck Tembusu-like, BYD virus, named after the region, Baiyangdian, where it was first isolated Because the symptoms of this virus were similar to Tembusu, an already identified disease, the nucleotide sequence of the complete genome of this virus was available in external resources. The known sequence was put into the online primer-designing softwear, LAMP Primer Explorer (http://primerexplorer.jp/e/), where the appropriate primers were designed and selected. With the selected primers, a RT-LAMP assay was done to amplify the RNA, with which the samples could then be visualized and confirmed under natural and UV light.\n\nAnother recent application of this method, was in a 2013 experiment to detect an Akabane virus using RT-LAMP. The experiment, done in China, isolated the virus from aborted calf fetuses, which is rarely successful but was able to be done because of RT-LAMP’s easy detection feature and high sensitivity. With the use of the Primer Explore V4 Software, and a sequence reference of the Akabane virus, the correct primers were developed and tested in an RT-LAMP assay. For specification purposes, the assay was also run against 4 other virus known to cause abortion in cattle. These comparative assays were unsuccessful due to the primers not binding to the template regions.\n\n", "id": "42447237", "title": "Reverse Transcription Loop-mediated Isothermal Amplification"}
{"url": "https://en.wikipedia.org/wiki?curid=43325101", "text": "PLINK (genetic tool-set)\n\nPLINK is a free, commonly used, open-source whole genome association analysis toolset designed by Shaun Purcell. The software is designed flexibly to perform a wide range of basic, large-scale genetic analyses.\n\nPLINK currently supports following functionalities:\n\n\n", "id": "43325101", "title": "PLINK (genetic tool-set)"}
{"url": "https://en.wikipedia.org/wiki?curid=44389849", "text": "Replication timing quantitative trait loci\n\nA replication timing quantitative trait loci (or rtQTL) is a genetic variation producing a differential use of replication origins, exhibiting allele-specific effects on replication timing.\n", "id": "44389849", "title": "Replication timing quantitative trait loci"}
{"url": "https://en.wikipedia.org/wiki?curid=44315318", "text": "Bulked segregant analysis\n\nBulked segregant analysis (BSA) is a technique used to identify genetic markers associated with a mutant phenotype. This allows geneticists to discover genes conferring disease resistance or susceptibility.\n\nThis technique involves forming two groups that display opposing phenotypes for a trait of interest. For example, the individuals in one group are resistant to a disease, whereas those in the second group are not. Two bulked DNA samples are then created by pooling the DNA of all individuals in each group.\n\nThese two bulked samples can then be analysed using techniques such as Restriction fragment length polymorphism or RAPD to detect similarities and differences in the various loci of the genome. The two groups will have a random distribution of alleles in all loci of the genome except for loci that are associated with the mutation. A consistent difference on a locus between the two bulked samples likely means that the locus is associated with the mutation of interest.\n\nIn animals, the individuals making up the two testing groups are usually produced by a cross between two siblings heterozygous for the mutation of interest. The use of siblings is necessary to ensure that the alleles contributing to the mutation are the same among the individuals. \n\nThere must be a minimum amount of heterozygosity in the various loci of the groups to allow the genes that are associated with the trait of interest to be identified. Since most laboratory strains are inbred, outcrossing of the homozygous mutated individual with a polymorphic strain is essential to generate effective testing groups. The offspring are crossed with each other to generate testing groups.\n\nBulked DNA samples can be analysed using Southern blotting. Use of restriction enzymes or PCR amplification on the DNA is required for RFLP or RAPD analysis respectively. In these techniques, the loci that are analysed are the restriction digest sites and the sequences on which PCR primers attach to. These sites are usually located throughout the genome. Once linked loci are detected, they can be mapped and the linkage distances between them determined.\n", "id": "44315318", "title": "Bulked segregant analysis"}
{"url": "https://en.wikipedia.org/wiki?curid=44431955", "text": "Partial dominance hypothesis\n\nThe partial dominance hypothesis in genetics states that inbreeding depression is the result of the frequency increase of homozygous deleterious recessive or partially recessive alleles. The partial dominance hypothesis can be explained by looking at a population that is divided into a large number of separately inbred lines. Deleterious alleles will eventually be eliminated from some lines and become fixed in other lines, while some lines disappear because of fixation of deleterious alleles. This will cause an overall decline in population and trait value, but then increase to a trait value that is equal to or greater than the trait value in the original population. Crossing inbred lines restores fitness in the overdominance hypothesis and a fitness increase in the partial dominance hypothesis.\n", "id": "44431955", "title": "Partial dominance hypothesis"}
{"url": "https://en.wikipedia.org/wiki?curid=33847153", "text": "Nagoya Protocol\n\nThe Nagoya Protocol on Access to Genetic Resources and the Fair and Equitable Sharing of Benefits Arising from their Utilization to the Convention on Biological Diversity, also known as the Nagoya Protocol on Access and Benefit Sharing (ABS) is a 2010 supplementary agreement to the 1992 Convention on Biological Diversity (CBD). Its aim is the implementation of one of the three objectives of the CBD: the fair and equitable sharing of benefits arising out of the utilization of genetic resources, thereby contributing to the conservation and sustainable use of biodiversity. However, there are concerns that the added bureaucracy and legislation will, overall, be damaging to the monitoring and collection of biodiversity, to conservation, to the international response to infectious diseases, and to research.\n\nThe Protocol was adopted on 29 October 2010 in Nagoya, Japan, and entered into force on 12 October 2014. It has been ratified by 97 parties, which includes 96 UN member states and the European Union. It is the second Protocol to the CBD; the first is the 2000 Cartagena Protocol on Biosafety.\n\nThe Nagoya Protocol applies to genetic resources that are covered by the CBD, and to the benefits arising from their utilization. The Protocol also covers traditional knowledge associated with genetic resources that are covered by the CBD and the benefits arising from its utilization\n\nThe Nagoya Protocol sets out obligations for its contracting parties to take measures in relation to access to genetic resources, benefit-sharing and compliance.\n\nDomestic-level access measures aim to:\n\nDomestic-level benefit-sharing measures aim to provide for the fair and equitable sharing of benefits arising from the utilization of genetic resources with the contracting party providing genetic resources. Utilization includes research and development on the genetic or biochemical composition of genetic resources, as well as subsequent applications and commercialization. Sharing is subject to mutually agreed terms. Benefits may be monetary or non-monetary such as royalties and the sharing of research results.\n\nSpecific obligations to support compliance with the domestic legislation or regulatory requirements of the contracting party providing genetic resources, and contractual obligations reflected in mutually agreed terms, are a significant innovation of the Nagoya Protocol. Contracting Parties are to:\n\nThe Nagoya Protocol's success will require effective implementation at the domestic level. A range of tools and mechanisms provided by the Nagoya Protocol will assist contracting Parties including:\n\nBased on a country’s self-assessment of national needs and priorities, capacity-building may help to:\n\nA growing number of Preferential Trade Agreements (PTAs) include provisions related to access to genetic resources or to the sharing of the benefits that arise out of their utilization. Indeed, some recent trade agreements, originating notably from Latin American countries, provide specific measures designed to facilitate the implementation of the ABS provisions contained in the Nagoya Protocol, including measures related to technical assistance, transparency and dispute settlement. \n\nMany scientists have voiced concern over the protocol, fearing the increased red tape will hamper disease prevention and conservation efforts, and that the threat of possible imprisonment of scientists will have a chilling effect on research. Non-commercial biodiversity researchers and institutions such as natural history museums fear maintaining biological reference collections and exchanging material between institutions will become difficult.\n\n\n", "id": "33847153", "title": "Nagoya Protocol"}
{"url": "https://en.wikipedia.org/wiki?curid=44746274", "text": "DNA phenotyping\n\nDNA phenotyping (\"noing\") is the process of predicting an organism’s phenotype using only genetic information collected from genotyping or DNA sequencing. This term, also known as molecular photofitting, is primarily used to refer to the prediction of a person’s physical appearance and/or biogeographic ancestry for forensic purposes.\n\nDNA phenotyping uses many of the same scientific methods as those being used for genetically-informed personalized medicine, in which drug responsiveness (pharmacogenomics) and medical outcomes are predicted from a patient’s genetic information. Significant genetic variants associated with a particular trait are discovered using a genome-wide association study (GWAS) approach, in which hundreds of thousands or millions of single-nucleotide polymorphisms (SNPs) are tested for their association with each trait of interest. Predictive modeling is then used to build a mathematical model for making trait predictions about new subjects.\n\nBiogeographic ancestry determination methods have been highly developed within the genetics community, as it is a key GWAS quality control step. These approaches typically use genome-wide human genetic clustering and/or principal component analysis to compare new subjects to curated individuals with known ancestry, such as the International HapMap Project or the 1000 Genomes Project. Another approach is to assay ancestry informative markers (AIMs), SNPs that vary in frequency between the major human populations.\n\nBeginning in 2009, academic groups developed and reported on predictive models for eye color and, more recently, hair color in the European population. More recently, companies such as Parabon NanoLabs and Identitas have begun offering forensic DNA phenotyping services for U.S. and international law enforcement.\n\nTraditional DNA profiling, sometimes referred to as DNA fingerprinting, uses DNA as a biometric identifier. Like an iris scan or fingerprint, a DNA profile can uniquely identify an individual with very high accuracy. For forensic purposes, this means that investigators must have already identified and obtained DNA from a potentially matching individual. DNA phenotyping is used when investigators need to narrow the pool of possible individuals or identify unknown remains by learning about the person’s ancestry and appearance. When the suspected individual is identified, traditional DNA profiling can be used to prove a match, provided there is a reference sample that can be used for comparison.\n\nFrom the late 1990s to the early 2000s, a series of murders were committed in Louisiana. Eyewitness statements and FBI profiling indicated the perpetrator was likely a Caucasian male. After investigators tested DNA samples from thousands of Caucasian males and found no matches with DNA from the crime scenes, DNA phenotyping was performed on a crime scene DNA sample by DNAPrint Genomics. This testing indicated the ancestry of the suspect was 85% sub-Saharan African and 15% Caucasian, pointing to an African-American individual and changing the direction of the investigation. Within two months, police arrested Derrick Todd Lee, who was later convicted for two of these murders.\n\n\n\n\n", "id": "44746274", "title": "DNA phenotyping"}
{"url": "https://en.wikipedia.org/wiki?curid=1423804", "text": "Forward genetics\n\nForward genetics is the approach of determining the genetic basis responsible for a phenotype. This was initially done by using naturally occurring mutations or inducing mutants with radiation, chemicals, or insertional mutagenesis (e.g. transposable elements). Subsequent breeding takes place, mutant individuals are isolated, and then the gene is mapped. Forward genetics can be thought of as a counter to reverse genetics, which determines the function of a gene by analyzing the phenotypic effects of altered DNA sequences. Mutant phenotypes are often observed long before having any idea which gene is responsible, which can lead to genes being named after their mutant phenotype (e.g. Drosophila \"rosy\" gene which is named after the eye colour in mutants).\n\nOften hundreds of thousands of mutations are generated. Chemicals like ethylmethanesulfonate (EMS) cause random point mutations. These types of mutagens can be useful because they are easily applied to any organism but they can be very difficult to map. Mutations can also be generated by insertional mutagenesis. For example, transposable elements containing a marker are mobilized into the genome at random. These transposons are often modified to transpose only once, and once inserted into the genome a selectable marker can be used to identify the mutagenized individuals. Since a known fragment of DNA was inserted this can make mapping and cloning the gene much easier. Other methods such as using radiation to cause deletions and chromosomal rearrangements can be used to generate mutants as well.\n\nOnce mutagenized and screened, typically a complementation test is done to ensure that mutant phenotypes arise from the same genes if the mutations are recessive. If the progeny after a cross between two recessive mutants have a normal phenotype, then it can be inferred that the phenotype is determined by more than one gene. Typically, the allele exhibiting the strongest phenotype is further analyzed. A genetic map can then be created using linkage and genetic markers, and then the gene of interest can be cloned and sequenced. If many alleles of the same genes are found, the screen is said to be saturated and it is likely that all of the genes involved producing the phenotype were found.\n\nBefore 1980 very few human genes had been identified as disease loci until advances in DNA technology gave rise to positional cloning and reverse genetics. Discovering disease loci using old forward genetic techniques was a very long and difficult process and much of the work went into mapping and cloning the gene through association studies and chromosome walking. Cystic fibrosis however demonstrates how the process of forward genetics can elucidate a human genetic disorder. Genetic-linkage studies were able to map the disease loci in cystic fibrosis to chromosome 7 by using protein markers. Afterward, chromosome walking and jumping techniques were used to identify the gene and sequence it. Forward genetics can work for single-gene-single phenotype situations but in more complicated diseases like cancer, reverse genetics is often used instead.\n\nBy the classical genetics approach, a researcher would then locate (map) the gene on its chromosome by crossbreeding with individuals that carry other unusual traits and collecting statistics on how frequently the two traits are inherited together. Classical geneticists would have used phenotypic traits to map the new mutant alleles. Eventually the hope is that such screens would reach a large enough scale that most or all newly generated mutations would represent a second hit of a locus, essentially saturating the genome with mutations. This type of saturation mutagenesis within classical experiments was used to define sets of genes that were a bare minimum for the appearance of specific phenotypes. However, such initial screens were either incomplete as they were missing redundant loci and epigenetic effects, and such screens were difficult to undertake for certain phenotypes that lack directly measurable phenotypes. Additionally a classical genetics approach takes significantly longer.\n\n", "id": "1423804", "title": "Forward genetics"}
{"url": "https://en.wikipedia.org/wiki?curid=12960037", "text": "Smash and grab\n\nA smash and grab is a particular form of burglary that involves smashing a barrier, usually a display window in a shop or a showcase, grabbing valuables, and making a quick getaway, without concern for setting off alarms or creating noise. Typically, display windows and showcases that are in enclosed areas, such as shopping malls and office buildings, are less vulnerable to smash and grab raids than those on open streets – particularly where the streets are poorly lit or unobserved (such as premises in pedestrian subways or unstaffed transport facilities). Recent smash and grab crimes have also involved ramming a pickup truck through the walls of a convenience store or gas station in order to remove the ATM from the premises and recover the cash. Smash and grab raids can occur in many scenarios, both in broad daylight and at night, and the perpetrators can range from experienced thieves to impulsive vandals. \n\nThe greatest cost of smash and grab raids can often be in replacing the windows or walls, which can sometimes far exceed the cost of the goods that are stolen.\n\nThere are several approaches to deterring smash and grab raiders. Shopkeepers can securely tether their goods, and make the tethering obvious to the onlooker. They can also avoid displaying goods of value in windows, an approach that has the disadvantage of reducing the attractiveness of the display to customers. Additionally, shopkeepers can strengthen window glass, to the extent that it can withstand, without breaking, being hit by the implements that smash and grab raiders are likely to use, such as hammers, bricks, and scaffolding poles.\n\nSmash and grab raids became common in the 1930s, and were particularly prevalent in the 1940s, but decreased in frequency as shopkeepers took to strengthening their windows and/or fitting protective grilles. By the 1950s, forced entry to shops was being effected by using cars and grappling irons to pull window bars off windows, a precursor to the 1980s phenomenon of ram-raiding.\n", "id": "12960037", "title": "Smash and grab"}
{"url": "https://en.wikipedia.org/wiki?curid=29866864", "text": "Structural Biochemistry/ Kiss Gene Expression\n\nKisspeptins are a group of proteins that come from precursors that are approximately 145 amino acids in length that get reduced to 54 and then finally about 10, 13, or 14 amino acid carboxyl terminals and are encoded by the Kiss 1 gene and are thought to be involved in the initiation of puberty by stimulating the release of the gonadotropin-releasing hormone (GnRH). As a result, there have been several studies conducted on the nuclei of Kiss neurons in the arcuate (ARC) and anteroventral periventricular (AVPV) parts of the brains of modules such as rats.\n\nKisspeptins behave as ligands for the G-protein-coupled receptor, GPR54 (21-23), which is believed to regulate GnRH and gonadotropin hormone secretion. Kisspeptins also trigger the release of GnRH/LH from the hypothalamus by\n\nIn an experiment conducted by researchers at the University of Washington, the National University of Córdoba, and the University of Maryland, it was discovered that female rats have a greater Kiss 1 gene expression because they have a more Kiss 1 neurons than their male counterparts. This conclusion was determined by comparing Kiss 1 mRNA levels in the AVPV and ARC in the male and female rats after subjecting them to different hormonal conditions. Through this, it was shown that female rats expressed higher levels of Kiss 1 mRNA in the AVPV, however there appeared to be no difference in the Kiss 1 expression in the ARC between male and female rats. Their research suggests that sex differences in kiss 1 gene expression in the AVPV region of the brain might play a role in regulating the GnRH/LH levels of male and female rats.\n\nThe following describes the series of experiments that were undergone in order to demonstrate that the Kiss 1 gene expression is sexually differentiated in rats:\n\nA single-labelled ISH (In Situ Hybridization), which is a technique that uses a single labeled complementary RNA or DNA strand to determine where a specific piece of RNA or DNA is located in a certain slice of tissue, was used to determine the number of neurons containing mRNA and the amount of Kiss 1 mRNA in each cell. This information was then used to compare the sexual differences in Kiss 1 neurons in the AVPV and ARC by comparing the levels of mRNA and the number of mRNA containing neurons in male and female rats.\n\nThis image depicts an example of the technique Fluorescence in Situ Hybridization that is used to locate certain DNA strands using complementary pieces of DNA strands (probes) that are labeled with fluorescent dye, and then are denatured so that they could hybridize with the DNA strands of interest.\n\nThe hormone level differences between males and destrous females can be shown by the differences in Kiss 1 mRNA expression between male and female rats during their neonatal period (time period between existing in the maternal uterus and existing completely independently). A single-labeled ISH was used to compare levels of Kiss 1 mRNA in AVPV and ARC originating from adult females that were treated with thymopoietin pentapeptide on the day they were born.\n\nThe serum levels of LH (Leutenizing Hormone) and FSH (follicle Stimulating Hormone) were measured using a double antibody method and an RIA kit. The rat’s LH-I-9 and FSH-I-9 were labeled with 125-I using the chloramines-T- method with LH-RP-3 and FSH-I-9 being the controls. The chloroamines-T-method is used to label small concentrations of a certain substances by producing a very specific radioactive tracer molecule—which in this case is radioactively labeled Iodine, the element that is normally used to label peptides using this technique.\n\nThe brain slices were first exposed to 4% formaldehyde, acetic anhydride, sodium citrate, sodium chloride, chloroform, dehydrated in ethanols, and then air dried to prepare for hybridization. Once the hybridization was completed, the slides were put in ribonuclease (RNAse) and were allowed to develop. The slides containing sections of the brain were also examined under double-labeled ISH in a similar fashion as the single labeled ISH and were allowed to hybridize and air-dry.\n\nThe single-labeled ISHs a software device counted the number of cells and the number of silver grains present on each cell, which was related to the mRNA content in each cell. When the number of silver grain clusters was 1000 times greater than the number of clusters in the background, then the cells were assumed to contain Kiss 1 (were positive for Kiss 1). In the double-labeled ISHs assays, the cells were subject to fluorescence microscopy, and a software device counted the silver grains (representing TH and mRNA). The ratios for each cell of the signal to the background was determined and a cell was defined as double-labeled if it had a signal to background ratio of greater than or equal to 3 and had at least three silver grains in it. The number of double-labeled cells made up a percentage of the total Kiss1 mRNA-expressing cells.\n", "id": "29866864", "title": "Structural Biochemistry/ Kiss Gene Expression"}
{"url": "https://en.wikipedia.org/wiki?curid=44631723", "text": "Genetics of posttraumatic stress disorder\n\nGenetics play some role in the development of PTSD. Approximately 30% of the variance in PTSD is caused from genetics alone. For twin pairs exposed to combat in Vietnam, having a monozygotic (identical) twin with PTSD was associated with an increased risk of the co-twin's having PTSD compared to twins that were dizygotic (non-identical twins). There is also evidence that those with a genetically smaller hippocampus are more likely to develop PTSD following a traumatic event. Research has also found that PTSD shares many genetic influences common to other psychiatric disorders. Panic and generalized anxiety disorders and PTSD share 60% of the same genetic variance. Alcohol, nicotine, and drug dependence share greater than 40% genetic similarities.\n\nGamma-aminobutyric acid (GABA) is the major inhibitory neurotransmitter in the brain. A recent study reported significant interactions between three polymorphisms in the GABA alpha-2 receptor gene and the severity of childhood trauma in predicting PTSD in adults. A study found those with a specific genotype for G-protein signaling 2 (RGS2), a protein that decreases G protein-coupled receptor signaling, and high environmental stress exposure as adults and a diagnosis of lifetime PTSD. This was particularly prevalent in adults with prior trauma exposure and low social support.\n\nRecently, it has been found that several single-nucleotide polymorphisms (SNPs) in FK506 binding protein 5 (FKBP5) interact with childhood trauma to predict severity of adult PTSD. These findings suggest that individuals with these SNPs who are abused as children are more susceptible to PTSD as adults.\n\nThis is particularly interesting given that FKBP5 SNPs have previously been associated with peritraumatic dissociation in medically injured children (that is, dissociation at the time of the birth trauma), which has itself been shown to be predictive of PTSD. Furthermore, FKBP5 may be less expressed in those with current PTSD. Another recent study found a single SNP in a putative estrogen response element on ADCYAP1R1 (encodes pituitary adenylate cyclase-activating polypeptide type I receptor or PAC1) to predict PTSD diagnosis and symptoms in females. Incidentally, this SNP is also associated with fear discrimination. The study suggests that perturbations in the PACAP-PAC1 pathway are involved in abnormal stress responses underlying PTSD.\n\nPTSD is a psychiatric disorder that requires an environmental event that individuals may have varied responses to so gene-environment studies tend to be the most indicative of their effect on the probability of PTSD then studies of the main effect of the gene. Recent studies have demonstrated the interaction between PFBP5 and childhood environment to predict the severity of PTSD. Polymorphisms in FKBP5 have been associated with peritraumatic dissociation in mentally ill children. A study of highly traumatized African-American subjects from inner city primary-care clinics indicated 4 polymorphisms of the FKBP5 gene, each of these functional. The interaction between the polymorphisms and the severity of childhood abuse predicts the severity of the adult PTSD symptoms. A more recent study of the African-American population indicated that the TT genotype of the FKBP5 gene is associated with the highest risk of PTSD among those having experienced childhood adversity, however those with this genotype that experienced no childhood adversity had the lowest risk of PTSD. In addition alcohol dependence interacts with the FKBP5 polymorphisms and childhood adversity to increase the risk of PTSD in these populations. Emergency room expression of the FKPB5 mRNA following trauma was shown to indicate a later development of PTSD.\n\nCatechol-O-methyl transferase (COMT) is an enzyme that catalyzes the extraneuronal breakdown of catecholamines. The gene that codes for COMT has a functional polymorphism in which a valine has been replaced with a methionine at codon 158. This polymorphism has lower enzyme activity and has been tied to slower breakdown of the catecholamines. A study, of Rwandan Genocide survivors, indicated that carriers of the Val allel demonstrated the expected response relationship between the higher number of lifetime traumatic events and a lifetime diagnosis of PTSD. However, those with homozygotes for the Met/Met genotype demonstrated a high risk of lifetime PTSD independent of the number of traumatic experiences. Those with Met/Met genotype also demonstrated a reduced extinction of conditioned fear responses with may account for the high risk for PTSD experienced by this genotype.\n\nMany genes impact the limbic-frontal neurocircuitry as a result of its complexity. The main effect of the D2A1 allele of the dopamine receptor D2 (DRD2) has a strong association with the diagnosis of PTSD. The D2A1 allele has also shown a significant association to PTSD in those having engaged in harmful drinking. In addition a polymorphism in the dopamine transporter SLC6A3 gene has a significant association with chronic PTSD. A polymorphism of the serotonin receptor 2A gene has been associated with PTSD in Korean women. The short allele of the promoter region of the serotonin transporter (5-HTTLPR) has been shown to be less efficient then the long allele and is associated with the amygdala response for extinction of fear conditioning. However, the short allele is associated with a decreased risk of PTSD in a low risk environment but a high risk of PTSD in a high risk environment. The s/s genotype demonstrated a high risk for development of PTSD even in response to a small number of traumatic events, but those with the l allele demonstrate increasing rates of PTSD with increasing traumatic experiences.\n\nGenome-wide association study (GWAS) offer an opportunity to identify novel risk variants for PTSD that will in turn inform our understanding of the etiology of the disorder. Early results indicate the feasibility and potential power of GWAS to identify biomarkers for anxiety-related behaviors that suggest a future of PTSD. These studies will lead to the discovery of novel loci for the susceptibility and symptomatology of anxiety disorders including PTSD.\n\nGene and environment studies alone fail to explain the importance the developmental stressor timing exposure to the phenotypic changes associated with PTSD. Epigenetic modification is the environmentally induced change in DNA that alters the function rather than the structure of the gene. The biological mechanism of epigenetic modification typically involves the methylation of cytosine within a gene that produces decreased transcription of that segment of DNA. The neuroendocrine alteration seen in animal models parallel those of PTSD in which low basal cortisol and enhanced suppression of cortisol in response to synthetic glucocorticoid becomes hereditary. Lower levels of glucocorticoid receptor (GR) mRNA have been demonstrated in the hippocampus of suicide victims with histories of childhood abuse. It has not been possible to monitor the state of methylation over time, however the interpretation is early developmental methylation changes are long-lasting and enduring. It is hypothesized that epigenetic-mediated changes in the HPA axis could be associated with an increased vulnerability to PTSD following traumatic events. These findings support the mechanism in which early life trauma strongly validates as a risk factor for PTSD development in adulthood by recalibrating the set point and stress-responsivity of the HPA axis. Studies have reported an increased risk for PTSD and low cortisol levels in the offspring of female holocaust survivors with PTSD. Epigenetic mechanisms may also be relevant to the intrauterine environment. Mothers with PTSD produced infants with lower salivary cortisol levels only if the traumatic exposure occurred during the third trimester of gestation. These changes occur via transmission of hormonal responses to the fetus leading to a reprogramming of the glucocorticoid responsivity in the offspring.\n\nEvolutionary psychology views different types of fears and reactions caused by fears as adaptations that may have been useful in the ancestral environment in order to avoid or cope with various threats. In general, mammals display several defensive behaviors roughly dependent on how close the threat is: avoidance, vigilant immobility, withdrawal, aggressive defense, appeasement, and finally complete frozen immobility (the last possibly to confuse a predator's attack reflex or to simulate a dead and contaminated body). PTSD may correspond to and be caused by overactivation of such fear circuits. Thus, PTSD avoidance behaviors may correspond to mammal avoidance of and withdrawal from threats. Heightened memory of past threats may increase avoidance of similar situations in the future as well as be a prerequisite for analyzing the past threat and develop better defensive behaviors if the threat should recur. PTSD hyperarousal may correspond to vigilant immobility and aggressive defense. Complex posttraumatic stress disorder (and phenomena such as the Stockholm syndrome) may in part correspond to the appeasement stage and possibly the frozen immobility stage.\n\nThere may be evolutionary explanations for differences in resilience to traumatic events. Thus, PTSD is rare following traumatic fire that may be explained by events such as forest fires' long being part of the evolutionary history of mammals. On the other hand, PTSD is much more common following modern warfare, which may be explained by modern warfare's being a new development and very unlike the quick inter-group raids that are argued to have characterized the paleolithic.\n", "id": "44631723", "title": "Genetics of posttraumatic stress disorder"}
{"url": "https://en.wikipedia.org/wiki?curid=646125", "text": "Heterosis\n\nHeterosis, hybrid vigor, or outbreeding enhancement, is the improved or increased function of any biological quality in a hybrid offspring. The adjective derived from \"heterosis\" is heterotic.\n\nAn offspring exhibits heterosis if its traits are enhanced as a result of mixing the genetic contributions of its parents. These effects can be due to Mendelian or non-Mendelian inheritance.\n\nIn proposing the term \"heterosis\" to replace the older term heterozygosis, G.H. Shull aimed to avoid limiting the term to the effects that can be explained by heterozygosity in Mendelian inheritance.\nHeterosis is often discussed as the opposite of inbreeding depression although differences in these two concepts can be seen in evolutionary considerations such as the role of genetic variation or the effects of genetic drift in small populations on these concepts. Inbreeding depression occurs when related parents have children with traits that negatively influence their fitness largely due to homozygosity. In such instances, outcrossing should result in heterosis.\n\nNot all outcrosses result in heterosis. For example, when a hybrid inherits traits from its parents that are not fully compatible, fitness can be reduced. This is a form of outbreeding depression.\n\nDominance versus overdominance is a scientific controversy in the field of genetics that has persisted for more than a century. These two alternative hypotheses were first stated in 1908.\n\nWhen a population is small or inbred, it tends to lose genetic diversity. Inbreeding depression is the loss of fitness due to loss of genetic diversity. Inbred strains tend to be homozygous for recessive alleles that are mildly harmful (or produce a trait that is undesirable from the standpoint of the breeder). Heterosis or hybrid vigor, on the other hand, is the tendency of outbred strains to exceed both inbred parents in fitness.\n\nSelective breeding of plants and animals, including hybridization, began long before there was an understanding of underlying scientific principles. In the early 20th century, after Mendel's laws came to be understood and accepted, geneticists undertook to explain the superior vigor of many plant hybrids. Two competing hypotheses, which are not mutually exclusive, were developed:\n\n\nDominance and overdominance have different consequences for the gene expression profile of the individuals. If over-dominance is the main cause for the fitness advantages of heterosis, then there should be an over-expression of certain genes in the heterozygous offspring compared to the homozygous parents. On the other hand, if dominance is the cause, fewer genes should be under-expressed in the heterozygous offspring compared to the parents. Furthermore, for any given gene, the expression should be comparable to the one observed in the fitter of the two parents.\n\nPopulation geneticist James Crow (1916-2012) believed, in his younger days, that overdominance was a major contributor to hybrid vigor. In 1998 he published a retrospective review of the developing science. According to Crow, the demonstration of several cases of heterozygote advantage in Drosophila and other organisms first caused great enthusiasm for the overdominance theory among scientists studying plant hybridization. But overdominance implies that yields on an inbred strain should decrease as inbred strains are selected for the performance of their hybrid crosses, as the proportion of harmful recessives in the inbred population rises. Over the years, experimentation in plant genetics has proven that the reverse occurs, that yields increase in both the inbred strains and the hybrids, suggesting that dominance alone may be adequate to explain the superior yield of hybrids. Only a few conclusive cases of overdominance have been reported in all of genetics. Since the 1980s, as experimental evidence has mounted, the dominance theory has made a comeback.\n\nCrow writes:\n\"The current view ... is that the dominance hypothesis is the major explanation of inbreeding decline and [of] the high yield of hybrids. There is little statistical evidence for contributions from overdominance and epistasis. But whether the best hybrids are getting an extra boost from overdominance or favorable epistatic contributions remains an open question.\"\nThe term heterosis often causes confusion and even controversy, particularly in selective breeding of domestic animals, because it is sometimes (incorrectly) claimed that all crossbred plants and animals are \"genetically superior\" to their parents, due to heterosis. However, there are two problems with this claim:\n\nAn example of the ambiguous value judgements imposed on hybrids and hybrid vigor is the mule. While mules are almost always infertile, they are valued for a combination of hardiness and temperament that is different from either of their horse or donkey parents. While these qualities may make them \"superior\" for particular uses by humans, the infertility issue implies that these animals would most likely become extinct without the intervention of humans through animal husbandry, making them \"inferior\" in terms of natural selection.\n\nSince the early 1900s, two competing genetic hypotheses, not necessarily mutually exclusive, have been developed to explain hybrid vigor. More recently, an epigenetic component of hybrid vigor has also been established.\n\nThe genetic dominance hypothesis attributes the superiority of hybrids to the masking of expression of undesirable (deleterious) recessive alleles from one parent by dominant (usually wild-type) alleles from the other (see Complementation (genetics)). It attributes the poor performance of inbred strains to the expression of homozygous deleterious recessive alleles. The genetic overdominance hypothesis states that some combinations of alleles (which can be obtained by crossing two inbred strains) are especially advantageous when paired in a heterozygous individual. This hypothesis is commonly invoked to explain the persistence of some alleles (most famously the Sickle cell trait allele) that are harmful in homozygotes. In normal circumstances, such harmful alleles would be removed from a population through the process of natural selection. Like the dominance hypothesis, it attributes the poor performance of inbred strains to expression of such harmful recessive alleles. In any case, outcross matings provide the benefit of masking deleterious recessive alleles in progeny. This benefit has been proposed to be a major factor in the maintenance of sexual reproduction among eukaryotes, as summarized in the article Evolution of sexual reproduction.\n\nAn epigenetic contribution to heterosis has been established in plants, and it has also been reported in animals. MicroRNAs (miRNAs), discovered in 1993, are a class of non-coding small RNAs which repress the translation of messenger RNAs (mRNAs) or cause degradation of mRNAs. In hybrid plants, most miRNAs have non-additive expression (it might be higher or lower than the levels in the parents). This suggests that the small RNAs are involved in the growth, vigor and adaptation of hybrids.\n\n'Heterosis without hybridity' effects on plant size have been demonstrated in genetically isogenic F1 triploid (autopolyploid) plants, where paternal genome excess F1 triploids display positive heterosis, whereas maternal genome excess F1s display negative heterosis effects. Such findings demonstrate that heterosis effects, with a genome dosage-dependent epigenetic basis, can be generated in F1 offspring that are genetically isogenic (i.e. harbour no heterozygosity). It has been shown that hybrid vigor in an allopolyploid hybrid of two \"Arabidopsis\" species was due to epigenetic control in the upstream regions of two genes, which caused major downstream alteration in chlorophyll and starch accumulation. The mechanism involves acetylation and/or methylation of specific amino acids in histone H3, a protein closely associated with DNA, which can either activate or repress associated genes.\n\nOne example of where particular genes may be important in vertebrate animals for heterosis is the major histocompatibility complex (MHC). Vertebrates inherit several copies of both MHC class I and MHC class II from each parent, which are used in antigen presentation as part of the adaptive immune system. Each different copy of the genes is able to bind and present a different set of potential peptides to T-lymphocytes. These genes are highly polymorphic throughout populations, but will be more similar in smaller, more closely related populations. Breeding between more genetically distant individuals will decrease the chance of inheriting two alleles which are the same or similar, allowing a more diverse range of peptides to be presented. This therefore gives a decreased chance that any particular pathogen will not be recognised, and means that more antigenic proteins on any pathogen are likely to be recognised, giving a greater range of T-cell activation and therefore a greater response. This will also mean that the immunity acquired to the pathogen will be against a greater range of antigens, meaning that the pathogen must mutate more before immunity is lost. Thus hybrids will be less likely to be succumb to pathogenic disease and will be more capable of fighting off infection.\n\nCrosses between inbreds from different heterotic groups result in vigorous F1 hybrids with significantly more heterosis than F1 hybrids from inbreds within the same heterotic group or pattern. Heterotic groups are created by plant breeders to classify inbred lines, and can be progressively improved by reciprocal recurrent selection.\n\nHeterosis is used to increase yields, uniformity, and vigor. Hybrid breeding methods are used in maize, sorghum, rice, sugar beet, onion, spinach, sunflowers, broccoli and to create a more psychoactive cannabis.\n\nNearly all field corn (maize) grown in most developed nations exhibits heterosis. Modern corn hybrids substantially outyield conventional cultivars and respond better to fertilizer.\n\nCorn heterosis was famously demonstrated in the early 20th century by George H. Shull and Edward M. East after hybrid corn was invented by Dr. William James Beal of Michigan State University based on work begun in 1879 at the urging of Charles Darwin. Dr. Beal's work led to the first published account of a field experiment demonstrating hybrid vigor in corn, by Eugene Davenport and Perry Holden, 1881. These various pioneers of botany and related fields showed that crosses of inbred lines made from a Southern dent and a Northern flint, respectively, showed substantial heterosis and outyielded conventional cultivars of that era. However, at that time such hybrids could not be economically made on a large scale for use by farmers. Donald F. Jones at the Connecticut Agricultural Experiment Station, New Haven invented the first practical method of producing a high-yielding hybrid maize in 1914-1917. Jones' method produced a double-cross hybrid, which requires two crossing steps working from four distinct original inbred lines. Later work by corn breeders produced inbred lines with sufficient vigor for practical production of a commercial hybrid in a single step, the single-cross hybrids. Single-cross hybrids are made from just two original parent inbreds. They are generally more vigorous and also more uniform than the earlier double-cross hybrids. The process of creating these hybrids often involves detasseling.\n\nTemperate maize hybrids are derived from two main heterotic groups: Iowa Stiff Stalk Synthetic, and non stiff stalk.\n\nRice production has seen enormous rise in China due to heavy uses of hybrid rice. In China, efforts have generated a super hybrid rice strain (LYP9) with a production capability of ~15 tons per hectare. In India also, several varieties have shown high vigor, including RH-10 and Suruchi 5401.\n\nThe concept of heterosis is also applied in the production of commercial livestock. In cattle, hybrids between Black Angus and Hereford produce a hybrid known as a \"Black Baldy\". In swine, \"blue butts\" are produced by the cross of Hampshire and Yorkshire. Other, more exotic hybrids such as \"beefalo\" are also used for specialty markets.\n\nWithin poultry, sex-linked genes have been used to create hybrids in which males and females can be sorted at one day old by color. Specific genes used for this are genes for barring and wing feather growth. Crosses of this sort create what are sold as Black Sex-links, Red Sex-links, and various other crosses that are known by trade names.\n\nCommercial broilers are produced by crossing different strains of White Rocks and White Cornish, the Cornish providing a large frame and the Rocks providing the fast rate of gain. The hybrid vigor produced allows the production of uniform birds with a marketable carcass at 6–9 weeks of age.\n\nLikewise, hybrids between different strains of White Leghorn are used to produce laying flocks that provide the majority of white eggs for sale in the United States.\n\nIn 2013, a study found that mixed breeds live on average 1.2 years longer than pure breeds.\n\nJohn Scott and John L. Fuller performed a detailed study of purebred cocker spaniels, purebred basenjis, and hybrids between them.\nThey found that hybrids ran faster than either parent, perhaps due to heterosis. Other characteristics, such as basal heart rate, did not show any heterosis—the dog's basal heart rate was close to the average of its parents—perhaps due to the additive effects of multiple genes.\n\nSometimes people working on a dog breeding program find no useful heterosis.\n\nHuman beings are all extremely genetically similar to one another, but less similar than dogs, for instance. Michael Mingroni has proposed heterosis, in the form of hybrid vigor associated with historical reductions of the levels of inbreeding, as an explanation of the Flynn effect, the steady rise in IQ test scores around the world during the twentieth century. However, James R. Flynn has pointed out that even if everyone mated with a sibling in 1900, subsequent increases in heterosis would not be a sufficient explanation of the observed IQ gains.\n\n\n", "id": "646125", "title": "Heterosis"}
{"url": "https://en.wikipedia.org/wiki?curid=45412506", "text": "Midphalangeal hair\n\nMidphalangeal hair, or the presence/absence of hair on the middle phalanx of the ring finger, is one of the most widely studied markers in classical genetics of human populations. Although this polymorphism was observed at other fingers as well, for this kind of research, the fourth finger of the hand has been conventionally selected.\n\nIn humans, hair is commonly present on all the basal segments of the digits and invariably absent from all the terminal ones. On the middle segments, there is wide fluctuation with apparent familial and racial tendencies. Hair is present on the middle segment of the fingers more frequently than on the middle segment of the toes. Hair is most often found on the middle segment of the fourth finger.\n\nWillier (1974), related citations quoted Danforth as stating that 'the hair follicle is a kind of biological microcosm in which almost any problem relating to growth, differentiation, decline and rejuvenescence of tissue can be studied to advantage...' While riding on a streetcar in Wilkes-Barre one summer, Danforth observed, in his own words, that 'a man in front of me draped his arm over the back of the seat and I noticed that while his arm was very hairy the middle segments of his fingers were free of hair and so, I observed, were my own; but I knew this was not generally true.' So far as he was aware, no one before had recognized this variation as possibly hereditary.\n\nThe genetic determination of presence or absence of hair on the dorsal aspect of the middle phalanx was first suggested by Danforth (1921). From a study of 80 families with a total of 178 children, he suggested that 'a phylogenetically progressive loss of hair is brought about through the action of one or more recessive genes, or of one primary recessive gene with several modifying factors that regulate the distribution of hair when it is present.' Stated conversely, 'despite the fact that in evolutionary progress hair is disappearing from the mid-digital region, its presence...may be regarded as the manifestation of a dominant trait.'\n\nBernstein and Burks (1942) \nsuggested that 5 allelic genes, A-0 to A-4, 'control the inheritance and distribution of middigital hair involving but a single gene substitution (the subscript denoting the number of fingers affected with middigital hair),' and that the genes for the presence of hair are dominant over the genes for its absence.\n\nFrom a literature review and their own study in Brazil, Saldanha and Guinsburg (1961) suggested that lack of middle phalangeal hair may be determined by a pair of recessive genes, but noted that the occurrence of sex, age, and possibly environmental differences make genetic analysis of the trait difficult.\n\nEgesi and Rashid (2010) reviewed the subject of middigital hair and its clinical significance.\n\nDanforth (1921) reported that middigital hair was present in men more often than in women. Caucasians were found to have a higher incidence of middle phalangeal hair than other ethnic groups, including Afro-Americans, American Indians, and Japanese.\n\nSaldanha and Guinsburg (1961) studied the presence or absence of middigital hair in a white population of Sao Paulo, Brazil, including 131 males and 158 females, and compared their findings with those of previous reports. The frequencies of individuals without midphalangeal hair showed striking population differences. The range among northern Europeans varied from 20 to 30%, and among Mediterraneans, from 30 to 50%. Among Japanese, American Indians, and blacks, the figures varied between about 60% and 90%. The trait was virtually absent among Eskimos.\n\n", "id": "45412506", "title": "Midphalangeal hair"}
{"url": "https://en.wikipedia.org/wiki?curid=45419051", "text": "AB(O)H antigens secretion\n\nABH antigens secretion, i.e. presence (phenotype: \"secretor\" - Se) or absence (\"nonsecretor\": se) of ABO blood group system antigens in saliva, milk, sweat, amniotic fluid, urine, feces and other body fluids is one of the most famous polymorphism in the field of blood antigens in body excretions. On red blood cells they are in the form of fat-soluble glycolipids, and in secretions they occur as water-soluble glycoproteins. Previous research has shown that the allele \"Se\" is completely dominant over the allele \"se\".\n\nSecretor locus is linked to the locus of \"Lutheran\" blood antigens locus were found. Parental couples with the ability to determine the secretor status of the offsprings from this linkage potentially allows prenatal diagnosis of myotonic dystrophy (\"DM\"). Se locus and the \"Hh\" (\"FUT1\") locus may be closely linked were also suggested.\n\nThe secretion of water-soluble \"A\", \"B\" and \"H\" antigens in the saliva most widely was studied. A wide variation in the frequency of recessive phenotype (nonsecretor: se) was registered.\nCompatibility of ABH antigens has important impacts on the prognoses of transplants of kidneys, livers and hearts, but less so on marrow, bone or cornea transplantation prognoses.\n\n\n", "id": "45419051", "title": "AB(O)H antigens secretion"}
{"url": "https://en.wikipedia.org/wiki?curid=2423780", "text": "Gene–environment interaction\n\nGene–environment interaction (or genotype–environment interaction or G×E) is when two different genotypes respond to environmental variation in different ways. A norm of reaction is a graph that shows the relationship between genes and environmental factors when phenotypic differences are continuous. They can help illustrate GxE interactions. When the norm of reaction is not parallel, as shown in the figure below, there is a gene by environment interaction. This indicates that each genotype responds to environmental variation in a different way. Environmental variation can be physical, chemical, biological, behavior patterns or life events.\n\nGene–environment interactions are studied to gain a better understanding of various phenomena. In genetic epidemiology, gene-environment interactions are useful for understanding some diseases. Sometimes, sensitivity to environmental risk factors for a disease are inherited rather than the disease itself being inherited. Individuals with different genotypes are affected differently by exposure to the same environmental factors, and thus gene-environment interactions can result in different disease phenotypes. For example, sunlight exposure has a stronger influence on skin cancer risk in fair-skinned humans than in individuals with darker skin.\n\nThese interactions are of particular interest to genetic epidemiologists for predicting disease rates and methods of prevention with respect to public health. The term is also used amongst developmental psychobiologists to better understand individual and evolutionary development.\n\nNature versus nurture debates assume that variation in a trait is primarily due to either genetic differences or environmental differences. However, the current scientific opinion holds that neither genetic differences nor environmental differences are solely responsible for producing phenotypic variation, and that virtually all traits are influenced by both genetic and environmental differences.\n\nStatistical analysis of the genetic and environmental differences contributing to the phenotype would have to be used to confirm these as gene-environment interactions. In developmental genetics, a causal interaction is enough to confirm gene-environment interactions. \n\nThe history of defining gene-environment interaction dates back to the 1930s and remains a topic of debate today. The first instance of debate occurred between Ronald Fisher and Lancelot Hogben.\n\nFisher sought to eliminate interaction from statistical studies as it was a phenomenon that could be removed using a variation in scale. Hogben believed that the interaction should be investigated instead of eliminated as it provided information on the causation of certain elements of development.\n\nA similar argument faced multiple scientists in the 1970s. Arthur Jensen published the study “How much can we boost IQ and scholastic achievement?”, which amongst much criticism also faced contention by scientists Richard Lewontin and David Layzer. Lewontin and Layzer argued that in order to conclude causal mechanisms, the gene-environment interaction could not be ignored in the context of the study while Jensen defended that interaction was purely a statistical phenomenon and not related to development.\n\nAround the same time, Kenneth J. Rothman supported the use of a statistical definition for interaction while researchers Kupper and Hogan believed the definition and existence of interaction was dependent on the model being used.\n\nThe most recent criticisms were spurred by Moffitt and Caspi's studies on \"5-HTTLPR\" and stress and its influence on depression. In contrast to previous debates, Moffitt and Caspi were now using the statistical analysis to prove that interaction existed and could be used to uncover the mechanisms of a vulnerability trait. Contention came from Zammit, Owen and Lewis who reiterated the concerns of Fisher in that the statistical effect was not related to the developmental process and would not be replicable with a difference of scale.\n\nThere are two different conceptions of gene–environment interaction today. Tabery has labeled them \"biometric\" and \"developmental\" interaction, while Sesardic uses the terms \"statistical\" and \"commonsense\" interaction.\n\nThe biometric (or statistical) conception has its origins in research programs that seek to measure the relative proportions of genetic and environmental contributions to phenotypic variation within populations. Biometric gene–environment interaction has particular currency in population genetics and behavioral genetics. Any interaction results in the breakdown of the additivity of the main effects of heredity and environment, but whether such interaction is present in particular settings is an empirical question. Biometric interaction is relevant in the context of research on individual differences rather than in the context of the development of a particular organism.\n\nDevelopmental gene–environment interaction is a concept more commonly used by developmental geneticists and developmental psychobiologists. Developmental interaction is not seen merely as a statistical phenomenon. Whether statistical interaction is present or not, developmental interaction is in any case manifested in the causal interaction of genes and environments in producing an individual's phenotype.\n\nIn epidemiology, the following models can be used to group the different interactions between gene and environment.\n\nModel A describes a genotype that increases the level of expression of a risk factor but does not cause the disease itself. For example, the PKU gene results in higher levels of phenylalanine than normal which in turn causes mental retardation.\n\nThe risk factor in Model B in contrast has a direct effect on disease susceptibility which is amplified by the genetic susceptibility. Model C depicts the inverse, where the genetic susceptibility directly effects disease while the risk factor amplifies this effect. In each independent situation, the factor directly effecting the disease can cause disease by itself.\n\nModel D differs as neither factor in this situation can effect disease risk, however, when both genetic susceptibility and risk factor are present the risk is increased. For example, the G6PD deficiency gene when combined with fava bean consumption results in hemolytic anemia. This disease does not arise in individuals that eat fava beans and lack G6PD deficiency nor in G6PD-deficient people who do not eat fava beans.\n\nLastly, Model E depicts a scenario where the environmental risk factor and genetic susceptibility can individually both influence disease risk. When combined, however, the effect on disease risk differs.\n\nThe models are limited by the fact that the variables are binary and so do not consider polygenic or continuous scale variable scenarios.\n\nAdoption studies have been used to investigate how similar individuals that have been adopted are to their biological parents with whom they did not share the same environment with. Additionally, adopted individuals are compared to their adoptive family due to the difference in genes but shared environment. For example, an adoption study showed that Swedish men with disadvantaged adoptive environments and a genetic predisposition were more likely to abuse alcohol.\n\nUsing monozygotic twins, the effects of different environments on identical genotypes could be observed. Later studies leverage biometrical modelling techniques to include the comparisons of dizygotic twins to ultimately determine the different levels of gene expression in different environments.\n\nFamily-based research focuses on the comparison of low-risk controls to high risk children to determine the environmental effect on subjects with different levels of genetic risk. For example, a Danish study on high-risk children with schizophrenic mothers depicted that children without a stable caregiver were associated with an increased risk of schizophrenia.\n\nThe often used method to detect gene-environment interactions is by studying the effect a single gene variation (candidate gene) has with respect to a particular environment. Single nucleotide polymorphisms (SNP’s) are compared with single binary exposure factors to determine any effects.\n\nCandidate studies such as these require strong biological hypotheses which are currently difficult to select given the little understanding of biological mechanisms that lead to higher risk.\n\nThese studies are also often difficult to replicate commonly due to small sample sizes which typically results in disputed results.\n\nThe polygenic nature of complex phenotypes suggests single candidate studies could be ineffective in determining the various smaller scale effects from the large number of influencing gene variants.\n\nSince the same environmental factor could interact with multiple genes, a polygenic approach can be taken to analyze GxE interactions. A polygenic score is generated using the alleles associated with a trait and their respective weights based on effect and examined in combination with environmental exposure. Though this method of research is still early, it is consistent with psychiatric disorders. As a result of the overlap of endophenotypes amongst disorders this suggests that the outcomes of gene-environment interactions are applicable across various diagnoses.\n\nA genome wide interaction scan (GEWIS) approach examines the interaction between the environment and a large number of independent SNP’s. A effective approach to this all-encompassing study occurs in two-steps where the genome is first filtered using gene-level tests and pathway based gene set analyses. The second step uses the SNP’s with G-E association and tests for interaction.\n\nThe differential susceptibility hypothesis has been reaffirmed through genome wide approaches.\n\nA particular concern with gene-environment interaction studies is the lack of reproducibility. Specifically complex traits studies have come under scrutiny for producing results that cannot be replicated. For example, studies of the 5HTTLPR gene and stress resulting in modified risk of depression have had conflicting results.\n\nA possible explanation behind the inconsistent results is the heavy use of multiple testing. Studies are suggested to produce inaccurate results due to the investigation of multiple phenotypes and environmental factors in individual experiments.\n\nThere are two different models for the scale of measurement that helps determine if gene-environment interaction exists in a statistical context. There is disagreement on which scale should be used. Under these analyses, if the combined variables fit either model then there is no interaction. The combined effects must either be greater for synergistic or less than for an antagonistic outcome. The additive model measures risk differences while the multiplicative model uses ratios to measure effects. The additive model has been suggested to be a better fit for predicting disease risk in a population while a multiplicative model is more appropriate for disease etiology.\n\nEpigenetics is an example of an underlying mechanism of gene-environment effects, however, it does not conclude whether environment effects are additive, multiplicative or interactive.\n\nNew studies have also revealed the interactive effect of multiple environment factors. For example, a child with a poor quality environment would be more sensitive to a poor environment as an adult which ultimately led to higher psychological distress scores. This depicts a three way interaction Gene x Environment x Environment. The same study suggests taking a life course approach to determining genetic sensitivity to environmental influences within the scope of mental illnesses.\n\nDoctors are interested in knowing whether disease can be prevented by reducing exposure to environmental risks. Some people carry genetic factors that confer susceptibility or resistance to a certain disorder in a particular environment. The interaction between the genetic factors and environmental stimulus is what results in the disease phenotype. There may be significant public health benefits in using gene by environment interactions to prevent or cure disease.\n\nAn individual’s response to a drug can result from various gene by environment interactions. Therefore, the clinical importance of pharmacogenetics and gene by environment interactions comes from the possibility that genomic, along with environmental information, will allow more accurate predictions of an individual’s drug response. This would allow doctors to more precisely select a certain drug and dosage to achieve therapeutic response in a patient while minimizing side effects and adverse drug reactions. This information could also help to prevent the health care costs associated with adverse drug reactions and inconveniently prescribing drugs to patients who likely won’t respond to them.\n\nIn a similar manner, an individual can respond to other environmental stimuli, factors or challenges differently according to specific genetic differences or alleles. These other factors include the diet and specific nutrients within the diet, physical activity, alcohol and tobacco use, sleep (bed time, duration), and any of a number of exposures (or exposome), including toxins, pollutants, sunlight (latitude north/south of the equator), among any number of others. The diet, for example, is modifiable and has significant impact on a host of cardiometabolic diseases, including cardiovascular disease, coronary artery disease, coronary heart disease, type 2 diabetes, hypertension, stroke, myocardial infarction, and non-alcoholic fatty liver disease. In the clinic, typically assessed risks of these conditions include blood lipids (triglyceride, and HDL, LDL and total cholesterol), glycemic traits (plasma glucose and insulin, HOMA-IR, beta cell function as HOMA-BC), obesity anthropometrics (BMI/obesity, adiposity, body weight, waist circumference, waist-to-hip ratio), vascular measures (diastolic and systolic blood pressure), and biomarkers of inflammation. Gene-Environment interactions can modulate the adverse effects of an allele that confers increased risk of disease, or can exacerbate the genotype-phenotype relationship and increase risk, in a manner often referred to as nutrigenetics. A catalog of genetic variants that associate with these and related cardiometabolic phenotypes and modified by common environmental factors is available.\n\nConversely, a disease study using breast cancer, type 2 diabetes, and rheumatoid arthritis shows that including G×E interactions in a risk prediction model does not improve risk identification.\n\n\n", "id": "2423780", "title": "Gene–environment interaction"}
{"url": "https://en.wikipedia.org/wiki?curid=45497515", "text": "Genotype-first approach\n\nThe genotype-first approach is a type of strategy used in genetic epidemiological studies to associate specific genotypes to apparent clinical phenotypes of a complex disease or trait. \nAs opposed to “phenotype-first”, the traditional strategy that have been guiding genome-wide association studies (GWAS) so far, this approach characterizes individuals first by a statistically common genotype based on molecular tests prior to clinical phenotypic classification. This method of grouping leads to patient evaluations based on a shared genetic etiology for the observed phenotypes, regardless of their suspected diagnosis. Thus, this approach can prevent initial phenotypic bias and allow for identification of genes that pose a significant contribution to the disease etiology.\n\nThis approach is unaffected by phenotypic heterogeneity, incomplete penetrance and levels of expressivity. Therefore, it is useful in complex diseases that also overlap, such as autism spectrum disorder and intellectual disability, enabling the diseases to be distinguished, and specific subtypes of the disease based on the genomic content to be determined.\n\nCurrently, the genotype-first approach is used primarily for research objectives. However, the implications from these studies can have valuable clinical applications, including improved diagnosis, counselling, and support groups for individuals with the same genetic etiology.\n\nInitially the idea of identifying the genotype of individuals and subsequently their associated phenotype(s) was first used in early cytogenetic studies. Around 1960 the discovery of Trisomy 21 led to the realization that genetics could be used to predict phenotype(s). From the 1960s to 1990s cytogenetic techniques such as chromosome banding and fluorescence \"in situ\" hybridization (FISH) were used to identify and phenotypically characterize patients with chromosomal abnormalities.\n\nComplex diseases and traits pose many difficulties for epidemiological studies due to their nature as multifactorial diseases. More than one gene can underlie a complex disease and generally contributes a smaller effect than what is observed in monogenic diseases (Mendelian diseases). In addition, many of these complex diseases exhibit diverse phenotypes as well as a wide range of expressivity and penetrance. Genes can also be pleiotropic, accounting for many seemingly distinct clinical phenotypes. These features limit the ability of both research and clinical studies to designate causal genes or variants to the observed phenotypes and to classify disorders.\n\nClinicians are starting to recognize the need to classify genomic diseases by a common genotype rather than a common phenotype and how genotype-first approach can benefit this purpose.\n\nSeveral methods can be used with a genotype-first approach, however, the following steps are usually included:\nThe genotyping is generated using next-generation sequencing technologies (including whole-genome sequencing and exome sequencing) and microarray analyses. The raw data is then statistically analyzed for population-based frequency of the variants. Common variants are filtered out, and pathogenicity is determined though predicted genetic implications. These steps allow for the identification of presumed highly penetrant variants and their specific locus. The selected variants are usually resequenced for validation (by targeted Sanger sequencing). Validated genomic variants can then be analyzed for recurrences among affected individuals within the cohort. Pathogenicity of a genomic variant is statistically based on its significantly abundant presence in the affected compared to the unaffected individuals, not exclusively on the deleteriousness of the variant. A candidate variant can then be associated with a shared phenotype with the aspiration that as more patients baring the same variant with the same phenotype will be identified, a stronger association can be made. \nFinally, delineation is made between a specific variant to associated clinical phenotypes [Figure 1].\n\nThe genotype-first approach has been used to diagnose patients with rare diseases, identify novel disease genotype-phenotypes associations, and characterize uncommon or heterogeneous diseases based on patient's genotype. In 2014 the genotype-first approach was used to assess rare and low-frequency variants in the Finnish population. As the Finnish population is isolated and has recently undergone a population bottleneck, relative to other countries, it offers two main benefits for genotype-first studies. Deleterious variants are found at higher frequencies within a smaller spectrum of rare variants in bottlenecked founder populations. By comparing the variants found using whole-exome sequencing (WES) in the Finnish population to WES from a control group of non-Finnish Europeans, loss-of-function (LOF) variants were seen at a higher frequency in the Finnish population. The phenotypes of Finnish individuals with these LOF variants were then analyzed to ascertain novel genotype-phenotype associations. These associations detected included one that could be embryonic lethal, information that might not have been discovered in research using a phenotype-first approach. In addition, researchers also discovered novel splice variants in the LPA gene that reduce apolipoprotein A levels and offer a protective phenotype against cardiovascular disease.\n\nGenotype-first assessment is becoming the standard approach for clinical diagnosis of complex heterogeneous diseases. Microduplication and microdeletion syndromes have a range of characteristics, including intellectual disability and developmental delay, which vary in severity making patients with these syndromes very difficult to diagnose. Since the development of next-generation sequencing technologies, clinicians have been able to use a genotype-first approach to group these patients based on their microdeletion or duplication and document the disease features present in these groups. Chromosomal microarray analysis, in particular, is being used clinically to assist in diagnosing patients with microdeletion and microdulplication syndromes. In diseases, such as Autism spectrum disorder (ASD), where differentiating patients into disease subtype groups based on phenotype is challenging, genotype-first studies allow the classification of patients into subtypes based on their genetics. This in turn will give a greater understanding of the genetic causes of ASD, and could in the future define specific subtypes of ASD for patients to be diagnosed with.\n\nGenotype-first research, through the identification of novel disease-associated genes, can also benefit pharmaceutical companies and drug development. For complex diseases, using phenotype first gene-association, developing therapeutics is often unsuccessful due to multiple genes contributing to one disease. With genotype-first associations, the potential therapeutic target is identified first.\n\n\n\n", "id": "45497515", "title": "Genotype-first approach"}
{"url": "https://en.wikipedia.org/wiki?curid=45648481", "text": "Hybrizyme\n\nHybrizyme is a term coined to indicate novel or normally rare gene variants (or alleles) that are associated with hybrid zones, geographic areas where two related species meet, mate, and produce hybrid offspring. Hybrizymes occur commonly in many, if not all, hybrid zones. Originally considered to be caused by elevated rates of mutation in hybrids, they are now believed to be the result of purifying selection: in the centre of the hybrid zone, alleles for hybrid fitness are selected, and linked alleles also increase in frequency by genetic hitchhiking. If those linked alleles happen to be rare variants, they will become more common, and their commonness will only be associated with the area where hybrids are formed.\n", "id": "45648481", "title": "Hybrizyme"}
{"url": "https://en.wikipedia.org/wiki?curid=46332309", "text": "Principles of genetics\n\nThe Principle of genetics is a genetics textbook authored by Robert H. Tamarin, an emeritus professor of biology, published by McGraw-Hill Publishers, London.\nThe 7th edition of the book was published on October 2008.\n\nThe book is sectioned into four parts. The first part, Genetics and the Scientific Method briefly review the History of genetics and the various methods used in genetic study. The second part focus on Mendelian inheritance, the third part deals with Molecular genetics and the last section deals with Quantitative genetics and Evolutionary Genetics.\n\nThe book had been reviewed and rated high by several editors and geneticists.\n", "id": "46332309", "title": "Principles of genetics"}
{"url": "https://en.wikipedia.org/wiki?curid=2354482", "text": "C-value\n\nC-value is the amount, in picograms, of DNA contained within a haploid nucleus (e.g. a gamete) or one half the amount in a diploid somatic cell of a eukaryotic organism. In some cases (notably among diploid organisms), the terms C-value and genome size are used interchangeably; however, in polyploids the C-value may represent two or more genomes contained within the same nucleus. Greilhuber \"et al.\" have suggested some new layers of terminology and associated abbreviations to clarify this issue, but these somewhat complex additions are yet to be used by other authors.\n\nMany authors have incorrectly assumed that the 'C' in \"C-value\" refers to \"characteristic\", \"content\", or \"complement\". Even among authors who have attempted to trace the origin of the term, there had been some confusion because Hewson Swift did not define it explicitly when he coined it in 1950. In his original paper, Swift appeared to use the designation \"1C value\", \"2C value\", etc., in reference to \"classes\" of DNA content (e.g., Gregory 2001, 2002); however, Swift explained in personal correspondence to Prof. Michael D. Bennett in 1975 that \"I am afraid the letter C stood for nothing more glamorous than 'constant', i.e., the amount of DNA that was characteristic of a particular genotype\" (quoted in Bennett and Leitch 2005). This is in reference to the report in 1948 by Vendrely and Vendrely of a \"remarkable constancy in the nuclear DNA content of all the cells in all the individuals within a given animal species\" (translated from the original French). Swift's study of this topic related specifically to variation (or lack thereof) among chromosome sets in different cell types within individuals, but his notation evolved into \"C-value\" in reference to the haploid DNA content of individual species and retains this usage today.\n\nC-values vary enormously among species. In animals they range more than 3,300-fold, and in land plants they differ by a factor of about 1,000. Protist genomes have been reported to vary more than 300,000-fold in size, but the high end of this range (\"Amoeba\") has been called into question. Variation in C-values bears no relationship to the complexity of the organism or the number of genes contained in its genome; for example, some single-celled protists have genomes much larger than that of humans. This observation was deemed counterintuitive before the discovery of non-coding DNA. It became known as the C-value paradox as a result. However, although there is no longer any paradoxical aspect to the discrepancy between C-value and gene number, this term remains in common usage. For reasons of conceptual clarification, the various puzzles that remain with regard to genome size variation instead have been suggested to more accurately comprise a complex but clearly defined puzzle known as the C-value enigma. C-values correlate with a range of features at the cell and organism levels, including cell size, cell division rate, and, depending on the taxon, body size, metabolic rate, developmental rate, organ complexity, geographical distribution, or extinction risk (for recent reviews, see Bennett and Leitch 2005; Gregory 2005).\n\nThe C-value enigma or C-value paradox is the complex puzzle surrounding the extensive variation in nuclear genome size among eukaryotic species. At the center of the C-value enigma is the observation that genome size does not correlate with organismal complexity; for example, some single-celled protists have genomes much larger than that of humans.\n\nSome prefer the term C-value enigma because it explicitly includes all of the questions that will need to be answered if a complete understanding of genome size evolution is to be achieved (Gregory 2005). Moreover, the term paradox implies a lack of understanding of one of the most basic features of eukaryotic genomes: namely that they are composed primarily of non-coding DNA. Some have claimed that the term paradox also has the unfortunate tendency to lead authors to seek simple one-dimensional solutions to what is, in actuality, a multi-faceted puzzle. For these reasons, in 2003 the term \"C-value enigma\" was endorsed in preference to \"C-value paradox\" at the Second Plant Genome Size Discussion Meeting and Workshop at the Royal Botanic Gardens, Kew, UK,\nand an increasing number of authors have begun adopting this term.\n\nIn 1948, Roger and Colette Vendrely reported a \"remarkable constancy in the nuclear DNA content of all the cells in all the individuals within a given animal species\", which they took as evidence that DNA, rather than protein, was the substance of which genes are composed. The term C-value reflects this observed constancy. However, it was soon found that C-values (genome sizes) vary enormously among species and that this bears no relationship to the \"presumed\" number of genes (\"as reflected by\" the complexity of the organism). For example, the cells of some salamanders may contain 40 times more DNA than those of humans. Given that C-values were assumed to be constant because genetic information is encoded by DNA, and yet bore no relationship to presumed gene number, this was understandably considered paradoxical; the term \"C-value paradox\" was used to describe this situation by C.A. Thomas, Jr. in 1971.\n\nThe discovery of non-coding DNA in the early 1970s resolved the main question of the C-value paradox: genome size does not reflect gene number in eukaryotes since most of their DNA is non-coding and therefore does not consist of genes. The human genome, for example, comprises less than 2% protein-coding regions, with the remainder being various types of non-coding DNA (especially transposable elements).\n\nThe term \"C-value enigma\" represents an update of the more common but outdated term \"C-value paradox\" (Thomas 1971), being ultimately derived from the term \"C-value\" (Swift 1950) in reference to haploid nuclear DNA contents. The term was coined by Canadian biologist Dr. T. Ryan Gregory of the University of Guelph in 2000/2001. In general terms, the C-value enigma relates to the issue of variation in the amount of non-coding DNA found within the genomes of different eukaryotes.\n\nThe C-value enigma, unlike the older C-value paradox, is explicitly defined as a series of independent but equally important component questions, including:\n\n\nThe formulas for converting the number of nucleotide pairs (or base pairs) to picograms of DNA and vice-versa are:\n\nBy using the data in Table 1, relative masses of nucleotide pairs can be calculated as follows: A/T = 615.383 and G/C = 616.3711, bearing in mind that formation of one phosphodiester linkage involves a loss of one HO molecule. Further, phosphates of nucleotides in the DNA chain are acidic, so at physiologic pH the H ion is dissociated. Provided the ratio of A/T to G/C pairs is 1:1 (the GC-content is 50%), the mean relative mass of one nucleotide pair is 615.8771.\n\nThe relative molecular mass may be converted to an absolute value by multiplying it by the atomic mass unit (1 u) in picograms. Thus, 615.8771 is multiplied by 1.660539 × 10 pg. Consequently, the mean mass per nucleotide pair would be 1.023 × 10 pg, and 1 pg of DNA would represent 0.978 × 10 base pairs (978 Mbp).\n\nNo species has a GC-content of exactly 50% (equal amounts of A/T and G/C nucleotide bases) as assumed by Doležel \"et al.\" However, as a G/C pair is only heavier than an A/T pair by about 1/6 of 1%, the effect of variations in GC content is small. The actual GC content varies between species, between chromosomes, and between isochores (sections of a chromosome with like GC content). Adjusting Doležel's calculation for GC content, the theoretical variation in base pairs per picogram ranges from 977.0317 Mbp/pg for 100% GC content to 978.6005 Mbp/pg for 0% GC content (A/T being lighter, has more Mbp/pg), with a midpoint of 977.8155 Mbp/pg for 50% GC content.\n\nThe Human genome varies in size; however, the current estimate of the nuclear haploid size of the reference human genome is 3,031,042,417 bp for the X gamete and 2,932,228,937 bp for the Y gamete. The X gamete and Y gamete both contain 22 autosomes whose combined lengths comprise the majority of the genome in both gametes. The X gamete contains an X chromosome, while the Y gamete contains a Y chromosome. The larger size of the X chromosome is responsible for the difference in the size of the two gametes. When the gametes are combined, the XX female zygote has a size of 6,062,084,834 bp while the XY male zygote has a size 5,963,271,354 bp. However, the base pairs of the XX female zygote are distributed among 2 homologous groups of 23 heterologous chromosomes each, while the base pairs of the XY male zygote are distributed among 2 homologous groups of 22 heterologous chromosomes each plus 2 heterologous chromosomes. Although each zygote has 46 chromosomes, 23 chromosomes of the XX female zygote are heterologous while 24 chromosomes of the XY male zygote are heterologous. As a result, the C-value for the XX female zygote is 3.099361 while the C-value for the XY male zygote is 3.157877.\n\nThe human genome's GC content is about 41%. Accounting for the autosomal, X, and Y chromosomes, human haploid GC contents are 40.97460% for X gametes, and 41.01724% for Y gametes.\n\nSummarizing these numbers:\n<div>\n\n", "id": "2354482", "title": "C-value"}
{"url": "https://en.wikipedia.org/wiki?curid=46233587", "text": "Universal stress protein\n\nThe universal stress protein (USP) domain is a superfamily of conserved genes which can be found in bacteria, archaea, fungi, protozoa and plants. Proteins containing the domain are induced many environmental stressors such as nutrient starvation, drought, extreme temperatures, high salinity, and the presence of uncouplers, antibiotics and metals.\n\nIn the presence of these stressors, \"Usp\" genes are upregulated resulting in large quantities of Usp proteins being produced by the cell. The over production of USP genes allows the organisms to better cope with stresses by largely unknown mechanisms. However, the USPs will alter the expression of a variety of genes that help to cope with stress.\n\nThe primary function of this superfamily is to protect the organism from environmental stress such as exposure to UV light, which may induce genes containing the USP domain in order to protect the DNA and more generally the cell from further damage. During bacterial starvation the USP genes upregulated will often arrest cell growth and promote its metabolism to dadapt to sparse nutrients.\n\nRecent research also suggests proteins containing this domain have functions beyond the realms of dealing with environmental stresses. Nachin et al. demonstrated in \"Escherichia coli\" that USPs are involved in actions such as adhesion and motility. The researchers, through means of \"knocking out\" USP genes known as \"UspE\" and \"UspC\", saw results suggesting an inability to swim and completely lack of motility, respectively. Conversely, mutants for genes \"UspF\" and \"UspG\" were shown to have enhanced swimming abilities. Therefore, mobility is affected both positively and negatively USPs within \"E. coli\". This demonstrates USPs influence throughout the cell could be widespread for a number of reasons.\n\nAdditionally, in \"Halmonas elongate\", there is a USP called TeaD has been described as a key regulator in the transport of Ectoine across the cell membrane. This demonstrates how versatile USPs can be. Their function, while primarily encompasses increasing survival during stressful conditions, is not always limited to this.\n\nThe ubiquitous nature of these proteins suggests the domain evolved in an ancestral species as well as highlighting the clear biological significance these proteins have in order to still be present the three domains of life. It has been suggested that the USP A domain was part of an ancient protein family. This is due to the similarity in structure between many distantly related organisms. Aravind et al. confirmed these ideas with extensive evolutionary analysis. Aravind suggested that these proteins were part of a much larger protein structural family which was present and diversified in our last universal common ancestor for all extant life. \nThe original function has been suggested to be a nucleotide binding domain which was implicated in signal transduction \n\nAs the USP domain is widespread across many organisms, there is great diversity in the structures of these proteins. For \"Haemophilus influenzae\", its UspA resides in the cytoplasm. The protein forms an asymmetric dimer with characteristic alpha and beta fold structures. There are differences among different bacteria in areas such as ATP binding sites. In this case, UspA does not have ATP binding activity. Generally, USPs form dimers and have domains for nucleotide binding activity. However, as it is such a diverse group, often with little known about the exact structure, it’s not possible to comment on each USP. In addition to this, UspA may reside in different areas of the cell. For example, in this case it was in the cytoplasm but for others, it may be in the cell membrane.\n\nMuch of the research into USP is done on bacteria, specifically \"E. coli\" (Strain K-12). Consequently, much is known about the USP domains in bacteria. In \"E. coli\" there are six families of USP domains which are present in more than 1000 different proteins. The six families are Usp A, -C, -D, -E, -F and –G which are triggered by differing environmental insults and often act via varying mechanisms.\n\nUspA is the most commonly studied USP due to its widespread presence within bacterial genomes. UspA is especially implicated in the resistance of a huge number of stressors most notably tetracycline exposure and high temperatures, with the exception of not forming a response to cold shock. It is thought UspA is especially important to the recovery of \"E. coli\" following starvation of nutrients. UspA during normal growth conditions does not seem to influence gene expression. However, during stressful conditions such as carbon starvation, UspA has been shown to have a global influence on gene expression. A proposed mechanism for such a change in gene expression is that UspA has been suggested to bind to DNA. When UspA is mutated, \"E. coli\" becomes far more vulnerable UV induced DNA damage.\nIt’s important to note the USP responses are independent of many other stress responses seen in bacteria such as rpoS.\n\nThe induction of USP proteins have also been implicated in transitions not only in metabolism or growth but in changes in the colonies' entire phenotype. Bacterial colonies can produce formations known as biofilms. Zhang and colleagues demonstrated that USPs may be involved in the promotion of intertidal biofilms. They observed that during stressful conditions involving metal ions and oxidative stresses that the biofilm phenotype would form. Upon analysis of these biofilms, it could be seen that there was a greatly upregulated level of UspA which Zhang suggests, may be involved with induction of biofilm formation. It is thought UspA may be involved in signalling processes which will upregulate genes involved with biofilm production. With findings such as these, it's beginning to be accepted that USPs are acting using an extremely wide range of mechanisms to ensure cell survival.\n\nIn bacteria, the USP genes can be regulated by sigma factors within RNA polymerases. This includes sigma factor σ70 which through binding to a single promoter region, upregulates the transcription of UspA in bacteria. The genes are regulated in a monocistronic fashion.\nAdditionally, UspA, UspC, UspD and UspE are over induced during stationary phase through regulation of RecA. RecA is known for its involvement in the repair of DNA via homologous recombination following damage. Consequently, the four Usp domain genes are thought to be mediating the management or protection of DNA. Whatever the mechanism exhibited by the proteins, one thing which can be concluded is that USP domains are crucial for survival of many bacterial species. Gomes et al. found that UspA deletions in \"Listeria\" severely impaired survival as well as listeria’s stress response by in vitro and in vivo.\n\nUSP domain genes are regulated by a number of proteins involved with growth, DNA repair and cell division. Notable positive regulation occurs via the action of ppGpp, RecA and FtsZ dependent regulatory pathways. USP domain genes are also under the negative control of FadR. \nPlants contain many hundreds of USP domains and genes. These genes are notably induced by environmental stresses such as drought. When a lack of hydration occurs, biochemical changes induced by the actions of USPs ensue. In response to drought, there is a reduction in photosynthetic carbon production as well as a reduction in energy metabolism. These actions are suggested to occur due to their implications in increasing energy conservation. Water limiting conditions are a common environmental pressure which plants will need to cope with on a regular basis, depending on their habitat. These resistant phenotypes will have an increased survival as they allow the plant to conserve energy in times of restricted water which is key to glucose production through photosynthesis.\n\nMycobacterium tuberculosis, the infectious agent responsible for Tuberculosis (TB), persists within an estimated two billion people. TB is known for its ability to transition into a latent state whereby there is slow growth but high persistence within the mammalian host in structures known as granulomas. These granuloma structures are made up of various cellular materials and immune cells. These include macrophages, neutrophils, cellulose and fats. It has long been proposed that USPs play a significant role in the persistence of TB within the human host. This is due to observations of elevated \"Usp\" genes within \"M. tuberculosis\" in the latent granuloma stage of the infection.\n\nThere are eight types of USPs within \"M. tuberculosis\", all of which have an ATP binding domain. It has been found that within \"M. tuberculosis\", these USPs are regulated by FtsK and FadR. One recent finding shows that the induction of USPs within \"M. tuberculosis\" results in USP binding activity with intracellular cAMP which has indirect implications on transcription within the bacteria.\n\nSome of \"M. tuberculosis\"' USPs are suggested to be induced by the hypoxic conditions found within the granuloma. Specifically, Rv2623, a type of USP in \"M. tuberculosis\", is induced by the presence of nitric oxide, reactive oxygen species and a downshift in pH. All of these conditions are suggested to be produced by the actions of macrophages which are particularly prevalent within the granuloma structures that are characteristic of TB latent infections. These conditions have been found to upregulate a particular USP gene called \"rv2623\", as well as an additional 50 genes involved in long term persistence in the mammalian host. It was suggested this USP gene was involved in inducing the latent response within the mammalian host. This stage of the infection is currently chronic with no effective treatments. This makes these kinds of findings extremely valuable.\n\nRv2623 has an ATP binding domain which if knocked out results in a hyper-virulent form of the bacteria. Understanding these processes aids researchers in their quest to provide effective treatment for those suffering from TB. Rv2623 is also a key biomarker aiding the diagnostic process for TB. Therefore, these USP genes could be crucial for the long term survival of the bacteria meaning that there may be potential therapeutic avenues of research to explore in treating latent TB. This comes at a time whereby TB kills many thousands of people a day and is becoming increasing problematic to treat with the rise of multi-drug-resistant TB.\n\nSimilarly, USPs are crucial for the survival of Salmonella, the causative agent in Salmonellosis. In developing countries, food poisoning of this kind is a potentially life-threatening condition. The USPs have influence in growth arrest, stress responses and virulence. UspA is induced by metabolic, oxidative and temperature related stress. In these conditions UspA is over produced through the transcriptional regulation by ppGpp and RecA. These responses have been suggested to be involved in the protection of DNA. As a result, UspA aids Salmonella to resist stressors produced by the mammalian immune system assisting in survival and hence, pathogenicity. When UspA is inactivated in Salmonella, the mutants die prematurely, demonstrating how crucial these proteins are to survival and persistence. Again, understanding these processes may aid researchers in developing effective drugs to treat these infections.\n\n", "id": "46233587", "title": "Universal stress protein"}
{"url": "https://en.wikipedia.org/wiki?curid=37050144", "text": "Cell–cell fusogens\n\nCell–cell fusogens are glycoproteins that facilitate the fusion of cell to cell membranes. Cell-cell fusion is critical for the merging of gamete genomes and development of organs in multicellular organisms. It drives cell membrane protrusions and fusogenic protein engagement.\n\nEFF-AFF are the identifiers of type 1 glycoproteins that make up cell-cell fusogens. They were first identified when EFF1 mutants were found to block cell fusion in all epidermal and vulval epithelia in the roundworm, \"Caenorhabditis elegans\". EFF-AFF is a family of type I membrane glycoproteins that act as cell–cell fusogens, named from 'Anchor cell fusion failure'. However, fusion between the anchor-cell and the (uterine seam) utse syncytium that establishes a continuous uterine-vulval tube proceeds normally in eff-1 mutants and thus Aff1 was established as necessary for this and the fusion of heterologous cells in \"C. elegans\". The transmembrane forms of these proteins, like most viral fusogens, possess an N-terminal signal sequence followed by a long extracellular portion, a predicted transmembrane domain, and a short intracellular tail. A striking conservation in the position and number of all 16 cysteines in the extracellular portion of EFF-AFF proteins from different nematode species suggests that these proteins are folded in a similar 3D structure that is\nessential for their fusogenic activity. C. elegans AFF-1 and EFF-1 proteins are essential for developmental cell-to-cell fusion and can merge insect cells. Thus FFs comprise an ancient\nfamily of cellular fusogens that can promote fusion when expressed on a viral particle.\n\n", "id": "37050144", "title": "Cell–cell fusogens"}
{"url": "https://en.wikipedia.org/wiki?curid=46786870", "text": "Topologically associating domain\n\nA topologically associating domain (TAD) is a self-interacting genomic region, meaning that DNA sequences within a TAD physically interact with each other more frequently than with sequences outside the TAD. These three-dimensional chromosome structures are present in animals as well as some plants, fungi, and bacteria. TADs can range in size from thousands to millions of DNA bases.\n\nThe functions of TADs are not fully understood, but in some cases, disrupting TADs leads to disease because changing the 3D organization of the chromosome disrupts gene regulation. The mechanisms underlying TAD formation are also complex and not yet fully elucidated, though a number of protein complexes and DNA elements are associated with TAD boundaries.\n\nTADs are defined as regions whose DNA sequences preferentially contact each other. They were discovered in 2012 using chromosome conformation capture techniques including Hi-C. They have been shown to be present in fruit flies (Drosophila), mouse and human genomes, but not in the wine yeast Saccharomyces cerevisiae.\n\nTAD locations are defined by applying an algorithm to Hi-C data. For example, TADs are often called according to the method in Dixon et al. (2012), using the so-called \"directionality index\". The directionality index is calculated for individual 40kb bins, by collecting the reads that fall in the bin, and observing whether their paired reads map upstream or downstream of the bin (read pairs are required to span no more than 2Mb). A positive value indicates that more read pairs lie downstream than upstream, and a negative value indicates the reverse. Mathematically, the directionality index is a signed chi-square statistic.\n\nA number of proteins are known to be associated with TAD formation including the protein CTCF and the protein complex cohesin. It is also unknown what components are required at TAD boundaries; however, in mammalian cells, it has been shown that these boundary regions have comparatively high levels of CTCF binding. In addition, some types of genes (such as transfer RNA genes and housekeeping genes) appear near TAD boundaries more often than would be expected by chance.\n\nTADs have been reported to be relatively constant between different cell types (in stem cells and blood cells, for example), and even between species in specific cases.\n\nThe majority of observed interactions between promoters and enhancers do not cross TAD boundaries. Removing a TAD boundary (for example, using CRISPR to delete the relevant region of the genome) can allow new promoter-enhancer contacts to form. This can affect gene expression nearby - such misregulation has been shown to cause limb malformations (e.g. polydactyly) in humans and mice.\n\nTADs have been reported to be the same as replication domains, regions of the genome that are copied (replicated) at the same time during S phase of cell division. Insulated neighborhoods, DNA loops formed by CTCF/cohesin-bound regions, are proposed to functionally underlie TADs.\n\nDisruption of TAD boundaries can affect the expression of nearby genes, and this can cause disease.\n\nFor example, genomic structural variants that disrupt TAD boundaries have been reported to cause developmental disorders such as human limb malformations. Additionally, several studies have provided evidence that the disruption or rearrangement of TAD boundaries can provide growth advantages to certain cancers, such as T-cell acute lymphoblastic leukemia(T-ALL), gliomas, and colorectal cancer.\n\nLamina-associated domains (LADs) are parts of the chromatin that heavily interact with the lamina, a network-like structure at the inner membrane of the nucleus. LADs consist mostly of transcriptionally silent chromatin, being enriched with trimethylated Lys27 on histone H3, which is a common posttranslational histone modification of heterochromatin. LADs have CTCF-binding sites at their periphery.\n\n", "id": "46786870", "title": "Topologically associating domain"}
{"url": "https://en.wikipedia.org/wiki?curid=207754", "text": "Somatic cell\n\nA somatic cell (Greek: σὠμα/soma = body) or vegetal cell is any biological cell forming the body of an organism; that is, in a multicellular organism, any cell other than a gamete, germ cell, gametocyte or undifferentiated stem cell.\n\nIn contrast, gametes are cells that fuse during sexual reproduction, germ cells are cells that give rise to gametes, and stem cells are cells that can divide through mitosis and differentiate into diverse specialized cell types. For example, in mammals, somatic cells make up all the internal organs, skin, bones, blood and connective tissue, while mammalian germ cells give rise to spermatozoa and ova which fuse during fertilization to produce a cell called a zygote, which divides and differentiates into the cells of an embryo. There are approximately 220 types of somatic cells in the human body.\n\nTheoretically, these cells are not germ cells (the source of gametes); they never transmit to their descendants the mutations they have undergone. However, in sponges, non-differentiated somatic cells form the germ line and, in Cnidaria, differentiated somatic cells are the source of the germline.\n\nThe word \"somatic\" is derived from the Greek word \"sōma\", meaning \"body\".\n\nAs multicellularity evolved many times, sterile somatic cells did too. The evolution of an immortal germline producing specialized somatic cells involved the emergence of mortality, and can be viewed in its simplest version in volvocine algae. Those species with a separation between sterile somatic cells and a germ line are called Weismannists. However, Weismannist development is relatively rare (e.g., vertebrates, arthropods, \"Volvox\"), as a great number of species have the capacity for somatic embryogenesis (e.g., land plants, most algae, many invertebrates).\n\nLike all cells, somatic cells contain DNA arranged in chromosomes. If a somatic cell contains chromosomes arranged in pairs, it is called diploid and the organism is called a diploid organism. (The gametes of diploid organisms contain only single unpaired chromosomes and are called haploid.) Each pair of chromosomes comprises one chromosome inherited from the father and one inherited from the mother. For example, in humans, somatic cells contain 46 chromosomes organized into 23 pairs. By contrast, gametes of diploid organisms contain only half as many chromosomes. In humans, this is 23 unpaired chromosomes. When two gametes (i.e. a spermatozoon and an ovum) meet during conception, they fuse together, creating a zygote. Due to the fusion of the two gametes, a human zygote contains 46 chromosomes (i.e. 23 pairs).\n\nHowever, a large number of species have the chromosomes in their somatic cells arranged in fours (\"tetraploid\") or even sixes (\"hexaploid\"). Thus, they can have diploid or even triploid germline cells. An example of this is the modern cultivated species of wheat, \"Triticum aestivum L.\", a hexaploid species whose somatic cells contain six copies of every chromatid.\n\nThe frequency of spontaneous mutations is significantly lower in advanced male germ cells than in somatic cell types from the same individual. Female germ cells also show a mutation frequency that is lower than that in corresponding somatic cells and similar to that in male germ cells. These findings appear to reflect employment of more effective mechanisms to limit the initial occurrence of spontaneous mutations in germ cells than in somatic cells. Such mechanisms likely include elevated levels of DNA repair enzymes that ameliorate most potentially mutagenic DNA damages.\n\nIn recent years, the technique of cloning whole organisms has been developed in mammals, allowing almost identical genetic clones of an animal to be produced. One method of doing this is called \"somatic cell nuclear transfer\" and involves removing the nucleus from a somatic cell, usually a skin cell. This nucleus contains all of the genetic information needed to produce the organism it was removed from. This nucleus is then injected into an ovum of the same species which has had its own genetic material removed. The ovum now no longer needs to be fertilized, because it contains the correct amount of genetic material (a diploid number of chromosomes). In theory, the ovum can be implanted into the uterus of a same-species animal and allowed to develop. The resulting animal will be a nearly genetically identical clone to the animal from which the nucleus was taken. The only difference is caused by any mitochondrial DNA that is retained in the ovum, which is different from the cell that donated the nucleus. In practice, this technique has so far been problematic, although there have been a few high-profile successes, such as Dolly the Sheep and, more recently, Snuppy, the first cloned dog.\nSomatic cells have also been collected in the practice of cryoconservation of animal genetic resources as a means of conserving animal genetic material, including to clone livestock.\n\nDevelopment of biotechnology has allowed for the genetic manipulation of somatic cells. This biotechnology deals with some ethical controversy in human genetic engineering.\n\n", "id": "207754", "title": "Somatic cell"}
{"url": "https://en.wikipedia.org/wiki?curid=3948246", "text": "HomoloGene\n\nHomoloGene, a tool of the United States National Center for Biotechnology Information (NCBI), is a system for automated detection of homologs (similarity attributable to descent from a common ancestor) among the annotated genes of several completely sequenced eukaryotic genomes.\n\nThe HomoloGene processing consists of the protein analysis from the input organisms. Sequences are compared using blastp, then matched up and put into groups, using a taxonomic tree built from sequence similarity, where closer related organisms are matched up first, and then further organisms are added to the tree. The protein alignments are mapped back to their corresponding DNA sequences, and then distance metrics as molecular distances Jukes and Cantor (1969), Ka/Ks ratio can be calculated.\n\nThe sequences are matched up by using a heuristic algorithm for maximizing the score globally, rather than locally, in a bipartite matching (see complete bipartite graph). And then it calculates the statistical significance of each match. Cutoffs are made per position and Ks values are set to prevent false \"orthologs\" from being grouped together. “Paralogs” are identified by finding sequences that are closer within species than other species.\n\n\"Homo sapiens, Pan troglodytes, Mus musculus, Rattus norvegicus, Canis lupus familiaris, Bos taurus, Gallus gallus, Xenopus tropicalis, Danio rerio\"\n\n\"Drosophila melanogaster, Anopheles gambiae, Caenorhabditis elegans\"\n\n\"Saccharomyces cerevisiae, Schizosaccharomyces pombe, Kluyveromyces lactis, Eremothecium gossypii, Magnaporthe grisea, Neurospora crassa\"\n\n\"Arabidopsis thaliana\"\n\n\"Oryza sativa\"\n\n\"Plasmodium falciparum\".\n\nThe HomoloGene is linked to all Entrez databases and based on homology and phenotype information of these links: \n\nAs a result, HomoloGene displays information about Genes, Proteins, Phenotypes, and Conserved Domains.\n\n", "id": "3948246", "title": "HomoloGene"}
{"url": "https://en.wikipedia.org/wiki?curid=46923448", "text": "Fim switch\n\nThe \"fim\" switch in \"Escherichia coli\" is the mechanism by which the \"fim\" gene cluster, encoding Type I Pili, is transcriptionally controlled.\n\nThese pili are virulence factors involved in adhesion, especially important in uropathogenic \"Escherichia coli\". The gene undergoes phase variation mediated via two recombinases and is a model example of site specific inversion.\n\nThe operon consists of the promoter region \"fim\" S, the main constituent \"fim\" A, its gene product forming a rod like structure and \"fim\" H, coding for an adhesin at the tip, to name just a few important elements. The \"fim\" S region is flanked by 9bp repeats that are mirror images of each other. These mirror images serve as substrates for two ATP-dependent recombinases, \"fim\" B and \"fim\" E. These recombinases can invert the orientation of the \"fim\" S region and only one orientation allows for 3' to 5' transcription.\n\n\"fim\" B \"flips\" the promoter region both ways, from the \"on\" position to the \"off\" position and \"vice versa\", whereas \"fim\" E can only facilitate recombination from \"on\" to \"off\". This equilibrium, shifted towards maintaining the \"off\" position, due to higher \"fim\" E activity, serves as a mode of expressing pili only when adhesion is needed. Another level of transcriptional control in \"E. coli\" is mediated by the sensitivity of the recombinases to pH and osmolarity, further ensuring appropriate expression levels of type-I pili, given the stark differences in osmolarity inside and outside an animal's body. Type-I pili are expressed by many species of \"Enterobacteriaceae\". However, it is to be noted that the transcriptional control can differ widely between species, in \"Salmonella typhimurium\", for example much influence is exerted by a leucine-responsive regulatory protein and there is no \"fim\" S element.\n", "id": "46923448", "title": "Fim switch"}
{"url": "https://en.wikipedia.org/wiki?curid=7955", "text": "DNA\n\nDeoxyribonucleic acid (; DNA) is a thread-like chain of nucleotides carrying the genetic instructions used in the growth, development, functioning and reproduction of all known living organisms and many viruses. DNA and ribonucleic acid (RNA) are nucleic acids; alongside proteins, lipids and complex carbohydrates (polysaccharides), they are one of the four major types of macromolecules that are essential for all known forms of life. Most DNA molecules consist of two biopolymer strands coiled around each other to form a double helix.\n\nThe two DNA strands are called polynucleotides since they are composed of simpler monomer units called nucleotides. Each nucleotide is composed of one of four nitrogen-containing nucleobases (cytosine [C], guanine [G], adenine [A] or thymine [T]), a sugar called deoxyribose, and a phosphate group. The nucleotides are joined to one another in a chain by covalent bonds between the sugar of one nucleotide and the phosphate of the next, resulting in an alternating sugar-phosphate backbone. The nitrogenous bases of the two separate polynucleotide strands are bound together, according to base pairing rules (A with T and C with G), with hydrogen bonds to make double-stranded DNA.\n\nThe complementary nitrogenous bases are divided into two groups, pyrimidines and purines. In a DNA molecule, the pyrimidines are thymine and cytosine, the purines are adenine and guanine.\n\nDNA stores biological information. The DNA backbone is resistant to cleavage, and both strands of the double-stranded structure store the same biological information. This information is replicated as and when the two strands separate. A large part of DNA (more than 98% for humans) is non-coding, meaning that these sections do not serve as patterns for protein sequences.\n\nThe two strands of DNA run in opposite directions to each other and are thus antiparallel. Attached to each sugar is one of four types of nucleobases (informally, \"bases\"). It is the sequence of these four nucleobases along the backbone that encodes biological information. RNA strands are created using DNA strands as a template in a process called transcription. Under the genetic code, these RNA strands are translated to specify the sequence of amino acids within proteins in a process called translation.\n\nWithin eukaryotic cells DNA is organized into long structures called chromosomes. During cell division these chromosomes are duplicated in the process of DNA replication, providing each cell its own complete set of chromosomes. Eukaryotic organisms (animals, plants, fungi and protists) store most of their DNA inside the cell nucleus and some of their DNA in organelles, such as mitochondria or chloroplasts. In contrast prokaryotes (bacteria and archaea) store their DNA only in the cytoplasm. Within the eukaryotic chromosomes, chromatin proteins such as histones compact and organize DNA. These compact structures guide the interactions between DNA and other proteins, helping control which parts of the DNA are transcribed.\n\nDNA was first isolated by Friedrich Miescher in 1869. Its molecular structure was first identified by James Watson and Francis Crick at the Cavendish Laboratory within the University of Cambridge in 1953, whose model-building efforts were guided by X-ray diffraction data acquired by Raymond Gosling, who was a post-graduate student of Rosalind Franklin. DNA is used by researchers as a molecular tool to explore physical laws and theories, such as the ergodic theorem and the theory of elasticity. The unique material properties of DNA have made it an attractive molecule for material scientists and engineers interested in micro- and nano-fabrication. Among notable advances in this field are DNA origami and DNA-based hybrid materials.\n\nDNA is a long polymer made from repeating units called nucleotides. The structure of DNA is dynamic along its length, being capable of coiling into tight loops, and other shapes. In all species it is composed of two helical chains, bound to each other by hydrogen bonds. Both chains are coiled round the same axis, and have the same pitch of 34 ångströms (3.4 nanometres). The pair of chains has a radius of 10 ångströms (1.0 nanometre). According to another study, when measured in a different solution, the DNA chain measured 22 to 26 ångströms wide (2.2 to 2.6 nanometres), and one nucleotide unit measured 3.3 Å (0.33 nm) long. Although each individual nucleotide repeating unit is very small, DNA polymers can be very large molecules containing millions to hundreds of millions of nucleotides. For instance, the DNA in the largest human chromosome, chromosome number 1, consists of approximately 220 million base pairs and would be 85 mm long if straightened.\n\nIn living organisms, DNA does not usually exist as a single molecule, but instead as a pair of molecules that are held tightly together. These two long strands entwine like vines, in the shape of a double helix. The nucleotide contains both a segment of the backbone of the molecule (which holds the chain together) and a nucleobase (which interacts with the other DNA strand in the helix). A nucleobase linked to a sugar is called a nucleoside and a base linked to a sugar and one or more phosphate groups is called a nucleotide. A polymer comprising multiple linked nucleotides (as in DNA) is called a polynucleotide.\n\nThe backbone of the DNA strand is made from alternating phosphate and sugar residues. The sugar in DNA is 2-deoxyribose, which is a pentose (five-carbon) sugar. The sugars are joined together by phosphate groups that form phosphodiester bonds between the third and fifth carbon atoms of adjacent sugar rings. These asymmetric bonds mean a strand of DNA has a direction. In a double helix, the direction of the nucleotides in one strand is opposite to their direction in the other strand: the strands are \"antiparallel\". The asymmetric ends of DNA strands are said to have a directionality of \"five prime\" (5′) and \"three prime\" (3′), with the 5′ end having a terminal phosphate group and the 3′ end a terminal hydroxyl group. One major difference between DNA and RNA is the sugar, with the 2-deoxyribose in DNA being replaced by the alternative pentose sugar ribose in RNA.\n\nThe DNA double helix is stabilized primarily by two forces: hydrogen bonds between nucleotides and base-stacking interactions among aromatic nucleobases. In the aqueous environment of the cell, the conjugated bonds of nucleotide bases align perpendicular to the axis of the DNA molecule, minimizing their interaction with the solvation shell. The four bases found in DNA are adenine (A), cytosine (C), guanine (G) and thymine (T). These four bases are attached to the sugar-phosphate to form the complete nucleotide, as shown for adenosine monophosphate. Adenine pairs with thymine and guanine pairs with cytosine. It was represented by A-T base pairs and G-C base pairs.\n\nThe nucleobases are classified into two types: the purines, A and G, being fused five- and six-membered heterocyclic compounds, and the pyrimidines, the six-membered rings C and T. A fifth pyrimidine nucleobase, uracil (U), usually takes the place of thymine in RNA and differs from thymine by lacking a methyl group on its ring. In addition to RNA and DNA, many artificial nucleic acid analogues have been created to study the properties of nucleic acids, or for use in biotechnology.\n\nUracil is not usually found in DNA, occurring only as a breakdown product of cytosine. However, in several bacteriophages, \"Bacillus subtilis\" bacteriophages PBS1 and PBS2 and \"Yersinia\" bacteriophage piR1-37, thymine has been replaced by uracil. Another phage - Staphylococcal phage S6 - has been identified with a genome where thymine has been replaced by uracil.\n\n5-hydroxymethyldeoxyuracil (dU) is also known to replace thymidine in several genomes including the \"Bacillus\" phages SPO1, ϕe, SP8, H1, 2C and SP82. Another modified uracil - 5-dihydroxypentauracil – has also been described.\n\nBase J (beta-d-glucopyranosyloxymethyluracil), a modified form of uracil, is also found in several organisms: the flagellates \"Diplonema\" and \"Euglena\", and all the kinetoplastid genera. Biosynthesis of J occurs in two steps: in the first step, a specific thymidine in DNA is converted into hydroxymethyldeoxyuridine; in the second, HOMedU is glycosylated to form J. Proteins that bind specifically to this base have been identified. These proteins appear to be distant relatives of the Tet1 oncogene that is involved in the pathogenesis of acute myeloid leukemia. J appears to act as a termination signal for RNA polymerase II.\n\nIn 1976 a bacteriophage - S-2L - which infects species of the genus \"Synechocystis\" was found to have all the adenosine bases within its genome replaced by 2,6-diaminopurine. In 2016 deoxyarchaeosine was found to be present in the genomes of several bacteria and the \"Escherichia\" phage 9g.\n\nModified bases also occur in DNA. The first of these recognised was 5-methylcytosine which was found in the genome of \"Mycobacterium tuberculosis\" in 1925. The complete replacement of cytosine by 5-glycosylhydroxymethylcytosine in T even phages (T2, T4 and T6) was observed in 1953 In the genomes of Xanthomonas oryzae bacteriophage Xp12 and halovirus FH the full complement of cystosine has been replaced by 5-methylcytosine. 6N-methyadenine was discovered to be present in DNA in 1955. N6-carbamoyl-methyladenine was described in 1975. 7-methylguanine was described in 1976. N4-methylcytosine in DNA was described in 1983. In 1985 5-hydroxycytosine was found in the genomes of the Rhizobium phages RL38JI and N17. α-putrescinylthymine occurs in both the genomes of the \"Delftia\" phage ΦW-14 and the \"Bacillus\" phage SP10. α-glutamythymidine is found in the Bacillus phage SP01 and 5-dihydroxypentyluracil is found in the Bacillus phage SP15.\n\nThe reason for the presence of these non canonical bases in DNA is not known. It seems likely that at least part of the reason for their presence in bacterial viruses (phages) is to avoid the restriction enzymes present in bacteria. This enzyme system acts at least in part as a molecular immune system protecting bacteria from infection by viruses.\n\nThis does not appear to be the entire story. Four modifications to the cytosine residues in human DNA have been reported. These modifications are the addition of methyl (CH)-, hydroxymethyl (CHOH)-, formyl (CHO)- and carboxyl (COOH)- groups. These modifications are thought to have regulatory functions.\n\nSeventeen non canonical bases are known to occur in DNA. Most of these are modifications of the canonical bases plus uracil.\n\n\nTwin helical strands form the DNA backbone. Another double helix may be found tracing the spaces, or grooves, between the strands. These voids are adjacent to the base pairs and may provide a binding site. As the strands are not symmetrically located with respect to each other, the grooves are unequally sized. One groove, the major groove, is 22 Å wide and the other, the minor groove, is 12 Å wide. The width of the major groove means that the edges of the bases are more accessible in the major groove than in the minor groove. As a result, proteins such as transcription factors that can bind to specific sequences in double-stranded DNA usually make contact with the sides of the bases exposed in the major groove. This situation varies in unusual conformations of DNA within the cell \"(see below)\", but the major and minor grooves are always named to reflect the differences in size that would be seen if the DNA is twisted back into the ordinary B form.\n\nIn a DNA double helix, each type of nucleobase on one strand bonds with just one type of nucleobase on the other strand. This is called complementary base pairing. Here, purines form hydrogen bonds to pyrimidines, with adenine bonding only to thymine in two hydrogen bonds, and cytosine bonding only to guanine in three hydrogen bonds. This arrangement of two nucleotides binding together across the double helix is called a Watson-Crick base pair. Another type of base pairing is Hoogsteen base pairing where two hydrogen bonds form between guanine and cytosine. As hydrogen bonds are not covalent, they can be broken and rejoined relatively easily. The two strands of DNA in a double helix can thus be pulled apart like a zipper, either by a mechanical force or high temperature. As a result of this base pair complementarity, all the information in the double-stranded sequence of a DNA helix is duplicated on each strand, which is vital in DNA replication. This reversible and specific interaction between complementary base pairs is critical for all the functions of DNA in living organisms.\n\nThe two types of base pairs form different numbers of hydrogen bonds, AT forming two hydrogen bonds, and GC forming three hydrogen bonds (see figures, right).\nDNA with high GC-content is more stable than DNA with low GC-content.\nAs noted above, most DNA molecules are actually two polymer strands, bound together in a helical fashion by noncovalent bonds; this double-stranded (dsDNA) structure is maintained largely by the intrastrand base stacking interactions, which are strongest for G,C stacks. The two strands can come apart – a process known as melting – to form two single-stranded DNA (ssDNA) molecules. Melting occurs at high temperature, low salt and high pH (low pH also melts DNA, but since DNA is unstable due to acid depurination, low pH is rarely used).\n\nThe stability of the dsDNA form depends not only on the GC-content (% G,C basepairs) but also on sequence (since stacking is sequence specific) and also length (longer molecules are more stable). The stability can be measured in various ways; a common way is the \"melting temperature\", which is the temperature at which 50% of the ds molecules are converted to ss molecules; melting temperature is dependent on ionic strength and the concentration of DNA.\nAs a result, it is both the percentage of GC base pairs and the overall length of a DNA double helix that determines the strength of the association between the two strands of DNA. Long DNA helices with a high GC-content have stronger-interacting strands, while short helices with high AT content have weaker-interacting strands. In biology, parts of the DNA double helix that need to separate easily, such as the TATAAT Pribnow box in some promoters, tend to have a high AT content, making the strands easier to pull apart.\n\nIn the laboratory, the strength of this interaction can be measured by finding the temperature necessary to break the hydrogen bonds, their melting temperature (also called \"T\" value). When all the base pairs in a DNA double helix melt, the strands separate and exist in solution as two entirely independent molecules. These single-stranded DNA molecules have no single common shape, but some conformations are more stable than others.\n\nA DNA sequence is called \"sense\" if its sequence is the same as that of a messenger RNA copy that is translated into protein. The sequence on the opposite strand is called the \"antisense\" sequence. Both sense and antisense sequences can exist on different parts of the same strand of DNA (i.e. both strands can contain both sense and antisense sequences). In both prokaryotes and eukaryotes, antisense RNA sequences are produced, but the functions of these RNAs are not entirely clear. One proposal is that antisense RNAs are involved in regulating gene expression through RNA-RNA base pairing.\n\nA few DNA sequences in prokaryotes and eukaryotes, and more in plasmids and viruses, blur the distinction between sense and antisense strands by having overlapping genes. In these cases, some DNA sequences do double duty, encoding one protein when read along one strand, and a second protein when read in the opposite direction along the other strand. In bacteria, this overlap may be involved in the regulation of gene transcription, while in viruses, overlapping genes increase the amount of information that can be encoded within the small viral genome.\n\nDNA can be twisted like a rope in a process called DNA supercoiling. With DNA in its \"relaxed\" state, a strand usually circles the axis of the double helix once every 10.4 base pairs, but if the DNA is twisted the strands become more tightly or more loosely wound. If the DNA is twisted in the direction of the helix, this is positive supercoiling, and the bases are held more tightly together. If they are twisted in the opposite direction, this is negative supercoiling, and the bases come apart more easily. In nature, most DNA has slight negative supercoiling that is introduced by enzymes called topoisomerases. These enzymes are also needed to relieve the twisting stresses introduced into DNA strands during processes such as transcription and DNA replication.\n\nDNA exists in many possible conformations that include A-DNA, B-DNA, and Z-DNA forms, although, only B-DNA and Z-DNA have been directly observed in functional organisms. The conformation that DNA adopts depends on the hydration level, DNA sequence, the amount and direction of supercoiling, chemical modifications of the bases, the type and concentration of metal ions, and the presence of polyamines in solution.\n\nThe first published reports of A-DNA X-ray diffraction patterns—and also B-DNA—used analyses based on Patterson transforms that provided only a limited amount of structural information for oriented fibers of DNA. An alternative analysis was then proposed by Wilkins \"et al.\", in 1953, for the \"in vivo\" B-DNA X-ray diffraction-scattering patterns of highly hydrated DNA fibers in terms of squares of Bessel functions. In the same journal, James Watson and Francis Crick presented their molecular modeling analysis of the DNA X-ray diffraction patterns to suggest that the structure was a double-helix.\n\nAlthough the \"B-DNA form\" is most common under the conditions found in cells, it is not a well-defined conformation but a family of related DNA conformations that occur at the high hydration levels present in living cells. Their corresponding X-ray diffraction and scattering patterns are characteristic of molecular paracrystals with a significant degree of disorder.\n\nCompared to B-DNA, the A-DNA form is a wider right-handed spiral, with a shallow, wide minor groove and a narrower, deeper major groove. The A form occurs under non-physiological conditions in partly dehydrated samples of DNA, while in the cell it may be produced in hybrid pairings of DNA and RNA strands, and in enzyme-DNA complexes. Segments of DNA where the bases have been chemically modified by methylation may undergo a larger change in conformation and adopt the Z form. Here, the strands turn about the helical axis in a left-handed spiral, the opposite of the more common B form. These unusual structures can be recognized by specific Z-DNA binding proteins and may be involved in the regulation of transcription.\n\nFor many years exobiologists have proposed the existence of a shadow biosphere, a postulated microbial biosphere of Earth that uses radically different biochemical and molecular processes than currently known life. One of the proposals was the existence of lifeforms that use arsenic instead of phosphorus in DNA. A report in 2010 of the possibility in the bacterium GFAJ-1, was announced, though the research was disputed, and evidence suggests the bacterium actively prevents the incorporation of arsenic into the DNA backbone and other biomolecules.\n\nAt the ends of the linear chromosomes are specialized regions of DNA called telomeres. The main function of these regions is to allow the cell to replicate chromosome ends using the enzyme telomerase, as the enzymes that normally replicate DNA cannot copy the extreme 3′ ends of chromosomes. These specialized chromosome caps also help protect the DNA ends, and stop the DNA repair systems in the cell from treating them as damage to be corrected. In human cells, telomeres are usually lengths of single-stranded DNA containing several thousand repeats of a simple TTAGGG sequence.\n\nThese guanine-rich sequences may stabilize chromosome ends by forming structures of stacked sets of four-base units, rather than the usual base pairs found in other DNA molecules. Here, four guanine bases form a flat plate and these flat four-base units then stack on top of each other, to form a stable G-quadruplex structure. These structures are stabilized by hydrogen bonding between the edges of the bases and chelation of a metal ion in the centre of each four-base unit. Other structures can also be formed, with the central set of four bases coming from either a single strand folded around the bases, or several different parallel strands, each contributing one base to the central structure.\n\nIn addition to these stacked structures, telomeres also form large loop structures called telomere loops, or T-loops. Here, the single-stranded DNA curls around in a long circle stabilized by telomere-binding proteins. At the very end of the T-loop, the single-stranded telomere DNA is held onto a region of double-stranded DNA by the telomere strand disrupting the double-helical DNA and base pairing to one of the two strands. This triple-stranded structure is called a displacement loop or D-loop.\n\nIn DNA, fraying occurs when non-complementary regions exist at the end of an otherwise complementary double-strand of DNA. However, branched DNA can occur if a third strand of DNA is introduced and contains adjoining regions able to hybridize with the frayed regions of the pre-existing double-strand. Although the simplest example of branched DNA involves only three strands of DNA, complexes involving additional strands and multiple branches are also possible. Branched DNA can be used in nanotechnology to construct geometric shapes, see the section on uses in technology below.\n\nThe expression of genes is influenced by how the DNA is packaged in chromosomes, in a structure called chromatin. Base modifications can be involved in packaging, with regions that have low or no gene expression usually containing high levels of methylation of cytosine bases. DNA packaging and its influence on gene expression can also occur by covalent modifications of the histone protein core around which DNA is wrapped in the chromatin structure or else by remodeling carried out by chromatin remodeling complexes (see Chromatin remodeling). There is, further, crosstalk between DNA methylation and histone modification, so they can coordinately affect chromatin and gene expression.\n\nFor one example, cytosine methylation produces 5-methylcytosine, which is important for X-inactivation of chromosomes. The average level of methylation varies between organisms – the worm \"Caenorhabditis elegans\" lacks cytosine methylation, while vertebrates have higher levels, with up to 1% of their DNA containing 5-methylcytosine. Despite the importance of 5-methylcytosine, it can deaminate to leave a thymine base, so methylated cytosines are particularly prone to mutations. Other base modifications include adenine methylation in bacteria, the presence of 5-hydroxymethylcytosine in the brain, and the glycosylation of uracil to produce the \"J-base\" in kinetoplastids.\n\nDNA can be damaged by many sorts of mutagens, which change the DNA sequence. Mutagens include oxidizing agents, alkylating agents and also high-energy electromagnetic radiation such as ultraviolet light and X-rays. The type of DNA damage produced depends on the type of mutagen. For example, UV light can damage DNA by producing thymine dimers, which are cross-links between pyrimidine bases. On the other hand, oxidants such as free radicals or hydrogen peroxide produce multiple forms of damage, including base modifications, particularly of guanosine, and double-strand breaks. A typical human cell contains about 150,000 bases that have suffered oxidative damage. Of these oxidative lesions, the most dangerous are double-strand breaks, as these are difficult to repair and can produce point mutations, insertions, deletions from the DNA sequence, and chromosomal translocations. These mutations can cause cancer. Because of inherent limits in the DNA repair mechanisms, if humans lived long enough, they would all eventually develop cancer. DNA damages that are naturally occurring, due to normal cellular processes that produce reactive oxygen species, the hydrolytic activities of cellular water, etc., also occur frequently. Although most of these damages are repaired, in any cell some DNA damage may remain despite the action of repair processes. These remaining DNA damages accumulate with age in mammalian postmitotic tissues. This accumulation appears to be an important underlying cause of aging.\n\nMany mutagens fit into the space between two adjacent base pairs, this is called \"intercalation\". Most intercalators are aromatic and planar molecules; examples include ethidium bromide, acridines, daunomycin, and doxorubicin. For an intercalator to fit between base pairs, the bases must separate, distorting the DNA strands by unwinding of the double helix. This inhibits both transcription and DNA replication, causing toxicity and mutations. As a result, DNA intercalators may be carcinogens, and in the case of thalidomide, a teratogen. Others such as benzo[\"a\"]pyrene diol epoxide and aflatoxin form DNA adducts that induce errors in replication. Nevertheless, due to their ability to inhibit DNA transcription and replication, other similar toxins are also used in chemotherapy to inhibit rapidly growing cancer cells.\n\nDNA usually occurs as linear chromosomes in eukaryotes, and circular chromosomes in prokaryotes. The set of chromosomes in a cell makes up its genome; the human genome has approximately 3 billion base pairs of DNA arranged into 46 chromosomes. The information carried by DNA is held in the sequence of pieces of DNA called genes. Transmission of genetic information in genes is achieved via complementary base pairing. For example, in transcription, when a cell uses the information in a gene, the DNA sequence is copied into a complementary RNA sequence through the attraction between the DNA and the correct RNA nucleotides. Usually, this RNA copy is then used to make a matching protein sequence in a process called translation, which depends on the same interaction between RNA nucleotides. In alternative fashion, a cell may simply copy its genetic information in a process called DNA replication. The details of these functions are covered in other articles; here the focus is on the interactions between DNA and other molecules that mediate the function of the genome.\n\nGenomic DNA is tightly and orderly packed in the process called DNA condensation, to fit the small available volumes of the cell. In eukaryotes, DNA is located in the cell nucleus, with small amounts in mitochondria and chloroplasts. In prokaryotes, the DNA is held within an irregularly shaped body in the cytoplasm called the nucleoid. The genetic information in a genome is held within genes, and the complete set of this information in an organism is called its genotype. A gene is a unit of heredity and is a region of DNA that influences a particular characteristic in an organism. Genes contain an open reading frame that can be transcribed, and regulatory sequences such as promoters and enhancers, which control transcription of the open reading frame.\n\nIn many species, only a small fraction of the total sequence of the genome encodes protein. For example, only about 1.5% of the human genome consists of protein-coding exons, with over 50% of human DNA consisting of non-coding repetitive sequences. The reasons for the presence of so much noncoding DNA in eukaryotic genomes and the extraordinary differences in genome size, or \"C-value\", among species, represent a long-standing puzzle known as the \"C-value enigma\". However, some DNA sequences that do not code protein may still encode functional non-coding RNA molecules, which are involved in the regulation of gene expression.\n\nSome noncoding DNA sequences play structural roles in chromosomes. Telomeres and centromeres typically contain few genes but are important for the function and stability of chromosomes. An abundant form of noncoding DNA in humans are pseudogenes, which are copies of genes that have been disabled by mutation. These sequences are usually just molecular fossils, although they can occasionally serve as raw genetic material for the creation of new genes through the process of gene duplication and divergence.\n\nA gene is a sequence of DNA that contains genetic information and can influence the phenotype of an organism. Within a gene, the sequence of bases along a DNA strand defines a messenger RNA sequence, which then defines one or more protein sequences. The relationship between the nucleotide sequences of genes and the amino-acid sequences of proteins is determined by the rules of translation, known collectively as the genetic code. The genetic code consists of three-letter 'words' called \"codons\" formed from a sequence of three nucleotides (e.g. ACT, CAG, TTT).\n\nIn transcription, the codons of a gene are copied into messenger RNA by RNA polymerase. This RNA copy is then decoded by a ribosome that reads the RNA sequence by base-pairing the messenger RNA to transfer RNA, which carries amino acids. Since there are 4 bases in 3-letter combinations, there are 64 possible codons (4 combinations). These encode the twenty standard amino acids, giving most amino acids more than one possible codon. There are also three 'stop' or 'nonsense' codons signifying the end of the coding region; these are the TAA, TGA, and TAG codons.\n\nCell division is essential for an organism to grow, but, when a cell divides, it must replicate the DNA in its genome so that the two daughter cells have the same genetic information as their parent. The double-stranded structure of DNA provides a simple mechanism for DNA replication. Here, the two strands are separated and then each strand's complementary DNA sequence is recreated by an enzyme called DNA polymerase. This enzyme makes the complementary strand by finding the correct base through complementary base pairing and bonding it onto the original strand. As DNA polymerases can only extend a DNA strand in a 5′ to 3′ direction, different mechanisms are used to copy the antiparallel strands of the double helix. In this way, the base on the old strand dictates which base appears on the new strand, and the cell ends up with a perfect copy of its DNA.\n\nNaked extracellular DNA (eDNA), most of it released by cell death, is nearly ubiquitous in the environment. Its concentration in soil may be as high as 2 μg/L, and its concentration in natural aquatic environments may be as high at 88 μg/L. Various possible functions have been proposed for eDNA: it may be involved in horizontal gene transfer; it may provide nutrients; and it may act as a buffer to recruit or titrate ions or antibiotics. Extracellular DNA acts as a functional extracellular matrix component in the biofilms of several bacterial species. It may act as a recognition factor to regulate the attachment and dispersal of specific cell types in the biofilm; it may contribute to biofilm formation; and it may contribute to the biofilm's physical strength and resistance to biological stress.\n\nCell-free fetal DNA is found in the blood of the mother, and can be sequenced to determine a great deal of information about the developing fetus.\n\nAll the functions of DNA depend on interactions with proteins. These protein interactions can be non-specific, or the protein can bind specifically to a single DNA sequence. Enzymes can also bind to DNA and of these, the polymerases that copy the DNA base sequence in transcription and DNA replication are particularly important.\n\nStructural proteins that bind DNA are well-understood examples of non-specific DNA-protein interactions. Within chromosomes, DNA is held in complexes with structural proteins. These proteins organize the DNA into a compact structure called chromatin. In eukaryotes, this structure involves DNA binding to a complex of small basic proteins called histones, while in prokaryotes multiple types of proteins are involved. The histones form a disk-shaped complex called a nucleosome, which contains two complete turns of double-stranded DNA wrapped around its surface. These non-specific interactions are formed through basic residues in the histones, making ionic bonds to the acidic sugar-phosphate backbone of the DNA, and are thus largely independent of the base sequence. Chemical modifications of these basic amino acid residues include methylation, phosphorylation, and acetylation. These chemical changes alter the strength of the interaction between the DNA and the histones, making the DNA more or less accessible to transcription factors and changing the rate of transcription. Other non-specific DNA-binding proteins in chromatin include the high-mobility group proteins, which bind to bent or distorted DNA. These proteins are important in bending arrays of nucleosomes and arranging them into the larger structures that make up chromosomes.\n\nA distinct group of DNA-binding proteins is the DNA-binding proteins that specifically bind single-stranded DNA. In humans, replication protein A is the best-understood member of this family and is used in processes where the double helix is separated, including DNA replication, recombination, and DNA repair. These binding proteins seem to stabilize single-stranded DNA and protect it from forming stem-loops or being degraded by nucleases.\nIn contrast, other proteins have evolved to bind to particular DNA sequences. The most intensively studied of these are the various transcription factors, which are proteins that regulate transcription. Each transcription factor binds to one particular set of DNA sequences and activates or inhibits the transcription of genes that have these sequences close to their promoters. The transcription factors do this in two ways. Firstly, they can bind the RNA polymerase responsible for transcription, either directly or through other mediator proteins; this locates the polymerase at the promoter and allows it to begin transcription. Alternatively, transcription factors can bind enzymes that modify the histones at the promoter. This changes the accessibility of the DNA template to the polymerase.\n\nAs these DNA targets can occur throughout an organism's genome, changes in the activity of one type of transcription factor can affect thousands of genes. Consequently, these proteins are often the targets of the signal transduction processes that control responses to environmental changes or cellular differentiation and development. The specificity of these transcription factors' interactions with DNA come from the proteins making multiple contacts to the edges of the DNA bases, allowing them to \"read\" the DNA sequence. Most of these base-interactions are made in the major groove, where the bases are most accessible.\n\nNucleases are enzymes that cut DNA strands by catalyzing the hydrolysis of the phosphodiester bonds. Nucleases that hydrolyse nucleotides from the ends of DNA strands are called exonucleases, while endonucleases cut within strands. The most frequently used nucleases in molecular biology are the restriction endonucleases, which cut DNA at specific sequences. For instance, the EcoRV enzyme shown to the left recognizes the 6-base sequence 5′-GATATC-3′ and makes a cut at the horizontal line. In nature, these enzymes protect bacteria against phage infection by digesting the phage DNA when it enters the bacterial cell, acting as part of the restriction modification system. In technology, these sequence-specific nucleases are used in molecular cloning and DNA fingerprinting.\n\nEnzymes called DNA ligases can rejoin cut or broken DNA strands. Ligases are particularly important in lagging strand DNA replication, as they join together the short segments of DNA produced at the replication fork into a complete copy of the DNA template. They are also used in DNA repair and genetic recombination.\n\nTopoisomerases are enzymes with both nuclease and ligase activity. These proteins change the amount of supercoiling in DNA. Some of these enzymes work by cutting the DNA helix and allowing one section to rotate, thereby reducing its level of supercoiling; the enzyme then seals the DNA break. Other types of these enzymes are capable of cutting one DNA helix and then passing a second strand of DNA through this break, before rejoining the helix. Topoisomerases are required for many processes involving DNA, such as DNA replication and transcription.\n\nHelicases are proteins that are a type of molecular motor. They use the chemical energy in nucleoside triphosphates, predominantly adenosine triphosphate (ATP), to break hydrogen bonds between bases and unwind the DNA double helix into single strands. These enzymes are essential for most processes where enzymes need to access the DNA bases.\n\nPolymerases are enzymes that synthesize polynucleotide chains from nucleoside triphosphates. The sequence of their products is created based on existing polynucleotide chains—which are called \"templates\". These enzymes function by repeatedly adding a nucleotide to the 3′ hydroxyl group at the end of the growing polynucleotide chain. As a consequence, all polymerases work in a 5′ to 3′ direction. In the active site of these enzymes, the incoming nucleoside triphosphate base-pairs to the template: this allows polymerases to accurately synthesize the complementary strand of their template. Polymerases are classified according to the type of template that they use.\n\nIn DNA replication, DNA-dependent DNA polymerases make copies of DNA polynucleotide chains. To preserve biological information, it is essential that the sequence of bases in each copy are precisely complementary to the sequence of bases in the template strand. Many DNA polymerases have a proofreading activity. Here, the polymerase recognizes the occasional mistakes in the synthesis reaction by the lack of base pairing between the mismatched nucleotides. If a mismatch is detected, a 3′ to 5′ exonuclease activity is activated and the incorrect base removed. In most organisms, DNA polymerases function in a large complex called the replisome that contains multiple accessory subunits, such as the DNA clamp or helicases.\n\nRNA-dependent DNA polymerases are a specialized class of polymerases that copy the sequence of an RNA strand into DNA. They include reverse transcriptase, which is a viral enzyme involved in the infection of cells by retroviruses, and telomerase, which is required for the replication of telomeres. For example, HIV reverse transcriptase is an enzyme for AIDS virus replication. Telomerase is an unusual polymerase because it contains its own RNA template as part of its structure. It synthesizes telomeres at the ends of chromosomes. Telomeres prevent fusion of the ends of neighboring chromosomes and protect chromosome ends from damage.\n\nTranscription is carried out by a DNA-dependent RNA polymerase that copies the sequence of a DNA strand into RNA. To begin transcribing a gene, the RNA polymerase binds to a sequence of DNA called a promoter and separates the DNA strands. It then copies the gene sequence into a messenger RNA transcript until it reaches a region of DNA called the terminator, where it halts and detaches from the DNA. As with human DNA-dependent DNA polymerases, RNA polymerase II, the enzyme that transcribes most of the genes in the human genome, operates as part of a large protein complex with multiple regulatory and accessory subunits.\n\nA DNA helix usually does not interact with other segments of DNA, and in human cells, the different chromosomes even occupy separate areas in the nucleus called \"chromosome territories\". This physical separation of different chromosomes is important for the ability of DNA to function as a stable repository for information, as one of the few times chromosomes interact is in chromosomal crossover which occurs during sexual reproduction, when genetic recombination occurs. Chromosomal crossover is when two DNA helices break, swap a section and then rejoin.\n\nRecombination allows chromosomes to exchange genetic information and produces new combinations of genes, which increases the efficiency of natural selection and can be important in the rapid evolution of new proteins. Genetic recombination can also be involved in DNA repair, particularly in the cell's response to double-strand breaks.\n\nThe most common form of chromosomal crossover is homologous recombination, where the two chromosomes involved share very similar sequences. Non-homologous recombination can be damaging to cells, as it can produce chromosomal translocations and genetic abnormalities. The recombination reaction is catalyzed by enzymes known as recombinases, such as RAD51. The first step in recombination is a double-stranded break caused by either an endonuclease or damage to the DNA. A series of steps catalyzed in part by the recombinase then leads to joining of the two helices by at least one Holliday junction, in which a segment of a single strand in each helix is annealed to the complementary strand in the other helix. The Holliday junction is a tetrahedral junction structure that can be moved along the pair of chromosomes, swapping one strand for another. The recombination reaction is then halted by cleavage of the junction and re-ligation of the released DNA. Only strands of like polarity exchange DNA during recombination. There are two types of cleavage: east-west cleavage and north-south cleavage. The north-south cleavage nicks both strands of DNA, while the east-west cleavage has one strand of DNA intact. The formation of a Holliday junction during recombination makes it possible for genetic diversity, genes to exchange on chromosomes, and expression of wild-type viral genomes.\n\nDNA contains the genetic information that allows all modern living things to function, grow and reproduce. However, it is unclear how long in the 4-billion-year history of life DNA has performed this function, as it has been proposed that the earliest forms of life may have used RNA as their genetic material. RNA may have acted as the central part of early cell metabolism as it can both transmit genetic information and carry out catalysis as part of ribozymes. This ancient RNA world where nucleic acid would have been used for both catalysis and genetics may have influenced the evolution of the current genetic code based on four nucleotide bases. This would occur, since the number of different bases in such an organism is a trade-off between a small number of bases increasing replication accuracy and a large number of bases increasing the catalytic efficiency of ribozymes. However, there is no direct evidence of ancient genetic systems, as recovery of DNA from most fossils is impossible because DNA survives in the environment for less than one million years, and slowly degrades into short fragments in solution. Claims for older DNA have been made, most notably a report of the isolation of a viable bacterium from a salt crystal 250 million years old, but these claims are controversial.\n\nBuilding blocks of DNA (adenine, guanine, and related organic molecules) may have been formed extraterrestrially in outer space. Complex DNA and RNA organic compounds of life, including uracil, cytosine, and thymine, have also been formed in the laboratory under conditions mimicking those found in outer space, using starting chemicals, such as pyrimidine, found in meteorites. Pyrimidine, like polycyclic aromatic hydrocarbons (PAHs), the most carbon-rich chemical found in the universe, may have been formed in red giants or in interstellar cosmic dust and gas clouds.\n\nMethods have been developed to purify DNA from organisms, such as phenol-chloroform extraction, and to manipulate it in the laboratory, such as restriction digests and the polymerase chain reaction. Modern biology and biochemistry make intensive use of these techniques in recombinant DNA technology. Recombinant DNA is a man-made DNA sequence that has been assembled from other DNA sequences. They can be transformed into organisms in the form of plasmids or in the appropriate format, by using a viral vector. The genetically modified organisms produced can be used to produce products such as recombinant proteins, used in medical research, or be grown in agriculture.\n\nForensic scientists can use DNA in blood, semen, skin, saliva or hair found at a crime scene to identify a matching DNA of an individual, such as a perpetrator. This process is formally termed DNA profiling, but may also be called \"genetic fingerprinting\". In DNA profiling, the lengths of variable sections of repetitive DNA, such as short tandem repeats and minisatellites, are compared between people. This method is usually an extremely reliable technique for identifying a matching DNA. However, identification can be complicated if the scene is contaminated with DNA from several people. DNA profiling was developed in 1984 by British geneticist Sir Alec Jeffreys, and first used in forensic science to convict Colin Pitchfork in the 1988 Enderby murders case.\n\nThe development of forensic science and the ability to now obtain genetic matching on minute samples of blood, skin, saliva, or hair has led to re-examining many cases. Evidence can now be uncovered that was scientifically impossible at the time of the original examination. Combined with the removal of the double jeopardy law in some places, this can allow cases to be reopened where prior trials have failed to produce sufficient evidence to convince a jury. People charged with serious crimes may be required to provide a sample of DNA for matching purposes. The most obvious defense to DNA matches obtained forensically is to claim that cross-contamination of evidence has occurred. This has resulted in meticulous strict handling procedures with new cases of serious crime.\nDNA profiling is also used successfully to positively identify victims of mass casualty incidents, bodies or body parts in serious accidents, and individual victims in mass war graves, via matching to family members.\n\nDNA profiling is also used in DNA paternity testing to determine if someone is the biological parent or grandparent of a child with the probability of parentage is typically 99.99% when the alleged parent is biologically related to the child. Normal DNA sequencing methods happen after birth, but there are new methods to test paternity while a mother is still pregnant.\n\nDeoxyribozymes, also called DNAzymes or catalytic DNA, are first discovered in 1994. They are mostly single stranded DNA sequences isolated from a large pool of random DNA sequences through a combinatorial approach called in vitro selection or systematic evolution of ligands by exponential enrichment (SELEX). DNAzymes catalyze variety of chemical reactions including RNA-DNA cleavage, RNA-DNA ligation, amino acids phosphorylation-dephosphorylation, carbon-carbon bond formation, and etc. DNAzymes can enhance catalytic rate of chemical reactions up to 100,000,000,000-fold over the uncatalyzed reaction. The most extensively studied class of DNAzymes is RNA-cleaving types which have been used to detect different metal ions and designing therapeutic agents. Several metal-specific DNAzymes have been reported including the GR-5 DNAzyme (lead-specific), the CA1-3 DNAzymes (copper-specific), the 39E DNAzyme (uranyl-specific) and the NaA43 DNAzyme (sodium-specific). The NaA43 DNAzyme, which is reported to be more than 10,000-fold selective for sodium over other metal ions, was used to make a real-time sodium sensor in living cells.\n\nBioinformatics involves the development of techniques to store, data mine, search and manipulate biological data, including DNA nucleic acid sequence data. These have led to widely applied advances in computer science, especially string searching algorithms, machine learning, and database theory. String searching or matching algorithms, which find an occurrence of a sequence of letters inside a larger sequence of letters, were developed to search for specific sequences of nucleotides. The DNA sequence may be aligned with other DNA sequences to identify homologous sequences and locate the specific mutations that make them distinct. These techniques, especially multiple sequence alignment, are used in studying phylogenetic relationships and protein function. Data sets representing entire genomes' worth of DNA sequences, such as those produced by the Human Genome Project, are difficult to use without the annotations that identify the locations of genes and regulatory elements on each chromosome. Regions of DNA sequence that have the characteristic patterns associated with protein- or RNA-coding genes can be identified by gene finding algorithms, which allow researchers to predict the presence of particular gene products and their possible functions in an organism even before they have been isolated experimentally. Entire genomes may also be compared, which can shed light on the evolutionary history of particular organism and permit the examination of complex evolutionary events.\n\nDNA nanotechnology uses the unique molecular recognition properties of DNA and other nucleic acids to create self-assembling branched DNA complexes with useful properties. DNA is thus used as a structural material rather than as a carrier of biological information. This has led to the creation of two-dimensional periodic lattices (both tile-based and using the \"DNA origami\" method) and three-dimensional structures in the shapes of polyhedra. Nanomechanical devices and algorithmic self-assembly have also been demonstrated, and these DNA structures have been used to template the arrangement of other molecules such as gold nanoparticles and streptavidin proteins.\n\nBecause DNA collects mutations over time, which are then inherited, it contains historical information, and, by comparing DNA sequences, geneticists can infer the evolutionary history of organisms, their phylogeny. This field of phylogenetics is a powerful tool in evolutionary biology. If DNA sequences within a species are compared, population geneticists can learn the history of particular populations. This can be used in studies ranging from ecological genetics to anthropology; For example, DNA evidence is being used to try to identify the Ten Lost Tribes of Israel.\n\nIn a paper published in \"Nature\" in January 2013, scientists from the European Bioinformatics Institute and Agilent Technologies proposed a mechanism to use DNA's ability to code information as a means of digital data storage. The group was able to encode 739 kilobytes of data into DNA code, synthesize the actual DNA, then sequence the DNA and decode the information back to its original form, with a reported 100% accuracy. The encoded information consisted of text files and audio files. A prior experiment was published in August 2012. It was conducted by researchers at Harvard University, where the text of a 54,000-word book was encoded in DNA.\n\nMoreover, in living cells, the storage can be turned active by enzymes. Light-gated protein domains fused to DNA processing enzymes are suitable for that task \"in vitro\". Fluorescent exonucleases can transmit the output according to the nucleotide they have read.\n\nDNA was first isolated by the Swiss physician Friedrich Miescher who, in 1869, discovered a microscopic substance in the pus of discarded surgical bandages. As it resided in the nuclei of cells, he called it \"nuclein\". In 1878, Albrecht Kossel isolated the non-protein component of \"nuclein\", nucleic acid, and later isolated its five primary nucleobases. In 1919, Phoebus Levene identified the base, sugar, and phosphate nucleotide unit. Levene suggested that DNA consisted of a string of nucleotide units linked together through the phosphate groups. Levene thought the chain was short and the bases repeated in a fixed order. In 1937, William Astbury produced the first X-ray diffraction patterns that showed that DNA had a regular structure.\n\nIn 1927, Nikolai Koltsov proposed that inherited traits would be inherited via a \"giant hereditary molecule\" made up of \"two mirror strands that would replicate in a semi-conservative fashion using each strand as a template\". In 1928, Frederick Griffith in his experiment discovered that traits of the \"smooth\" form of \"Pneumococcus\" could be transferred to the \"rough\" form of the same bacteria by mixing killed \"smooth\" bacteria with the live \"rough\" form. This system provided the first clear suggestion that DNA carries genetic information—the Avery–MacLeod–McCarty experiment—when Oswald Avery, along with coworkers Colin MacLeod and Maclyn McCarty, identified DNA as the transforming principle in 1943. DNA's role in heredity was confirmed in 1952 when Alfred Hershey and Martha Chase in the Hershey–Chase experiment showed that DNA is the genetic material of the T2 phage.\n\nLate in 1951, Francis Crick started working with James Watson at the Cavendish Laboratory within the University of Cambridge. In 1953, Watson and Crick suggested what is now accepted as the first correct double-helix model of DNA structure in the journal \"Nature\". Their double-helix, molecular model of DNA was then based on one X-ray diffraction image (labeled as \"Photo 51\") taken by Rosalind Franklin and Raymond Gosling in May 1952, and the information that the DNA bases are paired. On 28 February 1953 Crick interrupted patrons' lunchtime at The Eagle pub in Cambridge to announce that he and Watson had \"discovered the secret of life\".\n\nExperimental evidence supporting the Watson and Crick model was published in a series of five articles in the same issue of \"Nature\". Of these, Franklin and Gosling's paper was the first publication of their own X-ray diffraction data and original analysis method that partly supported the Watson and Crick model; this issue also contained an article on DNA structure by Maurice Wilkins and two of his colleagues, whose analysis and \"in vivo\" B-DNA X-ray patterns also supported the presence \"in vivo\" of the double-helical DNA configurations as proposed by Crick and Watson for their double-helix molecular model of DNA in the prior two pages of \"Nature\". In 1962, after Franklin's death, Watson, Crick, and Wilkins jointly received the Nobel Prize in Physiology or Medicine. Nobel Prizes are awarded only to living recipients. A debate continues about who should receive credit for the discovery.\n\nIn an influential presentation in 1957, Crick laid out the central dogma of molecular biology, which foretold the relationship between DNA, RNA, and proteins, and articulated the \"adaptor hypothesis\". Final confirmation of the replication mechanism that was implied by the double-helical structure followed in 1958 through the Meselson–Stahl experiment. Further work by Crick and coworkers showed that the genetic code was based on non-overlapping triplets of bases, called codons, allowing Har Gobind Khorana, Robert W. Holley, and Marshall Warren Nirenberg to decipher the genetic code. These findings represent the birth of molecular biology.\n\n", "id": "7955", "title": "DNA"}
{"url": "https://en.wikipedia.org/wiki?curid=26422437", "text": "Genetic structure\n\nGenetic structure refers to any pattern in the genetic makeup of individuals within a population.\n\nGenetic structure allows for information about an individual to be inferred from other members of the same population. In trivial terms, all populations have genetic structure, because all populations can be characterised by their genotype or allele frequencies: if only 1% of a large sample of moths drawn from a single population have spotted wings, then it is safe to assume that any unknown individual is unlikely to have spotted wings.\n\nA more complicated example arises in dense thickets of plants, where plants tend to be pollinated by near neighbours, and seeds tend to fall and germinate near the maternal plant. In such a scenario, plants tend to be more closely related to nearby plants than they are to distant plants; and yet they are more likely to breed with nearby plants than they are with distant plants. Thus an inbreeding cycle is created that perpetuates the pattern of plants being closely related to near neighbors. This is a form of genetic structure because one can infer much about the genetic makeup of any individual plant simply by studying plants in their immediate neighborhoods.\n", "id": "26422437", "title": "Genetic structure"}
{"url": "https://en.wikipedia.org/wiki?curid=1969807", "text": "Polygene\n\nA \"polygene” or \"multiple gene inheritance\" is a member of a group of non-epistatic genes that interact additively to influence a phenotypic trait. The term \"heterozygous\" is usually used to refer to a hypothetical gene as it is often difficult to characterise the effect of an individual gene from the effects of other genes and the environment on a particular phenotype. Advances in statistical methodology and high throughput sequencing are, however, allowing researchers to locate candidate genes for the trait. In the case that such a gene is identified, it is referred to as a quantitative trait locus (QTL). These genes are generally pleiotropic as well. The genes that contribute to type 2 diabetes are thought to be mostly polygenes. In July 2016, scientists reported identifying a set of 355 genes from the last universal common ancestor (LUCA) of all organisms living on Earth.\n\nTraits with polygenic determinism correspond to the classical quantitative characters, as opposed to the qualitative characters with monogenic or oligogenic determinism. In essence instead of two options, such as freckles or no freckles, there are many variations. Like the color of skin, hair, or even eyes.\n\nPolygenic locus is any individual locus which is included in the system of genes responsible for the genetic component of variation in a quantitative (polygenic) character. Allelic substitutions contribute to the variance in a specified quantitative character. Polygenic locus may be either a single or complex genetic locus in the conventional sense, i.e., either a single gene or closely linked block of functionally related genes.\n\nIn modern sense, the inheritance mode of polygenic patterns is called polygenic inheritance, whose main properties may be summarized as follows:\n\nPolygenic inheritance occurs when one characteristic is controlled by two or more genes. Often the genes are large in quantity but small in effect. Examples of human polygenic inheritance are height, skin color, eye color and weight. Polygenes exist in other organisms, as well. \"Drosophila\", for instance, display polygeny with traits such as wing morphology, bristle count (20170808 dead link) and many others.\n\nThe frequency of the phenotypes of these traits generally follows a normal continuous variation distribution pattern. This results from the many possible allelic combinations. When the values are plotted, a bell-shaped curve is obtained. The mode of the distribution represents the optimal, or fittest, phenotype. The more genes are involved, the smoother the estimated curve. However, in this model all genes must code for alleles with additive effects. This assumption is often unrealistic as many genes display epistasis effects which can have unpredictable effects on the distribution of outcomes, especially when looking at the distribution on a fine scale.\n\nTraditionally, mapping polygenes requires statistical tools available to help measure the effects of polygenes as well as narrow in on single genes. One of these tools is QTL-mapping. QTL-mapping utilizes a phenomenon known as linkage disequilibrium by comparing known marker genes with correlated phenotypes. Often, researchers will find a large region of DNA, called a locus, that accounts for a significant amount of the variation observed in the measured trait. This locus will usually contain a large number of genes that are responsible. A new form of QTL has been described as expression QTL (eQTL). eQTLs regulate the amount of expressed mRNA, which in turn regulates the amount of protein within the organism.\n\nAnother interest of statistical geneticists using QTL mapping is to determine the complexity of the genetic architecture underlying a phenotypic trait. For example, they may be interested in knowing whether a phenotype is shaped by many independent loci, or by a few loci, and do those loci interact. This can provide information on how the phenotype may be evolving.\n\n\n(de)Polygenie\n", "id": "1969807", "title": "Polygene"}
{"url": "https://en.wikipedia.org/wiki?curid=47685547", "text": "Genetic variance\n\nGenetic variance is a concept outlined by the English biologist and statistician Ronald Fisher in his Fisher's fundamental theorem of natural selection which he outlined in his 1930 book \"The Genetical Theory of Natural Selection\" which postulates that the rate of change of biological fitness can be calculated by the genetic variance of the fitness itself. Fisher tried to give a statistical formula about how the change of fitness in a population can be attributed to changes in the allele frequency. Fisher made no restrictive assumptions in his formula concerning fitness parameters, mate choices or the number of alleles and loci involved.\n\nPhenotypic variance, usually combines the genotype variance with the environmental variance. Genetic variance has three major components: the additive genetic variance, dominance variance, and epistatic variance.\n\nAdditive genetic variance involves the inheritance of a particular allele from your parent and this allele's independent effect on the specific phenotype, which will cause the phenotype deviation from the mean phenotype. Dominance genetic variance refers to the phenotype deviation caused by the interactions between alternative alleles that control one trait at one specific locus. Epistatic variance involves an interaction between different alleles in different loci.\n\nHeritability refers to how much of the phenotypic variance is due to variance in genetic factors. Usually after we know the total amount of genetic variance that is responsible for a trait, we can calculate the trait heritability. Heritability can be used as an important predictor to evaluate if a population can respond to artificial or natural selection.\n\nBroad-sense heritability, H = V/V, Involves the proportion of phenotypic variation due to the effects of dominance and epistasis variance. Narrow-sense heritability, h = V/V, refers to the proportion of phenotypic variation that is due to additive genetic values (V).\n\nThe phenotypic variance (V) in a population is influenced by genetic variance (V) and environmental sources (V)\n\nV = V + V\n\nThe total amount of genetic variance can be divided into several groups, including additive variance (V), dominance variance (V), and epistatic variance (V).\n\nV = V + V + V\n\n1.Traditionally, using pedigree data in humans, plants, and livestock species to estimate additive genetic variance.\n\n2. Using a single-nucleotide polymorphisms (SNP) regression method to quantify the contribution of additive, dominance, and imprinting variance to the total genetic variance\n\n3.Genetic variance–covariance (G ) matrices conveniently summarize the genetic relationships among a suite of traits and are a central parameter in the determination of the multivariate response to selection.\n\n1.The distribution of genetic variance across phenotypic space and the response to selection.\n\nUnderstand how empirical spectral distribution of G predicts the response to selection across phenotypic space. In particular, trait combinations that form a nearly null genetic subspace with little genetic variance respond only inconsistently to selection. They set out a framework for understanding how the empirical spectral distribution of G may differ from the random expectations that have been developed under random matrix theory (RMT). Using a data set containing a large number of gene expression traits.\n\n2.Comparing estimates of genetic variance across different relationship models.\n\nIn this research, the researchers use the different relationship models to compare estimates of genetic variance components and the heritability. However, different models may give different estimates of genetic variances. They found that expected genetic variances usually equals the estimated variance times a statistic, Dk, and for the most typical models of relationships, Dk is close to 1, which means most of these models can be used to estimate the genetic variance.\n\n3.Estimation of Additive, Dominance, and Imprinting Genetic Variance Using Genomic Data\n\nThe development of single-nucleotide polymorphisms (SNPs) mapping helps to explore the genetic variation of complex traits at individual loci. Researchers can quantify the contribution of additive, dominance, and imprinting variance to the total genetic variance by using a SNP regression method.\n\n", "id": "47685547", "title": "Genetic variance"}
{"url": "https://en.wikipedia.org/wiki?curid=47157896", "text": "Genetics of infertility\n\nAbout 10–15% of human couples are infertile, unable to conceive. In approximately in half of these cases, the underlying cause is related to the male. The underlying causative factors in the male infertility can be attributed to environmental toxins, systemic disorders such as, hypothalamic–pituitary disease, testicular cancers and germ-cell aplasia. Genetic factors including aneuploidies and single-gene mutations are also contributed to the male infertility. Patients suffering from nonobstructive azoospermia or oligozoospermia show microdeletions in the long arm of the Y chromosome and/or chromosomal abnormalities, each with the respective frequency of 9.7% and 13%. A large percentage of human male infertility is estimated to be caused by mutations in genes involved in primary or secondary spermatogenesis and sperm quality and function. Single-gene defects are the focus of most research carried out in this field.\n\nNR5A1 mutations are associated with male infertility, suggesting the possibility that these mutations cause the infertility. However, it is possible that these mutations individually have no major effect and only contribute to the male infertility by collaboration with other contributors such as environmental factors and other genomics variants. Vice versa, existence of the other alleles could reduce the phenotypic effects of impaired NR5A1 proteins and attenuate the expression of abnormal phenotypes and manifest male infertility solely.\n\nNuclear receptor subfamily 5 group A member 1 (NR5A1), also known as SF1 or Ad4BP (MIM 184757), is located on the long arm of chromosome 9 (9q33.3). The NR5A1 is an orphan nuclear receptor that was first identified following the search for a common regulator of the cytochrome P450 steroid hydroxylase enzyme family. This receptor is a pivotal transcriptional regulator of an array of genes involved in reproduction, steroidogenesis and male sexual differentiation and also plays a crucial role in adrenal gland formation in both sexes. NR5A1 regulates the mullerian inhibitory substance by binding to a conserved upstream regulatory element and directly participates in the process of mammalian sex determination through mullerian duct regression. Targeted disruption of NR5A1 (Ftzf1) in mice results in gonadal and adrenal agenesis, persistence of Mullerian structures and abnormalities of the hypothalamus and pituitary gonadotropes. Heterozygous animals demonstrate a milder phenotype including an impaired adrenal stress response and reduced testicular size. In humans, NR5A1 mutations were first described in patients with 46, XY karyotype and disorders of sex development (DSD), Mullerian structures and primary adrenal failure (MIM 612965). After that, heterozygous NR5A1 mutations were described in seven patients showing 46, XY karyotype and ambiguous genitalia, gonadal dysgenesis, but no adrenal insufficiency. Since then, studies have confirmed that mutations in NR5A1 in patients with 46, XY karyotype cause severe underandrogenisation, but no adrenal insufficiency, establishing dynamic and dosage-dependent actions for NR5A1. Subsequent studies revealed that NR5A1 heterozygous mutations cause primary ovarian insufficiency (MIM 612964).\n\nRecently, NR5A1 mutations have been related to human male infertility (MIM 613957). These findings substantially increase the number of NR5A1 mutations reported in humans and show that mutations in NR5A1 can be found in patients with a wide range of phenotypic features, ranging from 46,XY sex reversal with primary adrenal failure to male infertility. For the first time, Bashamboo et al. (2010) conducted a study on the nonobstructive infertile men (a non-Caucasian mixed ancestry n = 315), which resulted in the report of all missense mutations in the NR5A1 gene with 4% frequency. Functional studies of the missense mutations revealed impaired transcriptional activation of NR5A1-responsive target genes. Subsequently, three missense mutations were identified as associated with and most likely the cause of the male infertility, according to computational analyses. The study indicated that the mutation frequency is below 1% (Caucasian German origin, n = 488). In another study the coding sequence of NR5A1 has been analysed in a cohort of 90 well-characterised idiopathic Iranian azoospermic infertile men versus 112 fertile men. Heterozygous NR5A1 mutations were found in 2 of 90 (2.2%) of cases. These two patients harboured missense mutations within the hinge region (p.P97T) and ligand-binding domain (p.E237K) of the NR5A1 protein.\n", "id": "47157896", "title": "Genetics of infertility"}
{"url": "https://en.wikipedia.org/wiki?curid=47190810", "text": "Human Longevity Inc.\n\nHuman Longevity is a San Diego-based venture launched by Craig Venter and Peter Diamandis in 2013. Its goal is to build the world's most comprehensive database on human genotypes and phenotypes, and then subject it to machine learning so that it can help develop new ways to fight diseases associated with aging. The company received in investments in its Series A offering in summer 2014 and announced a further $220 million Series B investment offering in April 2016. It has made deals with drug companies Celgene and AstraZeneca to collaborate in its research.\n\nWhile it is conducting research, the company is offering a wellness service known as \"Health Nucleus,\" which offers customers a range of medical tests such as a full genome sequencing and tests for early indications of cancers, Alzheimer's and heart disease. This testing is meant to help people catch diseases earlier than otherwise possible and to identify risk factors for diseases later in life. \n\n", "id": "47190810", "title": "Human Longevity Inc."}
{"url": "https://en.wikipedia.org/wiki?curid=39346603", "text": "Xeno nucleic acid\n\nXeno nucleic acid (XNA) is a synthetic alternative to the natural nucleic acids DNA and RNA as information-storing biopolymers that differs in the sugar backbone. As of 2011, at least six types of synthetic sugars have been shown to form nucleic acid backbones that can store and retrieve genetic information. Research is now being done to create synthetic polymerases to transform XNA. The study of its production and application has created a field known as xenobiology.\n\nAlthough the genetic information is still stored in the four canonical base pairs (unlike other nucleic acid analogues), natural DNA polymerases cannot read and duplicate this information. Thus the genetic information stored in XNA is “invisible” and therefore useless to natural DNA-based organisms.\n\nThe structure of the DNA was discovered in 1953 and many scientists assumed that our understandings for the chemical basis of life was perfect. However, around the early 2000s, researchers were able to create a number of exotic DNA-like structures, XNA. XNA is a synthetic polymer that can carry the same information as DNA, but with different molecular constituents. The “X” in XNA stands for “xeno,” meaning stranger or alien, named by scientists to indicate the difference in the molecular structure of XNA when compared to DNA or RNA. Not much was done with XNA until the development of special polymerase Enzyme, capable of copying XNA from a DNA template as well as copying XNA back into DNA. More recently, synthetic biologists Philipp Holliger and Alexander Taylor, both from the University of Cambridge, managed to create XNAzymes, the XNA equivalent of a Ribozyme, enzymes made of DNA or ribonucleic acid; This demonstrates that XNAs not only store hereditary information, but can also serve as enzymes, raising the possibility that life elsewhere could have begun with something other than RNA or DNA.\n\nStrands of DNA and RNA are formed by stringing together long chains of molecules called nucleotides. A nucleotide is made up of three chemical components: a phosphate, a five-carbon sugar group (this can be either a deoxyribose sugar — which gives us the \"D\" in DNA — or a ribose sugar — the \"R\" in RNA), and one of five standard bases (adenine, guanine, cytosine, thymine or uracil).\n\nThe molecules that piece together to form the six xeno nucleic acids are almost identical to those of DNA and RNA, with one exception: in XNA nucleotides, the deoxyribose and ribose sugar groups of DNA and RNA have been replaced. Some of these replacement molecules contain four carbons atoms instead of the standard five. Others cram in as many as seven carbons. FANA even contains a fluorine atom. These substitutions make XNAs functionally and structurally analogous to DNA and RNA, but they also make them unnatural and artificial.\n\nXNA exhibits a variety of structural chemical changes relative to its natural counterparts. Types of synthetic 'XNA' created so far include 1,5-anhydrohexitol nucleic acid (HNA) and cyclohexene nucleic acid (CeNA). Threose nucleic acid (TNA), glycol nucleic acid (GNA), locked nucleic acid (LNA), and peptide nucleic acid (PNA) are also other currently known XNAs that have been made and have more background information currently available than the previous two mentioned. HNA could be used to potentially act as a drug that can recognize and bind to specified sequences. Scientists have been able to isolate HNAs for the possible binding of sequences that target HIV. With cyclohexene nucleic acid, research has shown that CeNAs with stereochemistry similar to the D form can create stable duplexes with itself and RNA. It was shown that CeNAs are not as stable when they form duplexes with DNA.\n\nThe study of XNA is intended not to give scientists a better understanding of biological evolution as it has occurred historically, but rather to explore ways in which we can control and even reprogram the genetic makeup of biological organisms moving forward. XNA has shown significant potential in solving the current issue of genetic pollution in genetically modified organisms. While DNA is incredibly efficient in its ability to store genetic information and lend complex biological diversity, its four-letter genetic alphabet is relatively limited. Using a genetic code of six XNAs rather than the four naturally occurring DNA nucleotide bases yields endless opportunities for genetic modification and expansion of chemical functionality.\n\nThe development of various hypotheses and theories about XNAs have altered a key factor in our current understanding of nucleic acids: that heredity and evolution are not limited to DNA and RNA as once thought, but are simply processes that have developed from polymers capable of storing information. Investigations into XNAs will allow for researchers to assess whether DNA and RNA are the most efficient and desirable building blocks of life, or if these two molecules were chosen randomly after evolving from a larger class of chemical ancestors.\n\nOne theory of XNA utilization is its incorporation into medicine as a disease-fighting agent. Some enzymes and antibodies that are currently administered for various disease treatments are broken down too quickly in the stomach or bloodstream. Because XNA is foreign and because it is believed that humans have not yet evolved the enzymes to break them down, XNAs may be able to serve as a more durable counterpart to the DNA and RNA-based treatment methodologies that are currently in use.\n\nExperiments with XNA have already allowed for the replacement and enlargement of this genetic alphabet, and XNAs have shown complementarity with DNA and RNA nucleotides, suggesting potential for its transcription and recombination. One experiment conducted at the University of Florida led to the production of an XNA aptamer by the AEGIS-SELEX (artificially expanded genetic information system - systematic evolution of ligands by exponential enrichment) method, followed by successful binding to a line of breast cancer cells. Furthermore, experiments in the model bacterium \"E. coli\" have demonstrated the ability for XNA to serve as a biological template for DNA \"in vivo.\"\n\nIn moving forward with genetic research on XNAs, various questions must come into consideration regarding biosafety, biosecurity, ethics, and governance/regulation. One of the key questions here is whether XNA in an \"in vivo\" setting would intermix with DNA and RNA in its natural environment, thereby rendering scientists unable to control or predict its implications in genetic mutation.\n\nXNA also has potential applications to be used as catalysts, much like RNA has the ability to be used as an enzyme. Researchers have shown XNA is able to cleave and ligate DNA, RNA and other XNA sequences, with the most activity being XNA catalyzed reactions on XNA molecules. This research may be used in determining whether DNA and RNA's role in life emerged through natural selection processes or if it was simply a coincidental occurrence.\n\n", "id": "39346603", "title": "Xeno nucleic acid"}
{"url": "https://en.wikipedia.org/wiki?curid=18487326", "text": "Germline mosaicism\n\nGermline mosaicism, also called gonadal mosaicism, is a type of genetic mosaicism where more than one set of genetic information is found specifically within the gamete cells. Somatic mosaicism, a type of genetic mosaicism found in somatic cells, and germline mosaicism can be present at the same time or individually depending on when the conditions occur. When the mosaicism is only found in the gametes and not in any somatic cells, it is referred to as pure germline mosaicism. Germline mosaicism can be caused either by a mutation that occurs after conception, or by epigenetic regulation, alterations to DNA such as methylation that do not involve changes in the DNA coding sequence.\n\nA mutation in an allele acquired by a somatic cell early in its development can be passed on to its daughter cells, including those that later specialize to gametes. It can also be caused by a sporadic mutation in a gamete cell. If the germline mosaicism causing mutation occurs in the somatic cell, it never results in pure germline mosaicism because it will be present in somatic cells as well.\n\nGenetic mutations, including those that cause disease, can be carried in gamete cells and passed on to offspring even if the mutation is not present (expressed) in the parents' phenotype. Diseases caused by germline mosaicism can be difficult to diagnose as genetically-inherited because the mutant alleles are not likely be present in the somatic cells. Somatic cells are more commonly used for genetic analysis because they are easier to obtain than gametes. If the disease is a result of pure germline mosaicism, then the disease causing mutant allele would never be present in the somatic cells. This is a source of uncertainty for genetic counselling. An individual may still be a carrier for a certain disease even if the disease causing mutant allele is not present in the cells that were analyzed because the causative mutation could still exist in some of the individual's gametes.\n\nGermline mosaicism may contribute to the inheritance of many genetic conditions. Conditions that are inherited by means of germline mosaicism are often mistaken as being the result of de novo mutations. Various diseases are now being re-examined for presence of mutant alleles in the germline of the parents in order to further our understanding of how they can be passed-on. The frequency of germline mosacism is not known due to the sporadic nature of the mutations causing it and the difficulty in obtaining the gametes that must be tested to diagnose it.\n\nThe recurrence rate of conditions caused by germline mosaicism varies greatly between subjects. Recurrence is proportional to the number of gamete cells that carry the particular mutation with the condition. If the mutation occurred earlier on in the development of the gamete cells, then the recurrence rate would be higher because a greater number of cells would carry the mutant allele.\n", "id": "18487326", "title": "Germline mosaicism"}
{"url": "https://en.wikipedia.org/wiki?curid=3648947", "text": "Gene map\n\nGene maps help describe the spatial arrangement of genes on a chromosome. Genes are designated to a specific location on a chromosome known as the locus and can be used as molecular markers to find the distance between other genes on a chromosome. Maps provide researchers with the opportunity to predict the inheritance patterns of specific traits, which can eventually lead to a better understanding of disease-linked traits.\n\nThe genetic basis to gene maps is to provide an outline that can potentially help researchers carry out DNA sequencing. A gene map helps point out the relative positions of genes and allows researchers to locate regions of interest in the genome. Genes can then be identified quickly and sequenced quickly.\n\nTwo approaches to generating gene maps include physical mapping and genetic mapping. Physical mapping utilizes molecular biology techniques to inspect chromosomes. These techniques consequently allow researchers to observe chromosomes directly so that a map may be constructed with relative gene positions. Genetic mapping on the other hand uses genetic techniques to indirectly find association between genes. Techniques can include cross-breeding (see Hybrid (biology)) experiments and examining pedigrees. These technique allow for maps to be constructed so that relative positions of genes and other important sequences can be analyzed.\n\nPhysical mapping techniques used to generate a gene map include: Restriction mapping, Fluorescent \"in situ\" hybridization (FISH), and Sequence tagged site (STS) mapping.\n\nRestriction mapping is a method in which structural information regarding a segment of DNA is obtained using restriction enzymes. Restriction enzymes are enzymes that help cut segments of DNA at specific recognition sequences. The basis to restriction mapping involves digesting (or cutting) DNA with restriction enzymes. The digested DNA fragments are then run on an agarose gel using electrophoresis, which provides one with information regarding the size of these digested fragments. The sizes of these fragments help indicate the distance between restriction enzyme sites on the DNA analyzed, and provides researchers with information regarding the structure of DNA analyzed.\n\nFISH is a method used to detect the presence (or absence) of a DNA sequence within a cell. DNA probes that are specific for chromosomal regions or genes of interest are labeled with fluorochromes. By attaching fluorochromes to probes, researchers are able to visualize multiple DNA sequences simultaneously. When a probe comes into contact with DNA on a specific chromosome, hybridization will occur. Consequently, information regarding the location of that sequence of DNA will be attained. FISH analyzes single stranded DNA (ssDNA). Once the DNA is in its single stranded state, the DNA can bind to its specific probe.\n\nA sequence tagged site (STS) is a short sequence of DNA (about 100 - 500 base pairs in length) that is seen to appear multiple times within an individual's genome. These sites are easily recognizable, usually appearing at least once in the DNA being analyzed. These sites usually contain genetic polymorphisms making them sources of viable genetic markers (as they differ from other sequences). Sequenced tagged sites can be mapped within our genome and require a group of overlapping DNA fragments. PCR is generally used to produce the collection of DNA fragments. After overlapping fragments are created, the map distance between STSs can be analyzed. In order to calculate the map distance between STSs, researchers determine the frequency at which breaks between the two markers occur (see shotgun sequencing)\n\nGenetic mapping is focused on the principles first established by Gregor Mendel. This approach primarily focuses on linkage analysis and gene association techniques.\n\nThe basis to linkage analysis is understanding chromosomal location and identifying disease genes. Certain genes that are linked or associated with each other are found to reside close to each other on the same chromosome. During meiosis, these genes are capable of being inherited together and can be used as a genetic marker to help identify the phenotype of diseases. Because linkage analysis can identify inheritance patterns, these studies are usually family based.\n\nGene association analysis is population based; it is not focused on inheritance patterns, but rather is based on the entire history of a population. Gene association analysis looks at a particular population and tries to identify whether the frequency of an allele in affected individuals is different from that of a control set of unaffected individuals of the same population. This method is particularly useful to identify complex diseases that do not have a Mendelian inheritance pattern.\n\nUsing the methods mentioned above, researchers are capable of mapping disease genes. Generating a gene map is the critical first step towards identifying disease genes. Gene maps allow for variant alleles to be identified and allow for researchers to make predictions about the genes they think are causing the mutant phenotype. An example of a disorder that was identified by Linkage analysis is Cystic Fibrosis. For example, with Cystic Fibrosis (CF), DNA samples from fifty families affected by CF were analyzed using linkage analysis. Hundreds of markers pertaining to CF were analyzed throughout the genome until CF was identified on the long arm of chromosome 7. Researchers then had completed linkage analysis on additional DNA markers within chromosome 7 to identify an even more precise location of the CF gene. They found that the CF gene resides around 7q31-q32 (see chromosomal nomenclature).\n\n", "id": "3648947", "title": "Gene map"}
{"url": "https://en.wikipedia.org/wiki?curid=9951136", "text": "Exonic splicing silencer\n\nAn exonic splicing silencer (ESS) is a short region (usually 4-18 nucleotides) of an exon and is a cis-regulatory element. A set of 103 hexanucleotides known as FAS-hex3 has been shown to be abundant in ESS regions. ESSs inhibit or silence splicing of the pre-mRNA and contribute to constitutive and alternate splicing. To elicit the silencing affect, ESSs recruit proteins that will negatively affect the core splicing machinery.\n\nExonic splicing silencers work by inhibiting the splicing of pre-mRNA strands or promoting exon skipping. The single stranded pre-mRNA molecules need to have their intronic and exonic regions spliced in order to be translated. ESSs silence splice sites adjacent to them by interfering with the components of the core splicing complex, such as the snRNP's, U1 and U2. This causes proteins that negatively influence splicing to be recruited to the splicing machinery.\n\nESSs have four general roles:\n\nMyotonic dystrophy (MD) is most noticeably caused by inheriting an unstable CTG triplet expansion in the DMPK gene. In healthy genotypes two isoforms of an insulin receptor mRNA transcript exist.The isoform IR-A lacks exon 11 and is expressed ubiquitously in cells. Isoform IR-B contains exon 11 and is expressed in cells of the liver, muscles, kidney, and adipocytes. In individuals with MD, IR-A is upregulated in high amounts in skeletal muscle leading to the disease phenotype.\n\nThe ESS nulceotide sequence exists within intron 10 and is thought to be dependent on the CUG triplet repeat in order to silence the splicing of exon 11. Silencing exon 11 splicing leads to the increased transcription of the IR-A isoform.\n\nMutations in the CFTR gene are responsible for causing cystic fibrosis. A particular mutation occurs in the CFTR pre-mRNA and leads to the exclusion of exon 9, mRNA lacking this exon folds a truncated protein (a protein shortened by a mutation).\n\nExclusion of exon 9 is mediated by a polymorphic locus with variable TG repeats and stretches of T nucleotides – this is abbreviated as (TG)mT(n). This locus is an exonic splicing silencer and is located upstream of the exon 9 splice site (site 3c). The silencing is related to the high number of TG repeats and decreased stretches of T repeats (T tracts). A combination of both these factors is shown to increase levels of exon skipping.\n\nThe TDP-43 protein is responsible for physically silencing the exon splicing site once it is recruited by the exonic splicing silencer (TG)mT(n). TDP-43 is a DNA binding protein and repressor, it binds to the TG repeat to cause exon 9 skipping. The role of the T tracts is not well understood.\n\nSpinal muscular atrophy (SMA) is caused by the homozygous loss of the SMN1 gene. Humans have two isoforms of the SMN (survival motor neuron) gene, SMN1 and SMN2. The SMN1 gene produces a complete transcript, while SMN2 produces a transcript without exon 7 which results in a truncated protein.\n\nThe ESS that contributes to the disease phenotype is the UAGACA nucleotide sequence. This sequence arises when a C-to-T mutation occurs at position +6 in exon 7 of the SMN2 gene. This transition point mutation leads to the exclusion of exon 7 from the mRNA transcript, it is also the only difference between the SMN2 and SMN1 gene.\n\nThe UAGACA ESS is thought to work by disrupting an exonic splicing enhancer and attracting proteins that inhibit splicing by binding sequences on exon 7.\n\nMutations in the ATM gene are responsible for Ataxia telangiecstasia. These mutations are generally single base pair substitutions, deletions, or micro-insertions. A 4-nucleotide deletion within intron 20 of the ATM gene disrupts an exonic splicing silencer and causes the inclusion of a 65-nucleotide cryptic exon in the mature transcript. The inclusion of the cryptic exon results in protein truncation and atypical splicing patterns.\n", "id": "9951136", "title": "Exonic splicing silencer"}
{"url": "https://en.wikipedia.org/wiki?curid=48827290", "text": "CAF-1\n\nCAF-1 (chromatin assembly factor-1) is a complex, including Chaf1a (p150), Chaf1b (p60) and p50 subunits that assembles histone tetramers onto replicating DNA \"in vitro\" This complex is histone chaperone involved in creating cellular memory of somatic cell identity – cellular differentiation. CAF-1 mediates the first step in nucleosome formation by tetramerization and depositing newly synthesized histone H3/H4 onto DNA rapidly behind replication forks. Several studies have shown that the interaction between CAF-1 and PCNA, which stabilizes CAF-1 at replication forks, is important for CAF-1's role in nucleosome assembly\n\nCAF-1 is required for the spatial organization and epigenetic marking of heterochromatin domains in pluripotent embryonic cells.\n\nCells resembling 2-cell-stage mouse embryos (totipotent cells) can be induced in vitro through downregulation of the chromatin-assembly activity of CAF-1 in embryonic stem cells.\n\nOptimal modulation of both CAF-1 and transcription factor levels increases reprogramming efficiency by several orders of magnitude and facilitated iPS cell formation in as little as 4 days. Mechanistically, CAF-1 suppression led to a more accessible chromatin structure at enhancer elements early during reprogramming. These changes were accompanied by a decrease in somatic heterochromatin domains, increased binding of Sox2 to pluripotency-specific targets and activation of associated genes. Suppression of CAF-1 also enhance the direct conversion of B cells into macrophages and fibroblasts into neurons.\n\n", "id": "48827290", "title": "CAF-1"}
{"url": "https://en.wikipedia.org/wiki?curid=25340583", "text": "Haploid-relative-risk\n\nThe Haplotype-relative-risk (HRR) method is a family-based method for determining gene allele association to a disease in the presence of actual genetic linkage. Nuclear families with one affected child are sampled using the parental haplotypes not transmitted as a control. While similar to the genotype relative risk (RR), the HRR provides a solution to the problem of population stratification by only sampling within family trios. The HRR method was first proposed by Rubinstein in 1981 then detailed in 1987 by Rubinstein and Falk and is an important tool in genetic association studies.\n\nThe original method proposed by Falk and Rubinstien fell under scrutiny in 1989, when Ott showed the equivalence of HRR to the classical RR method demonstrating that the HRR holds only when there is zero chance of recombination between a disease locus and its markers. Yet, even when the recombination factor for a locus and its genetic markers is >0 HRR estimates are still more conservative than RR estimates.\n\nWhile the HRR method has proven an effective means of avoiding population stratification biases, another family-based association test known as the transmission disequilibrium test, or TDT, is more commonly used. Some research uses both HRR and TDT for their ability to complement each other since one result may give no association while the other will. A positive association result from both TDT and HRR means there is strong evidence that a link exists and vice versa. For example, both HRR and TDT methods were used in a study looking for polymorphism in D2 and D3 dopamine receptor in association with schizophrenia and neither found any evidence for linkage, making an actual role of those genes in the etiology of the mental disorder all the more unlikely.\n\nThis model represents a case which there is a single locus where all genotypes may lead to expression of the allele in its most simplified definition. Under these parameters a linkage disequilibrium of more than 50% means there is a possible link to the gene allele and inheritance.\n\nformula_1\n\nGives the HHR which can be estimated by\n\nformula_2\n\na denotes the observed frequency of children who are positive for the gene allele H.\n\nb denotes the observed frequency of children who are negative for the gene allele H.\n\nc is the observed frequency of families with at least one transmitted parental marker allele H.\n\nd is the observed frequency of families with no transmitted parental marker allele H.\n\nP is the probability this child is positive for the allele of interest H.\n\nP is the probability that at least one of the nontransmitted parental marker alleles equals the allele of interest H.\n\nH is the allele of interest.\n", "id": "25340583", "title": "Haploid-relative-risk"}
{"url": "https://en.wikipedia.org/wiki?curid=48851579", "text": "Domestication islands\n\nDomestication islands refers to regions within DNA that do not change despite gene flow between wild and domesticated species.\n\nIn 2005, a study of genomic DNA within species of mosquitoes found that there were \"speciation islands\" of genes that remain differentiated despite considerable gene flow, and are therefore expected to contain the genes responsible for reproductive isolation between species. In the following years, there was criticism of this proposal and that there might be other explanations, including nucleotide diversity within species or reduced diversity. This debate continues.\n\nIn 2007, a study of the genomic DNA differences of wild compared to domesticated species of beans found that there was a large fraction of the genome of the common bean that appears to have been subjected to the effects of selection during domestication. Domestication appears to have affected not only the target genes, but also a large portion of the genome around these genes. These \"domestication islands\" have probably experienced a higher level of isolation between the wild and the domesticated forms in comparison with the rest of the genome, probably because of linkage to the loci selected during domestication.\n\nIn 2015, a study looked at the genome sequences of pigs, and found that the assumption of reproductive isolation and strong domestication bottlenecks were incompatible with the data. The domestication process was assumed to have been initiated by humans, involved few individuals, and relied on reproductive isolation between wild and domestic forms. Despite gene flow between domestic and wild pigs, the genomes of domestic pigs had strong signatures of selection at loci that affect behavior and morphology. The conclusion was that recurrent selection for domestic traits likely counteracted the homogenizing effect of gene flow from wild boars and created \"islands of domestication\" in the genome.\n", "id": "48851579", "title": "Domestication islands"}
{"url": "https://en.wikipedia.org/wiki?curid=11408311", "text": "Drosophila hybrid sterility\n\nThe concept of a biological species as a group of organisms capable of interbreeding to produce viable offspring dates back to at least the 18th century, although it is often associated today with Ernst Mayr. Species of the fruit-fly \"Drosophila\" are one of the most commonly used organisms in evolutionary research, and have been used to test many theories related to the evolution of species. The genus \"Drosophila\" comprises numerous species that have varying degrees of premating and postmating isolation (including hybrid sterility) between them. These species are useful for testing hypotheses of the reproductive mechanisms underlying speciation.\n\nWorking in the early 20th century T.H. Morgan, was the first to use \"Drosophila\" to explore heredity. Primarily on the basis of work with \"D. melanogaster\", Morgan and his colleagues C.B. Bridges, A.H. Sturtevant, and H.J. Mueller developed a chromosome theory of heredity, for which Morgan was awarded a Nobel Prize in 1933. Their experiments consisted of cross-breeding \"Drosophila\" mutants and documenting offspring. Another highly regarded figure in \"Drosophila\" research was Theodosius Dobzhansky, who invented the use of genetic markers and used them to study hybrid sterility between \"Drosophila pseudoobscura\" and \"Drosophila persimilis\" (Futuyma 1997). This experimental method has been used for many years.\n\nThe genome of \"D. melanogaster\", has been sequenced and studied in fine detail. It is now known that \"Drosophila\" has 6 chromosomes—an X/Y pair and four autosomal chromosomes. The genome comprises about 139.5 million base pairs. There are about 15,000 genes.\n\nGender is determined in \"Drosophila\" not by the presence or absence of the Y chromosome as in mammals, but by the ratio of X chromosomes to autosomes.\n\nIn the off-spring of crosses between \"Drosophila simulans\" and its island derivative \"Drosophila mauritiana,\" female hybrids are fertile but male hybrids are sterile. Recent studies have shown that a critical gene for gender determination in \"Drosophila\" known as the sex-lethal gene is highly misregulated in \"D. melanogaster\" and \"D. simulans\" hybrids, compared with the degree of misregulation of non-sex biased genes studied. The sex-lethal gene is often abnormally expressed in male hybrids from \"D. melanogaster\" mothers as a result of re-localization of the male-specific complex to the X chromosome, which contributes to the male sterility. Abnormalities in sperm array were found in very few individuals during their larval stage, meaning that disruptions in spermatogenesis most likely occur during later stages in life. \n\nExperiments involving crosses between \"D. pseudoananassae\" and \"D. bipectinata\", \"D. pseudoananassae\" and \"D. parabipectinata\", and \"D. pseudoananassae\" and \"D. malerkotliana\" have further shown that the Y chromosome has a role in hybrid male sterility. The possible interactions of Y chromosome are X-Y, Y-autosome and Y-cytoplasm (Paras 2006). The sterile males carry a set of conspecific autosomes to the Y chromosome, which results in fertility because of the Y-dominant autosome interactions. Therefore, Y-autosome interactions are ruled out in this type of hybrid sterility.\n\nSince cytoplasmic factors can be compatible between these species (Reference needed), such factors are also dismissed as the cause of sterility.\n\nHowever, in \"Drosophila paulistorum\" there is information suggesting that if Y chromosome and cytoplasm are from different parents, the male is usually sterile. (Perez-Salas & Ehrman 1971) X-Y interactions are the most likely cause of sterility in male hybrids. It has been shown that interbreeding organisms that have more genetic compatibilities have less disruption of spermatogenesis and normal sized testes, while species who are less genetically compatible have a higher disruption in spermatogenesis and generally have atrophied testes. (Reference needed)\n\nAnother possible cause of sterility among species in which presence or absence of one or the other sex chromosome determines gender, is if one sex chromosome of one species has recessive alleles interacting with autosomal alleles of the mating species. This could cause the heterogametic sex chromosome in the hybrid to be inviable or sterile, but homogametic sex chromosome will be fertile (Futuyma 1997). Consequently, in species where presence or absence of a Y chromosome determines gender, for instance, individuals carrying XY chromosomes (males) will be sterile and those carrying XX (females) will be fertile. This is closely related to Haldane's rule.\n\nIt has also been shown that microbial infections of invertebrates can cause modification of the gender and fertility of host's off-spring. For instance, infections of nematodes or of arthropods, including Drosophila, species of the rickettsial bacteria Wolbachia can produce a male-specific sterility, which is congenital by means of transmission through the female line.\n\nExperiments have led scientists to believe that many observations recorded in laboratories neglect existing polymorphism factors in hybrid sterility due to crossing parents from non-isofemale lines, as well as possibly underestimating actual degrees of sterility caused by inaccurate measures of motility. (Reference needed) Failure to account for potential polymorphism could lead to misinterpretation of the scale on which hybrid sterility occurs.\n\n", "id": "11408311", "title": "Drosophila hybrid sterility"}
{"url": "https://en.wikipedia.org/wiki?curid=8946", "text": "Decipherment\n\nIn philology, decipherment is the discovery of the meaning of texts written in ancient or obscure languages or scripts. Decipherment in cryptography refers to decryption. The term is used sardonically in everyday language to describe attempts to read poor handwriting. In genetics, decipherment is the successful attempt to understand DNA, which is viewed metaphorically as a text containing word-like units. Throughout science the term decipherment is synonymous with the understanding of biological and chemical phenomena.\n\nIn many cases, a multilingual artifact is necessary to facilitate decipherment, the Rosetta Stone being the classic example. Statistical techniques provide another pathway to decipherment, as does the analysis of modern languages derived from ancient languages in which undeciphered texts are written. Archaeological and historical information is helpful in verifying hypothesized decipherments. \n\n\n\n", "id": "8946", "title": "Decipherment"}
{"url": "https://en.wikipedia.org/wiki?curid=5017283", "text": "Dermatoglyphics\n\nDermatoglyphics (from ancient Greek derma=skin, glyph=carving) is the scientific study of fingerprints, lines, mounts, and shapes of hands. Dermatoglyphics refers to the formation of naturally occurring ridges on certain body parts, namely palms, fingers, soles and toes. These are areas where hair usually does not grow and these ridges allow for increased leverage when picking up objects or walking barefoot. The finger prints of both hands are not the same. They do not change size or shape throughout a person's life, except in cases of serious injuries that scar the dermis. As a term, dermatoglyphics is used to distinguish it from the superficially similar pseudoscience of palmistry. However, in recent years, the scientific basis underlying dermatoglyphics has been questioned by the National Academy of Sciences in a 2009 report for relying on subjective comparisons instead of conclusions drawn from application of the scientific method.\n\nDermatoglyphics which are correlated with genetic abnormalities are useful in diagnoses of these disorders at birth or soon after. They are used in the diagnosis of congenital malformations.\n\nKlinefelter's syndrome: Excess of arches on digit 1, more frequent ulnar loops on digit 2, overall fewer whorls, lower ridge counts for loops and whorls as compared with controls, and significant reduction of the total finger ridge count.\n\nCri du chat (5p-): Excess of arches on fingertips and single transverse palmar creases in 90%.\n\nCongenital blindness: Initial data points to abnormal triradius and excess of arches on fingertips.\n\nNaegeli–Franceschetti–Jadassohn syndrome: patients lack dermatoglyphics of any kind.\n\nNoonan syndrome: Increased frequency of whorls on fingertips, and the axial triradius t, as in Turner syndrome, is more often in position t' or t\" than in controls. Increased incidence of the single transverse palmar crease.\n\nTrisomy 13 (Patau syndrome): Excess of arches on fingertips and single transverse palmar creases in 60%.\n\nTrisomy 18 (Edward's syndrome): 6 - 10 arches on fingertips and single transverse palmar creases in 30%.\n\nTrisomy 21 (Down syndrome): People with Down syndrome have a finger print pattern with mainly ulnar loops, and a significantly different angle between the triradia a, t and d (the 'adt angle'). Other differences often include a single transverse palmar crease (\"Simian line\") (in 50%), and patterns in the hypothenar and interdigital areas, lower ridge counts along digital midlines, especially in little fingers, which corresponds to finger shortening in those with Down syndrome. There is less variation in dermatoglyphic patterns between people with Down syndrome than between controls, and dermatoglyphic patterns can be used to determine correlations with congenital heart defects in individuals with Down syndrome by examining the left hand digit ridge count minus the right hand digit ridge count, and the number of ridges on the fifth digit of the left hand.\n\nTurner syndrome: Predominance of whorls, although the pattern frequency depends on the particular chromosomal abnormality.\n\nRubinstein-Taybi Syndrome: preponderance of broad thumbs, low mean ridge count, and fingerprint patterns occurring on interdigital areas.\n\nSchizophrenia: A-B ridge counts are generally lower than in controls.\n\nThe scientific study of papillary ridges of the hands and feet is credited as the beginning with the work of Joannes Evangelista Purkinje in 1823. William Herschel (1858) was the first to experiment with fingerprints in India .Sir Francis Galton (1892) conducted extensive research on the significance of skin ridge patterns, not only to demonstrate their permanence but also their use as a means of identification.\n\nIn 1892, Galton published the book “Fingerprints” and in doing so, significantly advanced the science of\nfingerprint identification.\n\nSir Edward Henry (1893) published the book 'The classification and uses of fingerprints\" which established the modern era of finger print identification, which is now the basis for most of the other classification systems.Charles Midlo M D (1929) together with others published one of the most widely referred book \"Fingerprints, Palms and soles\", a bible in the field of Dermatoglyphics.\n\nPenrose L S (1945), inspired by the works of Cummins and Midlo, conducted his own dermatoglyphic\ninvestigations as a further aspect of his research into Down's syndrome and other congenital medical disorders.\n\nSchaumann and Alter's (1976) published a book 'Dermatoglyphics in Medical disorders' which summarizes the findings of dermatoglyphic patterns in various disease conditions (1). Engler et al (1982) conducted a study on patients with breast cancer and concluded that the presence of six or more whorls on the fingertips of a person provided a high risk for breast cancer.\n\nThe current state of medical dermatoglyphics is such, that the diagnosis of some illnesses can now be done on the basis of dermatoglyphic analysis alone and currently, several dermatoglyphic researches claim a very high degree of accuracy in their prognostic ability from the hand features.\n\nInternational Journal of Pharma and Bio-sciences", "id": "5017283", "title": "Dermatoglyphics"}
{"url": "https://en.wikipedia.org/wiki?curid=24219329", "text": "Neurogenomics\n\nNeurogenomics is the study of how the genome of an organism influences the development and function of its nervous system. This field intends to unite functional genomics and neurobiology in order to understand the nervous system as a whole from a genomic perspective.\n\nThe nervous system in vertebrates is made up of two major types of cells – neuroglial cells and neurons. Hundreds of different types of neurons exist in humans, with varying functions – some of them process external stimuli; others generate a response to stimuli; others organize in centralized structures (brain, spinal ganglia) that are responsible for cognition, perception, and regulation of motor functions. Neurons in these centralized locations tend to organize in giant networks and communicate extensively with each other. Prior to the availability of expression arrays and DNA sequencing methodologies, researchers sought to understand the cellular behaviour of neurons (including synapse formation and neuronal development and regionalization in the human nervous system) in terms of the underlying molecular biology and biochemistry, without any understanding of the influence of a neuron’s genome on its development and behaviour. As our understanding of the genome has expanded, the role of networks of gene interactions in the maintenance of neuronal function and behaviour has garnered interest in the neuroscience research community. Neurogenomics allows scientists to study the nervous system of organisms in the context of these underlying regulatory and transcriptional networks. This approach is distinct from neurogenetics, which emphasizes the role of single genes without a network-interaction context when studying the nervous system.\n\nIn 1999, Cirelli & Tononi first reported the association of genome-wide brain gene expression profiling (using microarrays) with a behavioural phenotype in mice. Since then, global brain gene expression data, derived from microarrays, has been aligned to various behavioural quantitative trait loci (QTLs) and reported in several publications. However, microarray based approaches have their own problems that confound analysis – probe saturation can result in very small measurable variance of gene expression between genetically unique individuals, and the presence of single nucleotide polymorphisms (SNPs) can result in hybridization artifacts. Furthermore, due to their probe-based nature, microarrays can miss out on many types of transcripts (ncRNAs, miRNAs, and mRNA isoforms). Probes can also have species-specific binding affinities that can confound comparative analysis.\n\nNotably, the association between behavioural patterns and high penetrance single gene loci falls under the purview of neurogenetics research, wherein the focus is to identify a simple causative relationship between a single, high penetrance gene and an observed function/behaviour. However, it has been shown that several neurological diseases tend to be polygenic, being influenced by multiple different genes and regulatory regions instead of one gene alone. There has hence been a shift from single gene approaches to network approaches for studying neurological development and diseases, a shift that has been greatly propelled by the advent of next generation sequencing methodologies.\n\nTwin studies have revealed that schizophrenia, bipolar disorder, autism spectrum disorder (ASD), and attention deficit hyperactivity disorder (ADHD) are highly heritable, genetically complex psychiatric disorders. However, linkage studies have largely failed at identifying causative variants for psychiatric disorders such as these, primarily because of their complex genetic architecture. Multiple low penetrance risk variants can be aggregated in affected individuals and families, and sets of causative variants could vary across families. Studies along these lines have determined a polygenic basis for several psychiatric disorders. Several independently occurring \"de novo\" mutations in patients Alzheimer's Disease have been found to disrupt a shared set of functional pathways involved with neuronal signalling, for example. The quest to understand the causative biology of psychiatric disorders is hence greatly assisted by the ability to analyse entire genomes of affected and unaffected individuals in an unbiased manner.\n\nWith the availability of massively parallel next generation sequencing methodologies, scientists have been able to look beyond the probe based captures of expressed genes. RNA-seq, for example, identifies 25-60% more expressed genes than microarrays do. In the upcoming field of neurogenomics, it is hoped that by understanding the genomic profiles of different parts of the brain, we might be able to improve our understanding of how the interactions between genes and pathways influence cellular function and development. This approach is expected to be able to identify the secondary gene networks that are disrupted in neurological disorders, subsequently assisting drug development stratagems for brain diseases. The BRAIN initiative launched in 2013, for example, seeks to \"\"inform the development of future treatments for brain disorders, including Alzheimer’s disease, epilepsy, and traumatic brain injury\"\" .\n\nRare variant association studies (RVAS) have highlighted the role of \"de novo\" mutations in several congenital and early-childhood-onset disorders like autism. Several of these protein disrupting mutations have been able to be identified only with the aid of whole genome sequencing efforts, and validated with RNA-Seq. Additionally, these mutations are not statistically enriched in individual genes, but rather, exhibit patterns of statistical enrichment in groups of genes associated with networks regulating neurological development and maintenance. Such a discovery would have been impossible with prior gene-centric approaches (neurogenetics, behavioural neuroscience). Neurogenomics allows for a high-throughput system-based approach for understanding the polygenic basis of neuropsychiatric disorders.\n\nWhen autism was identified as a distinct biological disorder in the 1980s, researchers found that autistic individuals showed a brain growth abnormality in the cerebellum in their early developmental years. Subsequent research has indicated that 90% of autistic children have a larger brain volume than their peers by 2 to 4 years of age, and show an expansion in the white and gray matter content in the cerebrum. The white and gray matter in the cerebrum is associated with learning and cognition respectively, and the formation of amyloid plaques in the white matter has been associated with Alzheimer’s disease. These findings highlighted the influence of structural variance in the brain on psychiatric disorders, and have motivated the use of imaging technologies to map regions of divergence between healthy and diseased brains. Furthermore, while it may not always be possible to retrieve biological specimens from different areas live human brains, neuroimaging techniques offer a noninvasive means to understanding the biological basis of neurological disorders. It is hoped that an understanding of localization patterns of different psychiatric diseases could in turn inform network analysis studies in neurogenomics.\n\nStructural Magnetic Resonance Imaging (MRI) can be used to identify the structural composition of the brain. Particularly in the context of neurogenomics, MRI has played an extensive role in the study of Alzheimer's disease(AD) over the past four decades. It was initially used to rule out other causes of dementia, but recent studies indicated the presence of characteristic changes in patients with AD. As a result, MRI scans are currently being used as a neuroimaging tool to help identify the temporal and spatial pathophysiology of Alzheimer's disease, such as specific cerebral alterations and amyloid imaging.\n\nThe ease and non-invasive nature of MRI scans has motivated research projects that trace the development and onset of psychiatric diseases in the brain. Alzheimer disease has become a key candidate in this topographical approach to psychiatric diseases. For example, MRI scans are currently being used to track the resting and task-dependent functional profiles of brains in children with autosomal dominant Alzheimer disease. These studies have found indications of early onset brain alterations in at-risk individuals for AD. The Autism Center of Excellence at University of California, San Diego, is also conducting MRI studies with children between 12 and 42 months, in the hopes of characterizing brain development abnormalities in children who present behavioural symptoms of autism.\n\nAdditional research has indicated that there are specifics patterns of atrophy in the cerebrum (as a repercussion of neurodegeneration) in different neurological disorders and diseases. These disease-specific patterns of progression of atrophy can be identified with MRI scans, and provide a clinical phenotype context to neurogenomic research. The temporal information about disease progression provided by this approach can also potentially inform the interpretation of gene network-level perturbations in psychiatric diseases.\n\nOne prohibitive feature of 2nd generation sequencing methodologies is the upper limit on the genomic range accessible by mate-pairing. Optical mapping is an emerging methodology used to span large-scale variants that cannot usually be detected using paired end reads. This approach has been successfully applied to detect structural variants in oligodendroglioma, a type of brain cancer. Recent work has also highlighted the versatility of optical maps in improving existing genome assemblies. Chromosomal rearrangements, microdeletions, and large-scale translocations have been associated with impaired neurological and cognitive function, for example in hereditary neuropathy and neurofibromatosis. Optical mapping can significantly improve variant detection and inform gene interaction network models for the diseased state in neurological disorders.\n\nApart from neurological disorders, there are additional diseases that manifest in the brain and have formed exemplar use-case scenarios for the application of brain imaging in network analysis. In a classic example of imaging-genomic analyses, a research study in 2012 compared MRI scans and gene expression profiles of 104 glioma patients in order to distinguish treatment outcomes and identify novel targetable genomic pathways in Glioblastoma Multiforme (GBM). Researchers found two distinct groups of patients with significantly different organization of white matter (invasive vs non-invasive). Subsequent pathway analysis of the gene expression data indicated mitochondrial dysfunction as the top canonical pathway in an aggressive, low-mortality GBM phenotype.\n\nExpansion of brain imaging approaches to other diseases can be used to rule out other medical illnesses while diagnosing psychiatric disorders. It must be noted that at this time, we cannot use solely brain scans to inform the presence or absence of a psychiatric disorder.\n\nThe current approaches in collecting gene expression data in human brains are to use either microarrays or RNA-seq. Currently, it is rare to gather \"live\" brain tissue – only when treatments involve brain surgery is there a chance that brain tissue is collected during the procedure. This is the case with epilepsy.\n\nCurrently, gene expression data is usually collected on post mortem brains and this is often a barrier to neurogenomics research in humans. After death, the amount of time between death and when the data from the post mortem brain is collected is known as the post mortem interval (PMI). Since RNA degrades after death, a fresh brain is optimal – but not always available. This in turn can influence a variety of downstream analyses. Consideration should be taken of the following factors when working with 'omics data collected from post-mortem brains:\nDifferential diagnosis also remains a critical pre-analytical confounder of cohort-wide studies of spectrum neurological disorders. Specifically, this has been noted to be a cromulant problem for Alzheimers disease and autism spectrum disorder studies. Furthermore, as our understanding of the diverse symptoms and genomic underpinnings of various neurogenomic disorders improves, the diagnostic criteria itself undergoes rearrangements and review.\n\nOngoing genomics research in neurological disorders tends to use animal models (and corresponding gene homologs) to understand the network interactions underlying a particular disorder due to ethical issues surrounding the retrieval of biological specimens from live human brains. This, too, is not without its roadblocks.\n\nNeurogenomic research with a model organism is contingent on the availability of a fully sequenced and annotated reference genome. Additionally, the RNA profiles (miRNA, ncRNA, mRNA) of the model organism need to be well catalogued, and any inferences applied from them to humans must have a basis in functional/sequence homology.\n\nZebrafish development relies on gene networks that are highly conserved among all vertebrates. Additionally, with an extremely well annotated set of 12,000 genes and 1,000 early development mutants that are actually visible in the optically clear zebrafish embryos and larvae, zebrafish offer a sophisticated system for mutagenesis and real-time imaging of developing pathologies. This early development model has been employed to study the nervous system at cellular resolution. The zebrafish model system has already been used to study neuroregeneration and severe polygenic human diseases like cancer and heart disease. Several zebrafish mutants with behavioural variations in response to cocaine and alcohol dosage have been isolated and can also form a basis for studying the pathogenesis of behavioural disorders.\n\nRodent models have been preeminent in studying human disorders. These models have been extensively annotated with gene homologs of several monogenic disorders in humans. Knockout studies of these homologs have led to expansion of our understanding of network interactions of genes in human tissues. For example, the \"FMR1\" gene has been implicated with autism from a number of network studies. Using a knockout of \"FMR\"1 in mice creates the model for Fragile X Syndrome, one of the disorders in the Autism spectrum.\n\nMice xenografts are particularly useful for drug discovery, and were extremely important in the discovery of early anti-psychotic drugs. The development of animal models for complex psychiatric diseases has also improved over the last few years. Rodent models have demonstrated behavioural phenotype changes resembling a positive schizophrenia state, either after genetic manipulation or after treatment with drugs that target the areas of the brain suspected to influence hyperactivity or neurodevelopment. Interest has been generated in identifying the network disruptions mediated by these laboratory manipulations, and collection of genomic data from rodent studies has contributed significantly to a better understanding of the genomics of psychiatric diseases.\n\nThe first mouse brain transcriptome was generated in 2008. Since then, extensive work has been done with building social-stress mice models to study the pathway level expression signatures of various psychiatric diseases. A recent paper simulated features of Post Traumatic Stress Disorder (PTSD) in mice, and profiled the entire transcriptome of these mice. The authors found differential regulation in many biological pathways, some of which were implicated in anxiety disorders (hyperactivity, fear response), mood disorders, and impaired cognition. These findings are backed by extensive transcriptomic analyses of anxiety disorders, and expression level changes in biological pathways involved with fear learning and memory are thought to contribute to the behavioural manifestations of these disorders. It is thought that functional enrichment of genes involved in long term synaptic potentiation, depression, and plasticity has an important role to play in the acquisition, consolidation, and maintenance of traumatic memories underlying anxiety disorders.\n\nA common approach to using a mouse model is to apply an experimental treatment to a pregnant mouse in order to affect a whole litter. However, a key issue in the field is the treatment of litters in a statistical analysis. Most studies consider the total number of offspring produced as that may lead to an increase in statistical power. However, the correct way is to count by the number of litters and to normalize based on litter size. It was found that several autism studies incorrectly performed their statistical analyses based on total number of offspring instead of number of litters.\n\nSeveral anxiety disorders such as post-traumatic stress disorder (PTSD) involve heterogeneous changes in several different brain regions, such as the hippocampus, amygdala, and nucleus accumbens. The cellular encoding of traumatic events and the behavioral responses triggered by such events has been shown to lie primarily in changes in signaling molecules associated with synaptic transmission.\n\nGlobal gene expression profiling of the various gene regions implicated in fear and anxiety processing, using mice models, has led to the identification of temporally and spatially distinct sets of differentially expressed genes. Pathway analysis of these genes has indicated possible roles in neurogenesis and anxiety-related behavioural responses, alongside other functional and phenotypic observations.\n\nMice models for brain research have contributed significantly to drug development and increased our understanding of the genomic underpinnings of several neurological diseases in the last generation. Chlorpromazine, the first antipsychotic drug (discovered in 1951), was identified as a viable treatment option after it was shown to suppress response to aversive stimuli in rats in a behavioural screen.\n\nThe modelling and assessment of latent symptoms (thoughts, verbal learning, social interactions, cognitive behaviour) remains a challenge when using model organisms to study psychiatric disorders with a complex genetic pathology. For example, a given genotype+phenotype in a mouse model must imitate the genomic underpinnings of a phenotype observed in a human.\n\nThis is a particularly crucial item of consideration in spectrum disorders such as autism. Autism is a disorder whose symptoms can be divided into two categories: (i) deficits of social interactions and (ii) repetitive behaviours and restricted interests. Since mice tend to be more social creatures amongst all members of the order \"Rodentia\" currently being used as model organisms, mice are generally used to model human psychiatric disorders as closely as possible. Particularly for autism, the following work-arounds are currently in place to emulate human behavioural symptoms:\nIn any of these experiments, the ‘autistic’ mice have a ‘normal’ socializing partner and the scientists observing the mice is unaware/\"'blind\"' to the genotypes of the mice.\n\nThe gene expression profile of the central nervous system (CNS) is unique. Eighty percent of all human genes are expressed in the brain; 5,000 of these genes are solely expressed in the CNS. The human brain has the highest amount of gene expression of all studied mammalian brains. In comparison, tissues outside of the brain will have more similar expression levels in comparison to their mammalian counterparts. One source of the increased expression levels in the human brain is from the non-protein coding region of the genome. Numerous studies have indicated that the human brain have a higher level of expression in regulatory regions in comparison to other mammalian brains. There is also notable enrichment for more alternative splicing events in the human brain.\n\nGene expression profiles also vary within specific regions of the brain. A microarray study showed that the transcriptome profile of the CNS clusters together based on region. A different study characterized the regulation of gene expression across 10 different regions based on their eQTL signals. The cause of the varying expression profiles relates to function, neuron migration and cellular heterogeneity of the region. Even the three layers of the cerebral cortex have distinct expression profiles.\n\nA study completed at Harvard Medical School in 2014 was able to identify developmental lineages stemming from single base neuronal mutations. The researchers sequenced 36 neurons from the cerebral cortex of three normal individuals, and found that highly expressed genes, and neural associated genes, were significantly enriched for single-neuron SNVs. These SNVs, in turn, were found to be correlated with chromatin markers of transcription from fetal brain.\n\nGene expression of the brain changes throughout the different phases of life. The most significant levels of expression are found during early development, with the rate of gene expression being highest during fetal development. This results from the rapid growth of neurons in the embryo. Neurons at this stage are undergoing neuronal differentiation, cell proliferation, migration events and dendritic and synaptic development. Gene expression patterns shift closer towards specialized functional functional profiles during embryonic development, however, certain developmental steps are still ongoing at parturition. Consequently, gene expression profiles of the two brain hemispheres appear asymmetrical at birth. At birth, gene expression profiles appear asymmetrical between brain hemispheres. As development continues, the gene expression profiles become similar between the hemispheres. Given a healthy adult, expression profiles stay relatively consistent from the late twenties into the late forties. From the fifties onwards, there is significant decrease in the expression of genes important for regular function. Despite this, there is an increase in the diversity of genes being expressed across the brain. This age related change in expression may be correlated with GC content. At later stages of life, there is an increase in the induction of low GC-content pivotal genes as well as an increase in the repression of high GC-content pivotal genes. Another cause of the shift in gene diversity is the accumulation of mutations and DNA damage. Gene expression studies show that genes that accrue these age-related mutations are consistent between individuals in the aging population. Interestingly, genes that are highly expressed at development decrease significantly at late stages in life, whereas genes that are highly repressed at development increase significantly at the late stages.\n\nThe evolution of \"Homo sapiens\" since the divergence from the primate common ancestor has shown a marked expansion in the size and complexity of the brain, especially in the cerebral cortex. In comparison to primates, the human cerebral cortex has an embiggened surface area but differs only slightly in thickness. Many large scale studies in understanding the differences of the human brain from other species have indicated expansion of gene families and changes in alternative splicing to be responsible for the corollary increase in cognitive capabilities and cooperative behaviour in humans. However, we are yet to determine the exact phenotypic consequences of all these changes. One difficulty is that only primates have developed subdivisions in their cerebral cortex, making the modeling of human specific neurological problems difficult to mimic in rodents.\n\nSequence data is used to understand the evolutionary genetic changes which led to the development of the human CNS. We can then understand how the neurological phenotypes differ between species. Comparative genomics entails comparison of sequence data across a phylogeny to pinpoint the genotypic changes that occur within specific lineages, and understand how these changes might have arisen. The increase in high quality mammalian reference sequences generally makes comparative analysis better as it increases statistical power. However, the increase in number of species in a phylogeny does risk adding unnecessary noise as the alignments of the orthologous sequences usually decrease in quality. Furthermore, different classes of species will have significant differences in their phenotypes.\n\nDespite this, comparative genomics has allowed us to connect the genetic changes found in a phylogeny to specific pathways. In order to determine this, lineages are tested for the functional changes that accrue over time. This is often measured as a ratio of nonsynonymous substitutions to synonymous substitutions or the dN/dS ratio (sometimes, further abbreviated to ω). When the dN/dS ratio is greater than 1, this indicates positive selection. A dN/dS ratio equal to 1 is evidence of no selective pressures. A dN/dS ratio less than 1 indicates negative selection. For example, the conserved regions of the genome will generally have a dN/dS ratio of less than 1 since any changes to those positions will likely be detrimental. Of the genes expressed in the human brain, it is estimated that 342 of them have a dN/dS ratio greater than 1 in the human lineage in comparison to other primate lineages. This indicates positive selection on the human lineage for brain phenotypes. Understanding the significance of the positive selection is generally the next step. For example, \"ASPM\", \"CDK5RAP2\" and \"NIN\" are genes that are positively selected for on the human lineage and have been directly correlated with brain size. This finding may help elucidate why human brains are larger than other mammalian brains.\n\nIt is thought that gene expression changes, being the ultimate response for any genetic changes, are a good proxy for understanding phenotypic differences within biological samples. Comparative studies have revealed a range of differences in the transcriptional controls between primates and rodents. For example, the gene \"CNTNAP2\" is specifically enriched for in the prefrontal cortex. The mouse homolog of \"CNTNAP2\" is not expressed in the mouse brain. Interestingly, \"CNTNAP2\" has been implicated in cognitive functions of language as well as neurodevelopmental disorders such as Autism Spectrum Disorder. This suggests that the control of expression plays a significant role in the development in unique human cognitive function. As a consequence, a number of studies have investigated the brain specific enhancers. Transcription factors such as \"SOX5\" have been found to be positively selected for on the human lineage. Gene expression studies in humans, chimpanzees and rhesus macaques, have identified human specific co-expression networks, and an elevation in gene expression in the human cortex in comparison to primates.\n\nNeurogenomic disorders manifest themselves as neurological disorders with a complex genetic architecture and a non-Mendelian-like pattern of inheritance. Some examples of these disorders include Bipolar disorder and Schizophrenia. Several genes may be involved in the manifestation of the disorder, and mutations in such disorders are generally rare and \"de novo.\" Hence it becomes extremely unlikely to observe the same (potentially causative) variant in two unrelated individuals affected with the same neurogenomic disorder. Ongoing research has implicated several \"de novo\" exonic variations and structural variations in Autism Spectrum Disorder (ASD), for example. The allelic spectrum of the rare and common variants in neurogenomic disorders therefore necessitates a need for large cohort studies in order to effectively exclude low effect variants and identify the overarching pathways frequently mutated in the different disorders, rather than specific genes and specific high penetrance mutations.\n\nWhole genome sequencing (WGS) and whole exome sequencing (WES) has been used in Genome Wide Association Studies (GWAS) to characterize genetic variants associated with neurogenomic disorders. However, the impact of these variants cannot always be verified because of the non-Mendelian inheritance patterns observed in several of these disorders. Another prohibitive feature in network analysis is the lack of large-scale datasets for many psychiatric (neurogenomic) diseases. Since several diseases with neurogenomic underpinnings tend to have a polygenic basis, several nonspecific, rare, and partially penetrant \"de novo\" mutations in different patients can contribute to the same observed range of phenotypes, as is the case with Autism Spectrum Disorder and schizophrenia. Extensive research in alcohol dependence (ALC) has also highlighted the need for high-quality genomic profiling of large sample sets when studying polygenic, spectrum disorders.\n\nThe 1000 Genomes Project was a successful demonstration of how a concerted effort to acquire representative genomic data from the broad spectrum of humans can result in identification of actionable biological insights for different diseases. However, a large-scale initiative like this is still lacking in the field of neurogenomic disorders specifically.\n\nOne major GWAS study identified 13 new risk loci for schizophrenia. Studying the impact of these candidates would ideally demonstrate a schizophrenia phenotype in animal models, which is usually difficult to observe due to its manifestation as a latent personality. This approach is able to determine the molecular impact the candidate gene. Ideally the candidate genes would have a neurological impact, which in turn would suggest that it plays a role in the neurological disorder. For example, in the aforementioned schizophrenia GWAS study, Ripke and colleagues determined that these candidate genes were all involved in calcium signalling. Alternatively, one can study these variants in model organisms in the context of affected neurological function. It is important to note that the high penetrance variants of these disorders tend to be \"de novo\" mutations.\n\nA further complication to studying neurogenomic disorders is the heterogeneous nature of the disorder. In many of these disorders, the mutations observed from case to case do not stay consistent. In autism, an affected individual may experience a large amount of deleterious mutations in gene X. A different affected individual may not have any significant mutations on gene X but have a large amount of mutations in gene Y. The alternative is to determine if gene X and gene Y impact the same biochemical pathway—one that influences a neurological function. A bioinformatics network analysis is one approach to this problem. Network analyses methodologies provide a generalized, systems overview of a molecular pathway.\n\nOne final complication to consider is the comorbidity of neurogenomic genes. Several disorders, especially at the more severe ends of the spectrum tend to be comorbid with each other. For example, more severe cases of ASD tend to be associated with intellectual disability (ID). This raises the question of whether or not there are true, unique ASD genes and unique ID genes or if there are just genes just associated with neurological function that can be mutated into an abnormal phenotype. One confounding factor may be the actual diagnostic category and methods of the spectrum disorders as symptoms between severe disorders may be similar. One study investigated the comorbid symptoms between groups of ID and ASD, and found no significant difference between the symptoms of ID children, ASD children with ID and ASD children without ID. Future research may help establish a more stringent genetic basis for the diagnoses of these disorders.\n\nThe main goal of network analysis in neurogenomics is to identify statistically significant nonrandom associations between genes that contain risk variants. While several algorithm implementations of this approach already exist, the general steps for network analysis remain the same. \n\nThe underlying principle of this approach is that the genes that cluster together, will also jointly affect the same molecular pathway. Again, they would ideally be part of a neurological function. The candidate genes can then be used to prioritize variants for wet lab validation.\n\nHistorically, due to the behavioural stimulation manifested as a symptom in several the neurogenomic disorders, the therapies would rely mostly on anti-psychotics or antidepressents. These classes of medications would suppress common symptoms of the disorders, but with questionable efficacy. The biggest barrier to neruopharmacogenomic research was the cohort sizes. Given newly available large-cohort sequencing data, there has been a recent push to expand therapeutic options. The heterogenous nature of neurological diseases is the key motivation for personalized medicine approaches to their therapies. It is rare to find single high penetrance causative genes in neurological diseases. The genomic profiles understandably vary between cases, and logically, the therapies would need to vary between cases. Further complicating the issue is that many of these disorders are spectrum disorders. Their genetic etiology will vary within this spectrum. For example, severe ASD is associated with high penetrance de novo mutations. Milder forms of ASD is usually associated with a mixture of common variants.\n\nThe key issue then is the translation of these newly identified genetic variants (from Copy Number Variant studies, candidate gene sequencing and high throughput sequencing technologies) into an intervention for patients with neurogenomic disorders. One aspect will be if the neurological disorder are medically actionable (i.e. is there a simple metabolic pathway that a therapy can target). For example, specific cases of ASD have been associated with microdeletions on TMLHE gene. This gene codes for the enzyme of carnitine biosynthesis. Supplements to elevate carnitine levels appeared to alleviate certain ASD symptoms but the study was confounded by many influencing factors. As mentioned earlier, using a gene network approach will help identify relevant pathways of interest. Many neuropharmacogenomic approaches have focused on targeting the downstream products of these pathways.\n\nStudies in animal models for several brain diseases has shown that the blood brain barrier (BBB) undergoes modification at many levels; for example, the surface glycoprotein composition can influence the types of HIV-1 strains transported by the BBB. Interestingly enough, the BBB has been found to be key in the onset of Alzheimer’s disease. It is extremely difficult, however, to be able to study this in humans due to obvious restrictions with accessing the brain and retrieving biological specimens for sequencing or morphological analysis. Mice models of the BBB and models of disease states have served well in conceptualizing the BBB as a regulatory interface between disease and good health in the brain.\n\nThe heterogenous nature of neurological diseases is the key motivation for personalized medicine approaches to their therapies. Genomic samples of individual patients could be used to identify predictive factors, or to better understand the specific prognosis of a neurogenomic disease, and use this information to guide treatment options. While there is a clear clinical utility to this approach, the adaptation of this approach is still nonexistent.\n\nThere are various issues prohibiting the application of personalized genomics to the assessment, diagnosis, and treatment of psychiatric disorders. \n\n", "id": "24219329", "title": "Neurogenomics"}
{"url": "https://en.wikipedia.org/wiki?curid=4292", "text": "Base pair\n\nA base pair (bp) is a unit consisting of two nucleobases bound to each other by hydrogen bonds. They form the building blocks of the DNA double helix, and contribute to the folded structure of both DNA and RNA. Dictated by specific hydrogen bonding patterns, Watson-Crick base pairs (guanine-cytosine and adenine-thymine) allow the DNA helix to maintain a regular helical structure that is subtly dependent on its nucleotide sequence. The complementary nature of this based-paired structure provides a backup copy of all genetic information encoded within double-stranded DNA. The regular structure and data redundancy provided by the DNA double helix make DNA well suited to the storage of genetic information, while base-pairing between DNA and incoming nucleotides provides the mechanism through which DNA polymerase replicates DNA, and RNA polymerase transcribes DNA into RNA. Many DNA-binding proteins can recognize specific base pairing patterns that identify particular regulatory regions of genes.\n\nIntramolecular base pairs can occur within single-stranded nucleic acids. This is particularly important in RNA molecules (e.g., transfer RNA), where Watson-Crick base pairs (guanine-cytosine and adenine-uracil) permit the formation of short double-stranded helices, and a wide variety of non-Watson-Crick interactions (e.g., G-U or A-A) allow RNAs to fold into a vast range of specific three-dimensional structures. In addition, base-pairing between transfer RNA (tRNA) and messenger RNA (mRNA) forms the basis for the molecular recognition events that result in the nucleotide sequence of mRNA becoming translated into the amino acid sequence of proteins via the genetic code.\n\nThe size of an individual gene or an organism's entire genome is often measured in base pairs because DNA is usually double-stranded. Hence, the number of total base pairs is equal to the number of nucleotides in one of the strands (with the exception of non-coding single-stranded regions of telomeres). The haploid human genome (23 chromosomes) is estimated to be about 3.2 billion bases long and to contain 20,000–25,000 distinct protein-coding genes. A kilobase (kb) is a unit of measurement in molecular biology equal to 1000 base pairs of DNA or RNA. The total amount of related DNA base pairs on Earth is estimated at 5.0 × 10, and weighs 50 billion tonnes. In comparison, the total mass of the biosphere has been estimated to be as much as 4 TtC (trillion tons of carbon).\n\nHydrogen bonding is the chemical interaction that underlies the base-pairing rules described above. Appropriate geometrical correspondence of hydrogen bond donors and acceptors allows only the \"right\" pairs to form stably. DNA with high GC-content is more stable than DNA with low GC-content, but, contrary to popular belief, the hydrogen bonds do not stabilize the DNA significantly, and stabilization is mainly due to stacking interactions.\n\nThe larger nucleobases, adenine and guanine, are members of a class of double-ringed chemical structures called purines; the smaller nucleobases, cytosine and thymine (and uracil), are members of a class of single-ringed chemical structures called pyrimidines. Purines are complementary only with pyrimidines: pyrimidine-pyrimidine pairings are energetically unfavorable because the molecules are too far apart for hydrogen bonding to be established; purine-purine pairings are energetically unfavorable because the molecules are too close, leading to overlap repulsion. Purine-pyrimidine base pairing of AT or GC or UA (in RNA) results in proper duplex structure. The only other purine-pyrimidine pairings would be AC and GT and UG (in RNA); these pairings are mismatches because the patterns of hydrogen donors and acceptors do not correspond. The GU pairing, with two hydrogen bonds, does occur fairly often in RNA (see wobble base pair).\n\nPaired DNA and RNA molecules are comparatively stable at room temperature but the two nucleotide strands will separate above a melting point that is determined by the length of the molecules, the extent of mispairing (if any), and the GC content. Higher GC content results in higher melting temperatures; it is, therefore, unsurprising that the genomes of extremophile organisms such as \"Thermus thermophilus\" are particularly GC-rich. On the converse, regions of a genome that need to separate frequently — for example, the promoter regions for often-transcribed genes — are comparatively GC-poor (for example, see TATA box). GC content and melting temperature must also be taken into account when designing primers for PCR reactions.\n\nThe following DNA sequences illustrate pair double-stranded patterns. By convention, the top strand is written from the 5' end to the 3' end; thus, the bottom strand is written 3' to 5'.\n\nChemical analogs of nucleotides can take the place of proper nucleotides and establish non-canonical base-pairing, leading to errors (mostly point mutations) in DNA replication and DNA transcription. This is due to their isosteric chemistry. One common mutagenic base analog is 5-bromouracil, which resembles thymine but can base-pair to guanine in its enol form.\n\nOther chemicals, known as DNA intercalators, fit into the gap between adjacent bases on a single strand and induce frameshift mutations by \"masquerading\" as a base, causing the DNA replication machinery to skip or insert additional nucleotides at the intercalated site. Most intercalators are large polyaromatic compounds and are known or suspected carcinogens. Examples include ethidium bromide and acridine.\n\nAn unnatural base pair (UBP) is a designed subunit (or nucleobase) of DNA which is created in a laboratory and does not occur in nature. DNA sequences have been described which use newly created nucleobases to form a third base pair, in addition to the two base pairs found in nature, A-T (adenine – thymine) and G-C (guanine – cytosine). A few research groups have been searching for a third base pair for DNA, including teams led by Steven A. Benner, Philippe Marliere, Floyd Romesberg and Ichiro Hirao. Some new base pairs have been reported.\n\nIn 1989 Steven Benner, then at the Swiss Federal Institute of Technology in Zurich, and his team led with modified forms of cytosine and guanine into DNA molecules \"in vitro\". The nucleotides, which encoded RNA and proteins, were successfully replicated \"in vitro\". Since then, Benner's team has been trying to engineer cells that can make foreign bases from scratch, obviating the need for a feedstock.\n\nIn 2002, Ichiro Hirao’s group in Japan developed an unnatural base pair between 2-amino-8-(2-thienyl)purine (s) and pyridine-2-one (y) that functions in transcription and translation, for the site-specific incorporation of non-standard amino acids into proteins. In 2006, they created 7-(2-thienyl)imidazo[4,5-b]pyridine (Ds) and pyrrole-2-carbaldehyde (Pa) as a third base pair for replication and transcription. Afterward, Ds and 4-[3-(6-aminohexanamido)-1-propynyl]-2-nitropyrrole (Px) was discovered as a high fidelity pair in PCR amplification. In 2013, they applied the Ds-Px pair to DNA aptamer generation by \"in vitro\" selection (SELEX) and demonstrated the genetic alphabet expansion significantly augment DNA aptamer affinities to target proteins.\n\nIn 2012, a group of American scientists led by Floyd Romesberg, a chemical biologist at the Scripps Research Institute in San Diego, California, published that his team designed an unnatural base pair (UBP). The two new artificial nucleotides or \"Unnatural Base Pair\" (UBP) were named d5SICS and dNaM. More technically, these artificial nucleotides bearing hydrophobic nucleobases, feature two fused aromatic rings that form a (d5SICS–dNaM) complex or base pair in DNA. His team designed a variety of \"in vitro\" or \"test tube\" templates containing the unnatural base pair and they confirmed that it was efficiently replicated with high fidelity in virtually all sequence contexts using the modern standard \"in vitro\" techniques, namely PCR amplification of DNA and PCR-based applications. Their results show that for PCR and PCR-based applications, the d5SICS–dNaM unnatural base pair is functionally equivalent to a natural base pair, and when combined with the other two natural base pairs used by all organisms, A–T and G–C, they provide a fully functional and expanded six-letter \"genetic alphabet\".\n\nIn 2014 the same team from the Scripps Research Institute reported that they synthesized a stretch of circular DNA known as a plasmid containing natural T-A and C-G base pairs along with the best-performing UBP Romesberg's laboratory had designed, and inserted it into cells of the common bacterium \"E. coli\" that successfully replicated the unnatural base pairs through multiple generations. The transfection did not hamper the growth of the \"E. coli\" cells, and showed no sign of losing its unnatural base pairs to its natural DNA repair mechanisms. This is the first known example of a living organism passing along an expanded genetic code to subsequent generations. Romesberg said he and his colleagues created 300 variants to refine the design of nucleotides that would be stable enough and would be replicated as easily as the natural ones when the cells divide. This was in part achieved by the addition of a supportive algal gene that expresses a nucleotide triphosphate transporter which efficiently imports the triphosphates of both d5SICSTP and dNaMTP into \"E. coli\" bacteria. Then, the natural bacterial replication pathways use them to accurately replicate a plasmid containing d5SICS–dNaM. Other researchers were surprised that the bacteria replicated these human-made DNA subunits.\n\nThe successful incorporation of a third base pair is a significant breakthrough toward the goal of greatly expanding the number of amino acids which can be encoded by DNA, from the existing 20 amino acids to a theoretically possible 172, thereby expanding the potential for living organisms to produce novel proteins. The artificial strings of DNA do not encode for anything yet, but scientists speculate they could be designed to manufacture new proteins which could have industrial or pharmaceutical uses. Experts said the synthetic DNA incorporating the unnatural base pair raises the possibility of life forms based on a different DNA code.\n\nThe following abbreviations are commonly used to describe the length of a D/RNA molecule:\n\nFor case of single-stranded DNA/RNA units of nucleotides are used, abbreviated nt (or knt, Mnt, Gnt), as they are not paired.\nFor distinction between units of computer storage and bases kbp, Mbp, Gbp, etc. may be used for base pairs.\n\nThe centimorgan is also often used to imply distance along a chromosome, but the number of base pairs it corresponds to varies widely. In the Human genome, the centimorgan is about 1 million base pairs.\n\n\n\n", "id": "4292", "title": "Base pair"}
{"url": "https://en.wikipedia.org/wiki?curid=12388", "text": "Genome\n\nIn terms of modern molecular biology and genetics, a genome is the genetic material of an organism. It consists of DNA (or RNA in RNA viruses). The genome includes both the genes (the coding regions) and the noncoding DNA, as well as the genetic material of the mitochondria and chloroplasts.\n\nA genome is all the genetic information of an organism. For example, the human genome is analogous to the instructions stored in a cookbook. Just as a cookbook gives the instructions needed to make a range of meals including a holiday feast or a summer picnic, the human genome contains all the instructions needed to make the full range of human cell types including muscle cells or neurons. \n\nThe term \"genome\" was created in 1920 by Hans Winkler, professor of botany at the University of Hamburg, Germany. The Oxford Dictionary suggests the name is a blend of the words \"gene\" and \"chromosome\". However, see omics for a more thorough discussion. A few related \"-ome\" words already existed—such as \"biome\", \"rhizome\", forming a vocabulary into which \"genome\" fits systematically.\n\nA genome sequence is the complete list of the nucleotides (A, C, G, and T for DNA genomes) that make up all the chromosomes of an individual or a species. Within a species, the vast majority of nucleotides are identical between individuals, but sequencing multiple individuals is necessary to understand the genetic diversity. \nIn 1976, Walter Fiers at the University of Ghent (Belgium) was the first to establish the complete nucleotide sequence of a viral RNA-genome (Bacteriophage MS2). The next year, Fred Sanger completed the first DNA-genome sequence: Phage Φ-X174, of 5386 base pairs. The first complete genome sequences among all three domains of life were released within a short period during the mid-1990s: The first bacterial genome to be sequenced was that of Haemophilus influenzae, completed by a team at The Institute for Genomic Research in 1995. A few months later, the first eukaryotic genome was completed, with sequences of the 16 chromosomes of budding yeast \"Saccharomyces cerevisiae\" published as the result of a European-led effort begun in the mid-1980s. The first genome sequence for an archaeon, \"Methanococcus jannaschii\", was completed in 1996, again by The Institute for Genomic Research.\n\nThe development of new technologies has made genome sequencing dramatically cheaper and easier, and the number of complete genome sequences is growing rapidly. The US National Institutes of Health maintains one of several comprehensive databases of genomic information. Among the thousands of completed genome sequencing projects include those for rice, a mouse, the plant \"Arabidopsis thaliana\", the puffer fish, and the bacteria E. coli. In December 2013, scientists first sequenced the entire \"genome\" of a Neanderthal, an extinct species of humans. The genome was extracted from the toe bone of a 130,000-year-old Neanderthal found in a Siberian cave.\n\nNew sequencing technologies, such as massive parallel sequencing have also opened up the prospect of personal genome sequencing as a diagnostic tool, as pioneered by Manteia Predictive Medicine. A major step toward that goal was the completion in 2007 of the full genome of James D. Watson, one of the co-discoverers of the structure of DNA.\n\nWhereas a genome sequence lists the order of every DNA base in a genome, a genome map identifies the landmarks. A genome map is less detailed than a genome sequence and aids in navigating around the genome. The Human Genome Project was organized to map and to sequence the human genome. A fundamental step in the project was the release of a detailed genomic map by Jean Weissenbach and his team at the Genoscope in Paris.\n\nReference genome sequences and maps continue to be updated, removing errors and clarifying regions of high allelic complexity. The decreasing cost of genomic mapping has permitted genealogical sites to offer it as a service, to the extent that one may submit one's genome to crowd sourced scientific endeavours such as DNA.land at the New York Genome Center, an example both of the economies of scale and of citizen science.\n\nViral genomes can be composed of either RNA or DNA. The genomes of RNA viruses can be either single-stranded or double-stranded RNA, and may contain one or more separate RNA molecules. DNA viruses can have either single-stranded or double-stranded genomes. Most DNA virus genomes are composed of a single, linear molecule of DNA, but some are made up of a circular DNA molecule.\n\nProkaryotes and eukaryotes have DNA genomes. Archaea have a single circular chromosome. Most bacteria also have a single circular chromosome; however, some bacterial species have linear chromosomes or multiple chromosomes. If the DNA is replicated faster than the bacterial cells divide, multiple copies of the chromosome can be present in a single cell. Most prokaryotes have very little repetitive DNA in their genomes.\n\nSome bacteria have auxiliary genetic material, which is carried in plasmids.\n\nEukaryotic genomes are composed of one or more linear DNA chromosomes. The number of chromosomes varies widely from Jack jumper ants and an asexual nemotode, which each have only one pair, to a fern species that has 720 pairs. A typical human cell has two copies of each of 22 autosomes, one inherited from each parent, plus two sex chromosomes, making it diploid. Gametes, such as ova, sperm, spores, and pollen, are haploid, meaning they carry only one copy of each chromosome.\n\nIn addition to the chromosomes in the nucleus, organelles such as the chloroplasts and mitochondria have their own DNA. Mitochondria are sometimes said to have their own genome often referred to as the \"mitochondrial genome\". The DNA found within the chloroplast may be referred to as the \"plastome\". Like the bacteria they originated from, mitochondria and chloroplasts have a circular chromosome.\n\nUnlike prokaryotes, eukaryotes have exon-intron organization of protein coding genes and variable amounts of repetitive DNA. In mammals and plants, the majority of the genome is composed of repetitive DNA.\n\nDNA sequences that carry the instructions to make proteins are coding sequences. The proportion of the genome occupied by coding sequences varies widely. A bigger genome does not mean more genes, and the proportion of non-repetitive DNA decreases along with increasing genome size in complex eukaryotes.\n\nMost bacteria have little or no repetitive DNA, hence their typical protein coding capacity is in the range of 85-90%. However, some symbiotic bacteria (e.g. \"Serratia symbiotica\") have reduced genomes and a high fraction of pseudogenes: only ~40% of their DNA encodes proteins. Simple eukaryotes such as \"C. elegans\" and fruit fly, possess more non-repetitive DNA than repetitive DNA. Higher eukaryotes tend to have more repetitive DNA than non-repetitive ones. In some plants and amphibians, the proportion of repetitive DNA is more than 80%. Similarly, only 2% of the human genome codes for proteins. \nNoncoding sequences include introns, sequences for non-coding RNAs, regulatory regions, and repetitive DNA. Noncoding sequences make up 98% of the human genome. There are two categories of repetitive DNA in the genome: tandem repeats and interspersed repeats.\n\nTandem repeats are short, non-coding sequences that are repeated head-to-tail. Microsatellites consist of 2-5 basepair repeats, while minisatellite repeats are 30-35 bp. Tandem repeats make up about 4% of the human genome and 9% of the fruit fly genome. Tandom repeats can be functional. For example, telomeres are composed of the tandem repeat TTAGGG in mammals, and they play an important role in protecting the ends of the chromosome. \n\nIn other cases, expansions in the number of tandem repeats in exons or introns can cause disease. For example, the human gene huntingtin typically contains 6-29 tandem repeats of the nucleotides CAG (encoding a polyglutamine tract). An expansion to over 36 repeats results in Huntington's disease, a neurodegenerative disease. Twenty human disorders are known to result from similar tandem repeat expansions in various genes. The mechanism by which proteins with expanded polygulatamine tracts cause death of neurons is not fully understood. One possibility is that the proteins fail to fold properly and avoid degradation, instead accumulating in aggregates that also sequester important transcription factors, thereby altering gene expression . \n\nTandem repeats are usually caused by slippage during replication, unequal crossing-over and gene conversion.\n\nTransposable elements (TEs) are sequences of DNA with a defined structure that are able to change their location in the genome. TEs are categorized as either class I TEs, which replicate by a copy-and-paste mechanism, or class II TEs, which can be excised from the genome and inserted at a new location. \n\nThe movement of TEs is a driving force of genome evolution in eukaryotes because their insertion can disrupt gene functions, homologous recombination between TEs can produce duplications, and TE can shuffle exons and regulatory sequences to new locations.\n\nRetrotransposons can be transcribed into RNA, which are then duplicated at another site into the genome. Retrotransposons can be divided into Long terminal repeats (LTRs) and Non-Long Terminal Repeats (Non-LTR).\n\nLong terminal repeats (LTRs) are derived from ancient retroviral infections, so they encode proteins related to retroviral proteins including gag (structural proteins of the virus), pol (reverse transcriptase and integrase), pro (protease), and in some cases env (envelope) genes. These genes are flanked by long repeats at both 5' and 3' ends. It has been reported that LTRs consist of the largest fraction in most plant genome and might account for the huge variation in genome size.\n\nNon-long terminal repeats (Non-LTRs) are classified as long interspersed elements (LINEs), short interspersed elements (SINEs), and Penelope-like elements. In \"Dictyostelium discoideum\", there is another DIRS-like elements belong to Non-LTRs. Non-LTRs are widely spread in eukaryotic genomes.\n\nLong interspersed elements (LINEs) encode genes for reverse transcriptase and endonuclease, making them autonomous transposable elements. The human genome has around 500,000 LINEs, taking around 17% of the genome.\n\nShort interspersed elements (SINEs) are usually less than 500 base pairs and are non-autonomous, so they rely on the proteins encoded by LINEs for transposition. The Alu element is the most common SINE found in primates. It is about 350 base pairs and occupies about 11% of the human genome with around 1,500,000 copies.\n\nDNA transposons encode a transposase enzyme between inverted terminal repeats. When expressed, the transposase recognizes the terminal inverted repeats that flank the transposon and catalyzes its excision and reinsertion in a new site. This cut-and-paste mechanism typically reinserts transposons near their original location (within 100kb). DNA transposons are found in bacteria and make up 3% of the human genome and 12% of the genome of the roundworm \"C. elegans\".\n\nGenome size is the total number of DNA base pairs in one copy of a haploid genome. In humans, the nuclear genome comprises approximately 3.2 billion nucleotides of DNA, divided into 24 linear molecules, the shortest 50 000 000 nucleotides in length and the longest 260 000 000 nucleotides, each contained in a different chromosome. The genome size is positively correlated with the morphological complexity among prokaryotes and lower eukaryotes; however, after mollusks and all the other higher eukaryotes above, this correlation is no longer effective. This phenomenon also indicates the mighty influence coming from repetitive DNA act on the genomes.\n\nSince genomes are very complex, one research strategy is to reduce the number of genes in a genome to the bare minimum and still have the organism in question survive. There is experimental work being done on minimal genomes for single cell organisms as well as minimal genomes for multi-cellular organisms (see Developmental biology). The work is both \"in vivo\" and \"in silico\".\n\nHere is a table of some significant or representative genomes. See #See also for lists of sequenced genomes.\n\nAll the cells of an organism originate from a single cell, so they are expected to have identical genomes; however, in some cases, differences arise. Both the process of copying DNA during cell division and exposure to environmental mutagens can result in mutations in somatic cells. In some cases, such mutations lead to cancer because they cause cells to divide more quickly and invade surrounding tissues. In certain lymphocytes in the human immune system, V(D)J recombination generates different genomic sequences such that each cell produces a unique antibody or T cell receptors.\n\nDuring meiosis, diploid cells divide twice to produce haploid germ cells. During this process, recombination results in a reshuffling of the genetic material from homologous chromosomes so each gamete has a unique genome.\n\nGenomes are more than the sum of an organism's genes and have traits that may be measured and studied without reference to the details of any particular genes and their products. Researchers compare traits such as karyotype (chromosome number), genome size, gene order, codon usage bias, and GC-content to determine what mechanisms could have produced the great variety of genomes that exist today (for recent overviews, see Brown 2002; Saccone and Pesole 2003; Benfey and Protopapas 2004; Gibson and Muse 2004; Reese 2004; Gregory 2005).\n\nDuplications play a major role in shaping the genome. Duplication may range from extension of short tandem repeats, to duplication of a cluster of genes, and all the way to duplication of entire chromosomes or even entire genomes. Such duplications are probably fundamental to the creation of genetic novelty.\n\nHorizontal gene transfer is invoked to explain how there is often an extreme similarity between small portions of the genomes of two organisms that are otherwise very distantly related. Horizontal gene transfer seems to be common among many microbes. Also, eukaryotic cells seem to have experienced a transfer of some genetic material from their chloroplast and mitochondrial genomes to their nuclear chromosomes. Recent empirical data suggest an important role of viruses and sub-viral RNA-networks to represent a main driving role to generate genetic novelty and natural genome editing.\n\nWorks of science fiction illustrate concerns about the availability of genome sequences. \n\nMichael Crichton's 1990 novel \"Jurassic Park\" and the subsequent film tell the story of a billionaire who creates a theme park of cloned dinosaurs on a remote island, with disastrous outcomes. A geneticist extracts dinosaur DNA from the blood of ancient mosquitoes and fills in the gaps with DNA from modern species to create several species of dinosaurs. A chaos theorist is asked to give his expert opinion on the safety of engineering an ecosystem with the dinosaurs, and he repeatedly warns that the outcomes of the project will be unpredictable and ultimately uncontrollable. These warnings about the perils of using genomic information are a major theme of the book.\n\nThe 1997 film \"Gattaca\" is set in a futurist society where genomes of children are engineered to contain the most ideal combination of their parents' traits, and metrics such as risk of heart disease and predicted life expectancy are documented for each person based on their genome. People conceived outside of the eugenics program, known as \"In-Valids\" suffer discrimination and are relegated to menial occupations. The protagonist of the film is an In-Valid who works to defy the supposed genetic odds and achieve his dream of working as a space navigator. The film warns against a future where genomic information fuels prejudice and extreme class differences between those who can and can't afford genetically engineered children.\n\n\n\n", "id": "12388", "title": "Genome"}
{"url": "https://en.wikipedia.org/wiki?curid=49665885", "text": "Circulating free DNA\n\nCirculating free DNA (cfDNA) are degraded DNA fragments released to the blood plasma. cfDNA can be used to describe various forms of DNA freely circulating the bloodstream, including circulating tumor DNA (ctDNA) and cell-free fetal DNA (cffDNA). Elevated levels of cfDNA are observed in cancer, especially in advanced disease. There is evidence that cfDNA becomes increasingly frequent in circulation with the onset of age. \n", "id": "49665885", "title": "Circulating free DNA"}
{"url": "https://en.wikipedia.org/wiki?curid=45783", "text": "Genotype–phenotype distinction\n\nThe genotype–phenotype distinction is drawn in genetics. \"Genotype\" is an organism's full hereditary information. \"Phenotype\" is an organism's actual observed properties, such as morphology, development, or behavior. This distinction is fundamental in the study of inheritance of traits and their evolution.\n\nIt is the organism's physical properties which directly determine its chances of survival and reproductive output, while the inheritance of physical properties occurs only as a secondary consequence of the inheritance of genes. Therefore, to properly understand the theory of evolution via natural selection, one must understand the genotype–phenotype distinction. The genes contribute to a trait, and the phenotype is the observable expression of the genes (and therefore the genotype that affects the trait). Say a white mouse had the recessive genes that caused the genes that cause the color of the mouse to be inactive (so \"cc\"). Its genotype would be responsible for its phenotype (the white color).\n\nThe mapping of a set of genotypes to a set of phenotypes is sometimes referred to as the genotype–phenotype map. \n\nAn organism's genotype is a major (the largest by far for morphology) influencing factor in the development of its phenotype, but it is not the only one. Even two organisms with identical genotypes normally differ in their phenotypes. One experiences this in everyday life with monozygous (i.e. identical) twins. Identical twins share the same genotype, since their genomes are identical; but they never have the same phenotype, although their phenotypes may be very similar. This is apparent in the fact that their mothers and close friends can always tell them apart, even though others might not be able to see the subtle differences. Further, identical twins can be distinguished by their fingerprints, which are never completely identical.\n\nThe concept of phenotypic plasticity defines the degree to which an organism's phenotype is determined by its genotype. A high level of plasticity means that environmental factors have a strong influence on the particular phenotype that develops. If there is little plasticity, the phenotype of an organism can be reliably predicted from knowledge of the genotype, regardless of environmental peculiarities during development. An example of high plasticity can be observed in larval newts: when these larvae sense the presence of predators such as dragonflies, they develop larger heads and tails relative to their body size and display darker pigmentation. Larvae with these traits have a higher chance of survival when exposed to the predators, but grow more slowly than other phenotypes.\n\nIn contrast to phenotypic plasticity, the concept of genetic canalization addresses the extent to which an organism's phenotype allows conclusions about its genotype. A phenotype is said to be canalized if mutations (changes in the genome) do not noticeably affect the physical properties of the organism. This means that a canalized phenotype may form from a large variety of different genotypes, in which case it is not possible to exactly predict the genotype from knowledge of the phenotype (i.e. the genotype–phenotype map is not invertible). If canalization is not present, small changes in the genome have an immediate effect on the phenotype that develops.\n\nThe terms \"genotype\" and \"phenotype\" were created by Wilhelm Johannsen in 1911, although the meaning of the terms and the significance of the distinction have evolved since they were introduced.\n\nAccording to Lewontin, the theoretical task for population genetics is a process in two spaces: a \"genotypic space\" and a \"phenotypic space\". The challenge of a \"complete\" theory of population genetics is to provide a set of laws that predictably map a population of genotypes (\"G\") to a phenotype space (\"P\"), where selection takes place, and another set of laws that map the resulting population (\"P\") back to genotype space (\"G\") where Mendelian genetics can predict the next generation of genotypes, thus completing the cycle. Even leaving aside for the moment the non-Mendelian aspects of molecular genetics, this is clearly a gargantuan task. Visualizing this transformation schematically:\n\n(adapted from Lewontin 1974, p. 12). \"T\" represents the genetic and epigenetic laws, the aspects of functional biology, or development, that transform a genotype into phenotype. We will refer to this as the \"genotype-phenotype map\". \"T\" is the transformation due to natural selection, \"T\" are epigenetic relations that predict genotypes based on the selected phenotypes and finally \"T\" the rules of Mendelian genetics.\n\nIn practice, there are two bodies of evolutionary theory that exist in parallel, traditional population genetics operating in the genotype space and the biometric theory used in plant and animal breeding, operating in phenotype space. The missing part is the mapping between the genotype and phenotype space. This leads to a \"sleight of hand\" (as Lewontin terms it) whereby variables in the equations of one domain, are considered parameters or \"constants\", where, in a full-treatment they would be transformed themselves by the evolutionary process and are in reality \"functions\" of the state variables in the other domain. The \"sleight of hand\" is assuming that we know this mapping. Proceeding as if we do understand it is enough to analyze many cases of interest. For example, if the phenotype is almost one-to-one with genotype (sickle-cell disease) or the time-scale is sufficiently short, the \"constants\" can be treated as such; however, there are many situations where it is inaccurate.\n\n", "id": "45783", "title": "Genotype–phenotype distinction"}
{"url": "https://en.wikipedia.org/wiki?curid=14769961", "text": "BAG3\n\nBAG family molecular chaperone regulator 3 is a protein that in humans is encoded by the \"BAG3\" gene. BAG3 is involved in chaperone-assisted selective autophagy (CASA).\n\nBAG proteins compete with Hip-1 for binding to the Hsc70/Hsp70 ATPase domain and promote substrate release. All the BAG proteins have an approximately 45-amino acid BAG domain near the C terminus but differ markedly in their N-terminal regions. The protein encoded by this gene contains a WW domain in the N-terminal region and a BAG domain in the C-terminal region. The BAG domains of BAG1, BAG2, and BAG3 interact specifically with the Hsc70 ATPase domain \"in vitro\" and in mammalian cells. All 3 proteins bind with high affinity to the ATPase domain of Hsc70 and inhibit its chaperone activity in a Hip-repressible manner.\n\nBAG gene has been implicated in age related neurodegenerative diseases such as Alzheimer's. It has been demonstrated that BAG1 and BAG 3 regulate the proteasomal and lysosomal protein elimination pathways, respectively. It has also been shown to be a cause of familial dilated cardiomyopathy.\nThat BAG3 mutations are responsible for familial dilated cardiomyopathy is confirmed by another study describing 6 new molecular variants (2 missense and 4 premature Stops\n). Moreover, the same publication reported that BAG3 polymorphisms are also associated with sporadic forms of the disease together with HSPB7 locus.\n\nIn muscle cells, BAG3 cooperates with the molecular chaperones Hsc70 and HspB8 to induce the degradation of mechanically damaged cytoskeleton components in lysosomes. This process is called chaperone-assisted selective autophagy (CASA) and is essential for maintaining muscle activity in flies, mice and men.\n\nBAG3 is able to stimulate the expression of cytoskeleton proteins in response to mechanical tension by activating the transcription regulators YAP1 and WWTR1. BAG3 balances protein synthesis and protein degradation under mechanical stress.\n\nPLCG1 has been shown to interact with:\n\n", "id": "14769961", "title": "BAG3"}
{"url": "https://en.wikipedia.org/wiki?curid=24310717", "text": "Somatic recombination\n\nSomatic recombination, as opposed to the genetic recombination that occurs in meiosis, is an alteration of the DNA of a somatic cell that is inherited by its daughter cells. The term is usually reserved for large-scale alterations of DNA such as chromosomal translocations and deletions and not applied to point mutations. Somatic recombination occurs physiologically in the assembly of the B cell receptor and T-cell receptor genes (V(D)J recombination), as well as in the class switching of immunoglobulins. Somatic recombination is also important in the process of carcinogenesis.\n", "id": "24310717", "title": "Somatic recombination"}
{"url": "https://en.wikipedia.org/wiki?curid=29291510", "text": "Structural variation\n\nStructural variation (also genomic structural variation) is the variation in structure of an organism's chromosome. It consists of many kinds of variation in the genome of one species, and usually includes microscopic and submicroscopic types, such as deletions, duplications, copy-number variants, insertions, inversions and translocations. Typically a structure variation affects a sequence length about 1Kb to 3Mb, which is larger than SNPs and smaller than chromosome abnormality (though the definitions have some overlap). The definition of structural variation does not imply anything about frequency or phenotypical effects. Many structural variants are associated with genetic diseases, however many are not. Recent research about SVs indicates that SVs are more difficult to detect than SNPs. Approximately 13% of the human genome are defined as structurally variant in the normal population, and there are at least 240 genes that exist as homozygous deletion polymorphisms in human populations, suggesting these genes are dispensable in humans. Rapidly accumulating evidence indicates that structural variations can comprise millions of nucleotides of heterogeneity within every genome, and are likely to make an important contribution to human diversity and disease susceptibility.\n\nMicroscopic means that it can be detected with optical microscopes, such as aneuploidies, marker chromosome, gross rearrangements and variation in chromosome size. The frequency in human population is thought to be underestimated due to the fact that some of these are not actually easy to identify. These structural abnormalities exist in 1 every 375 live births by putative information.\n\nSub-microscopic structural variants are much harder to detect owing to their small size. The first study in 2004 that used DNA microarrays could detect tens of genetic loci that exhibited copy number variation, deletions and duplications, greater than 100 kilobases in the human genome. However, by 2015 whole genome sequencing studies could detect around 5,000 of structural variants as small as 100 base pairs encompassing approximately 20 megabases in each individual genome. These structural variants include deletions, tandem duplications, inversions, mobile element insertions. The mutation rate is also much higher than microscopic structural variants, estimated by two studies at 16% and 20% respectively, both of which are probably underestimates due to the challenges of accurately detecting structural variants. It has also been shown that the generation of spontaneous structural variants significantly increases the likelihood of generating further spontaneous single nucleotide variants or indels within 100 kilobases of the structural variation event.\n\nCopy-number variation (CNV) is a large category of structural variation, which includes insertions, deletions and duplications. In recent studies, copy-number variations are tested on people who do not have genetic diseases, using methods that are used for quantitative SNP genotyping. Results show that 28% of the suspected regions in the individuals actually do contain copy number variations. Also, CNVs in human genome affect more nucleotides than Single Nucleotide Polymorphism (SNP).\nIt is also noteworthy that many of CNVs are not in coding regions. Because CNVs are usually caused by unequal recombination, widespread similar sequences such as LINEs and SINEs may be a common mechanism of CNV creation.\n\nThere are several inversions known which are related to human disease. For instance, recurrent 400kb inversion in factor VIII gene is a common cause of haemophilia A, and smaller inversions affecting idunorate 2-sulphatase (IDS) will cause Hunter syndrome. More examples include Angelman syndrome and Sotos syndrome. However, recent research shows that one person can have 56 putative inversions, thus the non-disease inversions are more common than previously supposed. Also in this study it's indicated that inversion breakpoints are commonly associated with segmental duplications. One 900 kb inversion in the chromosome 17 is under positive selection and are predicted to increase its frequency in European population.\n\nMore complex structural variants can occur include a combination of the above in a single event. The most common type of complex structural variation are non-tandem duplications, where sequence is duplicated and inserted in inverted or direct orientation into another part of the genome. Other classes of complex structural variant include deletion-inversion-deletions, duplication-inversion-duplications, and tandem duplications with nested deletions.\nThere are also cryptic translocations and segmental uniparental disomy (UPD). There are increasing reports of these variations, but are more difficult to detect than traditional variations because these variants are balanced and array-based or PCR-based methods are not able to locate them.\n\nSome genetic diseases are suspected to be caused by structural variations, but the relation is not very certain. It is not plausible to divide these variants into two classes as \"normal\" or \"disease\", because the actual output of the same variant will also vary. Also, a few of the variants are actually positively selected for (mentioned above).\nA series of studies have shown that gene disrupting spontaneous (\"de novo\") CNVs disrupt genes approximately four times more frequently in autism than in controls and contribute to approximately 5-10% of cases. Inherited variants also contribute to around 5-10% of cases of autism.\n\nStructural variations also have its function in population genetics. Different frequency of a same variation can be used as a genetic mark to infer relationship between populations in different areas. A complete comparison between human and chimpanzee structural variation also suggested that some of these may be fixed in one species because of its adaptative function. There are also deletions related to resistance against malaria and AIDS. Also, some highly variable segments are thought to be caused by balancing selection, but there are also studies against this hypothesis.\n\nSome of genome browsers and bioinformatic databases have a list of structural variations in human genome with an emphasis on CNVs, and can show them in the genome browsing page, for example, UCSC Genome Browser. Under the page viewing a part of the genome, there are \"Common Cell CNVs\" and \"Structural Var\" which can be enabled.\nOn NCBI, there is a special page for structural variation. In that system, both \"inner\" and \"outer\" coordinates are shown; they are both not actual breakpoints, but surmised minimal and maximum range of sequence affected by the structural variation. The types are classified as insertion, loss, gain, inversion, LOH, everted, transchr and UPD.\n\nSoftware using Next-generation sequencing data to detect or genotype structural variations.\n", "id": "29291510", "title": "Structural variation"}
{"url": "https://en.wikipedia.org/wiki?curid=50591385", "text": "Genotyping by sequencing\n\nIn the field of genetic sequencing, genotyping by sequencing, also called GBS, is a method to discover single nucleotide polymorphisms (SNP) in order to perform genotyping studies, such as genome-wide association studies (GWAS). GBS uses restriction enzymes to reduce genome complexity and genotype multiple DNA samples. After digestion, PCR is performed to increase fragments pool and then GBS libraries are sequenced using next generation sequencing technologies, usually resulting in about 100bp single-end reads. It is relatively inexpensive and has been used in plant breeding. Although GBS presents an approach similar to restriction-site-associated DNA sequencing (RAD-seq) method, they differ in some substantial ways.\n\nThe method was first described by Elshire et al. (2011). In summary, high molecular weight DNAs are extracted and digested using a specific restriction enzyme (RE) previously defined by cutting frequently in the major repetitive fraction of the genome. \"ApeKI\" is the most used RE. Barcode adapters are then ligated to sticky ends and PCR amplification is performed. Next-generation sequencing technology is performed resulting in about 100 bp single-end reads. Raw sequence data are filtered and aligned to a reference genome using usually Burrows-Wheeler alignment tool (BWA) or Bowtie 2. The next step is to identify SNPs from aligned tags and score all discovered SNPs for various coverage, depth and genotypic statistics. Once a large-scale, species-wide SNP production has been run, it is possible to quickly call known SNPs in newly sequenced samples.\n", "id": "50591385", "title": "Genotyping by sequencing"}
{"url": "https://en.wikipedia.org/wiki?curid=50454123", "text": "W-test\n\nIn statistics, the W-test is designed to test the distributional differences between cases and controls for categorical variable set, which can be a single SNP, SNP-SNP, or SNP-environment pairs. It takes a combined log of odds ratio form, calculated from the contingency table of the variable set. The test inherits a chi-squared distribution with data-set adaptive degrees of freedom \"f\", estimated from smaller bootstrapped samples of the data. The flexible and data-corrected probability distribution allows W-test to give relatively accurate p-values under complex genetic architectures.\n\nTheoretically, the test is not restricted to pairwise interactions, and can go to higher order if sample size of the data can support it. The W-test's application for pairwise interaction effect has been tested in common genome-wide association study (GWAS) dataset with less than 5,000 subjects [1]. Since it corrects for probability distribution bias due to sparse data through the bootstrapped parameters, it has persistent power in low frequency variant environment, when the minor allele frequency (MAF) of single-nucleotide polymorphism (SNP) is between 1% and 5%. \n\nThe W-test C++ software, linux version and R package are available from the wtest official website.\n\n", "id": "50454123", "title": "W-test"}
{"url": "https://en.wikipedia.org/wiki?curid=12385", "text": "Genetic code\n\nThe genetic code is the set of rules by which information encoded within genetic material (DNA or mRNA sequences) is translated into proteins by living cells. Translation is accomplished by the ribosome, which links amino acids in an order specified by messenger RNA (mRNA), using transfer RNA (tRNA) molecules to carry amino acids and to read the mRNA three nucleotides at a time. The genetic code is highly similar among all organisms and can be expressed in a simple table with 64 entries.\n\nThe code defines how sequences of nucleotide triplets, called \"codons\", specify which amino acid will be added next during protein synthesis. With some exceptions, a three-nucleotide codon in a nucleic acid sequence specifies a single amino acid. The vast majority of genes are encoded with a single scheme (see the RNA codon table). That scheme is often referred to as the canonical or standard genetic code, or simply \"the\" genetic code, though variant codes (such as in human mitochondria) exist.\n\nWhile the \"genetic code\" determines a protein's amino acid sequence, other genomic regions determine when and where these proteins are produced according to various \"gene regulatory codes\".\n\nEfforts to understand how proteins are encoded began after DNA's structure was discovered in 1953. George Gamow postulated that sets of three bases must be employed to encode the 20 standard amino acids used by living cells to build proteins, which would allow a maximum of 64 amino acids.\n\nThe Crick, Brenner et al. experiment first demonstrated that codons consist of three DNA bases. Marshall Nirenberg and Heinrich J. Matthaei were the first to reveal the nature of a codon in 1961.\n\nThey used a cell-free system to translate a poly-uracil RNA sequence (i.e., UUUUU...) and discovered that the polypeptide that they had synthesized consisted of only the amino acid phenylalanine. They thereby deduced that the codon UUU specified the amino acid phenylalanine.\n\nThis was followed by experiments in Severo Ochoa's laboratory that demonstrated that the poly-adenine RNA sequence (AAAAA...) coded for the polypeptide poly-lysine and that the poly-cytosine RNA sequence (CCCCC...) coded for the polypeptide poly-proline. Therefore, the codon AAA specified the amino acid lysine, and the codon CCC specified the amino acid proline. Using various copolymers most of the remaining codons were then determined.\n\nSubsequent work by Har Gobind Khorana identified the rest of the genetic code. Shortly thereafter, Robert W. Holley determined the structure of transfer RNA (tRNA), the adapter molecule that facilitates the process of translating RNA into protein. This work was based upon Ochoa's earlier studies, yielding the latter the Nobel Prize in Physiology or Medicine in 1959 for work on the enzymology of RNA synthesis.\n\nExtending this work, Nirenberg and Philip Leder revealed the code's triplet nature and deciphered its codons. In these experiments, various combinations of mRNA were passed through a filter that contained ribosomes, the components of cells that translate RNA into protein. Unique triplets promoted the binding of specific tRNAs to the ribosome. Leder and Nirenberg were able to determine the sequences of 54 out of 64 codons in their experiments. Khorana, Holley and Nirenberg received the 1968 Nobel for their work.\n\nThe three stop codons were named by discoverers Richard Epstein and Charles Steinberg. \"Amber\" was named after their friend Harris Bernstein, whose last name means \"amber\" in German. The other two stop codons were named \"ochre\" and \"opal\" in order to keep the \"color names\" theme.\n\nIn a broad academic audience, the concept of the evolution of the genetic code from the original and ambiguous genetic code to a well-defined (\"frozen\") code with the repertoire of 20 (+2) canonical amino acids is widely accepted.\nHowever, there are different opinions, concepts, approaches and ideas, which is the best way to change it experimentally. Even models are proposed that predict \"entry points\" for synthetic amino acid invasion of the genetic code.\n\nSince 2001, 40 non-natural amino acids have been added into protein by creating a unique codon (recoding) and a corresponding transfer-RNA:aminoacyl – tRNA-synthetase pair to encode it with diverse physicochemical and biological properties in order to be used as a tool to exploring protein structure and function or to create novel or enhanced proteins.\nH. Murakami and M. Sisido extended some codons to have four and five bases. Steven A. Benner constructed a functional 65th (\"in vivo\") codon.\n\nIn 2015 N. Budisa, D. Söll and co-workers reported the full substitution of all 20,899 tryptophan residues (UGG codons) with unnatural thienopyrrole-alanine in the genetic code of the bacterium \"Escherichia coli\".\n\nIn 2016 the first stable semisynthetic organism was created. It was a (single cell) bacterium with two synthetic bases (called X and Y). The bases survived cell division.\n\nIn 2017 a mouse engineered with an extended genetic code that can produce proteins with unnatural amino acids was reported.\n\nA reading frame is defined by the initial triplet of nucleotides from which translation starts. It sets the frame for a run of successive, non-overlapping codons, which is known as an \"open reading frame\" (ORF). For example, the string 5'-AAATGAACG-3' (see figure), if read from the first position, contains the codons AAA, TGA, and ACG ; if read from the second position, it contains the codons AAT and GAA ; and if read from the third position, it contains the codons ATG and AAC. Every sequence can, thus, be read in its 5' → 3' direction in three reading frames, each producing a possibly distinct amino acid sequence: in the given example, Lys (K)-Trp (W)-Thr (T), Asn (N)-Glu (E), or Met (M)-Asn (N), respectively (when translating with the vertebrate mitochondrial code). When DNA is double-stranded, six possible reading frames are defined, three in the forward orientation on one strand and three reverse on the opposite strand. Protein-coding frames are defined by a start codon, usually the first AUG (ATG) codon in the RNA (DNA) sequence.\n\nIn eukaryotes, ORFs in exons are often interrupted by introns.\n\nTranslation starts with a chain-initiation codon or start codon. The start codon alone is not sufficient to begin the process. Nearby sequences such as the Shine-Dalgarno sequence in \"E. coli\" and initiation factors are also required to start translation. The most common start codon is AUG, which is read as methionine or, in bacteria, as formylmethionine. Alternative start codons depending on the organism include \"GUG\" or \"UUG\"; these codons normally represent valine and leucine, respectively, but as start codons they are translated as methionine or formylmethionine.\n\nThe three stop codons have names: UAG is \"amber\", UGA is \"opal\" (sometimes also called \"umber\"), and UAA is \"ochre\". Stop codons are also called \"termination\" or \"nonsense\" codons. They signal release of the nascent polypeptide from the ribosome because no cognate tRNA has anticodons complementary to these stop signals, allowing a release factor to bind to the ribosome instead.\n\nDuring the process of DNA replication, errors occasionally occur in the polymerization of the second strand. These errors, mutations, can affect an organism's phenotype, especially if they occur within the protein coding sequence of a gene. Error rates are typically 1 error in every 10–100 million bases—due to the \"proofreading\" ability of DNA polymerases.\n\nMissense mutations and nonsense mutations are examples of point mutations that can cause genetic diseases such as sickle-cell disease and thalassemia respectively. Clinically important missense mutations generally change the properties of the coded amino acid residue among basic, acidic, polar or non-polar states, whereas nonsense mutations result in a stop codon.\n\nMutations that disrupt the reading frame sequence by indels (insertions or deletions) of a non-multiple of 3 nucleotide bases are known as frameshift mutations. These mutations usually result in a completely different translation from the original, and likely cause a stop codon to be read, which truncates the protein. These mutations may impair the protein's function and are thus rare in \"in vivo\" protein-coding sequences. One reason inheritance of frameshift mutations is rare is that, if the protein being translated is essential for growth under the selective pressures the organism faces, absence of a functional protein may cause death before the organism becomes viable. Frameshift mutations may result in severe genetic diseases such as Tay-Sachs disease.\n\nAlthough most mutations that change protein sequences are harmful or neutral, some mutations have benefits. These mutations may enable the mutant organism to withstand particular environmental stresses better than wild type organisms, or reproduce more quickly. In these cases a mutation will tend to become more common in a population through natural selection. Viruses that use RNA as their genetic material have rapid mutation rates, which can be an advantage, since these viruses thereby evolve rapidly, and thus evade the immune system defensive responses. In large populations of asexually reproducing organisms, for example, \"E. coli\", multiple beneficial mutations may co-occur. This phenomenon is called clonal interference and causes competition among the mutations.\n\nDegeneracy is the redundancy of the genetic code. This term was given by Bernfield and Nirenberg. The genetic code has redundancy but no ambiguity (see the codon tables below for the full correlation). For example, although codons GAA and GAG both specify glutamic acid (redundancy), neither specifies another amino acid (no ambiguity). The codons encoding one amino acid may differ in any of their three positions. For example, the amino acid leucine is specified by YUR or CUN (UUA, UUG, CUU, CUC, CUA, or CUG) codons (difference in the first or third position indicated using IUPAC notation), while the amino acid serine is specified by UCN or AGY (UCA, UCG, UCC, UCU, AGU, or AGC) codons (difference in the first, second, or third position). A practical consequence of redundancy is that errors in the third position of the triplet codon cause only a silent mutation or an error that would not affect the protein because the hydrophilicity or hydrophobicity is maintained by equivalent substitution of amino acids; for example, a codon of NUN (where N = any nucleotide) tends to code for hydrophobic amino acids. NCN yields amino acid residues that are small in size and moderate in hydropathy; NAN encodes average size hydrophilic residues. The genetic code is so well-structured for hydropathy that a mathematical analysis (Singular Value Decomposition) of 12 variables (4 nucleotides x 3 positions) yields a remarkable correlation (C = 0.95) for predicting the hydropathy of the encoded amino acid directly from the triplet nucleotide sequence, \"without translation.\" Note in the table, below, eight amino acids are not affected at all by mutations at the third position of the codon, whereas in the figure above, a mutation at the second position is likely to cause a radical change in the physicochemical properties of the encoded amino acid.\nThe frequency of codons, also known as codon usage bias, can vary from species to species with functional implications for the control of translation. The following codon usage table is for the human genome.\n\nThe DNA codon table is essentially identical to that for RNA, but with U replaced by T.\n\nIn some proteins, non-standard amino acids are substituted for standard stop codons, depending on associated signal sequences in the messenger RNA. For example, UGA can code for selenocysteine and UAG can code for pyrrolysine. Selenocysteine became to be seen as the 21st amino acid, and pyrrolysine as the 22nd. Unlike selenocysteine, pyrrolysine-encoded UAG is translated with the participation of a dedicated aminoacyl-tRNA synthetase. Both selenocysteine and pyrrolysine may be present in the same organism. Although the genetic code is normally fixed in an organism, the achaeal prokaryote \"Acetohalobium arabaticum\" can expand its genetic code from 20 to 21 amino acids (by including pyrrolysine) under different conditions of growth.\n\nVariations on the standard code were predicted in the 1970s. The first was discovered in 1979, by researchers studying human mitochondrial genes. Many slight variants were discovered thereafter, including various alternative mitochondrial codes. Small variants such as translation of the codon UGA as tryptophan in \"Mycoplasma\" species, and translation of CUG as a serine rather than leucine in yeasts of the \"CTG clade\" (such as \"Candida albicans\"). Because viruses must use the same genetic code as their hosts, modifications to the standard genetic code could interfere with viral protein synthesis or functioning. However, viruses such as totiviruses adapted to the host's genetic code modification. In bacteria and archaea, GUG and UUG are common start codons. In rare cases, certain proteins may use alternative start codons.\nSurprisingly, variations in the interpretation of the genetic code exist also in human nuclear-encoded genes: In 2016, researchers studying the translation of malate dehydrogenase found that in about 4% of the mRNAs encoding this enzyme the stop codon is naturally used to encode the amino acids tryptophan and arginine. This type of recoding is induced by a high-readthrough stop codon context and it is referred to as \"functional translational readthrough\".\n\nVariant genetic codes used by an organism can be inferred by identifying highly conserved genes encoded in that genome, and comparing its codon usage to the amino acids in homologous proteins of other organisms. For example, the program FACIL infers a genetic code by searching which amino acids in homologous protein domains are most often aligned to every codon. The resulting amino acid probabilities for each codon are displayed in a genetic code logo, that also shows the support for a stop codon.\n\nDespite these differences, all known naturally occurring codes are very similar. The coding mechanism is the same for all organisms: three-base codons, tRNA, ribosomes, single direction reading and translating single codons into single amino acids.\n\nThe genetic code is a key part of the story of life. The main hypothesis for life's origin is the RNA world hypothesis. Any model for the emergence of genetic code is intimately related to a model of the transfer from ribozymes (RNA enzymes) to proteins as the principal enzymes in cells. In line with the RNA world hypothesis, transfer RNA molecules appear to have evolved before modern aminoacyl-tRNA synthetases, so the latter cannot be part of the explanation of its patterns.\n\nA hypothetical randomly evolved genetic code further motivates a biochemical or evolutionary model for its origin. If amino acids were randomly assigned to triplet codons, there would be 1.5 × 10 possible genetic codes. This number is found by calculating the number of ways that 21 items (20 amino acids plus one stop) can be placed in 64 bins, wherein each item is used at least once. However, the distribution of codon assignments in the genetic code is nonrandom. In particular, the genetic code clusters certain amino acid assignments.\n\nAmino acids that share the same biosynthetic pathway tend to have the same first base in their codons. This could be an evolutionary relic of an early, simpler genetic code with fewer amino acids that later evolved to code a larger set of amino acids. It could also reflect steric and chemical properties that had another effect on the codon during its evolution. Amino acids with similar physical properties also tend to have similar codons, reducing the problems caused by point mutations and mistranslations.\n\nGiven the non-random genetic triplet coding scheme, a tenable hypothesis for the origin of genetic code could address multiple aspects of the codon table, such as absence of codons for D-amino acids, secondary codon patterns for some amino acids, confinement of synonymous positions to third position, the small set of only 20 amino acids (instead of a number approaching 64), and the relation of stop codon patterns to amino acid coding patterns.\n\nThree main hypotheses address the origin of the genetic code. Many models belong to one of them or to a hybrid:\n\n\nHypotheses have addressed a variety of scenarios:\n\n", "id": "12385", "title": "Genetic code"}
{"url": "https://en.wikipedia.org/wiki?curid=4087965", "text": "Genetic analysis\n\nGenetic analysis is the overall process of studying and researching in fields of science that involve genetics and molecular biology. There are a number of applications that are developed from this research, and these are also considered parts of the process. The base system of analysis revolves around general genetics. Basic studies include identification of genes and inherited disorders. This research has been conducted for centuries on both a large-scale physical observation basis and on a more microscopic scale.\nGenetic analysis can be used generally to describe methods both used in and resulting from the sciences of genetics and molecular biology, or to applications resulting from this research.\n\nGenetic analysis may be done to identify genetic/inherited disorders and also to make a differential diagnosis in certain somatic diseases such as cancer. Genetic analyses of cancer include detection of mutations, fusion genes, and DNA copy number changes.\n\nMuch of the research that set the foundation of genetic analysis began in prehistoric times. Early humans found that they could practice selective breeding to improve crops and animals. They also identified inherited traits in humans that were eliminated over the years. The many genetic analyses gradually evolved over time.\n\nModern genetic analysis began in the mid-1800s with research conducted by Gregor Mendel. Mendel, who is known as the \"father of modern genetics\", was inspired to study variation in plants. Between 1856 and 1863, Mendel cultivated and tested some 29,000 pea plants (i.e., Pisum sativum). This study showed that one in four pea plants had purebred recessive alleles, two out of four were hybrid and one out of four were purebred dominant. His experiments led him to make two generalizations, the Law of Segregation and the Law of Independent Assortment, which later became known as Mendel's Laws of Inheritance. Lacking the basic understanding of heredity, Mendel observed various organisms and first utilized genetic analysis to find that traits were inherited from parents and those traits could vary between children. Later, it was found that units within each cell are responsible for these traits. These units are called genes. Each gene is defined by a series of amino acids that create proteins responsible for genetic traits.\n\nGenetic analyses include molecular technologies such as PCR, RT-PCR, DNA sequencing, and DNA microarrays, and cytogenetic methods such as karyotyping and fluorescence in situ hybridisation.\n\nDNA sequencing is essential to the applications of genetic analysis. This process is used to determine the order of nucleotide bases. Each molecule of DNA is made from adenine, guanine, cytosine and thymine, which determine what function the genes will possess. This was first discovered during the 1970s. DNA sequencing encompasses biochemical methods for determining the order of the nucleotide bases, adenine, guanine, cytosine, and thymine, in a DNA oligonucleotide. By generating a DNA sequence for a particular organism, you are determining the patterns that make up genetic traits and in some cases behaviors.\n\nSequencing methods have evolved from relatively laborious gel-based procedures to modern automated protocols based on dye labelling and detection in capillary electrophoresis that permit rapid large-scale sequencing of genomes and transcriptomes. Knowledge of DNA sequences of genes and other parts of the genome of organisms has become indispensable for basic research studying biological processes, as well as in applied fields such as diagnostic or forensic research. The advent of DNA sequencing has significantly accelerated biological research and discovery.\n\nCytogenetics is a branch of genetics that is concerned with the study of the structure and function of the cell, especially the chromosomes. Polymerase chain reaction studies the amplification of DNA. Because of the close analysis of chromosomes in cytogenetics, abnormalities are more readily seen and diagnosed.\n\nA karyotype is the number and appearance of chromosomes in the nucleus of a eukaryotic cell. The term is also used for the complete set of chromosomes in a species, or an individual organism.\n\nKaryotypes describe the number of chromosomes, and what they look like under a light microscope. Attention is paid to their length, the position of the centromeres, banding pattern, any differences between the sex chromosomes, and any other physical characteristics.\nKaryotyping uses a system of studying chromosomes to identify genetic abnormalities and evolutionary changes in the past.\n\nA DNA microarray is a collection of microscopic DNA spots attached to a solid surface. Scientists use DNA microarrays to measure the expression levels of large numbers of genes simultaneously or to genotype multiple regions of a genome. When a gene is expressed in a cell, it generates messenger RNA (mRNA). Overexpressed genes generate more mRNA than underexpressed genes. This can be detected on the microarray\nSince an array can contain tens of thousands of probes, a microarray experiment can accomplish many genetic tests in parallel. Therefore arrays have dramatically accelerated many types of investigations.\n\nThe polymerase chain reaction (PCR) is a biochemical technology in molecular biology to amplify a single or a few copies of a piece of DNA across several orders of magnitude, generating thousands to millions of copies of a particular DNA sequence.\nPCR is now a common and often indispensable technique used in medical and biological research labs for a variety of applications. These include DNA cloning for sequencing, DNA-based phylogeny, or functional analysis of genes; the diagnosis of hereditary diseases; the identification of genetic fingerprints (used in forensic sciences and paternity testing); and the detection and diagnosis of infectious diseases.\n\nNumerous practical advancements have been made in the field of genetics and molecular biology through the processes of genetic analysis. One of the most prevalent advancements during the late 20th and early 21st centuries is a greater understanding of cancer's link to genetics. By identifying which genes in the cancer cells are working abnormally, doctors can better diagnose and treat cancers.\n\nThis research has been able to identify the concepts of genetic mutations, fusion genes and changes in DNA copy numbers, and advances are made in the field every day. Much of these applications have led to new types of sciences that use the foundations of genetic analysis. Reverse genetics uses the methods to determine what is missing in a genetic code or what can be added to change that code. Genetic linkage studies analyze the spatial arrangements of genes and chromosomes. There have also been studies to determine the legal and social and moral effects of the increase of genetic analysis. Genetic analysis may be done to identify genetic/inherited disorders and also to make a differential diagnosis in certain somatic diseases such as cancer. Genetic analyses of cancer include detection of mutations, fusion genes, and DNA copy number changes.\n", "id": "4087965", "title": "Genetic analysis"}
{"url": "https://en.wikipedia.org/wiki?curid=50686685", "text": "NgAgo\n\nNgAgo is a single-stranded DNA (ssDNA)-guided Argonaute endonuclease, an acronym for Natronobacterium\" gregoryi\" Argonaute. NgAgo binds 5′ phosphorylated ssDNA of ~24 nucleotides (gDNA) to guide it to its target site and will make DNA double-strand breaks at the gDNA site. Like the CRISPR/Cas system, NgAgo was reported to be suitable for genome editing, but this has not been replicated. In contrast to Cas9, the NgAgo–gDNA system does not require a protospacer adjacent motif (PAM).\n\nNgAgo was proposed in May 2016 to be useful for genome editing because of the system’s high accuracy and efficiency, which was said to minimize off-target effects. The specificity of the gDNA is essential, as cleavage efficiency is impaired by a single nucleotide mismatch between the guide and target molecules. Using 5’ phosphorylated ssDNAs as guide molecules reduces the possibility of cellular oligonucleotides misleading NgAgo. A guide molecule can only be attached to NgAgo during the expression of the protein. Once the guide is loaded, NgAgo cannot swap free floating ssDNA for its gDNA. Designing, synthesizing, and adjusting the concentration of ssDNAs is easier compared to systems using sgRNA. The required dosage of ssDNA is less than that of a sgRNA expression plasmid.\n\nDoubts about the technique were raised on gene editing forums as early as June and have persisted. There have been several allegations that this procedure is impossible to reproduce. \"Nature Biotechnology\", which originally published the research, is investigating.\n", "id": "50686685", "title": "NgAgo"}
{"url": "https://en.wikipedia.org/wiki?curid=50659905", "text": "Virome\n\nVirome refers to the collection of nucleic acids, both RNA and DNA, that make up the viral community associated with a particular ecosystem or holobiont. The word is derived from virus and genome and first used by Forest Rohwer and colleagues to describe viral shotgun metagenomes. All macro-organisms have viromes that include bacteriophage and viruses. Viromes are important in the nutrient and energy cycling, development of immunity, and a major source of genes through lysogenic conversion.\n\nViromes were the first examples of shotgun community sequence, which is now known as metagenomics. In the 2000s, the Rohwer lab sequenced viromes from seawater, marine sediments, adult human stool, infant human stool, soil, and blood. This group also performed the first RNA virome with collaborators from the Genomic Institute of Singapore. From these early works, it was concluded that most of the genomic diversity is contained in the global virome and that most of this diversity remains uncharacterized. This view was supported by individual genomic sequencing project, particularly the mycobacterium phage.\n\nIn order to study the virome, virus-like particles are separated from cellular components, usually using a combination of filtration, density centrifugation, and enzymatic treatments to get rid of free nucleic acids. The nucleic acids are then sequenced and analyzed using metagenomic methods.\n\nViruses are the most abundant biological entities on Earth, but challenges in detecting, isolating, and classifying unknown viruses have prevented exhaustive surveys of the global virome. Over 5 Tb of metagenomic sequence data were used from 3,042 geographically diverse samples to assess the global distribution, phylogenetic diversity, and host specificity of viruses.\n\nOver 125,000 partial DNA viral genomes, including the largest phage yet identified, increased the number of known viral genes by 16-fold. A suite of computational methods was used to identify putative host virus connections. The isolate viral host information was projected onto a group, resulting in host assignments for 2.4% of viral groups.\n\nThen the CRISPR–Cas prokaryotic immune system which holds a \"library\" of genome fragments from phages (proto-spacers) that have previously infected the host. Spacers from isolate microbial genomes with matches to metagenomic viral contigs (mVCs) were identified for 4.4% of the viral groups and 1.7% of singletons. The hypothesis was explored that viral transfer RNA (tRNA) genes originate from their host.\n\nViral tRNAs identified in 7.6% of the mVCs were matched to isolate genomes from a single species or genus. The specificity of tRNA-based host viral assignment was confirmed by CRISPR–Cas spacer matches showing a 94% agreement at the genus level. These approaches identified 9,992 putative host–virus associations enabling host assignment to 7.7% of mVCs. The majority of these connections were previously unknown, and include hosts from 16 prokaryotic phyla for which no viruses have previously been identified.\nMany viruses specialize in infecting related hosts. Viral generalists that infect hosts across taxonomic orders may exist. Most CRISPR spacer matches were from viral sequences to hosts within one species or genus. Some mVCs were linked to multiple hosts from higher taxa. A viral group composed of macs from human oral samples contained three distinct photo-spacers with nearly exact matches to spacers in Actionbacteria and Firmicutes.\n", "id": "50659905", "title": "Virome"}
{"url": "https://en.wikipedia.org/wiki?curid=50429846", "text": "Gene desert\n\nGene deserts are regions of the genome that are devoid of protein-coding genes. Gene deserts constitute an estimated 25% of the entire genome, leading to the recent interest in their true functions. Originally believed to contain inessential and “junk” DNA due to their inability to create proteins, gene deserts have since been linked to several vital regulatory functions, including distal enhancing and conservatory inheritance. Thus, an increasing number of risks that lead to several major diseases, including a handful of cancers, have been attributed to irregularities found in gene deserts. One of the most notable examples is the 8q24 gene region, which, when affected by certain single nucleotide polymorphisms, lead to a myriad of diseases. The major identifying factors of gene deserts lay in their low GpC content and their relatively high levels of repeats, which are not observed in coding regions. Recent studies have even further categorized gene deserts into variable and stable forms; regions are categorized based on their behavior through recombination and their genetic contents. Although current knowledge of gene deserts is rather limited, ongoing research and improved techniques are beginning to open the doors for exploration on the various important effects of these noncoding regions.\n\nAlthough the possibility of function in gene deserts was predicted as early as the 1960s, genetic identification tools were unable to uncover any specific characteristics of the long noncoding regions, other than that no coding occurred in those regions. Before the completion of the human genome in 2001 through the Human Genome Project, most of the early associative gene comparisons relied on the belief that essential housekeeping genes were clustered in the same areas of the genome for ease of access and tight regulation. This belief later constructed a hypothesis that gene deserts are therefore previous regulatory sequences that are highly linked (and hence do not undergo recombination), but have had substitutions between them over time. These substitutions could cause tightly conserved genes to separate over time, thus forming regions of nonsense codes with a few essential genes. However, uncertainty due to differential gene conservation rates in different portions of chromosomes prevented accurate identification. \nLater associations were remodeled when regulatory sequences were associated with transcription factors, leading to the birth of large-scale genome-wide mapping. Thus began the hunt for the contents and functions of gene deserts. Recent advancements in the screening of chromatin signatures on chromosomes (for instance, chromosome conformation capture, also known as 3C) have allowed the confirmation of the long-range gene activation model, which postulates that there are indeed physical links between regulatory enhancers and their target promoters. Research on gene deserts, although centralized on human genetics, has also been applied to mice, various birds, and Drosophila melanogaster. Although conservation is variable among selected species’ genomes, orthologous gene deserts function similarly. Thus, the prevailing the contention of gene deserts is that these noncoding sequences harbor active and important regulatory elements.\n\nOne study by focused on a regulatory archipelago, a region with “islands” of coding sequences surrounded by vast noncoding regions. The study, which explores the effects of regulation on the hox genes, initially focused on two enhancer sequences, GCR and Prox, which are located 200 basepairs and 50 basepairs respectively upstream of the Hox D locus. To manipulate the region, the study inverted the two enhancer sequences and discovered no major effects on the transcription of the Hox D gene, even though the two sequences were the closest sequences to the gene. Thus, the turned to the gene desert that flanked the GCR sequence upstream and found 5 regulatory islands within it that could regulate the gene. To select the most likely candidate, the study then applied several individual and multiple deletions to the five islands to observe the effects. These varied deletions only resulted in minor effects including physical abnormalities or a few missing digits. \nWhen the experiment was taken a step further and applied a deletion of the entire 830 kilobase gene desert, the functionality of the entire Hox D locus was rendered inactive. This indicates that the neighboring gene desert, as an entire 830 kilobase unit (including the five island sequences within it), serves as an important regulator of a single gene that spans merely 50 kilobases. Therefore, these results hinted at the regulatory effects of flanking gene deserts. This study was supported by a later observation through a comparison between fluorescence in situ hybridization and chromosome conformation capture which discovered that the Hox D locus was the most decondensed portion in the region. This meant that it had relatively higher activity in comparison to the flanking gene deserts. Hence, the Hox D could be regulated by specific nearby enhancer sequences that were not expressed in unison. However, the does caution that proximity is inaccurate when either analytical method is used. Thus, associations between regulatory gene deserts and their target promoters seem to have variable distances and are not required to act as borders. \nThe variability in distance demonstrates that distance may be another important factor that is determined by gene deserts. For instance, distal enhancers may interact with their target promoters through looping interactions which must act over a certain distance. Thus, proximity is not an accurate predictor of enhancers: enhancers do not need to border their target sequence to regulate them. While this leads to a variation in distances, the average distance between transcription start sites and the interaction complex mediated by their enhancer elements is 120 kilobases upstream of the start site. Gene deserts may play a role in constructing this distance to allow maximal looping to occur. Given that the mechanism of enhancer complex formation is a fairly simply regulated mechanism (the structures that are recruited into the enhancing complex have various regulatory controls that control construction), more than 50% of promoters have several long-range interactions. Certain core genes even have up to 20 possible enhancing interactions. There is a curious bias for complexes to form only upstream of the promoters. Thus, given the correlation that many regulatory gene deserts appear upstream of their target promoters, it is possible that the more immediate role that gene deserts play is in long-range regulation of key sequences. \nAs the ideal formation of enhancer interactions requires specific constructs, a possible side-product of the regulatory roles of gene deserts may be the conservation of genes: to retain the specific lengths of loops and order of regulating genes hidden in gene deserts, certain portions of gene deserts are more highly conserved than others when passing through inheritance events. These conserved noncoding sequences (CNS) are directly associated with syntenic inheritance in all vertebrates. Thus, the presence of these CNSs could serve to conserve of large regions of genes. Although distance may vary in regulatory gene deserts, distance appears to have an upper limit in conservative gene deserts. CNSs were initially thought to occur close to their conserved genes: earlier estimates placed most CNSs in proximity of gene sequences. However, the expansion of genetic data has revealed that several CNSs reside up to 2.5 megabases from their target genes, with the majority of CNSs falling between 1 and 2 megabases. This range, which was measured for the human genome, is varied among different species. For instance, in comparison to humans, the Fugu fish has a smaller range, with an estimated maximum distance of a few hundred kilobases. Regardless of the difference in lengths, CNSs work in similar methods in both species. Thus, as functions differ between gene deserts, so do their contents.\n\nCertain gene deserts are heavy regulators, while others may be deleted without any effect. As a possible classification, gene deserts can be broken down into two subtypes: stable and variable. Stable gene deserts have fewer repeats and have relatively higher Guanine to Cytosine (GpC) content than observed in variable gene deserts.\n\nGuanine and cytosine content is indicative of protein-coding functionality. For example, in a study on chromosomes 2 and 4, which have been linked to several genetic diseases, there were elevated GpC content in certain regions. Mutations in these GC-rich regions caused a variety of diseases, revealing the necessary integrity of these genes. High density CpG regions serve as regulatory regions for DNA methylation. Therefore, essential coding genes should be represented by high-CpG regions. In particular, regions with high GC content should tend to have high densities of genes that are devoted mainly to the essential housekeeping and tissue specific processes. These processes would require the most protein production to express functionality. Stable gene deserts, which have higher levels of GC content, should therefore contain the essential enhancer sequences. This could determine the conservatory functions of stable gene deserts.\n\nOn the other hand, approximately 80% of gene deserts have low GpC contents, indicating that they have very few essential genes. Thus, the majority of gene deserts are variable gene deserts, which may have alternate functions. One prevalent theory regarding the origins of gene deserts postulates that gene deserts are accumulations of essential genes that act as a distance. This may hold true, as given the low numbers of essential genes within them, these regions would have been less conserved. As a result, due to the prevalence of cytosine to thymine conversions, the most common SNP, would cause a gradual separation between the few essential genes within variable gene deserts. These essential sequences would have been maintained and conserved, leading to small regions of high density that regulate at a distance. GC content is therefore indication for the presence of coding or regulatory processes in DNA. \n\nWhile stable gene deserts have higher GC content, this relative value is only an average. Within stable gene deserts, although the ends contain very high levels of GC content, the main bulk of the DNA contains even less GC content than observed in variable gene deserts. This indicates that there are very few highly conserved regions in stable gene deserts that do not recombine, or do so at very low rates. Given that the ends of the stable gene deserts have particularly high levels of GC contents, these sequences must be extremely conserved. This conservation may in turn cause the flanking genes to also have higher conservation rates. Thus, stable genes should be directly linked to at least one of their flanking genes and cannot be separated from coding sequences by recombination events. Most gene deserts appear to cluster in pairs around a small number of genes. This clustering creates long loci that have very low gene density; small regions with high numbers of genes are surrounded by long stretches of gene deserts, creating a low gene average. Therefore, the minimized probability of recombination events in these long loci creates syntenic blocks that are inherited together over time. These syntenic blocks can be conserved for very long periods of time, preventing loss of essential material, even while the distance between essential genes may grow in time.\n\nAlthough this effect should theoretically be amplified through the even lower GC-content in variable gene deserts (thereby truly minimalizing gene density), the gene conservation rates in variable gene deserts are even lower than observed in stable gene deserts—in fact, the rate is far lower than the rest of the genome. A possible explanation for this phenomenon is that variable gene deserts may be recently evolved regions that have not yet been fixed into stable gene deserts. Therefore, shuffling may still occur before stabilizing regions within the variable gene deserts begin to cluster as whole units. There are a few exceptions to this minimal rate of conservation, as a few GC gene deserts are subjected to hypermethylation, which greatly reduces the accessibility to the DNA, thus effectively protecting the region from recombination. However, these occur rarely in observation.\n\nAlthough stable and variable gene deserts differ in content and function, both wield conservatory abilities. It is possible that since most variable gene deserts have regulatory elements that can act at a distance, conservation of the entire gene desert into a sytenic locus would not have been necessary, so long as these regulatory elements themselves were conserved as units. Given the particularly low levels of GC content, the regulatory elements would therefore be in a minimal gene density situation as observed similarly in flanking stable gene deserts, with the same effect. Thus, both types of gene deserts serve to retain essential genes within the genome.\n\nThe conservative nature of gene deserts confirms that these stretches of noncoding bases are essential to proper functioning. Indeed, a wide range of studies on irregularities in the noncoding genes discovered several associations to genetic diseases. One of the most studied gene deserts is the 8q24 region. Early genome wide association studies were focused on the 8q24 region (residing on chromosome 8) due to the abnormally high rates of SNPs that seem to occur in the region. These studies found that the region was linked to increased risks for a variety of cancers, notably in the prostate, breast, ovaries, colonic, and pancreas. Using inserts of the gene desert into bacterial artificial chromosomes, one study was able to produce enhancer activity in certain regions, which were isolated via cloning systems. This study successfully identified an enhancer sequence hidden in the region. Within this enhancer sequence, an SNP that conferred risk for prostate cancer, labeled SNP s6983267, was discovered in diseased mice. However, the 8q24 region is not solely limited to conferred risks of prostate cancer. A study in 2008 screened human subjects (and controls) with variations in the gene desert region, discovering five different regions that conferred different risks when affected by different SNPs. This study used identified SNP markers in the gene desert to identify risk conference from each of the regions to a specific tissue expression. Although these risks were successfully linked to various forms of cancer, Ghoussaini, M., et al. note their uncertainty in whether the SNPs functioned merely as markers or were the direct causants of the cancers.\n\nThese varied effects occur due to the different interactions between the SNPs in this region and MYC promoters of different organs. The MYC promoter, which is located at a short distance downstream of the 8q24 region, is perhaps the most studied oncogene due to its association with a myriad of diseases. Normal functioning of the MYC promoter ensures that cells divide regularly. The study postulates that the 8q region, which underwent a chromosomal translocation in humans, could have moved an essential enhancer for the MYC promoter. This areas around this region could have been subjected to recombination that may have hidden the essential MYC enhancer within the gene desert through time, although its enhancing effects are still very much retained. This analysis stems from disease associations observed in several mice species where this region is retained at proximity to the MYC promoter. Thus, the 8q24 gene desert should have been somewhat linked to the MYC promoter. The desert resembles a stable gene desert that has had very little recombination after the translocation event. Thus, a potential hypothesis is that SNPs affecting this region disrupt the important tissue-specific genes with the stable gene desert, which could explain the risks of cancer in various tissue forms. This effect of hidden enhancer elements can also be observed in other locations in the genome. For instance, SNPs in the 5p13.1 deregulate the PTGER4 coding region, leading to Crohn’s Disease. Another affected region in the 9p21 gene desert causes several coronary artery diseases. However, none of these risk-conferring gene deserts seem to be affected as much as the 8q24 regions. Current studies are still unsure about the SNP-affected processes in the 8q24 region that result in particularly amplified responses to the MYC promoter. With the aid of a more accessible population and more specific markers for genome wide association mapping, an increasing number of risk alleles are now being marked in gene deserts, where small, isolated, and seemingly-unimportant regions of genes may moderate important genes.\n\nIt is crucial to note that although most of the gene deserts explored here are essential, it could be that the majority of the contents in gene deserts are still likely to be inessential and disposable. Naturally, this is not to say that the roles that gene deserts play are inessential or unimportant, rather than their functions may include buffering effects. An example of essential gene deserts with inessential DNA content are the telomeres that protect the ends of genomes. Telomeres can be categorized as true gene deserts, given that they solely contain repeats of TTAGGG (in humans) and do not have apparent protein-coding functions. Without these telomeres, human genomes would be severely mutated within a fixed number of cell cycles. On the other hand, since telomeres do not code for proteins, their loss ensures that there is no effect in important processes. Therefore, the term “junk” DNA should no longer be applied to any region of the genome; every portion of the genome should play a role in protecting, regulating, or repairing the protein coding regions that determine the functions of life. Although there is still much to learn about the nooks and crannies of the immense (yet limited) human genome, with the aid of various new technologies and the synthesis of the full human genome, we may perhaps unravel a great collection of secrets in the approaching years about the marvels of our genetic code.\n\n", "id": "50429846", "title": "Gene desert"}
{"url": "https://en.wikipedia.org/wiki?curid=50904125", "text": "Second-degree relative\n\nA second-degree relative (SDR) is someone who shares 25% of a person's genes. It includes uncles, aunts, nephews, nieces, grandparents, grandchildren, half-siblings, and double cousins.\n\n", "id": "50904125", "title": "Second-degree relative"}
{"url": "https://en.wikipedia.org/wiki?curid=50992773", "text": "Comparison of DNA sequencing services\n\nThis page lists the different DNA sequencing services. \n2 main types can be distinguished:\n\nBoth allow people to detect the presence of hereditary diseases (and/or other imperfections) in their DNA, and (when WGS) is used, it even allows people to know the specifics of their hereditary diseases (and/or other imperfections). These specifics can be important, as in many cases, it's not a single gene that causes the disease, but rather a combination of genes. In some cases, the exact gene is not even known, but only the approximate location where the imperfect nucleotides are situated is known.\n", "id": "50992773", "title": "Comparison of DNA sequencing services"}
{"url": "https://en.wikipedia.org/wiki?curid=51112280", "text": "Genetic demixing\n\nIn biology, genetic demixing refers to a phenomenon in which an initial mixture of individuals with\ntwo or more distinct genotypes rearranges in the course of time,\ngiving birth to a spatial organization where some or all genotypes are concentrated in distinct patches.\n\n", "id": "51112280", "title": "Genetic demixing"}
{"url": "https://en.wikipedia.org/wiki?curid=21496", "text": "Nucleic acid\n\nNucleic acids are biopolymers, or small biomolecules, essential to all known forms of life. They are composed of nucleotides, which are monomers made of three components: a 5-carbon sugar, a phosphate group and a nitrogenous base. If the sugar is a simple ribose, the polymer is RNA (ribonucleic acid); if the sugar is derived from ribose as deoxyribose, the polymer is DNA (deoxyribonucleic acid).\n\nNucleic acids are the most important of all biomolecules. They are found in abundance in all living things, where they function to create and encode and then store information in the nucleus of every living cell of every life-form organism on Earth. In turn, they function to transmit and express that information inside and outside the cell nucleus—to the interior operations of the cell and ultimately to the next generation of each living organism. The encoded information is contained and conveyed via the nucleic acid sequence, which provides the 'ladder-step' ordering of nucleotides within the molecules of RNA and DNA.\n\nStrings of nucleotides are bonded to form helical backbones—typically, one for RNA, two for DNA—and assembled into chains of base-pairs selected from the five primary, or canonical, nucleobases, which are: adenine, cytosine, guanine, thymine, and uracil; note, thymine occurs only in DNA and uracil only in RNA. Using amino acids and the process known as protein synthesis, the specific sequencing in DNA of these nucleobase-pairs enables storing and transmitting coded instructions as genes. In RNA, base-pair sequencing provides for manufacturing new proteins that determine the frames and parts and most chemical processes of all life forms.\n\n\nExperimental studies of nucleic acids constitute a major part of modern biological and medical research, and form a foundation for genome and forensic science, and the biotechnology and pharmaceutical industries.\n\nThe term \"nucleic acid\" is the overall name for DNA and RNA, members of a family of biopolymers, and is synonymous with \"polynucleotide\". Nucleic acids were named for their initial discovery within the nucleus, and for the presence of phosphate groups (related to phosphoric acid). Although first discovered within the nucleus of eukaryotic cells, nucleic acids are now known to be found in all life forms including within bacteria, archaea, mitochondria, chloroplasts, viruses, and viroids. (note: there is debate as to whether viruses are living or non-living). All living cells contain both DNA and RNA (except some cells such as mature red blood cells), while viruses contain either DNA or RNA, but usually not both.\nThe basic component of biological nucleic acids is the nucleotide, each of which contains a pentose sugar (ribose or deoxyribose), a phosphate group, and a nucleobase.\nNucleic acids are also generated within the laboratory, through the use of enzymes (DNA and RNA polymerases) and by solid-phase chemical synthesis. The chemical methods also enable the generation of altered nucleic acids that are not found in nature, for example peptide nucleic acids.\n\nNucleic acids are generally very large molecules. Indeed, DNA molecules are probably the largest individual molecules known. Well-studied biological nucleic acid molecules range in size from 21 nucleotides (small interfering RNA) to large chromosomes (human chromosome 1 is a single molecule that contains 247 million base pairs).\n\nIn most cases, naturally occurring DNA molecules are double-stranded and RNA molecules are single-stranded. There are numerous exceptions, however—some viruses have genomes made of double-stranded RNA and other viruses have single-stranded DNA genomes, and, in some circumstances, nucleic acid structures with three or four strands can form.\n\nNucleic acids are linear polymers (chains) of nucleotides. Each nucleotide consists of three components: a purine or pyrimidine nucleobase (sometimes termed \"nitrogenous base\" or simply \"base\"), a pentose sugar, and a phosphate group. The substructure consisting of a nucleobase plus sugar is termed a nucleoside. Nucleic acid types differ in the structure of the sugar in their nucleotides–DNA contains 2'-deoxyribose while RNA contains ribose (where the only difference is the presence of a hydroxyl group). Also, the nucleobases found in the two nucleic acid types are different: adenine, cytosine, and guanine are found in both RNA and DNA, while thymine occurs in DNA and uracil occurs in RNA.\n\nThe sugars and phosphates in nucleic acids are connected to each other in an alternating chain (sugar-phosphate backbone) through phosphodiester linkages. In conventional nomenclature, the carbons to which the phosphate groups attach are the 3'-end and the 5'-end carbons of the sugar. This gives nucleic acids directionality, and the ends of nucleic acid molecules are referred to as 5'-end and 3'-end. The nucleobases are joined to the sugars via an N-glycosidic linkage involving a nucleobase ring nitrogen (N-1 for pyrimidines and N-9 for purines) and the 1' carbon of the pentose sugar ring.\n\nNon-standard nucleosides are also found in both RNA and DNA and usually arise from modification of the standard nucleosides within the DNA molecule or the primary (initial) RNA transcript. Transfer RNA (tRNA) molecules contain a particularly large number of modified nucleosides.\n\nDouble-stranded nucleic acids are made up of complementary sequences, in which extensive Watson-Crick base pairing results in a highly repeated and quite uniform double-helical three-dimensional structure. In contrast, single-stranded RNA and DNA molecules are not constrained to a regular double helix, and can adopt highly complex three-dimensional structures that are based on short stretches of intramolecular base-paired sequences including both Watson-Crick and noncanonical base pairs, and a wide range of complex tertiary interactions.\n\nNucleic acid molecules are usually unbranched, and may occur as linear and circular molecules. For example, bacterial chromosomes, plasmids, mitochondrial DNA, and chloroplast DNA are usually circular double-stranded DNA molecules, while chromosomes of the eukaryotic nucleus are usually linear double-stranded DNA molecules. Most RNA molecules are linear, single-stranded molecules, but both circular and branched molecules can result from RNA splicing reactions. The total amount of pyrimidine is equal to the total amount of purines. The diameter of the helix is about 20A.\n\nOne DNA or RNA molecule differs from another primarily in the sequence of nucleotides. Nucleotide sequences are of great importance in biology since they carry the ultimate instructions that encode all biological molecules, molecular assemblies, subcellular and cellular structures, organs, and organisms, and directly enable cognition, memory, and behavior (\"see Genetics\"). Enormous efforts have gone into the development of experimental methods to determine the nucleotide sequence of biological DNA and RNA molecules, and today hundreds of millions of nucleotides are sequenced daily at genome centers and smaller laboratories worldwide. In addition to maintaining the GenBank nucleic acid sequence database, the National Center for Biotechnology Information (NCBI, https://www.ncbi.nlm.nih.gov) provides analysis and retrieval resources for the data in GenBank and other biological data made available through the NCBI web site.\n\nDeoxyribonucleic acid (DNA) is a nucleic acid containing the genetic instructions used in the development and functioning of all known living organisms. The DNA segments carrying this genetic information are called genes. Likewise, other DNA sequences have structural purposes, or are involved in regulating the use of this genetic information. Along with RNA and proteins, DNA is one of the three major macromolecules that are essential for all known forms of life.\nDNA consists of two long polymers of simple units called nucleotides, with backbones made of sugars and phosphate groups joined by ester bonds. These two strands run in opposite directions to each other and are, therefore, anti-parallel. Attached to each sugar is one of four types of molecules called nucleobases (informally, bases). It is the sequence of these four nucleobases along the backbone that encodes information. This information is read using the genetic code, which specifies the sequence of the amino acids within proteins. The code is read by copying stretches of DNA into the related nucleic acid RNA in a process called transcription.\nWithin cells DNA is organized into long structures called chromosomes. During cell division these chromosomes are duplicated in the process of DNA replication, providing each cell its own complete set of chromosomes. Eukaryotic organisms (animals, plants, fungi, and protists) store most of their DNA inside the cell nucleus and some of their DNA in organelles, such as mitochondria or chloroplasts. In contrast, prokaryotes (bacteria and archaea) store their DNA only in the cytoplasm. Within the chromosomes, chromatin proteins such as histones compact and organize DNA. These compact structures guide the interactions between DNA and other proteins, helping control which parts of the DNA are transcribed.\n\nRibonucleic acid (RNA) functions in converting genetic information from genes into the amino acid sequences of proteins. The three universal types of RNA include transfer RNA (tRNA), messenger RNA (mRNA), and ribosomal RNA (rRNA). Messenger RNA acts to carry genetic sequence information between DNA and ribosomes, directing protein synthesis. Ribosomal RNA is a major component of the ribosome, and catalyzes peptide bond formation. Transfer RNA serves as the carrier molecule for amino acids to be used in protein synthesis, and is responsible for decoding the mRNA. In addition, many other classes of RNA are now known.\n\nArtificial nucleic acid analogues have been designed and synthesized by chemists, and include peptide nucleic acid, morpholino- and locked nucleic acid, glycol nucleic acid, and threose nucleic acid. Each of these is distinguished from naturally occurring DNA or RNA by changes to the backbone of the molecules.\n\n", "id": "21496", "title": "Nucleic acid"}
{"url": "https://en.wikipedia.org/wiki?curid=51366477", "text": "Genomics plc\n\nGenomics plc is a spin-out company from the University of Oxford founded in 2014 by Professors Peter Donnelly and Gilean McVean. The company has developed an analytical platform that utilises complex statistical and AI (artificial intelligence) models to increase understanding of human biology. They aim to use their platform, in combination with vast amounts of human genetic and phenotypic data from many sources, to elucidate \"the human wiring diagram\" in order to save the pharmaceutical and biotech industry billions of dollars. The company raised over £10M in initial funding rounds in 2014, and has also received over £2M in grant funding. It employs over 30 staff, mainly scientists and software engineers from a variety of disciplines. The company's offices are in central Oxford, UK.\n\nPeter Donnelly and Gil McVean are distinguished academics in the fields of statistics and genetics. Both are Fellows of the Royal Society and Academy of Medical Sciences.\n\nPeter Donnelly is a professor of statistical science at the University of Oxford, and has been Director of the Wellcome Trust Centre for Human Genetics since 2007. He played a major role in the International HapMap Project and chaired the Wellcome Trust Case Control Consortium (WTCCC) and its successor the WTCCC2. He also led the WGS500 project; a precursor to the Genomics England 100,000 Genomes Project. Donnelly chaired the Royal Society's Machine Learning Working Group from 2015-2017, and was widely quoted on the release of the group's report in April 2017. He also speaks regularly on the applications of AI and machine learning to healthcare, including a keynote talk at the 2017 International Conference on Machine Learning. He is a member of scientific advisory boards of several pharmaceutical and biotech companies.\n\nGil McVean is Professor of Statistical Genetics at the University of Oxford, and in 2016 was appointed as Director of the Big Data Institute at the Li Ka Shing Centre for Health Information and Discovery. McVean's research focuses on population genetics, statistics, recombination rates in the human genome, and evolutionary biology. He has worked on many leading genetics initiatives, including the International HapMap Project and the 1000 Genomes Project.\n\nThe Genomics plc Scientific Advisory Board was established in 2016. It is chaired by Sekar Kathiresan, and the current members are Nazneen Rahman, Richard Trembath and Chas Bountra.\n\n", "id": "51366477", "title": "Genomics plc"}
{"url": "https://en.wikipedia.org/wiki?curid=51415428", "text": "Pseudoalleles\n\nPseudoallelism is a state in which two genes with similar functions are located so close to one another on a chromosome that they are genetically linked. This means that the two genes (pseudoalleles) are nearly always inherited together. Since the two genes have related functions, they may appear to act as a single gene. In rare cases, the two linked pseudoalleles can be separated, or recombined. One hypothesis is that pseudoalleles are formed as a result of gene duplication events, and the duplicated genes can undergo gene evolution to develop new functions.\n\nCharacteristic of Pseudoalleles:\n\n\nExample:\n\nRed eye colour of \"Drosophila\" has different mutants like white and apricot.They affect pigmentation i.e., affect the same character.So, they are allelic.They can undergo recombination,i.e.,they are nonallelic.\n", "id": "51415428", "title": "Pseudoalleles"}
{"url": "https://en.wikipedia.org/wiki?curid=51846334", "text": "Distal promoter\n\nDistal promoter elements are regulatory DNA sequences that can be many kilobases distant from the gene that they regulate.\n\nThey can either be enhancers (increasing expression) or silencers (decreasing expression). They act by binding activator or repressor proteins (transcription factors) and the intervening DNA bends such that the bound proteins contact the core promoter and RNA polymerase.\n", "id": "51846334", "title": "Distal promoter"}
{"url": "https://en.wikipedia.org/wiki?curid=5219699", "text": "Human Genome Project\n\nThe Human Genome Project (HGP) was an international scientific research project with the goal of determining the sequence of nucleotide base pairs that make up human DNA, and of identifying and mapping all of the genes of the human genome from both a physical and a functional standpoint. It remains the world's largest collaborative biological project. After the idea was picked up in 1984 by the US government when the planning started, the project formally launched in 1990 and was declared complete in 2000. Funding came from the US government through the National Institutes of Health (NIH) as well as numerous other groups from around the world. A parallel project was conducted outside government by the Celera Corporation, or Celera Genomics, which was formally launched in 1998. Most of the government-sponsored sequencing was performed in twenty universities and research centers in the United States, the United Kingdom, Japan, France, Germany, Canada, and China.\n\nThe Human Genome Project originally aimed to map the nucleotides contained in a human haploid reference genome (more than three billion). The \"genome\" of any given individual is unique; mapping the \"human genome\" involved sequencing a small number of individuals and then assembling these together to get a complete sequence for each chromosome. Therefore, the finished human genome is a mosaic, not representing any one individual.\n\nThe Human Genome Project was a 15-year-long, publicly funded project initiated in 1990 with the objective of determining the DNA sequence of the entire euchromatic human genome within 15 years. In May 1985, Robert Sinsheimer organized a workshop to discuss sequencing the human genome, but for a number of reasons the NIH was uninterested in pursuing the proposal. The following March, the Santa Fe Workshop was organized by Charles DeLisi and David Smith of the Department of Energy's Office of Health and Environmental Research (OHER). At the same time Renato Dulbecco proposed whole genome sequencing in an essay in Science. James Watson followed two months later with a workshop held at the Cold Spring Harbor Laboratory.\n\nThe fact that the Santa Fe workshop was motivated and supported by a Federal Agency opened a path, albeit a difficult and tortuous one, for converting the idea into public policy. In a memo to the Assistant Secretary for Energy Research (Alvin Trivelpiece), Charles DeLisi, who was then Director of the OHER, outlined a broad plan for the project. This started a long and complex chain of events which led to approved reprogramming of funds that enabled the OHER to launch the Project in 1986, and to recommend the first line item for the HGP, which was in President Reagan's 1988 budget submission, and ultimately approved by the Congress. Of particular importance in Congressional approval was the advocacy of Senator Peter Domenici, whom DeLisi had befriended. Domenici chaired the Senate Committee on Energy and Natural Resources, as well as the Budget Committee, both of which were key in the DOE budget process. Congress added a comparable amount to the NIH budget, thereby beginning official funding by both agencies.\n\nAlvin Trivelpiece sought and obtained the approval of DeLisi's proposal by Deputy Secretary William Flynn Martin. This chart was used in the spring of 1986 by Trivelpiece, then Director of the Office of Energy Research in the Department of Energy, to brief Martin and Under Secretary Joseph Salgado regarding his intention to reprogram $4 million to initiate the project with the approval of Secretary Herrington. This reprogramming was followed by a line item budget of $16 million in the Reagan Administration’s 1987 budget submission to Congress. It subsequently passed both Houses. The Project was planned for 15 years.\n\nCandidate technologies were already being considered for the proposed undertaking at least as early as 1985.\n\nIn 1990, the two major funding agencies, DOE and NIH, developed a memorandum of understanding in order to coordinate plans and set the clock for the initiation of the Project to 1990. At that time, David Galas was Director of the renamed “Office of Biological and Environmental Research” in the U.S. Department of Energy’s Office of Science and James Watson headed the NIH Genome Program. In 1993, Aristides Patrinos succeeded Galas and Francis Collins succeeded James Watson, assuming the role of overall Project Head as Director of the U.S. National Institutes of Health (NIH) National Center for Human Genome Research (which would later become the National Human Genome Research Institute). A working draft of the genome was announced in 2000 and the papers describing it were published in February 2001. A more complete draft was published in 2003, and genome \"finishing\" work continued for more than a decade.\n\nThe $3-billion project was formally founded in 1990 by the US Department of Energy and the National Institutes of Health, and was expected to take 15 years. In addition to the United States, the international consortium comprised geneticists in the United Kingdom, France, Australia, China and myriad other spontaneous relationships.\n\nDue to widespread international cooperation and advances in the field of genomics (especially in sequence analysis), as well as major advances in computing technology, a 'rough draft' of the genome was finished in 2000 (announced jointly by U.S. President Bill Clinton and the British Prime Minister Tony Blair on June 26, 2000). This first available rough draft assembly of the genome was completed by the Genome Bioinformatics Group at the University of California, Santa Cruz, primarily led by then graduate student Jim Kent. Ongoing sequencing led to the announcement of the essentially complete genome on April 14, 2003, two years earlier than planned. In May 2006, another milestone was passed on the way to completion of the project, when the sequence of the last chromosome was published in \"Nature\".\n\nThe project was not able to sequence all the DNA found in human cells. It sequenced only \"euchromatic\" regions of the genome, which make up 92% of the human genome. The other regions, called \"heterochromatic\" are found in centromeres and telomeres, and were not sequenced under the project.\n\nThe Human Genome Project was declared complete in April 2003. An initial rough draft of the human genome was available in June 2000 and by February 2001 a working draft had been completed and published followed by the final sequencing mapping of the human genome on April 14, 2003. Although this was reported to cover 99% of the euchromatic human genome with 99.99% accuracy, a major quality assessment of the human genome sequence was published on May 27, 2004 indicating over 92% of sampling exceeded 99.99% accuracy which was within the intended goal. Further analyses and papers on the HGP continue to occur.\n\nThe sequencing of the human genome holds benefits for many fields, from molecular medicine to human evolution. The Human Genome Project, through its sequencing of the DNA, can help us understand diseases including: genotyping of specific viruses to direct appropriate treatment; identification of mutations linked to different forms of cancer; the design of medication and more accurate prediction of their effects; advancement in forensic applied sciences; biofuels and other energy applications; agriculture, animal husbandry, bioprocessing; risk assessment; bioarcheology, anthropology and evolution. Another proposed benefit is the commercial development of genomics research related to DNA based products, a multibillion-dollar industry.\n\nThe sequence of the DNA is stored in databases available to anyone on the Internet. The U.S. National Center for Biotechnology Information (and sister organizations in Europe and Japan) house the gene sequence in a database known as GenBank, along with sequences of known and hypothetical genes and proteins. Other organizations, such as the UCSC Genome Browser at the University of California, Santa Cruz, and Ensembl present additional data and annotation and powerful tools for visualizing and searching it. Computer programs have been developed to analyze the data, because the data itself is difficult to interpret without such programs. Generally speaking, advances in genome sequencing technology have followed Moore’s Law, a concept from computer science which states that integrated circuits can increase in complexity at an exponential rate. This means that the speeds at which whole genomes can be sequenced can increase at a similar rate, as was seen during the development of the above-mentioned Human Genome Project.\n\nThe process of identifying the boundaries between genes and other features in a raw DNA sequence is called genome annotation and is in the domain of bioinformatics. While expert biologists make the best annotators, their work proceeds slowly, and computer programs are increasingly used to meet the high-throughput demands of genome sequencing projects. Beginning in 2008, a new technology known as RNA-seq was introduced that allowed scientists to directly sequence the messenger RNA in cells. This replaced previous methods of annotation, which relied on inherent properties of the DNA sequence, with direct measurement, which was much more accurate. Today, annotation of the human genome and other genomes relies primarily on deep sequencing of the transcripts in every human tissue using RNA-seq. These experiments have revealed that over 90% of genes contain at least one and usually several alternative splice variants, in which the exons are combined in different ways to produce 2 or more gene products from the same locus.\n\nThe genome published by the HGP does not represent the sequence of every individual's genome. It is the combined mosaic of a small number of anonymous donors, all of European origin. The HGP genome is a scaffold for future work in identifying differences among individuals. Subsequent projects sequenced the genomes of multiple distinct ethnic groups, though as of today there is still only one \"reference genome.\"\n\nKey findings of the draft (2001) and complete (2004) genome sequences include:\n\n\nThe Human Genome Project was started in 1990 with the goal of sequencing and identifying all three billion chemical units in the human genetic instruction set, finding the genetic roots of disease and then developing treatments. It is considered a Mega Project because the human genome has approximately 3.3 billion base-pairs. With the sequence in hand, the next step was to identify the genetic variants that increase the risk for common diseases like cancer and diabetes.\n\nIt was far too expensive at that time to think of sequencing patients’ whole genomes. So the National Institutes of Health embraced the idea for a \"shortcut\", which was to look just at sites on the genome where many people have a variant DNA unit. The theory behind the shortcut was that, since the major diseases are common, so too would be the genetic variants that caused them. Natural selection keeps the human genome free of variants that damage health before children are grown, the theory held, but fails against variants that strike later in life, allowing them to become quite common. (In 2002 the National Institutes of Health started a $138 million project called the HapMap to catalog the common variants in European, East Asian and African genomes.)\n\nThe genome was broken into smaller pieces; approximately 150,000 base pairs in length. These pieces were then ligated into a type of vector known as \"bacterial artificial chromosomes\", or BACs, which are derived from bacterial chromosomes which have been genetically engineered. The vectors containing the genes can be inserted into bacteria where they are copied by the bacterial DNA replication machinery. Each of these pieces was then sequenced separately as a small \"shotgun\" project and then assembled. The larger, 150,000 base pairs go together to create chromosomes. This is known as the \"hierarchical shotgun\" approach, because the genome is first broken into relatively large chunks, which are then mapped to chromosomes before being selected for sequencing.\n\nFunding came from the US government through the National Institutes of Health in the United States, and a UK charity organization, the Wellcome Trust, as well as numerous other groups from around the world. The funding supported a number of large sequencing centers including those at Whitehead Institute, the Sanger Centre, Washington University in St. Louis, and Baylor College of Medicine.\n\nThe United Nations Educational, Scientific and Cultural Organization (UNESCO) served as an important channel for the involvement of developing countries in the Human Genome Project.\n\nIn 1998, a similar, privately funded quest was launched by the American researcher Craig Venter, and his firm Celera Genomics. Venter was a scientist at the NIH during the early 1990s when the project was initiated. The $300,000,000 Celera effort was intended to proceed at a faster pace and at a fraction of the cost of the roughly $3 billion publicly funded project. The Celera approach was able to proceed at a much more rapid rate, and at a lower cost than the public project because it relied upon data made available by the publicly funded project.\n\nCelera used a technique called whole genome shotgun sequencing, employing pairwise end sequencing, which had been used to sequence bacterial genomes of up to six million base pairs in length, but not for anything nearly as large as the three billion base pair human genome.\n\nCelera initially announced that it would seek patent protection on \"only 200–300\" genes, but later amended this to seeking \"intellectual property protection\" on \"fully-characterized important structures\" amounting to 100–300 targets. The firm eventually filed preliminary (\"place-holder\") patent applications on 6,500 whole or partial genes.\nCelera also promised to publish their findings in accordance with the terms of the 1996 \"Bermuda Statement\", by releasing new data annually (the HGP released its new data daily), although, unlike the publicly funded project, they would not permit free redistribution or scientific use of the data. The publicly funded competitors were compelled to release the first draft of the human genome before Celera for this reason. On July 7, 2000, the UCSC Genome Bioinformatics Group released a first working draft on the web. The scientific community downloaded about 500 GB of information from the UCSC genome server in the first 24 hours of free and unrestricted access.\n\nIn March 2000, President Clinton announced that the genome sequence could not be patented, and should be made freely available to all researchers. The statement sent Celera's stock plummeting and dragged down the biotechnology-heavy Nasdaq. The biotechnology sector lost about $50 billion in market capitalization in two days.\n\nAlthough the working draft was announced in June 2000, it was not until February 2001 that Celera and the HGP scientists published details of their drafts. Special issues of \"Nature\" (which published the publicly funded project's scientific paper) and \"Science\" (which published Celera's paper) described the methods used to produce the draft sequence and offered analysis of the sequence. These drafts covered about 83% of the genome (90% of the euchromatic regions with 150,000 gaps and the order and orientation of many segments not yet established). In February 2001, at the time of the joint publications, press releases announced that the project had been completed by both groups. Improved drafts were announced in 2003 and 2005, filling in to approximately 92% of the sequence currently.\n\nIn the IHGSC international public-sector HGP, researchers collected blood (female) or sperm (male) samples from a large number of donors. Only a few of many collected samples were processed as DNA resources. Thus the donor identities were protected so neither donors nor scientists could know whose DNA was sequenced. DNA clones from many different libraries were used in the overall project, with most of those libraries being created by Pieter J. de Jong's. Much of the sequence (>70%) of the reference genome produced by the public HGP came from a single anonymous male donor from Buffalo, New York (code name RP11).\n\nHGP scientists used white blood cells from the blood of two male and two female donors (randomly selected from 20 of each) – each donor yielding a separate DNA library. One of these libraries (RP11) was used considerably more than others, due to quality considerations. One minor technical issue is that male samples contain just over half as much DNA from the sex chromosomes (one X chromosome and one Y chromosome) compared to female samples (which contain two X chromosomes). The other 22 chromosomes (the autosomes) are the same for both sexes.\n\nAlthough the main sequencing phase of the HGP has been completed, studies of DNA variation continue in the International HapMap Project, whose goal is to identify patterns of single-nucleotide polymorphism (SNP) groups (called haplotypes, or “haps”). The DNA samples for the HapMap came from a total of 270 individuals: Yoruba people in Ibadan, Nigeria; Japanese people in Tokyo; Han Chinese in Beijing; and the French Centre d’Etude du Polymorphisme Humain (CEPH) resource, which consisted of residents of the United States having ancestry from Western and Northern Europe.\n\nIn the Celera Genomics private-sector project, DNA from five different individuals were used for sequencing. The lead scientist of Celera Genomics at that time, Craig Venter, later acknowledged (in a public letter to the journal \"Science\") that his DNA was one of 21 samples in the pool, five of which were selected for use.\n\nIn 2007, a team led by Jonathan Rothberg published James Watson's entire genome, unveiling the six-billion-nucleotide genome of a single individual for the first time.\n\nThe work on interpretation and analysis of genome data is still in its initial stages. It is anticipated that detailed knowledge of the human genome will provide new avenues for advances in medicine and biotechnology. Clear practical results of the project emerged even before the work was finished. For example, a number of companies, such as Myriad Genetics, started offering easy ways to administer genetic tests that can show predisposition to a variety of illnesses, including breast cancer, hemostasis disorders, cystic fibrosis, liver diseases and many others. Also, the etiologies for cancers, Alzheimer's disease and other areas of clinical interest are considered likely to benefit from genome information and possibly may lead in the long term to significant advances in their management.\n\nThere are also many tangible benefits for biologists. For example, a researcher investigating a certain form of cancer may have narrowed down their search to a particular gene. By visiting the human genome database on the World Wide Web, this researcher can examine what other scientists have written about this gene, including (potentially) the three-dimensional structure of its product, its function(s), its evolutionary relationships to other human genes, or to genes in mice or yeast or fruit flies, possible detrimental mutations, interactions with other genes, body tissues in which this gene is activated, and diseases associated with this gene or other datatypes. Further, deeper understanding of the disease processes at the level of molecular biology may determine new therapeutic procedures. Given the established importance of DNA in molecular biology and its central role in determining the fundamental operation of cellular processes, it is likely that expanded knowledge in this area will facilitate medical advances in numerous areas of clinical interest that may not have been possible without them.\n\nThe analysis of similarities between DNA sequences from different organisms is also opening new avenues in the study of evolution. In many cases, evolutionary questions can now be framed in terms of molecular biology; indeed, many major evolutionary milestones (the emergence of the ribosome and organelles, the development of embryos with body plans, the vertebrate immune system) can be related to the molecular level. Many questions about the similarities and differences between humans and our closest relatives (the primates, and indeed the other mammals) are expected to be illuminated by the data in this project.\n\nThe project inspired and paved the way for genomic work in other fields, such as agriculture. For example, by studying the genetic composition of Tritium aestivum, the world’s most commonly used bread wheat, great insight has been gained into the ways that domestication has impacted the evolution of the plant. Which loci are most susceptible to manipulation, and how does this play out in evolutionary terms? Genetic sequencing has allowed these questions to be addressed for the first time, as specific loci can be compared in wild and domesticated strains of the plant. This will allow for advances in genetic modification in the future which could yield healthier, more disease-resistant wheat crops.\n\nAt the onset of the Human Genome Project several ethical, legal, and social concerns were raised in regards to how increased knowledge of the human genome could be used to discriminate against people. One of the main concerns of most individuals was the fear that both employers and health insurance companies would refuse to hire individuals or refuse to provide insurance to people because of a health concern indicated by someone's genes. In 1996 the United States passed the Health Insurance Portability and Accountability Act (HIPAA) which protects against the unauthorized and non-consensual release of individually identifiable health information to any entity not actively engaged in the provision of healthcare services to a patient.\n\nAlong with identifying all of the approximately 20,000–25,000 genes in the human genome, the Human Genome Project also sought to address the ethical, legal, and social issues that were created by the onset of the project. For that the Ethical, Legal, and Social Implications (ELSI) program was founded in 1990. Five percent of the annual budget was allocated to address the ELSI arising from the project. This budget started at approximately $1.57 million in the year 1990, but increased to approximately $18 million in the year 2014.\n\nWhilst the project may offer significant benefits to medicine and scientific research, some authors have emphasized the need to address the potential social consequences of mapping the human genome. \"Molecularising disease and their possible cure will have a profound impact on what patients expect from medical help and the new generation of doctors' perception of illness.\"\n\n\n", "id": "5219699", "title": "Human Genome Project"}
{"url": "https://en.wikipedia.org/wiki?curid=2396643", "text": "R2d2 (mouse gene)\n\nR2d2 is a mouse gene that is sometimes a selfish gene. \n\nR2d2 is short for \"Responder to meiotic drive 2\", It was discovered by UNC School of Medicine researchers to display transmission bias.\nR2d2 is a stretch of DNA on mouse chromosome 2 that contains multiple copies of the Cwc22 gene. When seven or more copies of that latter gene are present, \"R2d2\" becomes selfish.\n\nIn one lab breeding population, in a selective sweep, \"R2d2\" increased from being in 50 percent of the lab mice's chromosomes to 85 percent in 10 generations. By 15 generations, it reached fixation.\n\nIn female mice, \"R2d2\" somehow displaces the chromosome that doesn’t contain it and it is preferentially incorporated into eggs. It has spread in the wild to several parts of the world.\n\n", "id": "2396643", "title": "R2d2 (mouse gene)"}
{"url": "https://en.wikipedia.org/wiki?curid=331535", "text": "Nucleic acid sequence\n\nA nucleic acid sequence is a succession of letters that indicate the order of nucleotides within a DNA (using GACT) or RNA (GACU) molecule. By convention, sequences are usually presented from the 5' end to the 3' end. For DNA, the sense strand is used. Because nucleic acids are normally linear (unbranched) polymers, specifying the sequence is equivalent to defining the covalent structure of the entire molecule. For this reason, the nucleic acid sequence is also termed the primary structure.\n\nThe sequence has capacity to represent information. Biological deoxyribonucleic acid represents the information which directs the functions of a living thing.\n\nNucleic acids also have a secondary structure and tertiary structure. Primary structure is sometimes mistakenly referred to as \"primary sequence\". Conversely, there is no parallel concept of secondary or tertiary sequence.\n\nNucleic acids consist of a chain of linked units called nucleotides. Each nucleotide consists of three subunits: a phosphate group and a sugar (ribose in the case of RNA, deoxyribose in DNA) make up the backbone of the nucleic acid strand, and attached to the sugar is one of a set of nucleobases. The nucleobases are important in base pairing of strands to form higher-level secondary and tertiary structure such as the famed double helix.\n\nThe possible letters are \"A\", \"C\", \"G\", and \"T\", representing the four nucleotide bases of a DNA strand — adenine, cytosine, guanine, thymine — covalently linked to a phosphodiester backbone. In the typical case, the sequences are printed abutting one another without gaps, as in the sequence AAAGTCTGAC, read left to right in the 5' to 3' direction. With regards to transcription, a sequence is on the coding strand if it has the same order as the transcribed RNA.\n\nOne sequence can be complementary to another sequence, meaning that they have the base on each position in the complementary (i.e. A to T, C to G) and in the reverse order. For example, the complementary sequence to TTAC is GTAA. If one strand of the double-stranded DNA is considered the sense strand, then the other strand, considered the antisense strand, will have the complementary sequence to the sense strand.\n\nComparing and determining % difference between two nucleotide sequences. \n\nWhile A, T, C, and G represent a particular nucleotide at a position, there are also letters that represent ambiguity which are used when more than one kind of nucleotide could occur at that position. The rules of the International Union of Pure and Applied Chemistry (IUPAC) are as follows:\n\nThese symbols are also valid for RNA, except with U (uracil) replacing T (thymine).\n\nApart from adenine (A), cytosine (C), guanine (G), thymine (T) and uracil (U), DNA and RNA also contain bases that have been modified after the nucleic acid chain has been formed. In DNA, the most common modified base is 5-methylcytidine (m5C). In RNA, there are many modified bases, including pseudouridine (Ψ), dihydrouridine (D), inosine (I), ribothymidine (rT) and 7-methylguanosine (m7G). Hypoxanthine and xanthine are two of the many bases created through mutagen presence, both of them through deamination (replacement of the amine-group with a carbonyl-group). Hypoxanthine is produced from adenine, xanthine from guanine. Similarly, deamination of cytosine results in uracil.\n\nIn biological systems, nucleic acids contain information which is used by a living cell to construct specific proteins. The sequence of nucleobases on a nucleic acid strand is translated by cell machinery into a sequence of amino acids making up a protein strand. Each group of three bases, called a codon, corresponds to a single amino acid, and there is a specific genetic code by which each possible combination of three bases corresponds to a specific amino acid.\n\nThe central dogma of molecular biology outlines the mechanism by which proteins are constructed using information contained in nucleic acids. DNA is transcribed into mRNA molecules, which travels to the ribosome where the mRNA is used as a template for the construction of the protein strand. Since nucleic acids can bind to molecules with complementary sequences, there is a distinction between \"sense\" sequences which code for proteins, and the complementary \"antisense\" sequence which is by itself nonfunctional, but can bind to the sense strand.\n\nDNA sequencing is the process of determining the nucleotide sequence of a given DNA fragment. The sequence of the DNA of a living thing encodes the necessary information for that living thing to survive and reproduce. Therefore, determining the sequence is useful in fundamental research into why and how organisms live, as well as in applied subjects. Because of the importance of DNA to living things, knowledge of a DNA sequence may be useful in practically any biological research. For example, in medicine it can be used to identify, diagnose and potentially develop treatments for genetic diseases. Similarly, research into pathogens may lead to treatments for contagious diseases. Biotechnology is a burgeoning discipline, with the potential for many useful products and services.\n\nRNA is not sequenced directly. Instead, it is copied to a DNA by reverse transcriptase, and this DNA is then sequenced.\n\nCurrent sequencing methods rely on the discriminatory ability of DNA polymerases, and therefore can only distinguish four bases. An inosine (created from adenosine during RNA editing) is read as a G, and 5-methyl-cytosine (created from cytosine by DNA methylation) is read as a C. With current technology, it is difficult to sequence small amounts of DNA, as the signal is too weak to measure. This is overcome by polymerase chain reaction (PCR) amplification.\n\nOnce a nucleic acid sequence has been obtained from an organism, it is stored \"in silico\" in digital format. Digital genetic sequences may be stored in sequence databases, be analyzed (see \"Sequence analysis\" below), be digitally altered and be used as templates for creating new actual DNA using artificial gene synthesis.\n\nDigital genetic sequences may be analyzed using the tools of bioinformatics to attempt to determine its function.\n\nThe DNA in an organism's genome can be analyzed to diagnose vulnerabilities to inherited diseases, and can also be used to determine a child's paternity (genetic father) or a person's ancestry. Normally, every person carries two variations of every gene, one inherited from their mother, the other inherited from their father. The human genome is believed to contain around 20,000 - 25,000 genes. In addition to studying chromosomes to the level of individual genes, genetic testing in a broader sense includes biochemical tests for the possible presence of genetic diseases, or mutant forms of genes associated with increased risk of developing genetic disorders.\n\nGenetic testing identifies changes in chromosomes, genes, or proteins. Usually, testing is used to find changes that are associated with inherited disorders. The results of a genetic test can confirm or rule out a suspected genetic condition or help determine a person's chance of developing or passing on a genetic disorder. Several hundred genetic tests are currently in use, and more are being developed.\n\nIn bioinformatics, a sequence alignment is a way of arranging the sequences of DNA, RNA, or protein to identify regions of similarity that may be due to functional, structural, or evolutionary relationships between the sequences. If two sequences in an alignment share a common ancestor, mismatches can be interpreted as point mutations and gaps as insertion or deletion mutations (indels) introduced in one or both lineages in the time since they diverged from one another. In sequence alignments of proteins, the degree of similarity between amino acids occupying a particular position in the sequence can be interpreted as a rough measure of how conserved a particular region or sequence motif is among lineages. The absence of substitutions, or the presence of only very conservative substitutions (that is, the substitution of amino acids whose side chains have similar biochemical properties) in a particular region of the sequence, suggest that this region has structural or functional importance. Although DNA and RNA nucleotide bases are more similar to each other than are amino acids, the conservation of base pairs can indicate a similar functional or structural role.\n\nComputational phylogenetics makes extensive use of sequence alignments in the construction and interpretation of phylogenetic trees, which are used to classify the evolutionary relationships between homologous genes represented in the genomes of divergent species. The degree to which sequences in a query set differ is qualitatively related to the sequences' evolutionary distance from one another. Roughly speaking, high sequence identity suggests that the sequences in question have a comparatively young most recent common ancestor, while low identity suggests that the divergence is more ancient. This approximation, which reflects the \"molecular clock\" hypothesis that a roughly constant rate of evolutionary change can be used to extrapolate the elapsed time since two genes first diverged (that is, the coalescence time), assumes that the effects of mutation and selection are constant across sequence lineages. Therefore, it does not account for possible difference among organisms or species in the rates of DNA repair or the possible functional conservation of specific regions in a sequence. (In the case of nucleotide sequences, the molecular clock hypothesis in its most basic form also discounts the difference in acceptance rates between silent mutations that do not alter the meaning of a given codon and other mutations that result in a different amino acid being incorporated into the protein.) More statistically accurate methods allow the evolutionary rate on each branch of the phylogenetic tree to vary, thus producing better estimates of coalescence times for genes.\n\nFrequently the primary structure encodes motifs that are of functional importance. Some examples of sequence motifs are: the C/D\nand H/ACA boxes\nof snoRNAs, Sm binding site found in spliceosomal RNAs such as U1, U2, U4, U5, U6, U12 and U3, the Shine-Dalgarno sequence,\nthe Kozak consensus sequence\nand the RNA polymerase III terminator.\n\nPeng found the existence of long-range correlations in the non-coding base pair sequences of DNA. In contrast, such correlations seem not to appear in coding DNA sequences.\n\nIn Bioinformatics, a sequence entropy, also known as sequence complexity or information profile, is a numerical sequence providing a quantitative measure of the local complexity of a DNA sequence, independently of the direction of processing. The manipulations of the information profiles enable the analysis of the sequences using alignment-free techniques, such as for example in motif and rearrangements detection.\n\n", "id": "331535", "title": "Nucleic acid sequence"}
{"url": "https://en.wikipedia.org/wiki?curid=37844522", "text": "Gene therapy for osteoarthritis\n\nGene transfer strategies for medical management of the Osteoarthritis have attracted the attention of scientists due to the complex pathology of this chronic disease. Unlike other pharmacological treatments, gene therapy targets the disease process rather than the symptoms.\n\nPassing from parents to children, genes are the building blocks of inheritance. They contain instructions for making proteins. If genes do not produce the right proteins in a correct way, a child can have a genetic disorder.\nGene therapy is a molecular method aiming to replace defective or absent genes, or to counteract the ones undergoing overexpression. For this purpose, three techniques may be utilized: gene isolation, manipulations, and transferring to target cells. \nThe most common form of gene therapy involves inserting a normal gene to replace an abnormal gene. Other approaches including repairing an abnormal gene and altering the degree to which a gene is turned on or off.\nTwo basic methodologies are utilized to transfer vectors into target tissues; Ex vivo gene transfer and In-vivo gene transfer.\nOne type of gene therapy in which the gene transfer takes place outside the patient's body is called ex vivo gene therapy. This method of gene therapy is more complicated but safer since it is possible to culture, test, and control the modified cells.\n\nOsteoarthritis (OA) is a degenerative joint disease which is the western world's leading cause of pain and disability. It is characterized by the progressive loss of normal structure and function of articular cartilage, the smooth tissue covering the end of the moving bones. This chronic disease not only affects the articular cartilage but the subchondral bone, the synovium and periarticular tissues are other candidates. People with OA can experience severe pain and limited motion. OA is mostly the result of natural aging of the joint due to biochemical changes in the cartilage extracellular matrix.\n\nOsteoarthritis is caused by mechanical factors such as obesity, joint trauma, mechanical overloading of joints or joint-instability. Genetics is also a leading factor that contributes to OA. Studies have shown that genetics is the source of at least 50% of OA cases in the hands and hips. Since the degeneration of cartilage is an irreversible phenomenon, it is incurable, costly and responds poorly to treatment. Due to the prevalence of this disease nowadays, the repair and regeneration of articular cartilage has become a dominant area of research. \nThe growing number of the patients suffering from osteoarthritis and the effectiveness of the current treatments attract a great deal of attention to genetic-based therapeutic methods to cure and prevent the progression of this chronic disease.\n\nVarious vectors have been developed to carry the therapeutic genes to cells. There are two broad categories of gene delivery vectors: Viral vectors, involving viruses and non-viral agents, such as polymers and liposomes.\n\nViral vectors proved to be more successful in transfecting cells as their life cycles require them to transfer their own genes to the host cells with high efficiency. A virus infects the human by inserting its gene directly into his cells. This can be deadly, but the brilliant idea is to take advantage of this natural ability. The idea is to remove all the dangerous genes in the virus and inject the healthy human genes. So, viruses are inserting positive elements to the host cells while attacking them and they will be helpful rather than harmful.\n\nWhile viral vectors are 40% more efficient in transferring genes, they are not fully appreciated for in vivo gene delivery because of their further adverse effects. Primarily, viral vectors induce an inflammatory response, which can cause minor side effects such as mild edema or serious ones like multisystem organ failure. It is also difficult to administer gene therapy repeatedly due to the immune system's enhanced response to viruses. Furthermore, viruses may spread out to other organs after intraarticular injection and this will be an important disadvantage. However, majority of problems associated with gene delivery using viral vectors solved by ex vivo gene delivery method. In Osteoarthritis gene therapy, ex vivo method makes it possible to transfect not only the cells of the synovial lining of joints but also articular chondrocytes and chondroprogenitor cells in cartilage.\n\nNon-viral methods involve complexing therapeutic DNA to various macromolecules including cationic lipids and liposomes, polymers, polyamines and polyethylenimine, and nanoparticles. FuGene 6 and modified cationic liposomes are two non-viral gene delivery methods that have so far been utilized for gene delivery to cartilage. FuGene 6 is a non-liposomal lipid formulation, which has proved to be successful in transfecting a variety of cell lines. Liposomes have shown to be an appropriate candidate for gene delivery, where cationic liposomes are made to facilitate the interaction with the cell membranes and nucleic acids.\nUnlike viral vectors, non-viral ones avoid the risk of acquiring replication competence. They have the capacity to deliver a large amount of therapeutic genes repeatedly, and it is convenient to produce them on a large scale. The most important of all, they do not elicit immune responses in the host organism. In spite of having advantages, non-viral vectors have not yet replaced viral vectors due to relatively low efficiency and short-term transgene expression.\n\nNovel non-viral vectors for osteoarthritis gene delivery including polymeric vectors are still under investigations.\n\nTarget cells in the OA therapy are autologous chondrocytes, Chondroprogenitor cells, Cells within the synovial cavity, and cells of adjacent tissues such as muscle, tendons, ligaments, and meniscus.\nDevelopment of cartilage function and structure may be achieved by: \n\nApproaches influencing several of these processes simultaneously have also shown to be successful, like transferring the combination of inhibitors of catabolism pathways and activators of anabolic events (IGF-I/IL-1RA), as well as that of activators of anabolic and proliferative processes (FGF- 2/SOX9 or FGF-2/IGF-I).\n\nOsteoarthritis has a great degree of heritability. Forms of osteoarthritis caused by single gene mutation have better chance of treatment by gene therapy. \nEpidemiological studies have shown that a genetic component may be an important risk factor in OA. Insulin-like growth factor I genes (IGF-1), Transforming growth factorβ, cartilage oligomeric matrix protein, bone morphogenetic protein, and other anabolic gene candidates are among the candidate genes for OA. Genetic changes in OA can lead to defects of a structural protein such as collagen, or changes in the metabolism of bone and cartilage. OA is rarely considered as a simple disorder following Mendelian inheritance being predominantly a multifactorial disease.\n\nHowever, in the field of OA gene therapy, researches has more focused on gene transfer as a delivery system for therapeutic gene products, rather counteracting genetic abnormalities or polymorphisms. Genes, which contribute to protect and restore the matrix of articular cartilage, are attracting the most attention. These Genes are listed in Table 1. Among all candidates listed below, proteins that block the actions of interleukin-1 (IL-1) or that promote the synthesis of cartilage matrix molecules have received the most experimental scrutiny.\n\nResearches suggest that among all potential mediators, a protein called Interleukin-1 is by far the most potent cause of the pain, joint inflammation and loss of cartilage associated with osteoarthritis. A therapeutic gene used to treat the arthritic joins produces a second protein, which naturally counteracts the effect of interleukin-1. \nThe Interleukin 1 receptor antagonist (IL-1Ra), the natural agonist of IL-1, is a protein that binds non-productively to the cell surface of interleukin-1 receptor, therefore blocks the activities of IL-1 by preventing it from sending a signal to IL-1 receptor. \nThere are three main researches that prove the benefits of local IL-1Ra gene therapy in animal models of osteoarthritis [4]. Series of experiments on canines, rabbits, and horses demonstrate that local IL-1Ra gene therapy is safe and effective in animal models of OA, according to the fact that recombinant human IL-1Ra strongly protected the articular cartilage from degenerative changes.\n\nIn the context of OA, the most attractive intra- articular sites for gene transfer are the synovium and the articular cartilage. Most experimental progress has been made with gene transfer to a convenient intra-articular tissue, such as the synovium, a tissue amenable to genetic modification by a variety of vectors, using both in vivo and ex vivo protocols.\n\nThe major purpose of gene delivery is to alter the lining of the joint in a way that enables them to serve as an endogenous source of therapeutic molecules (Table-1) therapeutic molecules can diffuse and influence the metabolism of adjacent tissues such as cartilage. \nGenes may be delivered to synovium in animal models of RA and OA by direct, in vivo injection of vector or by indirect, ex vivo methods involving autologous synovial cells, skin fibroblasts, or other cell types such as mesenchymal stem cells.\nThe direct in vivo approach is intra-articular insertion of a vector to affect synovicytes. Vectors play crucial role in success of this method. The effect of Different vectors for in vivo gene delivery to synovium is summarized in Table 2:\n\nThe indirect ex vivo approach involves harvest of synovium, isolation and culture of synoviocytes, in vitro transduction, and injection of engineered synovicytes into the joint.\n\nContrary to the synoviocytes which are dividing cells and can be efficacy transduced in vivo using either liposomes or viral vectors, in vivo delivery of genes to chondrocytes is hindered by the dense extra cellular matrix that surrounds these cells. Chondrocytes are non- dividing cells, embedded in a network of collagens and proteoglycans; however researches suggest that genes can be transferred to chondrocytes within normal cartilage by intraarticular injection of liposomes containing sendai virus (HVJ- liposomes) and adeno-associated virus.\n\nMost efficient methods of gene transfer to cartilage have involved ex vivo strategies using chondrocytes or chondroprogenitor cells. Chondrocytes are genetically enhanced by transferring complementary DNA encoding IL-1RA, IGF-1, or matrix break down inhibitors mentioned in Table 1. As discussed before, the transplanted cells could serve as an intra- articular source of therapeutic molecules.\n\nOne important issue related to human gene therapy is safety, particularly for the gene therapy of non-fatal diseases such as OA. The main concern is the high immunogenicity of certain viral vectors. Retroviral vectors integrate into the chromosomes of the cells they infect. There will be always a chance of integrating into a tumor suppressor gene or an oncogene, leading to virulent transformation of the cell. In general, gene transfer to humans is considered as a safe therapeutic method, despite recent events that have provided examples of random adverse events.\n\n", "id": "37844522", "title": "Gene therapy for osteoarthritis"}
{"url": "https://en.wikipedia.org/wiki?curid=51758505", "text": "Q-system (genetics)\n\nQ-system is a genetic tool that allows to express transgenes in a living organism. Originally the Q-system was developed for use in the vinegar fly \"Drosophila melanogaster\", and was rapidly adapted for use in cultured mammalian cells, zebrafish, worms and mosquitoes. The Q-system utilizes genes from the \"qa\" cluster of the bread fungus \"Neurospora crassa\", and consists of four components: the transcriptional activator (QF/QF2/QF2), the enhancer QUAS, the repressor QS, and the chemical de-repressor quinic acid. Similarly to GAL4/UAS and LexA/LexAop, the Q-system is a binary expression system that allows to express reporters or effectors (e.g. fluorescent proteins, ion channels, toxins and other genes) in a defined subpopulation of cells with the purpose of visualising these cells or altering their function. In addition, GAL4/UAS, LexA/LexAop and the Q-system function independently of each other and can be used simultaneously to achieve a desired pattern of reporter expression, or to express several reporters in different subsets of cells.\n\nThe Q-system is based on two out of the seven genes of the \"qa\" gene cluster of the bread fungus \"Neurospora crassa\". The genes of the \"qa\" cluster are responsible for the catabolism of quinic acid, which is used by the fungus as a carbon source in conditions of low glucose. The cluster contains a transcriptional activator \"qa-1F\", a transcriptional repressor \"qa-1S\", and five structural genes. The \"qa-1F\" binds to a specific DNA sequence, found upstream of the \"qa\" genes. The presence of quinic acid disrupts interaction between \"qa-1F\" and \"qa-1S\", thus disinhibiting the transcriptional activity of \"qa-1F\".\nGenes \"qa-1F\", \"qa-1S\" and the DNA binding sequence of \"qa-1F\" form the basis of the Q-system. The genes were renamed to simplify their use as follows: transcriptional activator \"qa-1F\" as QF, repressor \"qa-1S\" as QS, and the DNA binding sequence as QUAS. The quinic acid represents the fourth component of the Q-system.\nThe original transactivator QF appeared to be toxic when expressed broadly in \"Drosophila\". To overcome this problem, two new transactivators were developed: QF2 and QF2\n\nThe Q-system functions similarly to, and independently of, the GAL4/UAS and the LexA/LexAop systems. QF, QF2 and QF2 are analogous to GAL4 and LexA, and their expression is usually under the control of cell-type specific promoter, such as \"nsyb\" (to target neurons) or \"tubulin\" (to target all cells). QUAS is analogous to UAS and LexAop, and is placed upstream of an effector gene, such as GFP. QS is analogous to GAL80, and may be driven by any promoter (e.g. \"tubulin-QS\"). Quinic acid is a unique feature of the Q-system, and it must be fed to the flies or maggots in order to alleviate the QS-induced repression. In some ways, quinic acid is analogous to temperature in the case of GAL80.\nIn its basic form, two transgenic fly lines, one containing a QF transgene and the other one containing a QUAS transgene, are crossed together. Their progeny that had both a QF transgene and a QUAS transgene will be expressing a reporter gene in a subset of cells (e.g. \"nsyb-QF2, QUAS-GFP\" flies express GFP in all neurons). If a fly also expresses QS in some of the cells, the activity of QF will be repressed in these cells, but it may be restored of a fly is fed quinic acid (e.g. a \"nsyb-QF2, QUAS-GFP, tub-QS\" fly expresses no GFP when its diet doesn't contain quinic acid, and expresses GFP in its neurons when fed quinic acid). The use of QS repressor and quinic acid allows to fine-tune the temporal control of transgene expression.\n\nChimeric transactivators GAL4QF and LexAQF allow to combine the use of all three binary expression systems. GAL4QF binds to UAS, and may be repressed by QS while being unaffected by GAL80. Similarly, LexAQF binds to LexAop, and may be repressed by QS. LexAQF represents a useful extension of the LexA/LexAop system that doesn't have its own repressor.\n\nA variety of expression patterns may be achieved by combination of the three binary expression systems and the FLP/FRT or other recombinases. Expression patterns may be constructed as AND, OR, NOR etc. logic gates to e.g. narrow down expression patterns of available GAL4 lines. The resulting expression pattern somewhat depends on the developmental timing of activation of the transcription factors (discussed in ).\n\nQ-system appeared to be working successfully in a variety of organisms. It has been used to drive expression of luciferase, as a proof of principle, in cultured mammalian cells. In zebrafish the Q-system has been successfully used with several tissue-specific promoters, and was shown to work independently of the GAL4/UAS system when expressed in the same cell. In C. elegans the Q-system has been shown to work in muscles and in neuronal tissue. Most recently, the Q-system has been used to target, for the first time, the olfactory neurons of malaria mosquitoes Anopheles gambiae. These successes make the Q-system the system of choice when developing genetic tools for other organisms. Currently the main shortcoming of the Q-system is the low number of available transgenic lines, but it will be overcome as the scientific community creates and shares these resources.\n", "id": "51758505", "title": "Q-system (genetics)"}
{"url": "https://en.wikipedia.org/wiki?curid=52069950", "text": "Thanatotranscriptome\n\nThe thanatotranscriptome denotes (in the fields of biochemistry, microbiology and biophysics of thanatology and in particular forensic) all RNA from the transcript of the part of genome still active or awakened in the internal organs of a dead body for 24 to 48 hours following the time of the death. (It was recently showed that in these 48 hours, some genes continue to be expressed in cells, producing the mRNA and that certain genes are expressed again that had been inhibited since the end of fetal development) \n\nIt can from a serology postmortem characterize transcriptome of tissue particular cell type, or compare the transcriptomes between various conditions experimental.\n\nIt can be complementary to the analysis of thanatomicrobiome to better understand the process of transformation of the necromass in the days following the death;.\n\nCharacterization and quantification of the transcriptome in a tissue \"dead\" given and conditions data can identify genes assets, to determine the regulatory mechanisms of Gene Expression and set networks of gene expression.\n\nThe techniques commonly used for simultaneously measuring the concentration of a large number of different types of mRNA include Microarray, high throughput sequencing said RNA RNA-Seq.\n\nClues to the existence of a post-mortem transcriptome existed at least since the beginning of the 21st century, but in scientific publications the word \"thanatotranscriptome\" seems to have been first proposed by Javan et al. in 2015;\n\nAt the University of Washington, Peter Noble, Alex Pozhitkov and their colleagues recently (2016) confirmed that up to 2 days (48 hours) after the death of mice or zebrafish, many genes still function in their body. \nchanges in the quantities of mRNA in the body prove that hundreds of genes with very different functions awoke just after death 548 genes have thus awakened after the death of zebrafish and 515 in the laboratory mice.\nAmong the genes which thus awake, there are genes involved in the development of the organism, including genes that are normally activated only \"in utero\" or \"in ovo\" (in the egg) during fetal development.\n\nThis information could possibly in the future lead to:\n\n\n", "id": "52069950", "title": "Thanatotranscriptome"}
{"url": "https://en.wikipedia.org/wiki?curid=35752199", "text": "Genetic policy in the United States\n\nGenetic testing is the analysis of human genes, proteins, and certain metabolites, in order to detect inherited disease-related propensities. These tests can predict the risk of disease in adults, as well as establish prenatal and infant prognoses. The benefits can be substantial, but so can the risks. The possible adverse consequences of genetic tests include discrimination in employment and health insurance, and breaches of privacy. Government policies are therefore needed to assure the proper use of genetic tests. The first piece of federal legislation came into effect in 2000.\n\nA second federal law, the Genetic Information Nondiscrimination Act (GINA), has two parts. Title I prohibits genetic discrimination in health insurance. Title II prohibits employment discrimination.\n\nThe first of two pieces of federal legislation to directly address the use of genetic information in the United States was the Executive Order Protecting Federal Employees. Signed into law by U.S. President Bill Clinton on February 8, 2000, the Executive Order prohibited all federal agencies and departments from using genetic information to discriminate in the hiring or promoting of federal employees. The Executive Order was endorsed by the American Medical Association, the American College of Medical Genetics, the National Society of Genetic Counselors, and the Genetic Alliance. The Executive Order also provided explicit genetic privacy regulations within the federal government.\n\nThe second piece of federal legislation to address the use of genetic information and discrimination in the United States was the Genetic Information Nondiscrimination Act (GINA) of 2008. GINA protects U.S. citizens from genetic discrimination in employment as well as in health care and health insurance. The bill was signed into law on May 21, 2008 by President George W. Bush. Prior to the introduction of GINA in 2003, several bills of similar intent were introduced: the Genetic Privacy and Nondiscrimination Act of 1995, the Genetic Fairness Act of 1996, the Genetic Information Nondiscrimination in Health Insurance Act of 1995 and the Genetic Confidentiality and Nondiscrimination Act of 1996.\n\nGINA laws do not protect people from genetic discrimination in every circumstance. It does not apply when an employer has fewer than 15 employees. It does not cover people in the U.S. military or those receiving health benefits through the Veterans Health Administration or Indian Health Service. GINA also does not protect against genetic discrimination in forms of insurance other than health insurance, such as life, disability, or long-term care insurance.\n\nCurrently, legislation pertaining to the use of genetic information and genetic discrimination at the state level varies by state. The first state laws regarding genetic information were typically designed to prohibit genetic discrimination, including prohibiting employers from demanding workers and applicants to provide genetic information as a condition of their employment. More recent laws have permitted individuals to undergo genetic testing when that individual is filing a compensation claim or has requested the test to demonstrate susceptibility to potentially toxic substances.\n\n", "id": "35752199", "title": "Genetic policy in the United States"}
{"url": "https://en.wikipedia.org/wiki?curid=168927", "text": "Somatic cell nuclear transfer\n\nIn genetics and developmental biology, somatic cell nuclear transfer (SCNT) is a laboratory strategy for creating a viable embryo from a body cell and an egg cell. The technique consists of taking an enucleated oocyte (egg cell) and implanting a donor nucleus from a somatic (body) cell. It is used in both therapeutic and reproductive cloning. Dolly the Sheep became famous for being the first successful case of the reproductive cloning of a mammal. \"Therapeutic cloning\" refers to the potential use of SCNT in regenerative medicine; this approach has been championed as an answer to the many issues concerning embryonic stem cells (ESC) and the destruction of viable embryos for medical use, though questions remain on how homologous the two cell types truly are.\n\nThe process of somatic cell nuclear transplant involves two different cells. The first being a female gamete, known as the ovum (egg/oocyte). In human SCNT (Somatic Cell Nuclear Transfer) experiments, these eggs are obtained through consenting donors, utilizing ovarian stimulation. The second being a somatic cell, referring to the cells of the human body. Skin cells, fat cells, and liver cells are only a few examples. The nucleus of the donor egg cell is removed and discarded, leaving it 'deprogrammed.' What is left is a somatic cell and an denucleated egg cell. These are then fused by inserting the somatic cell into the 'empty' ovum. After being inserted into the egg, the somatic cell nucleus is reprogrammed by its host egg cell. The ovum, now containing the somatic cell's nucleus, is stimulated with a shock and will begin to divide. The egg is now viable and capable of producing an adult organism containing all the necessary genetic information from just one parent. Development will ensue normally and after many mitotic divisions, this single cell forms a blastocyst (an early stage embryo with about 100 cells) with an identical genome to the original organism (i.e. a clone). Stem cells can then be obtained by the destruction of this clone embryo for use in therapeutic cloning or in the case of reproductive cloning the clone embryo is implanted into a host mother for further development and brought to term.\n\nSomatic cell nuclear transplantation has become a focus of study in stem cell research. The aim of carrying out this procedure is to obtain pluripotent cells from a cloned embryo. These cells genetically matched the donor organism from which they came.This gives them the ability to create patient specific pluripotent cells, which could then be used in therapies or disease research.\n\nEmbryonic stem cells are undifferentiated cells of an embryo. These cells are deemed to have a pluripotent potential because they have the ability to give rise to all of the tissues found in an adult organism. This ability allows stem cells to create any cell type, which could then be transplanted to replace damaged or destroyed cells. Controversy surrounds human ESC work due to the destruction of viable human embryos. Leading scientists to seek an alternative method of obtaining stem cells, SCNT is one such method.\nA potential use of stem cells genetically matched to a patient would be to create cell lines that have genes linked to a patient's particular disease. By doing so, an \"in vitro\" model could be created, would be useful for studying that particular disease, potentially discovering its pathophysiology, and discovering therapies. For example, if a person with Parkinson's disease donated his or her somatic cells, the stem cells resulting from SCNT would have genes that contribute to Parkinson's disease. The disease specific stem cell lines could then be studied in order to better understand the condition.\n\nAnother application of SCNT stem cell research is using the patient specific stem cell lines to generate tissues or even organs for transplant into the specific patient. The resulting cells would be genetically identical to the somatic cell donor, thus avoiding any complications from immune system rejection.\n\nOnly a handful of the labs in the world are currently using SCNT techniques in human stem cell research. In the United States, scientists at the Harvard Stem Cell Institute, the University of California San Francisco, the Oregon Health & Science University, Stemagen (La Jolla, CA) and possibly Advanced Cell Technology are currently researching a technique to use somatic cell nuclear transfer to produce embryonic stem cells. In the United Kingdom, the Human Fertilisation and Embryology Authority has granted permission to research groups at the Roslin Institute and the Newcastle Centre for Life. SCNT may also be occurring in China.\n\nIn 2005, a South Korean research team led by Professor Hwang Woo-suk, published claims to have derived stem cell lines via SCNT, but supported those claims with fabricated data. Recent evidence has proved that he in fact created a stem cell line from a parthenote.\n\nThough there has been numerous successes with cloning animals, questions remain concerning the mechanisms of reprogramming in the ovum. Despite many attempts, success in creating human nuclear transfer embryonic stem cells has been limited. There lies a problem in the human cell's ability to form a blastocyst; the cells fail to progress past the eight cell stage of development. This is thought to be a result from the somatic cell nucleus being unable to turn on embryonic genes crucial for proper development. These earlier experiments used procedures developed in non-primate animals with little success. A research group from the Oregon Health & Science University demonstrated SCNT procedures developed for primates successfully reprogrammed skin cells into stem cells. The key to their success was utilizing oocytes in metaphase II (MII) of the cell cycle. Egg cells in MII contain special factors in the cytoplasm that have a special ability in reprogramming implanted somatic cell nuclei into cells with pluripotent states. When the ovum's nucleus is removed, the cell loses its genetic information. This has been blamed for why enucleated eggs are hampered in their reprogramming ability. It is theorized the critical embryonic genes are physically linked to oocyte chromosomes, enucleation negatively affects these factors. Another possibility is removing the egg nucleus or inserting the somatic nucleus causes damage to the cytoplast, affecting reprogramming ability. Taking this into account the research group applied their new technique in an attempt to produce human SCNT stem cells. In May 2013, the Oregon group reported the successful derivation of human embryonic stem cell lines derived through SCNT, using fetal and infant donor cells. Using MII oocytes from volunteers and their improved SCNT procedure, human clone embryos were successfully produced. These embryos were of poor quality, lacking a substantial inner cell mass and poorly constructed trophectoderm. The imperfect embryos prevented the acquisition of human ESC. The addition of caffeine during the removal of the ovum's nucleus and injection of the somatic nucleus improved blastocyst formation and ESC isolation. The ESC obtain were found to be capable of producing teratomas, expressed pluripotent transcription factors, and expressed a normal 46XX karyotype, indicating these SCNT were in fact ESC-like. This was the first instance of successfully using SCNT to reprogram human somatic cells. This study used fetal and infantile somatic cells to produce their ESC.\n\nIn April 2014, an international research team expanded on this break through. There remained the question of whether the same success could be accomplished using adult somatic cells. Epigenetic and age related changes were thought to possibly hinder an adult somatic cells ability to be reprogrammed. Implementing the procedure pioneered by the Oregon research group they indeed were able to grow stem cells generated by SCNT using adult cells from two donors, aged 35 and 75.Indicating age does not impede a cells ability to be reprogrammed\n\nLate April 2014, the New York Stem Cell Foundation was successful in creating SCNT stem cells derived from adult somatic cells. One of these lines of stem cells was derived from the donor cells of a type 1 diabetic. The group was then able to successfully culture these stem cells and induce differentiation. When injected into mice, cells of all three of the germ layers successfully formed. The most significant of these cells, were those who expressed insulin and were capable of secreting the hormone. These insulin producing cells could be used for replacement therapy in diabetics, demonstrating real SCNT stem cell therapeutic potential.\n\nThe impetus for SCNT-based stem cell research has been decreased by the development and improvement of alternative methods of generating stem cells. Methods to reprogram normal body cells into pluripotent stem cells were developed in humans in 2007. The following year, this method achieved a key goal of SCNT-based stem cell research: the derivation of pluripotent stem cell lines that have all genes linked to various diseases. Some scientists working on SCNT-based stem cell research have recently moved to the new methods of induced pluripotent stem cells. Though recent studies have put in question how similar iPS cells are to embryonic stem cells. Epigenetic memory in iPS affects the cell lineage it can differentiate into. For instance, an iPS cell derived from a blood cell will be more efficient at differentiating into blood cells, while it will be less efficient at creating a neuron. This raises the question of how well iPS cells can mimic the gold standard ESC in experiments, as stem cells are defined as having the ability to differentiate into any cell type. SCNT stem cells do not pose such a problem and continue to remain relevant in stem cell studies.\n\nThis technique is currently the basis for cloning animals (such as the famous Dolly the sheep), and has been theoretically proposed as a possible way to clone humans. Using SCNT in reproductive cloning has proven difficult with limited success. High fetal and neonatal death make the process very inefficient. Resulting cloned offspring are also plagued with development and imprinting disorders in non-human species. For these reasons, along with moral and ethical objections, reproductive cloning in humans is proscribed in more than 30 countries. Most researchers believe that in the foreseeable future it will not be possible to use the current cloning technique to produce a human clone that will develop to term. It remains a possibility, though critical adjustments will be required to overcome current limitations during early embryonic development in human SCNT.\n\nThere is also the potential for treating diseases associated with mutations in mitochondrial DNA. Recent studies show SCNT of the nucleus of a body cell afflicted with one of these diseases into a healthy oocyte prevents the inheritance of the mitochondrial disease. This treatment does not involve cloning but would produce a child with three genetic parents. A father providing a sperm cell, one mother providing the egg nucleus and another mother providing the enucleated egg cell.\n\nInterspecies nuclear transfer (iSCNT) is a means of somatic cell nuclear transfer used to facilitate the rescue of endangered species, or even to restore species after their extinction. The technique is similar to SCNT cloning which typically is between domestic animals and rodents, or where there is a ready supply of oocytes and surrogate animals. However, the cloning of highly endangered or extinct species requires the use of an alternative method of cloning. Interspecies nuclear transfer utilizes a host and a donor of two different organisms that are closely related species and within the same genus. In 2000, Robert Lanza was able to produce a cloned fetus of a gaur, \"Bos gaurus\", combining it successfully with a domestic cow, \"Bos taurus\".\n\nInterspecies nuclear transfer provides evidence of the universality of the triggering mechanism of the cell nucleus reprogramming. For example, Gupta et al., explored the possibility of producing transgenic cloned embryos by interspecies somatic cell nuclear transfer (iSCNT) of cattle, mice, and chicken donor cells into enucleated pig oocytes. Moreover, NCSU23 medium, which was designed for in vitro culture of pig embryos, was able to support the in vitro development of cattle, mice, and chicken iSCNT embryos up to the blastocyst stage. Furthermore, ovine oocyte cytoplast may be used for remodeling and reprogramming of human somatic cells back to the embryonic stage.\n\nSCNT can be inefficient. Stresses placed on both the egg cell and the introduced nucleus in early research were enormous, resulting in a low percentage of successfully reprogrammed cells. For example, in 1996 Dolly the sheep was born after 277 eggs were used for SCNT, which created 29 viable embryos. Only three of these embryos survived until birth, and only one survived to adulthood. As the procedure was not automated, but had to be performed manually under a microscope, SCNT was very resource intensive. The biochemistry involved in reprogramming the differentiated somatic cell nucleus and activating the recipient egg was also far from understood. However, by 2014, researchers were reporting success rates of 70-80% with cloning pigs and in 2016 a Korean company, Sooam Biotech, was reported to be producing 500 cloned embryos a day.\n\nIn SCNT, not all of the donor cell's genetic information is transferred, as the donor cell's mitochondria that contain their own mitochondrial DNA are left behind. The resulting hybrid cells retain those mitochondrial structures which originally belonged to the egg. As a consequence, clones such as Dolly that are born from SCNT are not perfect copies of the donor of the nucleus. This fact may also hamper the potential benefits of SCNT derived tissues/organs for therapy, as there may be an immunoresponse to the non-self mtDNA after transplant.\n\nProposals to use nucleus transfer techniques in human stem cell research raise a set of concerns beyond the moral status of any created embryo. These have led to some individuals and organizations who are \"not\" opposed to human embryonic stem cell research to be concerned about, or opposed to, SCNT research.\n\nOne concern is that blastula creation in SCNT-based human stem cell research will lead to the reproductive cloning of humans. Both processes use the same first step: the creation of a nuclear transferred embryo, most likely via SCNT. Those who hold this concern often advocate for strong regulation of SCNT to preclude implantation of any derived products for the intention of human reproduction, or its prohibition.\n\nA second important concern is the appropriate source of the eggs that are needed. SCNT requires human eggs, which can only be obtained from women. The most common source of these eggs today are eggs that are produced and in excess of the clinical need during IVF treatment. This is a minimally invasive procedure, but it does carry some health risks, such as ovarian hyperstimulation syndrome.\n\nOne vision for successful stem cell therapies is to create custom stem cell lines for patients. Each custom stem cell line would consist of a collection of identical stem cells each carrying the patient's own DNA, thus reducing or eliminating any problems with rejection when the stem cells were transplanted for treatment. For example, to treat a man with Parkinson's disease, a cell nucleus from one of his cells would be transplanted by SCNT into an egg cell from an egg donor, creating a unique lineage of stem cells almost identical to the patient's own cells. (There would be differences. For example, the mitochondrial DNA would be the same as that of the egg donor. In comparison, his own cells would carry the mitochondrial DNA of his mother.)\n\nPotentially millions of patients could benefit from stem cell therapy, and each patient would require a large number of donated eggs in order to successfully create a single custom therapeutic stem cell line. Such large numbers of donated eggs would exceed the number of eggs currently left over and available from couples trying to have children through assisted reproductive technology. Therefore, healthy young women would need to be induced to sell eggs to be used in the creation of custom stem cell lines that could then be purchased by the medical industry and sold to patients. It is so far unclear where all these eggs would come from.\n\nStem cell experts consider it unlikely that such large numbers of human egg donations would occur in a developed country because of the unknown long-term public health effects of treating large numbers of healthy young women with heavy doses of hormones in order to induce hyperovulation (ovulating several eggs at once). Although such treatments have been performed for several decades now, the long-term effects have not been studied or declared safe to use on a large scale on otherwise healthy women. Longer-term treatments with much lower doses of hormones are known to increase the rate of cancer decades later. Whether hormone treatments to induce hyperovulation could have similar effects is unknown. There are also ethical questions surrounding paying for eggs. In general, marketing body parts is considered unethical and is banned in most countries. Human eggs have been a notable exception to this rule for some time.\n\nTo address the problem of creating a human egg market, some stem cell researchers are investigating the possibility of creating artificial eggs. If successful, human egg donations would not be needed to create custom stem cell lines. However, this technology may be a long way off.\n\nSCNT involving human cells is currently legal for research purposes in the United Kingdom, having been incorporated into the Human Fertilisation and Embryology Act 1990. Permission must be obtained from the Human Fertilisation and Embryology Authority in order to perform or attempt SCNT.\n\nIn the United States, the practice remains legal, as it has not been addressed by federal law. However, in 2002, a moratorium on United States federal funding for SCNT prohibits funding the practice for the purposes of research. Thus, though legal, SCNT cannot be federally funded. American scholars have recently argued that because the product of SCNT is a clone embryo, rather than a human embryo, these policies are morally wrong and should be revised.\n\nIn 2003, the United Nations adopted a proposal submitted by Costa Rica, calling on member states to \"prohibit all forms of human cloning in as much as they are incompatible with human dignity and the protection of human life.\" This phrase may include SCNT, depending on interpretation.\n\nThe Council of Europe's \"Convention on Human Rights and Biomedicine\" and its \"Additional Protocol to the Convention for the Protection of Human Rights and Dignity of the Human Being with regard to the Application of Biology and Medicine, on the Prohibition of Cloning Human Being\" appear to ban SCNT of human beings. Of the Council's 45 member states, the \"Convention\" has been signed by 31 and ratified by 18. The \"Additional Protocol\" has been signed by 29 member nations and ratified by 14.\n\n\n\n", "id": "168927", "title": "Somatic cell nuclear transfer"}
{"url": "https://en.wikipedia.org/wiki?curid=1763082", "text": "History of genetics\n\nThe history of genetics started with the work of the Augustinian friar Gregor Johann Mendel. His work on pea plants, published in 1866, described what came to be known as Mendelian inheritance. Many theories of heredity proliferated in the centuries before and for several decades after Mendel's work.\n\nThe year 1900 marked the \"rediscovery of Mendel\" by Hugo de Vries, Carl Correns and Erich von Tschermak, and by 1915 the basic principles of Mendelian genetics had been applied to a wide variety of organisms—most notably the fruit fly \"Drosophila melanogaster\". Led by Thomas Hunt Morgan and his fellow \"drosophilists\", geneticists developed the Mendelian model, which was widely accepted by 1925. Alongside experimental work, mathematicians developed the statistical framework of population genetics, bringing genetic explanations into the study of evolution.\n\nWith the basic patterns of genetic inheritance established, many biologists turned to investigations of the physical nature of the gene. In the 1940s and early 1950s, experiments pointed to DNA as the portion of chromosomes (and perhaps other nucleoproteins) that held genes. A focus on new model organisms such as viruses and bacteria, along with the discovery of the double helical structure of DNA in 1953, marked the transition to the era of molecular genetics.\n\nIn the following years, chemists developed techniques for sequencing both nucleic acids and proteins, while others worked out the relationship between the two forms of biological molecules: the genetic code. The regulation of gene expression became a central issue in the 1960s; by the 1970s gene expression could be controlled and manipulated through genetic engineering. In the last decades of the 20th century, many biologists focused on large-scale genetics projects, sequencing entire genomes.\n\nThe most influential early theories of heredity were that of Hippocrates and Aristotle. Hippocrates' theory (possibly based on the teachings of Anaxagoras) was similar to Darwin's later ideas on pangenesis, involving heredity material that collects from throughout the body. Aristotle suggested instead that the (nonphysical) form-giving principle of an organism was transmitted through semen (which he considered to be a purified form of blood) and the mother's menstrual blood, which interacted in the womb to direct an organism's early development. For both Hippocrates and Aristotle—and nearly all Western scholars through to the late 19th century—the inheritance of acquired characters was a supposedly well-established fact that any adequate theory of heredity had to explain. At the same time, individual species were taken to have a fixed essence; such inherited changes were merely superficial.\n\nIn the Charaka Samhita of 300CE, ancient Indian medical writers saw the characteristics of the child as determined by four factors: 1) those from the mother’s reproductive material, (2) those from the father’s sperm, (3) those from the diet of the pregnant mother and (4) those accompanying the soul which enters into the foetus. Each of these four factors had four parts creating sixteen factors of which the karma of the parents and the soul determined which attributes predominated and thereby gave the child its characteristics.\n\nIn the 9th century CE, the Afro-Arab writer Al-Jahiz considered the effects of the environment on the likelihood of an animal to survive. In 1000 CE, the Arab physician, Abu al-Qasim al-Zahrawi (known as Albucasis in the West) was the first physician to describe clearly the hereditary nature of haemophilia in his \"Al-Tasrif\". In 1140 CE, Judah HaLevi described dominant and recessive genetic traits in The Kuzari.\n\nIn the 18th century, with increased knowledge of plant and animal diversity and the accompanying increased focus on taxonomy, new ideas about heredity began to appear. Linnaeus and others (among them Joseph Gottlieb Kölreuter, Carl Friedrich von Gärtner, and Charles Naudin) conducted extensive experiments with hybridization, especially species hybrids. Species hybridizers described a wide variety of inheritance phenomena, include hybrid sterility and the high variability of back-crosses.\n\nPlant breeders were also developing an array of stable varieties in many important plant species. In the early 19th century, Augustin Sageret established the concept of dominance, recognizing that when some plant varieties are crossed, certain characteristics (present in one parent) usually appear in the offspring; he also found that some ancestral characteristics found in neither parent may appear in offspring. However, plant breeders made little attempt to establish a theoretical foundation for their work or to share their knowledge with current work of physiology, although Gartons Agricultural Plant Breeders in England explained their system.\n\nIn breeding experiments between 1856 and 1865, Gregor Mendel first traced inheritance patterns of certain traits in pea plants and showed that they obeyed simple statistical rules with some traits being dominant and others being recessive. These patterns of Mendelian inheritance demonstrated that application of statistics to inheritance could be highly useful; they also contradicted 19th century theories of blending inheritance as the traits remained discrete through multiple generation of hybridization. Since that time many more complex forms of inheritance have been demonstrated.\n\nFrom his statistical analysis Mendel defined a concept that he described as a character (which in his mind holds also for \"determinant of that character\"). In only one sentence of his historical paper he used the term \"factors\" to designate the \"material creating\" the character: \" So far as experience goes, we find it in every case confirmed that constant progeny can only be formed when the egg cells and the fertilizing pollen are of like character, so that both are provided with the material for creating quite similar individuals, as is the case with the normal fertilization of pure species. We must therefore regard it as certain that exactly similar factors must be at work also in the production of the constant forms in the hybrid plants.\"(Mendel, 1866).\n\nMendel's work was published in 1866 as \"\"Versuche über Pflanzen-Hybriden\" (Experiments on Plant Hybridization)\" in the \"Verhandlungen des Naturforschenden Vereins zu Brünn (Proceedings of the Natural History Society of Brünn)\", following two lectures he gave on the work in early 1866.\n\nMendel's work was published in a relatively obscure scientific journal, and it was not given any attention in the scientific community. Instead, discussions about modes of heredity were galvanized by Darwin's theory of evolution by natural selection, in which mechanisms of non-Lamarckian heredity seemed to be required. Darwin's own theory of heredity, pangenesis, did not meet with any large degree of acceptance. A more mathematical version of pangenesis, one which dropped much of Darwin's Lamarckian holdovers, was developed as the \"biometrical\" school of heredity by Darwin's cousin, Francis Galton. Under Galton and his successor Karl Pearson, the biometrical school attempted to build statistical models for heredity and evolution, with some limited but real success, though the exact methods of heredity were unknown and largely unquestioned.\n\nIn 1883 August Weismann conducted experiments involving breeding mice whose tails had been surgically removed. His results — that surgically removing a mouse's tail had no effect on the tail of its offspring — challenged the theories of pangenesis and Lamarckism, which held that changes to an organism during its lifetime could be inherited by its descendants. Weismann proposed the germ plasm theory of inheritance, which held that hereditary information was carried only in sperm and egg cells.\n\nHugo de Vries wondered what the nature of germ plasm might be, and in particular he wondered whether or not germ plasm was mixed like paint or whether the information was carried in discrete packets that remained unbroken. In the 1890s he was conducting breeding experiments with a variety of plant species and in 1897 he published a paper on his results that stated that each inherited trait was governed by two discrete particles of information, one from each parent, and that these particles were passed along intact to the next generation. In 1900 he was preparing another paper on his further results when he was shown a copy of Mendel's 1866 paper by a friend who thought it might be relevant to de Vries's work. He went ahead and published his 1900 paper without mentioning Mendel's priority. Later that same year another botanist, Carl Correns, who had been conducting hybridization experiments with maize and peas, was searching the literature for related experiments prior to publishing his own results when he came across Mendel's paper, which had results similar to his own. Correns accused de Vries of appropriating terminology from Mendel's paper without crediting him or recognizing his priority. At the same time another botanist, Erich von Tschermak was experimenting with pea breeding and producing results like Mendel's. He too discovered Mendel's paper while searching the literature for relevant work. In a subsequent paper de Vries praised Mendel and acknowledged that he had only extended his earlier work.\n\nAfter the rediscovery of Mendel's work there was a feud between William Bateson and Pearson over the hereditary mechanism, solved by Ronald Fisher in his work \"The Correlation Between Relatives on the Supposition of Mendelian Inheritance\".\n\nIn 1910, Thomas Hunt Morgan showed that genes reside on specific chromosomes. He later showed that genes occupy specific locations on the chromosome. With this knowledge, Morgan and his students began the first chromosomal map of the fruit fly \"Drosophila\". In 1928, Frederick Griffith showed that genes could be transferred. In what is now known as Griffith's experiment, injections into a mouse of a deadly strain of bacteria that had been heat-killed transferred genetic information to a safe strain of the same bacteria, killing the mouse.\n\nA series of subsequent discoveries led to the realization decades later that the genetic material is made of DNA (deoxyribonucleic acid). In 1941, George Wells Beadle and Edward Lawrie Tatum showed that mutations in genes caused errors in specific steps in metabolic pathways. This showed that specific genes code for specific proteins, leading to the \"one gene, one enzyme\" hypothesis. Oswald Avery, Colin Munro MacLeod, and Maclyn McCarty showed in 1944 that DNA holds the gene's information. In 1952, Rosalind Franklin and Raymond Gosling produced a strikingly clear x-ray diffraction pattern indicating a helical form, and in 1953, James D. Watson and Francis Crick demonstrated the molecular structure of DNA. Together, these discoveries established the central dogma of molecular biology, which states that proteins are translated from RNA which is transcribed by DNA. This dogma has since been shown to have exceptions, such as reverse transcription in retroviruses.\n\nIn 1972, Walter Fiers and his team at the University of Ghent were the first to determine the sequence of a gene: the gene for bacteriophage MS2 coat protein. Richard J. Roberts and Phillip Sharp discovered in 1977 that genes can be split into segments. This led to the idea that one gene can make several proteins. The successful sequencing of many organisms' genomes has complicated the molecular definition of genes. In particular, genes do not seem to sit side by side on DNA like discrete beads. Instead, regions of the DNA producing distinct proteins may overlap, so that the idea emerges that \"genes are one long continuum\". It was first hypothesized in 1986 by Walter Gilbert that neither DNA nor protein would be required in such a primitive system as that of a very early stage of the earth if RNA could perform as simply a catalyst and genetic information storage processor.\n\nThe modern study of genetics at the level of DNA is known as molecular genetics and the synthesis of molecular genetics with traditional Darwinian evolution is known as the modern evolutionary synthesis.\n\n\n\n", "id": "1763082", "title": "History of genetics"}
{"url": "https://en.wikipedia.org/wiki?curid=14235691", "text": "Outline of genetics\n\nThe following outline is provided as an overview of and topical guide to genetics:\n\nGenetics – science of genes, heredity, and variation in living organisms. Genetics deals with the molecular structure and function of genes, and gene behavior in context of a cell or organism (e.g. dominance and epigenetics), patterns of inheritance from parent to offspring, and gene distribution, variation and change in populations.\n\n\n\n\nHistory of genetics\n\n\nHistory of genetics\n\n\n\n\n\n\n\n\n\n", "id": "14235691", "title": "Outline of genetics"}
{"url": "https://en.wikipedia.org/wiki?curid=52953323", "text": "Peritrich nuclear code\n\nThe peritrich nuclear code (translation table 30) is a genetic code used by the nuclear genome of the peritrich ciliates \"Vorticella\" and \"Opisthonecta\".\n\nBases: adenine (A), cytosine (C), guanine (G) and thymine (T) or uracil (U).\n\nAmino acids: Alanine (Ala, A), Arginine (Arg, R), Asparagine (Asn, N), Aspartic acid (Asp, D), Cysteine (Cys, C), Glutamic acid (Glu, E), Glutamine (Gln, Q), Glycine (Gly, G), Histidine (His, H), Isoleucine (Ile, I), Leucine (Leu, L), Lysine (Lys, K), Methionine (Met, M), Phenylalanine (Phe, F), Proline (Pro, P), Serine (Ser, S), Threonine (Thr, T), Tryptophan (Trp, W), Tyrosine (Tyr, Y), and Valine (Val, V).\n\n\n", "id": "52953323", "title": "Peritrich nuclear code"}
{"url": "https://en.wikipedia.org/wiki?curid=53102880", "text": "UPF0488\n\nUPF0488 is a protein that in humans is encoded by the C8orf33 (Chromosome 8 Open Reading Frame 33) gene. Chromosome 8 open reading frame 33 (C8orf33) is a human protein-coding gene of currently unknown function.\n\nThe UPF0488 protein is expressed in low-moderate levels in most tissues with some exceptions. It is predicted to localize in the nucleus and mitochondrion, though several orthologs were also predicted to localize in the cytosol; additionally, there is experimental evidence showing that human C8orf33 may localize in the peroxisomes. The expression of this gene is up-regulated after lithium exposure. C8orf33 is significantly up regulated in breast cancer drug treatment.\n\nSeveral post-translational modifications including phosphorylation, methylation, and acetylation are predicted. Additionally, it has several post-translational modifications such as acetylation, methylation, phosphoprotein – this includes amino acid modifications (or modified residues) such as N-acetylalanine, omega-N-methylarginine, and phosphoserine).\n\nThis gene has 5 transcripts (splice variants), 62 orthologues and is a member of 1 Ensembl protein family. This gene is a member of the Human CCDS set: CCDS34974.1 This gene is a member of the Human CCDS set: CCDS34974. C8orf33 expression profile revealed that this gene was over-expressed after lithium exposure.\n\nC8orf33 (UPF0488) has 31 alternatively spliced exons which combine in 13 different transcript variants –X1 variant is the longest and seems to have the greatest identity. Human tissue RNA sequencing of UPF0488.\n\nUPF0488 has 5 transcripts splice variants. In terms of common gene haplotype alleles, the frequency of haplotype is 96.3% for one variant site. The primary transcript is 3,593 bp while a similar variant is 1,666 bp. The mRNA secondary structure of 3’ and 5’ UTR’s indicate different fold energies. The 5’ UTR region contains a fold energy of -21.20 and consists of 54 bases, the energy of the bases is -0.393. The 3’UTR region contains a fold energy of -646.10, consisting of 1873 bases – while the energy of the bases is -0.345.\n\nAccording to microarray-assessed tissue expression analysis by NCBI GEO, the gene C8orf33 has average expression levels in most tissues save including thyroid gland and parathyroid gland. Expression seems to be low in the pancreas, small intestine and other digestive organs except the kidney which seems relatively higher.\n\nApproximate expression patterns inferred from EST sources. Norway rat putative protein-coding gene. Represented by 30 ESTs from 20 cDNA libraries. EST representation biased toward fetus. Gene expression seems to increase in the obesity-resistant categories\n\nThe promoter region for c8orf33 covers 1191 base pairs of DNA and contains over 700 potential factor binding sites. Fifteen transcription factors with highly conserved binding sites across multiple species’ promoter regions for c8orf33 were selected and shown (see Annotated Promoter Section). CDF1(Cycling DOF Factor 1) physically interacts with FKF1, CDF1 protein is more stable in FKF1 mutants. Another transcription factor, transcription factor II B (TFIIB) is a general transcription factor that is involved in the formation of the RNA polymerase II preinitiation complex (PIC).\n\nThe Isoelectric point of the protein (UPF0488) is 9.16, given a detailed analysis of isoelectric point according to different scales for individual proteins. The Net Charge had been determined using the values available from the Lehninger's Biochemistry book. The precursor protein has a molecular weight of approximately 24.9925 kDa. This is slightly greater than the average pI of 6.81 for the human proteome. It contains repeats from 149 to 166, and 167 to 186. However, the repeats contain a high degree of degeneracy.\n\nUPF0488 is an alanine rich protein relative to other proteins and low in all other amino acids besides arginine, leucine, and proline.\n\nThe evolutionary lineage of UPF0488 can be traced as distant as invertebrates with a rate of evolution greater than that of fibrinogen.\n\nGraph shows divergence of UPF0488 in a given time scale compared to fibrinogen and cytochrome c. Analignment using the SDSC Biology Workbench gives a 27.7% match Danio rerio. The ALIGN calculates a global alignment of two sequences, giving a Global alignment score of 215.\n\nThe mRNA of UPF0488 has a very high level of degeneracy across organisms. Sequences of very low identity to the human mRNA could only be identified in closely related organisms. However, the protein had far more distant relatives, including several invertebrates. Protein alignments for Homo Sapien UPF0488 was performed using the San Diego Workbench; these alignments were performed against several different taxa including vertebrates such as mammalia, reptilia, aves and invertebrates such as insecta. The protein sequences for UPf0488 are very highly conserved amongst close relatives of homo sapiens such as Gorilla Gorilla Gorilla (Gorilla). The similarity in protein sequence is inversely proportional to divergence (MYA) (table of homologs).\n\nC8orf33 activity was found to be associated with G protein-coupled receptor signaling pathway, neuroactive ligand-receptor interaction, calcium signaling pathway and the regulation of the actin cytoskeleton. The following substances interact with UPF0488: 7,8-dihydro-7,8-dihydroxybenzo(a)pyrene 9,10-oxide, benzo(a)pyrene, methotrexate, and vitamin E.\n\nThe expression of the UPF0488 gene increases after treatment with cephaloridine, a semisynthetic derivative of cephalosporin C that inhibits gluconeogenesis in both target (kidney) and non-target (liver) organs.\n", "id": "53102880", "title": "UPF0488"}
{"url": "https://en.wikipedia.org/wiki?curid=50708946", "text": "Genome Project-Write\n\nThe Genome Project - Write (also known as GP-Write) includes sub-projects like Human Genome Project-Write (HGP-Write), formally announced on 2 Jun 2016, is an extension of Genome Projects (aimed at reading genomes since 1984), now to include development of technologies for synthesis and testing of many genomes of microbes, plants and animals. This leverages two decades of work on Synthetic Biology and Artificial gene synthesis.\n\nThe newly created GP-Write project will be managed by the Center of Excellence for Engineering Biology, a new nonprofit organization. The researchers expect that the ability to synthesize large portions of many genomes would result in many scientific and medical advances.\n\nTechnologies for constructing and testing Yeast artificial chromosomes, synthetic yeast genomes (Sc2.0) and virus/phage-resistant bacterial genomes have industrial, agricultural and medical applications.\n\nThe human genome consists of three billion DNA nucleotides, which have been described in the Human Genome Project - Read program, 95% completed in 2004. \nExamples of goals of GP-Write include making cell lines that are resistant to all viruses and synthesis assembly lines to test variants of unknown significance that arise in research and diagnostic sequencing of human genomes (which has been exponentially improving in cost, quality and interpretation).\n\n\n", "id": "50708946", "title": "Genome Project-Write"}
