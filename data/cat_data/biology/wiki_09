{"url": "https://en.wikipedia.org/wiki?curid=624361", "text": "Autophagy\n\nAutophagy (or \"autophagocytosis\") (from the Ancient Greek \"autóphagos\", meaning \"self-devouring\" and \"kýtos\", meaning \"hollow\") is the natural, regulated, destructive mechanism of the cell that disassembles unnecessary or dysfunctional components.\n\nAutophagy allows the orderly degradation and recycling of cellular components. In macroautophagy, targeted cytoplasmic constituents are isolated from the rest of the cell within a double-membraned vesicle known as an autophagosome. The autophagosome eventually fuses with lysosomes and the contents are degraded and recycled. Three forms of autophagy are commonly described: macroautophagy, microautophagy, and chaperone-mediated autophagy (CMA). In disease, autophagy has been seen as an adaptive response to stress, which promotes survival, whereas in other cases it appears to promote cell death and morbidity. In the extreme case of starvation, the breakdown of cellular components promotes cellular survival by maintaining cellular energy levels.\n\nThe name \"autophagy\" was coined by Belgian biochemist Christian de Duve in 1963. The identification of autophagy-related genes in yeast in the 1990s let researchers figure out the mechanisms of autophagy, and led to the award of the 2016 Nobel Prize in Physiology or Medicine to Japanese autophagy researcher Yoshinori Ohsumi.\n\nAutophagy was first observed by Keith R. Porter and his student Thomas Ashford at the Rockefeller Institute. In January 1962 they reported an increased number of lysosomes in rat liver cells after the addition of glucagon, and that some displaced lysosomes towards the centre of the cell contained other cell organelles such as mitochondria. They called this autolysis after Christian de Duve and Alex B. Novikoff. However Porter and Ashford wrongly interpreted their data as lysosome formation (ignoring the pre-existing organelles). Lysosomes could not be cell organelles, but part of cytoplasm such as mitochondria, and that hydrolytic enzymes were produced by microbodies. In 1963 researchers published a detailed ultrastructural description of \"focal cytoplasmic degradation,\" which referenced a 1955 German study of injury-induced sequestration. The study recognized three continuous stages of maturation of the sequestered cytoplasm to lysosomes, and that the process was not limited to injury states that functioned under physiological conditions for \"reutilization of cellular materials,\" and the \"disposal of organelles\" during differentiation. Inspired by this discovery, de Duve christened the phenomena \"autophagy\". Unlike Porter and Ashford, de Duve conceived the term as a part of lysosomal function while describing the role of glucagon as a major inducer of cell degradation in the liver. With his student Peter, he established that lysosomes are responsible for glucagon-induced autophagy. This was the first time the fact that lysosomes were established as the sites of intracellular autophagy.\n\nA new era of autophagy research began in 1990s when several groups of scientists independently discovered autophagy-related genes using the budding yeast. Notably, Yoshinori Ohsumi and Michael Thumm examined starvation-induced non-selective autophagy; in the meantime, Daniel J Klionsky discovered the Cytoplasm-to-Vacuole Targeting (CVT) pathway, which is a form of selective autophagy. They soon found that they were in fact looking at essentially the same pathway, just from different angles. Initially, the genes discovered by these and other yeast groups were given different names (APG, AUT, CVT, GSA, PAG, PAZ, and PDD). A unified nomenclature was advocated in 2003 by the yeast researchers to use ATG to denote autophagy genes. The 2016 Nobel Prize in Physiology or Medicine was awarded to Yoshinori Ohsumi. Dr. Ohsumi’s contribution to autophagy research is well recognized. However, some have pointed out that the award could have been more inclusive.\n\nThe field of autophagy research experienced accelerated growth at the turn of the century. Knowledge of ATG genes provided scientists more convenient tools to dissect functions of autophagy in human health and disease. In 1999, a landmark discovery connecting autophagy with cancer was published by Beth Levine’s group. To this date, relationship between cancer and autophagy continues to be a main theme of autophagy research. The roles of autophagy in neurodegeneration and immune defense also received considerable attention. In 2003, the first Gordon conference on autophagy was held at Waterville. In 2005, Daniel J Klionsky launched “Autophagy”, a scientific journal dedicated to this field. The first Keystone Symposia on autophagy was held in 2007 at Monterey. In 2008, Carol A Mercer created a BHMT fusion protein (GST-BHMT), which showed starvation-induced site-specific fragmentation in cell lines. The degradation of betaine homo-cysteine methyltransferase (BHMT), a metabolic enzyme, could be used to assess autophagy flux in mammalian cells.\n\nThere are four pathways of autophagy and these are mediated by the autophagy-related genes and their associated enzymes.\n\nMacroautophagy is the main pathway, used primarily to eradicate damaged cell organelles or unused proteins. First the phagophore engulfs the material that needs to be degraded, which forms a double membrane known as an autophagosome, around the organelle marked for destruction. The autophagosome then travels through the cytoplasm of the cell to a lysosome, and the two organelles fuse. Within the lysosome, the contents of the autophagosome are degraded via acidic lysosomal hydrolases.\n\nMicroautophagy, on the other hand, involves the direct engulfment of cytoplasmic material into the lysosome. This occurs by invagination, meaning the inward folding of the lysosomal membrane, or cellular protrusion.\n\nChaperone-mediated autophagy, or CMA, is a very complex and specific pathway, which involves the recognition by the hsc70-containing complex. \nThis means that a protein must contain the recognition site for this hsc70 complex which will allow it to bind to this chaperone, forming the CMA- substrate/chaperone complex. This complex then moves to the lysosomal membrane-bound protein that will recognise and bind with the CMA receptor, allowing it to enter the cell. \nUpon recognition, the substrate protein gets unfolded and it is translocated across the lysosome membrane with the assistance of the lysosomal hsc70 chaperone. \nCMA is significantly different from other types of autophagy because it translocates protein material in a one by one manner, and it is extremely selective about what material crosses the lysosomal barrier.\n\nMitophagy is the selective degradation of mitochondria by autophagy. It often occurs to defective mitochondria following damage or stress. Mitophagy promotes turnover of mitochondria and prevents accumulation of dysfunctional mitochondria which can lead to cellular degeneration. It is mediated by Atg32 (in yeast) and NIX and its regulator BNIP3 in mammals. Mitophagy is regulated by PINK1 and parkin proteins. The occurrence of mitophagy is not limited to the damaged mitochondria but also involves undamaged ones.\n\nAutophagy is executed by autophagy-related (Atg) genes. The first autophagy genes were identified by genetic screens conducted in the budding yeast Saccharomyces cerevisiae. Following their identification those genes were functionally characterized and their orthologs in a variety of different organisms were identified and studied. \n\nIn mammals, amino acid sensing and additional signals such as growth factors and reactive oxygen species regulate the activity of the protein kinases mTOR and AMPK. These two kinases regulate autophagy through inhibitory phosphorylation of the Unc-51-like kinases ULK1 and ULK2 (mammalian homologues of Atg1). Induction of autophagy results in the dephosphorylation and activation of the ULK kinases. ULK is part of a protein complex containing Atg13, Atg101 and FIP200. ULK phosphorylates and activates Beclin-1 (mammalian homologue of Atg6), which is also part of a protein complex. The autophagy-inducible Beclin-1 complex contains the proteins p150, Atg14L and the class III phosphatidylinositol 3-phosphate kinase (PI(3)K) VPS34. The active ULK and Beclin-1 complexes re-localize to the site of autophagosome initiation, the phagophore, where they both contribute to the activation of downstream autophagy components. \n\nOnce active, VPS34 phosphorylates the lipid phosphatidylinositol to generate phosphatidylinositol 3-phosphate (PtdIns(3)P) on the surface of the phagophore. The generated PtdIns(3)P is used as a docking point for proteins harboring a PtdIns(3)P binding motif. WIPI2, a PtdIns(3)P binding protein of the WIPI (WD-repeat protein interacting with phosphoinositides) protein family, was recently shown to physically bind Atg16L1. Atg16L1 is a member of an E3-like protein complex involved in one of two ubiquitin-like conjugation systems essential for autophagosome formation. Its binding by WIPI2 recruits it to the phagophore and mediates its activity.\n\nThe first of the two ubiquitin-like conjugation systems involved in autophagy covalently binds the ubiquitin-like protein Atg12 to Atg5. The resulting conjugate protein then binds Atg16L1 to form an E3-like complex which functions as part of the second ubiquitin-like conjugation system. This complex binds and activates Atg3, which covalently attaches mammalian homologues of the ubiquitin-like yeast protein Atg8 (LC3A-C, GATE16, and GABARAPL1-3), the most studied being LC3 proteins, to the lipid phosphatidylethanolamine (PE) on the surface of autophagosomes. Lipidated LC3 contributes to the closure of autophagosomes, and enables the docking of specific cargos and adaptor proteins such as Sequestosome-1/p62. The completed autophagosome then fuses with a lysosome through the actions of multiple proteins, including SNAREs and UVRAG. Following the fusion LC3 is retained on the vesicle's inner side and degraded along with the cargo, while the LC3 molecules attached to the outer side are cleaved off by Atg4 and recycled. The contents of the autolysosome are subsequently degraded and their building blocks are released from the vesicle through the action of permeases.\n\nAutophagy has roles in various cellular functions. One particular example is in yeasts, where the nutrient starvation induces a high level of autophagy. This allows unneeded proteins to be degraded and the amino acids recycled for the synthesis of proteins that are essential for survival. In higher eukaryotes, autophagy is induced in response to the nutrient depletion that occurs in animals at birth after severing off the trans-placental food supply, as well as that of nutrient starved cultured cells and tissues. Mutant yeast cells that have a reduced autophagic capability rapidly perish in nutrition-deficient conditions. Studies on the \"apg\" mutants suggest that autophagy via autophagic bodies is indispensable for protein degradation in the vacuoles under starvation conditions, and that at least 15 APG genes are involved in autophagy in yeast. A gene known as ATG7 has been implicated in nutrient-mediated autophagy, as mice studies have shown that starvation-induced autophagy was impaired in \"atg7\"-deficient mice.\n\nIn microbiology, xenophagy is the autophagic degradation of infectious particles. Cellular autophagic machinery also play an important role in innate immunity. Intracellular pathogens, such as \"Mycobacterium tuberculosis\" (the bacterium which is responsible for tuberculosis) are targeted for degradation by the same cellular machinery and regulatory mechanisms that target host mitochondria for degradation. Incidentally, this is further evidence for the endosymbiotic hypothesis. This process generally leads to the destruction of the invasive organism, although some bacteria can block the maturation of phagosomes into degradative organelles called phagolysosomes. Stimulation of autophagy in infected cells can help overcome this phenomenon, restoring pathogen degradation.\n\n\"Vesicular stomatitis virus\" is believed to be taken up by the autophagosome from the cytosol and translocated to the endosomes where detection takes place by a member of the PRRs called toll-like receptor 7, detecting single stranded RNA. Following activation of the toll-like receptor, intracellular signaling cascades are initiated, leading to induction of interferon and other antiviral cytokines. A subset of viruses and bacteria subvert the autophagic pathway to promote their own replication. Galectin-8 has recently been identified as an intracellular \"danger receptor\", able to initiate autophagy against intracellular pathogens. When galectin-8 binds to a damaged vacuole, it recruits autophagy adaptor such as NDP52 leading to the formation of an autophagosome and bacterial degradation.\n\nAutophagy degrades damaged organelles, cell membranes and proteins, and the failure of autophagy is thought to be one of the main reasons for the accumulation of damaged cells and aging.\n\nOne of the mechanisms of programmed cell death (PCD) is associated with the appearance of autophagosomes and depends on autophagy proteins. This form of cell death most likely corresponds to a process that has been morphologically defined as autophagic PCD. One question that constantly arises, however, is whether autophagic activity in dying cells is the cause of death or is actually an attempt to prevent it. Morphological and histochemical studies so far did not prove a causative relationship between the autophagic process and cell death. In fact, there have recently been strong arguments that autophagic activity in dying cells might actually be a survival mechanism. Studies of the metamorphosis of insects have shown cells undergoing a form of PCD that appears distinct from other forms; these have been proposed as examples of autophagic cell death. Recent pharmacological and biochemical studies have proposed that survival and lethal autophagy can be distinguished by the type and degree of regulatory signaling during stress particularly after viral infection. Although promising, these findings have not been examined in non-viral systems.\n\nResearch suggests that autophagy is required for the lifespan-prolonging effects of caloric restriction. A 2010 French study of nematodes, mice and flies showed that inhibition of autophagy exposed cells to metabolic stress. Resveratrol and the dietary restriction prolonged the lifespan of normal, autophagy-proficient nematodes, but not of nematodes in which autophagy had been inhibited by knocking out Beclin 1 (a known autophagic modulator).\n\nAutophagy is essential for basal homeostasis; it is also extremely important in maintaining muscle homeostasis during physical exercise. Autophagy at the molecular level is only partially understood. A study of mice shows that autophagy is important for the ever-changing demands of their nutritional and energy needs, particularly through the metabolic pathways of protein catabolism. In a 2012 study conducted by the University of Texas Southwestern Medical Center in Dallas, mutant mice (with a knock-in mutation of BCL2 phosphorylation sites to produce progeny that showed normal levels of basal autophagy yet were deficient in stress-induced autophagy) were tested to challenge this theory. Results showed that when compared to a control group, these mice illustrated a decrease in endurance and an altered glucose metabolism during acute exercise.\nAnother study demonstrated that skeletal muscle fibres of collagen VI knockout mice showed signs of degeneration due to an insufficiency of autophagy which led to an accumulation of damaged mitochondria and excessive cell death. Exercise-induced autophagy was unsuccessful however; but when autophagy was induced artificially post-exercise, the accumulation of damaged organelles in collagen VI deficient muscle fibres was prevented and cellular homeostasis was maintained. Both studies demonstrate that autophagy induction may contribute to the beneficial metabolic effects of exercise and that it is essential in the maintaining of muscle homeostasis during exercise, particularly in collagen VI fibres.\n\nWork at the Institute for Cell Biology, University of Bonn, showed that a certain type of autophagy, i.e., chaperone-assisted selective autophagy (CASA), is induced in contracting muscles and is required for maintaining the muscle sarcomere under mechanical tension. The CASA chaperone complex recognizes mechanically damaged cytoskeleton components and directs these components through a ubiquitin-dependent autophagic sorting pathway to lysosomes for disposal. This is necessary for maintaining muscle activity.\n\nBecause autophagy decreases with age and age is a major risk factor for osteoarthritis, the role of autophagy in the development of this disease is suggested. Proteins involved in autophagy are reduced with age in both human and mouse articular cartilage. Mechanical injury to cartilage explants in culture also reduced autophagy proteins. Autophagy is constantly activated in normal cartilage but it is compromised with age and precedes cartilage cell death and structural damage. These results suggest autophagy is a normal protective process (chondroprotection) in the joint.\n\nOftentimes, cancer occurs when several different pathways that regulate cell differentiation are disturbed. Autophagy plays an important role in cancer – both in protecting against cancer as well as potentially contributing to the growth of cancer. Autophagy can contribute to cancer by promoting survival of tumor cells that have been starved, or that degrade apoptotic mediators through autophagy: in such cases, use of inhibitors of the late stages of autophagy (such as chloroquine), on the cells that use autophagy to survive, increases the number of cancer cells killed by antineoplastic drugs.\n\nThe role of autophagy in cancer is one that has been highly researched and reviewed. There is evidence that emphasizes the role of autophagy both as a tumor suppressor as well as a factor in tumor cell survival. However, recent research has been able to show that autophagy is more likely to be used as a tumor suppressor according to several models.\n\nSeveral experiments have been done with mice and varying Beclin1, a protein that regulates autophagy. When the Beclin1 gene was altered to be heterozygous (Beclin 1+/-), the mice were found to be tumor prone. However, when Beclin1 was overexpressed, tumor development was inhibited. Care should be exercised when interpreting phenotypes of beclin mutants and attributing the observations to a defect in autophagy, however: Beclin1 is generally required for phosphatidylinositol 3- phosphate production and as such it affects numerous lysosomal and endosomal functions, including endocytosis and endocytic degradation of activated growth factor receptors. In support of the possibility that Beclin1 affects cancer development through an autophagy-independent pathway is the fact that core autophagy factors which are not known to affect other cellular processes and are definitely not known to affect cell proliferation and cell death, such as Atg7 or Atg5, show a much different phenotype when the respective gene is knocked out, which does not include tumor formation. In addition, full knockout of Beclin1 is embryonic lethal whereas knockout of Atg7 or Atg5 is not.\n\nNecrosis and chronic inflammation also has been shown to be limited through autophagy which helps protect against the formation of tumor cells. Thus these experiments show autophagy's role as a tumor suppressor.\n\nAlternatively, autophagy has also been shown to play a huge role in tumor cell survival. In cancerous cells, autophagy is used as a way to deal with stress on the cell. Once these autophagy related genes were inhibited, cell death was potentiated. The increase in metabolic energy is offset by autophagy functions. These metabolic stresses include hypoxia, nutrient deprivation, and an increase in proliferation. These stresses activate autophagy in order to recycle ATP and maintain survival of the cancerous cells. Autophagy has been shown to enable continued growth of tumor cells by maintaining cellular energy production. By inhibiting autophagy genes in these tumors cells, regression of the tumor and extended survival of the organs affected by the tumors were found. Furthermore, inhibition of autophagy has also been shown to enhance the effectiveness of anticancer therapies.\n\nCells that undergo an extreme amount of stress experience cell death either through apoptosis or necrosis. Prolonged autophagy activation leads to a high turnover rate of proteins and organelles. A high rate above the survival threshold may kill cancer cells with a high apoptotic threshold. This technique can be utilized as a therapeutic cancer treatment.\n\nNew developments in research have found that targeted autophagy may be a viable therapeutic solution in fighting cancer. As discussed above, autophagy plays both a role in tumor suppression and tumor cell survival. Thus, the qualities of autophagy can be used as a strategy for cancer prevention. The first strategy is to induce autophagy and enhance its tumor suppression attributes. The second strategy is to inhibit autophagy and thus induce apoptosis.\n\nThe first strategy has been tested by looking at dose-response anti-tumor effects during autophagy-induced therapies. These therapies have shown that autophagy increases in a dose-dependent manner. This is directly related to the growth of cancer cells in a dose-dependent manner as well. This data supports the development of therapies that will encourage autophagy. Secondly, inhibiting the protein pathways directly known to induce autophagy may also serve as an anticancer therapy.\n\nThe second strategy is based on the idea that autophagy is a protein degradation system used to maintain homeostasis and the findings that inhibition of autophagy often leads to apoptosis. Inhibition of autophagy is riskier as it may lead to cell survival instead of the desired cell death.\n\nNegative regulators of autophagy, such as mTOR, cFLIP, and EGFR are orchestrated to function within different stages of the autophagy cascade. The end-products of autophagic digestion may also serve as a negative- feedback regulatory mechanism to stop prolonged activity .\n\nParkinson disease is a neurodegenerative disorder partially caused by the cell death of brain and brain stem cells in many nuclei like the substantia nigra. Parkinson’s disease is characterized by inclusions of a protein called alpha-synuclien (Lewy bodies) in affected neurons that cells cannot break down. Deregulation of the autophagy pathway and mutation of alleles regulating autophagy are believed to cause neurodegenerative diseases. Autophagy is essential for neuronal survival. Without efficient autophagy, neurons gather ubiquitinated protein aggregates and degrade. Ubiquitinated proteins are proteins that have been tagged with ubiquitin to get degraded. Mutations of synuclien alleles lead to lysosome pH increase and hydrolase inhibition. As a result, lysosomes degradative capacity is decreased. There are several genetic mutations implicated in the disease, including loss of function PINK1 and Parkin. Loss of function in these genes can lead to damaged mitochondrial accumulation and protein aggregates than can lead to cellular degeneration. Mitochondria is involved in Parkinson's disease. In idiopathic Parkinson's disease, the disease is commonly caused by dysfunctional mitochondria, cellular oxidative stress, autophagic alterations and the aggregation of proteins. These can lead to mitochondrial swelling and depolarization.\n\nRecent research has found that extra virgin olive oil can activate autophagy and is likely to be one of the primary reasons a Mediterranean diet (that involves high consumption of olive oil) leads to higher life expectancy and lower incidence of diseases related to aging such as Alzheimers. The study in mice fed with olive oil resulted in an increase in nerve cell autophagy activation compared to controls that had the same diet but without olive oil.\n\nStudy have shown that Cannabisin B, purified from hempseed hull, significantly inhibited cell proliferation by inducing autophagic cell death rather than typical apoptosis in a HepG2 human hepatoblastoma cell model. Cell viability transiently increased upon the addition of a low concentration, but decreased upon the addition of high concentrations. The findings suggest that cannabisin B possesses considerable antiproliferative activity and that it may be utilised as a promising chemopreventive agent against hepatoblastoma disease.\n\n\n", "id": "624361", "title": "Autophagy"}
{"url": "https://en.wikipedia.org/wiki?curid=26395288", "text": "Bulk endocytosis\n\nBulk endocytosis refers to a form of endocytosis of synaptic vesicles at nerve terminals. In bulk endocytosis, compared to clathrin-mediated endocytosis, a larger area of presynaptic plasma membrane is internalised as cisternae or endosomes from which multiple synaptic vesicles can subsequently bud off. Bulk endocytosis is activated specifically during intense stimulation, such as during high-frequency trains of action potentials or in response to membrane depolarization by high extracellular concentrations of potassium.\n\nThe molecular mechanisms of bulk endocytosis have not been determined in detail. However some important signaling events have been described. For example, during high levels of neural activity, presynaptic intracellular calcium activates calcineurin which dephosphorylates dynamin. The F-BAR-protein syndapin interacts with dephosphorylated dynamin and is a crucial factor in anchoring dynamin at the plasma membrane. In line with the hypothesis that syndapin I induces bulk endocytosis, characterization of syndapin I knock-out mice revealed a crucial role of syndapin I in \npresynaptic membrane trafficking processes and accumulation of endocytic intermediates was especially evident under high-capacity retrieval conditions. \nMechanistically, the F-BAR domain protein syndapin I possibly acts through further interactions with Arp2/3 and N-WASP. \nThe GTPase dynamin then pinches off the large membrane-vacuole, which is either degraded or reused for synaptic vesicle production (possibly through clathrin coating).\nClathrin-mediated endocytosis and bulk endocytosis appear to occur concurrently in highly active synaptic terminals. The dephosphorylation of dynamin does not prevent the association of amphiphysin, therefore allowing the two processes to happen independently of each other.\n\n\n", "id": "26395288", "title": "Bulk endocytosis"}
{"url": "https://en.wikipedia.org/wiki?curid=28777526", "text": "Synizesis (biology)\n\nSynizesis refers to a phenomenon sometimes observed in one of the subphases of meiosis. This phenomenon, sometimes referred to as a 'synizetic knot', and contrasted with the chromosome 'bouquet' more typically observed, is characterized by the localization of the meiotic chromosomes in a tight clump on one side of the nucleus. The term synizesis seems to have been coined by McClung in 1905.\n\nThe synizetic knot (Synizesis) was later found to be a technical artifact induced by the feature of strong acidic fixatives used during that time (e.g., Flemming's strong fixative) to precipitate the thread-like delicate chromosomes of the Leptotene stage of first meiotic prophase into a dark staining knot.\n\n", "id": "28777526", "title": "Synizesis (biology)"}
{"url": "https://en.wikipedia.org/wiki?curid=3991708", "text": "Stringent response\n\nThe stringent response, also called stringent control, is a stress response of bacteria and plant chloroplasts in reaction to amino-acid starvation, fatty acid limitation, iron limitation, heat shock and other stress conditions. The stringent response is signaled by the alarmone (p)ppGpp, and modulates transcription of up to 1/3 of all genes in the cell. This in turn causes the cell to divert resources away from growth and division and toward amino acid synthesis in order to promote survival until nutrient conditions improve.\n\nIn \"Escherichia coli\", (p)ppGpp production is mediated by the ribosomal protein L11 (\"rplK\" resp. \"relC\") and the ribosome-associated (p)ppGpp synthetase I, RelA; deacylated tRNA bound in the ribosomal A-site is the primary induction signal. RelA converts GTP and ATP into pppGpp by adding the pyrophosphate from ATP onto the 3' carbon of the ribose in GTP, releasing AMP. pppGpp is converted to ppGpp by the \"gpp\" gene product, releasing Pi. ppGpp is converted to GDP by the \"spoT\" gene product, releasing pyrophosphate (PPi).\nGDP is converted to GTP by the \"ndk\" gene product. Nucleoside triphosphate (NTP) provides the Pi, and is converted to Nucleoside diphosphate (NDP).\n\nIn other bacteria, the stringent response is mediated by a variety of RelA/SpoT Homologue (RSH) proteins, with some having only synthetic, or hydrolytic or both (Rel) activities.\n\nDuring the stringent response, (p)ppGpp accumulation affects the resource-consuming cell processes replication, transcription, and translation. (p)ppGpp is thought to bind RNA polymerase and alter the transcriptional profile, decreasing the synthesis of translational machinery (such as rRNA and tRNA), and increasing the transcription of biosynthetic genes. Additionally, the initiation of new rounds of replication is inhibited and the cell cycle arrests until nutrient conditions improve. Translational GTPases involved in protein biosynthesis are also affected by ppGpp, with Initiation Factor 2 (IF2) being the main target.\n\nChemical reaction catalyzed by RelA:\n\nATP + GTP → AMP + pppGpp\n\nChemical reaction catalyzed by SpoT:\n\nppGpp → GDP + PPi\nor\npppGpp -> GTP + PPi\n\nExtensive Mendeley collection of scientific papers covering stringent response is available here.\n", "id": "3991708", "title": "Stringent response"}
{"url": "https://en.wikipedia.org/wiki?curid=12492716", "text": "Trans-endocytosis\n\nTrans-endocytosis is the biological process where material created in one cell undergoes endocytosis (enters) into another cell. If the material is large enough, this can be observed using an electron microscope. Trans-endocytosis from neurons to glia has been observed using time-lapse microscopy.\n\nTrans-endocytosis also applies to molecules. For example, this process is involved when a part of the protein Notch is cleaved off and undergoes endocytosis into its neighboring cell. Without Notch trans-endocytosis, there would be too many neurons in a developing embryo. Trans-endocytosis is also involved in cell movement when the protein ephrin is bound by its receptor from a neighboring cell.\n", "id": "12492716", "title": "Trans-endocytosis"}
{"url": "https://en.wikipedia.org/wiki?curid=33231793", "text": "Binucleated cells\n\nBinucleated cells are cells that contain two nuclei. This type of cell is most commonly found in cancer cells and may arise from a variety of causes. Binucleation can be easily visualized through staining and microscopy. In general, binucleation has negative effects on cell viability and subsequent mitosis.\n\nThey also occur physiologically in hepatocytes, chondrocytes and in fungi (dikaryon).\n\n\n\nBinucleated cells can be observed using microscopy. Cells must first be fixed to arrest them wherever they are in the cell cycle and to keep their structures from degrading. Their nuclei and tubulin must next be made visible so that binucleation can be identified. DAPI is a dye that binds to DNA and fluoresces blue. For this reason, it is particularly useful at labeling nuclei. Antibody probes can be used to label tubulin fluorescently. The immunofluorescence may then be observed with microscopy. Binucleated cells are most easily identified by viewing tubulin, which surrounds the two nuclei in the cell. Binucleated cells may be mistaken for two cells in close proximity when viewing only nuclei.\n\nBinucleation occurs at a much higher rate in cancer cells. Other identifying features of cancer cells include multipolar spindles, micronuclei, and chromatin bridge. However, the increased rate of binucleation is usually not high enough to make it a conclusive diagnostic tool.\n\nThe fate of binucleated cells depends largely on the type of cell they originated from. A large percentage of binucleated cells arising from normal cells remain in interphase and never enter mitosis again. Cells that contain many mutations before they become binucleate are much more likely to proceed through subsequent rounds of mitosis. One study found that more than 50% of binucleated cells never entered mitosis again while greater than 95% of cancer cells were able to proceed through mitosis. Subsequent rounds of mitosis in binucleated cells have much higher rates of errors in chromosomal disjunction making it much more likely for cells to accumulate mutations.\n", "id": "33231793", "title": "Binucleated cells"}
{"url": "https://en.wikipedia.org/wiki?curid=34276211", "text": "Cytoplasm-to-vacuole targeting\n\nCytoplasm-to-vacuole targeting (Cvt) is an autophagy-related pathway in yeast. Under vegetative conditions it delivers hydrolases, such as aminopeptidase 1 (Ape1), to the vacuole. This makes the cvt pathway the only known biosynthetic pathway to utilize the machinery of autophagy for operation.\n\nThe abbreviation Cvt comes from the emphasis Cytoplasm vacuole targeting, not from Cytoplasm-to-vacuole targeting.\n", "id": "34276211", "title": "Cytoplasm-to-vacuole targeting"}
{"url": "https://en.wikipedia.org/wiki?curid=34462209", "text": "Squelching\n\nSquelching is a biological phenomenon in which a transcriptional activator acts to inhibit the expression of another gene. Squelching has been mostly studied in yeast, and most of the ideas regarding its mechanisms have come from research into modes of transcriptional control in yeast. One important study of this topic was conducted using the Gal4-VP16 artificial transcription factor system, where it was shown that the activating complex formed by VP-16 was sequestering adapters required for transcription of other targets.\n\nThe primary cause of squelching is believed to be the interaction of activator molecules disrupting the biochemical pathways associated with related processes due to structural similarity between the activators and important substrates along that pathway. In particular, the activator binds to transcription factors along alternative biochemical pathways, inhibiting the ability of these transcription factors to bind to their true targets. As in the example above, sequestration of an intermediate in a metabolic pathway is a confounding variable in genetic studies because knowledge of the expected binding targets of the primary molecules involved does not help predict why unexpected behavior results.\n", "id": "34462209", "title": "Squelching"}
{"url": "https://en.wikipedia.org/wiki?curid=417014", "text": "Passive transport\n\nPassive transport is a movement of ions and other atomic or molecular substances across cell membranes without need of energy input. Unlike active transport, it does not require an input of cellular energy because it is instead driven by the tendency of the system to grow in entropy. The rate of passive transport depends on the permeability of the cell membrane, which, in turn, depends on the organization and characteristics of the membrane lipids and proteins. The four main kinds of passive transport are simple diffusion, facilitated diffusion, filtration, and osmosis.\n\nDiffusion is the net movement of material from an area of high concentration to an area with lower concentration. The difference of concentration between the two areas is often termed as the \"concentration gradient\", and diffusion will continue until this gradient has been eliminated. Since diffusion moves materials from an area of higher concentration to an area of lower concentration, it is described as moving solutes \"down the concentration gradient\" (compared with active transport, which often moves material from area of low concentration to area of higher concentration, and therefore referred to as moving the material \"against the concentration gradient\"). \nHowever, in many cases (e.g. passive drug transport) the driving force of passive transport can not be simplified to the concentration gradient. If there are different solutions at the two sides of the membrane with different equilibrium solubility of the drug, the difference in degree of saturation is the driving force of passive membrane transport. It is also true for supersaturated solutions which are more and more important owing to the spreading of the application of amorphous solid dispersions for drug bioavailability enhancement.\n\nSimple diffusion and osmosis are in some ways similar. Simple diffusion is the passive movement of solute from a high concentration to a lower concentration until the concentration of the solute is uniform throughout and reaches equilibrium. Osmosis is much like simple diffusion but it specifically describes the movement of water (not the solute) across a selectively permeable membrane until there is an equal concentration of water and solute on both sides of the membrane. Simple diffusion and osmosis are both forms of passive transport and require none of the cell's ATP energy.\n\nFacilitated diffusion, also called carrier-mediated osmosis, is the movement of molecules across the cell membrane via special transport proteins that are embedded within the cellular membrane. Large, insoluble molecules, such as glucose, vesicles and proteins require a carrier molecule to move through the plasma membrane. Therefore, it will bind with its specific carrier proteins, and the complex will then be bonded to a receptor site and moved through the cellular membrane. Facilitated diffusion is a passive process: the solutes move down their concentration gradient and do not require the expenditure of cellular energy for this process. Carrier proteins and channel proteins allow for the diffusion of molecules across the cell membrane. Carrier proteins undergo conformational alterations to allow molecules to pass, while channel proteins form unblocked pores. \n\nFacilitated diffusion may be achieved as a consequence of charge gradients in addition to concentration gradients. Plant cells create an unequal distribution of charge across their plasma membrane by actively taking up or excluding ions. Active transport of protons by H ATPases alters membrane potential allowing for facilitated passive transport of particular ions such as Potassium down their charge gradient through high affinity transporters and channels.\n\nFiltration is movement of water and solute molecules across the cell membrane due to hydrostatic pressure generated by the cardiovascular system. Depending on the size of the membrane pores, only solutes of a certain size may pass through it. For example, the membrane pores of the Bowman's capsule in the kidneys are very small, and only albumins, the smallest of the proteins, have any chance of being filtered through. On the other hand, the membrane pores of liver cells are extremely large, but not forgetting cells are extremely small to allow a variety of solutes to pass through and be metabolized.\n\nOsmosis is the movement of water molecules across a selectively permeable membrane. The net movement of water molecules through a partially permeable membrane from a solution of high water potential to an area of low water potential. A cell with a less negative water potential will draw in water but this depends on other factors as well such as solute potential (pressure in the cell e.g. solute molecules) and pressure potential (external pressure e.g. cell wall). There are three types of Osmosis solutions: the isotonic solution, hypotonic solution, and hypertonic solution. Isotonic solution is when the extracellular solute concentration is balanced with the concentration inside the cell. In the Isotonic solution, the water molecules still moves between the solutions, but the rates are the same from both directions, thus the water movement is balanced between the inside of the cell as well as the outside of the cell. A hypotonic solution is when the solute concentration outside the cell is lower than the concentration inside the cell. In hypotonic solutions, the water moves into the cell, down its concentration gradient (from higher to lower water concentrations). That can cause the cell to swell. Cells that don't have a cell wall, such as animal cells, could burst in this solution. A hypertonic solution is when the solute concentration is higher (think of hyper - as high) than the concentration inside the cell. In hypertonic solution, the water will move out, causing the cell to shrink.\n\n\n", "id": "417014", "title": "Passive transport"}
{"url": "https://en.wikipedia.org/wiki?curid=11797534", "text": "Density dependence\n\nIn population ecology, density-dependent processes occur when population growth rates are regulated by the density of a population. This article will focus on density-dependence in the context of macroparasite life cycles.\n\nPositive density-dependence, density-dependent facilitation, or the Allee effect describes a situation in which population growth is facilitated by increased population density.\n\nFor dioecious (separate sex) obligatory parasites, mated female worms are required to complete a transmission cycle. At low parasite densities, the probability of a female worm encountering a male worm and forming a mating pair can become so low that reproduction is restricted due to single sex infections. At higher parasite densities, the probability of mating pairs forming and successful reproduction increases. This has been observed in the population dynamics of \"Schistosomes\".\n\nPositive density-dependence processes occur in macroparasite life cycles that rely on vectors with a cibarial armature, such as \"Anopheles\" or \"Culex\" mosquitoes. For \"Wuchereria bancrofti\", a filarial nematode, well-developed cibarial armatures in vectors can damage ingested microfilariae and impede the development of infective L3 larvae. At low microfilariae densities, most microfilariae can be ruptured by teeth, preventing successful development of infective L3 larvae. As more larvae are ingested, the ones that become entangled in the teeth may protect the remaining larvae, which are then left undamaged during ingestion.\n\nPositive density-dependence processes may also occur in macroparasite infections that lead to immunosuppression. \"Onchocerca volvulus\" infection promotes immunosuppressive processes within the human host that suppress immunity against incoming infective L3 larvae. This suppression of anti-parasite immunity causes parasite establishment rates to increase with higher parasite burden.\n\nNegative density-dependence, or density-dependent restriction, describes a situation in which population growth is curtailed by crowding, predators and competition. In cell biology, it describes the reduction in cell division. When a cell population reaches a certain density, the amount of required growth factors and nutrients available to each cell becomes insufficient to allow continued cell growth.\n\nThis is also true for other organisms because an increased density means an increase in intraspecific competition. Greater competition means an individual has a decreased contribution to the next generation i.e. offspring. \nDensity-dependent mortality can be overcompensating, undercompensating or exactly compensating. \n\nThere also exists density-independent inhibition, where other factors such as weather or environmental conditions and disturbances may affect a population's carrying capacity.\n\nAn example of a density-dependent variable is crowding and competition.\n\n Density-dependent fecundity exists, where the birth rate falls as competition increases. In the context of gastrointestinal nematodes, the weight of female \"Ascaris lumbricoides\" and its rates of egg production decrease as host infection intensity increases. Thus, the per-capita contribution of each worm to transmission decreases as a function of infection intensity.\n\n In macroparasite life cycles, density-dependent processes can influence parasite fecundity, survival, and establishment. Density-dependent processes can act across multiple points of the macroparasite life cycle. For filarial worms, density-dependent processes can act at the host/vector interface or within the host/vector life-cycle stages. At the host/vector interface, density-dependence may influence the input of L3 larvae into the host’s skin and the ingestion of microfilariae by the vector. Within the life-cycle stages taking place in the vector, density-dependence may influence the development of L3 larvae in vectors and vector life expectancy. Within the life-cycle stages taking place in the host, density-dependence may influence the development of microfilariae and host life expectancy.\n\nIn reality, combinations of negative (restriction) and positive (facilitation) density-dependent processes occur in the life cycles of parasites. However, the extent to which one process predominates over the other vary widely according to the parasite, vector, and host involved. This is illustrated by the \"W. bancrofti\" life cycle. In \"Culex\" mosquitoes, which lack a well-developed cibarial armature, restriction processes predominate. Thus, the number of L3 larvae per mosquito declines as the number of ingested microfilariae increases. Conversely, in \"Aedes\" and \"Anopheles\" mosquitoes, which have well-developed cibarial armatures, facilitation processes predominate. Consequently, the number of L3 larvae per mosquito increases as the number of ingested microfilariae increases.\n\nNegative density-dependent (restriction) processes contribute to the resilience of macroparasite populations. At high parasite populations, restriction processes tend to restrict population growth rates and contribute to the stability of these populations. Interventions that lead to a reduction in parasite populations will cause a relaxation of density-dependent restrictions, increasing per-capita rates of reproduction or survival, thereby contributing to population persistence and resilience.\n\nContrariwise, positive density-dependent or facilitation processes make elimination of a parasite population more likely. Facilitation processes cause the reproductive success of the parasite to decrease with lower worm burden. Thus, control measures that reduce parasite burden will automatically reduce per-capita reproductive success and increase the likelihood of elimination when facilitation processes predominate.\n\nThe extinction threshold refers to minimum parasite density level for the parasite to persist in a population. Interventions that reduce parasite density to a level below this threshold will ultimately lead to the extinction of that parasite in that population. Facilitation processes increase the extinction threshold, making it easier to achieve using parasite control interventions. Conversely, restriction processes complicates control measures by decreasing the extinction threshold.\n\nAnderson and Gordon (1982) propose that the distribution of macroparasites in a host population is regulated by a combination of positive and negative density-dependent processes. In overdispersed distributions, a small proportion of hosts harbour most of the parasite population. Positive density-dependent processes contribute to overdispersion of parasite populations, whereas negative density-dependent processes contribute to underdispersion of parasite populations. As mean parasite burden increases, negative density-dependent processes become more prominent and the distribution of the parasite population tends to become less overdispersed.\n\nConsequently, interventions that lead to a reduction in parasite burden will tend to cause the parasite distribution to become overdispersed. For instance, time-series data for \"Onchocerciasis\" infection demonstrates that 10 years of vector control lead to reduced parasite burden with a more overdispersed distribution.\n\n", "id": "11797534", "title": "Density dependence"}
{"url": "https://en.wikipedia.org/wiki?curid=1031206", "text": "Pinocytosis\n\nIn cellular biology, pinocytosis, otherwise known as cell expelling, fluid endocytosis, and bulk-phase pinocytosis, is a mode of endocytosis in which small particles are brought out to the mitochondria and then expelled from the cell, forming an invagination, and then suspended within a small vesicle. These pinocytotic vesicles subsequently fuse with lysosomes to hydrolyze (break down) the particles. This process requires energy in the form of adenosine triphosphate (ATP), the chemical compound mostly used as energy in the majority of animal cells.\n\nPinocytosis is used primarily for the expelling of extracellular fluids (ECF). In contrast to phagocytosis, it generates very small amounts of ATP from the wastes of alternative substances such as lipids (fat). Unlike receptor-mediated endocytosis, pinocytosis is nonspecific in the substances that it transports. The cell takes in surrounding fluids, including all solutes present. Pinocytosis also works as phagocytosis; the only difference is that phagocytosis is specific in the substances it transports. Phagocytosis engulfs whole particles, which are later broken down by enzymes, such as cathepsins, and absorbed into the cells. Pinocytosis, on the other hand, is when the cell engulfs already-dissolved or broken-down food.\n\nPinocytosis is non-specific and non-absorptive. Molecule-specific endocytosis is called receptor-mediated endocytosis.\n\nThe word \"pinocytosis\" () uses combining forms of \"pino-\" + \"cyto-\" + \"-osis\", all New Latin from Greek, reflecting \"píno\", to drink, and cytosis. The term was proposed by W. H. Lewis in 1931.\n\nNon-specific, adsorptive pinocytosis is a form of endocytosis, a process in which small particles are taken in by a cell by splitting off small vesicles from the cell surface. Cationic proteins bind to the negative cell surface and are taken up via the clathrin-mediated system, thus the uptake is intermediate between receptor-mediated endocytosis and non-specific, non-adsorptive pinocytosis. The clathrin-coated pits occupy about 2% of the surface area of the cell and only last about a minute, with an estimated 2500 leaving the average cell surface each minute. The clathrin coats are lost almost immediately, and the membrane is subsequently recycled to the cell surface.\n\n\n", "id": "1031206", "title": "Pinocytosis"}
{"url": "https://en.wikipedia.org/wiki?curid=33726612", "text": "Cytostasis\n\nCytostasis (cyto – cell; stasis – stoppage) is the inhibition of cell growth and multiplication.\nCytostatic refers to a cellular component or medicine that inhibits cell growth.\n\nCytostasis is an important prerequisite for structured multicellular organisms. Without regulation of cell growth and division only unorganized heaps of cells would be possible.\n\nChemotherapy of cancer, treatment of skin diseases and treatment of infections are common use cases of cytostatic drugs. Active hygienic products generally contain cytostatic substances.\n\nCytostatic mechanisms and drugs generally occur together with cytotoxic ones.\n\nNitric oxide – activated macrophages produce large amounts of nitric oxide (NO), which induces both cytostasis and cytotoxicity to tumor cells both \"in vitro\" and \"in vivo\". Nitric oxide-induced cytostasis targets ribonucleotide reductase by rapid and reversible inhibition. However, other studies show there could be other targets that are responsible for producing long-lasting cytostasis in cells.\n\nLipopolysaccharide (LPS) and lipid A-associated protein – studies have demonstrated that LPS and LAP are potent macrophage activators that have been shown to stimulate tumoricidal (cytostatic) activity \"in vitro\". LAP and LPS were shown to stimulate C3H/HeJ macrophages to kill target tumor cells. It was concluded that LAP can deliver at least one of the triggering signals necessary for inducing macrophage activity that leads to cytostasis.\n\nPolyunsaturated fatty acid – N-3 and n-6 polyunsaturated fatty acids were found to have a distinct effect on cell growth in certain human urothelial cells. Cystostatic concentrations of n-3 and n-6 PUFA did not induce apoptosis, but did cause permanent cellular growth arrest by effecting the cell cycle. Study shows that metabolites of the lipoxygenase pathway are involved with the antiproliferation induce by PUFA. However, PUFA cytostatic activity is not tumor-specific.\n\nCytostatic agents have been beneficial in fighting tumors with their ability to induce cell growth arrest.\n\nBreast cancer – One study indicates nitric oxide (NO) is able to have a cytostatic effect on the human breast cancer cell line MDA-MB-231. Not only does nitric oxide stop cell growth, the study shows that it can also induce apoptosis after the cancer cells have been exposed to NO over 48 hours\n\nMalignant epithelium – Long-chain polyunsaturated fatty acids inhibit cell division, cause cell cycle arrest, and can induce cell death in malignant epithelial cells from various tissue organs \"in vitro\"\n\n", "id": "33726612", "title": "Cytostasis"}
{"url": "https://en.wikipedia.org/wiki?curid=15354795", "text": "Cellular senescence\n\nCellular senescence is the phenomenon by which normal ploid cells cease to divide. In culture, fibroblasts can reach a maximum of 50 cell divisions before becoming senescent. This phenomenon is known as \"replicative senescence\", or the Hayflick limit. Replicative senescence is the result of telomere shortening that ultimately triggers a DNA damage response. Cells can also be induced to senesce via DNA damage in response to elevated reactive oxygen species (ROS), activation of oncogenes and cell-cell fusion, independent of telomere length. As such, cellular senescence represents a change in \"cell state\" rather than a cell becoming \"aged\" as the name misleadingly suggests. Nonetheless, the number of senescent cells in tissues rises substantially during normal aging. \n\nAlthough senescent cells can no longer replicate, they remain metabolically active and commonly adopt an immunogenic phenotype consisting of a pro-inflammatory secretome, the up-regulation of immune ligands, a pro-survival response, promiscuous gene expression (pGE) and stain positive for senescence-associated β-galactosidase activity. Senescence-associated beta-galactosidase, along with p16, is regarded to be a biomarker of cellular senescence. This results in false positives for maturing tissue macrophages and senescence-associated beta-galactosidase as well as for T-cells p16.\n\nA Senescence Associated Secretory Phenotype (SASP) consisting of inflammatory cytokines, growth factors, and proteases is another highly characteristic feature of senescent cells. SASP contributes to many age-related diseases, including type 2 diabetes and atherosclerosis. The damaging effects of SASP have motivated researchers to develop senolytic chemicals that would kill and eliminate senescent cells to improve health in the elderly. \n\nThe nucleus of senescent cells is characterized by senescence-associated heterochromatin foci (SAHF) and DNA segments with chromatin alterations reinforcing senescence (DNA-SCARS). Senescent cells affect tumour suppression, wound healing and possibly embryonic/placental development and a pathological role in age-related diseases.\n\nMoreover, cellular senescence is not observed in several organisms, including perennial plants, sponges, corals, and lobsters. In those species where cellular senescence is observed, cells eventually become post-mitotic when they can no longer replicate themselves through the process of cellular mitosis; i.e., cells experience \"replicative senescence\". How and why some cells become post-mitotic in some species has been the subject of much research and speculation, but (as noted above) it is sometimes suggested that cellular senescence evolved as a way to prevent the onset and spread of cancer. Somatic cells that have divided many times will have accumulated DNA mutations and would therefore be in danger of becoming cancerous if cell division continued. As such, it is becoming apparent that senescent cells undergo conversion to an immunogenic phenotype that enables them to be eliminated by the immune system.\n\nThe DNA damage response (DDR) arrests cell cycle progression until damages, such as double-strand breaks (DSBs), are repaired. Senescent cells display persistent DDR foci that appear to be resistant to endogenous DNA repair activities. Such senescent cells in culture and tissues from aged mammals retain true DSBs associated with DDR markers. It has been proposed that retained DSBs are major drivers of the aging process (see DNA damage theory of aging).\n\n", "id": "15354795", "title": "Cellular senescence"}
{"url": "https://en.wikipedia.org/wiki?curid=146539", "text": "Senescence\n\nSenescence () or biological aging (also spelled biological ageing) is the gradual deterioration of function characteristic of most complex lifeforms, arguably found in all biological kingdoms, that on the level of the organism increases mortality after maturation. The word \"senescence\" can refer either to cellular senescence or to senescence of the whole organism. It is commonly believed that cellular senescence underlies organismal senescence. The science of biological aging is biogerontology.\n\nSenescence is not the inevitable fate of all organisms and can be delayed. The discovery, in 1934, that calorie restriction can extend lifespan by 50% in rats, and the existence of species having negligible senescence and potentially immortal species such as \"Hydra\", have motivated research into delaying and preventing senescence and thus age-related diseases. Organisms of some taxonomic groups, including some animals, experience chronological decrease in mortality, for all or part of their life cycle. On the other extreme are accelerated aging diseases, rare in humans. There is also the extremely rare and poorly understood \"Syndrome X,\" whereby a person remains physically and mentally an infant or child throughout one's life.\n\nEven if environmental factors do not cause aging, they may affect it; in such a way, for example, overexposure to ultraviolet radiation accelerates skin aging. Different parts of the body may age at different rates. Two organisms of the same species can also age at different rates, so that biological aging and chronological aging are quite distinct concepts.\n\nAlbeit indirectly, senescence is by far the leading cause of death (other than in the trivially accurate sense that cerebral hypoxia, \"i.e.\", lack of oxygen to the brain, is the immediate cause of all human death). Of the roughly 150,000 people who die each day across the globe, about two thirds – 100,000 per day – die of age-related causes; in industrialized nations, moreover, the proportion is much higher, reaching 90%.<ref name=\"doi10.2202/1941-6008.1011\"></ref>\n\nThere are a number of hypotheses as to why senescence occurs; for example, some posit it is programmed by gene expression changes, others that it is the cumulative damage caused by biological processes. Whether senescence as a biological process itself can be slowed down, halted or even reversed, is a subject of current scientific speculation and research.\n\n\"Cellular senescence\" is the phenomenon by which normal diploid cells cease to divide. In culture, fibroblasts can reach a maximum of 50 cell divisions before becoming senescent. This phenomenon is known as \"replicative senescence\", or the Hayflick limit. Replicative senescence is the result of telomere shortening that ultimately triggers a DNA damage response. Cells can also be induced to senesce via DNA damage in response to elevated reactive oxygen species (ROS), activation of oncogenes and cell-cell fusion, independent of telomere length. As such, cellular senescence represents a change in \"cell state\" rather than a cell becoming \"aged\" as the name confusingly suggests.\n\nAlthough senescent cells can no longer replicate, they remain metabolically active and commonly adopt an immunogenic phenotype consisting of a pro-inflammatory secretome, the up-regulation of immune ligands, a pro-survival response, promiscuous gene expression (pGE) and stain positive for senescence-associated β-galactosidase activity. The nucleus of senescent cells is characterized by senescence-associated heterochromatin foci (SAHF) and DNA segments with chromatin alterations reinforcing senescence (DNA-SCARS). Senescent cells affect tumour suppression, wound healing and possibly embryonic/placental development and a pathological role in age-related diseases.\n\nThe experimental elimination of senescent cells from transgenic progeroid mice and non-progeroid, naturally-aged mice led to greater resistance against aging-associated diseases.\n\nAccording to a molecular biomarker of aging known as epigenetic clock,\nthe three major types of cellular senescence, namely replicative senescence, oncogene-induced senescence and DNA damage-induced senescence are distinct because induction of replicative senescence (RS) and oncogene-induced senescence (OIS) were found to be accompanied by epigenetic aging of primary cells but senescence induced by DNA damage was not, even though RS and OIS activate the cellular DNA damage response pathway. These results highlight the independence of cellular senescence from epigenetic aging. Consistent with this, telomerase-immortalised cells continued to age (according to the epigenetic clock) without having been treated with any senescence inducers or DNA-damaging agents, re-affirming the independence of the process of epigenetic ageing from telomeres, cellular senescence, and the DNA damage response pathway. Although the uncoupling of senescence from cellular aging appears at first sight to be inconsistent with the fact that senescent cells contribute to the physical manifestation of organism ageing, as demonstrated by Baker et al., where removal of senescent cells slowed down aging. \nHowever, the epigenetic clock analysis of senescence suggests that cellular senescence is a state that cells are forced into as a result of external pressures such as DNA damage, ectopic oncogene expression and exhaustive proliferation of cells to replenish those eliminated by external/environmental factors. These senescent cells, in sufficient numbers, will undoubtedly cause the deterioration of tissues, which is interpreted as organism ageing. However, at the cellular level, aging, as measured by the epigenetic clock, is distinct from senescence. It is an intrinsic mechanism that exists from the birth of the cell and continues. This implies that if cells are not shunted into senescence by the external pressures described above, they would still continue to age. This is consistent with the fact that mice with naturally long telomeres still age and eventually die even though their telomere lengths are far longer than the critical limit, and they age prematurely when their telomeres are forcibly shortened, due to replicative senescence. Hence senescence is a route by which cells exit prematurely from the natural course of cellular ageing.\n\n\"Organismal senescence\" is the aging of whole organisms.\nIn general, aging is characterized by the declining ability to respond to stress, increased homeostatic imbalance, and increased risk of aging-associated diseases. Death is the ultimate consequence of aging, though \"old age\" is not a scientifically recognized cause of death because there is always a specific proximal cause, such as cancer, heart disease, or liver failure. Aging of whole organisms is therefore a complex process that can be defined as \"a progressive deterioration of physiological function, an intrinsic age-related process of loss of viability and increase in vulnerability.\"\n\nDifferences in maximum life span among species correspond to different \"rates of aging.\" For example, inherited differences in the rate of aging make a mouse elderly at 3 years and a human elderly at 80 years. These genetic differences affect a variety of physiological processes, including the efficiency of DNA repair, antioxidant enzymes, and rates of free radical production.\n\nSenescence of the organism gives rise to the Gompertz–Makeham law of mortality, which says that mortality rate accelerates rapidly with age.\n\nSome animals, such as some reptiles and fish, age slowly (negligible senescence) and exhibit very long lifespans. Some even exhibit \"negative senescence\", in which mortality falls with age, in disagreement with the Gompertz–Makeham \"law\".\n\nWhether replicative senescence (Hayflick limit) plays a causative role in organismal aging is at present an active area of investigation.\n\nThe oft-quoted evolutionary theorist George Williams wrote, \"It is remarkable that after a seemingly miraculous feat of morphogenesis, a complex metazoan should be unable to perform the much simpler task of merely maintaining what is already formed.\"\n\nThere is a current debate as to whether or not the pursuit of longevity and the postponement of senescence are cost-effective health care goals given finite health care resources. Because of the accumulated infirmities of old age, bioethicist Ezekiel Emanuel, opines that the pursuit of longevity via the compression of morbidity hypothesis is a \"fantasy\" and that human life is not worth living after age 75; longevity then should not be a goal of health care policy. This opinion has been contested by neurosurgeon and medical ethicist Miguel Faria, who states that life can be worthwhile during old age, and that longevity should be pursued in association with the attainment of quality of life. Faria claims that postponement of senescence as well as happiness and wisdom can be attained in old age in a large proportion of those who lead healthy lifestyles and remain intellectually active.\n\nThe exact etiology of senescence is still largely unclear and yet to be discovered. The process of senescence is complex, and may derive from a variety of different mechanisms and exist for a variety of different reasons. However, senescence is not universal. In a few simple species, such as those in the genus \"Hydra\", senescence is negligible and cannot be detected.\n\nAnother related mechanism is that of the biologically immortal planarian flatworms, which have \"apparently limitless [telomere] regenerative capacity fueled by a population of highly proliferative adult stem cells.\" These organisms are biologically immortal but not immortal in the traditional sense as they are nonetheless susceptible to trauma and infectious and non-infectious disease. Moreover, average lifespans can vary greatly within and between species. This suggests that both genetic and environmental factors contribute to aging.\n\nIn general, theories that explain senescence have been divided between the programmed and stochastic theories of aging. Programmed theories imply that aging is regulated by biological clocks operating throughout the lifespan. This regulation would depend on changes in gene expression that affect the systems responsible for maintenance, repair, and defense responses. The reproductive-cell cycle theory suggests that aging is caused by changes in hormonal signaling over the lifespan. Stochastic theories blame environmental impacts on living organisms that induce cumulative damage at various levels as the cause of aging, examples of which ranging from damage to DNA, damage to tissues and cells by oxygen radicals (widely known as free radicals countered by the even more well-known antioxidants), and cross-linking.\n\nHowever, aging is seen as a progressive failure of homeodynamics–systemic preservation of homeostasis, involving genes for maintenance and repair, stochastic events leading to molecular damage and molecular heterogeneity, and chance events determining the probability of death. Since complex and interacting systems of maintenance and repair comprise the homeodynamic space of a biological system, aging is considered to be a progressive shrinkage of homeodynamic space mainly due to increased molecular heterogeneity. In 2013, a group of scientists defined nine hallmarks of aging that are common between organisms with emphasis on mammals: genomic instability, telomere attrition, epigenetic alterations, loss of proteostasis, deregulated nutrient sensing, mitochondrial dysfunction, cellular senescence, stem cell exhaustion, and altered intercellular communication.\n\nA gene can be expressed at various stages of life. Therefore, natural selection can support lethal and harmful alleles, if their expression occurs after reproduction. Senescence may be the product of such selection. In addition, ageing is believed to have evolved because of the increasingly smaller probability of an organism still being alive at older age, due to predation and accidents, both of which may be random and age-invariant. The antagonistic plietropy theory states that strategies which result in a higher reproductive rate at a young age, but shorter overall lifespan, result in a higher lifetime reproductive success and are therefore favoured by natural selection. In essence, aging is, therefore, the result of investing resources in reproduction, rather than maintenance of the body (the \"Disposable Soma\" theory), in light of the fact that accidents, predation, and disease kill organisms regardless of how much energy is devoted to repair of the body. Various other theories of aging exist, and are not necessarily mutually exclusive.\n\nThe geneticist J. B. S. Haldane wondered why the dominant mutation that causes Huntington's disease remained in the population, and why natural selection had not eliminated it. The onset of this neurological disease is (on average) at age 45 and is invariably fatal within 10–20 years. Haldane assumed that, in human prehistory, few survived until age 45. Since few were alive at older ages and their contribution to the next generation was therefore small relative to the large cohorts of younger age groups, the force of selection against such late-acting deleterious mutations was correspondingly small. However, if a mutation affected younger individuals, selection against it would be strong. Therefore, late-acting deleterious mutations could accumulate in populations over evolutionary time through genetic drift, which has been demonstrated experimentally. This concept of higher accumulation of deleterious mutations for older organisms came to be known as the selection shadow.\n\nPeter Medawar formalised this observation in his mutation accumulation theory of aging. \"The force of natural selection weakens with increasing age—even in a theoretically immortal population, provided only that it is exposed to real hazards of mortality. If a genetic disaster... happens late enough in individual life, its consequences may be completely unimportant\". The 'real hazards of mortality' are, in typical circumstances, predation, disease, and accidents. So, even an immortal population, whose fertility does not decline with time, will have fewer individuals alive in older age groups. This is called 'extrinsic mortality'. Young cohorts, not depleted in numbers yet by extrinsic mortality, contribute far more to the next generation than the few remaining older cohorts, so the force of selection against late-acting deleterious mutations, which affect only these few older individuals, is very weak. The mutations may not be selected against, therefore, and may spread over evolutionary time into the population.\n\nThe major testable prediction made by this model is that species that have high extrinsic mortality in nature will age more quickly and have shorter intrinsic lifespans. This is borne out among mammals, the best-studied in terms of life history. There is a correlation among mammals between body size and lifespan, such that larger species live longer than smaller species under controlled/optimum conditions, but there are notable exceptions. For instance, many bats and rodents are of similar size, yet bats live much longer. For instance, the little brown bat, half the size of a mouse, can live 30 years in the wild. A mouse will only live 2–3 years even under optimum conditions. The explanation is that bats have fewer predators, and therefore low extrinsic mortality. More individuals survive to later ages, so the force of selection against late-acting deleterious mutations is stronger. Fewer late-acting deleterious mutations equates to slower aging and therefore a longer lifespan. Birds are also warm-blooded and are similar in size to many small mammals, yet often live 5–10 times as long. They have less predation pressure than ground-dwelling mammals. Seabirds, which, in general, have the fewest predators of all birds, live longest.\n\nWhen examining the body-size vs. lifespan relationship, one also observes that predatory mammals tend to live longer than prey mammals in a controlled environment, such as a zoo or nature reserve. The explanation for the long lifespans of primates (such as humans, monkeys, and apes) relative to body size is that their intelligence, and often their sociality, help them avoid becoming prey. High position in the food chain, intelligence and cooperativeness all reduce extrinsic mortality in species.\n\nAnother evolutionary theory of aging was proposed by George C. Williams and involves antagonistic pleiotropy. A single gene may affect multiple traits. Some traits that increase fitness early in life may also have negative effects later in life. But, because many more individuals are alive at young ages than at old ages, even small positive effects early can be strongly selected for, and large negative effects later may be very weakly selected against. Williams suggested the following example: Perhaps a gene codes for calcium deposition in bones, which promotes juvenile survival and will therefore be favored by natural selection; however, this same gene promotes calcium deposition in the arteries, causing negative atherosclerotic effects in old age. Thus, harmful biological changes in old age may result from selection for pleiotropic genes that are beneficial early in life but harmful later on. In this case, selection pressure is relatively high when Fisher's reproductive value is high and relatively low when Fisher's reproductive value is low.\n\nA number of genetic components of aging have been identified using model organisms, ranging from the simple budding yeast \"Saccharomyces cerevisiae\" to worms such as \"Caenorhabditis elegans\" and fruit flies (\"Drosophila melanogaster\"). Study of these organisms has revealed the presence of at least two conserved aging pathways.\n\nOne of these pathways involves the gene \"Sir2\", a NAD+-dependent histone deacetylase. In yeast, Sir2 is required for genomic silencing at three loci: the yeast mating loci, the telomeres and the ribosomal DNA (rDNA). In some species of yeast, replicative aging may be partially caused by homologous recombination between rDNA repeats; excision of rDNA repeats results in the formation of extrachromosomal rDNA circles (ERCs). These ERCs replicate and preferentially segregate to the mother cell during cell division, and are believed to result in cellular senescence by titrating away (competing for) essential nuclear factors. ERCs have not been observed in other species (nor even all strains of the same yeast species) of yeast (which also display replicative senescence), and ERCs are not believed to contribute to aging in higher organisms such as humans (they have not been shown to accumulate in mammals in a similar manner to yeast). Extrachromosomal circular DNA (eccDNA) has been found in worms, flies, and humans. The origin and role of eccDNA in aging, if any, is unknown.\n\nDespite the lack of a connection between circular DNA and aging in higher organisms, extra copies of Sir2 are capable of extending the lifespan of both worms and flies (though, in flies, this finding has not been replicated by other investigators, and the activator of Sir2 resveratrol does not reproducibly increase lifespan in either species.) Whether the Sir2 homologues in higher organisms have any role in lifespan is unclear, but the human SIRT1 protein has been demonstrated to deacetylate p53, Ku70, and the forkhead family of transcription factors. SIRT1 can also regulate acetylates such as CBP/p300, and has been shown to deacetylate specific histone residues.\n\nRAS1 and RAS2 also affect aging in yeast and have a human homologue. RAS2 overexpression has been shown to extend lifespan in yeast.\n\nOther genes regulate aging in yeast by increasing the resistance to oxidative stress. Superoxide dismutase, a protein that protects against the effects of mitochondrial free radicals, can extend yeast lifespan in stationary phase when overexpressed.\n\nIn higher organisms, aging is likely to be regulated in part through the insulin/IGF-1 pathway. Mutations that affect insulin-like signaling in worms, flies, and the growth hormone/IGF1 axis in mice are associated with extended lifespan. In yeast, Sir2 activity is regulated by the nicotinamidase PNC1. PNC1 is transcriptionally upregulated under stressful conditions such as caloric restriction, heat shock, and osmotic shock. By converting nicotinamide to niacin, nicotinamide is removed, inhibiting the activity of Sir2. A nicotinamidase found in humans, known as PBEF, may serve a similar function, and a secreted form of PBEF known as visfatin may help to regulate serum insulin levels. It is not known, however, whether these mechanisms also exist in humans, since there are obvious differences in biology between humans and model organisms.\n\nSir2 activity has been shown to increase under calorie restriction. Due to the lack of available glucose in the cells, more NAD+ is available and can activate Sir2. Resveratrol, a stilbenoid found in the skin of red grapes, was reported to extend the lifespan of yeast, worms, and flies (the lifespan extension in flies and worms have proved to be irreproducible by independent investigators). It has been shown to activate Sir2 and therefore mimics the effects of calorie restriction, if one accepts that caloric restriction is indeed dependent on Sir2.\n\nGene expression is imperfectly controlled, and it is possible that random fluctuations in the expression levels of many genes contribute to the aging process as suggested by a study of such genes in yeast. Individual cells, which are genetically identical, none-the-less can have substantially different responses to outside stimuli, and markedly different lifespans, indicating the epigenetic factors play an important role in gene expression and aging as well as genetic factors.\n\nAccording to the GenAge database of aging-related genes there are over 700 genes associated with aging in model organisms: 555 in the soil roundworm (\"Caenorhabditis elegans\"), 87 in the bakers' yeast (\"Saccharomyces cerevisiae\"), 75 in the fruit fly (\"Drosophila melanogaster\") and 68 in the mouse (\"Mus musculus\").\nThe following is a list of genes connected to longevity through research on model organisms:\nAs noted above, senescence is not universal. It was once thought that senescence did not occur in single-celled organisms that reproduce through the process of cellular mitosis. Recent investigation has unveiled a more complex picture. Single cells do accumulate age-related damage. On mitosis the debris is not evenly divided between the new cells. Instead it passes to one of the cells leaving the other cell pristine. With successive generations the cell population becomes a mosaic of cells with half ageless and the rest with varying degrees of senescence.\n\nMoreover, cellular senescence is not observed in several organisms, including perennial plants, sponges, corals, and lobsters. In those species where cellular senescence is observed, cells eventually become post-mitotic when they can no longer replicate themselves through the process of cellular mitosis; i.e., cells experience \"replicative senescence\". How and why some cells become post-mitotic in some species has been the subject of much research and speculation, but (as noted above) it is sometimes suggested that cellular senescence evolved as a way to prevent the onset and spread of cancer. Somatic cells that have divided many times will have accumulated DNA mutations and would therefore be in danger of becoming cancerous if cell division continued. As such, it is becoming apparent that senescent cells undergo conversion to an immunogenic phenotype that enables them to be eliminated by the immune system.\n\nLately, the role of telomeres in cellular senescence has aroused general interest, especially with a view to the possible genetically adverse effects of cloning. The successive shortening of the chromosomal telomeres with each cell cycle is also believed to limit the number of divisions of the cell, thus contributing to aging. There have, on the other hand, also been reports that cloning could alter the shortening of telomeres. Some cells do not age and are, therefore, described as being \"biologically immortal\". It is theorized by some that when it is discovered exactly what allows these cells, whether it be the result of telomere lengthening or not, to divide without limit that it will be possible to genetically alter other cells to have the same capability. It is further theorized that it will eventually be possible to genetically engineer all cells in the human body to have this capability by employing gene therapy and, therefore, stop or reverse aging, effectively making the entire organism potentially immortal.\n\nThe length of the telomere strand has senescent effects; telomere shortening activates extensive alterations in alternative RNA splicing that produce senescent toxins such as progerin, which degrades the tissue and makes it more prone to failure.\n\nCancer cells are usually immortal. In about 85% of tumors, this evasion of cellular senescence is the result of up-activation of their telomerase genes. This simple observation suggests that reactivation of telomerase in healthy individuals could greatly increase their cancer risk. \n\nNed Sharpless and collaborators demonstrated the first in vivo link between p16-expression and lifespan. They found reduced p16 expression in some tissues of mice with mutations that extend lifespan, as well as in mice that had their lifespan extended by food restriction. Jan van Deursen and Darren Baker in collaboration with Andre Terzic at the Mayo Clinic in Rochester, Minn., provided the first in vivo evidence for a causal link between cellular senescence and aging by preventing the accumulation of senescent cells in BubR1 progeroid mice. In the absence of senescent cells, the mice’s tissues showed a major improvement in the usual burden of age-related disorders. They did not develop cataracts, avoided the usual wasting of muscle with age. They retained the fat layers in the skin that usually thin out with age and, in people, cause wrinkling. A second study led by Jan van Deursen in collaboration with a team of collaborators at the Mayo Clinic and Groningen University, provided the first direct in vivo evidence that cellular senescence causes signs of aging by eliminating senescent cells from progeroid mice by introducing a drug-inducible suicide gene and then treating the mice with the drug to kill senescent cells selectively, as opposed to decreasing whole body p16. Another Mayo study led by James Kirkland in collaboration with Scripps and other groups demonstrated that senolytics, drugs that target senescent cells, enhance cardiac function and improve vascular reactivity in old mice, alleviate gait disturbance caused by radiation in mice, and delay frailty, neurological dysfunction, and osteoporosis in progeroid mice. Discovery of senolytic drugs was based on a hypothesis-driven approach: the investigators leveraged the observation that senescent cells are resistant to apoptosis to discover that pro-survival pathways are up-regulated in these cells. They demonstrated that these survival pathways are the \"Achilles heel\" of senescent cells using RNA interference approaches, including Bcl-2-, AKT-, p21-, and tyrosine kinase-related pathways. They then used drugs known to target the identified pathways and showed these drugs kill senescent cells by apoptosis in culture and decrease senescent cell burden in multiple tissues in vivo. Importantly, these drugs had long term effects after a single dose, consistent with removal of senescent cells, rather than a temporary effect requiring continued presence of the drugs. This was the first study to show that clearing senescent cells enhances function in chronologically aged mice.\n\nOne of the earliest aging theories was the \"Rate of Living Hypothesis\" described by Raymond Pearl in 1928 (based on earlier work by Max Rubner), which states that fast basal metabolic rate corresponds to short maximum life span.\n\nWhile there may be some validity to the idea that for various types of specific damage detailed below that are by-products of metabolism, all other things being equal, a fast metabolism may reduce lifespan, in general this theory does not adequately explain the differences in lifespan either within, or between, species. Calorically restricted animals process as much, or more, calories per gram of body mass, as their \"ad libitum\" fed counterparts, yet exhibit substantially longer lifespans. Similarly, metabolic rate is a poor predictor of lifespan for birds, bats and other species that, it is presumed, have reduced mortality from predation, and therefore have evolved long lifespans even in the presence of very high metabolic rates. In a 2007 analysis it was shown that, when modern statistical methods for correcting for the effects of body size and phylogeny are employed, metabolic rate does not correlate with longevity in mammals or birds. (For a critique of the \"Rate of Living Hypothesis\" see \"Living fast, dying when?\")\n\nWith respect to specific types of chemical damage caused by metabolism, it is suggested that damage to long-lived biopolymers, such as structural proteins or DNA, caused by ubiquitous chemical agents in the body such as oxygen and sugars, are in part responsible for aging. The damage can include breakage of biopolymer chains, cross-linking of biopolymers, or chemical attachment of unnatural substituents (haptens) to biopolymers.\n\nUnder normal aerobic conditions, approximately 4% of the oxygen metabolized by mitochondria is converted to superoxide ion, which can subsequently be converted to hydrogen peroxide, hydroxyl radical and eventually other reactive species including other peroxides and singlet oxygen, which can, in turn, generate free radicals capable of damaging structural proteins and DNA. Certain metal ions found in the body, such as copper and iron, may participate in the process. (In Wilson's disease, a hereditary defect that causes the body to retain copper, some of the symptoms resemble accelerated senescence.) These processes termed oxidative stress are linked to the potential benefits of dietary polyphenol antioxidants, for example in coffee, red wine and tea.\n\nSugars such as glucose and fructose can react with certain amino acids such as lysine and arginine and certain DNA bases such as guanine to produce sugar adducts, in a process called \"glycation\". These adducts can further rearrange to form reactive species, which can then cross-link the structural proteins or DNA to similar biopolymers or other biomolecules such as non-structural proteins. People with diabetes, who have elevated blood sugar, develop senescence-associated disorders much earlier than the general population, but can delay such disorders by rigorous control of their blood sugar levels. There is evidence that sugar damage is linked to oxidant damage in a process termed \"glycoxidation\".\n\nFree radicals can damage proteins, lipids or DNA. Glycation mainly damages proteins. Damaged proteins and lipids accumulate in lysosomes as lipofuscin. Chemical damage to structural proteins can lead to loss of function; for example, damage to collagen of blood vessel walls can lead to vessel-wall stiffness and, thus, hypertension, and vessel wall thickening and reactive tissue formation (atherosclerosis); similar processes in the kidney can lead to renal failure. Damage to enzymes reduces cellular functionality. Lipid peroxidation of the inner mitochondrial membrane reduces the electric potential and the ability to generate energy. It is probably no accident that nearly all of the so-called \"accelerated aging diseases\" are due to defective DNA repair enzymes.\n\nIt is believed that the impact of alcohol on aging can be partly explained by alcohol's activation of the HPA axis, which stimulates glucocorticoid secretion, long-term exposure to which produces symptoms of aging.\n\nAlexander was the first to propose that DNA damage is the primary cause of aging. Early experimental evidence supporting this idea was reviewed by Gensler and Bernstein. By the early 1990s experimental support for this proposal was substantial, and further indicated that DNA damage due to reactive oxygen species was a major source of the DNA damages causing aging. The current state of evidence bearing on this theory is reviewed in DNA damage theory of aging and by Bernstein et al.\n\nReliability theory suggests that biological systems start their adult life with a high load of initial damage. Reliability theory is a general theory about systems failure. It allows researchers to predict the age-related failure kinetics for a system of given architecture (reliability structure) and given reliability of its components. Reliability theory predicts that even those systems which are composed entirely of non-aging elements (with a constant failure rate) will nevertheless deteriorate (fail more often) with age, if these systems are redundant in irreplaceable elements. Aging, therefore, is a direct consequence of systems.\n\nReliability theory also predicts the late-life mortality deceleration with subsequent leveling-off, as well as the late-life mortality plateaus, as an inevitable consequence of redundancy exhaustion at extreme old ages. The theory explains why mortality rates increase exponentially with age (the Gompertz law) in many species, by taking into account the initial flaws (defects) in newly formed systems. It also explains why organisms \"prefer\" to die according to the Gompertz law, while technical devices usually fail according to the Weibull (power) law. Reliability theory allows to specify conditions when organisms die according to the Weibull distribution: Organisms should be relatively free of initial flaws and defects. The theory makes it possible to find a general failure law applicable to all adult and extreme old ages, where the Gompertz and the Weibull laws are just special cases of this more general failure law. The theory explains why relative differences in mortality rates of compared populations (within a given species) vanish with age (compensation law of mortality), and mortality convergence is observed due to the exhaustion of initial differences in redundancy levels.\n\nBiological clocks, which objectively measure the biological age of cells and tissues, may become useful for testing different biological aging theories.\n\nA set of rare hereditary (genetic) disorders, each called progeria, has been known for some time. Sufferers exhibit symptoms resembling accelerated aging, including wrinkled skin. The cause of Hutchinson–Gilford progeria syndrome was reported in the journal \"Nature\" in May 2003.\nThis report suggests that DNA damage, not oxidative stress, is the cause of this form of accelerated aging.\n\nRecently, a kind of early senescence has been alleged to be a possible unintended outcome of early cloning experiments. The issue was raised in the case of Dolly the sheep, following her death from a contagious lung disease. The claim that Dolly's early death involved premature senescence has been vigorously contested, and Dolly's creator, Dr. Ian Wilmut has expressed the view that her illness and death were probably unrelated to the fact that she was a clone.\n\n\n", "id": "146539", "title": "Senescence"}
{"url": "https://en.wikipedia.org/wiki?curid=73614", "text": "Exocytosis\n\nExocytosis () is a form of active transport in which a cell transports molecules (e.g., neurotransmitters and proteins) out of the cell (\"exo-\" + \"cytosis\") by expelling them through an energy-dependent process. Exocytosis and its counterpart, endocytosis, are used by all cells because most chemical substances important to them are large polar molecules that cannot pass through the hydrophobic portion of the cell membrane by passive means.\n\nIn exocytosis, membrane-bound secretory vesicles are carried to the cell membrane, and their contents (i.e., water-soluble molecules) are secreted into the extracellular environment. This secretion is possible because the vesicle transiently fuses with the outer cell membrane. In the context of neurotransmission, neurotransmitters are typically released from synaptic vesicles into the synaptic cleft via exocytosis; however, neurotransmitters can also be released via reverse transport through membrane transport proteins.\n\nExocytosis is also a mechanism by which cells are able to insert membrane proteins (such as ion channels and cell surface receptors), lipids, and other components into the cell membrane. Vesicles containing these membrane components fully fuse with and become part of the outer cell membrane.\n\nThe term was proposed by De Duve in 1963.\n\nIn eukaryotes there are two types of exocytosis:\n1) Ca triggered non-constitutive (i.e., regulated exocytosis) and\n2) non-Ca triggered constitutive (i.e., non-regulated).\n\"Ca triggered non-constitutive\" exocytosis requires an external signal, a specific sorting signal on the vesicles, a clathrin coat, as well as an increase in intracellular calcium. Exocytosis in neuronal chemical synapses is Ca triggered and serves interneuronal signalling. \"Constitutive exocytosis\" is performed by all cells and serves the release of components of the extracellular matrix or delivery of newly synthesized membrane proteins that are incorporated in the plasma membrane after the fusion of the transport vesicle.\n\n\"Vesicular exocytosis\" in prokaryote gram negative bacteria is a third mechanism and latest finding in exocytosis. The periplasm is pinched off as bacterial outer membrane vesicles (OMVs) for translocating microbial biochemical signals into eukaryotic host cells or other microbes located nearby, accomplishing control of the secreting microbe on its environment - including invasion of host, endotoxemia, competing with other microbes for nutrition, etc. This finding of membrane vesicle trafficking occurring at the host-pathogen interface also dispels the myth that exocytosis is purely a eukaryotic cell phenomenon.\n\nFive steps are involved in exocytosis:\n\nCertain vesicle-trafficking steps require the transportation of a vesicle over a moderately small distance. For example, vesicles that transport proteins from the Golgi apparatus to the cell surface area, will be likely to use motor proteins and a cytoskeletal track to get closer to their target. Before tethering would have been appropriate, many of the proteins used for the active transport would have been instead set for passive transport, because the Golgi apparatus does not require ATP to transport proteins. Both the actin- and the microtubule-base are implicated in these processes, along with several motor proteins. Once the vesicles reach their targets, they come into contact with tethering factors that can restrain them.\n\nIt is useful to distinguish between the initial, loose \"tethering\" of vesicles to their objective from the more stable, \"packing\" interactions. Tethering involves links over distances of more than about half the diameter of a vesicle from a given membrane surface (>25 nm). Tethering interactions are likely to be involved in concentrating synaptic vesicles at the synapse.\n\nTethered vesicles are also involved in regular cell's transcription processes.\n\nSecretory vesicles transiently dock at the cell plasma membrane, preceding the formation of a tight t-/v-SNARE complex.\n\nIn neuronal exocytosis, the term \"priming\" has been used to include all of the molecular rearrangements and ATP-dependent protein and lipid modifications that take place after initial docking of a synaptic vesicle but before exocytosis, such that the influx of calcium ions is all that is needed to trigger nearly instantaneous neurotransmitter release. In other cell types, whose secretion is constitutive (i.e. continuous, calcium ion independent, non-triggered) there is no priming.\n\nTransient vesicle fusion is driven by SNARE proteins, resulting in release of vesicle contents into the extracellular space (or in case of neurons in the synaptic cleft).\n\nThe merging of the donor and the acceptor membranes accomplishes three tasks:\n\nRetrieval of synaptic vesicles occurs by endocytosis. Some synaptic vesicles are recycled without a full fusion into the membrane (kiss-and-run fusion), while others require a complete reformation of synaptic vesicles from the membrane by a specialized complex of proteins (clathrin). Non-constitutive exocytosis and subsequent endocytosis are highly energy expending processes, and thus, are dependent on mitochondria.\n\nExamination of cells following secretion using electron microscopy demonstrate increased presence of partially empty vesicles following secretion. This suggested that during the secretory process, only a portion of the vesicular content is able to exit the cell. This could only be possible if the vesicle were to temporarily establish continuity with the cell plasma membrane, expel a portion of its contents, then detach, reseal, and withdraw into the cytosol (endocytose). In this way, the secretory vesicle could be reused for subsequent rounds of exo-endocytosis, until completely empty of its contents.\n\n\n", "id": "73614", "title": "Exocytosis"}
{"url": "https://en.wikipedia.org/wiki?curid=37537790", "text": "Xenophagy\n\nXenophagy (Greek \"strange\" + \"eating\") and allotrophy (Greek \"other\" + \"nutrient\") are changes in established patterns of biological consumption, by individuals or groups.\n\n• In entomology, xenophagy is a categorical change in diet, such as an herbivore becoming carnivorous, a predator becoming necrophagous, a coprophage becoming necrophagous or carnivorous, or a reversal of such changes. Allotrophy is a less extreme change in diet, such as in the case of the seven-spot ladybird, which can diversify a diet of aphids to sometimes include pollen. There are several apparent cases of allotrophy in Israeli \"Longitarsus\" beetles.\n\n• In microbiology, xenophagy is the process by which a cell directs autophagy against pathogens, as reflected in the study of antiviral defenses. Cellular xenophagy is an innate component of immune responses, though the general importance of xenophagy is not yet certain.\n\n• In ecology, allotrophy is also reflected in eutrophication, being a change in nutrient source such as an aquatic ecosystem that starts receiving new nutrients from drainage of the surrounding land.\n", "id": "37537790", "title": "Xenophagy"}
{"url": "https://en.wikipedia.org/wiki?curid=37626088", "text": "DNA damage (naturally occurring)\n\nThroughout the cell cycle there are various checkpoints to ensure the cell is in good condition to progress to mitosis. The three main checkpoints are at G1/s, G2/m, and spindle assembly checkpoint regulating progression through anaphase. G1 and G2 checkpoints involves scanning for damaged DNA.  During S phase the cell is more vulnerable to DNA damage than any other part of the cell cycle. G2 checkpoint checks for damaged DNA and DNA replication completeness. DNA damage is an alteration in the chemical structure of DNA, such as a break in a strand of DNA, a base missing from the backbone of DNA, or a chemically changed base as 8-OHdG. DNA damage can occur naturally or via environmental factors. The DNA damage response (DDR) is a complex signal transduction pathway which recognizes when DNA is damaged influences cellular response to the damage.\n\nDNA damage is distinctly different from mutation, although both are types of error in DNA. DNA damage is an abnormal chemical structure in DNA, while a mutation is a change in the sequence of standard base pairs. DNA damages cause major changes in the structure of the genetic material and prevents the replication mechanism from functioning and performing properly. \n\nDNA damage and mutation have different biological consequences. While most DNA damages can undergo DNA repair, such repair is not 100% efficient. Un-repaired DNA damages accumulate in non-replicating cells, such as cells in the brains or muscles of adult mammals and can cause aging. (Also see DNA damage theory of aging.) In replicating cells, such as cells lining the colon, errors occur upon replication of past damages in the template strand of DNA or during repair of DNA damages. These errors can give rise to mutations or epigenetic alterations. Both of these types of alteration can be replicated and passed on to subsequent cell generations. These alterations can change gene function or regulation of gene expression and possibly contribute to progression to cancer. \n\nDamage to DNA that occurs naturally can result from metabolic or hydrolytic processes. Metabolism releases compounds that damage DNA including reactive oxygen species, reactive nitrogen species, reactive carbonyl species, lipid peroxidation products and alkylating agents, among others, while hydrolysis cleaves chemical bonds in DNA. Naturally occurring oxidative DNA damages arise at least 10,000 times per cell per day in humans and 50,000 times or more per cell per day in rats, as documented below.\n\nDNA can be damaged via environmental factors as well. Environmental agents such as UV light, ionizing radiation, and genotoxic chemicals. Replication forks can be stalled due to damaged DNA and double strand breaks are also a form of DNA damage. \n\nIn the presence of DNA damage, the cell can either repair the damage or induce cell death if the damage is beyond repair. Most damage can be repaired without trigger the damage response system, however more complex damage activates ATR and ATM, key protein kinases in the damage response system. DNA damage inhibits M-CDKs which are a key component of progression into Mitosis.\n\nSome forms of damage repair included, but are not limited to base excision repair, nucleotide excision repair, and non-homologous end joining. Base excision repair finds small changes in base structure and repairs them. Nucleotide excision repair detects and repairs major modifications that alter the conformation of the double helix. Non homologous end joining is one way to repair double strand breaks in DNA. \n\nIn all eukaryotic cells, ATR and ATM are protein kinases that detect DNA damage. They bind to DNA damaged sites and activate Chk1, Chk2, and, in animal cells, p53. Together, these proteins make up the DNA damage response system. Some DNA damage does not require the recruitment of ATR and ATM, it is only difficult and extensive damage that requires ATR and ATM. ATM and ATR are required for NHEJ, HR, ICL repair, and NER, as well as replication fork stability during unperturbed DNA replication and in response to replication blocks.\n\nATR is recruited for different forms of damage such as nucleotide damage, stalled replication forks and double strand breaks. ATM is specifically for the damage response to double strand breaks. The MRN complex (composed of Mre11, Rad50, and Nbs1) form immediately at the site of double strand break. This MRN complex recruits ATM to the site of damage. ATR and ATM phosphorylate various proteins that contribute to the damage repair system. The binding of ATR and ATM to damage sites on DNA lead to the recruitment of Chk1 and Chk2. These protein kinases send damage signals to the cell cycle control system to delay the progression of the cell cycle.\n\nChk1 leads to the production of DNA repair enzymes. Chk2 leads to reversible cell cycle arrest. Chk2 as well as ATR/ATM can activate p53 which leads to permanent cell cycle arrest or apoptosis. \n\nWhen there is too much damage, apoptosis is triggered in order to protect the organism from potentially harmful cells.7 p53, also known as a tumor suppressor gene, is a major regulatory protein in the DNA damage response system which binds directly to the promoters of its target genes. p53 acts primarily at the G1 checkpoint (controlling the G1 to S transition), where it blocks cell cycle progression. Activation of p53 can trigger cell death or permanent cell cycle arrest. p53 can also activate certain repair pathways such was NER. \n\nIn the absence of DNA damage, p53 is regulated by Mdm2 and constantly degraded. When there is DNA damage, Mdm2 is phosphorylated, most likely caused by ATM. The phosphorylation of Mdm2 leads to a reduction in the activity of Mdm2, thus preventing the degradation of p53. Normal, undamaged cell, usually has low levels of p53 while cells under stress and DNA damage, will have high levels of p53.\n\np53 serves as a transcription factors for both bax, a proapoptotic protein as well as p21, a CDK inhibitor. CDK Inhibitors result in cell cycle arrest. Arresting the cell provides the cell time to repair the damage, and if the damage is irreparable, p53 recruits bax to trigger apoptosis.\n\np53 is a major key player in the growth of cancerous cells. Damaged DNA cells with mutated p53 are at a higher risk of becoming cancerous. Common chemotherapy treatments are genotoxic. These treatments are ineffective in cancer tumor thats have mutated p53 since they do not have a functioning p53 to either arrest or kill the damaged cell.\n\nOne indication that DNA damages are a major problem for life is that DNA repair processes, to cope with DNA damages, have been found in all cellular organisms in which DNA repair has been investigated. For example, in bacteria, a regulatory network aimed at repairing DNA damages (called the SOS response in \"Escherichia coli\") has been found in many bacterial species. \"E. coli\" RecA, a key enzyme in the SOS response pathway, is the defining member of a ubiquitous class of DNA strand-exchange proteins that are essential for homologous recombination, a pathway that maintains genomic integrity by repairing broken DNA. Genes homologous to \"RecA\" and to other central genes in the SOS response pathway are found in almost all the bacterial genomes sequenced to date, covering a large number of phyla, suggesting both an ancient origin and a widespread occurrence of recombinational repair of DNA damage. Eukaryotic recombinases that are homologues of RecA are also widespread in eukaryotic organisms. For example, in fission yeast and humans, RecA homologues promote duplex-duplex DNA-strand exchange needed for repair of many types of DNA lesions.\n\nAnother indication that DNA damages are a major problem for life is that cells make large investments in DNA repair processes. As pointed out by Hoeijmakers, repairing just one double-strand break could require more than 10,000 ATP molecules, as used in signaling the presence of the damage, the generation of repair foci, and the formation (in humans) of the RAD51 nucleofilament (an intermediate in homologous recombinational repair). (RAD51 is a homologue of bacterial RecA.) If the structural modification occurs during the G1 phase of DNA replication, the G1-S checkpoint arrests or postpones the furtherance of the cell cycle before the product enters the S phase.\n\nThe list below shows some frequencies with which new naturally occurring DNA damages arise per day, due to endogenous cellular processes.\n\n\nAnother important endogenous DNA damage is M1dG, short for (3-(2'-deoxy-beta-D-erythro-pentofuranosyl)-pyrimido[1,2-a]-purin-10(3H)-one). The excretion in urine (likely reflecting rate of occurrence) of M1dG may be as much as 1,000-fold lower than that of 8-oxodG. However, a more important measure may be the steady-state level in DNA, reflecting both rate of occurrence and rate of DNA repair. The steady-state level of M1dG is higher than that of 8-oxodG. This points out that some DNA damages produced at a low rate may be difficult to repair and remain in DNA at a high steady-state level. Both M1dG and 8-oxodG are mutagenic.\n\nSteady-state levels of DNA damages represent the balance between formation and repair. More than 100 types of oxidative DNA damage have been characterized, and 8-oxodG constitutes about 5% of the steady state oxidative damages in DNA. Helbock et al. estimated that there were 24,000 steady state oxidative DNA adducts per cell in young rats and 66,000 adducts per cell in old rats. This reflects the accumulation of DNA damage with age. DNA damage accumulation with age is further described in DNA damage theory of aging.\n\nSwenberg et al. measured average amounts of selected steady state endogenous DNA damages in mammalian cells. The seven most common damages they evaluated are shown in Table 1.\n\nEvaluating steady-state damages in specific tissues of the rat, Nakamura and Swenberg indicated that the number of abasic sites varied from about 50,000 per cell in liver, kidney and lung to about 200,000 per cell in the brain.\n\nDifferentiated somatic cells of adult mammals generally replicate infrequently or not at all. Such cells, including, for example, brain neurons and muscle myocytes, have little or no cell turnover. Non-replicating cells do not generally generate mutations due to DNA damage-induced errors of replication. These non-replicating cells do not commonly give rise to cancer, but they do accumulate DNA damages with time that likely contribute to aging (\"\"). In a non-replicating cell, a single-strand break or other type of damage in the transcribed strand of DNA can block RNA polymerase II catalysed transcription. This would interfere with the synthesis of the protein coded for by the gene in which the blockage occurred.\n\nBrasnjevic et al. summarized the evidence showing that single-strand breaks accumulate with age in the brain (though accumulation differed in different regions of the brain) and that single-strand breaks are the most frequent steady-state DNA damages in the brain. As discussed above, these accumulated single-strand breaks would be expected to block transcription of genes. Consistent with this, as reviewed by Hetman et al., 182 genes were identified and shown to have reduced transcription in the brains of individuals older than 72 years, compared to transcription in the brains of those less than 43 years old. When 40 particular proteins were evaluated in a muscle of rats, the majority of the proteins showed significant decreases during aging from 18 months (mature rat) to 30 months (aged rat) of age.\n\nAnother type of DNA damage, the double strand break, was shown to cause cell death (loss of cells) through apoptosis. This type of DNA damage would not accumulate with age, since once a cell was lost through apoptosis, its double strand damage would be lost with it. Thus, damaged DNA segments undermine the DNA replication machinery because these altered sequences of DNA cannot be utilized as true templates to produce copies of one's genetic material. \n\nWhen DNA is damaged, the cell responds in various ways to fix the damage and minimize the effects on the cell. One such response, specifically in eukaryotic cells, is to delay cell division—the cell becomes arrested for some time in the G2 phase before progressing through the rest of the cell cycle. Various studies have been conducted to elucidate the purpose of this G2 arrest that is induced by DNA damage. Researchers have found that cells that are prematurely forced out of the delay have lower cell viability and higher rates of damaged chromosomes compared with cells that are able to undergo a full G2 arrest, suggesting that the purpose of the delay is to give the cell time to repair damaged chromosomes before continuing with the cell cycle. This ensures the proper functioning of mitosis. \n\nVarious species of animals exhibit similar mechanisms of cellular delay in response to DNA damage, which can be caused by exposure to x-irradiation. The budding yeast Saccharomyces cerevisiae has specifically been studied because progression through the cell cycle can be followed via nuclear morphology with ease. By studying Saccharomyces cerevisiae, researchers have been able to learn more about radiation-sensitive (RAD) genes, and the effect that RAD mutations may have on the typical cellular DNA damaged-induced delay response. Specifically, the RAD9 gene plays a crucial role in detecting DNA damage and arresting the cell in G2 until the damage is repaired. \n\nThrough extensive experiments, researchers have been able to illuminate the role that the RAD genes play in delaying cell division in response to DNA damage. When wild-type, growing cells are exposed to various levels of x-irradiation over a given time frame, and then analyzed with a microcolony assay, differences in the cell cycle response can be observed based on which genes are mutated in the cells. For instance, while unirradiated cells will progress normally through the cell cycle, cells that are exposed to x-irradiation either permanently arrest (become inviable) or delay in the G2 phase before continuing to divide in mitosis, further corroborating the idea that the G2 delay is crucial for DNA repair. However, rad strains, which are deficient in DNA repair, exhibit a markedly different response. For instance, rad52 cells, which cannot repair double-stranded DNA breaks, tend to permanently arrest in G2 when exposed to even very low levels of x-irradiation, and rarely end up progressing through the later stages of the cell cycle. This is because the cells cannot repair DNA damage and thus do not enter mitosis. Various other rad mutants exhibit similar responses when exposed to x-irradiation. \n\nHowever, the rad9 strain exhibits an entirely different effect. These cells fail to delay in the G2 phase when exposed to x-irradiation, and end up progressing through the cell cycle unperturbed, before dying. This suggests that the RAD9 gene, unlike the other RAD genes, plays a crucial role in initiating G2 arrest. To further investigate these findings, the cell cycles of double mutant strains have been analyzed. A mutant rad52 rad9 strain—which is both defective in DNA repair and G2 arrest—fails to undergo cell cycle arrest when exposed to x-irradiation. This suggests that even if DNA damage cannot be repaired, if RAD9 is not present, the cell cycle will not delay. Thus, unrepaired DNA damage is the signal that tells RAD9 to halt division and arrest the cell cycle in G2. Furthermore, there is a dose-dependent response; as the levels of x-irradiation—and subsequent DNA damage—increase, more cells, regardless of the mutations they have, become arrested in G2.\n\nAnother, and perhaps more helpful way to visualize this effect is to look at photomicroscopy slides. Initially, slides of RAD+ and rad9 haploid cells in the exponential phase of growth show simple, single cells, that are indistinguishable from each other. However, the slides look much different after being exposed to x-irradiation for 10 hours. The RAD+ slides now show RAD+ cells existing primarily as two-budded microcolonies, suggesting that cell division has been arrested. In contrast, the rad9 slides show the rad9 cells existing primarily as 3 to 8 budded colonies, and they appear smaller than the RAD+ cells. This is further evidence that the mutant RAD cells continued to divide and are deficient in G2 arrest. \n\nHowever, there is evidence that although the RAD9 gene is necessary to induce G2 arrest in response to DNA damage, giving the cell time to repair the damage, it does not actually play a direct role in repairing DNA. When rad9 cells are artificially arrested in G2 with MBC, a microtubule poison that prevents cellular division, and then treated with x-irradiation, the cells are able to repair their DNA and eventually progress through the cell cycle, dividing into viable cells. Thus, the RAD9 gene plays no role in actually repairing damaged DNA—it simply senses damaged DNA and responds by delaying cell division. The delay, then, is mediated by a control mechanism, rather than the physical damaged DNA. \n\nOn the other hand, it is possible that there are backup mechanisms that fill the role of RAD9 when it is not present. In fact, some studies have found that RAD9 does indeed play a critical role in DNA repair. In one study, rad9 mutant and normal cells in the exponential phase of growth were exposed to UV-irradiation and synchronized in specific phases of the cell cycle. After being incubated to permit DNA repair, the extent of pyrimidine dimerization (which is indicative of DNA damage) was assessed using sensitive primer extension techniques. It was found that the removal of DNA photolesions was much less efficient in rad9 mutant cells than normal cells, providing evidence that RAD9 is involved in DNA repair. Thus, the role of RAD9 in repairing DNA damage remains unclear. \n\nRegardless, it is clear that RAD9 is necessary to sense DNA damage and halt cell division. RAD9 has been suggested to possess 3’ to 5’ exonuclease activity, which is perhaps why it plays a role in detecting DNA damage. When DNA is damaged, it is hypothesized that RAD9 forms a complex with RAD1 and HUS1, and this complex is recruited to sites of DNA damage. It is in this way that RAD9 is able to exert its effects. \n\nAlthough the function of RAD9 has primarily been studied in the budding yeast Saccharomyces cerevisiae, many of the cell cycle control mechanisms are similar between species. Thus, we can conclude that RAD9 likely plays a critical role in the DNA damage response in humans as well. \n", "id": "37626088", "title": "DNA damage (naturally occurring)"}
{"url": "https://en.wikipedia.org/wiki?curid=563161", "text": "Membrane potential\n\nThe term \"membrane potential\" may refer to one of three kinds of membrane potential:\nMembrane potential (also transmembrane potential or membrane voltage) is the difference in electric potential between the interior and the exterior of a biological cell. With respect to the exterior of the cell, typical values of membrane potential range from –40 mV to –80 mV.\n\nAll animal cells are surrounded by a membrane composed of a lipid bilayer with proteins embedded in it. The membrane serves as both an insulator and a diffusion barrier to the movement of ions. Transmembrane proteins, also known as ion transporter or ion pump proteins, actively push ions across the membrane and establish concentration gradients across the membrane, and ion channels allow ions to move across the membrane down those concentration gradients. Ion pumps and ion channels are electrically equivalent to a set of batteries and resistors inserted in the membrane, and therefore create a voltage between the two sides of the membrane.\n\nVirtually all eukaryotic cells (including cells from animals, plants, and fungi) maintain a non-zero transmembrane potential, usually with a negative voltage in the cell interior as compared to the cell exterior ranging from –40 mV to –80 mV. The membrane potential has two basic functions. First, it allows a cell to function as a battery, providing power to operate a variety of \"molecular devices\" embedded in the membrane. Second, in electrically excitable cells such as neurons and muscle cells, it is used for transmitting signals between different parts of a cell. Signals are generated by opening or closing of ion channels at one point in the membrane, producing a local change in the membrane potential. This change in the electric field can be quickly affected by either adjacent or more distant ion channels in the membrane. Those ion channels can then open or close as a result of the potential change, reproducing the signal.\n\nIn non-excitable cells, and in excitable cells in their baseline states, the membrane potential is held at a relatively stable value, called the resting potential. For neurons, typical values of the resting potential range from –70 to –80 millivolts; that is, the interior of a cell has a negative baseline voltage of a bit less than one-tenth of a volt. The opening and closing of ion channels can induce a departure from the resting potential. This is called a depolarization if the interior voltage becomes less negative (say from –70 mV to –60 mV), or a hyperpolarization if the interior voltage becomes more negative (say from –70 mV to –80 mV). In excitable cells, a sufficiently large depolarization can evoke an action potential, in which the membrane potential changes rapidly and significantly for a short time (on the order of 1 to 100 milliseconds), often reversing its polarity. Action potentials are generated by the activation of certain voltage-gated ion channels.\n\nIn neurons, the factors that influence the membrane potential are diverse. They include numerous types of ion channels, some of which are chemically gated and some of which are voltage-gated. Because voltage-gated ion channels are controlled by the membrane potential, while the membrane potential itself is influenced by these same ion channels, feedback loops that allow for complex temporal dynamics arise, including oscillations and regenerative events such as action potentials.\n\nThe membrane potential in a cell derives ultimately from two factors: electrical force and diffusion. Electrical force arises from the mutual attraction between particles with opposite electrical charges (positive and negative) and the mutual repulsion between particles with the same type of charge (both positive or both negative). Diffusion arises from the statistical tendency of particles to redistribute from regions where they are highly concentrated to regions where the concentration is low (due to thermal energy).\n\nVoltage, which is synonymous with \"difference in electrical potential\", is the ability to drive an electric current across a resistance. Indeed, the simplest definition of a voltage is given by Ohm's law: V=IR, where V is voltage, I is current and R is resistance. If a voltage source such as a battery is placed in an electrical circuit, the higher the voltage of the source the greater the amount of current that it will drive across the available resistance. The functional significance of voltage lies only in potential \"differences\" between two points in a circuit. The idea of a voltage at a single point is meaningless. It is conventional in electronics to assign a voltage of zero to some arbitrarily chosen element of the circuit, and then assign voltages for other elements measured relative to that zero point. There is no significance in which element is chosen as the zero point—the function of a circuit depends only on the differences not on voltages \"per se\". However, in most cases and by convention, the zero level is most often assigned to the portion of a circuit that is in contact with ground.\n\nThe same principle applies to voltage in cell biology. In electrically active tissue, the potential difference between any two points can be measured by inserting an electrode at each point, for example one inside and one outside the cell, and connecting both electrodes to the leads of what is in essence a specialized voltmeter. By convention, the zero potential value is assigned to the outside of the cell and the sign of the potential difference between the outside and the inside is determined by the potential of the inside relative to the outside zero.\n\nIn mathematical terms, the definition of voltage begins with the concept of an electric field , a vector field assigning a magnitude and direction to each point in space. In many situations, the electric field is a conservative field, which means that it can be expressed as the gradient of a scalar function , that is, . This scalar field is referred to as the voltage distribution. Note that the definition allows for an arbitrary constant of integration—this is why absolute values of voltage are not meaningful. In general, electric fields can be treated as conservative only if magnetic fields do not significantly influence them, but this condition usually applies well to biological tissue.\n\nBecause the electric field is the gradient of the voltage distribution, rapid changes in voltage within a small region imply a strong electric field; on the converse, if the voltage remains approximately the same over a large region, the electric fields in that region must be weak. A strong electric field, equivalent to a strong voltage gradient, implies that a strong force is exerted on any charged particles that lie within the region.\n\nElectrical signals within biological organisms are, in general, driven by ions. The most important cations for the action potential are sodium (Na) and potassium (K). Both of these are \"monovalent\" cations that carry a single positive charge. Action potentials can also involve calcium (Ca), which is a \"divalent\" cation that carries a double positive charge. The chloride anion (Cl) plays a major role in the action potentials of some algae, but plays a negligible role in the action potentials of most animals.\n\nIons cross the cell membrane under two influences: diffusion and electric fields. A simple example wherein two solutions—A and B—are separated by a porous barrier illustrates that diffusion will ensure that they will eventually mix into equal solutions. This mixing occurs because of the difference in their concentrations. The region with high concentration will diffuse out toward the region with low concentration. To extend the example, let solution A have 30 sodium ions and 30 chloride ions. Also, let solution B have only 20 sodium ions and 20 chloride ions. Assuming the barrier allows both types of ions to travel through it, then a steady state will be reached whereby both solutions have 25 sodium ions and 25 chloride ions. If, however, the porous barrier is selective to which ions are let through, then diffusion alone will not determine the resulting solution. Returning to the previous example, let's now construct a barrier that is permeable only to sodium ions. Now, only sodium is allowed to diffuse cross the barrier from its higher concentration in solution A to the lower concentration in solution B. This will result in a greater accumulation of sodium ions than chloride ions in solution B and a lesser number of sodium ions than chloride ions in solution A.\n\nThis means that there is a net positive charge in solution B from the higher concentration of positively charged sodium ions than negatively charged chloride ions. Likewise, there is a net negative charge in solution A from the greater concentration of negative chloride ions than positive sodium ions. Since opposite charges attract and like charges repel, the ions are now also influenced by electrical fields as well as forces of diffusion. Therefore, positive sodium ions will be less likely to travel to the now-more-positive B solution and remain in the now-more-negative A solution. The point at which the forces of the electric fields completely counteract the force due to diffusion is called the equilibrium potential. At this point, the net flow of the specific ion (in this case sodium) is zero.\n\nEvery animal cell is enclosed in a plasma membrane, which has the structure of a lipid bilayer with many types of large molecules embedded in it. Because it is made of lipid molecules, the plasma membrane intrinsically has a high electrical resistivity, in other words a low intrinsic permeability to ions. However, some of the molecules embedded in the membrane are capable either of actively transporting ions from one side of the membrane to the other or of providing channels through which they can move.\n\nIn electrical terminology, the plasma membrane functions as a combined resistor and capacitor. Resistance arises from the fact that the membrane impedes the movement of charges across it. Capacitance arises from the fact that the lipid bilayer is so thin that an accumulation of charged particles on one side gives rise to an electrical force that pulls oppositely charged particles toward the other side. The capacitance of the membrane is relatively unaffected by the molecules that are embedded in it, so it has a more or less invariant value estimated at about 2 µF/cm (the total capacitance of a patch of membrane is proportional to its area). The conductance of a pure lipid bilayer is so low, on the other hand, that in biological situations it is always dominated by the conductance of alternative pathways provided by embedded molecules. Thus, the capacitance of the membrane is more or less fixed, but the resistance is highly variable.\n\nThe thickness of a plasma membrane is estimated to be about 7-8 nanometers. Because the membrane is so thin, it does not take a very large transmembrane voltage to create a strong electric field within it. Typical membrane potentials in animal cells are on the order of 100 millivolts (that is, one tenth of a volt), but calculations show that this generates an electric field close to the maximum that the membrane can sustain—it has been calculated that a voltage difference much larger than 200 millivolts could cause dielectric breakdown, that is, arcing across the membrane.\n\nThe resistance of a pure lipid bilayer to the passage of ions across it is very high, but structures embedded in the membrane can greatly enhance ion movement, either actively or passively, via mechanisms called facilitated transport and facilitated diffusion. The two types of structure that play the largest roles are ion channels and ion pumps, both usually formed from assemblages of protein molecules. Ion channels provide passageways through which ions can move. In most cases, an ion channel is permeable only to specific types of ions (for example, sodium and potassium but not chloride or calcium), and sometimes the permeability varies depending on the direction of ion movement. Ion pumps, also known as ion transporters or carrier proteins, actively transport specific types of ions from one side of the membrane to the other, sometimes using energy derived from metabolic processes to do so.\n\nIon pumps are integral membrane proteins that carry out active transport, i.e., use cellular energy (ATP) to \"pump\" the ions against their concentration gradient. Such ion pumps take in ions from one side of the membrane (decreasing its concentration there) and release them on the other side (increasing its concentration there). The ion pump most relevant to the action potential is the sodium–potassium pump, which transports three sodium ions out of the cell and two potassium ions in. As a consequence, the concentration of potassium ions K inside the neuron is roughly 20-fold larger than the outside concentration, whereas the sodium concentration outside is roughly ninefold larger than inside. In a similar manner, other ions have different concentrations inside and outside the neuron, such as calcium, chloride and magnesium.\n\nIon pumps influence the action potential only by establishing the relative ratio of intracellular and extracellular ion concentrations. The action potential involves mainly the opening and closing of ion channels not ion pumps. If the ion pumps are turned off by removing their energy source, or by adding an inhibitor such as ouabain, the axon can still fire hundreds of thousands of action potentials before their amplitudes begin to decay significantly. In particular, ion pumps play no significant role in the repolarization of the membrane after an action potential.\n\nA major contribution to establishing the membrane potential is made by the sodium-potassium pump. This is a complex of proteins embedded in the membrane that derives energy from ATP in order to transport sodium and potassium ions across the membrane. On each cycle, the pump exchanges three Na ions from the intracellular space for two K ions from the extracellular space. If the numbers of each type of ion were equal, the pump would be electrically neutral, but, because of the three-for-two exchange, it gives a net movement of one positive charge from intracellular to extracellular for each cycle, thereby contributing to a positive voltage difference. The pump has three effects: (1) it makes the sodium concentration high in the extracellular space and low in the intracellular space; (2) it makes the potassium concentration high in the intracellular space and low in the extracellular space; (3) it gives the intracellular space a negative voltage with respect to the extracellular space.\n\nThe sodium-potassium exchange pump is relatively slow in operation. If a cell were initialized with equal concentrations of sodium and potassium everywhere, it would take hours for the pump to establish equilibrium. The pump operates constantly, but becomes progressively less efficient as the concentrations of sodium and potassium available for pumping are reduced.\n\nAnother functionally important ion pump is the sodium-calcium exchanger. This pump operates in a conceptually similar way to the sodium-potassium pump, except that in each cycle it exchanges three Na from the extracellular space for one Ca from the intracellular space. Because the net flow of charge is inward, this pump runs \"downhill\", in effect, and therefore does not require any energy source except the membrane voltage. Its most important effect is to pump calcium outward—it also allows an inward flow of sodium, thereby counteracting the sodium-potassium pump, but, because overall sodium and potassium concentrations are much higher than calcium concentrations, this effect is relatively unimportant. The net result of the sodium-calcium exchanger is that in the resting state, intracellular calcium concentrations become very low.\n\nIon channels are integral membrane proteins with a pore through which ions can travel between extracellular space and cell interior. Most channels are specific (selective) for one ion; for example, most potassium channels are characterized by 1000:1 selectivity ratio for potassium over sodium, though potassium and sodium ions have the same charge and differ only slightly in their radius. The channel pore is typically so small that ions must pass through it in single-file order. Channel pores can be either open or closed for ion passage, although a number of channels demonstrate various sub-conductance levels. When a channel is open, ions permeate through the channel pore down the transmembrane concentration gradient for that particular ion. Rate of ionic flow through the channel, i.e. single-channel current amplitude, is determined by the maximum channel conductance and electrochemical driving force for that ion, which is the difference between the instantaneous value of the membrane potential and the value of the reversal potential.\n\nA channel may have several different states (corresponding to different conformations of the protein), but each such state is either open or closed. In general, closed states correspond either to a contraction of the pore—making it impassable to the ion—or to a separate part of the protein, stoppering the pore. For example, the voltage-dependent sodium channel undergoes \"inactivation\", in which a portion of the protein swings into the pore, sealing it. This inactivation shuts off the sodium current and plays a critical role in the action potential.\n\nIon channels can be classified by how they respond to their environment. For example, the ion channels involved in the action potential are \"voltage-sensitive channels\"; they open and close in response to the voltage across the membrane. \"Ligand-gated channels\" form another important class; these ion channels open and close in response to the binding of a ligand molecule, such as a neurotransmitter. Other ion channels open and close with mechanical forces. Still other ion channels—such as those of sensory neurons—open and close in response to other stimuli, such as light, temperature or pressure.\n\nLeakage channels are the simplest type of ion channel, in that their permeability is more or less constant. The types of leakage channels that have the greatest significance in neurons are potassium and chloride channels. It should be noted that even these are not perfectly constant in their properties: First, most of them are voltage-dependent in the sense that they conduct better in one direction than the other (in other words, they are rectifiers); second, some of them are capable of being shut off by chemical ligands even though they do not require ligands in order to operate.\n\nLigand-gated ion channels are channels whose permeability is greatly increased when some type of chemical ligand binds to the protein structure. Animal cells contain hundreds, if not thousands, of types of these. A large subset function as neurotransmitter receptors—they occur at postsynaptic sites, and the chemical ligand that gates them is released by the presynaptic axon terminal. One example of this type is the AMPA receptor, a receptor for the neurotransmitter glutamate that when activated allows passage of sodium and potassium ions. Another example is the GABA receptor, a receptor for the neurotransmitter GABA that when activated allows passage of chloride ions.\n\nNeurotransmitter receptors are activated by ligands that appear in the extracellular area, but there are other types of ligand-gated channels that are controlled by interactions on the intracellular side.\n\nVoltage-gated ion channels, also known as \"voltage dependent ion channels\", are channels whose permeability is influenced by the membrane potential. They form another very large group, with each member having a particular ion selectivity and a particular voltage dependence. Many are also time-dependent—in other words, they do not respond immediately to a voltage change but only after a delay.\n\nOne of the most important members of this group is a type of voltage-gated sodium channel that underlies action potentials—these are sometimes called \"Hodgkin-Huxley sodium channels\" because they were initially characterized by Alan Lloyd Hodgkin and Andrew Huxley in their Nobel Prize-winning studies of the physiology of the action potential. The channel is closed at the resting voltage level, but opens abruptly when the voltage exceeds a certain threshold, allowing a large influx of sodium ions that produces a very rapid change in the membrane potential. Recovery from an action potential is partly dependent on a type of voltage-gated potassium channel that is closed at the resting voltage level but opens as a consequence of the large voltage change produced during the action potential.\n\nThe reversal potential (or \"equilibrium potential\") of an ion is the value of transmembrane voltage at which diffusive and electrical forces counterbalance, so that there is no net ion flow across the membrane. This means that the transmembrane voltage exactly opposes the force of diffusion of the ion, such that the net current of the ion across the membrane is zero and unchanging. The reversal potential is important because it gives the voltage that acts on channels permeable to that ion—in other words, it gives the voltage that the ion concentration gradient generates when it acts as a battery.\n\nThe equilibrium potential of a particular ion is usually designated by the notation \"E\".The equilibrium potential for any ion can be calculated using the Nernst equation. For example, reversal potential for potassium ions will be as follows:\n\nwhere\n\nEven if two different ions have the same charge (i.e., K and Na), they can still have very different equilibrium potentials, provided their outside and/or inside concentrations differ. Take, for example, the equilibrium potentials of potassium and sodium in neurons. The potassium equilibrium potential \"E\" is −84 mV with 5 mM potassium outside and 140 mM inside. On the other hand, the sodium equilibrium potential, \"E\", is approximately +66 mV with approximately 12 mM sodium inside and 140 mM outside.\n\nA neuron’s resting membrane potential actually changes during the development of an organism. In order for a neuron to eventually adopt its full adult function, its potential must be tightly regulated during development. As an organism progresses through development the resting membrane potential becomes more negative. Glial cells are also differentiating and proliferating as development progresses in the brain. The addition of these glial cells increases the organism’s ability to regulate extracellular potassium. The drop in extracellular potassium can lead to a decrease in membrane potential of 35 mV.\n\nElectrophysiologists model the effects of ionic concentration differences, ion channels, and membrane capacitance in terms of an equivalent circuit, which is intended to represent the electrical properties of a small patch of membrane. The equivalent circuit consists of a capacitor in parallel with four pathways each consisting of a battery in series with a variable conductance. The capacitance is determined by the properties of the lipid bilayer, and is taken to be fixed. Each of the four parallel pathways comes from one of the principal ions, sodium, potassium, chloride, and calcium. The voltage of each ionic pathway is determined by the concentrations of the ion on each side of the membrane; see the Reversal potential section above. The conductance of each ionic pathway at any point in time is determined by the states of all the ion channels that are potentially permeable to that ion, including leakage channels, ligand-gated channels, and voltage-gated ion channels.\nFor fixed ion concentrations and fixed values of ion channel conductance, the equivalent circuit can be further reduced, using the Goldman equation as described below, to a circuit containing a capacitance in parallel with a battery and conductance. In electrical terms, this is a type of RC circuit (resistance-capacitance circuit), and its electrical properties are very simple. Starting from any initial state, the current flowing across either the conductance or the capacitance decays with an exponential time course, with a time constant of , where is the capacitance of the membrane patch, and is the net resistance. For realistic situations, the time constant usually lies in the 1—100 millisecond range. In most cases, changes in the conductance of ion channels occur on a faster time scale, so an RC circuit is not a good approximation; however, the differential equation used to model a membrane patch is commonly a modified version of the RC circuit equation.\n\nWhen the membrane potential of a cell can go for a long period of time without changing significantly, it is referred to as a resting potential or resting voltage. This term is used for the membrane potential of non-excitable cells, but also for the membrane potential of excitable cells in the absence of excitation. In excitable cells, the other possible states are graded membrane potentials (of variable amplitude), and action potentials, which are large, all-or-nothing rises in membrane potential that usually follow a fixed time course. Excitable cells include neurons, muscle cells, and some secretory cells in glands. Even in other types of cells, however, the membrane voltage can undergo changes in response to environmental or intracellular stimuli. For example, depolarization of the plasma membrane appears to be an important step in programmed cell death.\n\nThe interactions that generate the resting potential are modeled by the Goldman equation. This is similar in form to the Nernst equation shown above, in that it is based on the charges of the ions in question, as well as the difference between their inside and outside concentrations. However, it also takes into consideration the relative permeability of the plasma membrane to each ion in question.\n\nThe three ions that appear in this equation are potassium (K), sodium (Na), and chloride (Cl). Calcium is omitted, but can be added to deal with situations in which it plays a significant role. Being an anion, the chloride terms are treated differently from the cation terms; the intracellular concentration is in the numerator, and the extracellular concentration in the denominator, which is reversed from the cation terms. \"P\" stands for the relative permeability of the ion type i.\n\nIn essence, the Goldman formula expresses the membrane potential as a weighted average of the reversal potentials for the individual ion types, weighted by permeability. (Although the membrane potential changes about 100 mV during an action potential, the concentrations of ions inside and outside the cell do not change significantly. They remain close to their respective concentrations when then membrane is at resting potential.) In most animal cells, the permeability to potassium is much higher in the resting state than the permeability to sodium. As a consequence, the resting potential is usually close to the potassium reversal potential. The permeability to chloride can be high enough to be significant, but, unlike the other ions, chloride is not actively pumped, and therefore equilibrates at a reversal potential very close to the resting potential determined by the other ions.\n\nValues of resting membrane potential in most animal cells usually vary between the potassium reversal potential (usually around -80 mV) and around -40 mV. The resting potential in excitable cells (capable of producing action potentials) is usually near -60 mV—more depolarized voltages would lead to spontaneous generation of action potentials. Immature or undifferentiated cells show highly variable values of resting voltage, usually significantly more positive than in differentiated cells. In such cells, the resting potential value correlates with the degree of differentiation: undifferentiated cells in some cases may not show any transmembrane voltage difference at all.\n\nMaintenance of the resting potential can be metabolically costly for a cell because of its requirement for active pumping of ions to counteract losses due to leakage channels. The cost is highest when the cell function requires an especially depolarized value of membrane voltage. For example, the resting potential in daylight-adapted blowfly (\"Calliphora vicina\") photoreceptors can be as high as -30 mV. This elevated membrane potential allows the cells to respond very rapidly to visual inputs; the cost is that maintenance of the resting potential may consume more than 20% of overall cellular ATP.\n\nOn the other hand, the high resting potential in undifferentiated cells can be a metabolic advantage. This apparent paradox is resolved by examination of the origin of that resting potential. Little-differentiated cells are characterized by extremely high input resistance, which implies that few leakage channels are present at this stage of cell life. As an apparent result, potassium permeability becomes similar to that for sodium ions, which places resting potential in-between the reversal potentials for sodium and potassium as discussed above. The reduced leakage currents also mean there is little need for active pumping in order to compensate, therefore low metabolic cost.\n\nAs explained above, the potential at any point in a cell's membrane is determined by the ion concentration differences between the intracellular and extracellular areas, and by the permeability of the membrane to each type of ion. The ion concentrations do not normally change very quickly (with the exception of Ca, where the baseline intracellular concentration is so low that even a small influx may increase it by orders of magnitude), but the permeabilities of the ions can change in a fraction of a millisecond, as a result of activation of ligand-gated ion channels. The change in membrane potential can be either large or small, depending on how many ion channels are activated and what type they are, and can be either long or short, depending on the lengths of time that the channels remain open. Changes of this type are referred to as graded potentials, in contrast to action potentials, which have a fixed amplitude and time course.\n\nAs can be derived from the Goldman equation shown above, the effect of increasing the permeability of a membrane to a particular type of ion shifts the membrane potential toward the reversal potential for that ion. Thus, opening Na channels pulls the membrane potential toward the Na reversal potential, which is usually around +100 mV. Likewise, opening K channels pulls the membrane potential toward about –90 mV, and opening Cl channels pulls it toward about –70 mV (resting potential of most membranes). Because –90 to +100 mV is the full operating range of membrane potential, the effect is that Na channels always pull the membrane potential up, K channels pull it down, and Cl channels pull it toward the resting potential.\nGraded membrane potentials are particularly important in neurons, where they are produced by synapses—a temporary change in membrane potential produced by activation of a synapse by a single graded or action potential is called a postsynaptic potential. Neurotransmitters that act to open Na channels typically cause the membrane potential to become more positive, while neurotransmitters that act on K channels typically cause it to become more negative.\n\nWhether a postsynaptic potential is considered excitatory or inhibitory depends on the reversal potential for the ions of that current, and the threshold for a cell to fire an action potential (around –50mV). A postsynaptic potential with a reversal potential above threshold, such as a typical Na current, is considered excitatory. A potential with a reversal potential below threshold, such as a typical K or Cl current, is considered inhibitory. Even if a current depolarizes a cell, it will inhibit the cell if its reversal potential is below threshold. This is due to the fact that multiple postsynaptic potentials do not have an added effect but average, so a current with a reversal potential above the resting potential, but below threshold, will not contribute to reaching threshold. Thus, neurotransmitters that act to open Na channels produce excitatory postsynaptic potentials, or EPSPs, whereas neurotransmitters that act to open K or Cl channels produce inhibitory postsynaptic potentials, or IPSPs. When multiple types of channels are open within the same time period, their postsynaptic potentials summate (add) nonlinearly.\n\nFrom the viewpoint of biophysics, the \"resting\" membrane potential is merely the membrane potential that results from the membrane permeabilities that predominate when the cell is resting. The above equation of weighted averages always applies, but the following approach may be more easily visualized.\nAt any given moment, there are two factors for an ion that determine how much influence that ion will have over the membrane potential of a cell:\n\nIf the driving force is high, then the ion is being \"pushed\" across the membrane. If the permeability is high, it will be easier for the ion to diffuse across the membrane.\n\n\nSo, in a resting membrane, while the driving force for potassium is low, its permeability is very high. Sodium has a huge driving force but almost no resting permeability. In this case, potassium carries about 20 times more current than sodium, and thus has 20 times more influence over \"E\" than does sodium.\n\nHowever, consider another case—the peak of the action potential. Here, permeability to Na is high and K permeability is relatively low. Thus, the membrane moves to near \"E\" and far from \"E\".\n\nThe more ions are permeant the more complicated it becomes to predict the membrane potential. However, this can be done using the Goldman-Hodgkin-Katz equation or the weighted means equation. By plugging in the concentration gradients and the permeabilities of the ions at any instant in time, one can determine the membrane potential at that moment. What the GHK equations means is that, at any time, the value of the membrane potential will be a weighted average of the equilibrium potentials of all permeant ions. The \"weighting\" is the ions relative permeability across the membrane.\n\nWhile cells expend energy to transport ions and establish a transmembrane potential, they use this potential in turn to transport other ions and metabolites such as sugar. The transmembrane potential of the mitochondria drives the production of ATP, which is the common currency of biological energy.\n\nCells may draw on the energy they store in the resting potential to drive action potentials or other forms of excitation. These changes in the membrane potential enable communication with other cells (as with action potentials) or initiate changes inside the cell, which happens in an egg when it is fertilized by a sperm.\n\nIn neuronal cells, an action potential begins with a rush of sodium ions into the cell through sodium channels, resulting in depolarization, while recovery involves an outward rush of potassium through potassium channels. Both these fluxes occur by passive diffusion.\n\n\n\n", "id": "563161", "title": "Membrane potential"}
{"url": "https://en.wikipedia.org/wiki?curid=156998", "text": "Action potential\n\nIn physiology, an action potential occurs when the membrane potential of a specific axon location rapidly rises and falls: this depolarisation then causes adjacent locations to similarly depolarise. Action potentials occur in several types of animal cells, called excitable cells, which include neurons, muscle cells, endocrine cells, and in some plant cells. \n\nIn neurons, action potentials play a central role in cell-to-cell communication by providing for—or, with regard to saltatory conduction, assisting—the propagation of signals along the neuron's axon towards synaptic boutons situated at the ends of an axon; these signals can then connect with other neurons at synapses, or to motor cells or glands. In other types of cells, their main function is to activate intracellular processes. In muscle cells, for example, an action potential is the first step in the chain of events leading to contraction. In beta cells of the pancreas, they provoke release of insulin. Action potentials in neurons are also known as \"nerve impulses\" or \"spikes\", and the temporal sequence of action potentials generated by a neuron is called its \"spike train\". A neuron that emits an action potential, or nerve impulse, is often said to \"fire\".\n\nAction potentials are generated by special types of voltage-gated ion channels embedded in a cell's plasma membrane. These channels are shut when the membrane potential is near the (negative) resting potential of the cell, but they rapidly begin to open if the membrane increases to a precisely defined threshold voltage, depolarising the transmembrane potential. When the channels open, they allow an inward flow of sodium ions, which changes the electrochemical gradient, which in turn produces a further rise in the membrane potential. This then causes more channels to open, producing a greater electric current across the cell membrane, and so on. The process proceeds explosively until all of the available ion channels are open, resulting in a large upswing in the membrane potential. The rapid influx of sodium ions causes the polarity of the plasma membrane to reverse, and the ion channels then rapidly inactivate. As the sodium channels close, sodium ions can no longer enter the neuron, and then they are actively transported back out of the plasma membrane. Potassium channels are then activated, and there is an outward current of potassium ions, returning the electrochemical gradient to the resting state. After an action potential has occurred, there is a transient negative shift, called the afterhyperpolarization.\n\nIn animal cells, there are two primary types of action potentials. One type is generated by voltage-gated sodium channels, the other by voltage-gated calcium channels. Sodium-based action potentials usually last for under one millisecond, but calcium-based action potentials may last for 100 milliseconds or longer. In some types of neurons, slow calcium spikes provide the driving force for a long burst of rapidly emitted sodium spikes. In cardiac muscle cells, on the other hand, an initial fast sodium spike provides a \"primer\" to provoke the rapid onset of a calcium spike, which then produces muscle contraction.\n\nIn the Hodgkin–Huxley membrane capacitance model, the speed of transmission of an action potential was undefined and it was assumed that adjacent areas became depolarised due to released ion interference with neighbouring channels. Measurements of ion diffusion and radii have since shown this not to be possible. Moreover, contradictory measurements of entropy changes and timing disputed the capacitance model as acting alone.\n\nNearly all cell membranes in animals, plants and fungi maintain a voltage difference between the exterior and interior of the cell, called the membrane potential. A typical voltage across an animal cell membrane is –70 mV. This means that the interior of the cell has a negative voltage of approximately one-fifteenth of a volt relative to the exterior. In most types of cells, the membrane potential usually stays fairly constant. Some types of cells, however, are electrically active in the sense that their voltages fluctuate over time. In some types of electrically active cells, including neurons and muscle cells, the voltage fluctuations frequently take the form of a rapid upward spike followed by a rapid fall. These up-and-down cycles are known as \"action potentials\". In some types of neurons, the entire up-and-down cycle takes place in a few thousandths of a second. In muscle cells, a typical action potential lasts about a fifth of a second. In some other types of cells, and also in plants, an action potential may last three seconds or more.\n\nThe electrical properties of a cell are determined by the structure of the membrane that surrounds it. A cell membrane consists of a lipid bilayer of molecules in which larger protein molecules are embedded. The lipid bilayer is highly resistant to movement of electrically charged ions, so it functions as an insulator. The large membrane-embedded proteins, in contrast, provide channels through which ions can pass across the membrane. Action potentials are driven by channel proteins whose configuration switches between closed and open states as a function of the voltage difference between the interior and exterior of the cell. These voltage-sensitive proteins are known as voltage-gated ion channels.\n\nAll cells in animal body tissues are electrically polarized – in other words, they maintain a voltage difference across the cell's plasma membrane, known as the membrane potential. This electrical polarization results from a complex interplay between protein structures embedded in the membrane called ion pumps and ion channels. In neurons, the types of ion channels in the membrane usually vary across different parts of the cell, giving the dendrites, axon, and cell body different electrical properties. As a result, some parts of the membrane of a neuron may be excitable (capable of generating action potentials), whereas others are not. Recent studies have shown that the most excitable part of a neuron is the part after the axon hillock (the point where the axon leaves the cell body), which is called the initial segment, but the axon and cell body are also excitable in most cases.\n\nEach excitable patch of membrane has two important levels of membrane potential: the resting potential, which is the value the membrane potential maintains as long as nothing perturbs the cell, and a higher value called the threshold potential. At the axon hillock of a typical neuron, the resting potential is around –70 millivolts (mV) and the threshold potential is around –55 mV. Synaptic inputs to a neuron cause the membrane to depolarize or hyperpolarize; that is, they cause the membrane potential to rise or fall. Action potentials are triggered when enough depolarization accumulates to bring the membrane potential up to threshold. When an action potential is triggered, the membrane potential abruptly shoots upward and then equally abruptly shoots back downward, often ending below the resting level, where it remains for some period of time. The shape of the action potential is stereotyped; this means that the rise and fall usually have approximately the same amplitude and time course for all action potentials in a given cell. (Exceptions are discussed later in the article). In most neurons, the entire process takes place in about a thousandth of a second. Many types of neurons emit action potentials constantly at rates of up to 10–100 per second. However, some types are much quieter, and may go for minutes or longer without emitting any action potentials.\n\nAction potentials result from the presence in a cell's membrane of special types of voltage-gated ion channels. A voltage-gated ion channel is a cluster of proteins embedded in the membrane that has three key properties:\n\nThus, a voltage-gated ion channel tends to be open for some values of the membrane potential, and closed for others. In most cases, however, the relationship between membrane potential and channel state is probabilistic and involves a time delay. Ion channels switch between conformations at unpredictable times: The membrane potential determines the rate of transitions and the probability per unit time of each type of transition.\n\nVoltage-gated ion channels are capable of producing action potentials because they can give rise to positive feedback loops: The membrane potential controls the state of the ion channels, but the state of the ion channels controls the membrane potential. Thus, in some situations, a rise in the membrane potential can cause ion channels to open, thereby causing a further rise in the membrane potential. An action potential occurs when this positive feedback cycle proceeds explosively. The time and amplitude trajectory of the action potential are determined by the biophysical properties of the voltage-gated ion channels that produce it. Several types of channels capable of producing the positive feedback necessary to generate an action potential do exist. Voltage-gated sodium channels are responsible for the fast action potentials involved in nerve conduction. Slower action potentials in muscle cells and some types of neurons are generated by voltage-gated calcium channels. Each of these types comes in multiple variants, with different voltage sensitivity and different temporal dynamics.\n\nThe most intensively studied type of voltage-dependent ion channels comprises the sodium channels involved in fast nerve conduction. These are sometimes known as Hodgkin-Huxley sodium channels because they were first characterized by Alan Hodgkin and Andrew Huxley in their Nobel Prize-winning studies of the biophysics of the action potential, but can more conveniently be referred to as \"Na\" channels. (The \"V\" stands for \"voltage\".) An \"Na\" channel has three possible states, known as \"deactivated\", \"activated\", and \"inactivated\". The channel is permeable only to sodium ions when it is in the \"activated\" state. When the membrane potential is low, the channel spends most of its time in the \"deactivated\" (closed) state. If the membrane potential is raised above a certain level, the channel shows increased probability of transitioning to the \"activated\" (open) state. The higher the membrane potential the greater the probability of activation. Once a channel has activated, it will eventually transition to the \"inactivated\" (closed) state. It tends then to stay inactivated for some time, but, if the membrane potential becomes low again, the channel will eventually transition back to the \"deactivated\" state. During an action potential, most channels of this type go through a cycle \"deactivated\"→\"activated\"→\"inactivated\"→\"deactivated\". This is only the population average behavior, however — an individual channel can in principle make any transition at any time. However, the likelihood of a channel's transitioning from the \"inactivated\" state directly to the \"activated\" state is very low: A channel in the \"inactivated\" state is refractory until it has transitioned back to the \"deactivated\" state.\n\nThe outcome of all this is that the kinetics of the \"Na\" channels are governed by a transition matrix whose rates are voltage-dependent in a complicated way. Since these channels themselves play a major role in determining the voltage, the global dynamics of the system can be quite difficult to work out. Hodgkin and Huxley approached the problem by developing a set of differential equations for the parameters that govern the ion channel states, known as the Hodgkin-Huxley equations. These equations have been extensively modified by later research, but form the starting point for most theoretical studies of action potential biophysics.\n\nAs the membrane potential is increased, sodium ion channels open, allowing the entry of sodium ions into the cell. This is followed by the opening of potassium ion channels that permit the exit of potassium ions from the cell. The inward flow of sodium ions increases the concentration of positively charged cations in the cell and causes depolarization, where the potential of the cell is higher than the cell's resting potential. The sodium channels close at the peak of the action potential, while potassium continues to leave the cell. The efflux of potassium ions decreases the membrane potential or hyperpolarizes the cell. For small voltage increases from rest, the potassium current exceeds the sodium current and the voltage returns to its normal resting value, typically −70 mV. However, if the voltage increases past a critical threshold, typically 15 mV higher than the resting value, the sodium current dominates. This results in a runaway condition whereby the positive feedback from the sodium current activates even more sodium channels. Thus, the cell \"fires\", producing an action potential. The frequency at which cellular action potentials are produced is known as its \"firing rate\".\n\nCurrents produced by the opening of voltage-gated channels in the course of an action potential are typically significantly larger than the initial stimulating current. Thus, the amplitude, duration, and shape of the action potential are determined largely by the properties of the excitable membrane and not the amplitude or duration of the stimulus. This all-or-nothing property of the action potential sets it apart from graded potentials such as receptor potentials, electrotonic potentials, and synaptic potentials, which scale with the magnitude of the stimulus. A variety of action potential types exist in many cell types and cell compartments as determined by the types of voltage-gated channels, leak channels, channel distributions, ionic concentrations, membrane capacitance, temperature, and other factors.\n\nThe principal ions involved in an action potential are sodium and potassium cations; sodium ions enter the cell, and potassium ions leave, restoring equilibrium. Relatively few ions need to cross the membrane for the membrane voltage to change drastically. The ions exchanged during an action potential, therefore, make a negligible change in the interior and exterior ionic concentrations. The few ions that do cross are pumped out again by the continuous action of the sodium–potassium pump, which, with other ion transporters, maintains the normal ratio of ion concentrations across the membrane. Calcium cations and chloride anions are involved in a few types of action potentials, such as the cardiac action potential and the action potential in the single-cell alga \"Acetabularia\", respectively.\n\nAlthough action potentials are generated locally on patches of excitable membrane, the resulting currents can trigger action potentials on neighboring stretches of membrane, precipitating a domino-like propagation. In contrast to passive spread of electric potentials (electrotonic potential), action potentials are generated anew along excitable stretches of membrane and propagate without decay. Myelinated sections of axons are not excitable and do not produce action potentials and the signal is propagated passively as electrotonic potential. Regularly spaced unmyelinated patches, called the nodes of Ranvier, generate action potentials to boost the signal. Known as saltatory conduction, this type of signal propagation provides a favorable tradeoff of signal velocity and axon diameter. Depolarization of axon terminals, in general, triggers the release of neurotransmitter into the synaptic cleft. In addition, backpropagating action potentials have been recorded in the dendrites of pyramidal neurons, which are ubiquitous in the neocortex. These are thought to have a role in spike-timing-dependent plasticity.\n\nA neuron's ability to generate and propagate an action potential changes during development. How much the membrane potential of a neuron changes as the result of a current impulse is a function of the membrane input resistance. As a cell grows, more channels are added to the membrane, causing a decrease in input resistance. A mature neuron also undergoes shorter changes in membrane potential in response to synaptic currents. Neurons from a ferret lateral geniculate nucleus have a longer time constant and larger voltage deflection at P0 than they do at P30. One consequence of the decreasing action potential duration is that the fidelity of the signal can be preserved in response to high frequency stimulation. Immature neurons are more prone to synaptic depression than potentiation after high frequency stimulation.\n\nIn the early development of many organisms, the action potential is actually initially carried by calcium current rather than sodium current. The opening and closing kinetics of calcium channels during development are slower than those of the voltage-gated sodium channels that will carry the action potential in the mature neurons. The longer opening times for the calcium channels can lead to action potentials that are considerably slower than those of mature neurons. Xenopus neurons initially have action potentials that take 60–90 ms. During development, this time decreases to 1 ms. There are two reasons for this drastic decrease. First, the inward current becomes primarily carried by sodium channels. Second, the delayed rectifier, a potassium channel current, increases to 3.5 times its initial strength.\n\nIn order for the transition from a calcium-dependent action potential to a sodium-dependent action potential to proceed new channels must be added to the membrane. If Xenopus neurons are grown in an environment with RNA synthesis or protein synthesis inhibitors that transition is prevented. Even the electrical activity of the cell itself may play a role in channel expression. If action potentials in Xenopus myocytes are blocked, the typical increase in sodium and potassium current density is prevented or delayed.\n\nThis maturation of electrical properties is seen across species. Xenopus sodium and potassium currents increase drastically after a neuron goes through its final phase of mitosis. The sodium current density of rat cortical neurons increases by 600% within the first two postnatal weeks.\n\nSeveral types of cells support an action potential, such as plant cells, muscle cells, and the specialized cells of the heart (in which occurs the cardiac action potential). However, the main excitable cell is the neuron, which also has the simplest mechanism for the action potential.\n\nNeurons are electrically excitable cells composed, in general, of one or more dendrites, a single soma, a single axon and one or more axon terminals. Dendrites are cellular projections whose primary function is to receive synaptic signals. Their protrusions, known as dendritic spines, are designed to capture the neurotransmitters released by the presynaptic neuron. They have a high concentration of ligand-gated ion channels. These spines have a thin neck connecting a bulbous protrusion to the dendrite. This ensures that changes occurring inside the spine are less likely to affect the neighboring spines. The dendritic spine can, with rare exception (see LTP), act as an independent unit. The dendrites extend from the soma, which houses the nucleus, and many of the \"normal\" eukaryotic organelles. Unlike the spines, the surface of the soma is populated by voltage activated ion channels. These channels help transmit the signals generated by the dendrites. Emerging out from the soma is the axon hillock. This region is characterized by having a very high concentration of voltage-activated sodium channels. In general, it is considered to be the spike initiation zone for action potentials. Multiple signals generated at the spines, and transmitted by the soma all converge here. Immediately after the axon hillock is the axon. This is a thin tubular protrusion traveling away from the soma. The axon is insulated by a myelin sheath. Myelin is composed of either Schwann cells (in the peripheral nervous system) or oligodendrocytes (in the central nervous system), both of which are types of glial cells. Although glial cells are not involved with the transmission of electrical signals, they communicate and provide important biochemical support to neurons. To be specific, myelin wraps multiple times around the axonal segment, forming a thick fatty layer that prevents ions from entering or escaping the axon. This insulation prevents significant signal decay as well as ensuring faster signal speed. This insulation, however, has the restriction that no channels can be present on the surface of the axon. There are, therefore, regularly spaced patches of membrane, which have no insulation. These nodes of Ranvier can be considered to be \"mini axon hillocks\", as their purpose is to boost the signal in order to prevent significant signal decay. At the furthest end, the axon loses its insulation and begins to branch into several axon terminals. These presynaptic terminals, or synaptic boutons, are a specialized area within the axon of the presynaptic cell that contains neurotransmitters enclosed in small membrane-bound spheres called synaptic vesicles.\n\nBefore considering the propagation of action potentials along axons and their termination at the synaptic knobs, it is helpful to consider the methods by which action potentials can be initiated at the axon hillock. The basic requirement is that the membrane voltage at the hillock be raised above the threshold for firing. There are several ways in which this depolarization can occur.\n\nAction potentials are most commonly initiated by excitatory postsynaptic potentials from a presynaptic neuron. Typically, neurotransmitter molecules are released by the presynaptic neuron. These neurotransmitters then bind to receptors on the postsynaptic cell. This binding opens various types of ion channels. This opening has the further effect of changing the local permeability of the cell membrane and, thus, the membrane potential. If the binding increases the voltage (depolarizes the membrane), the synapse is excitatory. If, however, the binding decreases the voltage (hyperpolarizes the membrane), it is inhibitory. Whether the voltage is increased or decreased, the change propagates passively to nearby regions of the membrane (as described by the cable equation and its refinements). Typically, the voltage stimulus decays exponentially with the distance from the synapse and with time from the binding of the neurotransmitter. Some fraction of an excitatory voltage may reach the axon hillock and may (in rare cases) depolarize the membrane enough to provoke a new action potential. More typically, the excitatory potentials from several synapses must work together at nearly the same time to provoke a new action potential. Their joint efforts can be thwarted, however, by the counteracting inhibitory postsynaptic potentials.\n\nNeurotransmission can also occur through electrical synapses. Due to the direct connection between excitable cells in the form of gap junctions, an action potential can be transmitted directly from one cell to the next in either direction. The free flow of ions between cells enables rapid non-chemical-mediated transmission. Rectifying channels ensure that action potentials move only in one direction through an electrical synapse. Electrical synapses are found in all nervous systems, including the human brain, although they are a distinct minority.\n\nThe amplitude of an action potential is independent of the amount of current that produced it. In other words, larger currents do not create larger action potentials. Therefore, action potentials are said to be all-or-none signals, since either they occur fully or they do not occur at all. This is in contrast to receptor potentials, whose amplitudes are dependent on the intensity of a stimulus. In both cases, the frequency of action potentials is correlated with the intensity of a stimulus.\n\nIn sensory neurons, an external signal such as pressure, temperature, light, or sound is coupled with the opening and closing of ion channels, which in turn alter the ionic permeabilities of the membrane and its voltage. These voltage changes can again be excitatory (depolarizing) or inhibitory (hyperpolarizing) and, in some sensory neurons, their combined effects can depolarize the axon hillock enough to provoke action potentials. Some examples in humans include the olfactory receptor neuron and Meissner's corpuscle, which are critical for the sense of smell and touch, respectively. However, not all sensory neurons convert their external signals into action potentials; some do not even have an axon! Instead, they may convert the signal into the release of a neurotransmitter, or into continuous graded potentials, either of which may stimulate subsequent neuron(s) into firing an action potential. For illustration, in the human ear, hair cells convert the incoming sound into the opening and closing of mechanically gated ion channels, which may cause neurotransmitter molecules to be released. In similar manner, in the human retina, the initial photoreceptor cells and the next layer of cells (comprising bipolar cells and horizontal cells) do not produce action potentials; only some amacrine cells and the third layer, the ganglion cells, produce action potentials, which then travel up the optic nerve.\n\nIn sensory neurons, action potentials result from an external stimulus. However, some excitable cells require no such stimulus to fire: They spontaneously depolarize their axon hillock and fire action potentials at a regular rate, like an internal clock. The voltage traces of such cells are known as pacemaker potentials. The cardiac pacemaker cells of the sinoatrial node in the heart provide a good example. Although such pacemaker potentials have a natural rhythm, it can be adjusted by external stimuli; for instance, heart rate can be altered by pharmaceuticals as well as signals from the sympathetic and parasympathetic nerves. The external stimuli do not cause the cell's repetitive firing, but merely alter its timing. In some cases, the regulation of frequency can be more complex, leading to patterns of action potentials, such as bursting.\n\nThe course of the action potential can be divided into five parts: the rising phase, the peak phase, the falling phase, the undershoot phase, and the refractory period. During the rising phase the membrane potential depolarizes (becomes more positive). The point at which depolarization stops is called the peak phase. At this stage, the membrane potential reaches a maximum. Subsequent to this, there is a falling phase. During this stage the membrane potential becomes more negative, returning towards resting potential. The undershoot, or afterhyperpolarization, phase is the period during which the membrane potential temporarily becomes more negatively charged than when at rest (hyperpolarized). Finally, the time during which a subsequent action potential is impossible or difficult to fire is called the refractory period, which may overlap with the other phases.\n\nThe course of the action potential is determined by two coupled effects. First, voltage-sensitive ion channels open and close in response to changes in the membrane voltage \"V\". This changes the membrane's permeability to those ions. Second, according to the Goldman equation, this change in permeability changes in the equilibrium potential \"E\", and, thus, the membrane voltage \"V\". Thus, the membrane potential affects the permeability, which then further affects the membrane potential. This sets up the possibility for positive feedback, which is a key part of the rising phase of the action potential. A complicating factor is that a single ion channel may have multiple internal \"gates\" that respond to changes in \"V\" in opposite ways, or at different rates. For example, although raising \"V\" \"opens\" most gates in the voltage-sensitive sodium channel, it also \"closes\" the channel's \"inactivation gate\", albeit more slowly. Hence, when \"V\" is raised suddenly, the sodium channels open initially, but then close due to the slower inactivation.\n\nThe voltages and currents of the action potential in all of its phases were modeled accurately by Alan Lloyd Hodgkin and Andrew Huxley in 1952, for which they were awarded the Nobel Prize in Physiology or Medicine in 1963. However, their model considers only two types of voltage-sensitive ion channels, and makes several assumptions about them, e.g., that their internal gates open and close independently of one another. In reality, there are many types of ion channels, and they do not always open and close independently.\n\nA typical action potential begins at the axon hillock with a sufficiently strong depolarization, e.g., a stimulus that increases \"V\". This depolarization is often caused by the injection of extra sodium cations into the cell; these cations can come from a wide variety of sources, such as chemical synapses, sensory neurons or pacemaker potentials.\n\nFor a neuron at rest, there is a high concentration of sodium and chloride ions in the extracellular fluid compared to the intracellular fluid while there is a high concentration of potassium ions in the intracellular fluid compared to the extracellular fluid. This concentration gradient along with potassium leak channels present on the membrane of the neuron causes an efflux of potassium ions making the resting potential close to \"E\" ≈ –75 mV. The depolarization opens both the sodium and potassium channels in the membrane, allowing the ions to flow into and out of the axon, respectively. If the depolarization is small (say, increasing \"V\" from −70 mV to −60 mV), the outward potassium current overwhelms the inward sodium current and the membrane repolarizes back to its normal resting potential around −70 mV.However, if the depolarization is large enough, the inward sodium current increases more than the outward potassium current and a runaway condition (positive feedback) results: the more inward current there is, the more \"V\" increases, which in turn further increases the inward current. A sufficiently strong depolarization (increase in \"V\") causes the voltage-sensitive sodium channels to open; the increasing permeability to sodium drives \"V\" closer to the sodium equilibrium voltage \"E\"≈ +55 mV. The increasing voltage in turn causes even more sodium channels to open, which pushes \"V\" still further towards \"E\". This positive feedback continues until the sodium channels are fully open and \"V\" is close to \"E\". The sharp rise in \"V\" and sodium permeability correspond to the \"rising phase\" of the action potential.\n\nThe critical threshold voltage for this runaway condition is usually around −45 mV, but it depends on the recent activity of the axon. A membrane that has just fired an action potential cannot fire another one immediately, since the ion channels have not returned to the deactivated state. The period during which no new action potential can be fired is called the \"absolute refractory period\". At longer times, after some but not all of the ion channels have recovered, the axon can be stimulated to produce another action potential, but with a higher threshold, requiring a much stronger depolarization, e.g., to −30 mV. The period during which action potentials are unusually difficult to evoke is called the \"relative refractory period\".\n\nThe positive feedback of the rising phase slows and comes to a halt as the sodium ion channels become maximally open. At the peak of the action potential, the sodium permeability is maximized and the membrane voltage \"V\" is nearly equal to the sodium equilibrium voltage \"E\". However, the same raised voltage that opened the sodium channels initially also slowly shuts them off, by closing their pores; the sodium channels become \"inactivated\". This lowers the membrane's permeability to sodium relative to potassium, driving the membrane voltage back towards the resting value. At the same time, the raised voltage opens voltage-sensitive potassium channels; the increase in the membrane's potassium permeability drives \"V\" towards \"E\". Combined, these changes in sodium and potassium permeability cause \"V\" to drop quickly, repolarizing the membrane and producing the \"falling phase\" of the action potential.\n\nThe raised voltage opened many more potassium channels than usual, and some of these do not close right away when the membrane returns to its normal resting voltage. In addition, further potassium channels open in response to the influx of calcium ions during the action potential. The potassium permeability of the membrane is transiently unusually high, driving the membrane voltage \"V\" even closer to the potassium equilibrium voltage \"E\". Hence, there is an undershoot or hyperpolarization, termed an afterhyperpolarization in technical language, that persists until the membrane potassium permeability returns to its usual value.\n\nEach action potential is followed by a refractory period, which can be divided into an \"absolute refractory period\", during which it is impossible to evoke another action potential, and then a \"relative refractory period\", during which a stronger-than-usual stimulus is required. These two refractory periods are caused by changes in the state of sodium and potassium channel molecules. When closing after an action potential, sodium channels enter an \"inactivated\" state, in which they cannot be made to open regardless of the membrane potential—this gives rise to the absolute refractory period. Even after a sufficient number of sodium channels have transitioned back to their resting state, it frequently happens that a fraction of potassium channels remains open, making it difficult for the membrane potential to depolarize, and thereby giving rise to the relative refractory period. Because the density and subtypes of potassium channels may differ greatly between different types of neurons, the duration of the relative refractory period is highly variable.\n\nThe absolute refractory period is largely responsible for the unidirectional propagation of action potentials along axons. At any given moment, the patch of axon behind the actively spiking part is refractory, but the patch in front, not having been activated recently, is capable of being stimulated by the depolarization from the action potential.\n\nThe action potential generated at the axon hillock propagates as a wave along the axon. The currents flowing inwards at a point on the axon during an action potential spread out along the axon, and depolarize the adjacent sections of its membrane. If sufficiently strong, this depolarization provokes a similar action potential at the neighboring membrane patches. This basic mechanism was demonstrated by Alan Lloyd Hodgkin in 1937. After crushing or cooling nerve segments and thus blocking the action potentials, he showed that an action potential arriving on one side of the block could provoke another action potential on the other, provided that the blocked segment was sufficiently short.\n\nOnce an action potential has occurred at a patch of membrane, the membrane patch needs time to recover before it can fire again. At the molecular level, this \"absolute refractory period\" corresponds to the time required for the voltage-activated sodium channels to recover from inactivation, i.e., to return to their closed state. There are many types of voltage-activated potassium channels in neurons, some of them inactivate fast (A-type currents) and some of them inactivate slowly or not inactivate at all; this variability guarantees that there will be always an available source of current for repolarization, even if some of the potassium channels are inactivated because of preceding depolarization. On the other hand, all neuronal voltage-activated sodium channels inactivate within several millisecond during strong depolarization, thus making following depolarization impossible until a substantial fraction of sodium channels have returned to their closed state. Although it limits the frequency of firing, the absolute refractory period ensures that the action potential moves in only one direction along an axon. The currents flowing in due to an action potential spread out in both directions along the axon. However, only the unfired part of the axon can respond with an action potential; the part that has just fired is unresponsive until the action potential is safely out of range and cannot restimulate that part. In the usual orthodromic conduction, the action potential propagates from the axon hillock towards the synaptic knobs (the axonal termini); propagation in the opposite direction—known as antidromic conduction—is very rare. However, if a laboratory axon is stimulated in its middle, both halves of the axon are \"fresh\", i.e., unfired; then two action potentials will be generated, one traveling towards the axon hillock and the other traveling towards the synaptic knobs.\n\nIn order to enable fast and efficient transduction of electrical signals in the nervous system, certain neuronal axons are covered with myelin sheaths. Myelin is a multilamellar membrane that enwraps the axon in segments separated by intervals known as nodes of Ranvier. It is produced by specialized cells: Schwann cells exclusively in the peripheral nervous system, and oligodendrocytes exclusively in the central nervous system. Myelin sheath reduces membrane capacitance and increases membrane resistance in the inter-node intervals, thus allowing a fast, saltatory movement of action potentials from node to node. Myelination is found mainly in vertebrates, but an analogous system has been discovered in a few invertebrates, such as some species of shrimp. Not all neurons in vertebrates are myelinated; for example, axons of the neurons comprising the autonomous nervous system are not, in general, myelinated.\n\nMyelin prevents ions from entering or leaving the axon along myelinated segments. As a general rule, myelination increases the conduction velocity of action potentials and makes them more energy-efficient. Whether saltatory or not, the mean conduction velocity of an action potential ranges from 1 meter per second (m/s) to over 100 m/s, and, in general, increases with axonal diameter.\n\nAction potentials cannot propagate through the membrane in myelinated segments of the axon. However, the current is carried by the cytoplasm, which is sufficient to depolarize the first or second subsequent node of Ranvier. Instead, the ionic current from an action potential at one node of Ranvier provokes another action potential at the next node; this apparent \"hopping\" of the action potential from node to node is known as saltatory conduction. Although the mechanism of saltatory conduction was suggested in 1925 by Ralph Lillie, the first experimental evidence for saltatory conduction came from Ichiji Tasaki and Taiji Takeuchi and from Andrew Huxley and Robert Stämpfli. By contrast, in unmyelinated axons, the action potential provokes another in the membrane immediately adjacent, and moves continuously down the axon like a wave.\n\nMyelin has two important advantages: fast conduction speed and energy efficiency. For axons larger than a minimum diameter (roughly 1 micrometre), myelination increases the conduction velocity of an action potential, typically tenfold. Conversely, for a given conduction velocity, myelinated fibers are smaller than their unmyelinated counterparts. For example, action potentials move at roughly the same speed (25 m/s) in a myelinated frog axon and an unmyelinated squid giant axon, but the frog axon has a roughly 30-fold smaller diameter and 1000-fold smaller cross-sectional area. Also, since the ionic currents are confined to the nodes of Ranvier, far fewer ions \"leak\" across the membrane, saving metabolic energy. This saving is a significant selective advantage, since the human nervous system uses approximately 20% of the body's metabolic energy.\n\nThe length of axons' myelinated segments is important to the success of saltatory conduction. They should be as long as possible to maximize the speed of conduction, but not so long that the arriving signal is too weak to provoke an action potential at the next node of Ranvier. In nature, myelinated segments are generally long enough for the passively propagated signal to travel for at least two nodes while retaining enough amplitude to fire an action potential at the second or third node. Thus, the safety factor of saltatory conduction is high, allowing transmission to bypass nodes in case of injury. However, action potentials may end prematurely in certain places where the safety factor is low, even in unmyelinated neurons; a common example is the branch point of an axon, where it divides into two axons.\n\nSome diseases degrade myelin and impair saltatory conduction, reducing the conduction velocity of action potentials. The most well-known of these is multiple sclerosis, in which the breakdown of myelin impairs coordinated movement.\n\nThe flow of currents within an axon can be described quantitatively by cable theory and its elaborations, such as the compartmental model. Cable theory was developed in 1855 by Lord Kelvin to model the transatlantic telegraph cable and was shown to be relevant to neurons by Hodgkin and Rushton in 1946. In simple cable theory, the neuron is treated as an electrically passive, perfectly cylindrical transmission cable, which can be described by a partial differential equation\n\nwhere \"V\"(\"x\", \"t\") is the voltage across the membrane at a time \"t\" and a position \"x\" along the length of the neuron, and where λ and τ are the characteristic length and time scales on which those voltages decay in response to a stimulus. Referring to the circuit diagram on the right, these scales can be determined from the resistances and capacitances per unit length.\n\nThese time and length-scales can be used to understand the dependence of the conduction velocity on the diameter of the neuron in unmyelinated fibers. For example, the time-scale τ increases with both the membrane resistance \"r\" and capacitance \"c\". As the capacitance increases, more charge must be transferred to produce a given transmembrane voltage (by the equation \"Q\" = \"CV\"); as the resistance increases, less charge is transferred per unit time, making the equilibration slower. In similar manner, if the internal resistance per unit length \"r\" is lower in one axon than in another (e.g., because the radius of the former is larger), the spatial decay length λ becomes longer and the conduction velocity of an action potential should increase. If the transmembrane resistance \"r\" is increased, that lowers the average \"leakage\" current across the membrane, likewise causing \"λ\" to become longer, increasing the conduction velocity.\n\nIn general, action potentials that reach the synaptic knobs cause a neurotransmitter to be released into the synaptic cleft. Neurotransmitters are small molecules that may open ion channels in the postsynaptic cell; most axons have the same neurotransmitter at all of their termini. The arrival of the action potential opens voltage-sensitive calcium channels in the presynaptic membrane; the influx of calcium causes vesicles filled with neurotransmitter to migrate to the cell's surface and release their contents into the synaptic cleft. This complex process is inhibited by the neurotoxins tetanospasmin and botulinum toxin, which are responsible for tetanus and botulism, respectively.\n\nSome synapses dispense with the \"middleman\" of the neurotransmitter, and connect the presynaptic and postsynaptic cells together. When an action potential reaches such a synapse, the ionic currents flowing into the presynaptic cell can cross the barrier of the two cell membranes and enter the postsynaptic cell through pores known as connexons. Thus, the ionic currents of the presynaptic action potential can directly stimulate the postsynaptic cell. Electrical synapses allow for faster transmission because they do not require the slow diffusion of neurotransmitters across the synaptic cleft. Hence, electrical synapses are used whenever fast response and coordination of timing are crucial, as in escape reflexes, the retina of vertebrates, and the heart.\n\nA special case of a chemical synapse is the neuromuscular junction, in which the axon of a motor neuron terminates on a muscle fiber. In such cases, the released neurotransmitter is acetylcholine, which binds to the acetylcholine receptor, an integral membrane protein in the membrane (the \"sarcolemma\") of the muscle fiber. However, the acetylcholine does not remain bound; rather, it dissociates and is hydrolyzed by the enzyme, acetylcholinesterase, located in the synapse. This enzyme quickly reduces the stimulus to the muscle, which allows the degree and timing of muscular contraction to be regulated delicately. Some poisons inactivate acetylcholinesterase to prevent this control, such as the nerve agents sarin and tabun, and the insecticides diazinon and malathion.\n\nThe cardiac action potential differs from the neuronal action potential by having an extended plateau, in which the membrane is held at a high voltage for a few hundred milliseconds prior to being repolarized by the potassium current as usual. This plateau is due to the action of slower calcium channels opening and holding the membrane voltage near their equilibrium potential even after the sodium channels have inactivated.\n\nThe cardiac action potential plays an important role in coordinating the contraction of the heart. The cardiac cells of the sinoatrial node provide the pacemaker potential that synchronizes the heart. The action potentials of those cells propagate to and through the atrioventricular node (AV node), which is normally the only conduction pathway between the atria and the ventricles. Action potentials from the AV node travel through the bundle of His and thence to the Purkinje fibers. Conversely, anomalies in the cardiac action potential—whether due to a congenital mutation or injury—can lead to human pathologies, especially arrhythmias. Several anti-arrhythmia drugs act on the cardiac action potential, such as quinidine, lidocaine, beta blockers, and verapamil.\n\nThe action potential in a normal skeletal muscle cell is similar to the action potential in neurons. Action potentials result from the depolarization of the cell membrane (the sarcolemma), which opens voltage-sensitive sodium channels; these become inactivated and the membrane is repolarized through the outward current of potassium ions. The resting potential prior to the action potential is typically −90mV, somewhat more negative than typical neurons. The muscle action potential lasts roughly 2–4 ms, the absolute refractory period is roughly 1–3 ms, and the conduction velocity along the muscle is roughly 5 m/s. The action potential releases calcium ions that free up the tropomyosin and allow the muscle to contract. Muscle action potentials are provoked by the arrival of a pre-synaptic neuronal action potential at the neuromuscular junction, which is a common target for neurotoxins.\n\nPlant and fungal cells are also electrically excitable. The fundamental difference to animal action potentials is that the depolarization in plant cells is not accomplished by an uptake of positive sodium ions, but by release of negative \"chloride\" ions. Together with the following release of positive potassium ions, which is common to plant and animal action potentials, the action potential in plants infers, therefore, an osmotic loss of salt (KCl), whereas the animal action potential is osmotically neutral, when equal amounts of entering sodium and leaving potassium cancel each other osmotically. The interaction of electrical and osmotic relations in plant cells indicates an osmotic function of electrical excitability in the common, unicellular ancestors of plants and animals under changing salinity conditions, whereas the present function of rapid signal transmission is seen as a younger accomplishment of metazoan cells in a more stable osmotic environment. It must be assumed that the familiar signalling function of action potentials in some vascular plants (e.g. \"Mimosa pudica\") arose independently from that in metazoan excitable cells.\n\nAction potentials are found throughout multicellular organisms, including plants, invertebrates such as insects, and vertebrates such as reptiles and mammals. Sponges seem to be the main phylum of multicellular eukaryotes that does not transmit action potentials, although some studies have suggested that these organisms have a form of electrical signaling, too. The resting potential, as well as the size and duration of the action potential, have not varied much with evolution, although the conduction velocity does vary dramatically with axonal diameter and myelination.\n\nGiven its conservation throughout evolution, the action potential seems to confer evolutionary advantages. One function of action potentials is rapid, long-range signaling within the organism; the conduction velocity can exceed 110 m/s, which is one-third the speed of sound. For comparison, a hormone molecule carried in the bloodstream moves at roughly 8 m/s in large arteries. Part of this function is the tight coordination of mechanical events, such as the contraction of the heart. A second function is the computation associated with its generation. Being an all-or-none signal that does not decay with transmission distance, the action potential has similar advantages to digital electronics. The integration of various dendritic signals at the axon hillock and its thresholding to form a complex train of action potentials is another form of computation, one that has been exploited biologically to form central pattern generators and mimicked in artificial neural networks.\n\nThe study of action potentials has required the development of new experimental methods. The initial work, prior to 1955, was carried out primarily by Alan Lloyd Hodgkin and Andrew Fielding Huxley, who were, along John Carew Eccles, awarded the 1963 Nobel Prize in Physiology or Medicine for their contribution to the description of the ionic basis of nerve conduction. It focused on three goals: isolating signals from single neurons or axons, developing fast, sensitive electronics, and shrinking electrodes enough that the voltage inside a single cell could be recorded.\n\nThe first problem was solved by studying the giant axons found in the neurons of the squid (\"Loligo forbesii\" and \"Doryteuthis pealeii\", at the time classified as \"Loligo pealeii\"). These axons are so large in diameter (roughly 1 mm, or 100-fold larger than a typical neuron) that they can be seen with the naked eye, making them easy to extract and manipulate. However, they are not representative of all excitable cells, and numerous other systems with action potentials have been studied.\n\nThe second problem was addressed with the crucial development of the voltage clamp, which permitted experimenters to study the ionic currents underlying an action potential in isolation, and eliminated a key source of electronic noise, the current \"I\" associated with the capacitance \"C\" of the membrane. Since the current equals \"C\" times the rate of change of the transmembrane voltage \"V\", the solution was to design a circuit that kept \"V\" fixed (zero rate of change) regardless of the currents flowing across the membrane. Thus, the current required to keep \"V\" at a fixed value is a direct reflection of the current flowing through the membrane. Other electronic advances included the use of Faraday cages and electronics with high input impedance, so that the measurement itself did not affect the voltage being measured.\n\nThe third problem, that of obtaining electrodes small enough to record voltages within a single axon without perturbing it, was solved in 1949 with the invention of the glass micropipette electrode, which was quickly adopted by other researchers. Refinements of this method are able to produce electrode tips that are as fine as 100 Å (10 nm), which also confers high input impedance. Action potentials may also be recorded with small metal electrodes placed just next to a neuron, with neurochips containing EOSFETs, or optically with dyes that are sensitive to Ca or to voltage.\n\nWhile glass micropipette electrodes measure the sum of the currents passing through many ion channels, studying the electrical properties of a single ion channel became possible in the 1970s with the development of the patch clamp by Erwin Neher and Bert Sakmann. For this discovery, they were awarded the Nobel Prize in Physiology or Medicine in 1991. Patch-clamping verified that ionic channels have discrete states of conductance, such as open, closed and inactivated.\n\nOptical imaging technologies have been developed in recent years to measure action potentials, either via simultaneous multisite recordings or with ultra-spatial resolution. Using voltage-sensitive dyes, action potentials have been optically recorded from a tiny patch of cardiomyocyte membrane.\n\nSeveral neurotoxins, both natural and synthetic, are designed to block the action potential. Tetrodotoxin from the pufferfish and saxitoxin from the \"Gonyaulax\" (the dinoflagellate genus responsible for \"red tides\") block action potentials by inhibiting the voltage-sensitive sodium channel; similarly, dendrotoxin from the black mamba snake inhibits the voltage-sensitive potassium channel. Such inhibitors of ion channels serve an important research purpose, by allowing scientists to \"turn off\" specific channels at will, thus isolating the other channels' contributions; they can also be useful in purifying ion channels by affinity chromatography or in assaying their concentration. However, such inhibitors also make effective neurotoxins, and have been considered for use as chemical weapons. Neurotoxins aimed at the ion channels of insects have been effective insecticides; one example is the synthetic permethrin, which prolongs the activation of the sodium channels involved in action potentials. The ion channels of insects are sufficiently different from their human counterparts that there are few side effects in humans.\n\nThe role of electricity in the nervous systems of animals was first observed in dissected frogs by Luigi Galvani, who studied it from 1791 to 1797. Galvani's results stimulated Alessandro Volta to develop the Voltaic pile—the earliest-known electric battery—with which he studied animal electricity (such as electric eels) and the physiological responses to applied direct-current voltages.\n\nScientists of the 19th century studied the propagation of electrical signals in whole nerves (i.e., bundles of neurons) and demonstrated that nervous tissue was made up of cells, instead of an interconnected network of tubes (a \"reticulum\"). Carlo Matteucci followed up Galvani's studies and demonstrated that cell membranes had a voltage across them and could produce direct current. Matteucci's work inspired the German physiologist, Emil du Bois-Reymond, who discovered the action potential in 1843. The conduction velocity of action potentials was first measured in 1850 by du Bois-Reymond's friend, Hermann von Helmholtz. To establish that nervous tissue is made up of discrete cells, the Spanish physician Santiago Ramón y Cajal and his students used a stain developed by Camillo Golgi to reveal the myriad shapes of neurons, which they rendered painstakingly. For their discoveries, Golgi and Ramón y Cajal were awarded the 1906 Nobel Prize in Physiology. Their work resolved a long-standing controversy in the neuroanatomy of the 19th century; Golgi himself had argued for the network model of the nervous system.\n\nThe 20th century was a significant era for electrophysiology. In 1902 and again in 1912, Julius Bernstein advanced the hypothesis that the action potential resulted from a change in the permeability of the axonal membrane to ions. Bernstein's hypothesis was confirmed by Ken Cole and Howard Curtis, who showed that membrane conductance increases during an action potential. In 1907, Louis Lapicque suggested that the action potential was generated as a threshold was crossed, what would be later shown as a product of the dynamical systems of ionic conductances. In 1949, Alan Hodgkin and Bernard Katz refined Bernstein's hypothesis by considering that the axonal membrane might have different permeabilities to different ions; in particular, they demonstrated the crucial role of the sodium permeability for the action potential. They made the first actual recording of the electrical changes across the neuronal membrane that mediate the action potential. This line of research culminated in the five 1952 papers of Hodgkin, Katz and Andrew Huxley, in which they applied the voltage clamp technique to determine the dependence of the axonal membrane's permeabilities to sodium and potassium ions on voltage and time, from which they were able to reconstruct the action potential quantitatively. Hodgkin and Huxley correlated the properties of their mathematical model with discrete ion channels that could exist in several different states, including \"open\", \"closed\", and \"inactivated\". Their hypotheses were confirmed in the mid-1970s and 1980s by Erwin Neher and Bert Sakmann, who developed the technique of patch clamping to examine the conductance states of individual ion channels. In the 21st century, researchers are beginning to understand the structural basis for these conductance states and for the selectivity of channels for their species of ion, through the atomic-resolution crystal structures, fluorescence distance measurements and cryo-electron microscopy studies.\n\nJulius Bernstein was also the first to introduce the Nernst equation for resting potential across the membrane; this was generalized by David E. Goldman to the eponymous Goldman equation in 1943. The sodium–potassium pump was identified in 1957 and its properties gradually elucidated, culminating in the determination of its atomic-resolution structure by X-ray crystallography. The crystal structures of related ionic pumps have also been solved, giving a broader view of how these molecular machines work.\n\nMathematical and computational models are essential for understanding the action potential, and offer predictions that may be tested against experimental data, providing a stringent test of a theory. The most important and accurate of the early neural models is the Hodgkin–Huxley model, which describes the action potential by a coupled set of four ordinary differential equations (ODEs). Although the Hodgkin–Huxley model may be a simplification with few limitations compared to the realistic nervous membrane as it exists in nature, its complexity has inspired several even-more-simplified models, such as the Morris–Lecar model and the FitzHugh–Nagumo model, both of which have only two coupled ODEs. The properties of the Hodgkin–Huxley and FitzHugh–Nagumo models and their relatives, such as the Bonhoeffer–van der Pol model, have been well-studied within mathematics, computation and electronics. However the simple models of generator potential and action potential fail to accurately reproduce the near threshold neural spike rate and spike shape, specifically for the mechanoreceptors like the Pacinian corpuscle. More modern research has focused on larger and more integrated systems; by joining action-potential models with models of other parts of the nervous system (such as dendrites and synapses), researchers can study neural computation and simple reflexes, such as escape reflexes and others controlled by central pattern generators.\n\n\n\n\n\n", "id": "156998", "title": "Action potential"}
{"url": "https://en.wikipedia.org/wiki?curid=2930542", "text": "Epithelial–mesenchymal transition\n\nThe epithelial–mesenchymal transition (EMT) is a process by which epithelial cells lose their cell polarity and cell-cell adhesion, and gain migratory and invasive properties to become mesenchymal stem cells; these are multipotent stromal cells that can differentiate into a variety of cell types. EMT is essential for numerous developmental processes including mesoderm formation and neural tube formation. EMT has also been shown to occur in wound healing, in organ fibrosis and in the initiation of metastasis in cancer progression.\n\nEpithelial–mesenchymal transition was first recognized as a feature of embryogenesis by Betty Hay in the 1980s. EMT, and its reverse process, MET (mesenchymal-epithelial transition) are critical for development of many tissues and organs in the developing embryo, and numerous embryonic events such as gastrulation, neural crest formation, heart valve formation, palatogenesis and myogenesis. Epithelial and mesenchymal cells differ in phenotype as well as function, though both share inherent plasticity. Epithelial cells are closely connected to each other by tight junctions, gap junctions and adherens junctions, have an apico-basal polarity, polarization of the actin cytoskeleton and are bound by a basal lamina at their basal surface. Mesenchymal cells, on the other hand, lack this polarization, have a spindle-shaped morphology and interact with each other only through focal points. Epithelial cells express high levels of E-cadherin, whereas mesenchymal cells express those of N-cadherin, fibronectin and vimentin. Thus, EMT entails profound morphological and phenotypic changes to a cell.\n\nBased on the biological context, EMT has been categorized into 3 types : developmental (Type I), fibrosis and wound healing (Type II), and cancer (Type III).\n\nLoss of E-cadherin is considered to be a fundamental event in EMT. Many transcription factors (TFs) that can repress E-cadherin directly or indirectly can be considered as EMT-TF (EMT inducing TFs). SNAI1/Snail 1, SNAI2/Snail 2 (also known as Slug), ZEB1, ZEB2, TCF3 and KLF8 (Kruppel-like factor 8) can bind to the E-cadherin promoter and repress its transcription, whereas factors such as Twist, Goosecoid, TCF4 (also known as E2.2), homeobox protein SIX1 and FOXC2 (fork-head box protein C2) repress E-cadherin indirectly. SNAIL and ZEB factors bind to E-box consensus sequences on the promoter region, while KLF8 binds to promoter through GT boxes. These EMT-TFs not only directly repress E-cadherin, but also repress transcriptionally other junctional proteins, including claudins and desmosomes, thus facilitating EMT. On the other hand, transcription factors such as grainyhead-like protein 2 homologue (GRHL2), and ETS-related transcription factors ELF3 and ELF5 are downregulated during EMT and are found to actively drive MET when overexpressed in mesenchymal cells. Since EMT in cancer progression recaptures EMT in developmental programs, many of the EMT-TFs are involved in promoting metastatic events.\n\nSeveral signaling pathways (TGF-β, FGF, EGF, HGF, Wnt/beta-catenin and Notch) and hypoxia may induce EMT. In particular, Ras-MAPK has been shown to activate Snail and Slug. Slug triggers the steps of desmosomal disruption, cell spreading, and partial separation at cell–cell borders, which comprise the first and necessary phase of the EMT process. On the other hand, Slug cannot trigger the second phase, which includes the induction of cell motility, repression of the cytokeratin expression, and activation of vimentin expression. Snail and Slug are known to regulate the expression of p63 isoforms, another transcription factor that is required for proper development of epithelial structures. The altered expression of p63 isoforms reduced cell–cell adhesion and increased the migratory properties of cancer cells. The p63 factor is involved in inhibiting EMT and reduction of certain p63 isoforms may be important in the development of epithelial cancers. Some of them are known to regulate the expression of cytokeratins. The phosphatidylinositol 3' kinase (PI3K)/AKT axis, Hedgehog signaling pathway, nuclear factor-kappaB and Activating Transcription Factor 2 have also been implicated to be involved in EMT.\n\nWnt signaling pathway regulates EMT in gastrulation, cardiac valve formation and cancer. Activation of Wnt pathway in breast cancer cells induces the EMT regulator SNAIL and upregulates the mesenchymal marker, vimentin. Also, active Wnt/beta-catenin pathway correlates with poor prognosis in breast cancer patients in the clinic. Similarly, TGF-β activates the expression of SNAIL and ZEB to regulate EMT in heart development, palatogenesis, and cancer. The breast cancer bone metastasis has activated TGF-β signaling, which contributes to the formation of these lesions. However, on the other hand, p53, a well-known tumor suppressor, represses EMT by activating the expression of various microRNAs – miR-200 and miR-34 that inhibit the production of protein ZEB and SNAIL, and thus maintain the epithelial phenotype.\n\nAfter the initial stage of embryogenesis, the implantation of the embryo and the initiation of placenta formation are associated with EMT. The trophoectoderm cells undergo EMT to facilitate the invasion of endometrium and appropriate placenta placement, thus enabling nutrient and gas exchange to the embryo. Later in embryogenesis, during gastrulation, EMT allows the cells to ingress in a specific area of the embryo – the primitive streak in amniotes, and the ventral furrow in Drosophila. The cells in this tissue express E-cadherin and apical-basal polarity. Since gastrulation is a very rapid process, E-cadherin is repressed transcriptionally by Twist and SNAI1 (commonly called \"Snail\"), and at the protein level by P38 interacting protein. The primitive streak, through invagination, further generates mesoendoderm, which separates to form a mesoderm and an endoderm, again through EMT. Mesenchymal cells from the primitive streak participate also in the formation of many epithelial mesodermal organs, such as notochord as well as somites, through the reverse of EMT, i.e. mesenchymal–epithelial transition. Amphioxus forms an epithelial neural tube and dorsal notochord but does not have the EMT potential of the primitive streak. In higher chordates, the mesenchyme originates out of the primitive streak migrates anteriorly to form the somites and participate with neural crest mesenchyme in formation of the heart mesoderm.\n\nIn vertebrates, epithelium and mesenchyme are the basic tissue phenotypes. During embryonic development, migratory neural crest cells are generated by EMT involving the epithelial cells of the neuroectoderm. As a result, these cells dissociate from neural folds, gain motility, and disseminate to various parts of the embryo, where they differentiate to many other cell types. Also, craniofacial crest mesenchyme that forms the connective tissue forming the head and face, is formed by neural tube epithelium by EMT. EMT takes place during the construction of the vertebral column out of the extracellular matrix, which is to be synthesized by fibroblasts and osteoblasts that encircle the neural tube. The major source of these cells are sclerotome and somite mesenchyme as well as primitive streak. Mesenchymal morphology allows the cells to travel to specific targets in the embryo, where they differentiate and/or induce differentiation of other cells.\n\nDuring wound healing, keratinocytes at the border of the wound undergo EMT and undergo re-epithelialization or MET when the wound is closed. Snail2 expression at the migratory front influences this state, as its overexpression accelerates wound healing. Similarly, in each menstrual cycle, the ovarian surface epithelium undergoes EMT during post-ovulatory wound healing.\n\nInitiation of metastasis requires invasion, which is enabled by EMT. Carcinoma cells in a primary tumor lose cell-cell adhesion mediated by E-cadherin repression and break through the basement membrane with increased invasive properties, and enter the bloodstream through intravasation. Later, when these circulating tumor cells (CTCs) exit the bloodstream to form micro-metastases, they undergo MET for clonal outgrowth at these metastatic sites. Thus, EMT and MET form the initiation and completion of the invasion-metastasis cascade. At this new metastatic site, the tumor may undergo other processes to optimize growth. For example, EMT has been associated with PD-L1 expression, particularly in lung cancer. Increased levels of PD-L1 suppresses the immune system which allows the cancer to spread more easily. \n\nEMT confers resistance to oncogene-induced premature senescence. Twist1 and Twist2, as well as ZEB1 protects human cells and mouse embryonic fibroblasts from senescence. Similarly, TGF-β can promote tumor invasion and evasion of immune surveillance at advanced stages. When TGF-β acts on activated Ras-expressing mammary epithelial cells, EMT is favored and apoptosis is inhibited. This effect can be reversed by inducers of epithelial differentiation, such as GATA-3.\n\nEMT has been shown to be induced by androgen deprivation therapy in metastatic prostate cancer. Activation of EMT programs via inhibition of the androgen axis provides a mechanism by which tumor cells can adapt to promote disease recurrence and progression. Brachyury, Axl, MEK, and Aurora kinase A are molecular drivers of these programs, and inhibitors are currently in clinical trials to determine therapeutic applications.\n\nEMT has been indicated to be involved in acquiring drug resistance. Gain of EMT markers was found to be associated with the resistance of ovarian carcinoma epithelial cell lines to paclitaxel. Similarly, SNAIL also confers resistance to paclitaxel, adriamycin and radiotherapy by inhibiting p53-mediated apoptosis. Furthermore, inflammation, that has been associated with the progression of cancer and fibrosis, was recently shown to be related to cancer through inflammation-induced EMT. Consequently, EMT enables cells to gain a migratory phenotype, as well as induce multiple immunosuppression, drug resistance, evasion of apoptosis mechanisms.\n\nSome evidence suggests that cells that undergo EMT gain stem cell-like properties, thus giving rise to Cancer Stem Cells (CSCs). Upon transfection by activated Ras, a subpopulation of cells exhibiting the putative stem cell markers CD44high/CD24low increases with the concomitant induction of EMT. Also, ZEB1 is capable of conferring stem cell-like properties, thus strengthening the relationship between EMT and stemness. Thus, EMT may present increased danger to cancer patients, as EMT not only enables the carcinoma cells to enter the bloodstream, but also endows them with properties of stemness which increases tumorigenic and proliferative potential.\n\nHowever, recent studies have further shifted the primary effects of EMT away from invasion and metastasis, toward resistance to chemotherapeutic agents. Research on breast cancer and pancreatic cancer both demonstrated no difference in cells' metastatic potential upon acquisition of EMT. These are in agreement with another study showing that the EMT transcription factor TWIST actually requires intact adherens junctions in order to mediate local invasion in breast cancer. The effects of EMT and its relationship to invasion and metastasis may therefore be highly context specific.\n\nPlatelets in the blood have the ability to initiate the induction of EMT in cancer cells. When platelets are recruited to a site in the blood vessel they can release a variety of growth factors (PDGF, VEGF, Angiopoietin-1) and cytokines including the EMT inducer TGF-β. The release of TGF-β by platelets in blood vessels near primary tumors enhances invasiveness and promotes metastasis of cancer cells in the tumor. Studies looking at defective platelets and reduced platelet counts in mouse models have shown that impaired platelet function is associated with decreased metastatic formation. In humans, platelet counts and thrombocytosis within the upper end of the normal range have been associated with advanced, often metastatic, stage cancer in cervical cancer, ovarian cancer, gastric cancer, and esophageal cancer. Although a great deal of research has been applied to studying interactions between tumor cells and platelets, a cancer therapy targeting this interaction has not yet been established. This may be in part due to the redundancy of prothrombotic pathways which would require the use of multiple therapeutic approaches in order to prevent pro-metastatic events via EMT induction in cancer cells by activated platelets.\n\nTo improve the chances for the development of a cancer metastasis, a cancer cell must avoid detection and targeting by the immune system once it enters the bloodstream. Activated platelets have the ability to bind glycoproteins and glycolipids (P-selectin ligands such as PSGL-1) on the surface of cancer cells to form a physical barrier that protects the cancer cell from natural killer cell-mediated lysis in the bloodstream. Furthermore, activated platelets promote the adhesion of cancer cells to activated endothelial cells lining blood vessels using adhesion molecules present on platelets. P-selectin ligands on the surface of cancer cells remain to be elucidated and may serve as potential biomarkers for disease progression in cancer.\n\nMany studies have proposed that induction of EMT is the primary mechanism by which epithelial cancer cells acquire malignant phenotypes that promote metastasis. Drug development targeting the activation of EMT in cancer cells has thus become an aim of pharmaceutical companies.\n\nSmall molecules that are able to inhibit TGF-β induced EMT are under development. Silmitasertib (CX-4945) is a small molecule inhibitor of protein kinase CK2, which has been supported to be linked with TGF-β induced EMT, and is currently in clinical trials for chlangiocarcinoma (bile duct cancer), as well as in preclinical development for hematological and lymphoid malignancies. In January 2017, Silmitasertib was granted orphan drug status by the U.S. Food and Drug Administration for cholangiocarcinoma and is currently in phase II study. Silmitasertib is being developed by Senhwa Biosciences. Another small molecule inhibitor Galunisertib (LY2157299) is a potent TGF-β type I receptor kinase inhibitor that was demonstrated to reduce the size, the growth rate of tumors, and the tumor forming potential in triple negative breast cancer cell lines using mouse xenografts. Galunisertib is currently being developed by Lilly Oncology and is in phase I/II clinical trials for hepatocellular carcinoma, unresectable pancreatic cancer, and malignant glioma. Small molecule inhibitors of EMT are suggested to not act as a replacement for traditional chemotherapeutic agents but are likely to display the greatest efficacy in treating cancers when used in conjunction with them.\n\nAntagomirs and microRNA mimics have gained interest as a potential source of therapeutics to target EMT induced metastasis in cancer as well as treating many other diseases. Antagomirs were first developed to target miR-122, a microRNA that was abundant and specific to the liver, and this discovery has led to the development of other antagomirs that can pair with specific microRNAs present in the tumor microenvironment or in the cancer cells. A microRNA mimic to miR-655 was found to suppress EMT through the targeting of EMT inducing transcription factor ZEB1 and TGF-β receptor 2 in a pancreatic cancer cell line. Overexpression of the miR-655 mimic in the Panc1 cancer cell line upregulated the expression of E-cadherin and suppressed the migration and invasion of mesenchymal-like cancer cells. The use of microRNA mimics to suppress EMT has expanded to other cancer cell lines and holds potential for clinical drug development. However, microRNA mimics and antagomirs suffer from a lack of stability \"in vivo\" and lack an accurate delivery system to target these molecules to the tumor cells or tissue for treatment. Improvements to antagomir and microRNA mimic stability through chemical modifications such as locked nucleic acid (LNA) oligonucleotides or peptide nucleic acids (PNA) can prevent the fast clearing of the these small molecules by RNases. Delivery of antagomirs and microRNA mimics into cells by enclosing these molecules in liposome-nanoparticles has generated interest however liposome structures suffer from their own drawbacks that will need to be overcome for their effective use as a drug delivery mechanism. These drawbacks of liposome-nanoparticles include nonspecific uptake by cells and induction of immune responses. The role that microRNAs play in cancer development and metastasis is under much scientific investigation and it is yet to be demonstrated whether microRNA mimics or antagomirs may serve as standard clinical treatments to suppress EMT or oncogenic microRNAs in cancers.\n\nSimilar to generation of Cancer Stem Cells, EMT was demonstrated to generate endocrine progenitor cells from human pancreatic islets. Initially, the human islet-derived progenitor cells (hIPCs) were proposed to be better precursors since β-cell progeny in these hIPCs inherit epigenetic marks that define an active insulin promoter region. However, later, another set of experiments suggested that labelled β-cells de-differentiate to a mesenchymal-like phenotype \"in vitro\", but fail to proliferate; thus initiating a debate in 2007.\n\nSince these studies in human islets lacked lineage-tracing analysis, these findings from irreversibly tagged beta cells in mice were extrapolated to human islets. Thus, using a dual lentiviral and genetic lineage tracing system to label β-cells, it was convincingly demonstrated that adult human islet β-cells undergo EMT and proliferate \"in vitro\". Also, these findings were confirmed in human fetal pancreatic insulin-producing cells, and the mesenchymal cells derived from pancreatic islets can undergo the reverse of EMT – MET – to generate islet-like cell aggregates. Thus, the concept of generating progenitors from insulin-producing cells by EMT or generation of Cancer Stem Cells during EMT in cancer may have potential for replacement therapy in diabetes, and call for drugs targeting inhibition of EMT in cancer.\n\nNot all cells undergo a complete EMT, i.e. losing their cell-cell adhesion and gaining solitary migration characteristics. Instead, most cells undergo partial EMT, \na state in which they retain some epithelial traits such as cell-cell adhesion or apico-basal polarity, and gain migratory traits, thus cells in this hybrid epithelial/mesenchymal (E/M) phenotype are endowed with special properties such as collective cell migration. Two mathematical models have been proposed, attempting to explain the emergence of this hybrid E/M phenotype, and its highly likely that different cell lines adopt different hybrid state(s), as shown by experiments in MCF10A, HMLE and H1975 cell lines. Although a hybrid E/M state has been referred to as 'metastable' or transient, recent experiments in H1975 cells suggest that this state can be stably maintained by cells.\n\n\n", "id": "2930542", "title": "Epithelial–mesenchymal transition"}
{"url": "https://en.wikipedia.org/wiki?curid=40733699", "text": "Actin nucleation core\n\nAn actin nucleation core is a protein trimer with three actin monomers. It is called a nucleation core because it leads to the energetically favorable elongation reaction once a tetramer is formed from a trimer. Actin protein dimers and trimers are energetically unfavorable.\n", "id": "40733699", "title": "Actin nucleation core"}
{"url": "https://en.wikipedia.org/wiki?curid=656613", "text": "Cytoplasmic streaming\n\nCytoplasmic streaming, also called protoplasmic streaming and cyclosis, is the directed flow of cytosol (the liquid component of the cytoplasm) and organelles around fungal and plant cells. This movement aids in the delivery of organelles, nutrients, metabolites, genetic information, and other materials to all parts of the cell. Cytoplasmic streaming occurs as myosin-coated organelles move along actin filaments in the cytoskeleton of the cell, causing the cytosol to move as well. Cytoplasmic streaming was first discovered in the 1830s.\n\nThe green alga genus \"Chara\" and other genera in the Division Charophyta, such as \"Coleochaete\", are thought to be the closest relatives of land plants. These haploid organisms contain some of the largest plant cells on earth, a single cell of which can reach up to 10 cm in length. The large size of these cells demands an efficient means to distribute resources, which is enabled via cytoplasmic streaming.\n\nCytoplasmic streaming is strongly dependent upon intracellular pH and temperature. It has been observed that the effect of temperature on cytoplasmic streaming created linear variance and dependence at different high temperatures in comparison to low temperatures. This process is complicated, with temperature alterations in the system increasing its efficiency, with other factors such as the transport of ions across the membrane being simultaneously affected. This is due to cells homeostasis depending upon active transport which may be affected at some critical temperatures.\n\nIn plant cells, chloroplasts may be moved around with the stream, possibly to a position of optimum light absorption for photosynthesis. The rate of motion is usually affected by light exposure, temperature, and pH levels.\n\nThe optimal pH at which cytoplasmic streaming is highest, is achieved at neutral pH and decreases at both low and high pH.\n\nThe flow of cytoplasm may be stopped by:\n\nWhat is clearly visible in plants cells which exhibit cytoplasmic streaming is the motion of the chloroplasts moving with the cytoplasmic flow. This motion results from fluid being entrained by moving motor molecules of the plant cell. Myosin filaments connect cell organelles to actin filaments. These actin filaments are generally attached to the chloroplasts and/or membranes of plant cells. As the myosin molecules “walk” along the actin filaments dragging the organelles with them, the cytoplasmic fluid becomes entrained and is pushed/pulled along. Cytoplasmic flow rates can range between 1 and 100 micron/sec.\n\n\"Chara corallina\" exhibits cyclic cytoplasmic flow around a large central vacuole. The large central vacuole is one the largest organelles in a plant cell and is generally used for storage. In \"Chara coralina\", cells can grow up to 10 cm long and 1 mm in diameter. The diameter of the vacuole can occupy around 80% of the cell’s diameter. Thus for a 1 mm diameter cell, the vacuole can have a diameter of 0.8 mm, leaving only a path width of about 0.1 mm around the vacuole for cytoplasm to flow. The cytoplasm flows at a rate of 100 microns/sec, the fastest of all known cytoplasmic streaming phenomena.\n\nThe flow of the cytoplasm in the cell of \"Chara corallina\" is belied by the \"barber pole\" movement of the chloroplasts. Two sections of chloroplast flow are observed with the aid of a microscope. These sections are arranged helically along the longitudinal axis of the cell. In one section, the chloroplasts move upward along one band of the helix, while in the other, the chloroplasts move downwardly. The area between these sections are known as indifferent zones. Chloroplasts are never seen to cross these zones, and as a result it was thought that cytoplasmic and vacuolar fluid flow are similarly restricted, but this is not true. First, Kamiya and Kuroda, experimentally determined that cytoplasmic flow rate varies radially within the cell, a phenomenon not clearly depicted by the chloroplast movement. Second, Raymond Goldstein and others developed a mathematical fluid model for the cytoplasmic flow which not only predicts the behavior noted by Kamiya and Kuroda, but predicts the trajectories of cytoplasmic flow through indifferent zones. It should be noted that the Goldstein model ignores the vacuolar membrane, and simply assumes that shear forces are directly translated to the vacuolar fluid from the cytoplasm. The Goldstein model predicts there is net flow toward one of the indifferent zones from the other. This actually is suggested by the flow of the chloroplasts. At one indifferent zone, the section with the chloroplasts moving at a downward angle will be above the chloroplasts moving at an upward angle. This section is known as the minus different zone (IZ-). Here, if each direction is broken into components in the theta (horizontal) and z (vertical) directions, the sum of these components oppose each other in the z direction, and similarly diverges in theta direction. The other indifferent zone has the upwardly angled chloroplast movement on top and is known as the positive indifferent zone (IZ+). Thus, while the z directional components oppose each other again, the theta components now converge. The net effect of the forces is cytoplasmic/vacuolar flow moves from the minus indifferent zone to the positive indifferent zone. As stated, these directional components are suggested by chloroplast movement, but are not obvious. Further, the effect of this cytoplasmic/vacuolar flow from one indifferent zone to the other demonstrates that cytoplasmic particles do cross the indifferent zones even if the chloroplasts at the surface do not. Particles, as they rise in the cell, spiral around in a semicircular manner near the minus indifferent zone, cross one indifferent zone, and end up near a positive indifferent zone. Further experiments on the Characean cells support of the Goldstein model for vacuolar fluid flow. However, due to the vacuolar membrane (which was ignored in the Goldstein model), the cytoplasmic flow follows a different flow pattern. Further, recent experiments have shown that the data collected by Kamiya and Kuroda which suggested a flat velocity profile in the cytoplasm are not fully accurate. Kikuchi worked with \"Nitella flexillis\" cells, and found an exponential relationship between fluid flow velocity and distance from cell membrane. Although this work is not on Characean cells, the flows between \"Nitella flexillis\" and \"Chara coralina\" are visually and structurally similar.\n\nThe Goldstein model predicts enhanced transport (over transport characterized by strictly longitudinal cytoplasmic flow) into the vacuolar cavity due to the complicated flow trajectories arising from the cytoplasmic streaming. Although, a nutrient concentration gradient would result from longitudinally uniform concentrations and flows, the complicated flow trajectories predicted produce a larger concentration gradient across the vacuolar membrane. By Fick’s laws of diffusion, it is known that larger concentration gradients lead to larger diffusive flows. Thus, the unique flow trajectories of the cytoplasmic flow in \"Chara coralina\" lead to enhanced nutrient transport by diffusion into the storage vacuole. This allows for higher concentrations of nutrients inside the vacuole than would be allowed by strictly longitudinal cytoplasmic flows. Goldstein also demonstrated the faster the cytoplasmic flow along these trajectories, the larger the concentration gradient that arises, and the larger diffusive nutrient transport into the storage vacuole that occurs. The enhanced nutrient transport into the vacuole leads to striking differences in growth rate and overall growth size. Experiments have been performed in Arabidopsis Thaliana. Wild type versions of this plant exhibit cytoplasmic streaming due to the entrainment of fluid similar to \"Chara coralina\", only at slower flow rates. One experiment removes the wild type myosin motor molecule from the plant and replaces it with a faster myosin molecule which moves along the actin filaments at 16 microns/sec. In another set of plants, the myosin molecule is replaced with the slower homo sapiens Vb myosin motor molecule. Human myosin Vb only moves at a rate of .19 microns/sec. Resulting cytoplasmic flows rates are 4.3 microns/sec for the wild type and 7.5 microns/sec for the plants implanted with the rapidly moving myosin protein. The plants implanted with human myosin Vb do not exhibit continuous cytoplasmic streaming. The plants are then allowed to grow under similar conditions. Faster cytoplasmic rates produced larger plants with larger and more abundant leaves. This suggests that the enhanced nutrient storage demonstrated by the Goldstein model allows for plants to grow larger and faster.\n\nPhotosynthesis converts light energy into chemical energy in the form of adenosine triphosphate (ATP). This occurs in the chloroplasts of plants cells. Light photons interact with various intermebrane proteins of the cholorplast to accomplish this. However, these proteins can become saturated with photons, making them unable to function until the saturation is alleviated. This is known as the Kautsky effect and is a cause of inefficiency on the ATP production mechanism. Cytoplasmic streaming in \"Chara corallina\", however, enables chloroplasts to move around the stem of the plant. Thus, the chloroplasts move into lighted regions and shaded regions. This intermittent exposure to photons due to cytoplasmic streaming actually increases the photosynthetic efficiency of chloroplasts. Photosynthetic activity is generally assessed using chlorophyll fluorescence analysis.\n\nGravisensing is the ability to sense the gravitational force and react to it. Many plants use gravisensing to direct growth. For example, depending on root orientation, amyloplasts will settle within a plant cell differently. These different settling patterns cause the protein auxin to be distributed differently within the plant. This differences in the distribution pattern direct roots to grow downward or outward. In most plants, gravisensing requires a coordinated multi-cellular effort, but in \"Chara corallina\", once cell detects gravity and responds to it. The barber pole chloroplast motion resulting from cytoplasmic streaming has one flow upward and another downward. The downward motion of the chloroplasts moves a bit faster than the upward flow producing a ratio of speeds of 1.1. This ratio is known as the polar ratio and depends on the force of gravity. This increase in speed is not a direct result of the force of gravity, but an indirect result. Gravity causes the plant protoplast to settle within the cell wall. Thus, the cell membrane is put into tension at the top, and into compression at the bottom. The resulting pressures on the membrane allow for gravisensing which result in the differing speeds of cytoplasmic flow observed in \"Chara coralina\". This gravitational theory of gravisensing is directly opposed to the statolith theory exhibited by the settling of amyloplasts.\n\nCytoplasmic streaming occurs due to the motion of organelles attached to actin filaments via myosin motor proteins. However, in \"Chara corallina\", the organization of actin filaments is highly ordered. Actin is a polar molecule, which means that myosin only moves in one direction along the actin filament. Thus, in \"Chara corallina\", where motion of the chloroplasts and the mysoin molecule follow a barber pole pattern, the actin filaments must all be similarly oriented within each section. In other words, the section where the chloroplasts move upward will have all of the actin filaments oriented in the same upward direction, and the section where the chloroplasts move downward will have all the actin filaments oriented in the downward direction. This organization emerges naturally from basic principles. With basic, realistic assumptions about the actin filament, Woodhouse demonstrated that the formation of two sets of actin filament orientations in a cylindrical cell is likely. His assumptions included a force keeping the actin filament in place once set down, an attractive force between filaments leading them to be more likely align as a filament already in place, and a repulsive force preventing alignment perpendicular to the length of the cylindrical cell. The first two assumptions derive from the molecular forces within the actin filament, while the last assumption was made due to the actin molecule’s dislike of curvature. Computer simulations run with these assumptions with varying parameters for the assumptive forces almost always leads to highly ordered actin organizations. However, no order was as organized and consistent as the barber pole pattern found in nature, which suggests this mechanism plays role, but is not wholly responsible for the organization of actin filaments in \"Chara corallina\".\n\nCytoplasmic streaming in some species is caused by pressure gradients along the length of the cell.\n\n\"Physarum polycephalum\" is a parasitic protozoan slime mold. Biological investigations into the myosin and actin molecules in this mold have demonstrated striking physical and mechanistic similarities to human muscle myosin and actin molecules. Contraction and relaxation of these molecules leads to pressure gradients along the length of the mold. These contractions force cytoplasmic fluid in one direction from cell to cell and contributes to growth. It has been demonstrated that while the molecules are similar to those in humans, the molecule blocking the binding site of myosin to actin is different. While, in humans, tropomyosin covers the site, only allowing contraction when calcium ions are present, in this mold, a different molecule known as calmodulin blocks the site, allowing relaxation in the presence of high calcium ion levels.\n\n\"Neurospora Crassa\" is a multicellular fungus with many off shooting hyphae. Cells can be up to 10 cm long, and are separated by a small septum. Small holes in the septum allow cytoplasm and cytoplasmic contents to flow from cell to cell. Osmotic pressure gradients occur through the length of the cell to drive this cytoplasmic flow. Flows contribute to growth and the formation of cellular subcompartments.\n\nCytoplasmic flows created through osmotic pressure gradients flow longitudinally along the fungal hyphae and crash into the end causing growth. It has been demonstrated that the greater pressure at the hyphal tip corresponds to faster growth rates. Longer hyphae have greater pressure differences along their length allowing for faster cytoplasmic flow rates and larger pressures at the hyphal tip. This is why longer hyphae grow faster than shorter ones. Tip growth increases as cytoplasmic flow rate increases over a 24 hour period until a max rate of 1 micron/second growth rate is observed. Off shoots from the main hyphae are shorter and have slower cytoplasmic flow rates and correspondingly slower growth rates.\n\nCytoplasmic flow in \"Neurospora Crassa\" carry microtubules. The presence of microtubules create interesting aspects to the flow. Modelling the fungal cells as a pipe separated at regular points with a septum with a hole in the center should produce very symmetrical flow. Basic fluid mechanics suggest that eddies should form both before and after each septum. However, eddies only form before the septum in \"Neurospora Crassa\". This is because when microtubules enter the septal hole, they are arranged parallel to flow and contribute very little to flow characteristics, however, as the exit the septal hole, the orient themselves perpendicular to flow, slowing acceleration, and preventing eddy formation. The eddies formed just before the septum allow for the formation of subcompartments where nuclei spotted with special proteins aggregate. These proteins, one of which is called SPA-19, contribute to septum maintenance. Without it, the septum would degrade and the cell would leak large amounts of cytoplasm into the neighboring cell leading to cell death.\n\nIn many animal cells, centrioles and spindles keep nuclei centered within a cell for mitotic, meiotic, and other processes. Without such a centering mechanism, disease and death can result. While mouse oocytes do have centrioles, they play no role in nucleus positioning, yet, the nucleus of the oocyte maintains a central position. This is a result of cytoplasmic streaming. Microfilaments, independent of microtubules and myosin 2, form a mesh network throughout the cell. Nuclei, positioned in non-centered cell locations, have been demonstrated to migrate distances greater than 25 microns to the cell center. They will do this without going off course by more than 6 microns when the network is present. This network of microfilaments has organelles bound to it by the myosin Vb molecule. Cytoplasmic fluid is entrained by the motion of these organelles, however, no pattern of directionality is associated with the movement of the cytoplasm. In fact, the motion has been demonstrated to fulfill Brownian motion characteristics. For this reason, there is some debate as to whether this should be called cytoplasmic streaming. Nonetheless, directional movement of organelles does result from this situation. Since the cytoplasm fills the cell, it is geometrically arranged into the shape of a sphere. As the radius of a sphere increases, surface area increases. Further, the motion in any given direction is proportional to the surface area. So thinking of the cell as a series of concentric spheres, it is clear that spheres with larger radii produce a greater amount of movement than spheres with smaller radii. Thus, the movement toward the center is greater than the movement away from the center, and net movement pushing the nucleus towards a central cellular location exists. In other words, the random motion of the cytoplasmic particles create a net force toward the center of the cell. Additionally, the increased motion with the cytoplasm reduces cytoplasmic viscosity allowing the nucleus to move more easily within the cell. These two factors of the cytoplasmic streaming center the nucleus in the oocyte cell.\n\n\n\n", "id": "656613", "title": "Cytoplasmic streaming"}
{"url": "https://en.wikipedia.org/wiki?curid=10116", "text": "Endocytosis\n\nEndocytosis is a form of active transport in which a cell transports molecules (such as proteins) into the cell (\"endo-\" + \"cytosis\") by engulfing them in an energy-using process. Endocytosis and its counterpart, exocytosis, are used by all cells because most chemical substances important to them are large polar molecules that cannot pass through the hydrophobic plasma or cell membrane by passive means.\n\nEndocytosis includes pinocytosis (cell drinking) and phagocytosis (cell eating).\n\nThe term was proposed by De Duve in 1963. Phagocytosis was discovered by Élie Metchnikoff in 1882.\n\nEndocytosis pathways can be subdivided into four categories: namely, receptor-mediated endocytosis (also known as clathrin-mediated endocytosis), caveolae, macropinocytosis, and phagocytosis.\n\nStudy by \nin mammalian cells confirm a reduction in clathrin coat size in an increased tension environment. In addition, it suggests that the two apparently distinct clathrin assembly modes, namely coated pits and coated plaques, observed in experimental investigations might be a consequence of varied tensions in the plasma membrane.\n\n\n\nMore recent experiments have suggested that these morphological descriptions of endocytic events may be inadequate, and a more appropriate method of classification may be based upon the clathrin-dependence of particular pathways, with multiple subtypes of clathrin-dependent and clathrin-independent endocytosis. Mechanistic insight into non-phagocytic, clathrin-independent endocytosis has been lacking, but a recent study has shown how Graf1 regulates a highly prevalent clathrin-independent endocytic pathway known as the CLIC/GEEC pathway.\n\nThe endocytic pathway of mammalian cells consists of distinct membrane compartments, which internalize molecules from the plasma membrane and recycle them back to the surface (as in early endosomes and recycling endosomes), or sort them to degradation (as in late endosomes and lysosomes). The principal components of the endocytic pathway are:\n\nIt was recently found that an eisosome serves as a portal of endocytosis in yeast.\n\nThe major route for endocytosis in most cells, and the best-understood, is that mediated by the molecule clathrin. This large protein assists in the formation of a coated pit on the inner surface of the plasma membrane of the cell. This pit then buds into the cell to form a coated vesicle in the cytoplasm of the cell. In so doing, it brings into the cell not only a small area of the surface of the cell but also a small volume of fluid from outside the cell.\n\nCoats function to deform the donor membrane to produce a vesicle, and they also function in the selection of the vesicle cargo. Coat complexes that have been well characterized so far include coat protein-I (COP-I), COP-II, and clathrin. Clathrin coats are involved in two crucial transport steps: (i) receptor-mediated and fluid-phase endocytosis from the plasma membrane to early endosome and (ii) transport from the TGN to endosomes. In endocytosis, the clathrin coat is assembled on the cytoplasmic face of the plasma membrane, forming pits that invaginate to pinch off (scission) and become free CCVs. In cultured cells, the assembly of a CCV takes ~ 1min, and several hundred to a thousand or more can form every minute. The main scaffold component of clathrin coat is the 190-kD protein called clathrin heavy chain (CHC), which is associated with a 25- kD protein called clathrin light chain (CLC), forming three-legged trimers called triskelions.\n\nVesicles selectively concentrate and exclude certain proteins during formation and are not representative of the membrane as a whole. AP2 adaptors are multisubunit complexes that perform this function at the plasma membrane. The best-understood receptors that are found concentrated in coated vesicles of mammalian cells are the LDL receptor (which removes LDL from circulating blood), the transferrin receptor (which brings ferric ions bound by transferrin into the cell) and certain hormone receptors (such as that for EGF).\n\nAt any one moment, about 25% of the plasma membrane of a fibroblast is made up of coated pits. As a coated pit has a life of about a minute before it buds into the cell, a fibroblast takes up its surface by this route about once every 16 minutes. Coated vesicles formed from the plasma membrane have a diameter of about 36 nm and a lifetime measured in a few seconds. Once the coat has been shed, the remaining vesicle fuses with endosomes and proceeds down the endocytic pathway. The actual budding-in process, whereby a pit is converted to a vesicle, is carried out by clathrin assisted by a set of cytoplasmic proteins, which includes dynamin and adaptors such as adaptin.\n\nCoated pits and vesicles were first seen in thin sections of tissue in the electron microscope by Matt Lions and Parker George. The importance of them for the clearance of LDL from blood was discovered by Richard G. Anderson, Michael S. Brown and Joseph L. Goldstein in 1977. Coated vesicles were first purified by Barbara Pearse, who discovered the clathrin coat molecule in 1976.\n\n\n", "id": "10116", "title": "Endocytosis"}
{"url": "https://en.wikipedia.org/wiki?curid=206508", "text": "Phagocytosis\n\nIn cell biology, phagocytosis () is the process by which a cell—often a phagocyte or a protist—engulfs a solid particle to form an internal compartment known as a phagosome. It is distinct from other forms of endocytosis like pinocytosis that involves the internalization of extracellular liquids. Phagocytosis is involved in the acquisition of nutrients for some cells. The process is homologous to eating at the level of single-celled organisms; in multicellular animals, the process has been adapted to eliminate debris and pathogens, as opposed to taking in fuel for cellular processes, except in the case of the animal \"Trichoplax\".\n\nIn an organism's immune system, phagocytosis is a major mechanism used to remove pathogens and cell debris. For example, when a macrophage ingests a pathogenic microorganism, the pathogen becomes trapped in a phagosome which then fuses with a lysosome to form a phagolysosome. Within the phagolysosome, enzymes and toxic peroxides digest the pathogen. Bacteria, dead tissue cells, and small mineral particles are all examples of objects that may be phagocytized.\n\nPhagocytosis was first noted by Canadian physician William Osler (1876), and later studied and named by Élie Metchnikoff (1880, 1883).\n\nPhagocytosis in mammalian immune cells is activated by attachment to pathogen-associated molecular patterns (PAMPS), which leads to NF-κB activation. Opsonins such as C3b and antibodies can act as attachment sites and aid phagocytosis of pathogens.\n\nEngulfment of material is facilitated by the actin-myosin contractile system. The phagosome of ingested material is then fused with the lysosome, forming a phagolysosome and leading to degradation.\n\nDegradation can be oxygen-dependent or oxygen-independent.\n\nIt is possible for cells other than dedicated phagocytes (such as dendritic cells) to engage in phagocytosis.\nSome white blood cells in human immune system perform phagocytosis by gulping in some pathogenic and disease causing cells.\n\nLeukocytes generate hydrogen cyanide during phagocytosis, and can kill bacteria, fungi, and other pathogens by generating several toxic chemicals, one of which is hydrogen cyanide.\n\nFollowing apoptosis, the dying cells need to be taken up into the surrounding tissues by macrophages in a process called efferocytosis. One of the features of an apoptotic cell is the presentation of a variety of intracellular molecules on the cell surface, such as calreticulin, phosphatidylserine (from the inner layer of the plasma membrane), annexin A1, oxidised LDL and altered glycans. These molecules are recognised by receptors on the cell surface of the macrophage such as the phosphatidylserine receptor or by soluble (free-floating) receptors such as thrombospondin 1, GAS6, and MFGE8, which themselves then bind to other receptors on the macrophage such as CD36 and alpha-v beta-3 integrin. Defects in apoptotic cell clearance is usually associated with impaired phagocytosis of macrophages. Accumulation of apoptotic cell remnants often causes autoimmune disorders; thus pharmacological potentiation of phagocytosis has a medical potential in treatment of certain forms of autoimmune disorders.\n\nIn many protists, phagocytosis is used as a means of feeding, providing part or all of their nourishment. This is called phagotrophic nutrition, distinguished from osmotrophic nutrition which takes place by absorption.\n\n\nAs in phagocytic immune cells, the resulting phagosome may be merged with lysosomes containing digestive enzymes, forming a phagolysosome. The food particles will then be digested, and the released nutrients are diffused or transported into the cytosol for use in other metabolic processes.\n\nMixotrophy can involve phagotrophic nutrition and phototrophic nutrition.\n\n", "id": "206508", "title": "Phagocytosis"}
{"url": "https://en.wikipedia.org/wiki?curid=42026572", "text": "Microautophagy\n\nMicroautophagy (unlike Macroautophagy and Chaperone-mediated autophagy) is the type of autophagic pathway which is mediated by direct lysosomal (mammals) or vacuolar (plants and fungi) engulfment of the cytoplasmic cargo. Cytoplasmic material is trapped in the lysosome/vacuole by the random process of membrane invagination. Microautophagic pathway is especially important for survival of cells under starvation, nitrogen deprivation or after treatment with rapamycin. \nMicroautophagy generally is a non-selective process but there are 3 special cases of selective microautophagic pathway (micropexophagy, Piecemeal microautophagy of the nucleus, micromitophagy) which are activated under a specific conditions.\n\nMicroautophagy together with macroautophagy is necessary for nutrient recycling under starvation. Microautophagy due to degradation of lipids incorporated into vesicles regulates the composition of lysosomal/vacuolar membrane. \nMicroautophagic pathway functions also as one of the mechanism of glycogen delivery into the lysosomes. \nThis autophagic pathway engulfs multivesicular bodies formed after endocytosis therefore it plays role in membrane proteins turnover. \nMicroautophagy is also connected with organellar size maintenance, composition of biological membranes, cell survival under nitrogen restriction, and the transition pathway from starvation-induced growth arrest to logarithmic growth.\n\nNon-selective microautophagic process can be dissected into 5 distinct steps. Majority of experiments were done on yeast (vacuolar invaginations) but the molecular principles seem to be more general \n\nInvagination is a constitutive process but its frequency is dramatically increased during periods of starvation. Invagination is a tubular process by which is formed the autophagic tube.\n\nFormation of the autophagic tubes is mediated through Atg7-dependent ubiquitin-like conjugation (Ublc) or via vacuolar transporter chaperone (VTC) molecular complex which acts through calmodulin-dependent manner. Interestingly, calmodulin involvement in tube formation is calcium independent process.\n\nThe mechanism of vesicle formation is based on lateral sorting mechanism. Changed composition of membrane molecules (lipid enrichment in the autophagic tubes due to removal of transmembrane proteins) leads spontaneous vesicle formation via phase separation mechanism.\n\nThe process of microautophagic vesicle formation is similar to multivesicular bodies formation process \n\nEnlargement of vesicle is mediated by binding enzymes inside of unclosed vesicle. Basically, this process is reversal to endocytosis. Process follows by pich of the vesicle into the lysosomal/vacuolar lumen. This process is independent on SNARE proteins.\n\nVesicle moves freely in the lumen and during the time is degraded by hydrolases (ec. Atg15p). Nutrients are then released by Atg22p.\n\nProcess of non-selective microautophagy can be observed in all types of eucaryotic cells. On the other hand, selective microautophagy is commonly observed in yeast cells.\nThree types of selective microautophagy selective microautophagy can be distinguished: micropexophagy, piecemeal microautophagy of the nucleus and micromitophagy \n", "id": "42026572", "title": "Microautophagy"}
{"url": "https://en.wikipedia.org/wiki?curid=42255617", "text": "Phagoptosis\n\nPhagoptosis is a type of cell death caused by the cell being phagocytosed (i.e. eaten) by another cell, and therefore this form of cell death is prevented by blocking phagocytosis.\n\nPhagocytosis of an otherwise-viable cell may occur because the cell is recognised as stressed, activated, senescent, damaged, pathogenic or non-self, or is misrecognised. Cells are phagocytosed as a result of: i) expressing eat-me signals on their surface, ii) losing don’t-eat-me signals, and/or iii) binding of opsonins. It is clear that otherwise-viable cells can expose/bind such phagocytosis-promoting signals as a result of cell stress, activation or senescence. Phagoptosis is probably the most common form of cell death in the body as it is responsible for erythrocyte turnover. And there is increasing evidence that it mediates physiological death of neutrophils, T cells, platelets and stem cells, and thereby regulates inflammation, immunity, clotting and neurogenesis. Phagoptosis is a major form of host defence against pathogens and cancer cells. However, recent evidence indicates that excessive phagoptosis may kill host cells in inflammatory conditions, contributing to haemophagic conditions, and neuronal loss in the inflamed brain.\n\nPhagoptosis is normally caused by: the cell exposing on its surface so-called \"eat-me\" signals, and/or the cell no longer exposing \"don't-eat-me\" signals and/or the cell being opsonised i.e. binding soluble proteins that tag the cell for phagocytosis. For example, phosphatidylserine is an \"eat-me\" signal that, when exposed on the surface of a cell, triggers phagocytes (i.e. cells that eat other cells) to eat that cell. Phosphatidylserine is normally found on the inside of healthy cells, but can become exposed on the surface of dying, activated or stressed cells. Phagocytosis of such cells requires specific receptors on the phagocyte that recognise either phosphatidylserine directly or opsonins bound to the phosphatidylserine or other \"eat-me\" signals, such as calreticulin. \"Don't-eat-me\" signals include CD47, which when expressed on the surface of a cell, inhibit phagocytosis of that cell, by activating SIRP-alpha receptors on the phagocyte. Opsonins are normally soluble proteins, which when bound to the surface of a cell induce phagocytes to phagocytose that cell. Opsonins include Mfge8, Gas6, Protein S, antibodies and complement factors C1q and C3b.\n\nPhagoptosis has multiple functions including removal and disposal of: pathogenic cells, aged cells, damaged cells, stressed cells and activated cells. Pathogenic cells such as bacteria can be opsonised by antibodies or complement factors, enabling their phagocytosis and phagoptosis by macrophages and neutrophils. \"Aged\" erythrocytes and neutrophils, as well as \"activated\" platelets, neutrophils and T-cells, are thought to be phagocytosed alive by macrophages.\n\nDevelopment. Phagoptosis removes excess cells during development in C. elegant. During mammalian development multiple cells undergo programmed cell senescence and are then phagocytosed by macrophages. Brain macrophages (microglia) can regulate the number of neural precursor cells in the developing brain by phagocytosing these otherwise viable precursors and thus limiting neurogenesis.\n\nTurnover of blood cells. Red blood cells (erythrocytes) live for roughly 3 months in the blood before being phagocytksed by macrophages. Old erythrocytes do not die, but rather display changes in the cell surface that enable macrophages to recognise them as old or damaged, including exposure of phosphatidylserine, desialylation of glycoproteins, loss or changed conformation of the \"don't-eat-me\" signal CD47, and exposure of novel antigens that bind endogenous antibodies. Neutrophils have a daily rhythm of entry and exit from the blood, driven by neutrophil “aging” in the circulation, causing decreased expression of CD62L and increased expression of CXCR4, which directs the “aged” neutrophils to the bone marrow, where they are phagocytosed by macrophages. However, it is still unclear how or why neutrophils turnover at such an enormous rate. Antigen recognition causes phosphatidylserine exposure on activated T-cells, which is recognized by Tim-4 on macrophages, inducing phagoptosis of the activated T-cells, and thus the contraction phase of the adaptive response.\n\nHost defence against pathogens. Phagocytosis of otherwise-viable pathogens, such as bacteria, can be mediated by neutrophils, monocytes, macrophages, microglia and dendritic cells, and is central to host defence against pathogens. Dendritic cells can phagocytose viable neutrophils, and present antigens derived from bacteria or cancer cell debris previously phagocytosed by the neutrophils. Thus phagoptosis can contribute to host defence in a variety of ways.\n\nHost defence against cancer. It has been known for some time that animals defend themselves against cancer by antibody-mediated or antibody-independent phagocytosis of viable tumour cells by macrophages. Recognition of viable cancer cells for phagocytosis may be based on the expression of novel antigens, senescence markers, phosphatidylserine or calreticulin. More recently it has become clear that most human cancer cells overexpress CD47 on their surface to prevent themselves being phagocytosed, and that if this ‘don’t-eat-me’ signalling is blocked then a variety of cancers can be cleared from the body. Thus it would appear that phagoptosis is an important defence against cancer, but that tumour cells can suppress this, and blocking this suppression is an attractive therapeutic option.\n\nPathological phagoptosis of blood cells. Hemophagocytosis is a clinical condition, found in many infectious and inflammatory disorders, where activated macrophages have engulfed apparently viable blood cells, resulting in reduced white or red cell count (cytopenia). IFN-γ (and possibly other cytokines) appears to drive hemophagocytosis during infection by directly stimulating phagoptosis of blood cells by macrophages. Hemophagocytic lymphohistiocytosis (HLH) is characterized by excessive engulfment of hematopoietic stem cells (HSCs) by bone marrow macrophages, and this has been found to result from down regulation of CD47 expression on HSCs, enabling macrophages to eat them alive.\n\nPathological phagoptosis in the brain. Microglial phagocytosis of stressed-but-viable neurons occurs under inflammatory conditions, and may contribute to neuronal loss in brain pathologies [2].\n", "id": "42255617", "title": "Phagoptosis"}
{"url": "https://en.wikipedia.org/wiki?curid=158005", "text": "Genetic recombination\n\nDuring meiosis in eukaryotes, genetic recombination involves the pairing of homologous chromosomes. This may be followed by information transfer between the chromosomes. The information transfer may occur without physical exchange (a section of genetic material is copied from one chromosome to another, without the donating chromosome being changed) (see SDSA pathway in Figure); or by the breaking and rejoining of DNA strands, which forms new molecules of DNA (see DHJ pathway in Figure).\n\nRecombination may also occur during mitosis in eukaryotes where it ordinarily involves the two sister chromosomes formed after chromosomal replication. In this case, new combinations of alleles are not produced since the sister chromosomes are usually identical. In meiosis and mitosis, recombination occurs between similar molecules of DNA (homologs). In meiosis, non-sister homologous chromosomes pair with each other so that recombination characteristically occurs between non-sister homologues. In both meiotic and mitotic cells, recombination between homologous chromosomes is a common mechanism used in DNA repair.\n\nGenetic recombination and recombinational DNA repair also occurs in bacteria and archaea, which use asexual reproduction.\n\nRecombination can be artificially induced in laboratory (in vitro) settings, producing recombinant DNA for purposes including vaccine development.\n\nV(D)J recombination in organisms with an adaptive immune system is a type of site-specific genetic recombination that helps immune cells rapidly diversify to recognize and adapt to new pathogens.\n\nDuring meiosis, synapsis (the pairing of homologous chromosomes) ordinarily precedes genetic recombination.\n\nGenetic recombination is catalyzed by many different enzymes. Recombinases are key enzymes that catalyse the strand transfer step during recombination. RecA, the chief recombinase found in \"Escherichia coli\", is responsible for the repair of DNA double strand breaks (DSBs). In yeast and other eukaryotic organisms there are two recombinases required for repairing DSBs. The RAD51 protein is required for mitotic and meiotic recombination, whereas the DNA repair protein, DMC1, is specific to meiotic recombination. In the archaea, the ortholog of the bacterial RecA protein is RadA.\n\n\nIn Bacteria there are:\n\nIn eukaryotes, recombination during meiosis is facilitated by chromosomal crossover. The crossover process leads to offspring having different combinations of genes from those of their parents, and can occasionally produce new chimeric alleles. The shuffling of genes brought about by genetic recombination produces increased genetic variation. It also allows sexually reproducing organisms to avoid Muller's ratchet, in which the genomes of an asexual population accumulate genetic deletions in an irreversible manner.\n\nChromosomal crossover involves recombination between the paired chromosomes inherited from each of one's parents, generally occurring during meiosis. During prophase I (pachytene stage) the four available chromatids are in tight formation with one another. While in this formation, homologous sites on two chromatids can closely pair with one another, and may exchange genetic information.\n\nBecause recombination can occur with small probability at any location along chromosome, the frequency of recombination between two locations depends on the distance separating them. Therefore, for genes sufficiently distant on the same chromosome, the amount of crossover is high enough to destroy the correlation between alleles.\n\nTracking the movement of genes resulting from crossovers has proven quite useful to geneticists. Because two genes that are close together are less likely to become separated than genes that are farther apart, geneticists can deduce roughly how far apart two genes are on a chromosome if they know the frequency of the crossovers. Geneticists can also use this method to infer the presence of certain genes. Genes that typically stay together during recombination are said to be linked. One gene in a linked pair can sometimes be used as a marker to deduce the presence of another gene. This is typically used in order to detect the presence of a disease-causing gene.\n\nThe recombination frequency between two loci observed is the \"crossing-over value\". It is the frequency of crossing over between two linked gene loci (markers), and depends on the mutual distance of the genetic loci observed. For any fixed set of genetic and environmental conditions, recombination in a particular region of a linkage structure (chromosome) tends to be constant, and the same is then true for the crossing-over value which is used in the production of genetic maps.\n\nIn gene conversion, a section of genetic material is copied from one chromosome to another, without the donating chromosome being changed. Gene conversion occurs at high frequency at the actual site of the recombination event during meiosis. It is a process by which a DNA sequence is copied from one DNA helix (which remains unchanged) to another DNA helix, whose sequence is altered. Gene conversion has often been studied in fungal crosses where the 4 products of individual meioses can be conveniently observed. Gene conversion events can be distinguished as deviations in an individual meiosis from the normal 2:2 segregation pattern (e.g. a 3:1 pattern).\n\nRecombination can occur between DNA sequences that contain no sequence homology. This can cause chromosomal translocations, sometimes leading to cancer.\n\nB cells of the immune system perform genetic recombination, called immunoglobulin class switching. It is a biological mechanism that changes an antibody from one class to another, for example, from an isotype called IgM to an isotype called IgG.\n\nIn genetic engineering, recombination can also refer to artificial and deliberate recombination of disparate pieces of DNA, often from different organisms, creating what is called recombinant DNA. A prime example of such a use of genetic recombination is gene targeting, which can be used to add, delete or otherwise change an organism's genes. This technique is important to biomedical researchers as it allows them to study the effects of specific genes. Techniques based on genetic recombination are also applied in protein engineering to develop new proteins of biological interest.\n\nDuring both mitosis and meiosis, DNA damages caused by a variety of exogenous agents (e.g. UV light, X-rays, chemical cross-linking agents) can be repaired by homologous recombinational repair (HRR). These findings suggest that DNA damages arising from natural processes, such as exposure to reactive oxygen species that are byproducts of normal metabolism, are also repaired by HRR. In humans and rodents, deficiencies in the gene products necessary for HRR during meiosis cause infertility. In humans, deficiencies in gene products necessary for HRR, such as BRCA1 and BRCA2, increase the risk of cancer (see DNA repair-deficiency disorder).\n\nIn bacteria, transformation is a process of gene transfer that ordinarily occurs between individual cells of the same bacterial species. Transformation involves integration of donor DNA into the recipient chromosome by recombination. This process appears to be an adaptation for repairing DNA damages in the recipient chromosome by HRR. Transformation may provide a benefit to pathogenic bacteria by allowing repair of DNA damage, particularly damages that occur in the inflammatory, oxidizing environment associated with infection of a host.\n\nWhen two or more viruses, each containing lethal genomic damages, infect the same host cell, the virus genomes can often pair with each other and undergo HRR to produce viable progeny. This process, referred to as multiplicity reactivation, has been studied in lambda and T4 bacteriophages, as well as in several pathogenic viruses. In the case of pathogenic viruses, multiplicity reactivation may be an adaptive benefit to the virus since it allows the repair of DNA damages caused by exposure to the oxidizing environment produced during host infection.\n\nMolecular models of meiotic recombination have evolved over the years as relevant evidence accumulated. A major incentive for developing a fundamental understanding of the mechanism of meiotic recombination is that such understanding is crucial for solving the problem of the adaptive function of sex, a major unresolved issue in biology. A recent model that reflects current understanding was presented by Anderson and Sekelsky, and is outlined in the first figure in this article. The figure shows that two of the four chromatids present early in meiosis (prophase I) are paired with each other and able to interact. Recombination, in this version of the model, is initiated by a double-strand break (or gap) shown in the DNA molecule (chromatid) at the top of the first figure in this article. However, other types of DNA damage may also initiate recombination. For instance, an inter-strand cross-link (caused by exposure to a cross-linking agent such as mitomycin C) can be repaired by HRR.\n\nAs indicated in the first figure, above, two types of recombinant product are produced. Indicated on the right side is a “crossover” (CO) type, where the flanking regions of the chromosomes are exchanged, and on the left side, a “non-crossover” (NCO) type where the flanking regions are not exchanged. The CO type of recombination involves the intermediate formation of two “Holliday junctions” indicated in the lower right of the figure by two X shaped structures in each of which there is an exchange of single strands between the two participating chromatids. This pathway is labeled in the figure as the DHJ (double-Holliday junction) pathway.\n\nThe NCO recombinants (illustrated on the left in the figure) are produced by a process referred to as “synthesis dependent strand annealing” (SDSA). Recombination events of the NCO/SDSA type appear to be more common than the CO/DHJ type. The NCO/SDSA pathway contributes little to genetic variation, since the arms of the chromosomes flanking the recombination event remain in the parental configuration. Thus, explanations for the adaptive function of meiosis that focus exclusively on crossing-over are inadequate to explain the majority of recombination events.\n\nAchiasmy is the phenomenon where autosomal recombination is completely absent in one sex of a species. Achiasmatic chromosomal segregation is well documented in male \"Drosophila melanogaster\". Heterochiasmy is the term used to describe recombination rates which differ between the sexes of a species. This sexual dimorphic pattern in recombination rate has been observed in many species. In mammals, females most often have higher rates of recombination. The \"Haldane-Huxley rule\" states that achiasmy usually occurs in the heterogametic sex.\n\n\n", "id": "158005", "title": "Genetic recombination"}
{"url": "https://en.wikipedia.org/wiki?curid=4961951", "text": "Transcytosis\n\nTranscytosis is a type of transcellular transport in which various macromolecules are transported across the interior of a cell. Macromolecules are captured in vesicles on one side of the cell, drawn across the cell, and ejected on the other side. Examples of macromolecules transported include IgA, transferrin, and insulin. While transcytosis is most commonly observed in cells of an epithelium, the process is also present elsewhere. Blood capillaries are a well-known site for transcytosis, though it occurs in other cells, including neurons, osteoclasts and M cells of the intestine.\n\nThe regulation of transcytosis varies greatly due to the many different tissues in which this process is observed. Various tissue specific mechanisms of transcytosis have been identified. Brefeldin A, a commonly used inhibitor of ER to Golgi apparatus transport, has been shown to inhibit transcytosis in dog kidney cells which provided the first clues as to the nature of transcytosis regulation. Transcytosis in dog kidney cells has also been shown be regulated at the apical membrane by Rab17, as well as Rab11a and Rab25. Further work on dog kidney cells has shown that a signaling cascade involving the phosphorylation of EGFR by Yes leading to the activation of Rab11FIP5 by MAPK1 upregulates transcytosis. Transcytosis has been shown to be inhibited by the combination of progesterone and estradiol followed by activation mediated by prolactin in the rabbit mammary gland during pregnancy. In the thyroid, follicular cell transcytosis is regulated positively by TSH. The phosphorylation of caveolin 1 induced by hydrogen peroxide has been shown to be critical to the activation of transcytosis in pulmonary vascular tissue. It can therefore be concluded that the regulation of transcytosis is a complex process that varies between tissues.\n\nDue to the function of transcytosis as a process that transports macromolecules across cells, it can be a convenient mechanism by which pathogens can invade a tissue. Transcytosis has been shown to be critical to the entry of \"Cronobacter sakazakii\" across the intestinal epithelium as well as the blood–brain barrier. \"Listeria monocytogenes\" has been shown to enter the intestinal lumen via transcytosis across goblet cells. Shiga toxin secreted by enterohemorrhagic \"E. coli\" has been shown to be transcytosed into the intestinal lumen. From these examples, it can be said that transcytosis is vital to the process of pathogenesis for a variety of infectious agents.\n\nPharmaceutical companies, such as Lundbeck, are currently exploring the use of transcytosis as a mechanism for transporting therapeutic drugs across the human blood–brain barrier (BBB). Exploiting the body’s own transport mechanism can help to overcome the high selectivity of the BBB which typically blocks the uptake of most therapeutic antibodies into the brain and Central Nervous System (CNS). The pharmaceutical company Genentech, after having synthesized a therapeutic antibody that effectively inhibited BACE1 enzymatic function, experienced problems transferring adequate, efficient levels of the antibody within the brain. BACE1 is the enzyme which processes amyloid precursor proteins into amyloid-β peptides, including the species that aggregate to form amyloid plaques associated with Alzheimer's disease.\n\nMolecules are transported across an epithelial or endothelial barrier by one of two routes: 1) a trans-cellular route through the intra-cellular compartment of the cell, or 2) a para-cellular route through the extra-cellular space between adjacent cells. The trans-cellular route is also called transcytosis. Transcytosis can be receptor-mediated and consists of three steps: 1) receptor-mediated endocytosis of the molecule on one side of the cell, e.g. the luminal side; 2) movement of the molecule through the intracellular compartment typically within the endosomal system; and 3) exocytosis of the molecule to the extracellular space on the other side of the cell, e.g. the abluminal side.\n\nTranscytosis may be either unidirectional or bidirectional. Unidirectional transcytosis may occur selectively in the luminal to abluminal direction, or in the reverse direction, in the abluminal to luminal direction.\n\nTranscytosis is prominent in brain microvascular peptide and protein transport, because the brain microvascular endothelium, which forms the blood-brain barrier (BBB) in vivo, expresses unique, epithelial-like, high resistance tight junctions. The brain endothelial tight junctions virtually eliminate the para-cellular pathway of solute transport across the microvascular endothelial wall in brain. In contrast, the endothelial barrier in peripheral organs does not express tight junctions, and solute movement through the para-cellular pathway is prominent at the endothelial barrier in organs other than the brain or spinal cord.\n\nReceptor-mediated transcytosis, or RMT, across the BBB is a potential pathway for drug delivery to the brain, particularly for biologic drugs such as recombinant proteins. The non-transportable drug, or therapeutic protein, is genetically fused to a transporter protein. The transporter protein may be an endogenous peptide, or peptidomimetic monoclonal antibody, which undergoes RMT across the BBB via transport on brain endothelial receptors such as the insulin receptor or transferrin receptor. The transporter protein acts as a molecular Trojan horse to ferry into brain the therapeutic protein that is genetically fused to the receptor-specific Trojan horse protein.\n\nMonoclonal antibody Trojan horses that target the BBB insulin or transferrin receptor have been in drug development for over 10 years at ArmaGen, Inc., a biotechnology company in Los Angeles. ArmaGen has developed genetically engineered antibodies against both the insulin and transferrin receptors, and has fused to these antibodies different therapeutic proteins, including lysosomal enzymes, therapeutic antibodies, decoy receptors, and neurotrophins. These therapeutic proteins alone do not cross the BBB, but following genetic fusion to the Trojan horse antibody, the therapeutic protein penetrates the BBB at a rate comparable to small molecules. In 2015, ArmaGen will be the first to enter human clinical trials with the BBB Trojan horse fusion proteins that delivery protein drugs to the brain via the transcytosis pathway. The human diseases initially targeted by ArmaGen are lysosomal storage diseases that adversely affect the brain. Inherited diseases create a condition where a specific lysosomal enzyme is not produced, leading to serious brain conditions including mental retardation, behavioral problems, and then dementia. Although the missing enzyme can be manufactured by drug companies, the enzyme drug alone does not treat the brain, because the enzyme alone does not cross the BBB. ArmaGen has re-engineered the missing lysosomal enzyme as a Trojan horse-enzyme fusion protein that crosses the BBB. The first clinical trials of the new Trojan horse fusion protein technology will treat the brain in lysosomal storage disorders, including Mucopolysaccharidosis (MPS) Type I, also called Hurlers syndrome, and MPS Type II, also called Hunter syndrome.\n\nResearchers at Genentech proposed the creation of a bispecific antibody that could bind the BBB membrane, induce receptor-mediated transcytosis, and release itself on the other side into the brain and CNS. They utilized a mouse bispecific antibody with two active sites performing different functions. One arm had a low-affinity anti-transferrin receptor binding site that induces transcytosis. A high-affinity binding site would result in the antibody not being able to release from the BBB membrane after transcytosis. This way the amount of transported antibody is based on the concentration of antibody on either side of the barrier. The other arm had the previously developed high-affinity anti-BACE1 binding site that would inhibit BACE1 function and prevent amyloid plaque formation. Genentech was able to demonstrate in mouse models that the new bispecific antibody was able to reach therapeutic levels in the brain. Genentech’s method of disguising and transporting the therapeutic antibody by attaching it to a receptor-mediated transcytosis activator has been referred to as the \"Trojan Horse\" method.\n\n", "id": "4961951", "title": "Transcytosis"}
{"url": "https://en.wikipedia.org/wiki?curid=1067082", "text": "Cell death\n\nCell death is the event of a biological cell ceasing to carry out its functions. This may be the result of the natural process of old cells dying and being replaced by new ones, or may result from such factors as disease, localized injury, or the death of the organism of which the cells are part. Kinds of cell death include the following:\n\nProgrammed cell death (or PCD) is cell death mediated by an intracellular program. PCD is carried out in a regulated process, which usually confers advantage during an organism's life-cycle. For example, the differentiation of fingers and toes in a developing human embryo occurs because cells between the fingers apoptose; the result is that the digits are separate. PCD serves fundamental functions during both plant and metazoa (multicellular animals) tissue development.\n\nApoptosis or Type I cell-death, and autophagy or Type II cell-death are both forms of programmed cell death, while necrosis is a non-physiological process that occurs as a result of infection or injury. Necrosis is cell death caused by external factors such as trauma or infection, and occurs in several different forms. Recently a form of programmed necrosis, called necroptosis, has been recognized as an alternate form of programmed cell death. It is hypothesized that necroptosis can serve as a cell-death backup to apoptosis when the apoptosis signaling is blocked by endogenous or exogenous factors such as viruses or mutations.\n\nMitotic catastrophe is a mode of cell death that is due to premature or inappropriate entry of cells into mitosis. It is the most common mode of cell death in cancer cells exposed to ionizing radiation and many other anti-cancer treatments.\n\nAutophagy is \"cytoplasmic\", characterized by the formation of large vacuoles that eat away organelles in a specific sequence prior to the destruction of the nucleus.\n\nApoptosis is the process of programmed cell death (PCD) that may occur in multicellular organisms. Biochemical events lead to characteristic cell changes (morphology) and death. These changes include blebbing, cell shrinkage, nuclear fragmentation, chromatin condensation, and chromosomal DNA fragmentation. It is now thought that – in a developmental context – cells are induced to positively commit suicide whilst in a homeostatic context; the absence of certain survival factors may provide the impetus for suicide. There appears to be some variation in the morphology and indeed the biochemistry of these suicide pathways; some treading the path of \"apoptosis\", others following a more generalized pathway to deletion, but both usually being genetically and synthetically motivated. There is some evidence that certain symptoms of \"apoptosis\" such as endonuclease activation can be spuriously induced without engaging a genetic cascade, however, presumably true apoptosis and programmed cell death must be genetically mediated. It is also becoming clear that mitosis and apoptosis are toggled or linked in some way and that the balance achieved depends on signals received from appropriate growth or survival factors.\n\nMacroautophagy, often referred to as autophagy, is a catabolic process that results in the autophagosomic-lysosomal degradation of bulk cytoplasmic contents, abnormal protein aggregates, and excess or damaged organelles. Autophagy is generally activated by conditions of nutrient deprivation but has also been associated with physiological as well as pathological processes such as development, differentiation, neurodegenerative diseases, stress, infection and cancer.\n\nOther pathways of programmed cell death have been discovered.\nCalled \"non-apoptotic programmed cell-death\" (or \"caspase-independent programmed cell-death\" or \"necroptosis\"), these alternative routes to death are as efficient as apoptosis and can function as either backup mechanisms or the main type of PCD.\n\nOther forms of programmed cell death include anoikis, almost identical to apoptosis except in its induction; cornification, a form of cell death exclusive to the eyes; excitotoxicity; ferroptosis, an iron-dependent form of cell death and Wallerian degeneration.\n\nPlant cells undergo particular processes of PCD similar to autophagic cell death. However, some common features of PCD are highly conserved in both plants and metazoa.\n\nActivation-induced cell death (AICD) is a programmed cell death caused by the interaction of Fas receptor (Fas, CD95)and Fas ligand (FasL, CD95 ligand). It occurs as a result of repeated stimulation of specific T-cell receptors (TCR) and it helps to maintain the periphery immune tolerance. Therefore, an alteration of the process may lead to autoimmune diseases. In the other words AICD is the negative regulator of activated T-lymphocytes.\n\nIschemic cell death, or oncosis, is a form of accidental, or passive cell death that is often considered a lethal injury. The process is characterized by mitochondrial swelling, cytoplasm vacuolization, and swelling of the nucleus and cytoplasm.\n\nImmunogenic cell death or immunogenic apoptosis is a form of cell death caused by some cytostatic agents such as anthracyclines, oxaliplatin and bortezomib, or radiotherapy and photodynamic therapy (PDT).\n\nPyroptosis is a highly inflammatory form of programmed cell death that occurs most frequently upon infection with intracellular pathogens and is likely to form part of the antimicrobial response in myeloid cells.\n\nThe term \"cell necrobiology\" has been used to describe the life processes associated with morphological, biochemical, and molecular changes which predispose, precede, and accompany cell death, as well as the consequences and tissue response to cell death. The word is derived from the Greek νεκρό meaning \"death\", βìο meaning \"life\", and λόγος meaning \"the study of\". The term was initially coined to broadly define investigations of the changes that accompany cell death, detected and measured by multiparameter flow- and laser scanning- cytometry. It has been used to describe the real-time changes during cell death, detected by flow cytometry \n\n", "id": "1067082", "title": "Cell death"}
{"url": "https://en.wikipedia.org/wiki?curid=7241138", "text": "Endoplasmic-reticulum-associated protein degradation\n\nEndoplasmic-reticulum-associated protein degradation (ERAD) designates a cellular pathway which targets misfolded proteins of the endoplasmic reticulum for ubiquitination and subsequent degradation by a protein-degrading complex, called the proteasome.\n\nThe process of ERAD can be divided into three steps:\n\nThe recognition of misfolded or mutated proteins depends on the detection of substructures within proteins such as exposed hydrophobic regions, unpaired cysteine residues and immature glycans.\n\nIn mammalian cells for example, there exists a mechanism called glycan processing. In this mechanism, the lectin-type chaperones calnexin/calreticulin (CNX/CRT) provide immature glycoproteins the opportunity to reach their native conformation. They can do this by way of reglucosylating these glycoproteins by an enzyme called UDP-glucose-glycoprotein glucosyltransferase. Terminally misfolded proteins, however, must be extracted from CNX/CRT. This is carried out by members of the EDEM (ER degradation-enhancing α-mannosidase-like protein) family (EDEM1-3) and ER mannosidase I. This mannosidase removes one mannose residue from the glycoprotein and the latter is recognized by EDEM. Eventually EDEM will target the misfolded glycoproteins for degradation by facilitating binding of ERAD lectins OS9 and XTP3-B.\n\nBecause the ubiquitin–proteasome system (UPS) is located in the cytosol, terminally misfolded proteins have to be transported from the endoplasmic reticulum back into cytoplasm. Most evidence suggest that the Hrd1 E3 ubiquitin-protein ligase can function as a retrotranslocon or dislocon to transport substrates into the cytosol. Hrd1 is not required for all ERAD events, so it is likely that other proteins contribute to this process. Further, this translocation requires a driving force that determines the direction of transport. Since polyubiquitination is essential for the export of substrates, it is widely thought that this driving force is provided by ubiquitin-binding factors. One of these ubiquitin-binding factors is the Cdc48p-Npl4p-Ufd1p complex in yeast. Humans have the homolog of Cdc48p known as valosin-containing protein (VCP/p97) with the same function as Cdc48p. VCP/p97 transports substrates from the endoplasmic reticulum to the cytoplasm with its ATPase activity.\n\nThe ubiquitination of terminally misfolded proteins is caused by a cascade of enzymatic reactions. The first of these reactions takes place when the ubiquitin-activating enzyme E1 hydrolyses ATP and forms a high-energy thioester linkage between a cysteine residue in its active site and the C-terminus of ubiquitin. The resulting activated ubiquitin is then passed to E2, which is a ubiquitin-conjugating enzyme. Another group of enzymes, more specifically ubiquitin protein ligases called E3, bind to the misfolded protein. Next they align the protein and E2, thus facilitating the attachment of ubiquitin to lysine residues of the misfolded protein. Following successive addition of ubiquitin molecules to lysine residues of the previously attached ubiquitin, a polyubiquitin chain is formed. A polyubiquitinated protein is produced and this is recognized by specific subunits in the 19S capping complexes of the 26S proteasome. Hereafter, the polypeptide chain is fed into the central chamber of the 20S core region that contains the proteolytically active sites. Ubiquitin is cleaved before terminal digestion by deubiquitinating enzymes. This third step is very closely associated with the second one, since ubiquitination takes place during the translocation event. However, the proteasomal degradation takes place in the cytoplasm.\n\nThe ER membrane anchored RING finger containing ubiquitin ligases Hrd1 and Doa10 are the major mediators of substrate ubiquitination during ERAD. The tail anchored membrane protein Ubc6 as well as Ubc1 and the Cue1 dependent membrane bound Ubc7 are the ubiquitin conjugating enzymes involved in ERAD.\n\nAs the variation of ERAD-substrates is enormous, several variations of the ERAD mechanism have been proposed. Indeed, it was confirmed that soluble, membrane and transmembrane proteins were recognized by different mechanisms. This led to the identification of 3 different pathways that constitute in fact 3 checkpoints.\n\n\nAs ERAD is a central element of the secretory pathway, disorders in its activity can cause a range of human diseases. These disorders can be classified into two groups.\n\nThe first group is the result of mutations in ERAD components, which subsequently lose their function. By losing their function, these components are no longer able to stabilize aberrant proteins, so that the latter accumulate and damage the cell. An example of a disease caused by this first group of disorders is Parkinson's disease. It is caused by a mutation in the parkin gene. Parkin is a protein that functions in complex with CHIP as a ubiquitin ligase and overcomes the accumulation and aggregation of misfolded proteins.\n\n[There are numerous theories addressing the causes of Parkinson's disease, besides the one presented here. Many of these can be found in the section of Wikipedia devoted to Parkinson's disease.]\n\nIn contrast to this first group of disorders, the second group is caused by premature degradation of secretory or membrane proteins. In this way, these proteins aren’t able to be deployed to distal compartments, as is the case in cystic fibrosis.\n\nAs described before, the addition of polyubiquitin chains to ERAD substrates is crucial for their export. HIV uses an efficient mechanism to dislocate a single-membrane-spanning host protein, CD4, from the ER and submits it to ERAD.The Vpu protein of HIV-1 is a protein on the ER membrane and targets newly-made CD4 in the endoplasmic reticulum for degradation by cytosolic proteasomes. Vpu only utilizes part of the ERAD process to degrade CD4. CD4 is normally a stable protein and is not likely to be a target for ERAD. However, HIV produces the membrane protein Vpu that binds to CD4.The Vpu protein mainly retains the CD4 in the ER by SCFβ-TrCP-dependent ubiquitination of the CD4 cytosolic tail and transmembrane domain (TMD) interactions. The CD4 Gly415 is a contributor to CD4-Vpu interactions, several TMD-mediated mechanisms by HIV-1 Vpu are necessary to downregulate CD4 and thus promote viral pathogenesis. CD4 retained in the ER will be a target for a variant ERAD pathway rather than predominantly appearing at the plasma membrane without the presence of Vpu through the RESET pathway. Vpu mediates the CD4 retention in the ER and the addition of degradation. As Vpu is phosphorylated, it mimics substrates for the E3 complex SCF. In cells that are infected with HIV, SCF interacts with Vpu and ubiquitinates CD4, which is subsequently degraded by the proteasome. Vpu itself escapes from the degradation.\n\nThe big open questions related to ERAD are:\n\n\n\n", "id": "7241138", "title": "Endoplasmic-reticulum-associated protein degradation"}
{"url": "https://en.wikipedia.org/wiki?curid=37431639", "text": "Necroptosis\n\nNecroptosis is a programmed form of necrosis, or inflammatory cell death. Conventionally, necrosis is associated with unprogrammed cell death resulting from cellular damage or infiltration by pathogens, in contrast to orderly, programmed cell death via apoptosis. The discovery of necroptosis showed that cells can execute necrosis in a programmed fashion and that apoptosis is not always preferable to necrotic cell death. Furthermore, the immunogenic nature of necroptosis favors its use in certain circumstances, such as aiding targeting of pathogens by the immune system. Necroptosis is well defined as a viral defense mechanism, allowing the cell to undergo “cellular suicide” in a caspase-independent fashion in the presence of viral caspase inhibitors. In addition to being a response to disease, necroptosis has also been characterized as a component of inflammatory diseases such as Crohn’s disease, pancreatitis, and myocardial infarction.\n\nThe signaling pathway responsible for carrying out necroptosis is generally understood. Production of TNFα during viral infection leads to stimulation of its receptor TNFR1. The TNFR-associated death protein TRADD signals to RIPK1 which recruits RIPK3 forming the necrosome. Phosphorylation of MLKL by the ripoptosome drives oligomerization of MLKL, allowing MLKL to insert into and permeabilize plasma membranes and organelles. Integration of MLKL leads to the inflammatory phenotype and release of damage-associated molecular patterns (DAMPs), which elicit immune responses.\n\nNecroptosis is specific to vertebrates and may have originated as an additional defense to pathogens. Necroptosis also acts as an alternative “fail-safe” cell death pathway in cases where cells are unable to undergo apoptosis, such as during viral infection in which apoptosis signaling proteins are blocked by the virus.\n\nCell suicide is an effective means of stemming the spread of a pathogen throughout an organism. In apoptotic responses to infection, the contents of an infected cell (including the pathogen) are contained and engulfed by phagocytosis. Some pathogens, such as human cytomegalovirus, express caspase inhibitors that arrest the apoptotic machinery of the host cell. The caspase-independence of necroptosis allows the cell to bypass caspase activation, decreasing the time during which the pathogen can inhabit the cell.\n\nToll-like receptors (TLRs) can also signal to the necrosome, leading to necroptosis. TLRs are a class of receptors that function in the innate immune system to recognize conserved components of pathogens, such as flagellin.\n\nIn apoptosis, extrinsic signaling via cell surface receptors or intrinsic signaling by release of cytochrome c from mitochondria leads to caspase activation. Proteolytic degradation of the cell’s interior culminates with the packaging of the cell’s remains into apoptotic bodies, which are degraded and recycled by phagocytosis. Unlike in apoptosis, necrosis and necroptosis do not involve caspase activation. Necrotic cell death culminates in leakage of cell contents into the extracellular space, in contrast to the organized disposal of cellular contents into apoptotic bodies.\n\nAs in all forms of necrotic cell death, cells undergoing necroptosis rupture and leak their contents into the intercellular space. Unlike in necrosis, permeabilization of the cell membrane during necroptosis is tightly regulated. While many of these mechanisms and components of the pathway are still being uncovered, the major steps of necroptotic signaling have been outlined in recent years. First, extrinsic stimulus through the TNF receptor by TNFα signals the recruitment of the TNF receptor-associated death domain (TRADD) which in turn recruits RIPK1. In the absence of active Caspase 8, RIPK1 and RIPK3 auto- and transphosphorylate each other, leading to the formation of a microfilament-like complex called the necrosome. The necrosome then activates the pro-necroptotic protein MLKL via phosphorylation. MLKL actuates the necrosis phenotype by inserting into the bilipid membranes of organelles and plasma membrane leading to expulsion of cellular contents into the extracellular space. The inflammatory rupturing of the cell releases Damage Associated Molecular Patterns (DAMPs) into the extracellular space. Many of these DAMPs remain unidentified, however, the “find me” and “eat me” DAMP signals are known to recruit immune cells to the damaged/infected tissue. Necrotic cells are cleared from the immune system by a mechanism called pinocytosis, or cellular drinking, which is mediated by macropinosomes, a subcellular component of macrophages. This process is in contrast to removal of apoptotic cells by the immune system in which cells are removed via phagocytosis, or cellular eating.\n\nRecent studies have shown substantial interplay between the apoptosis and necroptosis pathways. At multiple stages of their respective signalling cascades, the two pathways can regulate each other. The best characterized example of this co-regulation is the ability of caspase 8 to inhibit the formation of the necrosome by cleaving RIPK1. Conversely, caspase 8 inhibition of necroptosis can be bypassed by the necroptotic machinery through the anti-apoptotic protein cFLIP which inactivates caspase 8 through formation of a heterodimer.\n\nMany components of the two pathways are also shared. The Tumor Necrosis Factor Receptor can signal for both apoptosis and necroptosis. The RIPK1 protein can also signal for both apoptosis and necroptosis depending on post-translational modifications mediated by other signalling proteins. Furthermore, RIPK1 can be regulated by cellular inhibitor of apoptosis proteins 1 and 2 (cIAP1, cIAP2) which polyubiquitinate RIPK1 leading to cell survival through downstream NF-kB signalling. cIAP1 and cIAP2 can also be regulated by the pro-apoptotic protein SMAC (second mitochondria-derived activator of caspases) which can cleave cIAP1 and cIAP2 driving the cell towards an apoptotic death.\n\nCells can undergo necroptosis in response to perturbed homeostasis in specific circumstances. In response to DNA damage, the RIPK1 and RIPK3 are phosphorylated and lead to deterioration of the cell in the absence of caspase activation. The necrosome inhibits the adenine nucleotide translocase in mitochondria to decrease cellular ATP levels. Uncoupling of the mitochondrial electron transport chain leads to additional mitochondrial damage and opening of the mitochondrial permeability transition pore, which releases mitochondrial proteins into the cytosol. The necrosome also causes leakage of lysosomal digestive enzymes into the cytoplasm by induction of reactive oxygen species by JNK, sphingosine production, and calpain activation by calcium release.\n\nNecroptosis has been implicated in the pathology of many types of acute tissue damage, including myocardinal infarction, stroke, ischemia-reperfusion injury. In addition, necroptosis is noted to contribute to atherosclerosis, pancreatitis, inflammatory bowel disease, neurodegeneration, and some cancers.\n\nIn solid-organ transplantation, ischemia-reperfusion injury can occur when blood returns to tissue for the first time in the transplant recipient. A major contributor to tissue damage results from activation of regulated necroptosis, which could include contributions from both necroptosis and mitochondrial permeability transition. Treatment with the drug cyclosporine, which represses the mitochondrial permeability transition effector Cyclophilin D, improves tissue survival primarily by inhibiting necrotic cell death, rather than its additional function as an immunosuppressant.\n\nRecently, necroptosis-based cancer therapy, using a distinctive molecular pathway for regulation of necroptosis, has been suggested as an alternative method to overcome apoptosis-resistance. For instance, necroptotic cells release highly immunogenic DAMPs, initiating adaptive immunity. These dying cells can also activate NF-κB to express cytokines, recruiting macrophages. So far, little is known about negative regulators of necroptosis, but CHIP, cFLIP and FADD appear to be potential targets for necroptosis based therapy.\n", "id": "37431639", "title": "Necroptosis"}
{"url": "https://en.wikipedia.org/wiki?curid=45702632", "text": "Quantal neurotransmitter release\n\nNeurotransmitters are released into a synapse in packaged vesicles called quanta. One quantum generates what is known as a miniature end plate potential (MEPP) which is the smallest amount of stimulation that one neuron can send to another neuron. Quantal release is the mechanism by which most traditional endogenous neurotransmitters are transmitted throughout the body. The aggregate sum of many MEPPs is known as an end plate potential (EPP). A normal end plate potential usually causes the postsynaptic neuron to reach its threshold of excitation and elicit an action potential. Electrical synapses do not use quantal neurotransmitter release and instead use gap junctions between neurons to send current flows between neurons. The goal of any synapse is to produce either an excitatory postsynaptic potential (EPSP) or an inhibitory postsynaptic potential (IPSP), which generate or repress the expression, respectively, of an action potential in the postsynaptic neuron. It is estimated that an action potential will trigger the release of approximately 20% of an axon terminal's neurotransmitter load.\n\nNeurotransmitters are synthesized in the axon terminal where they are stored in vesicles. These neurotransmitter-filled vesicles are the quanta that will be released into the synapse. Quantal vesicles release their contents into the synapse by binding to the presynaptic membrane and combining their phospholipid bilayers. Individual quanta may randomly diffuse into the synapse and cause a subsequent MEPP. These spontaneous occurrences are completely random and are not the result of any kind of signaling pathway.\n\nCalcium ion signaling to the axon terminal is the usual signal for presynaptic release of neurotransmitters. Calcium ion diffusion into the presynaptic membrane signals the axon terminal to release quanta to generate either an IPSP or EPSP in the postsynaptic membrane. Release of different neurotransmitters will lead to different postsynaptic potentials. Releases of glutamate or GABA will lead to EPSPs or IPSPs, respectively. Action potentials that transmit down to the axon terminal will depolarize the terminal's membrane and cause a conformational change in the membrane's calcium ion channels. These calcium channels will adopt an \"open\" configuration that will allow only calcium ions to enter the axon terminal. The influx of calcium ions will further depolarize the interior of the axon terminal and will signal the quanta in the axon terminal to bind to the presynaptic membrane. Once bound, the vesicles will fuse into the membrane and the neurotransmitters will be released into the membrane by exocytosis.\n\nThe exact mechanism of calcium ion signaling to the presynaptic membrane is unknown, but it has been well established that calcium ion influxes in the axon terminal are linked to neurotransmitter release. Current research suggests that neurotransmitter release into neuromuscular junctions is signaled using a hierarchy of calcium ion channels and receptors in the presynaptic membrane, with different channels and receptors showing varying degrees of excitability in the presynaptic membrane. The variety in calcium channels suggests that more efficient channels are utilized first and that differing use of calcium ion channels leads to differing levels of quantal release.\n\nOnce in the synapse, neurotransmitters will rapidly move across the synapse to attach themselves to receptors on the postsynaptic membrane. Neurotransmitter receptors will either signal postsynaptic channels to \"open\" or \"close\" which will affect the rates that ions are able to cross the synaptic membrane. The relative change in ion flow will polarize the membrane based on the properties of the affected ion channel. For example, opening a potassium ion channel in the presynaptic membrane will create a flow of positive potassium ions out of the neuron; loss of the positively charged potassium ions will cause the neuron to become more negatively charged. It is through the use of a variety of neurotransmitters and receptors that neurons are able to send a plethora of potential signals to each other. Estimations of quantal release time courses can be roughly estimated from the original quantal release events following presynaptic simulation. Such estimations cannot be reliably used in all synapses, but can be useful tools in developing the understanding of neurotransmitter release time courses in general.\n\nAs described above, the synaptic vesicle will remain fused to the presynaptic membrane after its neurotransmitter contents have been released into the synapse. The repeated additions to the axon terminal membrane would eventually result in the uncontrolled growth of the axon terminal, which could lead to disastrous breakdown of the synaptic complex. The axon terminal compensates for this problem by reuptaking the vesicle by endocytosis and reusing its components to form new synaptic vesicles. The exact mechanism and signaling cascade which triggers synaptic vesicle recycling is still unknown.\n\nNo one method of synaptic vesicle recycling seems to hold true in all scenarios, which suggests the existence of multiple pathways for synaptic vesicle recycling. Multiple proteins have been linked with synaptic vesicle reuptake and then subsequently been linked to different synaptic vesicle recycling pathways. Clathrin-mediated endocytosis (CME) and activity-dependent bulk endocytosis (ADBE) are the two most predominant forms of synaptic vesicle recycling, with ADBE being more active during periods of high neuronal activity and CME being active for long periods of time after neuronal activity has ceased.\n", "id": "45702632", "title": "Quantal neurotransmitter release"}
{"url": "https://en.wikipedia.org/wiki?curid=46319796", "text": "Intracellular transport\n\nIntracellular transport is the movement of vesicles and substances within the cell. Eukaryotic cells transport packets of components (membrane‐bound vesicles and organelles, protein rafts, mRNA, chromosomes) to particular intracellular locations by attaching them to molecular motors that haul them along microtubules and actin filaments. This method of transport is often confused with intercellular transport, which deals solely with the movement of cargo between cells not the net movement within a cell. Since intracellular transport heavily relies on microtubules for movement, the components of the cytoskeleton play a vital role in trafficking vesicles between organelles and the plasma membrane.\nThe cell is not a static structure; it is akin to a large city with an intricate highway system connecting one area to the next. In order to properly maintain cell growth utilization of these “cellular highways” is achieved through intracellular transport. Through this pathway it is possible to facilitate the movement of essential molecules such as membrane‐bounded vesicles and organelles, protein rafts, mRNA and chromosomes. In addition to the various organelles, the cytoskeleton plays a key role in intracellular transport by providing the mechanical support necessary for the cell to divide and facilitates movement. It is composed of actin, intermediate filaments and microtubules which each have a role in locomotion, intracellular transport of organelles, cell shape and chromosome separation.\n\nIntracellular transport is unique to eukaryotic cells because they possess organelles enclosed in membranes that need to be mediated for exchange of cargo to take place. Conversely, in prokaryotic cells there is no need for this specialized transport mechanism because there are no membranous organelles and compartments to traffic between. Prokaryotes are able to subsist by allowing materials to enter the cell via simple diffusion. Intracellular transport is more specialized than diffusion, it is a multifaceted process which utilizes transport vesicles. Transport vesicles are small structures within the cell consisting of a fluid enclosed by a lipid bilayer, that have the capacity to hold cargo. These vesicles will typically execute cargo loading and vesicle budding, vesicle transport, the binding of the vesicle to a target membrane and the fusion of the vesicle membranes to target membrane. To ensure that these vesicles embark in the right direction and to further organize the cell, special motor proteins attach to cargo-filled vesicles and carry them along the cytoskeleton. It is helpful to think of these vesicles as cars and the components of the cytoskeleton as the roadways they travel upon. The selectivity of these vesicles is a key component in keeping the cell organized, for example they have to ensure that lysosomal enzymes are transferred specifically to the Golgi Apparatus not to another part of the cell which could lead to deleterious effects.\n\nSmall membrane bound vesicles responsible for transporting proteins from one organelle to another are commonly found in endocytic and secretory pathways. Vesicles bud from their donor organelle and release the contents of their vesicle by a fusion event in a particular target organelle. The net movement of proteins from the Endoplasmic Reticulum (ER) to the Golgi Apparatus represents one form of intracellular transport through this mode of vesicle budding. Since the ER is the site of protein synthesis it would serve as the parent organelle and the cis face of the golgi, where proteins and signals are received, would be the acceptor. In order for the transport vesicle to accurately undergo a fusion event it must first recognize the correct target membrane then fuse with that membrane.\n\nThis fusion event allows for the delivery of the vesicles contents mediated by proteins such as SNARE proteins.SNAREs are small, tail-anchored proteins which are often post-translationally inserted into membranes that are responsible for the fusion event necessary for vesicles to transport between organelles in the cytosol. There are two forms of SNARES, the t-SNARE and v-SNARE which fit together similar to a lock and key.\n\nIt is important to recognize that intracellular transport is an overarching category of how cells obtain nutrients and signals. One very well understood form of intracellular transport is known as endocytosis. Endocytosis is defined as the uptake of material by the invagination of plasma membrane. More specifically, eukaryotic cells use endocytosis of the uptake of nutrients, down regulation of growth factor receptors’ and as a mass regulator of the signaling circuit. While this method of transport is largely intercellular in lieu of uptake of large particles such as bacteria via phagocytosis in which a cell engulfs a solid particle to form an internal vesicle called a phagosome. However, much of these processes have an intracellular component.\nPhagocytosis is of great importance to intracellular transport because once a substance is deemed harmful and engulfed in a vesicle, it can be trafficked to the appropriate location for degradation. These endocytosed molecules are sorted into early endosomes within the cell which serves to further sort these substances to the correct final destination(in the same way the Golgi does in the secretory pathway). From here, the early endosome starts a cascade of transport where the cargo is eventually hydrolyzed inside the lysosome for degradation. This capability is necessary for degradation of any cargo that is harmful or unnecessary for the cell, this is commonly seen in response to foreign material. Phagocytosis has an immunologic function and role in apoptosis. Additionally, endocytosis can be observed through the nonspecific internalization of fluid droplets via pinocytosis and in receptor mediated endocytosis.\n\nMicrotubules function as tracks in the intracellular transport of membrane-bound vesicles and organelles, and this process is propelled by motor proteins such as dynein.\nMicrotubules are cytoskeletal fibers that have an important role in the mitotic spindle during mitosis. They are organized in such a way that their plus ends extend through the periphery of the cells and their minus ends are anchored within the centrosome, so they utilize the motor proteins kinesin’s (positive end directed) and dynein’s (negative end directed) to transport vesicles and organelles in opposite directions through the cytoplasm (cite: the cell a molecular approach). Each type of membrane vesicle is specifically bound to its own kinesin motor protein via binding within the tail domain.\nOne of the major roles of microtubules is to transport membrane vesicles and organelles through the cytoplasm of eukaryotic cells. It is speculated that areas within the cell considered \"microtubule-poor\" are probably transported along microfilaments aided by a myosin motor protein.\nAnother important role Microtubules play is during mitosis when the cell has undergone a duplication event and is in the process of creating daughter cells. Microtubules make up the mitotic spindle which are responsible for distributing the chromosomes to daughter nuclei during anaphase. During Anaphase there is a shortening event, called Anaphase A, in which the chromosomes move towards the spindle poles by utilizing the dynein motor proteins.\n\nBy understanding the components and mechanisms of intracellular transport it is possible to see its implication in diseases. Defects encompass improper sorting of cargo into transport carriers, vesicle budding, issues in movement of vesicles along cytoskeletal tracks, and fusion at the target membrane. Since the life cycle of the cell is a highly regulated and important process, if any component goes awry there is the possibility for deleterious effects. If the cell is unable to correctly execute components of the intracellular pathway there is the impending possibility for protein aggregates to form. Growing evidence supports the concept that deficits in axonal transport contributes to pathogenesis in multiple neurodegenerative diseases. It is proposed that protein aggregations due to faulty transport is a leading cause of the development of ALS, Alzheimer’s and Dementia.\n\nOn the other hand, targeting the intracellular transport processes of these motor proteins constitutes the possibility for pharmacological targeting of drugs. By understanding the method in which substances move along neurons or microtubules it is possible to target specific pathways for disease. Currently, many drug companies are aiming to utilize the trajectory of intracellular transport mechanisms to deliver drugs to localized regions and target cells without harming healthy neighboring cells. The potential for this type of treatment in anti-cancer drugs is an exciting, promising area of research.\n", "id": "46319796", "title": "Intracellular transport"}
{"url": "https://en.wikipedia.org/wiki?curid=1186919", "text": "Fibrinoid necrosis\n\nFibrinoid necrosis is a form of necrosis, or tissue death, in which there is accumulation of amorphous, basic, proteinaceous material in the tissue matrix with a staining pattern reminiscent of fibrin. It is associated with conditions such as immune vasculitis (e.g. polyarteritis nodosa), malignant hypertension, preeclampsia, or hyperacute transplant rejection.\n\nIn small vessel vasculitis, fibrin plugs frequently occur in the vessel lumen, but the term fibrinoid is usually used to refer to material outside the lumen of a vessel. Fibrinoid necrosis also occurs in the walls of arterioles in malignant hypertension (blood pressure greater than 200/130 mmHg).\n\nFibrinoid necrosis is a special form of necrosis usually seen in immune reactions involving blood vessels, known as Type III hypersensitivity reactions. This pattern of necrosis typically occurs when complexes of antigens and antibodies are deposited in the walls of arteries. Deposits of these immune complexes, together with fibrin that has leaked out of vessels, result in a bright pink and amorphous appearance in H&E stains, called “fibrinoid” (fibrin-like) by pathologists. Ultimately, in the living patient most necrotic cells and their contents disappear by phagocytosis of the debris and enzymatic digestion by leukocytes. If necrotic cells and cellular debris are not promptly destroyed and reabsorbed, they tend to attract calcium salts and other minerals and to become calcified.\n\nCommon sites undergoing fibrinoid necrosis are, SLE, Aschoff bodies seen in rheumatic heart disease, Arthus reaction, serum sickness, polyarteritis nodosa, poststreptococcal glomerulonephritis, hyperacutely rejected grafts, malignant hypertension (>200/130 mm Hg).\n", "id": "1186919", "title": "Fibrinoid necrosis"}
{"url": "https://en.wikipedia.org/wiki?curid=46389483", "text": "Interference (genetic)\n\nCrossover interference is the term used to refer to the non-random placement of crossovers with respect to each other during meiosis. The term is attributed to Muller, who observed that one crossover \"interferes with the coincident occurrence of another crossing over in the same pair of chromosomes, and I have accordingly termed this phenomenon ‘interference’.\"\n\nMeiotic crossovers (COs) appear to be regulated to ensure that COs on the same chromosome are distributed far apart (crossover interference). In the nematode worm \"Caenorhabditis elegans\", meiotic double-strand breaks (DSBs) outnumber COs. Thus not all DSBs are repaired by a recombination process(es) leading to COs. The RTEL-1 protein is required to prevent excess meiotic COs. In \"rtel-1\" mutants meiotic CO recombination is significantly increased and crossover interference appears to be absent. RTEL1 likely acts by promoting synthesis-dependent strand annealing which results in non-crossover (NCO) recombinants instead of COs (see diagram). Normally, about half of all DSBs are converted into NCOs. RTEL-1 appears to enforce meiotic crossover interference by directing the repair of some DSBs towards NCOs rather than COs.\n\nIn humans, recombination rate increases with maternal age. Furthermore, placement of female recombination events appears to become increasingly deregulated with maternal age, with a larger fraction of events occurring within closer proximity to each other than would be expected under simple models of crossover interference.\n", "id": "46389483", "title": "Interference (genetic)"}
{"url": "https://en.wikipedia.org/wiki?curid=20975192", "text": "Branch migration\n\nBranch migration is the process by which base pairs on homologous DNA strands are consecutively exchanged at a Holliday junction, moving the branch point up or down the DNA sequence. Branch migration is the second step of genetic recombination, following the exchange of two single strands of DNA between two homologous chromosomes. The process is random, and the branch point can be displaced in either direction on the strand, influencing the degree of which the genetic material is exchanged. Branch migration can also be seen in DNA repair and replication, when filling in gaps in the sequence. It can also be seen when a foreign piece of DNA invades the strand.\n\nThe mechanism for branch migration differs between prokaryotes and eukaryotes.\n\nThe mechanism for prokaryotic branch migration has been studied many times in \"Escherichia coli\". In \"E. coli,\" the proteins RuvA and RuvB come together and form a complex that facilitates the process in a number of ways. RuvA is a tetramer and binds to the DNA at the Holliday junction when it is in the open X form. The protein binds in a way that the DNA entering/departing the junction is still free to rotate and slide through. RuvA has a domain with acidic amino acid residues that interfere with the base pairs in the centre of the junction. This forces the base pairs apart so that they can re-anneal with base pairs on the homologous strands.\n\nIn order for migration to occur, RuvA must be associated with RuvB and ATP. RuvB has the ability to hydrolyze ATP, driving the movement of the branch point. RuvB is a hexamer with helicase activity, and also binds the DNA. As ATP is hydrolyzed, RuvB rotates the recombined strands while pulling them out of the junction, but does not separate the strands as helicase would.\n\nThe final step in branch migration is called resolution and requires the protein RuvC. The protein is a dimer, and will bind to the Holliday junction when it takes on the stacked X form. The protein has endonuclease activity, and cleaves the strands at exactly the same time. The cleavage is symmetrical, and gives two recombined DNA molecules with single stranded breaks. The breaks are then ligated together to complete the process.\n\nThe eukaryotic mechanism is much more complex involving different and additional proteins, but follows the same general path.\n\nThe rate of branch migration is dependent on the amount of divalent ions, specifically magnesium ions (Mg), present during recombination. The ions determine which structure the Holliday junction will adopt, as they play a stabilizing role. When the ions are absent, the backbones repel each other and the junction takes on the open X structure. In this condition, migration is optimal and the junction will be free to move up and down the strands. When the ions are present, they neutralize the negatively charged backbone. This allows the strands to move closer together and the junction adopts the stacked X structure. It is during this state that resolution will be optimal, allowing RuvC to bind to the junction.\n", "id": "20975192", "title": "Branch migration"}
{"url": "https://en.wikipedia.org/wiki?curid=48712402", "text": "Parthanatos\n\nParthanatos (derived from the Greek Θάνατος, “Death”) is a form of programmed cell death that is distinct from other cell death processes such as necrosis and apoptosis. While necrosis is caused by acute cell injury resulting in traumatic cell death and apoptosis is a highly controlled process signalled by apoptotic intracellular signals, parthanatos is caused by the accumulation of PAR and the nuclear translocation of apoptosis-inducing factor (AIF) from mitochondria. Parthanatos is also known as PARP-1 dependent cell death. PARP-1 mediates parthanatos when it is over-activated in response to extreme genomic stress and synthesizes PAR which causes nuclear translocation of AIF. Parthanatos is involved in diseases that afflict hundreds of millions of people worldwide. Well known diseases involving parthanatos include Parkinson’s disease, stroke, heart attack, and diabetes. It also has potential use as a treatment for ameliorating disease and various medical conditions such as diabetes and obesity.\n\nThe term parthanatos was not coined until a review in 2009. The word parthanatos is derived from Thanatos, the personification of death in Greek mythology.\n\nParthanatos was first discovered in a 2006 paper by Yu et al. studying the increased production of mitochondrial reactive oxygen species (ROS) by hyperglycemia. This phenomenon is linked with negative effects arising from clinical complications of diabetes and obesity.\n\nResearchers noticed that high glucose-induced overproduction of reactive oxygen species led mitochondria to undergo rapid fragmentation while exposed. Inhibition of mitochondrial pyruvate uptake that blocked ROS increase did not prevent mitochondrial fragmentation. Neither reactive oxygen species increase nor mitochondrial fragmentation were observed after incubating cells with non-metabolizable stereoisomer L-glucose.\n\nWhen mitochondrial pyruvate uptake that blocked ROS increase was inhibited, there was no prevention of mitochondrial fragmentation in high glucose conditions. Ultimately, the researchers found that mitochondrial fragmentation mediated by the fission process is a necessary component for high glucose-induced respiration increase and ROS overproduction. \nExtended exposure to high glucose conditions are similar to untreated diabetic conditions, and so the effects mirror each other. In this condition, the exposure creates a periodic and prolonged increase in ROS production along with mitochondrial morphology change. If mitochondrial fission was inhibited, the periodic fluctuation of ROS production in a high glucose environment was prevented. This research shows that when cell damage to the ROS is too great, PARP-1 will initiate cell death.\n\nPoly(ADP-ribose) polymerase-1 (PARP-1) is a nuclear enzyme that is found universally in all eukaryotes and is encoded by the PARP-1 gene. It belongs to the PARP family, which is a group of catalysts that transfer ADP-ribose units from NAD (nicotinamide dinucleotide) to protein targets, thus creating branched or linear polymers. The major domains of PARP-1 impart the ability to fulfill its functions. These protein sections include the DNA-binding domain on the N-terminus (allows PARP-1 to detect DNA breaks), the automodification domain (has a BRCA1 C terminus motif which is key for protein-protein interactions), and a catalytic site with the NAD+-fold (characteristic of mono-ADP ribosylating toxins).\n\nNormally, PARP-1 is involved in a variety of functions that are important for cell homeostasis such as mitosis. Another of these roles is DNA repair, including the repair of base lesions and single-strand breaks. PARP-1 interacts with a wide variety of substrates including histones, DNA helicases, high mobility group proteins, topoisomerases I and II, single-strand break repair factors, base-excision repair factors, and several transcription factors.\n\nPARP-1 accomplishes many of its roles through regulating poly(ADP-ribose) (PAR). PAR is a polymer that varies in length and can be either linear or branched. It is negatively charged which allows it to alter the function of the proteins it binds to either covalently or non-covalently. PAR binding affinity is strongest for branched polymers, weaker for long linear polymers and weakest for short linear polymers. PAR also binds selectively with differing strengths to the different histones. It is suspected that PARP-1 modulates processes (such as DNA repair, DNA transcription, and mitosis) through the binding of PAR to its target proteins.\n\nThe parthanatos pathway is activated by DNA damage caused by genotoxic stress or excitotoxicity. This damage is recognized by the PARP-1 enzyme which causes an upregulation in PAR. PAR causes translocation of apoptosis-inducing factor (AIF) from the mitochondria to the nucleus where it induces DNA fragmentation and ultimately cell death. This general pathway has been outlined now for almost a decade. While considerable success has been made in understanding the molecular events in parthanatos, efforts are still ongoing to completely identify all of the major players within the pathway, as well how spatial and temporal relationships between mediators affect them.\n\nExtreme damage of DNA causing breaks and changes in chromatin structure have been shown to induce the parthanatos pathway. Stimuli that causes the DNA damage can come from a variety of different sources. Methylnitronitrosoguanidine, an alkylating agent, has been widely used in several studies to induce the parthanatos pathway. A noted number of other stimuli or toxic conditions have also been used to cause DNA damage such as H2O2, NO, and ONOO- generation (oxygenglucose deprivation).\nThe magnitude, length of exposure, type of cell used, and purity of the culture, are all factors that can influence the activation of the pathway. The damage must be extreme enough for the chromatin structure to be altered. This change in structure is recognized by the N-terminal zinc-finger domain on the PARP-1 protein. The protein can recognize both single and double DNA breaks.\n\nOnce the PARP-1 protein recognizes the DNA damage, it catalyzes post-transcriptional modification of PAR. PAR will be formed either as a branched or linear molecule. Branching and long-chain polymers will be more toxic to the cell than simple short polymers. The more extreme the DNA damage, the more PAR accumulates in the nucleus. Once enough PAR has accumulated, it will translocate from the nucleus into the cytosol. One study has suggested that PAR can translocate as a free polymer, however translocation of a protein-conjugated PAR cannot be ruled out and is in fact a topic of active research. PAR moves through the cytosol and enters the mitochondria through depolarization. Within the mitochondria, PAR binds directly to the AIF which has a PAR polymer binding site, causing the AIF to dissociate from the mitochondria. AIF is then translocated to the nucleus where it induces chromatin condensation and large scale (50Kb) DNA fragmentation. How AIF induces these effects is still unknown. It is thought that an AIF associated nuclease (PAAN) that is currently unidentified may be present. Human AIF have a DNA binding site that would indicate that AIF binds directly to the DNA in the nucleus directly causing the changes. However, as mice AIF do not have this binding domain and are still able to undergo parthanatos, it is evident that there must be another mechanism involved.\n\nPAR, which is responsible for the activation of AIF, is regulated in the cell by the enzyme poly(ADP-ribose) glycohydrolase (PARG). After PAR is synthesized by PAR-1, it is degraded through a process catalyzed by PARG. PARG has been found to protect against PAR-mediated cell death while its deletion has increased toxicity through the accumulation of PAR.\n\nBefore the discovery of the PAR and AIF pathway, it was thought that the overactivation of PARP-1 lead to over consumption of NAD+. As a result of NAD+ depletion, a decrease of ATP production would occur, and the resulting loss of energy would kill the cell. However it is now known that this loss of energy would not be enough to account for cell death. In cells lacking PARG, activation of PARP-1 leads to cell death in the presence of ample NAD+.\n\nParthanatos is defined as a unique cell death pathway from apoptosis for a few key reasons. Primarily, apoptosis is dependent on the caspase pathway activated by cytochrome c release, while the parthanatos pathway is able to act independently of caspase. Furthermore, unlike apoptosis, parthanatos causes large scale DNA fragmentation (apoptosis only produces small scale fragmentation) and does not form apoptotic bodies.\nWhile parthanatos does share similarities with necrosis, is also has several differences. Necrosis is not a regulated pathway and does not undergo any controlled nuclear fragmentation. While parthanatos does involve loss of cell membrane integrity like necrosis, it is not accompanied by cell swelling.\n\nThe PAR enzyme was originally connected to neural degradation pathways in 1993. Elevated levels of nitric oxide (NO) have been shown to cause neurotoxicity in samples of rat hippocampal neurons. A deeper look into the effects of NO on neurons showed that nitric oxides cause damage to DNA strands; the damage in turn elicits PAR enzyme activity that leads to further degradation and neuronal death. PAR- blockers halted the cell death mechanisms in the presence of elevated NO levels.\n\nPARP activity has also been linked to the neurodegenerative properties of toxin induced Parkinsonism. 1-Methyl-4-phenyl-1,2,3,6-tetrahydropyridine (MPTP) is a neurotoxin that has been linked to neurodegeneration and development of Parkinson Disease-like symptoms in patients since 1983. The MPTP toxin’s effects were discovered when four people were intravenously injecting the toxin that they produced inadvertently when trying to street-synthesise the merpyridine (MPPP) drug. The link between MPTP and PARP was found later when research showed that the MPTP effects on neurons were reduced in mutated cells lacking the PARP gene. The same research also showed highly increased PARP activation in dopamine producing cells in the presence of MPTP.\n\nParthanatos, as a cell death pathway, is being increasingly linked to several syndromes connected with specific tissue damage outside of the nervous system. This is highlighted in the mechanism of streptozotocin (STZ) induced diabetes. STZ is a chemical that is naturally produced by the human body. However, in high doses, STZ has been shown to produce diabetic symptoms by damaging pancreatic β cells, which are insulin-producing. The degradation of β cells by STZ was linked to PARP in 1980 when studies showed that a PAR synthesis inhibitor reduced STZ’s effects on insulin synthesis. Inhibition of PARP causes pancreatic tissue to sustain insulin synthesis levels, and reduce β cell degradation even with elevated STZ toxin levels.\n\nPARP activation has also been preliminarily connected with arthritis, colitis, and liver toxicity.\n\nThe multi-step nature of the parthanatos pathway allows for chemical manipulation of its activation and inhibition for use in therapy. This rapidly developing field seems to be currently focused on the use of PARP blockers as treatments for chronically degenerative illnesses. This culminated in 3rd generation inhibitors such as midazoquinolinone and isoquinolindione currently going to clinical trials.\n\nAnother path for treatments is to recruit the parthanatos pathway to induce cancer cells into apoptosis, however no treatments have passed the theoretical stage.\n\n", "id": "48712402", "title": "Parthanatos"}
{"url": "https://en.wikipedia.org/wiki?curid=28274526", "text": "Emperipolesis\n\nIn medicine, emperipolesis is the presence of an intact cell within the cytoplasm of another cell. It is derived from Greek (\"en\" is \"inside\", \"peripoleomai\" is \"go round\"). Emperipolesis is an uncommon biological process, and can be physiological or pathological.\n\nIt is related to \"peripolesis\", which is the attachment of one cell to another.\n\nEmperipolesis is unlike phagocytosis, in which the engulfed cell is killed by the lysosomal enzymes of the macrophage. Instead, the engulfed cell remains viable within the other, and can exit at any time without causing structural or functional abnormalities in either cell.\n\nEmperipolesis has been classified into two categories:\n\nIt is seen in various conditions including:\n\n", "id": "28274526", "title": "Emperipolesis"}
{"url": "https://en.wikipedia.org/wiki?curid=50582008", "text": "Peripolesis\n\nPeripolesis is the process in which a cell attaches itself to another cell. This is differentiated from \"emperipolesis\", which is when one cell is engulfed by another.\n\nPeripolesis is thought to be a physiological mechanism involved in regulating some processes of immune response. It was observed between lymphocytes and macrophages following skin grafts between subjects, and after immune challenge with antigens. Peripolesis was also observed in lung alveoli, where the peripolesed macrophages were not injured, but the cell membrane did appear to be temporarily altered. In patients with active sarcoidosis, which is characterized by lymphocyte-marophage cooperaction, lymphocyte peripolesis appeared to occur in clusters and could last for minutes to hours. The lymphocytes could be seen moving around a macrophage while maintaining contact.\n", "id": "50582008", "title": "Peripolesis"}
{"url": "https://en.wikipedia.org/wiki?curid=49628551", "text": "Poly(adp-ribose) polymerase family member 14\n\nPoly(ADP-ribose) polymerase family member 14 is a protein that, in humans, is encoded by the PARP14 gene.\nPoly(ADP-ribosyl)ation is an immediate DNA damage-dependent post-translational modification of histones and other nuclear proteins that contributes to the survival of injured proliferating cells. PARP14 belongs to the superfamily of enzymes that perform this modification (Ame et al., 2004 [PubMed 15273990]).\n", "id": "49628551", "title": "Poly(adp-ribose) polymerase family member 14"}
{"url": "https://en.wikipedia.org/wiki?curid=51376905", "text": "Klerokinesis\n\nKlerokinesis (from the Greek root for allotted inheritance) has been claimed to be a new form of cell division in human cells. It is thought to serve as a natural back-up mechanism during faulty cell division (Cytokinesis), thus preventing some cells from going down a path that can lead to cancer. It is considered to be a recovery mechanism.\n\nThe new cellular process was identified when researchers at University of Wisconsin were blocking cells from undergoing cytokinesis — a process during cell division in which the cytoplasm of a single eukaryotic cell is divided to form two daughter cells. The group generated a population of human retinal pigment epithelial (RPE) cells that would undergo karyokinesis but miss the cytokinesis stage. Using live cell imaging, klerokinesis was found while studying human RPE cells to establish the hypothesis of German biologist Theodor Boveri, that the presence of an abnormal number of sets of chromosomes in a cell (polyploidy) would lead to out-of-control cell division, causing cancer. However, contrary to Boveri's hypothesis, researchers found that abnormal cell division rarely has long-term negative effects in human cells, rather, some cell divisions occurred later than the end of mitosis in such a way that the normal chromosome complement was restored. \n\nSimilar divisions like klerokinesis have been observed in slime molds. It is thought that klerokinesis is a primitive mechanism of cell division that appears to be preserved in humans. Researchers saw that this mechanism was happening about 90% of the time and that the cells were getting abnormal chromosome sets about 10% of the time.\n\nKlerokinesis was presented by a team from the UW Carbone Cancer Center in 2012, at the annual meeting of the American Society for Cell Biology in San Francisco.\n\n", "id": "51376905", "title": "Klerokinesis"}
{"url": "https://en.wikipedia.org/wiki?curid=50449991", "text": "Nuclear organization\n\nNuclear organization refers to the spatial distribution of chromatin within a cell nucleus. There are many different levels and scales of nuclear organisation.\n\nAt the smallest scale, DNA is packaged into units called nucleosomes. The quantity and organisation of these nucleosomes can affect the accessibility of local chromatin. This has a knock-on effect on the expression of nearby genes, additionally determining whether or not they can be regulated by transcription factors.\n\nAt slightly larger scales, DNA looping can physically bring together DNA elements that would otherwise be separated by large distances. These interactions allow regulatory signals to cross over large genomic distances - for example, from enhancers to promoters.\n\nIn contrast, on a large-scale, the arrangement of chromosomes can determine their properties. Chromosomes are organised into two compartments labelled A (\"active\") and B (\"inactive\"), each with distinct properties. Moreover, entire chromosomes segregate into distinct regions called chromosome territories.\n\nEach human cell contains around two metres of DNA, which must be tightly folded to fit inside the cell nucleus. However, in order for the cell to function, proteins must be able to access the sequence information contained within the DNA, in spite of its tightly-packed nature. Hence, the cell has a number of mechanisms in place to control how DNA is organized.\n\nMoreover, nuclear organization can play a role in establishing cell identity. Cells within an organism have near identical nucleic acid sequences, but often exhibit different phenotypes. One way in which this individuality occurs is through changes in genome architecture, which can alter the expression of different sets of genes. These alterations can have a downstream effect on cellular functions such as cell cycle facilitation, DNA replication, nuclear transport, and alteration of nuclear structure. Controlled changes in nuclear organization are essential for proper cellular function.\n\nThe organization of chromosomes into distinct regions within the nucleus was first proposed in 1885 by Carl Rabl. Later in 1909, with the help of the microscopy technology at the time, Theodor Boveri coined the termed chromosome territories after observing that chromosomes occupy individually distinct nuclear regions. Since then, mapping genome architecture has become a major topic of interest.\n\nOver the last ten years, rapid methodological developments have greatly advanced understanding in this field. Large-scale DNA organization can be assessed with DNA imaging using fluorescent tags, such as DNA Fluorescence in situ hybridization (FISH), and specialized microscopes. Additionally, high-throughput sequencing technologies such as Chromosome Conformation Capture-based methods can measure how often DNA regions are in close proximity. At the same time, progress in genome-editing techniques (such as CRISPR/Cas9, ZFNs, and TALENs) have made it easier to test the organizational function of specific DNA regions and proteins.\n\nArchitectural proteins regulate chromatin structure by establishing physical interactions between DNA elements. These proteins tend to be highly conserved across a majority of eukaryotic species.\n\nIn mammals, key architectural proteins include:\n\n\nThe first level of genome organization concerns how DNA is arranged linearly, and how it is packaged into chromosomes. DNA is composed of two antiparallel strands of nucleic acids, with two bound and opposing nucleic acids referred to as DNA base pairs. In order for DNA to pack inside the tiny cell nucleus, each strand is wrapped around histones, forming nucleosome structures. These nucleosome pack together to form chromosomes. Depending on the eukaryote, there are multiple independent chromosomes of varying sizes within each nucleus - for example, humans have 46 while giraffes have 30.\nWithin regions of the chromosome, the order of the DNA base pairs makes up specific elements for gene expression and DNA replication. Some of the more common elements include protein coding genes (containing exons and introns), noncoding DNA, enhancers, promoters, operators, origins of replication, telomeres, and centromeres. As of yet, there is not much evidence towards the importance of specific order of these elements along or between individual chromosomes. For example, the distance between an enhancer and a promoter, interacting elements that form a basis of gene expression, can range from a few hundred base pairs to 100s of kb away. As well, individual enhancers can interact with a number of different promoters and the same is true for a single promoter interacting with multiple different enhancers.\n\nHowever, on a larger scale, chromosomes are heterogeneous in the context of euchromatin and heterochromatin composition. As well, there is evidence of gene rich and poor regions and various domains associated with cell differentiation, active or repressed gene expression, DNA replication, and DNA recombination and repair. All of these help determine chromosome territories.\n\nDNA looping is the first level of nuclear organization that involves chromosomal folding. In a DNA looping event, chromatin forms physical loops, bringing DNA regions into close contact. Thus, even regions that are far apart along the linear chromosome can be brought together in three-dimensional space. The process is facilitated by a number of factors including architectural proteins (primarily CTCF and Cohesin), transcription factors, co-activators, and ncRNAs. Importantly, DNA looping can be used to regulate gene expression - looping events can repress or activate genes, depending on the elements involved. Approximately 50% of human genes are believed to be involved in long range chromatin interactions through the process of DNA looping.\n\nLooping was first observed by Walther Flemming in 1878 when he was studying amphibian oocytes. It was not until the late 20th century when DNA looping was correlated with gene expression. For example, in 1990, Mandal and colleagues showed the importance of DNA looping in repressing the galactose and lactose operons in \"E coli\". In the presence of galactose or lactose, repressor proteins form protein-protein and protein-DNA interactions to loop the DNA. This in turn connects the gene promoters with upstream and downstream operators, effectively repressing gene expression by blocking transcription preinitiation complex (PIC) assembly at the promoter and therefore preventing transcription initiation.\nIn gene activation, DNA looping typically brings together distal gene promoters and enhancers. Enhancers can recruit a large complex of proteins, such as the mediator complex, PIC, and other cell specific transcription factors, involved in initiating the transcription of a gene.\n\nSelf-interacting (or self-associating) domains are found in many organisms - in bacteria, they are referred to as Chromosomal Interacting Domains (CIDs), whereas in mammalian cells, they are called Topologically Associating Domains (TADs). Self-interacting domains can range from the 1-2 mb scale in larger organisms to 10s of kb in single celled organisms. What characterizes a self-interacting domain is a set of common features. The first is that self-interacting domains have a higher of ratio of chromosomal contacts within the domain than outside it. They are formed through the help of architectural proteins and contain within them many chromatin loops. This characteristic was discovered using Hi-C techniques. Second, self-interacting domains correlate with regulation of gene expression. There specific domains that are associated with active transcription and other domains that repress transcription. What distinguishes whether a domain takes a particular form is dependent on which associated genes need to be active/inactive during particular phase of growth, cell cycle stage, or within a specific cell type. Cellular differentiation is determined by particular sets of genes being on or off, corresponding with the unique makeup of an individual cell’s self-interacting domains. Lastly, the outside boundaries of these domains contain a higher frequency of architectural protein binding sites, regions and epigenetic marks correlated to active transcription, housekeeping genes, and short interspaced nuclear elements (SINEs).\n\nAn interesting example of a subset of self-interacting domains is active chromatin hubs (ACHs). These hubs were discovered during observation of activated alpha- and beta-globin loci. ACHs are formed through extensive DNA looping to form a \"hub\" of regulatory elements in order to coordinate the expression of a subset of genes.\n\nLamina-associating domains (LADs) and nucleolar-associating domains (NADs) are regions of the chromosome that interact with the nuclear lamina and nucleolus, respectively.\n\nMaking up approximately 40% of the genome, LADs consist mostly of gene poor regions and span between 40kb to 30Mb in size. There are two known types of LADs: constitutive LADs (cLADs) and facultative LADs (fLADs). cLADs are A-T rich heterochromatin regions that remain on lamina and are seen across many types of cells and species. There is evidence that these regions are important to the structural formation of interphase chromosome. On the other hand, fLADs have varying lamina interactions and contain genes that are either activated or repressed between individual cells indicating cell-type specificity. The boundaries of LADs, like self-interacting domains, are enriched in transcriptional elements and architectural protein binding sites.\n\nNADs, which constitutes 4% of the genome, share nearly all of the same physical characteristics as LADs. In fact, DNA analysis of these two types of domains have shown that many sequences overlap, indicating that certain regions may switch between lamina-binding and nucleolus-binding. Interestingly, NADs are associated with nucleolus function. The nucleolus is the largest sub-organelle within the nucleus and is the principal site for rRNA transcription. It also acts in signal recognition particle biosynthesis, protein sequestration, and viral replication. The nucleolus forms around rDNA genes from different chromosomes. However, only a subset of rDNA genes is transcribed at a time and do so by looping into the interior of the nucleolus. The rest of the genes lay on the periphery of the sub-nuclear organelle in silenced heterochromatin state.\n\nA/B compartments were first discovered in early Hi-C studies. Researchers noticed that the whole genome could be split into two spatial compartments, labelled \"A\" and \"B\", where regions in compartment A tend to interact preferentially with A compartment-associated regions than B compartment-associated ones. Similarly, regions in compartment B tend to associate with other B compartment-associated regions.\n\nA/B compartment-associated regions are on the multi-Mb scale and correlate with either open and expression-active chromatin (\"A\" compartments) or closed and expression-inactive chromatin (\"B\" compartments). A compartments tend to be gene-rich, have high GC-content, contain histone markers for active transcription, and usually displace the interior of the nucleus. As well, they are typically made up of self-interacting domains and contain early replication origins. B compartments, on the other hand, tend to be gene-poor, compact, contain histone markers for gene silencing, and lie on the nuclear periphery. They are consisted mostly of LADs and contain late replication origins.\n\nThe fact that compartments self-interact is consistent with the idea that the nucleus localizes proteins, and other factors such as long non-coding RNA (lncRNA), in regions suited for their individual roles. An example of this is the presence of multiple transcription factories throughout the nuclear interior. These factories are associated with elevated levels of transcription due to the high concentration of transcription factors (such as transcription protein machinery, active genes, regulatory elements, and nascent RNA). Around 95% of active genes are transcribed within transcription factories. Each factory can transcribe multiple genes - these genes need not have similar product functions, nor do they need to lie on the same chromosome. Finally, the co-localization of genes within transcription factories is known to depend on cell type.\nA/B often compartments vary between cell types. \n\nThe last level of organization concerns the distinct positioning of individual chromosomes within the nucleus. The region occupied by a chromosome is called a chromosome territory (CT). Among eukaryotes, CTs have several common properties. First, although chromosomal locations are not the same across cells within a population, there is some preference among individual chromosomes for particular regions. For example, large, gene-poor chromosomes are commonly located on the periphery near the nuclear lamina while smaller, gene-rich chromosomes group closer to the center of the nucleus. Second, individual chromosome preference is variable among different cell types. For example, the X-chromosome has shown to localize to the periphery more often in liver cells than in kidney cells. Another conserved property of chromosome territories is that homologous chromosomes tend to be far apart from one another during cell interphase. The final characteristic is that the position of individual chromosomes during each cell cycle stays relatively the same until the start of mitosis. The mechanisms and reasons behind chromosome territory characteristics is still unknown and further experimentation is needed.\n", "id": "50449991", "title": "Nuclear organization"}
{"url": "https://en.wikipedia.org/wiki?curid=5290467", "text": "Ribosome biogenesis\n\nRibosome biogenesis is the process of making ribosomes. In prokaryotic cells, it takes place in the cytoplasm with the transcription of many ribosome gene operons. In eukaryotes, it takes place both in the cytoplasm and in the nucleolus. It involves the coordinated function of over 200 proteins in the synthesis and processing of the three prokaryotic or four eukaryotic rRNAs, as well as assembly of those rRNAs with the ribosomal proteins. Most of the ribosomal proteins fall into various energy-consuming enzyme families including ATP-dependent RNA helicases, AAA-ATPases, GTPases, and kinases. About 60℅ of a cell's energy is spent on ribosome production and maintenance. \n\nSome have speculated that in the origin of life, ribosome biogenesis predates cells, and that genes and cells evolved to enhance the reproductive capacity of ribosomes.\n\nRibosomes are the macromolecular machines that are responsible for mRNA translation into proteins. The eukaryotic ribosome, also called the 80S ribosome, is made up of two subunits – the large 60S subunit (which contains the 25S (in plants) or 28S (in mammals), 5.8S, and 5S rRNA) and a small 40S subunit (which contains the 18S rRNA and 33 R proteins) as well as 46 ribosomal proteins. The ribosomal proteins are encoded by ribosomal genes or rDNA.\n\nThere are 52 genes that encode the ribosomal proteins, and they can be found in 20 operons within prokaryotic DNA. Regulation of ribosome synthesis hinges on the regulation of the rRNA itself.\n\nFirst, a reduction in aminoacyl-tRNA will cause the prokaryotic cell to respond by lowering transcription and translation. This occurs through a series of steps, beginning with stringent factors binding to ribosomes and catalyzing the reaction:\"GTP + ATP --> pppGpp + AMP\"\n\nThe γ-phosphate is then removed and ppGpp will bind to and inhibit RNA polymerase. This binding causes a reduction in rRNA transcription. A reduced amount of rRNA means that ribosomal proteins (r-proteins) will be translated but will not have an rRNA to bind to. Instead, they will negatively feedback and bind to their \"own\" mRNA, repressing r-protein synthesis. Note that r-proteins preferentially bind to its complementary rRNA if it is present, rather than mRNA.\n\nThe ribosome operons also include the genes for RNA polymerase and elongation factors (used in RNA translation). Regulation of all of these genes at once illustrate the coupling between transcription and translation in prokaryotes.\n\nRibosomal protein synthesis in eukaryotes is a major metabolic activity. It occurs, like most protein synthesis, in the cytoplasm just outside the nucleus. Individual ribosomal proteins are synthesized and imported into the nucleus through nuclear pores. See nuclear import for more about the movement of the ribosomal proteins into the nucleus.\n\nThe rRNA is transcribed, at a high speed, in the nucleolus, which contains all 45S rRNA genes. The only exception is the 5S rRNA which is transcribed outside the nucleolus. After transcription, the rRNAs associate with the ribosomal proteins, forming the two types of ribosomal subunits (large and small). These will later assemble in the cytosol to make a functioning ribosome. See nuclear export for more about the movement of the ribosomal subunits out of the nucleus.\n\nEukaryotic cells co-transcribe three of the mature rRNA species though a series of steps. The maturation process of the rRNAs and the process of recruiting the r-proteins happen in precursor ribosomal particles, sometimes called pre-ribosomes, and takes place in the nucleolus, nucleoplasm, and cytoplasm. The yeast, \"S. cerevisiae\" is the eukaryotic model organism for the study of ribosome biogenesis. \nRibosome biogenesis starts in the nucleolus. There, the 18S, 5.8S, and 25S subunits of the rRNA are cotranscribed from ribosomal genes as a polycistronic transcript by RNA polymerase I, and is called 35S pre-RNA.\n\nTranscription of polymerase I starts with a Pol I initiation complex that binds to the rDNA promoter. The formation of this complex requires the help of an upstream activating factor or UAF that associates with TATA-box binding protein and the core factor (CF). Together the two transcription factors allow the RNA pol I complex to bind with the polymerase I initiation factor, Rrn3. As the pol I transcript is produced, approximately 75 small nucleolar ribonucleoparticles (snoRNPs) facilitate the co-transcriptional covalent modifications of >100 rRNA residues. These snoRNPs control 2’-O-ribose methylation of nucleotides and also assist in the creation of pseudouridines. At the 5’ end of rRNA transcripts, small subunit ribosomal proteins (Rps) and non-ribosomal factors assemble with the pre-RNA transcripts to create ball-like knobs. These knobs are the first pre-ribosomal particles in the small (40S) ribosomal subunit pathway. The rRNA transcript is cleaved at the A2 site, and this separates the early 40S pre-ribosome from the remaining pre-rRNA that will combine with large subunit ribosomal proteins (Rpl) and other non-ribosomal factors to create the pre-60S ribosomal particles.\n\nThe transcriptional assembly of the 40S subunit precursor, sometimes referred to as the small subunit processome (SSU) or 90S particle happens in a hierarchical fashion – essentially a stepwise incorporation of the UTP-A, UTP-B, and UTP-C subcomplexes. These subcomplexes are made up of over 30 non ribosomal protein factors, the U3 snoRNP particle, a few Rps proteins, and the 35S pre-rRNA. Their exact role, though has not been discovered. The composition of the pre-40S particle changes drastically once cleavage at the U3 snoRNPA dependent sites (sites A0, A1, and A2) are made. This cleavage event creates the 20S pre-rRNA and causes ribosomal factors to dissociate from the pre-40S particle. At this point in the ribosome biogenesis process, the 40S pre-ribosome already shows the “head” and “body” structures of the mature 40S subunit. The 40S pre-ribosome is transported out of the nucleolus and into the cytoplasm. The cytoplasmic 40S pre-ribosome now contains ribosomal proteins, the 20s rRNA and a few non-ribosomal factors. The final formation of the 40S subunit “beak” structure occurs after a phosphorylation and dephosphorylation event involving the Enp1-Ltv1-Rps3 complex and the kinase, Hrr25. Cleavage of the 20S pre-rRNA at the D-site creates the mature 18s rRNA. This cleavage event is dependent on several non-ribosomal factors such as Nob1, Rio1, Rio2, Tsr1 and Fap7.\n\nThe maturation of the pre-60S subunit into a mature 60S subunit requires many biogenesis factors that associate and disassociate. In addition, some assembly factors associate with the 60S subunit while others only interact with it transiently. As an overall trend, the maturation of the pre-60S subunit is marked a gradual decrease in complexity. The subunit matures as it moves from the nucleolus to the cytoplasm and gradually the number of trans-acting factors are reduced. The maturation of the 60S subunit requires the help of about 80 factors. Eight of these factors are directly involved with the processing of the 27S A3 pre-rRNA, which actually completes the formation of the mature 5’end of the 5.8S rRNA. The A3 factors bind to distant sites on the pre-RNA as well as to each other. Subsequently, they bring areas of rRNA close together and promote the processing of pre-rRNA and the recruitment of ribosomal proteins. Three AAA-type ATPases work to strip the factors from the maturing 60S pre-ribosome. One of the ATPases is a dynein-like Rea1 protein made up of 6 different ATPase domains that form a ring structure. The ring structure is attached to a flexible tail that happens to have a MIDAS (Metal ion-dependentant adhesion site) tip. The Rea1 interacts with the 60S pre-ribosome via its ring while two substrates, Ytm1 and Rsa1, interact with Rea1 through its MIDAS tip. The role of these substrates has not yet been defined. Both though, along with their interactions, are removed in the maturation process of the 60S pre-ribosome. The other two ATPases, Rix7 and Drg1 also function to remove assembly factors from the maturing 60S subunit. Helicases and GTPases are also involved in the removal of assembly factors and the rearrangement of RNA to form the completed 60S subunit. Once in the cytoplasm (see nuclear export), the 60S subunit further undergoes processing in order to be functional. The rest of the large subunit ribosomal particles associate with the 60S unit and the remaining non-ribosomal assembly factors disassociate. The release of the biogenesis factors is mediated mostly by GTPases such as Lsg1 and ATPases such as Drg1. The precise sequence of these events remains unclear. The pathway of 60S cytoplasmic maturation remains incomplete as far as current knowledge is concerned.\n\nIn order for the pre-ribosomal units to fully mature, they must be exported to the cytoplasm. To effectively move from the nucleolus to the cytoplasm, the pre-ribosomes interact with export receptors to move through the hydrophobic central channel of the nuclear pore complex. The karyopherin Crm1 is the receptor for both ribosomal subunits and mediates export in a Ran-GTP dependent fashion. It recognizes molecules that have leucine-rich nuclear export signals. The Crm1 is pulled to the large 60S subunit by the help of an adapter protein called Nmd3. The adapter protein for the 40S unit is unknown. In addition to Crm1, other factors play a role in nuclear export of pre-ribosomes. A general mRNA export receptor, called Mex67, as well as a HEAT-repeating-containing protein, Rrp12, facilitate the export of both subunits. These factors are non-essential proteins and help to optimize the export of the pre-ribosomes since they are large molecules.\n\nBecause ribosomes are so complex, a certain number of ribosomes are assembled incorrectly and could potentially waste cellular energy and resources when synthesizing non-functional proteins. To prevent this, cells have an active surveillance system to recognize damaged or defective ribosomes and target them for degradation. The surveillance mechanism is in place to detect nonfunctional pre-ribosomes as well as nonfunctional mature ribosomes. In addition, the surveillance system brings the necessary degradation equipment and actually degrades the nonfunctional ribosomes. Pre-ribosomes that build up in the nucleus are destroyed by the exosome, which is a multisubunit complex with exonuclease activity. If perhaps defective ribosomal subunits do make it to the cytoplasm, there is an additional surveillance system in place to target their degradation in the cytoplasm. Certain mutations in residues of the large ribosome subunit will actually result in RNA decay and thus degradation of the unit. Because the amount of defects that are possible in ribosome assembly are so extensive, it is still unknown as to how the surveillance system detects all defects, but it has been postulated that instead of targeting specific defects, the surveillance system recognizes the consequences of those defects – such as assembly delays. Meaning, if there is a disruption in the assembly or maturation of a mature ribosome, the surveillance system will act as if the subunit is defective.\n\nMutations in ribosome biogenesis are linked to several human ribosomopathy genetic diseases, including inherited bone marrow failure syndromes, which are characterized by a predisposition to cancer and a reduced number of blood cells. Ribosomal dysregulation may also play a role in muscle wasting.\n", "id": "5290467", "title": "Ribosome biogenesis"}
{"url": "https://en.wikipedia.org/wiki?curid=51735357", "text": "Hertwig rule\n\nHertwig's rule or 'long axis rule' states that a cell divides along its long axis. It was introduced by german zoologist Oscar Hertwig in 1884. The rule emphasizes the cell shape to be a default mechanism of spindle apparatus orientation. Hertwig's rule predicts cell division orientation, that is important for tissue architecture, cell fate and morphogenesis.\n\nIn Hertwig's experiments the orientation of divisions of the frog egg was studied. Frog egg has a round shape and the first division occur in the random orientation. Hertwig compressed the egg between two parallel plates. The compression forced egg to change its shape from round to elongated. Hertwig noticed that elongated egg divides not randomly, but orthogonal to its long axis. The new daughter cells were formed along the longest axis of the cell. That observation got name of 'Hertwig's rule' or 'long axis rule'.\n\nRecent studies in animal and plant systems support the 'long axis rule'. The studied systems include the mouse embryo, Drosophila epithelium, Xenopus blastomeres (Strauss 2006), MCDK cell monolayers and plants (Gibson et al., 2011).\nThe mechanism of the 'long axis rule' relies on interphase cell long axis sensing. However, during division many animal cell types undergo cell rounding, causing the long axis to disappear as the cell becomes round. It is at this rounding stage that the decision on the orientation of the cell division is made by the spindle apparatus. The spindle apparatus then rotates in the round cell and after several minutes the spindle position is stabilised preferentially along the interphase cell long axis. The cell then divides along the spindle apparatus orientation. The first insights into how cells could remember their long axis came from studies on the Drosophila epithelium. The study indicates Cell junction#Tricellular junctions (TCJ) participation in determining the spindle orientation. TCJ localized at the regions where three or more cells meet. As cells round up during mitosis, TCJs serve as spatial landmarks. The orientation of TCJ remains stable, independent of the shape changes associated with cell rounding. The position of TCJ encode information about interphase cell shape anisotropy to orient division in the rounded mitotic cell. However this study is limited to only one type of epithelia in \"Drosophila melanogaster\" and has not been shown to be true in other epithelial types.\n\nCell divisions along 'long axis' are proposed to be implicated in the morphogenesis, tissue response to stresses and tissue architecture.\nDivision along the long cell axis reduces global tissue stress more rapidly than random divisions or divisions along the axis of mechanical stress. Long-axis division contributes to the formation of isotropic cell shapes within the monolayer.\n", "id": "51735357", "title": "Hertwig rule"}
{"url": "https://en.wikipedia.org/wiki?curid=51758582", "text": "Cell division orientation\n\nCell division orientation is the direction along which the new daughter cells are formed. Cell division orientation is important for morphogenesis, cell fate and tissue homeostasis. Abnormalities in the cell division orientation leads to the malformations during development and cancerous tissues. Factors that influence cell division orientation are cell shape , anisotropic localization of specific proteins and mechanical tensions.\n\nCell division orientation is one of the mechanisms that shapes tissue during development and morphogenesis. Along with cell shape changes, cell rearrangements, apoptosis and growth, oriented cell division modifies the geometry and topology of live tissue in order to create new organs and shape the organisms. Reproducible patterns of oriented cell divisions were described during mophogenesis of \"Drosophila\" embryos, \"Arabidopsis thaliana\" embryos, \"Drosophila\" pupa, zebrafish embryos and mouse early embryos. Oriented cell divisions contribute to the tissue elongation and the release of mechanical stress. While in the first case oriented cell division acts as active contributor to the morphogenesis, the latter case is a passive response to the external mechanical tensions.\n\nIn several tissues, such as columnar epithelium, the cells divide along the plane of the epithelium. Such divisions insert new formed cells in the epithelium layer. The disregulation of the orientation of cell divisions result in the creation of the cell out of epithelium and is observed at the initial stages of cancer.\n\nMore than a century ago Oskar Hertwig proposed that the cell division orientation is determined by the shape of the cell (1884), known as Hertwig rule. In the epithelium the cells 'reads' its shape through the specific cell junction called tricellular junctions (TCJ). TCJ provide mechanical and geometrical clues for the spindle apparatus to ensure that cell divide along its long axis. Several factors could regulate cell shape and therefore orientation of cell division. Among these factors is the anisotropic mechanical stress. This stress could be the result of the external mechanical deformation of generated intracellularly by non-isotropic localization of specific proteins.\n", "id": "51758582", "title": "Cell division orientation"}
{"url": "https://en.wikipedia.org/wiki?curid=52802727", "text": "Crossover value\n\nIn genetics, the crossover value is the linked frequency of chromosomal crossover between two gene loci (markers). For fixed set of genetic and environmental conditions, recombination in a particular region of a linkage structure (chromosome) tends to be constant and the same is then true for the crossover value which is used in the production of genetic map.\n\nCrossover implies the exchange of chromosomal segments between non-sister chromatids, in meiosis during the production of gametes. The effect is to assort the alleles on parental chromosomes, so that the gametes carry \"recombinations\" of genes different from either parent. This has the overall effect of increasing the variety of phenotypes present in a population.\n\nThe process of non-sister chromatide exchanges, including the crossover value, can be observed directly in stained cells, and indirectly by the presence or absence of genetic markers on the chromosomes. The visible \"crossovers\" are called \"chiasmata\".\n\nThe large-scale effect of crossover is to spread genetic variations within a population, as well as genetic basis for the selection of the most adaptable phenotypes \n\nThe crossover value depends on the mutual distance of the genetic loci observed.\n\nThe crossover value is equal to the recombination value or fraction when the distance between the markers in question is short.\n\n", "id": "52802727", "title": "Crossover value"}
{"url": "https://en.wikipedia.org/wiki?curid=40537197", "text": "Ferroptosis\n\nFerroptosis is a type of programmed cell death dependent on iron and characterized by the accumulation of lipid peroxides, and is genetically and biochemically distinct from other forms of regulated cell death such as apoptosis. Ferroptosis is initiated by the failure of the glutathione-dependent antioxidant defences, resulting in unchecked lipid peroxidation and eventual cell death. Lipophilic antioxidants and iron chelators can prevent ferroptotic cell death.\n\nFerroptosis plays a role in physiological and pathological cell death events such as kidney failure, iron overload, and cystine deprivation. Many cancers are susceptible to death by ferroptosis. The tumor suppressor p53 can initiate ferroptosis by preventing the import of extracellular cystine. A new study shows that ferroptosis contributes to neuronal death after hemorrhagic stroke.\n\n", "id": "40537197", "title": "Ferroptosis"}
{"url": "https://en.wikipedia.org/wiki?curid=1264722", "text": "Malignant transformation\n\nMalignant transformation is the process by which cells acquire the properties of cancer. This may occur as a primary process in normal tissue, or secondarily as \"malignant degeneration\" of a previously existing benign tumor.\n\nThere are many causes of primary malignant transformation, or tumorigenesis. Most human cancers in the United States are caused by external factors, and these factors are largely avoidable. These factors were summarized by Doll and Peto in 1981, and were still considered to be valid in 2015. These factors are listed in the table.\n\nColon cancer provides one example of the mechanisms by which diet, the top factor listed in the table, is an external factor in cancer. The Western diet of African Americans in the United States is associated with a yearly colon cancer rate of 65 per 100,000 individuals, while the high fiber/low fat diet of rural Native Africans in South Africa is associated with a yearly colon cancer rate of <5 per 100,000. Feeding the Western diet for two weeks to Native Africans increased their secondary bile acids, including carcinogenic deoxycholic acid, by 400%, and also changed the colonic microbiota. Evidence reviewed by Sun and Kato indicates that differences in human colonic microbiota play an important role in the progression of colon cancer.\n\nA second example, relating a dietary component to a cancer, is illustrated by lung cancer. Two large population-based studies were performed, one in Italy and one in the United States. In Italy, a study population of 1721 individuals diagnosed with lung cancer and no severe disease and 1918 control individuals with absence of lung cancer history or any advanced diseases. All individuals filled out a food frequency questionnaire including consumption of walnuts, hazelnuts, almonds, and peanuts, and indicating smoking status. In the United States, 495,785 members of AARP were questioned on consumption of peanuts, walnuts, seeds, or other nuts in addition to other foods and smoking status. In this U.S. study 18,533 incident lung cancer cases were identified during up to 16 years of follow-up. Overall, individuals in the highest quintile of frequency of nut consumption had a 26% lower risk of lung cancer in the Italian study and a 14% lower risk of lung cancer in the U.S. study. Similar results were obtained among individuals who were smokers.\n\nThe most important chemical compounds in smoked tobacco that are carcinogenic are those that produce DNA damage since such damage appears to be the primary underlying cause of cancer. Cunningham et al. combined the microgram weight of the compound in the smoke of one cigarette with the known genotoxic effect per microgram to identify the most carcinogenic compounds in cigarette smoke. These compounds and their genotoxic effects are listed in the article Cigarette. The top three compounds are acrolein, formaldehyde and acrylonitrile, all known carcinogens.\n\nIn 2002 the World Health Organizations International Agency for Research on Cancer estimated that 11.9% of human cancers are caused by one of seven viruses (see Oncovirus overview table). These are Epstein-Barr virus (EBV or HHV4); Kaposi's sarcoma-associated herpesvirus (KSHV or HHV8); Hepatitis B and Hepatitis C viruses (HBV and HCV); Human T-lymphotrophic virus 1 (HTLV-1); Merkel cell polyomavirus (MCPyV); and a group of alpha Human papillomaviruses (HPVs).\n\nIn 1995 epidemiologic evidence indicated that \"Helicobacter pylori\" infection increases the risk for gastric carcinoma. More recently, experimental evidence showed that infection with \"Helicobacter pylori\" cagA+ve bacterial strains results in severe degrees of inflammation and oxidative DNA damage, leading to progression to gastric cancer.\n\nPerera et al. referred to a number of articles pointing to roles of bacteria in other cancers. They pointed to single studies on the role of \"Chlamydia trachomatis\" in cervical cancer, \"Salmonella typhi\" in gallbladder cancer, and both \"Bacteroides fragilis\" and \"Fusobacterium nucleatum\" in colon cancer. Meurman has recently summarized evidence connecting oral microbiota with carcinogenesis. However, these studies are suggestive but still need further confirmation.\n\nOne underlying commonality in cancers is genetic mutation, acquired either by inheritance, or, more commonly, by mutations in one's somatic DNA over time. The mutations considered important in cancers are those that alter protein coding genes (the exome). As Vogelstein et al. point out, a typical tumor contains two to eight exome \"driver gene\" mutations, and a larger number of exome mutations that are \"passengers\" that confer no selective growth advantage.\n\nCancers also generally have genome instability, that includes a high frequency of mutations in the noncoding DNA that makes up about 98% of the human genome. The average number of DNA sequence mutations in the entire genome of breast cancer tissue is about 20,000. In an average melanom (where melanomas have a higher exome mutation frequency) the total number of DNA sequence mutations is about 80,000.\n\nA second underlying commonality in cancers is altered epigenetic regulation of transcription. In cancers, loss of gene expression occurs about 10 times more frequently by epigenetic transcription silencing (caused, for example, by promoter hypermethylation of CpG islands) than by mutations. As Vogelstein et al. point out, in a colorectal cancer there are usually about 3 to 6 driver mutations and 33 to 66 hitchhiker or passenger mutations. In contrast, the frequency of epigenetic alterations is much higher. In colon tumors compared to adjacent normal-appearing colonic mucosa, there are about 600 to 800 heavily methylated CpG islands in promoters of genes in the tumors while the corresponding CpG islands are not methylated in the adjacent mucosa. Such methylation turns off expression of a gene as completely as a mutation would. Around 60–70% of human genes have a CpG island in their promoter region. In colon cancers, in addition to hypermethylated genes, several hundred other genes have hypomethylated (under-methylated) promoters, thereby causing these genes to be turned on when they ordinarily would be turned off.\n\nEpigenetic alterations are also carried out by another major regulatory element, the microRNAs. In mammals, microRNAs (miRNAs) regulate about 60% of the transcriptional activity of protein-encoding genes. Epigenetic silencing or epigenetic over-expression of miRNA genes, caused by aberrant DNA methylation of the promoter regions controlling their expression, is a frequent event in cancer cells. Almost one third of miRNA promoters active in normal mammary cells were found to be hypermethylated in breast cancer cells, and that is a several fold greater proportion of promoters with altered methylation than is usually observed for protein coding genes. Other microRNA promoters are hypomethylated in breast cancers, and, as a result, these microRNAs are over-expressed. Several of these over-expressed microRNAs have a major influence in progression to breast cancer. BRCA1 is normally expressed in the cells of breast and other tissue, where it helps repair damaged DNA, or destroy cells if DNA cannot be repaired. BRCA1 is involved in the repair of chromosomal damage with an important role in the error-free repair of DNA double-strand breaks. BRCA1 expression is reduced or undetectable in the majority of high grade, ductal breast cancers. Only about 3%-8% of all women with breast cancer carry a mutation in BRCA1 or BRCA2. \"BRCA1\" promoter hypermethylation was present in only 13% of unselected primary breast carcinomas. However, breast cancers were found to have an average of about 100-fold increase in miR-182, compared to normal breast tissue. In breast cancer cell lines, there is an inverse correlation of BRCA1 protein levels with miR-182 expression. Thus it appears that much of the reduction or absence of BRCA1 in high grade ductal breast cancers may be due to over-expressed miR-182. (For review see ref.) In addition to miR-182, a pair of almost identical microRNAs, miR-146a and miR-146b-5p, also repress BRCA1 expression. These two microRNAs are over-expressed in triple-negative tumors and their over-expression results in BRCA1 inactivation. Thus, miR-146a and/or miR-146b-5p may also contribute to reduced expression of BRCA1 in these triple-negative breast cancers.\n\nPost-transcriptional regulation by microRNA occurs either through translational silencing of the target mRNA or through degradation of the target mRNA, via complementary binding, mostly to specific sequences in the three prime untranslated region of the target gene's mRNA. The mechanism of translational silencing or degradation of target mRNA is implemented through the RNA-induced silencing complex (RISC).\n\nSilencing of a DNA repair gene by hypermethylation or other epigenetic alteration appears to be a frequent step in progression to cancer. As summarized in a review, promoter hypermethylation of DNA repair gene \"MGMT\" occurs in 93% of bladder cancers, 88% of stomach cancers, 74% of thyroid cancers, 40%-90% of colorectal cancers and 50% of brain cancers. In addition, promoter hypermethylation of DNA repair genes \"LIG4\", \"NEIL1\", \"ATM\", \"MLH1\" or \"FANCB\" occurs at frequencies between 33% to 82% in one or more of head and neck cancers, non-small-cell lung cancers or non-small-cell lung cancer squamous cell carcinomas. Further, the article Werner syndrome ATP-dependent helicase indicates the DNA repair gene \"WRN\" has a promoter that is often hypermethylated in a variety of cancers, with \"WRN\" hypermethylation occurring in 11% to 38% of colorectal, head and neck, stomach, prostate, breast, thyroid, non-Hodgkin lymphoma, chondrosarcoma and osteosarcoma cancers.\n\nSuch silencing likely acts similarly to a germ-line mutation in a DNA repair gene, and predisposes the cell and its descendants to progression to cancer. Another review points out that when a gene necessary for DNA repair is epigenetically silenced, DNA repair would tend to be deficient and DNA damages can accumulate. Increased DNA damage can cause increased errors during DNA synthesis, leading to mutations that give rise to cancer.\n\nThe heavy metals cadmium, arsenic and nickel are all carcinogenic when present above certain levels.\n\nCadmium is known to be carcinogenic, possibly due to reduction of DNA repair. Lei et al. evaluated five DNA repair genes in rats after exposure of the rats to low levels of cadmium. They found that cadmium caused repression of three of the DNA repair genes: XRCC1 needed for base excision repair, OGG1 needed for base excision repair, and ERCC1 needed for nucleotide excision repair. Repression of these genes was not due to methylation of their promoters.\n\nArsenic carcinogenicity was reviewed by Bhattacharjee et al. They summarized the role of arsenic and its metabolites in generating oxidative stress, resulting in DNA damage. In addition to causing DNA damage, arsenic also causes repression of several DNA repair enzymes in both the base excision repair pathway and the nucleotide excision repair pathway. Bhattacharjee et al. further reviewed the role of arsenic in causing telomere dysfunction, mitotic arrest, defective apoptosis, as well as altered promoter methylation and miRNA expression. Each of these alterations could contribute to arsenic-induced carcinogenesis.\n\nNickel compounds are carcinogenic and occupational exposure to nickel is associated with an increased risk of lung and nasal cancers. Nickel compounds exhibit weak mutagenic activity, but they considerably alter the transcriptional landscape of the DNA of exposed individuals. Arita et al. examined the peripheral blood mononuclear cells of eight nickel-refinery workers and ten non-exposed workers. They found 2756 differentially expressed genes with 770 up-regulated genes and 1986 down-regulated genes. DNA repair genes were significantly over-represented among the differentially expressed genes, with 29 DNA repair genes repressed in the nickel-refinery workers and two over-expressed. The alterations in gene expression appear to be due to epigenetic alterations of histones, methylations of gene promoters, and hypermethylation of at least microRNA miR-152.\n\nMalignant transformation of cells in a benign tumor may be detected by pathologic examination of tissues. Often the clinical signs and symptoms are suggestive of a malignant tumor. The physician, during the medical history examination, can find that there have been changes in size or patient sensation and, upon direct examination, that there has been a change in the lesion itself.\n\nRisk assessments can be done and are known for certain types of benign tumor which are known to undergo malignant transformation. One of the better-known examples of this phenomenon is the progression of a nevus to melanoma.\n\n", "id": "1264722", "title": "Malignant transformation"}
{"url": "https://en.wikipedia.org/wiki?curid=5054772", "text": "Mitotic recombination\n\nMitotic recombination is a type of genetic recombination that may occur in somatic cells during their preparation for mitosis in both sexual and asexual organisms. In asexual organisms, the study of mitotic recombination is one way to understand genetic linkage because it is the only source of recombination within an individual. Additionally, mitotic recombination can result in the expression of recessive genes in an otherwise heterozygous individual. This expression has important implications for the study of tumorigenesis and lethal recessive genes.\nMitotic homologous recombination occurs mainly between sister chromatids subsequent to replication (but prior to cell division). Inter-sister homologous recombination is ordinarily genetically silent. During mitosis the incidence of recombination between non-sister homologous chromatids is only about 1% of that between sister chromatids.\nThe discovery of mitotic recombination came from the observation of twin spotting in \"Drosophila melanogaster\". This twin spotting, or mosaic spotting, was observed in \"D. melanogaster\" as early as 1925, but it was only in 1936 that Curt Stern explained it as a result of mitotic recombination. Prior to Stern's work, it was hypothesized that twin spotting happened because certain genes had the ability to eliminate the chromosome on which they were located. Later experiments uncovered when mitotic recombination occurs in the cell cycle and the mechanisms behind recombination. \n\nMitotic recombination can happen at any locus but is observable in individuals that are heterozygous at a given locus. If a crossover event between non-sister chromatids affects that locus, then both homologous chromosomes will have one chromatid containing each genotype. The resulting phenotype of the daughter cells depends on how the chromosomes line up on the metaphase plate. If the chromatids containing different alleles line up on the same side of the plate, then the resulting daughter cells will appear heterozygous and be undetectable, despite the crossover event. However, if chromatids containing the same alleles line up on the same side, the daughter cells will be homozygous at that locus. This results in \"twin spotting\", where one cell presents the homozygous recessive phenotype and the other cell has the homozygous wild type phenotype. If those daughter cells go on to replicate and divide, the twin spots will continue to grow and reflect the differential phenotype.\n\nMitotic recombination takes place during interphase. It has been suggested that recombination takes place during G1, when the DNA is in its 2-strand phase, and replicated during DNA synthesis. It is also possible to have the DNA break leading to mitotic recombination happen during G1, but for the repair to happen after replication.\n\nIn the budding yeast \"Saccharomyces cerevisiae\", mutations in several genes needed for mitotic (and meiotic) recombination cause increased sensitivity to inactivation by radiation and/or genotoxic chemicals. For example, gene \"rad52\" is required for mitotic recombination as well as meiotic recombination. \"Rad52\" mutant yeast cells have increased sensitivity to killing by X-rays, methyl methanesulfonate and the DNA crosslinking agent 8-methoxypsoralen-plus-UV light, suggesting that mitotic recombinational repair is required for removal of the different DNA damages caused by these agents.\n\nThe mechanisms behind mitotic recombination are similar to those behind meiotic recombination. These include sister chromatid exchange and mechanisms related to DNA double strand break repair by homologous recombination such as single-strand annealing, synthesis-dependent strand annealing (SDSA), and gene conversion through a double-Holliday Junction intermediate or SDSA. In addition, non-homologous mitotic recombination is a possibility and can often be attributed to non-homologous end joining.\n\nThere are several theories on how mitotic crossover occurs. In the simple crossover model, the two homologous chromosomes overlap on or near a common Chromosomal fragile site (CFS). This leads to a double-strand break, which is then repaired using one of the two strands. This can lead to the two chromatids switching places. In another model, two overlapping sister chromosomes form a double Holliday junction at a common repeat site and are later sheared in such a way that they switch places. In either model, the chromosomes are not guaranteed to trade evenly, or even to rejoin on opposite sides thus most patterns of cleavage do not result in any crossover event. Uneven trading introduces many of the deleterious effects of mitotic crossover.\n\nAlternatively, a crossover can occur during DNA repair if, due to extensive damage, the homologous chromosome is chosen to be the template over the sister chromatid. This leads to gene synthesis since one copy of the allele is copied across from the homologous chromosome and then synthesized into the breach on the damaged chromosome. The net effect of this would be one heterozygous chromosome and one homozygous chromosome.\n\nMitotic crossover is known to occur in \"D. melanogaster\", some asexually reproducing fungi and in normal human cells, where the event may allow normally recessive cancer-causing genes to be expressed and thus predispose the cell in which it occurs to the development of cancer. Alternately, a cell may become a homozygous mutant for a tumor-suppressing gene, leading to the same result. For example, Bloom's syndrome is caused by a mutation in RecQ helicase, which plays a role in DNA replication and repair. This mutation leads to high rates of mitotic recombination in mice, and this recombination rate is in turn responsible for causing tumor susceptibility in those mice. At the same time, mitotic recombination may be beneficial: it may play an important role in repairing double stranded breaks, and it may be beneficial to the organism if having homozygous dominant alleles is more functional than the heterozygous state. For use in experimentation with genomes in model organisms such as \"Drosophila melanogaster\", mitotic recombination can be induced via X-ray and the FLP-FRT recombination system.\n\n", "id": "5054772", "title": "Mitotic recombination"}
{"url": "https://en.wikipedia.org/wiki?curid=36869", "text": "Cell division\n\nCell division is the process by which a parent cell divides into two or more daughter cells. Cell division usually occurs as part of a larger cell cycle. In eukaryotes, there are two distinct types of cell division: a vegetative division, whereby each daughter cell is genetically identical to the parent cell (mitosis), and a reproductive cell division, whereby the number of chromosomes in the daughter cells is reduced by half to produce haploid gametes (meiosis). Meiosis results in four haploid daughter cells by undergoing one round of DNA replication followed by two divisions. Homologous chromosomes are separated in the first division, and sister chromatids are separated in the second division. Both of these cell division cycles are used in the process of sexual reproduction at some point in their life cycle. Both are believed to be present in the last eukaryotic common ancestor. Prokaryotes undergo a vegetative cell division known as binary fission, where their genetic material is segregated equally into two daughter cells. All cell divisions, regardless of organism, are preceded by a single round of DNA replication.\n\nFor simple unicellular microorganisms such as the amoeba, one cell division is equivalent to reproduction – an entire new organism is created. On a larger scale, mitotic cell division can create progeny from multicellular organisms, such as plants that grow from cuttings. Mitotic cell division enables sexually reproducing organisms to develop from the one-celled zygote, which itself was produced by meiotic cell division from gametes. After growth, cell division by mitosis allows for continual construction and repair of the organism. The human body experiences about 10 quadrillion cell divisions in a lifetime.\n\nThe primary concern of cell division is the maintenance of the original cell's genome. Before division can occur, the genomic information that is stored in chromosomes must be replicated, and the duplicated genome must be separated cleanly between cells. A great deal of cellular infrastructure is involved in keeping genomic information consistent between generations.\n\nInterphase is the process a cell must go through before mitosis, meiosis, and cytokinesis. Interphase consists of four main stages: G1, S, G0, and G2. G1 is a time of growth for the cell. If the cell does not progress through G1, the cell then enters a stage called G0. In G0, cells are still living but they are put on hold. The cells may later be called back into interphase if needed at a later time. There are checkpoints during interphase that allow the cell to be either progressed or denied further development. In S phase, the chromosomes are replicated in order for the genetic content to be maintained. During G2, the cell undergoes the final stages of growth before it enters the M phase. The M phase, can be either mitosis or meiosis depending on the type of cell. Germ cells undergo meiosis, while somatic cells will undergo mitosis. After the cell proceeds successfully through the M phase, it may then undergo cell division through cytokinesis. The control of each checkpoint is controlled by cyclin and cyclin dependent kinases. The progression of interphase is the result of the increased amount of cyclin. As the amount of cyclin increases, more and more cyclin dependent kinases attach to cyclin signaling the cell further into interphase. The peak of the cyclin attached to the cyclin dependent kinases this system pushes the cell out of interphase and into the M phase, where mitosis, meiosis, and cytokinesis occur.\n\nProphase is the first stage of division. The nuclear envelope is broken down, long strands of chromatin condense to form shorter more visible strands called chromosomes, the nucleolus disappears, and microtubules attach to the chromosomes at the kinetochores present in the centromere. Microtubules associated with the alignment and separation of chromosomes are referred to as the spindle and spindle fibers. Chromosomes will also be visible under a microscope and will be connected at the centromere. During this condensation and alignment period, homologous chromosomes may swap portions of their DNA in a process known as crossing over.\n\nMetaphase is the stage in cell division when the chromosomes line up in the middle of the cell by MTOCs ( microtubule organizing center) by pushing and pulling on centromeres of both chromatids which causes the chromosome to move to the center. The chromosomes are still condensing and are currently at one step away from being the most coiled and condensed they will be. Spindle and spindle fibers have already connected to the kinetochores. At this point, the chromosomes are ready to split into opposite poles of the cell towards the spindle to which they are connected.\nAnaphase is a very short stage of the cell cycle and occurs after the chromosomes align at the mitotic plate. After the chromosomes line up in the middle of the cell, the spindle fibers will pull them apart. The chromosomes are split apart as the sister chromatids move to opposite sides of the cell.\n\nTelophase is the last stage of the cell cycle. Two cells form around the chromatin at the two poles of the cell. Two nuclear membranes begin to reform and the chromatin begin to unwind.\n\nCells are broadly classified into two main categories: simple, non-nucleated prokaryotic cells, and complex, nucleated eukaryotic cells. Owing to their structural differences, eukaryotic and prokaryotic cells do not divide in the same way. Also, the pattern of cell division that transforms eukaryotic stem cells into gametes (sperm cells in males or egg cells in females), termed meiosis, is different from that of the division of somatic cells in the body.\n\nMulticellular organisms replace worn-out cells through cell division. In some animals, however, cell division eventually halts. In humans this occurs, on average, after 52 divisions, known as the Hayflick limit. The cell is then referred to as senescent. Cells stop dividing because the telomeres, protective bits of DNA on the end of a chromosome required for replication, shorten with each copy, eventually being consumed. Cancer cells, on the other hand, are not thought to degrade in this way, if at all. An enzyme called telomerase, present in large quantities in cancerous cells, rebuilds the telomeres, allowing division to continue indefinitely.\n\nA cell division under microscope was first discovered by German botanist Hugo von Mohl in 1835 as he worked over Green algae \"Cladophora glomerata\".\n\n\n\n", "id": "36869", "title": "Cell division"}
{"url": "https://en.wikipedia.org/wiki?curid=53877635", "text": "Action potential pulse\n\nAn action potential pulse is a mathematically and experimentally correct Synchronized Oscillating Lipid Pulse coupled with an Action Potential. This is a continuation of Hodgkin Huxley's work in 1952 with the inclusion of accurately modelling ion channel proteins, including their dynamics and speed of activation.\n\nThe action potential pulse is a model of the speed an action potential that is dynamically dependant upon the position and number of ion channels, and the shape and make up of the axon. The action potential pulse model takes into account entropy and the conduction speed of the action potential along an axon. It is an addition to the Hodgkin Huxley model.\n\nInvestigation into the membranes of axons have shown that the spaces in between the channels are sufficiently large, such that cable theory cannot apply to them, because it depends upon the capacitance potential of a membrane to be transferred almost instantly to other areas of the membrane surface. In electrical circuits this can happen because of the special properties of electrons, which are negatively charged, whereas in membrane biophysics potential is defined by positively charged ions instead. These ions are usually Na or Ca, which move slowly by diffusion and have limited ionic radii in which they can affect adjacent ion channels. It is mathematically impossible for these positive ions to move from one channel to the next, in the time required by the action potential flow model, due to instigated depolarization. Furthermore entropy measurements have long demonstrated that an action potential's flow starts with a large increase in entropy followed by a steadily decreasing state, which does not match the Hodgkin Huxley theory. In addition a soliton pulse is known to flow at the same rate and follow the action potential. From measurements of the speed of an action potential, hyperpolarization must have a further component of which the 'soliton' mechanical pulse is the only candidate.\n\nThe resulting action potential pulse therefore is a synchronized, coupled pulse with the entropy from depolarization at one channel providing sufficient entropy for a pulse to travel to sequential channels and mechanically open them.\n\nThis mechanism explains the speed of transmission through both myelinated and unmyelinated axons.\n\nThis is a timed pulse, that combines the entropy from ion transport with the efficiency of a flowing pulse.\n\nThe action potential pulse model has many advantages over the simpler Hodgkin Huxley version including evidence, efficiency, timing entropy measurements, and the explanation of nerve impulse flow through myelinated axons.\n\nMyelinated axons\n\nThis model replaces saltatory conduction, which was a historical theory that relied upon cable theory to explain conduction, and was an attempt at a model that has no basis is either physiology or membrane biophysics.\n\nIn myelinated axons the myelin acts as a mechanical transducer preserving the entropy of the pulse and insulating against mechanical loss. In this model the nodes of Ranvier (where ion channels are highly concentrated) concentrate the ion channels providing maximum entropy to instigate a pulse that travels from node to node along the axon with the entropy being preserved by the shape and dynamics of the myelin sheath.\n", "id": "53877635", "title": "Action potential pulse"}
{"url": "https://en.wikipedia.org/wiki?curid=12794834", "text": "Parasexual cycle\n\nThe parasexual cycle, a process peculiar to fungi and single-celled organisms, is a nonsexual mechanism of parasexuality for transferring genetic material without meiosis or the development of sexual structures. It was first described by Italian geneticist Guido Pontecorvo in 1956 during studies on \"Aspergillus nidulans\" (also called \"Emericella nidulans\" when referring to its sexual form, or teleomorph). A parasexual cycle is initiated by the fusion of hyphae (anastomosis) during which nuclei and other cytoplasmic components occupy the same cell (heterokaryosis and plasmogamy). Fusion of the unlike nuclei in the cell of the heterokaryon results in formation of a diploid nucleus (karyogamy), which is believed to be unstable and can produce segregants by recombination involving mitotic crossing-over and haploidization. Mitotic crossing-over can lead to the exchange of genes on chromosomes; while haploidization probably involves mitotic nondisjunctions which randomly reassort the chromosomes and result in the production of aneuploid and haploid cells. Like a sexual cycle, parasexuality gives the species the opportunity to recombine the genome and produce new genotypes in their offspring. Unlike a sexual cycle, the process lacks coordination and is exclusively mitotic.\n\nThe parasexual cycle resembles sexual reproduction. In both cases, unlike hyphae (or modifications thereof) may fuse (plasmogamy) and their nuclei will occupy the same cell. The unlike nuclei fuse (karyogamy) to form a diploid (zygote) nucleus. In contrast to the sexual cycle, in the parasexual cycle recombination takes place during mitosis followed by haploidization (but without meiosis). The recombined haploid nuclei appear among vegetative cells, which differ genetically from those of the parent mycelium.\n\nBoth heterokaryosis and the parasexual cycle are very important for those fungi that have no sexual reproduction. Those cycles provide for somatic variation in the vegetative phase of their life cycles. This is also true for fungi where the sexual phase is present, although in this case, additional and significant variation is incorporated through the sexual reproduction.\n\nOccasionally, two haploid nuclei fuse to form a diploid nucleus—with two homologous copies of each chromosome. The mechanism is largely unknown, and it seems to be a relatively rare event, but once a diploid nucleus has been formed it can be very stable and divide to form further diploid nuclei, along with the normal haploid nuclei. Thus the heterokaryon consists of a mixture of the two original haploid nuclear types as well as diploid fusion nuclei.\n\nChiasma formation is common in meiosis, where two homologous chromosomes break and rejoin, leading to chromosomes that are hybrids of the parental types. It can also occur during mitosis but at a much lower frequency because the chromosomes do not pair in a regular arrangement. Nevertheless, the result will be the same when it does occur—the recombination of genes.\n\nOccasionally, nondisjunction of chromosomes occurs during division of a diploid nucleus, so that one of the daughter nuclei has one chromosome too many (2n+1) and the other has one chromosome too few (2n–1). Such nuclei with incomplete multiples of the haploid number are termed \"aneuploid\", as they do not have even chromosome number sets such as n or 2n. They tend to be unstable and to lose further chromosomes during subsequent mitotic divisions, until the 2n+1 and 2n-1 nuclei progressively revert to n. Consistent with this, in \"E. nidulans\" (where normally, n=8) nuclei have been found with 17 (2n+1), 16 (2n), 15 (2n–1), 12, 11, 10, and 9 chromosomes.\n\nEach of these events is relatively rare, and they do not constitute a regular cycle like the sexual cycle. But the outcome would be similar. Once a diploid nucleus has formed by fusion of two haploid nuclei from different parents, the parental genes can potentially recombine. And, the chromosomes that are lost from an aneuploid nucleus during its reversion to a euploid could be a mixture of those in the parental strain.\n\nThe potential to undergo a parasexual cycle under laboratory conditions has been demonstrated in many species of filamentous fungi, including \"Fusarium monoliforme\", \"Penicillium roqueforti\" (used in making blue cheeses), \"Verticillium dahliae\", \"Verticillium alboatrum\", \"Pseudocercosporella herpotrichoides\", \"Ustilago scabiosae\", \"Magnaporthe grisea\", \"Cladosporium fulvum\", and the human pathogens \"Candida albicans\" and \"Candida tropicalis\".\n\nParasexuality has become a useful tool for industrial mycologists to produce strains with desired combinations of properties. Its significance in nature is largely unknown and will depend on the frequency of heterokaryosis, determined by cytoplasmic incompatibility barriers.\n", "id": "12794834", "title": "Parasexual cycle"}
{"url": "https://en.wikipedia.org/wiki?curid=1680989", "text": "Effector cell\n\nAn effector cell is any of various types of cell that actively responds to a stimulus and effects some change (brings it about).\n\nExamples of effector cells include:\n\n\nAs an effector cell, cytokine-induced killer cells can recognize infected or malignant cells even when antibodies and MHC are not available. This allows a quick immune reaction to take place. Cytokine-Induced killer cells are important because harmful cells that do not contain MHC cannot be traced and removed by other immune cells. CIK cells are being studied intensely as a possible therapy treatment for cancer and other types of viral infections. CIK cells respond to lymphokines by lysing tumorous cells that are resistant to NK cells or LAK cell activity. CIK cells show a large amount of cytotoxic potential against various types of tumors. Side effects of CIK cells are also considered very minor. In a few cases, CIK cell treatment lead to the complete disappearance of tumor burdens, extended periods of survival, and improved quality of life, even if the cancerous tumor cells were in advanced stages. At the moment, the exact mechanism of tumor recognition in CIK cells are not completely understood.\n\nFibroblast are types of cells that form the extracellular matrix and collagen. Fibroblasts are the most common connective tissues in animals. They have branched cytoplasm surrounding their nucleus, which contain two or more nucleoli. Fibroblasts play a key role when responding to tissue injury. They initiate inflammation in the presence of foreign microorganisms. Receptors found on the surface of fibroblasts regulate hematopoietic cells, start chemokine synthesis, and provide a pathway that allows immune cells to regulate the fibroblast cells. Fibroblasts are also known as tumor mediators. They suppress the tumor as an inflammatory response.\n\nMicroglia are located throughout the brain and spinal cord. They are the first line of immune defense in the CNS. Microglia are of utmost importance in brain maintenance. They constantly search around the CNS for any type of plaques, damaged neurons, and infections. Microglia are extremely sensitive forms of effector cells, because they must be alert enough to address possible life-threatening damage. This sensitivity is caused by unique forms of potassium channels. Microglia must always be capable of recognizing any foreign bodies, engulf them, and activate T-cells. Microglia can be found under a variety of different shapes and sizes, based on the location where they are found. The vast amount of shapes are required for the microglia to carry out their primary function. Microglia are distinguishable from macrophages because of their ability to transform, which allows them to protect the CNS under relatively short amounts of time. Microglia take on a unique phenotype when they detect local chemical signals. Microglia have a variety of different functions required to maintain homeostasis in the host body.\n\nA mast cell is a white blood cell. Mast cells are protective cells that are involved in wound healing and blood-brain barrier function. Mast cells are very similar to basophils, and mast cells once were mistaken for them. It is proven that the two cells have different lineages. Mast cells respond to pathogenic parasites through Immunoglobin E signaling. These cells play a role in the inflammatory process. They can either release selective amounts or rapid amounts of compounds that induce inflammation from granules. Mast cells are inactive during allergic reactions unless an allergen binds to Immunoglobin E.\n", "id": "1680989", "title": "Effector cell"}
{"url": "https://en.wikipedia.org/wiki?curid=55008417", "text": "Tissue nanotransfection\n\nTissue nanotransfection (TNT) an \"in vivo\" research technique that its creators claim reprograms skin cells into other cell types. It uses a small electrical current to transfer DNA into pores in the cellular membrane. The DNA then activates existing genes that trigger the change.\n\nThe approach uses a stamp-sized silicon DNA reservoir and injector. Arrays of nanochannels connect to microscopic pits where the DNA is held. The electric current lasts for ten-milliseconds. The nanochannels allow for more precise control of the distribution of the DNA.\n\nThe injected DNA is selected for its ability to activate genes already present in the skin cells that cause the cell to become the desired cell type. The transformed cells then transform adjacent cells by releasing fatty membrane bubbles onto them. Harvesting these bubbles and injecting them into mice subjected to experimental stroke, triggered the mouse brain to generate new neurons and repair itself.\n\nThe conversion is direct, in that it does not require the target cells to first revert to stem cells before re-maturing in to the desired type.\n\nThe technique was announced in 2017. The researchers claimed to have created neurons and blood vessels.\n\nEarlier techniques used viruses to deliver DNA into cells \"in vitro\". That technique has been used in cancer research to reprogram T cells to attack tumor cells. In some animal studies, this approach instead created undesirable mutations. When used for so-called bulk transfection, many cells received too much or too little DNA, which did not occur with TNT.\n\n", "id": "55008417", "title": "Tissue nanotransfection"}
{"url": "https://en.wikipedia.org/wiki?curid=7252", "text": "Cell cycle\n\nThe cell cycle or cell-division cycle is the series of events that take place in a cell leading to its division and duplication of its DNA (DNA replication) to produce two daughter cells. In bacteria, which lack a cell nucleus, the cell cycle is divided into the B, C, and D periods. The B period extends from the end of cell division to the beginning of DNA replication. DNA replication occurs during the C period. The D period refers to the stage between the end of DNA replication and the splitting of the bacterial cell into two daughter cells. In cells with a nucleus, as in eukaryotes, the cell cycle is also divided into three periods: interphase, the mitotic (M) phase, and cytokinesis. During interphase, the cell grows, accumulating nutrients needed for mitosis, preparing it for cell division and duplicating its DNA. During the mitotic phase, the chromosomes separate. During the final stage, cytokinesis, the chromosomes and cytoplasm separate into two new daughter cells. To ensure the proper division of the cell, there are control mechanisms known as cell cycle checkpoints.\n\nThe cell-division cycle is a vital process by which a single-celled fertilized egg develops into a mature organism, as well as the process by which hair, skin, blood cells, and some internal organs are renewed. After cell division, each of the daughter cells begin the interphase of a new cycle. Although the various stages of interphase are not usually morphologically distinguishable, each phase of the cell cycle has a distinct set of specialized biochemical processes that prepare the cell for initiation of cell divisions.\n\nThe cell cycle consists of four distinct phases: G phase, S phase (synthesis), G phase (collectively known as interphase) and M phase (mitosis). M phase is itself composed of two tightly coupled processes: karyokinesis, in which the cell's chromosomes are divided, and cytokinesis, in which the cell's cytoplasm divides forming two daughter cells. Activation of each phase is dependent on the proper progression and completion of the previous one. Cells that have temporarily or reversibly stopped dividing are said to have entered a state of quiescence called G phase.\n\nAfter cell division, each of the daughter cells begin the interphase of a new cycle. Although the various stages of interphase are not usually morphologically distinguishable, each phase of the cell cycle has a distinct set of specialized biochemical processes that prepare the cell for initiation of cell division.\n\nG is a resting phase where the cell has left the cycle and has stopped dividing. The cell cycle starts with this phase. The word \"post-mitotic\" is sometimes used to refer to both quiescent and senescent cells. Non-proliferative (non-dividing) cells in multicellular eukaryotes generally enter the quiescent G state from G and may remain quiescent for long periods of time, possibly indefinitely (as is often the case for neurons). This is very common for cells that are fully differentiated. Cellular senescence occurs in response to DNA damage and external stress and usually constitutes an arrest in G. Some cells enter the G phase semi-permanently and are considered post-mitotic, e.g., some liver, kidney, and stomach cells. Many cells do not enter G and continue to divide throughout an organism's life, e.g., epithelial cells.\n\nCellular senescence is also a state that occurs in response to DNA damage or degradation that would make a cell's progeny nonviable; it is often a biochemical alternative to the self-destruction of such a damaged cell by apoptosis.\n\nBefore a cell can enter cell division, it needs to take in nutrients. All of the preparations are done during interphase. Interphase is a series of changes that takes place in a newly formed cell and its nucleus, before it becomes capable of division again. It is also called preparatory phase or intermitosis. Previously it was called resting stage because there is no apparent activity related to cell division.Typically interphase lasts for at least 90% of the total time required for the cell cycle.\n\nInterphase proceeds in three stages, G, S, and G, followed by the cycle of mitosis and cytokinesis. The cell's nuclear DNA contents are duplicated during S phase.\n\nThe first phase within interphase, from the end of the previous M phase until the beginning of DNA synthesis, is called G (G indicating \"gap\"). It is also called the growth phase. During this phase, the biosynthetic activities of the cell, which are considerably slowed down during M phase, resume at a high rate. The duration of G is highly variable, even among different cells of the same species. In this phase, the cell increases its supply of proteins, increases the number of organelles (such as mitochondria, ribosomes), and grows in size. In G phase, a cell has three options. \n(1) To continue cell cycle and enter S phase\n(2) Stop cell cycle and enter G phase for undergoing differentiation.\n(3) Get arrested in G phase hence it may enter G phase or re-enter cell cycle. \nThe deciding factor is availability of nitrogens and storage of energy rich compounds at the deciding point called check point. This check point is called G cyclin or C. \nIt causes transition of G to S phase. Once the check point of G phase is crossed, cell cycle will go uninterrupted till it is completed.\n\nThe ensuing S phase starts when DNA synthesis commences; when it is complete, all of the chromosomes have been replicated, i.e., each chromosome has two (sister) chromatids. Thus, during this phase, the amount of DNA in the cell has effectively doubled, though the ploidy of the cell remains the same. Rates of RNA transcription and protein synthesis are very low during this phase. An exception to this is histone production, most of which occurs during the S phase.\n\nG2 phase occurs after DNA replication and is a period of protein synthesis and rapid cell growth to prepare the cell for mitosis.\nDuring this phase microtubules begin to reorganize to form a spindle.\n\nThe relatively brief \"M phase\" consists of nuclear division (karyokinesis). It is a relatively short period of the cell cycle. M phase is complex and highly regulated. The sequence of events is divided into phases, corresponding to the completion of one set of activities and the start of the next. These phases are sequentially known as:\nMitosis is the process by which a eukaryotic cell separates the chromosomes in its cell nucleus into two identical sets in two nuclei. During the process of mitosis the pairs of chromosomes condense and attach to fibers that pull the sister chromatids to opposite sides of the cell.\n\nMitosis occurs exclusively in eukaryotic cells, but occurs in different ways in different species. For example, animals undergo an \"open\" mitosis, where the nuclear envelope breaks down before the chromosomes separate, while fungi such as \"Aspergillus nidulans\" and \"Saccharomyces cerevisiae\" (yeast) undergo a \"closed\" mitosis, where chromosomes divide within an intact cell nucleus. Prokaryotic cells, which lack a nucleus, divide by a process called binary fission.\n\nMitosis is immediately followed by cytokinesis, which divides the nuclei, cytoplasm, organelles and cell membrane into two cells containing roughly equal shares of these cellular components. Mitosis and cytokinesis together define the division of the mother cell into two daughter cells, genetically identical to each other and to their parent cell. This accounts for approximately 10% of the cell cycle.\n\nBecause cytokinesis usually occurs in conjunction with mitosis, \"mitosis\" is often used interchangeably with \"M phase\". However, there are many cells where mitosis and cytokinesis occur separately, forming single cells with multiple nuclei in a process called endoreplication. This occurs most notably among the fungi and slime moulds, but is found in various groups. Even in animals, cytokinesis and mitosis may occur independently, for instance during certain stages of fruit fly embryonic development. Errors in mitosis can either kill a cell through apoptosis or cause mutations that may lead to cancer.\n\nThe process of mitosis is complex and highly regulated. The sequence of events is divided into phases, corresponding to the completion of one set of activities and the start of the next. These stages are prophase, prometaphase, metaphase, anaphase and telophase. During the process of mitosis the pairs of chromosomes condense and attach to fibers that pull the sister chromatids to opposite sides of the cell. The cell then divides in cytokinesis, to produce two identical daughter cells.\n\nRegulation of the cell cycle involves processes crucial to the survival of a cell, including the detection and repair of genetic damage as well as the prevention of uncontrolled cell division. The molecular events that control the cell cycle are ordered and directional; that is, each process occurs in a sequential fashion and it is impossible to \"reverse\" the cycle.\n\nTwo key classes of regulatory molecules, cyclins and cyclin-dependent kinases (CDKs), determine a cell's progress through the cell cycle. Leland H. Hartwell, R. Timothy Hunt, and Paul M. Nurse won the 2001 Nobel Prize in Physiology or Medicine for their discovery of these central molecules. Many of the genes encoding cyclins and CDKs are conserved among all eukaryotes, but in general more complex organisms have more elaborate cell cycle control systems that incorporate more individual components. Many of the relevant genes were first identified by studying yeast, especially \"Saccharomyces cerevisiae\"; genetic nomenclature in yeast dubs many of these genes \"cdc\" (for \"cell division cycle\") followed by an identifying number, e.g. \"cdc25\" or \"cdc20\".\n\nCyclins form the regulatory subunits and CDKs the catalytic subunits of an activated heterodimer; cyclins have no catalytic activity and CDKs are inactive in the absence of a partner cyclin. When activated by a bound cyclin, CDKs perform a common biochemical reaction called phosphorylation that activates or inactivates target proteins to orchestrate coordinated entry into the next phase of the cell cycle. Different cyclin-CDK combinations determine the downstream proteins targeted. CDKs are constitutively expressed in cells whereas cyclins are synthesised at specific stages of the cell cycle, in response to various molecular signals.\n\nUpon receiving a pro-mitotic extracellular signal, G cyclin-CDK complexes become active to prepare the cell for S phase, promoting the expression of transcription factors that in turn promote the expression of S cyclins and of enzymes required for DNA replication. The G cyclin-CDK complexes also promote the degradation of molecules that function as S phase inhibitors by targeting them for ubiquitination. Once a protein has been ubiquitinated, it is targeted for proteolytic degradation by the proteasome. However, results from a recent study of E2F transcriptional dynamics at the single-cell level argue that the role of G1 cyclin-CDK activities, in particular cyclin D-CDK4/6, is to tune the timing rather than the commitment of cell cycle entry.\n\nActive S cyclin-CDK complexes phosphorylate proteins that make up the pre-replication complexes assembled during G phase on DNA replication origins. The phosphorylation serves two purposes: to activate each already-assembled pre-replication complex, and to prevent new complexes from forming. This ensures that every portion of the cell's genome will be replicated once and only once. The reason for prevention of gaps in replication is fairly clear, because daughter cells that are missing all or part of crucial genes will die. However, for reasons related to gene copy number effects, possession of extra copies of certain genes is also deleterious to the daughter cells.\n\nMitotic cyclin-CDK complexes, which are synthesized but inactivated during S and G phases, promote the initiation of mitosis by stimulating downstream proteins involved in chromosome condensation and mitotic spindle assembly. A critical complex activated during this process is a ubiquitin ligase known as the anaphase-promoting complex (APC), which promotes degradation of structural proteins associated with the chromosomal kinetochore. APC also targets the mitotic cyclins for degradation, ensuring that telophase and cytokinesis can proceed.\n\nCyclin D is the first cyclin produced in the cell cycle, in response to extracellular signals (e.g. growth factors). Cyclin D binds to existing CDK4, forming the active cyclin D-CDK4 complex. Cyclin D-CDK4 complex in turn phosphorylates the retinoblastoma susceptibility protein (Rb). The hyperphosphorylated Rb dissociates from the E2F/DP1/Rb complex (which was bound to the E2F responsive genes, effectively \"blocking\" them from transcription), activating E2F. Activation of E2F results in transcription of various genes like cyclin E, cyclin A, DNA polymerase, thymidine kinase, etc. Cyclin E thus produced binds to CDK2, forming the cyclin E-CDK2 complex, which pushes the cell from G to S phase (G/S, which initiates the G/M transition). Cyclin B-cdk1 complex activation causes breakdown of nuclear envelope and initiation of prophase, and subsequently, its deactivation causes the cell to exit mitosis.\nA quantitative study of E2F transcriptional dynamics at the single-cell level by using engineered fluorescent reporter cells provided a quantitative framework for understanding the control logic of cell cycle entry, challenging the canonical textbook model. Genes that regulate the amplitude of E2F accumulation, such as Myc, determine the commitment into cell cycle and S phase entry. G1 cyclin-CDK activities are not the driver of cell cycle entry. Instead, they primarily tune the timing of E2F increase, thereby modulating the pace of cell cycle progression.\n\nTwo families of genes, the \"cip/kip\" (\"CDK interacting protein/Kinase inhibitory protein\") family and the INK4a/ARF (\"In\"hibitor of \"K\"inase 4/\"A\"lternative \"R\"eading \"F\"rame) family, prevent the progression of the cell cycle. Because these genes are instrumental in prevention of tumor formation, they are known as tumor suppressors.\n\nThe \"cip/kip\" family includes the genes p21, p27 and p57. They halt cell cycle in G phase, by binding to, and inactivating, cyclin-CDK complexes. p21 is activated by p53 (which, in turn, is triggered by DNA damage e.g. due to radiation). p27 is activated by Transforming Growth Factor of β (TGF β), a growth inhibitor.\n\nThe INK4a/ARF family includes p16, which binds to CDK4 and arrests the cell cycle in G phase, and p14 which prevents p53 degradation.\n\nSynthetic inhibitors of Cdc25 could also be useful for the arrest of cell cycle and therefore be useful as antineoplastic and anticancer agents.\n\nCurrent evidence suggests that a semi-autonomous transcriptional network acts in concert with the CDK-cyclin machinery to regulate the cell cycle. Several gene expression studies in \"Saccharomyces cerevisiae\" have identified 800–1200 genes that change expression over the course of the cell cycle. They are transcribed at high levels at specific points in the cell cycle, and remain at lower levels throughout the rest of the cycle. While the set of identified genes differs between studies due to the computational methods and criteria used to identify them, each study indicates that a large portion of yeast genes are temporally regulated.\n\nMany periodically expressed genes are driven by transcription factors that are also periodically expressed. One screen of single-gene knockouts identified 48 transcription factors (about 20% of all non-essential transcription factors) that show cell cycle progression defects. Genome-wide studies using high throughput technologies have identified the transcription factors that bind to the promoters of yeast genes, and correlating these findings with temporal expression patterns have allowed the identification of transcription factors that drive phase-specific gene expression. The expression profiles of these transcription factors are driven by the transcription factors that peak in the prior phase, and computational models have shown that a CDK-autonomous network of these transcription factors is sufficient to produce steady-state oscillations in gene expression).\n\nExperimental evidence also suggests that gene expression can oscillate with the period seen in dividing wild-type cells independently of the CDK machinery. Orlando \"et al.\" used microarrays to measure the expression of a set of 1,271 genes that they identified as periodic in both wild type cells and cells lacking all S-phase and mitotic cyclins (\"clb1,2,3,4,5,6\"). Of the 1,271 genes assayed, 882 continued to be expressed in the cyclin-deficient cells at the same time as in the wild type cells, despite the fact that the cyclin-deficient cells arrest at the border between G and S phase. However, 833 of the genes assayed changed behavior between the wild type and mutant cells, indicating that these genes are likely directly or indirectly regulated by the CDK-cyclin machinery. Some genes that continued to be expressed on time in the mutant cells were also expressed at different levels in the mutant and wild type cells. These findings suggest that while the transcriptional network may oscillate independently of the CDK-cyclin oscillator, they are coupled in a manner that requires both to ensure the proper timing of cell cycle events. Other work indicates that phosphorylation, a post-translational modification, of cell cycle transcription factors by Cdk1 may alter the localization or activity of the transcription factors in order to tightly control timing of target genes.\n\nWhile oscillatory transcription plays a key role in the progression of the yeast cell cycle, the CDK-cyclin machinery operates independently in the early embryonic cell cycle. Before the midblastula transition, zygotic transcription does not occur and all needed proteins, such as the B-type cyclins, are translated from maternally loaded mRNA.\n\nAnalyses of synchronized cultures of \"Saccharomyces cerevisiae\" under conditions that prevent DNA replication initiation without delaying cell cycle progression showed that origin licensing decreases the expression of genes with origins near their 3' ends, revealing that downstream origins can regulate the expression of upstream genes. This confirms previous predictions from mathematical modeling of a global causal coordination between DNA replication origin activity and mRNA expression, and shows that mathematical modeling of DNA microarray data can be used to correctly predict previously unknown biological modes of regulation.\n\nCell cycle checkpoints are used by the cell to monitor and regulate the progress of the cell cycle. Checkpoints prevent cell cycle progression at specific points, allowing verification of necessary phase processes and repair of DNA damage. The cell cannot proceed to the next phase until checkpoint requirements have been met. Checkpoints typically consist of a network of regulatory proteins that monitor and dictate the progression of the cell through the different stages of the cell cycle.\n\nThere are several checkpoints to ensure that damaged or incomplete DNA is not passed on to daughter cells. Three main checkpoints exist: the G/S checkpoint, the G/M checkpoint and the metaphase (mitotic) checkpoint.\n\nG/S transition is a rate-limiting step in the cell cycle and is also known as restriction point. This is where the cell checks whether it has enough raw materials to fully replicate its DNA (nucleotide bases, DNA synthase, chromatin, etc.). An unhealthy or malnourished cell will get stuck at this checkpoint.\n\nThe G/M checkpoint is where the cell ensures that it has enough cytoplasm and phospholipids for two daughter cells. But sometimes more importantly, it checks to see if it is the right time to replicate. There are some situations where many cells need to all replicate simultaneously (for example, a growing embryo should have a symmetric cell distribution until it reaches the mid-blastula transition). This is done by controlling the G/M checkpoint.\n\nThe metaphase checkpoint is a fairly minor checkpoint, in that once a cell is in metaphase, it has committed to undergoing mitosis. However that's not to say it isn't important. In this checkpoint, the cell checks to ensure that the spindle has formed and that all of the chromosomes are aligned at the spindle equator before anaphase begins.\n\nWhile these are the three \"main\" checkpoints, not all cells have to pass through each of these checkpoints in this order to replicate. Many types of cancer are caused by mutations that allow the cells to speed through the various checkpoints or even skip them altogether. Going from S to M to S phase almost consecutively. Because these cells have lost their checkpoints, any DNA mutations that may have occurred are disregarded and passed on to the daughter cells. This is one reason why cancer cells have a tendency to exponentially accrue mutations. Aside from cancer cells, many fully differentiated cell types no longer replicate so they leave the cell cycle and stay in G until their death. Thus removing the need for cellular checkpoints. An alternative model of the cell cycle response to DNA damage has also been proposed, known as the postreplication checkpoint.\n\nCheckpoint regulation plays an important role in an organism's development. In sexual reproduction, when egg fertilization occurs, when the sperm binds to the egg, it releases signalling factors that notify the egg that it has been fertilized. Among other things, this induces the now fertilized oocyte to return from its previously dormant, G, state back into the cell cycle and on to mitotic replication and division.\n\np53 plays an important role in triggering the control mechanisms at both G/S and G/M checkpoints. In addition to p53, checkpoint regulators are being heavily researched for their roles in cancer growth and proliferation.\n\nPioneering work by Atsushi Miyawaki and coworkers developed the fluorescent ubiquitination-based cell cycle indicator (FUCCI), which enables fluorescence imaging of the cell cycle. Originally, a green fluorescent protein, mAG, was fused to hGem(1/110) and an orange fluorescent protein (mKO) was fused to hCdt1(30/120). Note, these fusions are fragments that contain a nuclear localization signal and ubiquitination sites for degradation, but are not functional proteins. The green fluorescent protein is made during the S, G, or M phase and degraded during the G or G phase, while the orange fluorescent protein is made during the G or G phase and destroyed during the S, G, or M phase. A far-red and near-infrared FUCCI was developed using a cyanobacteria-derived fluorescent protein (smURFP) and a bacteriophytochrome-derived fluorescent protein (movie found at this link).\n\nA disregulation of the cell cycle components may lead to tumor formation. As mentioned above, when some genes like the cell cycle inhibitors, RB, p53 etc. mutate, they may cause the cell to multiply uncontrollably, forming a tumor. Although the duration of cell cycle in tumor cells is equal to or longer than that of normal cell cycle, the proportion of cells that are in active cell division (versus quiescent cells in G phase) in tumors is much higher than that in normal tissue. Thus there is a net increase in cell number as the number of cells that die by apoptosis or senescence remains the same.\n\nThe cells which are actively undergoing cell cycle are targeted in cancer therapy as the DNA is relatively exposed during cell division and hence susceptible to damage by drugs or radiation. This fact is made use of in cancer treatment; by a process known as debulking, a significant mass of the tumor is removed which pushes a significant number of the remaining tumor cells from G to G phase (due to increased availability of nutrients, oxygen, growth factors etc.). Radiation or chemotherapy following the debulking procedure kills these cells which have newly entered the cell cycle.\n\nThe fastest cycling mammalian cells in culture, crypt cells in the intestinal epithelium, have a cycle time as short as 9 to 10 hours. Stem cells in resting mouse skin may have a cycle time of more than 200 hours. Most of this difference is due to the varying length of G, the most variable phase of the cycle. M and S do not vary much.\n\nIn general, cells are most radiosensitive in late M and G phases and most resistant in late S phase.\n\nFor cells with a longer cell cycle time and a significantly long G phase, there is a second peak of resistance late in G.\n\nThe pattern of resistance and sensitivity correlates with the level of sulfhydryl compounds in the cell. Sulfhydryls are natural substances that protect cells from radiation damage and tend to be at their highest levels in S and at their lowest near mitosis.\n\n\n", "id": "7252", "title": "Cell cycle"}
{"url": "https://en.wikipedia.org/wiki?curid=152611", "text": "Cellular differentiation\n\nIn developmental biology, cellular differentiation is the process where a cell changes from one cell type to another. Most commonly the cell changes to a more specialized type. Differentiation occurs numerous times during the development of a multicellular organism as it changes from a simple zygote to a complex system of tissues and cell types. Differentiation continues in adulthood as adult stem cells divide and create fully differentiated daughter cells during tissue repair and during normal cell turnover. Some differentiation occurs in response to antigen exposure. Differentiation dramatically changes a cell's size, shape, membrane potential, metabolic activity, and responsiveness to signals. These changes are largely due to highly controlled modifications in gene expression and are the study of epigenetics. With a few exceptions, cellular differentiation almost never involves a change in the DNA sequence itself. Thus, different cells can have very different physical characteristics despite having the same genome.\n\nThere are multiple levels of cell potency, the cell's ability to differentiate into other cell types. A greater potency indicates a larger number of cells that can be derived. A cell that can differentiate into all cell types, including the placental tissue, is known as \"totipotent\". In mammals, only the zygote and subsequent blastomeres are totipotent, while in plants many differentiated cells can become totipotent with simple laboratory techniques. A cell that can differentiate into all cell types of the adult organism is known as \"pluripotent\". Such cells are called meristematic cells in higher plants and embryonic stem cells in animals, though some groups report the presence of adult pluripotent cells. Virally induced expression of four transcription factors Oct4, Sox2, c-Myc, and Kfl4 (Yamanaka factors) is sufficient to create pluripotent (iPS) cells from adult fibroblasts. A multipotent cell is one that can differentiate into multiple different, but closely related cell types. Oligopotent cells are more restricted than multipotent, but can still differentiate into a few closely related cell types. Finally, unipotent cells can differentiate into only one cell type, but are capable of self-renewal. In cytopathology, the level of cellular differentiation is used as a measure of cancer progression. \"Grade\" is a marker of how differentiated a cell in a tumor is.\n\nThree basic categories of cells make up the mammalian body: germ cells, somatic cells, and stem cells. Each of the approximately 100 trillion (10) cells in an adult human has its own copy or copies of the genome except certain cell types, such as red blood cells, that lack nuclei in their fully differentiated state. Most cells are diploid; they have two copies of each chromosome. Such cells, called somatic cells, make up most of the human body, such as skin and muscle cells. Cells differentiate to specialize for different functions.\n\nGerm line cells are any line of cells that give rise to gametes—eggs and sperm—and thus are continuous through the generations. Stem cells, on the other hand, have the ability to divide for indefinite periods and to give rise to specialized cells. They are best described in the context of normal human development. \nDevelopment begins when a sperm fertilizes an egg and creates a single cell that has the potential to form an entire organism. In the first hours after fertilization, this cell divides into identical cells. In humans, approximately four days after fertilization and after several cycles of cell division, these cells begin to specialize, forming a hollow sphere of cells, called a blastocyst. The blastocyst has an outer layer of cells, and inside this hollow sphere, there is a cluster of cells called the inner cell mass. The cells of the inner cell mass go on to form virtually all of the tissues of the human body. Although the cells of the inner cell mass can form virtually every type of cell found in the human body, they cannot form an organism. These cells are referred to as pluripotent.\n\nPluripotent stem cells undergo further specialization into multipotent progenitor cells that then give rise to functional cells. Examples of stem and progenitor cells include: \nA pathway that is guided by the cell adhesion molecules consisting of four amino acids, arginine, glycine, asparagine, and serine, is created as the cellular blastomere differentiates from the single-layered blastula to the three primary layers of germ cells in mammals, namely the ectoderm, mesoderm and endoderm (listed from most distal (exterior) to proximal (interior)). The ectoderm ends up forming the skin and the nervous system, the mesoderm forms the bones and muscular tissue, and the endoderm forms the internal organ tissues.\n\nDedifferentiation, or integration is a cellular process often seen in more basal life forms such as worms and amphibians in which a partially or terminally differentiated cell reverts to an earlier developmental stage, usually as part of a regenerative process. Dedifferentiation also occurs in plants. Cells in cell culture can lose properties they originally had, such as protein expression, or change shape. This process is also termed dedifferentiation.\n\nSome believe dedifferentiation is an aberration of the normal development cycle that results in cancer, whereas others believe it to be a natural part of the immune response lost by humans at some point as a result of evolution.\n\nA small molecule dubbed reversine, a purine analog, has been discovered that has proven to induce dedifferentiation in myotubes. These dedifferentiated cells could then redifferentiate into osteoblasts and adipocytes.\n\nEach specialized cell type in an organism expresses a subset of all the genes that constitute the genome of that species. Each cell type is defined by its particular pattern of regulated gene expression. Cell differentiation is thus a transition of a cell from one cell type to another and it involves a switch from one pattern of gene expression to another. Cellular differentiation during development can be understood as the result of a gene regulatory network. A regulatory gene and its cis-regulatory modules are nodes in a gene regulatory network; they receive input and create output elsewhere in the network. The systems biology approach to developmental biology emphasizes the importance of investigating how developmental mechanisms interact to produce predictable patterns (morphogenesis). (However, an alternative view has been proposed recently. Based on stochastic gene expression, cellular differentiation is the result of a Darwinian selective process occurring among cells. In this frame, protein and gene networks are the result of cellular processes and not their cause. See: Cellular Darwinism)\nA few evolutionarily conserved types of molecular processes are often involved in the cellular mechanisms that control these switches. The major types of molecular processes that control cellular differentiation involve cell signaling. Many of the signal molecules that convey information from cell to cell during the control of cellular differentiation are called growth factors. Although the details of specific signal transduction pathways vary, these pathways often share the following general steps. A ligand produced by one cell binds to a receptor in the extracellular region of another cell, inducing a conformational change in the receptor. The shape of the cytoplasmic domain of the receptor changes, and the receptor acquires enzymatic activity. The receptor then catalyzes reactions that phosphorylate other proteins, activating them. A cascade of phosphorylation reactions eventually activates a dormant transcription factor or cytoskeletal protein, thus contributing to the differentiation process in the target cell. Cells and tissues can vary in competence, their ability to respond to external signals.\n\nSignal induction refers to cascades of signaling events, during which a cell or tissue signals to another cell or tissue to influence its developmental fate. Yamamoto and Jeffery investigated the role of the lens in eye formation in cave- and surface-dwelling fish, a striking example of induction. Through reciprocal transplants, Yamamoto and Jeffery found that the lens vesicle of surface fish can induce other parts of the eye to develop in cave- and surface-dwelling fish, while the lens vesicle of the cave-dwelling fish cannot.\n\nOther important mechanisms fall under the category of asymmetric cell divisions, divisions that give rise to daughter cells with distinct developmental fates. Asymmetric cell divisions can occur because of asymmetrically expressed maternal cytoplasmic determinants or because of signaling. In the former mechanism, distinct daughter cells are created during cytokinesis because of an uneven distribution of regulatory molecules in the parent cell; the distinct cytoplasm that each daughter cell inherits results in a distinct pattern of differentiation for each daughter cell. A well-studied example of pattern formation by asymmetric divisions is body axis patterning in Drosophila. RNA molecules are an important type of intracellular differentiation control signal. The molecular and genetic basis of asymmetric cell divisions has also been studied in green algae of the genus \"Volvox\", a model system for studying how unicellular organisms can evolve into multicellular organisms. In \"Volvox carteri\", the 16 cells in the anterior hemisphere of a 32-cell embryo divide asymmetrically, each producing one large and one small daughter cell. The size of the cell at the end of all cell divisions determines whether it becomes a specialized germ or somatic cell.\n\nSince each cell, regardless of cell type, possesses the same genome, determination of cell type must occur at the level of gene expression. While the regulation of gene expression can occur through cis- and trans-regulatory elements including a gene's promoter and enhancers, the problem arises as to how this expression pattern is maintained over numerous generations of cell division. As it turns out, epigenetic processes play a crucial role in regulating the decision to adopt a stem, progenitor, or mature cell fate. This section will focus primarily on mammalian stem cells.\n\nIn systems biology and mathematical modeling of gene regulatory networks, cell-fate determination is predicted to exhibit certain dynamics, such as attractor-convergence (the attractor can be an equilibrium point, limit cycle or strange attractor) or oscillatory.\n\nThe first question that can be asked is the extent and complexity of the role of epigenetic processes in the determination of cell fate. A clear answer to this question can be seen in the 2011 paper by Lister R, \"et al.\" on aberrant epigenomic programming in human induced pluripotent stem cells. As induced pluripotent stem cells (iPSCs) are thought to mimic embryonic stem cells in their pluripotent properties, few epigenetic differences should exist between them. To test this prediction, the authors conducted whole-genome profiling of DNA methylation patterns in several human embryonic stem cell (ESC), iPSC, and progenitor cell lines.\n\nFemale adipose cells, lung fibroblasts, and foreskin fibroblasts were reprogrammed into induced pluripotent state with the OCT4, SOX2, KLF4, and MYC genes. Patterns of DNA methylation in ESCs, iPSCs, somatic cells were compared. Lister R, \"et al.\" observed significant resemblance in methylation levels between embryonic and induced pluripotent cells. Around 80% of CG dinucleotides in ESCs and iPSCs were methylated, the same was true of only 60% of CG dinucleotides in somatic cells. In addition, somatic cells possessed minimal levels of cytosine methylation in non-CG dinucleotides, while induced pluripotent cells possessed similar levels of methylation as embryonic stem cells, between 0.5 and 1.5%. Thus, consistent with their respective transcriptional activities, DNA methylation patterns, at least on the genomic level, are similar between ESCs and iPSCs.\n\nHowever, upon examining methylation patterns more closely, the authors discovered 1175 regions of differential CG dinucleotide methylation between at least one ES or iPS cell line. By comparing these regions of differential methylation with regions of cytosine methylation in the original somatic cells, 44-49% of differentially methylated regions reflected methylation patterns of the respective progenitor somatic cells, while 51-56% of these regions were dissimilar to both the progenitor and embryonic cell lines. In vitro-induced differentiation of iPSC lines saw transmission of 88% and 46% of hyper and hypo-methylated differentially methylated regions, respectively.\n\nTwo conclusions are readily apparent from this study. First, epigenetic processes are heavily involved in cell fate determination, as seen from the similar levels of cytosine methylation between induced pluripotent and embryonic stem cells, consistent with their respective patterns of transcription. Second, the mechanisms of de-differentiation (and by extension, differentiation) are very complex and cannot be easily duplicated, as seen by the significant number of differentially methylated regions between ES and iPS cell lines. Now that these two points have been established, we can examine some of the epigenetic mechanisms that are thought to regulate cellular differentiation.\n\nThree transcription factors, OCT4, SOX2, and NANOG – the first two of which are used in induced pluripotent stem cell (iPSC) reprogramming, along with Klf4 and c-Myc – are highly expressed in undifferentiated embryonic stem cells and are necessary for the maintenance of their pluripotency. It is thought that they achieve this through alterations in chromatin structure, such as histone modification and DNA methylation, to restrict or permit the transcription of target genes. While highly expressed, their levels require a precise balance to maintain pluripotency, perturbation of which will promote differentiation towards different lineages based on how the gene expression levels change. Differential regulation of Oct-4 and SOX2 levels have been shown to precede germ layer fate selection. Increased levels of Oct4 and decreased levels of Sox2 promote a mesendodermal fate, with Oct4 actively suppressing genes associated with a neural ectodermal fate. Similarly, Increased levels of Sox2 and decreased levels of Oct4 promote differentiation towards a neural ectodermal fate, with Sox2 inhibiting differentiation towards a mesendodermal fate. Regardless of the lineage cells differentiate down, suppression of NANOG has been identified as a necessary prerequisite for differentiation.\n\nIn the realm of gene silencing, Polycomb repressive complex 2, one of two classes of the Polycomb group (PcG) family of proteins, catalyzes the di- and tri-methylation of histone H3 lysine 27 (H3K27me2/me3). By binding to the H3K27me2/3-tagged nucleosome, PRC1 (also a complex of PcG family proteins) catalyzes the mono-ubiquitinylation of histone H2A at lysine 119 (H2AK119Ub1), blocking RNA polymerase II activity and resulting in transcriptional suppression. PcG knockout ES cells do not differentiate efficiently into the three germ layers, and deletion of the PRC1 and PRC2 genes leads to increased expression of lineage-affiliated genes and unscheduled differentiation. Presumably, PcG complexes are responsible for transcriptionally repressing differentiation and development-promoting genes.\n\nAlternately, upon receiving differentiation signals, PcG proteins are recruited to promoters of pluripotency transcription factors. PcG-deficient ES cells can begin differentiation but cannot maintain the differentiated phenotype. Simultaneously, differentiation and development-promoting genes are activated by Trithorax group (TrxG) chromatin regulators and lose their repression. TrxG proteins are recruited at regions of high transcriptional activity, where they catalyze the trimethylation of histone H3 lysine 4 (H3K4me3) and promote gene activation through histone acetylation. PcG and TrxG complexes engage in direct competition and are thought to be functionally antagonistic, creating at differentiation and development-promoting loci what is termed a \"bivalent domain\" and rendering these genes sensitive to rapid induction or repression.\n\nRegulation of gene expression is further achieved through DNA methylation, in which the DNA methyltransferase-mediated methylation of cytosine residues in CpG dinucleotides maintains heritable repression by controlling DNA accessibility. The majority of CpG sites in embryonic stem cells are unmethylated and appear to be associated with H3K4me3-carrying nucleosomes. Upon differentiation, a small number of genes, including OCT4 and NANOG, are methylated and their promoters repressed to prevent their further expression. Consistently, DNA methylation-deficient embryonic stem cells rapidly enter apoptosis upon in vitro differentiation.\n\nWhile the DNA sequence of most cells of an organism is the same, the binding patterns of transcription factors and the corresponding gene expression patterns are different. To a large extent, differences in transcription factor binding are determined by the chromatin accessibility of their binding sites through histone modification and/or pioneer factors. In particular, it is important to know whether a nucleosome is covering a given genomic binding site or not. This can be determined using a chromatin immunoprecipitation (ChIP) assay.\n\nDNA-nucleosome interactions are characterized by two states: either tightly bound by nucleosomes and transcriptionally inactive, called heterochromatin, or loosely bound and usually, but not always, transcriptionally active, called euchromatin. The epigenetic processes of histone methylation and acetylation, and their inverses demethylation and deacetylation primarily account for these changes. The effects of acetylation and deacetylation are more predictable. An acetyl group is either added to or removed from the positively charged Lysine residues in histones by enzymes called histone acetyltransferases or histone deacteylases, respectively. The acetyl group prevents Lysine's association with the negatively charged DNA backbone. Methylation is not as straightforward, as neither methylation nor demethylation consistently correlate with either gene activation or repression. However, certain methylations have been repeatedly shown to either activate or repress genes. The trimethylation of lysine 4 on histone 3 (H3K4Me3) is associated with gene activation, whereas trimethylation of lysine 27 on histone 3 represses genes\n\nDuring differentiation, stem cells change their gene expression profiles. Recent studies have implicated a role for nucleosome positioning and histone modifications during this process. There are two components of this process: turning off the expression of embryonic stem cell (ESC) genes, and the activation of cell fate genes. Lysine specific demethylase 1 (KDM1A) is thought to prevent the use of enhancer regions of pluripotency genes, thereby inhibiting their transcription. It interacts with Mi-2/NuRD complex (nucleosome remodelling and histone deacetylase) complex, giving an instance where methylation and acetylation are not discrete and mutually exclusive, but intertwined processes.\n\nA final question to ask concerns the role of cell signaling in influencing the epigenetic processes governing differentiation. Such a role should exist, as it would be reasonable to think that extrinsic signaling can lead to epigenetic remodeling, just as it can lead to changes in gene expression through the activation or repression of different transcription factors. Interestingly, little direct data is available concerning the specific signals that influence the epigenome, and the majority of current knowledge consist of speculations on plausible candidate regulators of epigenetic remodeling. We will first discuss several major candidates thought to be involved in the induction and maintenance of both embryonic stem cells and their differentiated progeny, and then turn to one example of specific signaling pathways in which more direct evidence exists for its role in epigenetic change.\n\nThe first major candidate is Wnt signaling pathway. The Wnt pathway is involved in all stages of differentiation, and the ligand Wnt3a can substitute for the overexpression of c-Myc in the generation of induced pluripotent stem cells. On the other hand, disruption of ß-catenin, a component of the Wnt signaling pathway, leads to decreased proliferation of neural progenitors.\n\nGrowth factors comprise the second major set of candidates of epigenetic regulators of cellular differentiation. These morphogens are crucial for development, and include bone morphogenetic proteins, transforming growth factors (TGFs), and fibroblast growth factors (FGFs). TGFs and FGFs have been shown to sustain expression of OCT4, SOX2, and NANOG by downstream signaling to Smad proteins. Depletion of growth factors promotes the differentiation of ESCs, while genes with bivalent chromatin can become either more restrictive or permissive in their transcription.\n\nSeveral other signaling pathways are also considered to be primary candidates. Cytokine leukemia inhibitory factors are associated with the maintenance of mouse ESCs in an undifferentiated state. This is achieved through its activation of the Jak-STAT3 pathway, which has been shown to be necessary and sufficient towards maintaining mouse ESC pluripotency. Retinoic acid can induce differentiation of human and mouse ESCs, and Notch signaling is involved in the proliferation and self-renewal of stem cells. Finally, Sonic hedgehog, in addition to its role as a morphogen, promotes embryonic stem cell differentiation and the self-renewal of somatic stem cells.\n\nThe problem, of course, is that the candidacy of these signaling pathways was inferred primarily on the basis of their role in development and cellular differentiation. While epigenetic regulation is necessary for driving cellular differentiation, they are certainly not sufficient for this process. Direct modulation of gene expression through modification of transcription factors plays a key role that must be distinguished from heritable epigenetic changes that can persist even in the absence of the original environmental signals. Only a few examples of signaling pathways leading to epigenetic changes that alter cell fate currently exist, and we will focus on one of them.\n\nExpression of Shh (Sonic hedgehog) upregulates the production of BMI1, a component of the PcG complex that recognizes H3K27me3. This occurs in a Gli-dependent manner, as Gli1 and Gli2 are downstream effectors of the Hedgehog signaling pathway. In culture, Bmi1 mediates the Hedgehog pathway's ability to promote human mammary stem cell self-renewal. In both humans and mice, researchers showed Bmi1 to be highly expressed in proliferating immature cerebellar granule cell precursors. When Bmi1 was knocked out in mice, impaired cerebellar development resulted, leading to significant reductions in postnatal brain mass along with abnormalities in motor control and behavior. A separate study showed a significant decrease in neural stem cell proliferation along with increased astrocyte proliferation in Bmi null mice.\n\nIn summary, the role of signaling in the epigenetic control of cell fate in mammals is largely unknown, but distinct examples exist that indicate the likely existence of further such mechanisms.\n\nIn order to fulfill the purpose of regenerating a variety of tissues, adult stems are known to migrate from their niches, adhere to new extracellular matrices (ECM) and differentiate. The ductility of these microenvironments are unique to different tissue types. The ECM surrounding brain, muscle and bone tissues range from soft to stiff. The transduction of the stem cells into these cells types is not directed solely by chemokine cues and cell to cell signaling. The elasticity of the microenvironment can also affect the differentiation of mesenchymal stem cells (MSCs which originate in bone marrow.) When MSCs are placed on substrates of the same stiffness as brain, muscle and bone ECM, the MSCs take on properties of those respective cell types.\nMatrix sensing requires the cell to pull against the matrix at focal adhesions, which triggers a cellular mechano-transducer to generate a signal to be informed what force is needed to deform the matrix. To determine the key players in matrix-elasticity-driven lineage specification in MSCs, different matrix microenvironments were mimicked. From these experiments, it was concluded that focal adhesions of the MSCs were the cellular mechano-transducer sensing the differences of the matrix elasticity. The non-muscle myosin IIa-c isoforms generates the forces in the cell that lead to signaling of early commitment markers. Nonmuscle myosin IIa generates the least force increasing to non-muscle myosin IIc. There are also factors in the cell that inhibit non-muscle myosin II, such as blebbistatin. This makes the cell effectively blind to the surrounding matrix.\nResearchers have obtained some success in inducing stem cell-like properties in HEK 239 cells by providing a soft matrix without the use of diffusing factors. The stem-cell properties appear to be linked to tension in the cells' actin network. One identified mechanism for matrix-induced differentiation is tension-induced proteins, which remodel chromatin in response to mechanical stretch. The RhoA pathway is also implicated in this process.\n\n", "id": "152611", "title": "Cellular differentiation"}
{"url": "https://en.wikipedia.org/wiki?curid=18976", "text": "Meiosis\n\nMeiosis is a specialized type of cell division that reduces the chromosome number by half, creating four haploid cells, each genetically distinct from the parent cell that gave rise to them. This process occurs in all sexually reproducing single-celled and multicellular eukaryotes, including animals, plants, and fungi. Errors in meiosis resulting in aneuploidy are the leading known cause of miscarriage and the most frequent genetic cause of developmental disabilities.\n\nIn meiosis, DNA replication is followed by two rounds of cell division to produce four daughter cells, each with half the number of chromosomes as the original parent cell. The two meiotic divisions are known as \"Meiosis I\" and \"Meiosis II\". Before meiosis begins, during S phase of the cell cycle, the DNA of each chromosome is replicated so that it consists of two identical sister chromatids, which remain held together through sister chromatid cohesion. This S-phase can be referred to as \"premeiotic S-phase\" or \"meiotic S-phase\". Immediately following DNA replication, meiotic cells enter a prolonged G2-like stage known as meiotic prophase. During this time, homologous chromosomes pair with each other and undergo genetic recombination, a programmed process in which DNA is cut and then repaired, which allows them to exchange some of their genetic information. A subset of recombination events results in crossovers, which create physical links known as chiasmata (singular: chiasma, for the Greek letter Chi (X)) between the homologous chromosomes. In most organisms, these links are essential to direct each pair of homologous chromosomes to segregate away from each other during Meiosis I, resulting in two haploid cells that have half the number of chromosomes as the parent cell. During Meiosis II, the cohesion between sister chromatids is released and they segregate from one another, as during mitosis. In some cases all four of the meiotic products form gametes such as sperm, spores, or pollen. In female animals, three of the four meiotic products are typically eliminated by extrusion into polar bodies, and only one cell develops to produce an ovum.\n\nBecause the number of chromosomes is halved during meiosis, gametes can fuse (i.e. fertilization) to form a diploid zygote that contains two copies of each chromosome, one from each parent. Thus, alternating cycles of meiosis and fertilization enable sexual reproduction, with successive generations maintaining the same number of chromosomes. For example, diploid human cells contain 23 pairs of chromosomes including 1 pair of sex chromosomes (46 total), half of maternal origin and half of paternal origin. Meiosis produces haploid gametes (ova or sperm) that contain one set of 23 chromosomes. When two gametes (an egg and a sperm) fuse, the resulting zygote is once again diploid, with the mother and father each contributing 23 chromosomes. This same pattern, but not the same number of chromosomes, occurs in all organisms that utilize meiosis.\n\nAlthough the process of meiosis is related to the more general cell division process of mitosis, it differs in two important respects:\nMeiosis begins with a diploid cell, which contains two copies of each chromosome, termed homologs. First, the cell undergoes DNA replication, so each homolog now consists of two identical sister chromatids. Then each set of homologs pair with each other and exchange DNA by homologous recombination leading to physical connections (crossovers) between the homologs. In the first meiotic division, the homologs are segregated to separate daughter cells by the spindle apparatus. The cells then proceed to a second division without an intervening round of DNA replication. The sister chromatids are segregated to separate daughter cells to produce a total of four haploid cells. Female animals employ a slight variation on this pattern and produce one large ovum and two small polar bodies. Because of recombination, an individual chromatid can consist of a new combination of maternal and paternal DNA, resulting in offspring that are genetically distinct from either parent. Furthermore, an individual gamete can include an assortment of maternal, paternal, and recombinant chromatids. This genetic diversity resulting from sexual reproduction contributes to the variation in traits upon which natural selection can act.\n\nMeiosis uses many of the same mechanisms as mitosis, the type of cell division used by eukaryotes to divide one cell into two identical daughter cells. In some plants, fungi, and protists meiosis results in the formation of spores: haploid cells that can divide vegetatively without undergoing fertilization. Some eukaryotes, like bdelloid rotifers, do not have the ability to carry out meiosis and have acquired the ability to reproduce by parthenogenesis.\n\nMeiosis does not occur in archaea or bacteria, which generally reproduce via asexual processes such as binary fission. However, a \"sexual\" process known as horizontal gene transfer involves the transfer of DNA from one bacterium or archaeon to another and recombination of these DNA molecules of different parental origin.\n\nMeiosis was discovered and described for the first time in sea urchin eggs in 1876 by the German biologist Oscar Hertwig. It was described again in 1883, at the level of chromosomes, by the Belgian zoologist Edouard Van Beneden, in \"Ascaris\" roundworm eggs. The significance of meiosis for reproduction and inheritance, however, was described only in 1890 by German biologist August Weismann, who noted that two cell divisions were necessary to transform one diploid cell into four haploid cells if the number of chromosomes had to be maintained. In 1911 the American geneticist Thomas Hunt Morgan detected crossovers in meiosis in the fruit fly \"Drosophila melanogaster\", which helped to establish that genetic traits are transmitted on chromosomes.\n\nThe term meiosis (originally spelled \"maiosis\") was introduced to biology by J.B. Farmer and J.E.S. Moore in 1905:\nWe propose to apply the terms Maiosis or Maiotic phase to cover the whole series of nuclear changes included in the two divisions that were designated as Heterotype and Homotype by Flemming.\nIt is derived from the Greek word , meaning 'lessening'.\n\nMeiosis occurs in eukaryotic life cycles involving sexual reproduction, consisting of the constant cyclical process of meiosis and fertilization. This takes place alongside normal mitotic cell division. In multicellular organisms, there is an intermediary step between the diploid and haploid transition where the organism grows. At certain stages of the life cycle, germ cells produce gametes. Somatic cells make up the body of the organism and are not involved in gamete production.\n\nCycling meiosis and fertilization events produces a series of transitions back and forth between alternating haploid and diploid states. The organism phase of the life cycle can occur either during the diploid state (\"gametic\" or \"diploid\" life cycle), during the haploid state (\"zygotic\" or \"haploid\" life cycle), or both (\"sporic\" or \"haplodiploid\" life cycle, in which there are two distinct organism phases, one during the haploid state and the other during the diploid state). In this sense there are three types of life cycles that utilize sexual reproduction, differentiated by the location of the organism phase(s).\n\nIn the \"gametic life cycle\" or \" diplontic life cycle\", of which humans are a part, the organism is diploid, grown from a diploid cell called the zygote. The organism's diploid germ-line stem cells undergo meiosis to create haploid gametes (the spermatozoa for males and ova for females), which fertilize to form the zygote. The diploid zygote undergoes repeated cellular division by mitosis to grow into the organism.\n\nIn the \"zygotic life cycle\" the organism is haploid instead, spawned by the proliferation and differentiation of a single haploid cell called the gamete. Two organisms of opposing sex contribute their haploid gametes to form a diploid zygote. The zygote undergoes meiosis immediately, creating four haploid cells. These cells undergo mitosis to create the organism. Many fungi and many protozoa utilize the zygotic life cycle. \n\nFinally, in the \"sporic life cycle\", the living organism alternates between haploid and diploid states. Consequently, this cycle is also known as the alternation of generations. The diploid organism's germ-line cells undergo meiosis to produce spores. The spores proliferate by mitosis, growing into a haploid organism. The haploid organism's gamete then combines with another haploid organism's gamete, creating the zygote. The zygote undergoes repeated mitosis and differentiation to become a diploid organism again. The sporic life cycle can be considered a fusion of the gametic and zygotic life cycles.\n\nThe preparatory steps that lead up to meiosis are identical in pattern and name to interphase of the mitotic cell cycle.\n\nInterphase is divided into three phases:\nInterphase is followed by meiosis I and then meiosis II. Meiosis I separates homologous chromosomes, each still made up of two sister chromatids, into two daughter cells, thus reducing the chromosome number by half. During meiosis II, sister chromatids decouple and the resultant daughter chromosomes are segregated into four daughter cells. For diploid organisms, the daughter cells resulting from meiosis are haploid and contain only one copy of each chromosome. In some species, cells enter a resting phase known as interkinesis between meiosis I and meiosis II.\n\nMeiosis I and II are each divided into prophase, metaphase, anaphase, and telophase stages, similar in purpose to their analogous subphases in the mitotic cell cycle. Therefore, meiosis includes the stages of meiosis I (prophase I, metaphase I, anaphase I, telophase I) and meiosis II (prophase II, metaphase II, anaphase II, telophase II).\n\nMeiosis generates gamete genetic diversity in two ways: (1) Law of Independent Assortment. The independent orientation of homologous chromosome pairs along the metaphase plate during metaphase I & orientation of sister chromatids in metaphase II, this is the subsequent separation of homologs and sister chromatids during anaphase I & II, it allows a random and independent distribution of chromosomes to each daughter cell (and ultimately to gametes); and (2) Crossing Over. The physical exchange of homologous chromosomal regions by homologous recombination during prophase I results in new combinations of DNA within chromosomes.\n\nDuring meiosis, specific genes are more highly transcribed. In addition to strong meiotic stage-specific expression of mRNA, there are also pervasive translational controls (e.g. selective usage of preformed mRNA), regulating the ultimate meiotic stage-specific protein expression of genes during meiosis. Thus, both transcriptional and translational controls determine the broad restructuring of meiotic cells needed to carry out meiosis.\n\nMeiosis is divided into meiosis I and meiosis II which are further divided into Karyokinesis I and Cytokinesis I and Karyokinesis II and Cytokinesis II respectively.\n\nMeiosis I segregates homologous chromosomes, which are joined as tetrads (2n, 4c), producing two haploid cells (n chromosomes, 23 in humans) which each contain chromatid pairs (1n, 2c). Because the ploidy is reduced from diploid to haploid, meiosis I is referred to as a \"reductional division\". Meiosis II is an \"equational division\" analogous to mitosis, in which the sister chromatids are segregated, creating four haploid daughter cells (1n, 1c).\nProphase I is typically the longest phase of meiosis. During prophase I, homologous chromosomes pair and exchange DNA (homologous recombination). This often results in chromosomal crossover. This process is critical for pairing between homologous chromosomes and hence for accurate segregation of the chromosomes at the first meiosis division. The new combinations of DNA created during crossover are a significant source of genetic variation, and result in new combinations of alleles, which may be beneficial. The paired and replicated chromosomes are called bivalents or tetrads, which have two chromosomes and four chromatids, with one chromosome coming from each parent. The process of pairing the homologous chromosomes is called synapsis. At this stage, non-sister chromatids may cross-over at points called chiasmata (plural; singular chiasma). Prophase I has historically been divided into a series of substages which are named according to the appearance of chromosomes.\n\nThe first stage of prophase I is the \"leptotene\" stage, also known as \"leptonema\", from Greek words meaning \"thin threads\".In this stage of prophase I, individual chromosomes—each consisting of two sister chromatids—become \"individualized\" to form visible strands within the nucleus. The two sister chromatids closely associate and are visually indistinguishable from one another. During leptotene, lateral elements of the synaptonemal complex assemble. Leptotene is of very short duration and progressive condensation and coiling of chromosome fibers takes place.\n\nThe \"zygotene\" stage, also known as \"zygonema\", from Greek words meaning \"paired threads\", occurs as the chromosomes approximately line up with each other into homologous chromosome pairs. In some organisms, this is called the bouquet stage because of the way the telomeres cluster at one end of the nucleus. At this stage, the synapsis (pairing/coming together) of homologous chromosomes takes place, facilitated by assembly of central element of the synaptonemal complex. Pairing is brought about in a zipper-like fashion and may start at the centromere (procentric), at the chromosome ends (proterminal), or at any other portion (intermediate). Individuals of a pair are equal in length and in position of the centromere. Thus pairing is highly specific and exact. The paired chromosomes are called bivalent or tetrad chromosomes.\n\nThe \"pachytene\" (pronounced ) stage, also known as \"pachynema\", from Greek words meaning \"thick threads\". At this point a tetrad of the chromosomes has formed known as a bivalent. This is the stage when homologous recombination, including chromosomal crossover (crossing over), occurs. Nonsister chromatids of homologous chromosomes may exchange segments over regions of homology. Sex chromosomes, however, are not wholly identical, and only exchange information over a small region of homology. At the sites where exchange happens, chiasmata form. The exchange of information between the non-sister chromatids results in a recombination of information; each chromosome has the complete set of information it had before, and there are no gaps formed as a result of the process. Because the chromosomes cannot be distinguished in the synaptonemal complex, the actual act of crossing over is not perceivable through the microscope, and chiasmata are not visible until the next stage.\n\nDuring the \"diplotene\" stage, also known as \"diplonema\", from Greek words meaning \"two threads\", the synaptonemal complex degrades and homologous chromosomes separate from one another a little. The chromosomes themselves uncoil a bit, allowing some transcription of DNA. However, the homologous chromosomes of each bivalent remain tightly bound at chiasmata, the regions where crossing-over occurred. The chiasmata remain on the chromosomes until they are severed at the transition to anaphase I.\n\nIn human fetal oogenesis, all developing oocytes develop to this stage and are arrested in prophase I before birth. This suspended state is referred to as the \"dictyotene stage\" or dictyate. It lasts until meiosis is resumed to prepare the oocyte for ovulation, which happens at puberty or even later.\n\nChromosomes condense further during the \"diakinesis\" stage, from Greek words meaning \"moving through\". This is the first point in meiosis where the four parts of the tetrads are actually visible. Sites of crossing over entangle together, effectively overlapping, making chiasmata clearly visible. Other than this observation, the rest of the stage closely resembles prometaphase of mitosis; the nucleoli disappear, the nuclear membrane disintegrates into vesicles, and the meiotic spindle begins to form.\n\nDuring these stages, two centrosomes, containing a pair of centrioles in animal cells, migrate to the two poles of the cell. These centrosomes, which were duplicated during S-phase, function as microtubule organizing centers nucleating microtubules, which are essentially cellular ropes and poles. The microtubules invade the nuclear region after the nuclear envelope disintegrates, attaching to the chromosomes at the kinetochore. The kinetochore functions as a motor, pulling the chromosome along the attached microtubule toward the originating centrosome, like a train on a track. There are four kinetochores on each tetrad, but the pair of kinetochores on each sister chromatid fuses and functions as a unit during meiosis I.\n\nMicrotubules that attach to the kinetochores are known as \"kinetochore microtubules\". Other microtubules will interact with microtubules from the opposite centrosome: these are called \"nonkinetochore microtubules\" or \"polar microtubules\". A third type of microtubules, the aster microtubules, radiates from the centrosome into the cytoplasm or contacts components of the membrane skeleton.\n\nHomologous pairs move together along the metaphase plate: As \"kinetochore microtubules\" from both centrosomes attach to their respective kinetochores, the paired homologous chromosomes align along an equatorial plane that bisects the spindle, due to continuous counterbalancing forces exerted on the bivalents by the microtubules emanating from the two kinetochores of homologous chromosomes. This attachment is referred to as a bipolar attachment. The physical basis of the independent assortment of chromosomes is the random orientation of each bivalent along the metaphase plate, with respect to the orientation of the other bivalents along the same equatorial line. The protein complex cohesin holds sister chromatids together from the time of their replication until anaphase. In mitosis, the force of kinetochore microtubules pulling in opposite directions creates tension. The cell senses this tension and does not progress with anaphase until all the chromosomes are properly bi-oriented. In meiosis, establishing tension requires at least one crossover per chromosome pair in addition to cohesin between sister chromatids.\n\nKinetochore microtubules shorten, pulling homologous chromosomes (which consist of a pair of sister chromatids) to opposite poles. Nonkinetochore microtubules lengthen, pushing the centrosomes farther apart. The cell elongates in preparation for division down the center. Unlike in mitosis, only the cohesin from the chromosome arms is degraded while the cohesin surrounding the centromere remains protected. This allows the sister chromatids to remain together while homologs are segregated.\n\nThe first meiotic division effectively ends when the chromosomes arrive at the poles. Each daughter cell now has half the number of chromosomes but each chromosome consists of a pair of chromatids. The microtubules that make up the spindle network disappear, and a new nuclear membrane surrounds each haploid set. The chromosomes uncoil back into chromatin. Cytokinesis, the pinching of the cell membrane in animal cells or the formation of the cell wall in plant cells, occurs, completing the creation of two daughter cells. Sister chromatids remain attached during telophase I.\n\nCells may enter a period of rest known as interkinesis or interphase II. No DNA replication occurs during this stage.\n\nMeiosis II is the second meiotic division, and usually involves equational segregation, or separation of sister chromatids. Mechanically, the process is similar to mitosis, though its genetic results are fundamentally different. The end result is production of four haploid cells (n chromosomes, 23 in humans) from the two haploid cells (with n chromosomes, each consisting of two sister chromatids) produced in meiosis I. The four main steps of meiosis II are: prophase II, metaphase II, anaphase II, and telophase II.\n\nIn prophase II we see the disappearance of the nucleoli and the nuclear envelope again as well as the shortening and thickening of the chromatids. Centrosomes move to the polar regions and arrange spindle fibers for the second meiotic division.\n\nIn metaphase II, the centromeres contain two kinetochores that attach to spindle fibers from the centrosomes at opposite poles. The new equatorial metaphase plate is rotated by 90 degrees when compared to meiosis I, perpendicular to the previous plate.\n\nThis is followed by anaphase II, in which the remaining centromeric cohesin is cleaved allowing the sister chromatids to segregate. The sister chromatids by convention are now called sister chromosomes as they move toward opposing poles.\n\nThe process ends with telophase II, which is similar to telophase I, and is marked by decondensation and lengthening of the chromosomes and the disassembly of the spindle. Nuclear envelopes reform and cleavage or cell plate formation eventually produces a total of four daughter cells, each with a haploid set of chromosomes.\n\nMeiosis is now complete and ends up with four new daughter cells.\n\nThe normal separation of chromosomes in meiosis I or sister chromatids in meiosis II is termed \"disjunction\". When the segregation is not normal, it is called \"nondisjunction\". This results in the production of gametes which have either too many or too few of a particular chromosome, and is a common mechanism for trisomy or monosomy. Nondisjunction can occur in the meiosis I or meiosis II, phases of cellular reproduction, or during mitosis.\n\nMost monosomic and trisomic human embryos are not viable, but some aneuploidies can be tolerated, such as trisomy for the smallest chromosome, chromosome 21. Phenotypes of these aneuploidies range from severe developmental disorders to asymptomatic. Medical conditions include but are not limited to:\n\nThe probability of nondisjunction in human oocytes increases with increasing maternal age, presumably due to loss of cohesin over time.\n\nMeiosis occurs in all animals and plants. The end result, the production of gametes with half the number of chromosomes as the parent cell, is the same, but the detailed process is different. In animals, meiosis produces gametes directly. In land plants and some algae, there is an alternation of generations such that meiosis in the diploid sporophyte generation produces haploid spores. These spores multiply by mitosis, developing into the haploid gametophyte generation, which then gives rise to gametes directly (i.e. without further meiosis). In both animals and plants, the final stage is for the gametes to fuse, restoring the original number of chromosomes.\n\nIn females, meiosis occurs in cells known as oocytes (singular: oocyte). Each primary oocyte divides twice in meiosis, unequally in each case. The first division produces a daughter cell, and a much smaller polar body which may or may not undergo a second division. In meiosis II, division of the daughter cell produces a second polar body, and a single haploid cell, which enlarges to become an ovum. Therefore, in females each primary oocyte that undergoes meiosis results in one mature ovum and one or two polar bodies.\n\nNote that there are pauses during meiosis in females. Maturing oocytes are arrested in prophase I of meiosis I and lie dormant within a protective shell of somatic cells called the follicle. At the beginning of each menstrual cycle, FSH secretion from the anterior pituitary stimulates a few follicles to mature in a process known as folliculogenesis. During this process, the maturing oocytes resume meiosis and continue until metaphase II of meiosis II, where they are again arrested just before ovulation. If these oocytes are fertilized by sperm, they will resume and complete meiosis. During folliculogenesis in humans, usually one follicle becomes dominant while the others undergo atresia. The process of meiosis in females occurs during oogenesis, and differs from the typical meiosis in that it features a long period of meiotic arrest known as the dictyate stage and lacks the assistance of centrosomes.\n\nIn males, meiosis occurs during spermatogenesis in the seminiferous tubules of the testicles. Meiosis during spermatogenesis is specific to a type of cell called spermatocytes, which will later mature to become spermatozoa. Meiosis of primordial germ cells happens at the time of puberty, much later than in females. Tissues of the male testis suppress meiosis by degrading retinoic acid, a stimulator of meiosis. This is overcome at puberty when cells within seminiferous tubules called Sertoli cells start making their own retinoic acid. Sensitivity to retinoic acid is also adjusted by proteins called nanos and DAZL.\n\nIn female mammals, meiosis begins immediately after primordial germ cells migrate to the ovary in the embryo. It is retinoic acid, derived from the primitive kidney (mesonephros) that stimulates meiosis in ovarian oogonia. Tissues of the male testis suppress meiosis by degrading retinoic acid, a stimulator of meiosis. This is overcome at puberty when cells within seminiferous tubules called Sertoli cells start making their own retinoic acid.\n\nIn order to understand meiosis, a comparison to mitosis is helpful. The table below shows the differences between meiosis and mitosis.\n\n", "id": "18976", "title": "Meiosis"}
{"url": "https://en.wikipedia.org/wiki?curid=14602439", "text": "Melanoblast\n\nA melanoblast is a precursor cell of a melanocyte.\nThese cells migrate from the trunk neural crest cells (in terms of axial level from neck to posterior end) dorsolaterally between the ectoderm and dorsal surface of the somites.\n\n", "id": "14602439", "title": "Melanoblast"}
{"url": "https://en.wikipedia.org/wiki?curid=31048192", "text": "Copigmentation\n\nCopigmentation is a phenomenon where pigmentation due to anthocyanidins is reinforced by the presence of other colorless flavonoids known as cofactors or “copigments”. This occurs by the formation of a non-covalently-linked complex. \n\nAn example is the bluish purple flowers of the Japanese garden iris (\"Iris ensata\"). The characteristic floral jade coloration of \"Strongylodon macrobotrys\" has been shown to be an example of copigmentation, a result of the presence of malvin (the anthocyanin) and saponarin (a flavone glucoside) in the ratio 1:9.\n\nIt is a phenomenon observed in the berry color of the porcelain berry (\"Ampelopsis glandulosa\").\n\nPart of the color of red wine can be due to the copigmentation phenomenon. Copigmentation is only important during the early stages of a wine's age. Anthocyanins begin to polymerize with other wine compounds, such as hydroxycinnamic acids, tannins, glyceraldehyde or proteins, to form more complex structures with covalent C–C bonds.\n\n\n", "id": "31048192", "title": "Copigmentation"}
{"url": "https://en.wikipedia.org/wiki?curid=7454563", "text": "Autumn leaf color\n\nAutumn leaf color is a phenomenon that affects the normally green leaves of many deciduous trees and shrubs by which they take on, during a few weeks in the autumn season, various shades of red, yellow, purple, black, orange, pink, magenta, blue and brown. The phenomenon is commonly called autumn colours or autumn foliage in British English and fall colors, fall foliage or simply foliage in American English.\n\nIn some areas of Canada and the United States, \"leaf peeping\" tourism is a major contribution to economic activity. This tourist activity occurs between the beginning of color changes and the onset of leaf fall, usually around September and October in the Northern Hemisphere and April to May in the Southern Hemisphere.\n\nA green leaf is green because of the presence of a pigment known as chlorophyll, which is inside an organelle called a chloroplast. When they are abundant in the leaf's cells, as they are during the growing season, the chlorophyll's green color dominates and masks out the colors of any other pigments that may be present in the leaf. Thus, the leaves of summer are characteristically green.\n\nChlorophyll has a vital function: it captures solar rays and uses the resulting energy in the manufacture of the plant's food — simple sugars which are produced from water and carbon dioxide. These sugars are the basis of the plant's nourishment — the sole source of the carbohydrates needed for growth and development. In their food-manufacturing process, the chlorophylls break down, thus are being continually \"used up\". During the growing season, however, the plant replenishes the chlorophyll so that the supply remains high and the leaves stay green.\n\nIn late summer, as daylight hours shorten and temperatures cool, the veins that carry fluids into and out of the leaf are gradually closed off as a layer of special cork cells forms at the base of each leaf. As this cork layer develops, water and mineral intake into the leaf is reduced, slowly at first, and then more rapidly. During this time, the chlorophyll begins to decrease. Often, the veins are still green after the tissues between them have almost completely changed color.\n\nMuch chlorophyll is in photosystem II (light-harvesting complex II or LHC II), the most abundant membrane protein on earth. LHC II captures light in photosynthesis. It is located in the thylakoid membrane of the chloroplast and it is composed of an apoprotein along with several ligands, the most important of which are chlorophylls a and b. In the fall, this complex is broken down. Chlorophyll degradation is thought to occur first. Recent research suggests that the beginning of chlorophyll degradation is catalyzed by chlorophyll b reductase, which reduces chlorophyll b to 7‑hydroxymethyl chlorophyll a, which is then reduced to chlorophyll a. This is believed to destabilize the complex, at which point breakdown of the apoprotein occurs. An important enzyme in the breakdown of the apoprotein is FtsH6, which belongs to the FtsH family of proteases.\n\nChlorophylls degrade into colorless tetrapyrroles known as nonfluorescent chlorophyll catabolites.\nAs the chlorophylls degrade, the hidden pigments of yellow xanthophylls and orange beta-carotene are revealed. These pigments are present throughout the year, but the red pigments, the anthocyanins, are synthesized \"de novo\" once roughly half of chlorophyll has been degraded. The amino acids released from degradation of light harvesting complexes are stored all winter in the tree's roots, branches, stems, and trunk until next spring, when they are recycled to releaf the tree.\n\nCarotenoids are present in leaves the whole year round, but their orange-yellow colors are usually masked by green chlorophyll. As autumn approaches, certain influences both inside and outside the plant cause the chlorophylls to be replaced at a slower rate than they are being used up. During this period, with the total supply of chlorophylls gradually dwindling, the \"masking\" effect slowly fades away. Then other pigments that have been present (along with the chlorophylls) in the cells all during the leaf's life begin to show through. These are carotenoids and they provide colorations of yellow, brown, orange, and the many hues in between.\n\nThe carotenoids occur, along with the chlorophyll pigments, in tiny structures called plastids, within the cells of leaves. Sometimes, they are in such abundance in the leaf that they give a plant a yellow-green color, even during the summer. Usually, however, they become prominent for the first time in autumn, when the leaves begin to lose their chlorophyll.\n\nCarotenoids are common in many living things, giving characteristic color to carrots, corn, canaries, and daffodils, as well as egg yolks, rutabagas, buttercups, and bananas.\n\nTheir brilliant yellows and oranges tint the leaves of such hardwood species as hickories, ash, maple, yellow poplar, aspen, birch, black cherry, sycamore, cottonwood, sassafras, and alder. Carotenoids are the dominant pigment in coloration of about 15-30% of tree species.\n\nThe reds, the purples, and their blended combinations that decorate autumn foliage come from another group of pigments in the cells called anthocyanins. Unlike the carotenoids, these pigments are not present in the leaf throughout the growing season, but are actively produced towards the end of summer. They develop in late summer in the sap of the cells of the leaf, and this development is the result of complex interactions of many influences—both inside and outside the plant. Their formation depends on the breakdown of sugars in the presence of bright light as the level of phosphate in the leaf is reduced.\n\nDuring the summer growing season, phosphate is at a high level. It has a vital role in the breakdown of the sugars manufactured by chlorophyll, but in the fall, phosphate, along with the other chemicals and nutrients, moves out of the leaf into the stem of the plant. When this happens, the sugar-breakdown process changes, leading to the production of anthocyanin pigments. The brighter the light during this period, the greater the production of anthocyanins and the more brilliant the resulting color display. When the days of autumn are bright and cool, and the nights are chilly but not freezing, the brightest colorations usually develop.\n\nAnthocyanins temporarily color the edges of some of the very young leaves as they unfold from the buds in early spring. They also give the familiar color to such common fruits as cranberries, red apples, blueberries, cherries, strawberries, and plums.\n\nAnthocyanins are present in about 10% of tree species in temperate regions, although in certain areas — most famously New England — up to 70% of tree species may produce the pigment. In autumn forests, they appear vivid in the maples, oaks, sourwood, sweetgums, dogwoods, tupelos, cherry trees and persimmons. These same pigments often combine with the carotenoids' colors to create the deeper orange, fiery reds, and bronzes typical of many hardwood species.\n\nThe brown color of leaves is not the result of a pigment, but rather cell walls, which may be evident when no coloring pigment is visible.\n\nDeciduous plants were traditionally believed to shed their leaves in autumn primarily because the high costs involved in their maintenance would outweigh the benefits from photosynthesis during the winter period of low light availability and cold temperatures. In many cases, this turned out to be oversimplistic — other factors involved include insect predation, water loss, and damage from high winds or snowfall.\n\nAnthocyanins, responsible for red-purple coloration, are actively produced in autumn, but not involved in leaf-drop. A number of hypotheses on the role of pigment production in leaf-drop have been proposed, and generally fall into two categories: interaction with animals, and protection from nonbiological factors.\n\nAccording to the photoprotection theory, anthocyanins protects the leaf against the harmful effects of light at low temperatures. Indeed, the leaves are about to fall, so protection is not of extreme importance for the tree. Photo-oxidation and photoinhibition, however, especially at low temperatures, make the process of reabsorbing nutrients less efficient. By shielding the leaf with anthocyanins, according to the photoprotection theory, the tree manages to reabsorb nutrients (especially nitrogen) more efficiently.\n\nAccording to the coevolution theory, the colors are warning signals towards insects that use the trees as a host for the winter, for example aphids. If the colors are linked to the amount of chemical defenses against insects, then the insects will avoid red leaves and increase their fitness; at the same time, trees with red leaves have an advantage because they reduce their parasite load. This has been shown in the case of apple trees where some domesticated apple varieties, unlike wild ones, lack red leaves in autumn. A greater proportion of aphids that avoid apple trees with red leaves manage to grow and develop compared to those that do not. A trade-off, moreover, exists between fruit size, leaf color, and aphids resistance as varieties with red leaves have smaller fruits, suggesting a cost to the production of red leaves linked to a greater need for reduced aphid infestation.\n\nConsistent with red-leaved trees providing reduced survival for aphids, tree species with bright leaves tend to select for more specialist aphid pests than do trees lacking bright leaves (autumn colors are useful only in those species coevolving with insect pests in autumn).\n\nThe coevolution theory of autumn colors was proposed by W. D. Hamilton in 2001 as an example of evolutionary signalling theory. With biological signals such as red leaves, it is argued that because they are costly to produce, they are usually honest, so signal the true quality of the signaller with low-quality individuals being unable to fake them and cheat. Autumn colors would be a signal if they are costly to produce, or be impossible to fake (for example if autumn pigments were produced by the same biochemical pathway that produces the chemical defenses against the insects).\n\nThe change of leaf colors prior to fall have also been suggested as adaptations that may help to undermine the camouflage of herbivores.\n\nMany plants with berries attract birds with especially visible berry and/or leaf color, particularly bright red. The birds get a meal, while the shrub, vine, or typically small tree gets undigested seeds carried off and deposited with the birds' manure. Poison ivy is particularly notable for having bright-red foliage drawing birds to its off-white seeds (which are edible for birds, but not most mammals).\n\nThe brilliant red autumn color of some species of maple is created by processes separate from those in chlorophyll breakdown. When the tree is struggling to cope with the energy demands of a changing and challenging season, maple trees are involved in an additional metabolic expenditure to create anthocyanins. These anthocyanins, which create the visual red hues, have been found to aid in interspecific competition by stunting the growth of nearby saplings (allelopathy).\n\nAlthough some autumn coloration occurs wherever deciduous trees are found, the most brightly colored autumn foliage is found in four or five regions of the world: most of southern mainland Canada; and some areas of the northern United States,Scandinavia, Northern, and Western Europe north of the Alps; the Caucasus region near the Black Sea, Russia; and Eastern Asia, including much of northern and eastern China, as well as Argentina, Chile, southern Brazil, Korea, Japan and New Zealand's South Island.\n\nCompared to Western Europe, North America provides many more arbor species (more than 800 species and about 70 oaks, compared to 51 and three, respectively, in Western Europe) which adds many more different colors to the spectacle. The main reason was the different effect of the ice ages—while in North America, species were protected in more southern regions along north–south ranging mountains, which was not the case in Europe.\n\nGlobal warming and rising carbon dioxide levels in the atmosphere may delay the usual autumn spectacle of changing colors and falling leaves in northern hardwood forests in the future, and increase forest productivity. Experiments with poplar trees showed that they stayed greener longer with higher CO levels, independent of temperature changes. However, the experiments over two years were too brief to indicate how mature forests may be affected over time. Also, other factors, such as increasing ozone levels close to the ground (tropospheric ozone pollution), can negate the beneficial effects of elevated carbon dioxide.\n\n", "id": "7454563", "title": "Autumn leaf color"}
{"url": "https://en.wikipedia.org/wiki?curid=6074076", "text": "Viral entry\n\nViral entry is the earliest stage of infection in the viral life cycle, as the virus comes into contact with the host cell and introduces viral material into the cell. The major steps involved in viral entry are shown below. Despite the variation among viruses, there are several shared generalities concerning viral entry.\n\nA virus floating around an enclosed space with possible host cells faces a large hurdle, the thermodynamics of diffusion. Because neutrally charged objects do not naturally clump around each other, the virus must find a way to move even near a host cell. It does this by attachment -- or adsorption --- onto a susceptible cell; a cell which holds a receptor that the virus can bind to. The receptors on the viral envelope effectively become connected to complementary receptors on the cell membrane. This attachment causes the two membranes to remain in mutual proximity, favoring further interactions between surface proteins. This is also the first requisite that must be satisfied before a cell can become infected. Satisfaction of this requisite makes the cell susceptible. Viruses that exhibit this behavior include many enveloped viruses such as HIV and Herpes simplex virus\n\nThis basic idea extends to viruses that do not contain an envelope. Well studied examples are the viruses that infect bacteria, known as bacteriophages or simply phages. Typical phages have long tails used to attach to receptors on the bacterial surface.\n\nPrior to entry, a virus must attach to a host cell. Attachment is achieved when specific proteins on the viral capsid or viral envelope bind to specific proteins called to receptor proteins receptors on the cell membrane of the target cell. A virus must now enter the cell, which is covered by a phospholipid bilayer, a cell's natural barrier to the outside world. The process by which this barrier is breached depends upon the virus. Types of entry are:\n\n\nThrough the use of green fluorescent protein (GFP), virus entry and infection can be visualized in real-time. Once a virus enters a cell, replication is not immediate and indeed takes some time (seconds to hours).\n\nThe most well-known example is through membrane fusion. In viruses with a viral envelope, viral receptors attach to the receptors on the surface of the cell and secondary receptors may be present to initiate the puncture of the membrane or fusion with the host cell. Following attachment, the viral envelope fuses with the host cell membrane, emptying the now-bare virus into the cell. In essence, the virus's envelope \"blends\" with the host cell membrane, releasing its contents into the cell. Obviously, this can only be done with viruses that have an envelope (examples of such enveloped viruses include HIV and herpes simplex virus.)\n\nViruses with no viral envelope enter the cell through endocytosis; they are ingested by the host cell through the cell membrane. In essence, the virus tricks the cell into thinking that the virus knocking at the door is nothing more than nutrition or harmless goods. A cell, which naturally takes in resources from the environment by attaching goods onto surface receptors and bringing them into the cell, will engulf the virus. Once inside the cell, the virus must now break out of the vesicle by which it was taken up in order to gain access to the cytoplasm. Examples include the poliovirus, Hepatitis C virus and Foot-and-mouth disease virus.\n\nMany enveloped viruses also enter the cell through endocytosis. Entry via the endosome guarantees low pH and exposure to proteases which are needed to open the viral capsid and release the genetic material inside. Further, endosomes transport the virus through the cell and ensure that no trace of the virus is left on the surface, which could be a substrate for immune recognition.\n\nA third and more specific example, is by simply attaching to the surface of the cell via receptors on the cell, and injecting only its genome into the cell, leaving the rest of the virus on the surface. This is restricted to viruses in which only the gene is required for infection of a cell (most positive-sense, single-stranded RNA viruses because they can be immediately translated) and further restricted to viruses that actually exhibit this behavior. The best studied example includes the bacteriophages; for example, when the tail fibers of the T2 phage land on a cell, its central sheath pierces the cell membrane and the phage injects DNA from the head capsid directly into the cell.\n\nOnce a virus is in a cell, it will activate formation of proteins (either by itself or using the host) to gain full control of the host cell, if it is able to. Control mechanisms include the suppression of intrinsic cell defenses, suppression of cell signaling and suppression of host cellular transcription and translation. Often, it is these cytotoxic effects that lead to the death and decline of a cell infected by a virus.\n\nA cell is classified as susceptible to a virus if the virus is able to enter the cell. After the introduction of the viral particle, unpacking of the contents (viral proteins in the tegument and the viral genome via some form of nucleic acid) occurs as preparation of the next stage of viral infection: viral replication.\n", "id": "6074076", "title": "Viral entry"}
{"url": "https://en.wikipedia.org/wiki?curid=1649775", "text": "Viral replication\n\nViral replication is the formation of biological viruses during the infection process in the target host cells. Viruses must first get into the cell before viral replication can occur. From the perspective of the virus, the purpose of viral replication is to allow production and survival of its kind. By generating abundant copies of its genome and packaging these copies into viruses, the virus is able to continue infecting new hosts. Replication between viruses is greatly varied and depends on the type of genes involved in them. Most DNA viruses assemble in the nucleus while most RNA viruses develop solely in cytoplasm.\n\nViruses multiply only in living cells. The host cell must \nprovide the energy and synthetic machinery and the low molecular-weight precursors for the synthesis of viral proteins and nucleic acids.\n\nThe virus replication occurs in seven stages, namely;\n\nThe virus attaches to the Cell Membrane of the host cell. It then injects its DNA or RNA into the host to initiate infection.\n\nThe cell membrane of the host cell invaginates the virus particle, enclosing it in a pinocytotic vacoule. This protects the cell from antibodies like in the case of the HIV virus.\n\nCell enzymes (from lysosomes) strip off the virus protein coat.\nThis releases or renders accessible the virus nucleic acid or genome.\n\nFor some RNA viruses, the infecting RNA produces messenger RNA (mRNA). This is translation of the genome into protein produces.\nFor others with negative stranded RNA and DNA, viruses are produced by transcription then translation.\n\nThe mRNA is used to instruct the host cell to make virus components. The virus takes advantage of the existing cell structures to replicate itself.\n\nThe following components are manufactured by the virus through the host's existing organelles:\n\nA virion is simply an active or intact virus particle. In this stage, newly synthesized genome(nucleic acid), and proteins are assembled to form new virus particles.\n\nThis may take place in the cell's nucleus, cytoplasm, or at plasma membrane for most developed viruses.\n\nThe viruses, now being mature are released by either sudden rupture of the cell, or gradual extrusion(budding) of enveloped viruses through the cell membrane.\n\nThe new viruses may invade or attack other cells, or remain dormant in the cell.\nIn the case of bacterial viruses, the release of progeny virions takes place by lysis of the infected bacterium. However, in the case of animal viruses, release usually occurs without cell lysis.\n\nViruses are classed into 7 types of genes, each of which has its own families of viruses, which in turn have differing replication strategies themselves. David Baltimore, a Nobel Prize-winning biologist, devised a system called the Baltimore Classification System to classify different viruses based on their unique replication strategy. There are seven different replication strategies based on this system (Baltimore Class I, II, III, IV, V, VI, VII). The seven classes of viruses are listed here briefly and in generalities.\n\nThis type of virus usually must enter the host nucleus before it is able to replicate. Some of these viruses require host cell polymerases to replicate their genome, while others, such as adenoviruses or herpes viruses, encode their own replication factors. However, in either cases, replication of the viral genome is highly dependent on a cellular state permissive to DNA replication and, thus, on the cell cycle. The virus may induce the cell to forcefully undergo cell division, which may lead to transformation of the cell and, ultimately, cancer. An example of a family within this classification is the Adenoviridae\n\nThere is only one well-studied example in which a class 1 family of viruses does not replicate within the nucleus. This is the Poxvirus family, which comprises highly pathogenic viruses that infect vertebrates.\n\nViruses that fall under this category include ones that are not as well-studied, but still do pertain highly to vertebrates. Two examples include the Circoviridae and Parvoviridae. They replicate within the nucleus, and form a double-stranded DNA intermediate during replication. A human Circovirus called TTV is included within this classification and is found in almost all humans, infecting them asymptomatically in nearly every major organ.\n\nLike most viruses with RNA genomes, double-stranded RNA viruses do not rely on host polymerases for replication to the extent that viruses with DNA genomes do. Double-stranded RNA viruses are not as well-studied as other classes. This class includes two major families, the Reoviridae and Birnaviridae. Replication is monocistronic and includes individual, segmented genomes, meaning that each of the genes codes for only one protein, unlike other viruses, which exhibit more complex translation.\n\nThese viruses consist of two types, however both share the fact that replication is primarily in the cytoplasm, and that replication is not as dependent on the cell cycle as that of DNA viruses. This class of viruses is also one of the most-studied types of viruses, alongside the double-stranded DNA viruses.\n\nThe positive-sense RNA viruses and indeed all genes defined as positive-sense can be directly accessed by host ribosomes to immediately form proteins. These can be divided into two groups, both of which replicate in the cytoplasm:\n\n\nExamples of this class include the families Coronaviridae, Flaviviridae, and Picornaviridae.\n\nThe negative-sense RNA viruses and indeed all genes defined as negative-sense cannot be directly accessed by host ribosomes to immediately form proteins. Instead, they must be transcribed by viral polymerases into the \"readable\" complementary positive-sense. These can also be divided into two groups:\n\n\nExamples in this class include the families Orthomyxoviridae, Paramyxoviridae, Bunyaviridae, Filoviridae, and Rhabdoviridae (which includes rabies).\n\nA well-studied family of this class of viruses include the retroviruses. One defining feature is the use of reverse transcriptase to convert the positive-sense RNA into DNA. Instead of using the RNA for templates of proteins, they use DNA to create the templates, which is spliced into the host genome using integrase. Replication can then commence with the help of the host cell's polymerases\n\nThis small group of viruses, exemplified by the Hepatitis B virus, have a double-stranded, gapped genome that is subsequently filled in to form a covalently closed circle (ccc DNA) that serves as a template for production of viral mRNAs and a subgenomic RNA. The pregenome RNA serves as template for the viral reverse transcriptase and for production of the DNA genome.\n", "id": "1649775", "title": "Viral replication"}
{"url": "https://en.wikipedia.org/wiki?curid=14353229", "text": "Viral shedding\n\nViral shedding refers to the expulsion and release of virus progeny following successful reproduction during a host-cell infection. Once replication has been completed and the host cell is exhausted of all resources in making viral progeny, the viruses may begin to leave the cell by several methods.\n\nThe term is used to refer to shedding from a single cell, shedding from one part of the body into another part of the body, and shedding from bodies into the environment where the viruses may infect other bodies.\n\n\"Budding\" through the cell envelope—in effect, borrowing from the cell membrane to create the virus's own viral envelope—is most effective for viruses that need an envelope in the first place. These include enveloped viruses such as HIV, HSV, SARS or smallpox. Prior to budding, the virus may put its own receptor onto the surface of the cell in preparation for the virus to bud through, forming an envelope with the viral receptors already on it. Though budding does not immediately destroy the host cell, this process will slowly use up the cell membrane and eventually lead to the cell's demise. This is also how antiviral responses are able to detect virus-infected cells. Budding has been most extensively studied for viruses of eukaryotes. However, it has been demonstrated that viruses infecting prokaryotes of the domain Archaea also employ this mechanism of virion release.\n\nAnimal cells are programmed to self-destruct when they are under viral attack or damaged in some other way. By forcing the cell to undergo apoptosis or cell suicide, release of progeny into the extracellular space is possible. However, apoptosis does not necessarily result in the cell simply popping open and spilling its contents into the extracellular space. Rather, apoptosis is usually controlled and results in the cell's genome being chopped up, before apoptotic bodies of dead cell material clump off the cell to be absorbed by macrophages. This is a good way for a virus to get into macrophages either to infect them or simply travel to other tissues in the body.\n\nAlthough this process is primarily used by non-enveloped viruses, enveloped viruses may also use this. HIV is an example of an enveloped virus that exploits this process for the infection of macrophages.\n\nViruses also leave cells through exocytosis, in which the host cell is not destroyed. Viral progeny are synthesized within the cell and the host cell's transport system is used to enclose them in vesicles; the vesicles of virus progeny are carried to the cell membrane and then released into the extracellular space. This is used primarily by non-enveloped viruses, although enveloped viruses display this too. An example is the use of recycling viral particle receptors in the enveloped varicella-zoster virus.\n", "id": "14353229", "title": "Viral shedding"}
{"url": "https://en.wikipedia.org/wiki?curid=3783315", "text": "Virus latency\n\nVirus latency (or viral latency) is the ability of a pathogenic virus to lie dormant (latent) within a cell, denoted as the lysogenic part of the viral life cycle. A latent viral infection is a type of persistent viral infection which is distinguished from a chronic viral infection. Latency is the phase in certain viruses' life cycles in which, after initial infection, proliferation of virus particles ceases. However, the viral genome is not fully eradicated. The result of this is that the virus can reactivate and begin producing large amounts of viral progeny without the host being infected by new outside virus, denoted as the lytic part of the viral life cycle, and stays within the host indefinitely.\n\nVirus latency is not to be confused with clinical latency during the incubation period when a virus is \"not\" dormant.\n\nEpisomal latency refers to the use of genetic episomes during latency. In this type, viral genes are stabilized floating in the cytoplasm or nucleus as distinct objects, both as linear or lariat structures. Episomal latency is more vulnerable to ribozymes or host foreign gene degradation than provirus latency.\n\nOne example is Herpes Virus family, \"Herpesviridae\", all of which establish latent infection. Herpes virus include Chicken-pox virus and Herpes simplex viruses (HSV-1, HSV-2), all of which establish episomal latency in neurons and leave linear genetic material floating in the cytoplasm. The \"Gammaherpesvirinae\" subfamily is associated with episomal latency established in cells of the immune system, such as B-cells in the case of Epstein-Barr Virus. In the case of Herpes simplex (HSV), the virus has been shown that it fuses with DNA in neurons, such as brain cells, and HSV reactivates upon even minor chromatin loosening with stress, although the chromatin compacts (becomes latent) upon oxygen and nutrient deprivation.\n\nAdvantages of episomal latency include the fact that the virus may not need to enter the nucleus, and hence may avoid Nuclear Domain 10 (ND10) from activating interferon via that pathway.\n\nDisadvantages include more exposure to cellular defenses, leading to possible degradation of viral gene via cellular enzymes.\n\nReactivation may be due to stress, UV, etc.\n\nA provirus is a virus genome that is integrated into the DNA of a host cell.\n\nAdvantages include automatic host cell division results in replication of the virus's genes, and the fact that it is nearly impossible to remove an integrated provirus from an infected cell without killing the cell.\n\nA disadvantage of this method is the need to enter the nucleus (and the need for packaging proteins that will allow for that). However, viruses that integrate into the host cell's genome can stay there as long as the cell lives.\n\nOne of the best-studied viruses that does this is HIV. HIV uses reverse transcriptase to create a DNA copy of its RNA genome. HIV latency allows the virus to largely avoid the immune system. Like other viruses that go latent, it does not typically cause symptoms while latent. Unfortunately, HIV in proviral latency is nearly impossible to target with antiretroviral drugs.\n\nBoth proviral and episomal latency may require maintenance for continued infection and fidelity of viral genes. Latency is generally maintained by viral genes expressed primarily during latency. Expression of these \"latency-associated\" genes may function to keep the viral genome from being digested by cellular ribozymes or being found out by the immune system. Certain viral gene products (RNA transcripts such as non-coding RNAs and proteins) may also inhibit apoptosis or induce cell growth and division to allow more copies of the infected cell to be produced.\n\nAn example of such a gene product is the \"Latency Associated Transcripts (LAT)\" in Herpes simplex virus, which interfere with apoptosis by downregulating a number of host factors, including Major Histocompatibility Complex (MHC) and inhibiting the apoptotic pathway.\n\nA certain type of latency could be ascribed to the endogenous retroviruses. These viruses have incorporated into the human genome in the distant past, and are now passed through reproduction. Generally these types of viruses have become highly evolved, and have lost the expression of many gene products. Some of the proteins expressed by these viruses have co-evolved with host cells to play important roles in normal processes.\n\nWhile viral latency exhibits no active viral shedding nor causes any pathologies or symptoms, the virus is still able to reactivate via external activators (i.e. sunlight, stress) to cause an acute infection. In the case of Herpes simplex virus, which generally infects an individual for life, a serotype of the virus reactivates occasionally to cause cold sores. Although the sores are quickly resolved by the immune system, they may be a minor annoyance from time to time. In the case of varicella zoster virus, after an initial acute infection (chickenpox) the virus lies dormant until reactivated as herpes zoster.\n\nMore serious ramifications of a latent infection could be the possibility of transforming the cell, and forcing the cell into uncontrolled cell division. This is a result of the random insertion of the viral genome into the hosts own gene and expression of host cellular growth factors for the benefit of the virus. A famous event of this actually happening with gene therapy through the use of retroviral vectors is the Necker Hospital in Paris, where 20 young boys received treatment for a genetic disorder, after which 5 developed leukemia-like syndromes.\n\nThis is also seen with infections of the human papilloma virus in which persistent infection may lead to cervical cancer as a result of cellular transformation.\n\nIn the field of HIV research, proviral latency in specific long-lived cell types is the basis for the concept of one or more viral reservoirs, referring to locations (cell types or tissues) characterized by persistence of latent virus. Specifically, the presence of replication-competent HIV in resting CD4-positive T cells, allows this virus to persist for years without evolving despite prolonged exposure to antiretroviral drugs. This latent reservoir of HIV may explain the inability of antiretroviral treatment to cure HIV infection.\n\n", "id": "3783315", "title": "Virus latency"}
{"url": "https://en.wikipedia.org/wiki?curid=4395348", "text": "Viral life cycle\n\nViruses are only able to replicate themselves by commandeering the reproductive apparatus of cells and making them reproduce the virus's genetic structure instead. Thus, a virus cannot function or reproduce outside a cell, thereby being totally dependent on a host cell in order to survive. Most viruses are species specific, and related viruses typically only infect a narrow range of plants, animals, bacteria, or fungi.\n\nFor the virus to reproduce and thereby establish infection, it must enter cells of the host organism and use those cells' materials. To enter the cells, proteins on the surface of the virus interact with proteins of the cell. Attachment, or adsorption, occurs between the viral particle and the host cell membrane. A hole forms in the cell membrane, then the virus particle or its genetic contents are released into the host cell, where replication of the viral genome may commence.\n\nNext, a virus must take control of the host cell's replication mechanisms. It is at this stage a distinction between susceptibility and permissibility of a host cell is made. Permissibility determines the outcome of the infection. After control is established and the environment is set for the virus to begin making copies of itself, replication occurs quickly by the millions.\n\nAfter a virus has made many copies of itself, it has usually exhausted the cell of its resources. The host cell is now no longer useful to the virus, therefore the cell often dies and the newly produced viruses must find a new host. The process by which virus progeny are released to find new hosts, is called shedding. This is the final stage in the viral life cycle.\n\nSome viruses can \"hide\" within a cell, either to evade the host cell defenses or immune system, or simply because it is not in the best interest of the virus to continually replicate. This hiding is deemed latency. During this time, the virus does not produce any progeny, it remains inactive until external stimuli—such as light or stress—prompts it to activate.\n", "id": "4395348", "title": "Viral life cycle"}
{"url": "https://en.wikipedia.org/wiki?curid=36726853", "text": "Natural genetic engineering\n\nNatural genetic engineering (NGE) is a class of process proposed by molecular biologist James Shapiro to account for novelty created in the course of biological evolution. Shapiro developed this work in several peer-reviewed publications and later in his book \"Evolution: A View from the 21st Century\". He uses NGE to account for several proposed counterexamples to the central dogma of molecular biology (the subsequently partly rejected proposal of 1970 that the direction of the flow of sequence information is only from DNA to DNA or DNA to RNA to proteins, and never the reverse). Shapiro drew from work as diverse as the adaptivity of the mammalian immune system, ciliate macronuclei and epigenetics. The work gained some measure of notoriety after being championed by proponents of Intelligent Design, despite Shapiro's explicit repudiation of that movement.\n\nShapiro first laid out his ideas of natural genetic engineering in 1992 and has continued to develop them in both the primary scientific literature and in work directed to wider audiences, culminating in the 2011 publication of \"Evolution: A View from the 21st Century\".\n\nNatural genetic engineering is a reaction against the modern synthesis and the central dogma of molecular biology. The modern synthesis was formulated before the elucidation of the double-helix structure of DNA and the establishment of molecular biology in its current status of prominence. Given what was known at the time a simple, powerful model of genetic change through undirected mutation (loosely described as \"random\") and natural selection, was seen as sufficient to explain evolution as observed in nature. With the discovery of the nature and roles of nucleic acids in genetics, this model prompted Francis Crick's so-called Central Dogma of Molecular Biology: \"[Sequential] information cannot be transferred back from protein to either protein or nucleic acid.\"\n\nShapiro points out that multiple cellular systems can affect DNA in response to specific environmental stimuli. These \"directed\" changes stand in contrast to both the undirected mutations in the modern synthesis and (in Shapiro's interpretation) the ban on information flowing from the environment into the genome.\n\nIn the 1992 Genetica paper that introduced the concept, Shapiro begins by listing three lessons from molecular genetics:\nFrom these, Shapiro concludes:\n\n[I]t can be argued that much of genome change in evolution results from a genetic engineering process utilizing the biochemical systems for mobilizing and reorganizing DNA structures present in living cells.\nIn a 1997 Boston Review article, Shapiro lists\nfour categories of discoveries made in molecular biology that, in his\nestimation, are not adequately accounted for by the Modern Synthesis: genome organization, cellular repair capabilities, mobile genetic elements and cellular information processing. Shapiro concludes:\nWhat significance does an emerging interface between biology and information\nscience hold for thinking about evolution? It opens up the possibility of\naddressing scientifically rather than ideologically the central issue so hotly\ncontested by fundamentalists on both sides of the Creationist-Darwinist debate:\nIs there any guiding intelligence at work in the origin of species displaying\nexquisite adaptations that range from lambda prophage repression and the Krebs\ncycle through the mitotic apparatus and the eye to the immune system, mimicry,\nand social organization?\nWithin the context of the article in particular and Shapiro's work on Natural\nGenetic Engineering in general, the \"guiding intelligence\" is to be found\nwithin the cell. (For example, in a Huffington Post essay entitled\nCell Cognition and Cell Decision-Making Shapiro\ndefines cognitive actions as those that are \"knowledge-based and involve decisions appropriate\nto acquired information,\" arguing that cells meet this criteria.) However,\nthe combination of disagreement with the Modern Synthesis and discussion of\na creative intelligence has brought his work to the attention of advocates\nof Intelligent Design.\n\nNatural genetic engineering has been cited as a legitimate scientific controversy (in contrast to the controversies raised by various branches of creationism). While Shapiro considers the questions raised by Intelligent Design to be interesting, he parts ways with creationists by considering these problems to be scientifically tractable (specifically by understanding how NGE plays a role in the evolution of novelty).\n\nWith the publication of \"Evolution: A View from the 21st Century\",\nShapiro's work again came under discussion in the Intelligent design community.\nIn a conversation with Shapiro, William Dembski asked for Shapiro's\nthoughts on the origins of natural genetic engineering systems. Shapiro replied that \"where they come from in the first place is not a question we can realistically answer right now.\"\nWhile Dembski sees this position as at least not inconsistent with Intelligent\nDesign, Shapiro has explicitly and repeatedly rejected both creationism in\ngeneral and Intelligent Design in particular.\n\nWhile Shapiro developed NGE in the peer-reviewed literature, the idea attracted far more attention when he summarized his work in his book \"Evolution: A View from the 21st Century\".\n\nShapiro responded to the review in \"Evolutionary Intelligence\".\n", "id": "36726853", "title": "Natural genetic engineering"}
{"url": "https://en.wikipedia.org/wiki?curid=41537818", "text": "Evolution of the opposed cerebral hemisphere control\n\nParts of the visual system and the motor control can be explained by the evolution of the brain and an additional stage in the evolution of the eye. Today, in the visual system the left visual field is represented in the right hemisphere and the right visual field in the left hemisphere (Figure 1, Right). In addition, the left hemisphere controls the right part of the body, and the right hemisphere controls the left part of the body.\n\nDuring eye evolution there existed an additional convex stadium of the retina (Figure 1, Left), which explains why today the left visual field is represented in the right hemisphere and vice versa. With the change from convex to concave retinas, the visual input changed sides on the retina.\n\nEvolutionary stages are sometimes conserved in developmental stages, which serve as a proof for stages during evolution. The convex stadium is also mirrored in the embryonal development in form of the optic vesicle (Figure 2). Also insects have convex eyes which supports the notion of a convex stadium too. Originally, when the retinas were turned outside, the left visual field of the eye went to the left part of the retina and the left hemisphere, and the right to the right (Figure 1, Left); back then the motor control must have been direct. For the protection of the retina, the retina grew into the body. But after the retina did sink in, the light reached the retina vice versa (Figure 1). Now the right visual field of the eye reaches the left part of the retina but still the left hemisphere, and vice versa. This explains why the right visual field of the eye connects today to the left hemisphere and vice versa. \n\nThe efferent motor neurons on the other hand adapted to the new situation and crossed. The reason is that the visual input on one side still controls the corresponding part of the body and hence the motor control. So the right hemisphere controls the left part of the body and vice versa, which has in addition the advantage that the motor control can be maintained after injury on the injured side.\n\nThe change in the architecture within the eye, from convex to concave, also explains why afferent nerves are located on the retina. It is due to the photo receptors sinking through the afferent nerves.\n\nThe evolution of the retina also hints to a principle of the evolution of senses, namely the retina and the ear: First they are turned to the environment, but for their protection they grow into the body.\n\nToday the eyes are our most important sense with 90% of our perception being visual. Since most efferent motor neurons are crossed it seems that the CNS evolution is strongly influenced by the visual input.\n", "id": "41537818", "title": "Evolution of the opposed cerebral hemisphere control"}
{"url": "https://en.wikipedia.org/wiki?curid=7695103", "text": "Free recall\n\nFree recall is a basic paradigm in the psychological study of memory. In this paradigm, participants study a list of items on each trial, and then are prompted to recall the items in any order (hence the name \"free\" recall). Items are usually presented one at a time for a short duration, and can be any of a number of nameable materials, although traditionally, words from a larger set are chosen. The recall period typically lasts a few minutes, and can involve spoken or written recall. The standard paradigm involves the recall period starting immediately after the final list item; this can be referred to as immediate free recall (IFR) to distinguish it from delayed free recall (DFR). In delayed free recall, a short distraction period is interpolated between the final list item and the start of the recall period. Both immediate free recall and delayed free recall have been used to test certain effects that appear during recall tests, such as the primacy effect and recency effect. Free recall consists of two stages: the emptying of a limited capacity working memory and a reactivation stage.\n\nOne of the basic measures of performance in the free recall paradigm is simply the number of words recalled from a list, which varies with a number of factors, including the list length, the type of material studied, and any task used to process the words (e.g., a simple judgement). When one examines the probability of recall by the position of the item in the list (its serial position), one finds that the initial and terminal items in the list are better remembered than those in the middle (also known as the primacy and recency items, respectively). Primacy effects generally come from the idea that greater attention is devoted to items that appear at the beginning of presentation lists. Murdock presents a classic study of serial position effects in free recall. In his experiment, Murdock used six groups of 103 participants. Each group was given different combinations of list lengths and presentation rates. Three of the groups were shown lists of ten, fifteen, and twenty words with a presentation rate of two seconds per word. The other three groups were shown lists of twenty, thirty, and forty words with a one-second presentation rate for each word. There were 80 lists in total that included randomly selected common English words. After the presentation of each list, subjects were asked to recall as many words as possible in any order. Results from the experiment showed that all groups expressed both primacy effects and recency effects. Recency effects were exhibited regardless of the length of the list, and it was strongest for the words in the last eight serial positions. The primacy effect extended over the first four serial positions.\n\nAnother evidence of the recency effect is found in the way that participants initiate recall of a list: they most often start with terminal (recent) list items (an early description of the recency effect in the probability of first recall can be found in Hogan, 1975). Recency effects come from the notion that terminal list items tend to be better recalled than other items. This particular effect has generated much controversy and experimentation due to the speculation about why items that are rehearsed less should be so well remembered. A standard explanation for these effects is that they represent output from primary memory, or the short-term memory buffer system.\n\nRecency effects show how well subjects can remember the last items relative to how well they remember the other items. Glenberg's theory can be used to determine the magnitude of the recency effect, depending on how effective the retrieval cues are for the last item relative to the other items. Several types of experiments can be done to test the recency effect for free recall. One experiment that is commonly used is the distractor-recall paradigm, as done by Rundus (1980). Another study that exhibits the recency effect during free recall is when subjects learn several different lists followed by recall tests, and then a final unexpected recall test at the end of the experiment where they are required to recall as many items as possible from all of the lists. Results show that participants tend to recall items from the more recent lists.\n\nStudies have also been done to address the best method for recalling lists of unrelated words. In contrast to free recall, another type of study is known as the serial recall paradigm, where participants are asked to recall the presented items in their correct order rather than the order that comes to mind at the time of testing, randomly. Experiments have shown that in comparison to free recall, the serial recall learning curve increases linearly with the number of trials. The purpose of a study by Bruner, Miller, and Zimmerman (1955) was to determine if this learning difference is a result of the order in which the participant sees the items, or if it is instead dependent on the order in which the participant is told to recall the items. The study involved three different conditions: serial recall, free recall with items to be recalled randomized before each trial, and free recall with the order of the items kept constant. The experiment tested nine college students on 18 series of words. In addition to the linear serial recall learning curve, it was found that more words are forgotten when recall is free than when it is serial. This study also supported the notion that the difference between the types of recall depends on the order in which the learner must recall the items, and not on the order in which the items are presented.\n\nBeyond examining the relative probability of particular items being recalled, one can examine the order in which items are retrieved during the recall period. When a participant is asked to recall a set of random words, there is a marked tendency for items from neighboring positions in the study set to also be recalled successively also known as the contiguity effect, characterized by Michael J. Kahana.\n\nClassic studies of free recall often focused on the multi-trial free recall paradigm, in which the same set of items appear on successive trials (although usually the order of the items is scrambled across trials). In this version of the paradigm, researchers would focus on how many trials it took to learn a certain proportion of the items. Tulving (1968) describes the phenomenon of subjective organization, in which words that are recalled successively during the first recall period also tend to be recalled successively during later recall periods.\nIn addition to subjective organization, these multi-trial free recall paradigms are also used to analyze the effects of practice on recall tasks. Improvement in recall of items over multiple trials has been termed the learning-to-learn effect (LTL). To explore the results of practice on item recall, two experiments have been done to compare effects on free recall and ordered recall. The first experiment consisted of multiple presentations of words, and required the subjects to recall the lists by either ordered or free recall. The second experiment had multiple trials, where each trial consisted of the presentation of words followed by a recall test. Participants were given five trials for each of the lists. Results of the experiments showed that in order to produce the learning-to-learn effect in free recall, participants should be given multiple trials rather than multiple presentations.\n\nFree recall studies have given yield to new understanding of neurological processes. In particular, the Dynamic Tagging Theory makes use of statistical data taken from such experiments in formulating a phenomenological explanation of short-term memory. George A. Miller wrote a widely known paper describing the limitations of memory and the power of categories to improve recall, especially in short-term memory. He popularized the short term memory limitation by calling it \"The Magical Number Seven, Plus or Minus Two\".\n", "id": "7695103", "title": "Free recall"}
{"url": "https://en.wikipedia.org/wiki?curid=10477839", "text": "Dual-task paradigm\n\nA dual-task paradigm is a procedure in experimental (neuro)psychology that requires an individual to perform two tasks simultaneously, in order to compare performance with single-task conditions.When performance scores on one and/or both tasks are lower when they are done simultaneously compared to separately, these two tasks interfere with each other, and it is assumed that both tasks compete for the same class of information processing resources in the brain.\n\nFor instance, reciting poetry while riding a bike are two tasks that can be performed just as well separately as simultaneously. However, reciting poetry while writing an essay should deteriorate performance on at least one of these two tasks, because they interfere with each other.\n\nThe interpretation of dual-task paradigms follows the view that human processing resources are limited and shareable and that they can be subdivided into several classes.\n\n", "id": "10477839", "title": "Dual-task paradigm"}
{"url": "https://en.wikipedia.org/wiki?curid=1717129", "text": "Explicit memory\n\nExplicit memory (or declarative memory) is one of the two main types of long-term human memory. It is the conscious, intentional recollection of factual information, previous experiences and concepts. Explicit memory can be divided into two categories: \"episodic memory\", which stores specific personal experiences, and \"semantic memory\", which stores factual information.\n\nDeclarative memory's counterpart is known as implicit memory or procedural memory, which refers to unconscious memories such as skills (e.g. learning to ride a bicycle).\n\nPeople use explicit memory throughout the day, such as remembering the time of an appointment or recollecting an event from years ago. Explicit memory involves conscious recollection, compared with implicit memory which is an unconscious, unintentional form of memory. Remembering a specific driving lesson is an example of explicit memory, while improved driving skill as a result of the lesson is an example of implicit memory.\n\nSometimes, the distinction between explicit memory and declarative memory is made. In such cases, explicit memory relates to any kind of conscious memory, and declarative memory relates to any kind of memory that can be described in words; however, if we assume that a memory cannot be described without being conscious and vice versa, then the two concepts are identical.\n\nEpisodic memory consists of the storage and recollection of observational information attached to specific life-events. These can be memories that happened to the subject directly or just memories of events that happened around them. Episodic memory allows for mental time travel – recalling various contextual and situational details of one's previous experiences.\n\nSome examples of episodic memory include the memory of entering a specific classroom for the first time, the memory of storing your carry-on baggage while boarding a plane headed to a specific destination on a specific day and time, the memory of being notified that you are being terminated from your job, or the memory of notifying a subordinate that they are being terminated from their job. The retrieval of these episodic memories can be thought of as the action of mentally reliving in detail the past events that they concern. Episodic memory is believed to be the system that provides the basic support for semantic memory.\n\nSemantic memory refers to general world knowledge (facts, ideas, meaning and concepts) that can be articulated and is independent of personal experience. This includes world knowledge, object knowledge, language knowledge, and conceptual priming. Semantic memory is distinct from episodic memory, which is our memory of experiences and specific events that occur during our lives, from which we can recreate at any given point. For instance, semantic memory might contain information about what a cat is, whereas episodic memory might contain a specific memory of petting a particular cat. We can learn about new concepts by applying our knowledge learned from things in the past.\n\nOther examples of semantic memory include types of food, capital cities of a geographic region, or the lexicon of a common language, such as a person's vocabulary.\n\nAutobiographical memory is a memory system consisting of episodes recollected from an individual's life, based on a combination of episodic (personal experiences and specific objects, people and events experienced at particular time and place) and semantic (general knowledge and facts about the world) memory.\n\nSpatial memory is the part of memory responsible for recording information about one's environment and its spatial orientation. For example, a person's spatial memory is required in order to navigate around a familiar city, just as a rat's spatial memory is needed to learn the location of food at the end of a maze. It is often argued that in both humans and animals, spatial memories are summarized as a cognitive map. Spatial memory has representations within working, short-term and long-term memory. Research indicates that there are specific areas of the brain associated with spatial memory. Many methods are used for measuring spatial memory in children, adults, and animals.\n\nThe study of human memory stretches back over the last 2000 years. An early attempt to understand memory can be found in Aristotle's major treatise, On the Soul, in which he compares the human mind to a blank slate. He theorized that all humans are born free of any knowledge and are the sum of their experiences. It wasn't until the late 1800s, however, that a young German philosopher by the name of Herman Ebbinghaus developed the first scientific approach to studying memory. While some of his findings have endured and remain relevant to this day (Learning Curve), his greatest contribution to the field of memory research was demonstrating that memory can be studied scientifically. In 1972, Endel Tulving proposed the distinction between episodic and semantic memory. This was quickly adopted and is now widely accepted. Following this, in 1985, Daniel Schacter proposed a more general distinction between explicit (declarative) and implicit (procedural) memory With the recent advances in neuroimaging technology, there have been a multitude of findings linking specific brain areas to declarative memory. Despite these advances in Cognitive psychology, there is still much to be discovered in terms of the operating mechanisms of declarative memory. It is unclear whether declarative memory is mediated by a particular \"memory system\" or if it is more accurately classified as a \"type of knowledge\" and it is not known how or why declarative memory evolved to begin with.\n\nAlthough many psychologists believe that the entire brain is involved with memory, the \"hippocampus\" and surrounding structures appear to be most important in declarative memory specifically. The ability to retain and recall episodic memories is highly dependent on the hippocampus, whereas the formation of new declarative memories relies on both the hippocampus and \"parahippocampus\" Other studies have found that the parahippocampal cortices were related to superior \"Recognition Memory\".\n\nThe Three Stage Model was developed by Eichenbaum, et. Al (2001), and proposes that the hippocampus does three things with episodic memory:\nTo support this model, a version of Piaget's Transitive Inference Task was used to show that the hippocampus is in fact used as the memory space.\n\nWhen experiencing an event for the first time, a link is formed in the hippocampus allowing us to recall that event in the future. Separate links are also made for features related to that event. For example, when you meet someone new, a unique link is created for them. More links are then connected to that person's link so you can remember what colour their shirt was, what the weather was like when you met them, etc. Specific episodes are made easier to remember and recall by repeatedly exposing oneself to them (which strengthens the links in the memory space) allowing for faster retrieval when remembering.\n\nHippocampal cells (\"neurons\") are activated depending on what information one is exposed to at that moment. Some cells are specific to spatial information, certain stimuli (smells, etc.), or behaviours as has been shown in a \"Radial Maze Task\". It is therefore the hippocampus that allows us to recognize certain situations, environments, etc. as being either distinct or similar to others. However, the Three Stage Model does not incorporate the importance of other cortical structures in memory.\n\nThe anatomy of the hippocampus is largely conserved across mammals, and the role of these areas in declarative memory are conserved across species as well. The organization and neural pathways of the hippocampus are very similar in humans and other mammal species. In humans and other mammals, a cross-section of the hippocampus shows the dentate gyrus as well as the dense cell layers of the CA fields. The intrinsic connectivity of these areas are also conserved.\n\nResults from an experiment by Davachi, Mitchell, and Wagner (2003) and numerous subsequent studies (Davachi, 2006) show that activation in the hippocampus during encoding is related to a subject's ability to recall prior events or later relational memories. These tests did not differentiate between individual test items later seen and those forgotten.\n\nThe lateral Prefrontal cortex (PFC) is essential for remembering contextual details of an experience rather than for memory formation. The PFC is also more involved with episodic memory than semantic memory, although it does play a small role in semantics.\n\nUsing PET studies and word stimuli, Endel Tulving found that remembering is an automatic process. It is also well documented that a hemispheric asymmetry occurs in the PFC: When encoding memories, the Left Dorsolateral PFC (LPFC) is activated, and when retrieving memories, activation is seen in the Right Dorsolateral PFC (RPFC).\n\nStudies have also shown that the PFC is extremely involved with autonoetic consciousness (See Tulving's theory). This is responsible for humans' recollective experiences and 'mental time travelling' abilities (characteristics of episodic memory).\nThe amygdala is believed to be involved in the encoding and retrieval of emotionally charged memories. Much of the evidence for this has come from research on a phenomenon known as flashbulb memories. These are instances in which memories of powerful emotional events are more highly detailed and enduring than regular memories (e.g. September 11 attacks, assassination of JFK). These memories have been linked to increased activation in the amygdala. Recent studies of patients with damage to the amygdala suggest that it is involved in memory for general knowledge, and not for specific information.\n\nThe regions of the diencephalon have shown brain activation when a remote memory is being recovered and the occipital lobe, ventral temporal lobe, and fusiform gyrus all play a role in memory formation.\n\nLesion studies are commonly used in cognitive neuroscience research. Lesions can occur naturally through trauma or disease, or they can be surgically induced by researchers. In the study of declarative memory, the hippocampus and the amygdala are two structures frequently examined using this technique.\n\nThe \"Morris water navigation task\" tests spatial learning in rats. In this test rats learn to escape from a pool by swimming toward a platform submerged just below the surface of the water. Visual cues that surround the pool (e.g. a chair or window) help the rat to locate the platform on subsequent trials. The rats' use of specific events, cues and places are all forms of declarative memory. Two groups of rats are observed: a control group with no lesions and an experimental group with hippocampal lesions. In this task created by Morris, \"et al.\", rats are placed in the pool at the same position for 12 trials. Each trial is timed and the path taken by the rats is recorded. Rats with hippocampal lesions successfully learn to find the platform. If the starting point is moved, the rats with hippocampal lesions typically fail to locate the platform. The control rats, however, are able to find the platform using the cues acquired during the learning trials. This demonstrates the involvement of the hippocampus in declarative memory.\n\nThe \"Odor-odor Recognition Task\", devised by Bunsey and Eichenbaum, involves a social encounter between two rats (a \"subject\" and a \"demonstrator\"). The demonstrator, after eating a specific type of food, interacts with the subject rat, who then smells the food odor on the other's breath. The experimenters then present the subject rat with a decision between two food options; the food previously eaten by the demonstrator, and a novel food. The researchers found that when there was no time delay, both control rats and rats with lesions chose the familiar food. After 24 hours, however, the rats with hippocampal lesions were just as likely to eat both types of food, while control rats chose the familiar food. This can be attributed to the inability to form episodic memories due to lesions in the hippocampus. The effects of this study can be observed in humans with amnesia, indicating the role of the hippocampus in developing episodic memories that can be generalized to similar situations.\n\nHenry Molaison, previously known as H.M., had parts of both his left and right medial temporal lobes (hippocampi) removed which resulted in the loss of the ability to form new memories. The long-term declarative memory was crucially affected when the structures from the medial temporal lobe were removed, including the ability to form new semantic knowledge and memories. The dissociation in Molaison between the acquisition of declarative memory and other kinds of learning was seen initially in motor learning. Molaison's declarative memory was not functioning, as was seen when Molaison completed the task of repetition priming. His performance does improve over trials, however, his scores were inferior to those of control participants. In the condition of Molaison the same results from this priming task are reflected when looking at the other basic memory functions like remembering, recall and recognizing. Lesions should not be interpreted as an all-or-nothing condition, in the case of Molaison not all memory and recognition is lost, although the declarative memory is severely damaged he still has a sense of self and memories that were developed before the lesion occurred.\n\nPatient R.B. was another clinical case reinforcing the role of the hippocampus in declarative memory. After suffering an ischemic episode during a cardiac bypass operation, Patient R.B. awoke with a severe anterograde amnesic disorder. IQ and cognition were unaffected, but declarative memory deficits were observed (although not to the extent of that seen in Molaison). Upon death, an autopsy revealed that Patient R.B. had bilateral lesions of the CA1 cell region along the whole length of the hippocampus.\n\nAdolph, Cahill and Schul completed a study showing that emotional arousal facilitates the encoding of material into long term declarative memory. They selected two subjects with bilateral damage to the amygdala, as well as six control subjects and six subjects with brain damage. All subjects were shown a series of twelve slides accompanied by a narrative. The slides varied in the degree to which they evoked emotion – slides 1 through 4 and slides 9 through 12 contain non-emotional content. Slides 5 through 8 contain emotional material, and the seventh slide contained the most emotionally arousing image and description (a picture of surgically repaired legs of a car crash victim).\n\nThe emotionally arousing slide (slide 7) was remembered no better by the bilateral damage participants than any of the other slides. All other participants notably remembered the seventh slide the best and in most detail out of all the other slides. This shows that the amygdala is necessary to facilitate encoding of declarative knowledge regarding emotionally arousing stimuli, but is not required for encoding knowledge of emotionally neutral stimuli.\n\nStress may have an effect on the recall of declarative memories. Lupien, et al. completed a study that had 3 phases for participants to take part in. Phase 1 involved memorizing a series of words, phase 2 entailed either a stressful (public speaking) or non-stressful situation (an attention task), and phase 3 required participants to recall the words they learned in phase 1. There were signs of decreased declarative memory performance in the participants that had to complete the stressful situation after learning the words. Recall performance after the stressful situation was found to be worse overall than after the non-stressful situation, where performance differed based on whether the participant responded to the stressful situation with an increase in measured levels of salivary cortisol.\n\nPosttraumatic stress disorder (PTSD) emerges after exposure to a traumatic event eliciting fear, horror or helplessness that involves bodily injury, the threat of injury, or death to one's self or another person The chronic stress in PTSD contributes to an observed decrease in hippocampal volume and declarative memory deficits.\n\nCortisol is the primary glucocorticoid in the human body. In the brain, it modulates the ability of the hippocampus and prefrontal cortex to process memories. Although the exact molecular mechanism of how glucocorticoids influence memory formation is unknown, the presence of glucocorticoid receptors in the hippocampus and prefrontal cortex tell us these structures are some of its many targets. It has been demonstrated that cortisone, a glucocorticoid, impaired blood flow in the right parahippocampal gyrus, left visual cortex and cerebellum.\n\nA study by Damoiseaux et al. (2007) evaluated the effects of glucocorticoids on hippocampal and prefrontal cortex activation during declarative memory retrieval. They found that administration of hydrocortisone (name given to cortisol when it is used as a medication) to participants one hour before retrieval of information impairs free recall of words, yet when administered before or after learning they had no effect on recall. They also found that hydrocortisone decreases brain activity in the above-mentioned areas during declarative memory retrieval. Therefore, naturally occurring elevations of cortisol during periods of stress lead to impairment of declarative memory.\n\nIt is important to note that this study involved only male subjects, which may be significant as sex steroid hormones may have different effects in response to cortisol administration. Men and women also respond to emotional stimuli differently and this may affect cortisol levels. This was also the first Functional magnetic resonance imaging(fMRI) study done utilising glucocorticoids, therefore more research is necessary to further substantiate these findings.\n\nIt is believed that sleep plays an active role in consolidation of declarative memory. Specifically, sleep's unique properties enhance \"memory consolidation\", such as the reactivation of newly learned memories during sleep. For example, it has been suggested that the central mechanism for consolidation of declarative memory during sleep is the reactivation of hippocampal memory representations. This reactivation transfers information to neocortical networks where it is integrated into long-term representations. Studies on rats involving maze learning found that hippocampal neuronal assemblies that are used in the encoding of spatial information are reactivated in the same temporal order. Similarly, positron emission tomography (PET) has shown reactivation of the \"hippocampus\" in slow-wave sleep (SWS) after spatial learning. Together these studies show that newly learned memories are reactivated during sleep and through this process new memory traces are consolidated. In addition, researchers have identified three types of sleep (SWS, sleep spindle and REM) in which declarative memory is consolidated.\n\nSlow-wave sleep, often referred to as deep sleep, plays the most important role in consolidation of declarative memory and there is a large amount of evidence to support this claim. One study found that the first 3.5 hours of sleep offer the greatest performance enhancement on memory recall tasks because the first couple of hours are dominated by SWS. Additional hours of sleep do not add to the initial level of performance. Thus this study suggests that full sleep may not be important for optimal performance of memory. Another study shows that people who experience SWS during the first half of their sleep cycle compared to subjects who did not, showed better recall of information. However this is not the case for subjects who were tested for the second half of their sleep cycle, as they experience less SWS.\n\nAnother key piece of evidence regarding SWS's involvement in declarative memory consolidation is a finding that people with pathological conditions of sleep, such as insomnia, exhibit both reduction in \"Slow-Wave Sleep\" and also have impaired consolidation of declarative memory during sleep. Another study found that middle aged people compared to young group had a worse retrieval of memories. This in turn indicated that SWS is associated with poor declarative memory consolidation but not with age itself.\n\nSome researchers suggest that \"sleep spindle\", a burst of brain activity occurring during stage 2 sleep, plays a role in boosting consolidation of declarative memories. Critics point out that spindle activity is positively correlated with intelligence. In contrast, Schabus and Gruber point out that sleep spindle activity only relates to performance on newly learned memories and not to absolute performance. This supports the hypothesis that sleep spindle helps to consolidate recent memory traces but not memory performance in general. The relationship between sleep spindles and declarative memory consolidation is not yet fully understood.\n\nThere is a relatively small body of evidence that supports the idea that \"REM sleep\" helps consolidate highly emotional declarative memories. For instance Wagner, et al. compared memory retention for emotional versus neutral text over two instances; early sleep that is dominated by SWS and late sleep that is dominated by REM phase. This study found that sleep improved memory retention of emotional text only during late sleep phase, which was primarily REM. Similarly, Hu & Stylos-Allen, et al. performed a study with emotional versus neutral pictures and concluded that REM sleep facilitates consolidation of emotional declarative memories.\n\nThe view that sleep plays an \"active\" role in declarative memory consolidation is not shared by all researchers. For instance Ellenbogen, et al. argue that sleep actively protects declarative memory from associative interference. Furthermore, Wixted believes that the sole role of sleep in declarative memory consolidation is nothing more but creating ideal conditions for memory consolidation. For example, when awake, people are bombarded with mental activity which interferes with effective consolidation. However, during sleep, when interference is minimal, memories can be consolidated without associative interference. More research is needed to make a definite statement whether sleep creates favourable conditions for consolidation or it actively enhances declarative memory consolidation.\n\nThe encoding of explicit memory depends on conceptually driven, top-down processing, in which a subject reorganizes the data to store it. The subject makes associations with previously related stimuli or experiences. The later recall of information is thus greatly influenced by the way in which the information was originally processed. \nThe depth-of-processing effect is the improvement in subsequent recall of an object about which a person has given thought to its meaning or shape. Simply put: To create explicit memories, you have to \"do\" something with your experiences: think about them, talk about them, write them down, study them, etc. The more you do, the better you will remember. Testing of information while learning has also shown to improve encoding in explicit memory. If a student reads a text book and then tests themselves afterward, their semantic memory of what was read is improved. This study – test method improves encoding of information. This Phenomenon is referred to as the Testing Effect.\n\nRetrieval: Because a person has played an active role in processing explicit information, the internal cues that were used in processing it can also be used to initiate spontaneous recall. When someone talks about an experience, the words they use will help when they try to remember this experience at a later date. The conditions in which information is memorized can affect recall. If a person has the same surroundings or cues when the original information is presented, they are more likely to remember it. This is referred to as encoding specificity and it also applies to explicit memory. In a study where subjects were asked to perform a cued recall task participants with a high working memory did better than participants with a low working memory when the conditions were maintained. When the conditions were changed for recall both groups dropped. The subjects with higher working memory declined more. This is thought to happen because matching environments activates areas of the brain known as the left inferior frontal gyrus and the hippocampus.\n\nSeveral neural structures are proposed to be involved in explicit memory. Most are in the temporal lobe or closely related to it, such as the amygdala, the hippocampus, the rhinal cortex in the temporal lobe, and the prefrontal cortex. Nuclei in the thalamus also are included, because many connections between the prefrontal cortex and temporal cortex are made through the thalamus. The regions that make up the explicit memory circuit receive input from the neocortex and from brainstem systems, including acetylcholine, serotonin, and noradrenaline systems.\n\nWhile the human brain is certainly regarded for its plasticity, there is some evidence that shows traumatic brain injury (TBI) in young children can have negative effects on explicit memory. Researchers have looked at children with TBI in early childhood (i.e. infancy) and late childhood. Findings showed that children with severe TBI in late childhood experienced impaired explicit memory while still maintaining implicit memory formation. Researchers also found that children with severe TBI in early childhood had both increased chance of having both impaired explicit memory and implicit memory. While children with severe TBI are at risk for impaired explicit memory, the chances of impaired explicit memory in adults with severe TBI is much greater.\n\nAlzheimer's disease has a profound effect on explicit memory. Mild cognitive impairment is an early sign of Alzheimer's disease. People with memory conditions often receive cognitive training. When an fMRI was used to view brain activity after training, it found increased activation in various neural systems that are involved with explicit memory. People with Alzheimer's have problems learning new tasks. However, if the task is presented repeatedly they can learn and retain some new knowledge of the task. This effect is more apparent if the information is familiar. The person with Alzheimer's must also be guided through the task and prevented from making errors. Alzheimer's also has an effect on explicit spatial memory. This means that people with Alzheimer's have difficulty remembering where items are placed in unfamiliar environments. The hippocampus has been shown to become active in semantic and episodic memory. \nThe effects of Alzheimer's disease are seen in the episodic part of explicit memory. This can lead to problems with communication. A study was conducted where Alzheimer's patients were asked to name a variety of objects from different periods. The results shown that their ability to name the object depended on frequency of use of the item and when the item was first acquired. This effect on semantic memory also has an effect on music and tones. Alzheimer's patients have difficulty distinguishing between different melodies they have never heard before. People with Alzheimer's also have issues with picturing future events. This is due to a deficit in episodic future thinking.\n\nAmnesiacs are frequently portrayed in television and movies. Some of the better-known examples include:\n\nIn the romantic comedy \"50 First Dates\" (2004), Adam Sandler plays veterinarian Henry Roth, who falls for Lucy Whitmore, played by Drew Barrymore. Having lost her short term memory in a car crash, Lucy can only remember the current day's events until she falls asleep. When she wakes up the next morning, she has no recollection of the previous day's experiences. These experiences would normally be transferred into declarative knowledge, allowing them to be recalled in the future. Although this movie is not the most accurate representation of a true amnesic patient, it is useful for informing viewers of the detrimental effects of amnesia.\n\n\"Memento\" (2000) a film inspired by the case of Henry Molaison (H.M.). Guy Pearce plays an ex-insurance investigator suffering from severe anterograde amnesia caused by a head injury. Unlike most amnesiacs, Leonard retains his identity and the memories of events that occurred before the injury, but loses all ability to form new memories. This loss of ability to form new memories indicates that the head injury affected the medial temporal lobe of the brain resulting in the inability for Leonard to form declarative memory.\n\n\"Finding Nemo\" features a reef fish named Dory with an inability to develop declarative memory. This prevents her from learning or retaining any new information such as names or directions. The exact origin of Dory's impairment is not mentioned in the film, but her memory loss accurately portrays the difficulties facing amnesiacs.\n\n", "id": "1717129", "title": "Explicit memory"}
{"url": "https://en.wikipedia.org/wiki?curid=8920983", "text": "Reminiscence bump\n\nThe reminiscence bump is the tendency for older adults to have increased recollection for events that occurred during their adolescence and early adulthood. It was identified through the study of autobiographical memory and the subsequent plotting of the age of encoding of memories to form the lifespan retrieval curve.\n\nThe \"lifespan retrieval curve\" is a graph that represents the number of autobiographical memories encoded at various ages during the life span. The lifespan retrieval curve contains three different parts. From birth to five years old is a period of childhood amnesia, from 10 to 30 years old is the reminiscence bump and last is a period of forgetting from the end of the reminiscence bump to present time. The reminiscence bump has been observed on the lifespan retrieval curve in multiple studies.\n\nThe reminiscence bump occurs because memory storage in autobiographical memory is not consistent through time. Rather, memory storage increases during times of changes in the self and in life goals, such as the changes in identity that occur during adolescence.\nResearchers have consistently observed the reminiscence bump, the period of increased memory accessibility in participants' lifespan retrieval curves, and the bump has been reproduced under a range of study conditions.\nAdolescence and early adulthood have been described as special times in memory encoding because individuals typically recall a disproportionate number of autobiographical memories from those periods. The reminiscence bump accounts for this disproportionate number of memories. The reminiscence bump typically occurs between 10 years of age and 30 years of age and is the period that individuals produce the most memories during free recall tasks. Research suggests that memories are easily accessible from the reminiscence bump because they are linked to self-identity. The memories found within the reminiscence bump significantly contribute to an individual's life goals, self-theories, attitudes, and beliefs. Additionally, life events that occur during the period of the reminiscence bump, such as graduation, marriage, or the birth of a child, are often very novel, thus, making them more memorable.\n\nThere are three possible explanations of the reminiscence bump: a cognitive account, a narrative/identity account, and a biological/maturational account.\n\nThe \"cognitive account\" suggests that memories are remembered best because they occur during a period of rapid change followed by a period of relative stability. There is an assumed memory advantage for the novel and distinct events that is followed by a period of stability. The novel events are subject to greater elaborative cognitive processing leading to better encoding of these memories. Moreover, the period of stability that follows increases the stability of the cues for these memories and increases the chances of recall.\n\nThe \"narrative/identity account\" suggests that the reminiscence bump occurs because a sense of identity develops during adolescence and early adulthood. Research suggests that memories that have more influence and significance to one's self are more frequently rehearsed in defining one's identity, and are therefore better remembered later in life (Ece & Gulgoz, 2014). Identity formation provides added motivation for using cognitive processes to ensure recall of these memories. The events from this period are more likely to be organized into a story or view of oneself, and benefits from the advantage of schematic organization in memory.\n\nThe \"biological/maturational account\" suggests that genetic fitness is improved by having many memories that fall within the reminiscence bump. Cognitive capacities are at their optimum from the ages of 10 to 30 and the reminiscence bump may reflect a peak in cognitive performance. This account is therefore sometimes called the \"cognitive abilities\" account. Researchers have suggested that the increase of cognitive ability in early adulthood may cause memories during this time period to be more adequately stored (Ece & Gulgoz, 2014). The reminiscence bump is caused by age-related differences in encoding efficiency, which cause more memories to be stored in adolescence and early adulthood.\n\nThere is one additional theory that explains the occurrence of the reminiscence bump: \"the life script account\". A life script refers to the series of culturally important transitional events that are expected to occur in a certain order at various points during the life span. During early adulthood one starts to make important decisions and have influencing experiences on his or her identity. The memories during this time period are therefore more significantly remembered because they are what has essentially determined and influenced their life script (Habernas & Bluck, 2000). A life script typically has the majority of expected transitional experiences occur during early adulthood (Gluck & Bluck, 2007), and usually include includes positive experiences such as marriage, the birth of a baby, or buying a house. Events that deviate from the life script are typically sad and traumatic. These events, such as the death of a child, are not culturally expected and often do not show a peak of recall at any specific point during the life span. Life scripts act as a way to structure memory and lead to the expectation that the happiest and the most important life events form the reminiscence bump. Contrary to the recall of happy events, the recall of sad events remains stable across the life span and does not exhibit a bump in recall.\n\nUsed objects from his environment to cue memories from his life. Galton created lists of cue-words to stimulate memory recall. He recorded the amount of time required to recall an autobiographical memory related to the cue- word and note the distribution of memories over the life span.\n\nMimicked Galton's technique and had participants recall and date autobiographical memories in response to the cue- words. Participants were encouraged to share the first memory they thought of. This technique is consistent and reproducible under different conditions, such as using different participants or cue- words. There is a consistent peak of recalled memories within the reminiscence bump. However, different types of cue words, such as nouns and adjectives, elicited memories from different time periods and with different phenomenology.\n\nThese researchers presented cue-words in the form of common locations (restaurants, markets, parks), objects (chair, table, television), positive emotions (happy, joy, cheerful), negative emotions (frustration, pain, sad), and significant others (father, mother, friend). Participants were asked to retrieve a memory that reminded them of the cue- word, recall memories from across their lifespan, and recall memories that were at least a year old. Then participants were asked to date the memory to the nearest month and year. The results were graphed on a lifespan retrieval curve, and the reminiscence bump was observed.\n\nHave participants indicate important personal events on a timeline. Participants are given a timeline with intervals (e.g., 5 years per interval) and asked to fill in events that come to mind and indicate age during the event. This gives the participant the ability to recall freely rather than be limited to artificial cue- words. The time intervals focus the participant on certain periods and control the amount of time spent searching for memories.\n\nParticipants are given a time period and asked to recall as many personal events as possible from that period. It assesses the ability to generate both personal semantic and personal incident memories. Recalling the personal semantic memories, participants try to produce as many examples of name of people known to them in a 90-second period. This is repeated for three lifetime periods: childhood, early adulthood, and recent adult life. Recalling personal incident memories, participants try to produce as many personally experienced events as possible in a 90-second period, and it is also repeated for three lifetime periods.\n\nSubjects share personal events and researches compare involuntary and voluntary memories of young and older children. Participants are read ten words and asked to recall as many of the words in no particular order. Then participants were asked to keep a diary on their memory process between two one- hour experimental sessions. They were asked to record autobiographical memories as soon as possible, using a two- page questionnaire for each memory. Allowed researched to keep track of personal thoughts and events.\n\nParticipants provide 20 self-concepts that start with \"I am...\" This enables the collection of concepts and roles that are important to the definition of self. There is no unitary structure to self, it is a combination of self-schemata such as \"I am a mother\". These self-concepts, or self-images, are then used as autobiographical memory cues, enabling the distribution of highly self-relevant memories to be plotted across the lifespan.\n\nThe reminiscence bump can be observed in the distributions of people's favorite books, movies and records.\n\nGeneration identity occurs when an individual recognises that they are part of a particular social subgroup, a cultural generation, and concern themselves with the internalization of external features of the world. The reminiscence bump occurs during a period of life where people form their individual and generational identity. The earlier years of the reminiscence bump coincide with the formation of generation identity, while the later years coincide with the formation of adult identity.\n\nThe influence of generation identity on the reminiscence bump can be attributed to the idea that all members of the subgroup are likely to have memories of similar types of experiences. Evidence attesting to the influence of generation identity on the reminiscence bump has been witnessed in populations that have experience with traumatic events. In 1999, researchers compared younger and older groups of Bangladeshis. The younger group (ages 20–42) showed the usual increase in memories during the reminiscence bump while the older group (ages 46–86) showed a second increase in memories between the ages of 35 and 55. The second bump experienced by the older group corresponded to the period of the Bangladesh Liberation War. The two generations showed similar patterns and timing of memory recall within their own subgroup, suggesting that they did have similar experiences, either living or not living through the war. This finding may suggests that each generation remember personal events and generational events.\n\nResearchers have studied different types of memories in order to find some clues into how the reminiscence bump works and how memories are stored and retrieved. Participants were asked to retrieve memories that were classified as either public or private to try to find differences in terms of when these memories were stored. Public events are events that everyone living at the time know about and are exposed to (political, war/murder, sports/entertainment or news events), but private events are only experienced by the individual (relationships, births/deaths, work/education, home/leisure, illness and religion). When recalling public events, participants were between ages 10 and 19; and when recalling private events, participants were between 20 and 29. Researchers suggest that public events are recalled at an earlier age because individuals are gaining a sense of generational identity. People are starting to create their own beliefs and their individual identity, so these experiences are being rehearsed, practiced and stored in long-term memory. Private events are recalled later because this is when individuals are developing intimate relationship. Private events are more easily stored and recalled because they are unique to each person and are likely occurring without any outside influence.\n\nStudies suggest gender differences in the reminiscence bump. Researchers found that men have more positive life events in their thirties and that women have more positive life events in their twenties. This finding suggests that the timing of important positive life events, such as marriage or employment, differ for men and women. Most of the major life events start between the age of 15 and 30 but there is a small gender difference shown here with respect to when the event takes place. Researchers found that women have a slightly earlier bump than men. The earlier age of the bump in women may be attributed to earlier hormonal changes in adolescence.\n\nThe reminiscence bump has been studied cross-culturally and has been observed in various cultural backgrounds and experiences. It is important to consider that the process of remembering occurs in the context of culture, and memories found within the reminiscence bump are congruent with the culture's goals, values, and belief systems. The age of occurrence of the reminiscence bump is culturally influenced and reflects the age at which a culture identifies individuals as adults. In Western societies the reminiscence bump corresponds with adolescence and early adulthood, approximately from the ages of 10 to 30. In Asian cultures the reminiscence bump appears later than in Western cultures because adulthood may not be reached until 30.\n\nCulture not only influences the age at which the reminiscence bump appears but also the content of the memories found within the bump. Western cultures often value individuality, agency, and distinctiveness, where individuals are encouraged to develop and maintain their private beliefs, attitudes, and personality traits. Alternatively, Asian cultures often value group solidarity, communion, and interconnectedness, where individuals are encouraged to develop a sense of self that is aligned with social roles, duties, and responsibilities. The differences in societal norms expressed by Western and Asian cultures influence the focus of memories. Individuals from Western cultures have memories that cast the individual as the central character and often have self-focused autobiographical memories. Individuals from Asian cultures have memories that express a strong group orientation and often have relation-centered autobiographical memories.\n\nA study done in 2005 showed more accurate age results that were dependent on culture and gender. American women and men showed reminiscence bumps at age 13 and 17, respectively, while Dutch participants show a more progressive development of encoding strength around age 15. Studies with an identical methodology found peaks in the same period with Polish and Japanese participants. Supporting this study, there was research done that compared the distribution of memories of participants from Bangladesh, China, England, Japan, and the United States. The research found a greater number of childhood memories among American participants than in other cultural groups. While the timing of the reminiscence bump is generally thought to be culturally universal, with some minor difference in the period of life from which memories are recalled, there are studies that offer support for the notion that similar life scripts, at least for positive events, might also be found across different cultures.\n\nResearch suggests that people younger than 40 years also show a reminiscence bump. It was initially hidden because of the possibility of the recency effect. When researchers examined the encoding values, it showed a progressive increase with participant age around the reminiscence bump for participants older than 50. This proves that the reminiscence bump is stronger for older adults than younger adults.\n\nThese findings suggest that two processes affect the reminiscence bump phenomenon: (1) Events in adolescence are encoded more strongly than events that occur in other periods of life. (2) Because these events are initially stored more intensely, they will be retrieved more frequently.\n\nA life script is a schema that outlines cultural appropriate events that individuals should experience during a specific time period. (Berntsen & Rubin, 2004). The life script account argues that the reminiscence bump occurs because individuals use life script memories to provide a basis of recalling important memories. The life script events often contain more memories during early adulthood, or the reminiscence bump, than any other age period (Berntsen & Rubin, 2004).The life script account emphasizes norms and expectations typical of a given culture with regard to the timing of major life events. The order and timing of the major events may differ depending on the culture. The life script is a representation of the sequence of normative and major life events. Individuals know the culturally shared expectations of the order and timing of life events in a prototypical life, and are also aware of their own timing in relations to those norms. It assumes that people have an internalized culturally-aware script of the events that make up an expected, skeletal life course; this script acts as a template for the recall of life events in association with each life phase.\n\nA script has two parts: slots and requirements. In the life script, slots are culturally important transitional events that are expected to occur within a circumscribed age span in the life course of an individual; and, the requirement is the prescribed appropriate age for the culturally expected event that leads to causal sequencing, in a series of succeeding events.\n\nThe events in a life script are often positive, celebrated, and normative. It represents and idealized life story. An average life would also include negative events and people would learn from experience. But the life script is handed down from older generations, through stories, and observations of the behavior of others, typically older, people within the same culture. Individuals should also remember more during their young adulthood because that era is the time of biological maturation, new experiences, adult identity formation, and major events in life scripts.\n\nLife scripts have the following ten properties:\n\n(1) A life script is semantic knowledge about the expectations in a given culture about life events, not a form of episodic memory for those events.\n\n(2) A life scripts is a series of temporally ordered events.\n\n(3) A life script can be described in terms of slots and their requirements.\n\n(4) Life scripts form a hierarchical arrangement, with transitional events forming a higher order \"scene\" in which a series of subordinate actions or episodes are nested.\n\n(5) Life scripts are used to process stories—here, life stories.\n\n(6) The slots and their requirements for life scripts are culturally important transitional events and their culturally sanctioned timing.\n\n(7) Because life scripts represent a normative life course, life scripts are not extracted from personal actions in recurrent contexts but are transmitted by tradition. Young people who have lived through only a small part of their lives know the life script of their culture.\n\n(8) Life scripts do not represent an average life but instead represent an idealized life, in that many common and important events are left out.\n\n(9) Life scripts are distorted from actual lives to favor positive events.\n\n(10) Life scripts are distorted from actual lives to favor events expected to occur in the period covered by the bump.\n\nStudies confirmed the hypothesis:\n\n(1) a high overlap among the events generated by the participants, supporting the assumption of a shared cognitive structure\n\n(2) a correlation between the order in which events were generated and their estimated dates, consistent with life scripts having a temporal structure\n\n(3) a dominance of positive events, consistent with the assumption of an idealized version of life\n\n(4) the age estimates for negative events had higher standard deviation than age estimates for positive and neutral events, consistent with the assumption that negative events have more poorly specified temporal slots than the positive events\n\n(5) the frequency by which events were recorded was predicted strongly by valence and by whether the event fell during the period of the bump\n\n(6) a dominance of culturally sanctioned transitional events, rather than purely biological events, consistent with the claim that mainly culturally sanctioned transitional events go into the life script\n\n(7) the majority of positive events were estimated to occur between the ages of 15 and 30 years, whereas the life span for negative events was relatively flat.\n\nModification of the cultural life script theory include: (1) the theory may benefit from explicitly distinguishing between specific transitional events and extended periods of stability (2) life script theory proposes that requests for negative events should rarely active the cultural life script, because negative events are often unexpected. Their study was based on the individual's unique chapters and specific memories, whereas the cultural life script is a de-personalized and culturally shared knowledge structure. Finally, the cultural life script theory seems unable to explain the reminiscence bump for public events.\n\nThere is preferential access to memories of experiences that corresponds with an individual's sense of self especially in relation to the mechanisms critical to someone's sense of independence or dependence.\n\nIt has been identified that individuals with either a strong intimacy motivation or with a distinctive power motivation, and found that the intimacy-motivation group recalled peak experiences with a much higher percentage of intimacy themes, while the power-motivation group tended to recall peak experiences with strong themes of power and satisfaction.\n\nGenerativity refers to nurturing and caring for those things, products, and people that have the potential to outlast the self. Individuals who were judged high in generativity, (i.e. who had a commitment story) were found to recall a higher proportion of events related to aspects of generativity. In contrast, those participants without a prominent disposition towards generativity showed no such bias.\n\nTwo groups of individuals were investigated, those concerned with personal power, achievement, and independence (agentic group) or those concerned with relationships, interdependence, and others (communion group). Agentic types consistently recalled emotional memories of events that involved issues of agency such as those involving mastery, dominance, and humiliation. In contrast, communal types showed a recollection bias for emotional memories featuring others, often significant others, in acts of love and friendship.\n\nDifferent emotions have been shown to affect memory. Life events that have stronger emotions attached will be remembered more vividly. In studies looking at emotional events and the reminiscence bump, older adults tend to remember more positive events than younger adults. Typically, during the reminiscence bump only happy memories and memories of important events are recalled. It is postulated that sad events are easier to forget because there may be an increased motivation to forget them. Conversely, individuals are more likely to recall and relive happy events because they produced pleasurable memories. A second explanation suggests that remembering positive events can help regulate emotions and even enhance moods. It is also possible to regulate emotions through remembering negative events and comparing these events to present positive events. Positive or negative events can also be used to share life experiences with others and compare life events.\n\nFlashbulb memory occurs when a very vivid memory of a traumatic, emotional, or significant event is recalled. Researchers typically use public events such as the John F. Kennedy assassination and 9/11 as cues when studying flashbulb memories. Participants are asked to recall very specific information such as where they were, how they felt, and what they were doing when the event was taking place. Memories of these events are easily recalled and the individual believes their account of the event to be perfectly accurate.\n\nThese memories have been tested in different ways, including measuring the ability to recall details of the event and the impact of additional cues on the recall of the event. Denver, Lane and Cherry found that flashbulb memories that took place in the reminiscence bump were exceptionally vivid and easily accessible. It is suggested that the flashbulb memories encoded during the reminiscence bump are so vivid because the events happened during a time of identity formation and peak brain function. Additionally, these events are recalled well because they undergo more rehearsal due to their serious nature and frequent discussion.\n\nThe impaired functioning of autobiographical memory due to damage or disease can have profound effects on an individual's episodic memory. This can skew an individual's lifespan retrieval curve and influence the presentation of the reminiscence bump. Memories an individual has for personal life events can show a different pattern than the average individual if they have brain damage caused by an event like an accident, a blow to the head or disease. For these individuals with brain damage, the lifespan retrieval curve can look different. An example of this is an individual having the reminiscence bump between the age of 5 and 13 rather than 10 to 30, which is the pattern for the average individual.\n\nBrain injury in the frontal lobes can obscure memories and lead to the loss of details. In more extreme cases patients may even construct available autobiographical knowledge into plausible but false memories.\n\nThe effect of temporal lobe damage on the reminiscence bump is highly variable as a function of age at the time of injury. This is because patients with damage to the temporal lobes or underlying structures in the limbic system may lose the ability to form new memories, especially if the damage is within the hippocampal formation, while retaining access to at least some memories from before the injury. Individuals with this type of brain damage are not able to form new memories after the incident that caused the brain damage, but they still have access to memories that happened before the brain damage occurred. If the brain damage was present at the age of 10, the individual may not remember anything from between the age of 10 and 30 and have no reminiscence bump.\n\nThose with damage to regions of the brain involved in visual processing, such as the occipital lobes, may develop amnesia. The episodic content of autobiographical memories is predominantly encoded in the form of visual images. If the ability to generate visual images is compromised or lost then access to specific details of the past held in episodic images is lost as well. When life events or episodic memories are encoded in the brain, they are in the form of pictures or visual images. They develop amnesia since they can no longer bring these visual images of the past to mind.\n\nA common psychological phenomenon is the severe clouding of autobiographical memory, which results in the overgeneralization of memories. For instance, clinically depressed individuals, schizophrenic individuals, and those suffering from obsessive compulsive disorder tend to recall many memories that lack detail (clouded) and are much more schematic than typical autobiographical memories. In these instances, a patient asked to recall specific memories of his father could only recall general events such as \"walks in the park after Sunday lunch\" and was unable to generate a single specific memory of a single walk.\n\n\n", "id": "8920983", "title": "Reminiscence bump"}
{"url": "https://en.wikipedia.org/wiki?curid=734667", "text": "Iconic memory\n\nIconic memory is the visual sensory memory (SM) register pertaining to the visual domain and a fast-decaying store of visual information. It is a component of the visual memory system which also includes visual short-term memory (VSTM) and long-term memory (LTM). Iconic memory is described as a very brief (<1000 ms), pre-categorical, high capacity memory store. It contributes to VSTM by providing a coherent representation of our entire visual perception for a very brief period of time. Iconic memory assists in accounting for phenomena such as change blindness and continuity of experience during saccades. Iconic memory is no longer thought of as a single entity but instead, is composed of at least two distinctive components. Classic experiments including Sperling's partial report paradigm as well as modern techniques continue to provide insight into the nature of this SM store.\n\nThe occurrence of a sustained physiological image of an object after its physical offset has been observed by many individuals throughout history. One of the earliest documented accounts of the phenomenon was by Aristotle who proposed that afterimages were involved in the experience of a dream. Natural observation of the light trail produced by glowing ember at the end of a quickly moving stick sparked the interest of researchers in the 1700s and 1800s. They became the first to begin empirical studies on this phenomenon which later became known as visible persistence. In the 1900s, the role of visible persistence in memory gained considerable attention due to its hypothesized role as a pre-categorical representation of visual information in visual short-term memory (VSTM). In 1960, George Sperling began his classic partial-report experiments to confirm the existence of visual sensory memory and some of its characteristics including capacity and duration. It was not until 1967 that Ulric Neisser termed this quickly decaying memory store \"iconic memory\". Approximately 20 years after Sperling's original experiments, two separate components of visual sensory memory began to emerge: visual persistence and informational persistence. Sperling's experiments mainly tested the information pertaining to a stimulus, whereas others such as Coltheart performed directs tests of visual persistence. In 1978, Di Lollo proposed a two-state model of visual sensory memory. Although it has been debated throughout history, current understanding of iconic memory makes a clear distinction between visual and informational persistence which are tested differently and have fundamentally different properties. Informational persistence which is the basis behind iconic memory is thought to be the key contributor to visual short term memory as the precategorical sensory store.\nA similar storage area serves as a temporary warehouse for sounds.\n\nThe two main components of iconic memory are \"visible persistence\" and \"informational persistence\". The first is a relatively brief (150 ms) pre-categorical visual representation of the physical image created by the sensory system. This would be the \"snapshot\" of what the individual is looking at and perceiving. The second component is a longer-lasting memory store which represents a coded version of the visual image into post-categorical information. This would be the \"raw data\" that is taken in and processed by the brain. A third component may also be considered which is \"neural persistence\": the physical activity and recordings of the visual system. Neural persistence is generally represented by neuroscientific techniques such as EEG and fMRI.\n\nVisible persistence is the phenomenal impression that a visual image remains present after its physical offset. This can be considered a by-product of neural persistence. Visible persistence is more sensitive to the physical parameters of the stimulus than informational persistence which is reflected in its two key properties.: \nDifferent techniques have been used to attempt to identify the duration of visible persistence. The Duration of Stimulus Technique is one in which a probe stimulus (auditory \"click\") is presented simultaneously with the onset, and on a separate trial, with the offset of a visual display. The difference represents the duration of the visible store which was found to be approximately 100-200 ms. Alternatively, the Phenomenal Continuity and Moving Slit Technique estimated visible persistence to be 300 ms. In the first paradigm, an image is presented discontinuously with blank periods in between presentations. If the duration is short enough, the participant will perceive a continuous image. Similarly, the Moving Slit Technique is also based on the participant observing a continuous image. Only instead of flashing the entire stimulus on and off, only a very narrow portion or \"slit\" of the image is displayed. When the slit is oscillated at the correct speed, a complete image is viewed.\n\nUnderlying visible persistence is neural persistence of the visual sensory pathway. A prolonged visual representation begins with activation of photoreceptors in the retina. Although activation in both rods and cones has been found to persist beyond the physical offset of a stimulus, the rod system persists longer than cones. Other cells involved in a sustained visible image include M and P retinal ganglion cells. M cells (transient cells), are active only during stimulus onset and stimulus offset. P cells (sustained cells), show continuous activity during stimulus onset, duration, and offset. Cortical persistence of the visual image has been found in the primary visual cortex (V1) in the occipital lobe which is responsible for processing visual information.\n\nInformation persistence represents the \"information\" about a stimulus that persists after its physical offset. It is \"visual\" in nature, but not \"visible.\" Sperling's experiments were a test of informational persistence. Stimulus duration is the key contributing factor to the duration of informational persistence. As stimulus duration increases, so does the duration of the visual code. The non-visual components represented by informational persistence include the abstract characteristics of the image, as well as its spatial location. Due to the nature of informational persistence, unlike visible persistence, it is immune to masking effects. The characteristics of this component of iconic memory suggest that it plays the key role in representing a post-categorical memory store for which VSTM can access information for consolidation. \n\nAlthough less research exists regarding the neural representation of informational persistence compared to visual persistence, new electrophysiological techniques have begun to reveal cortical areas involved. Unlike visible persistence, informational persistence is thought to rely on higher-level visual areas beyond the visual cortex. The anterior superior temporal sulcus (STS), a part of the ventral stream, was found to be active in macaques during iconic memory tasks. This brain region is associated with object recognition and object identity. Iconic memory's role in change detection has been related to activation in the middle occipital gyrus (MOG). MOG activation was found to persist for approximately 2000ms suggesting a possibility that iconic memory has a longer duration than what was currently thought. Iconic memory is also influenced by genetics and proteins produced in the brain. Brain-derived neurotrophic factor (BDNF) is a part of the neurotrophin family of nerve growth factors. Individuals with mutations to the BDNF gene which codes for BDNF have been shown to have shortened, less stable informational persistence.\n\nIconic memory provides a smooth stream of visual information to the brain which can be extracted over an extended period of time by VSTM for consolidation into more stable forms. One of iconic memory's key roles is involved with change detection of our visual environment which assists in the perception of motion.\n\nIconic memory enables integrating visual information along a continuous stream of images, for example when watching a movie. In the primary visual cortex new stimuli do not erase information about previous stimuli. Instead the responses to the most recent stimulus contain about equal amounts of information about both this and the preceding stimulus. This one-back memory may be the main substrate for both the integration processes in iconic memory and masking effects. The particular outcome depends on whether the two subsequent component images (i.e., the \"icons\") are meaningful only when isolated (masking) or only when superimposed (integration).\n\nThe brief representation in iconic memory is thought to play a key role in the ability to detect change in a visual scene. The phenomenon of change blindness has provided insight into the nature of the iconic memory store and its role in vision. Change blindness refers to an inability to detect differences in two successive scenes separated by a very brief blank interval, or interstimulus interval (ISI). As such change blindness can be defined as being a slight lapse in iconic memory. When scenes are presented without an ISI, the change is easily detectable. It is thought that the detailed memory store of the scene in iconic memory is erased by each ISI, which renders the memory inaccessible. This reduces the ability to make comparisons between successive scenes.\n\nIt has been suggested that iconic memory plays a role in providing continuity of experience during saccadic eye movements. These rapid eye movements occur in approximately 30 ms and each fixation lasts for approximately 300 ms. Research suggests however, that memory for information between saccades is largely dependent on VSTM and not iconic memory. Instead of contributing to trans-saccadic memory, information stored in iconic memory is thought to actually be erased during saccades. A similar phenomenon occurs during eye-blinks whereby both automatic and intentional blinking disrupts the information stored in iconic memory.\n\nThe development of iconic memory begins at birth and continues as development of the primary and secondary visual system occurs. By 6 months of age, infants' iconic memory capacity approaches adults'. By 5 years of age, children have developed the same unlimited capacity of iconic memory that adults possess. The duration of informational persistence however increases from approximately 200 ms at age 5, to an asymptotic level of 1000 ms as an adult (>11 years). A small decrease in visual persistence occurs with age. A decrease of approximately 20 ms has been observed when comparing individuals in their early 20s to those in their late 60s. Throughout one's lifetime, mild cognitive impairments (MCIs) may develop such as errors in episodic memory (autobiographical memory about people, places, and their contex), and working memory (the active processing component of STM) due to damage in hippocampal and association cortical areas. Episodic memories are autobiographical events that a person can discuss. Individuals with MCIs have been found to show decreased iconic memory capacity and duration. Iconic memory impairment in those with MCIs may be used as a predictor for the development of more severe deficits such as Alzheimer's disease and dementia later in life.\n\nIn 1960, George Sperling became the first to use a partial report paradigm to investigate the bipartite model of VSTM. In Sperling's initial experiments in 1960, observers were presented with a tachistoscopic visual stimulus for a brief period of time (50 ms) consisting of either a 3x3 or 3x4 array of alphanumeric characters such as: \n\nRecall was based on a cue which followed the offset of the stimulus and directed the subject to recall a specific line of letters from the initial display. Memory performance was compared under two conditions: whole report and partial report.\n\nThe whole report condition required participants to recall as many elements from the original display in their proper spatial locations as possible. Participants were typically able to recall three to five characters from the twelve character display (~35%). This suggests that whole report is limited by a memory system with a capacity of four-to-five items.\n\nThe partial report condition required participants to identify a subset of the characters from the visual display using cued recall. The cue was a tone which sounded at various time intervals (~50 ms) following the offset of the stimulus. The frequency of the tone (high, medium, or low) indicated which set of characters within the display were to be reported. Due to the fact that participants did not know which row would be cued for recall, performance in the partial report condition can be regarded as a random sample of an observer's memory for the entire display. This type of sampling revealed that immediately after stimulus offset, participants could recall most letters (9 out of 12 letters) in a given row suggesting that 75% of the entire visual display was accessible to memory. This is a dramatic increase in the hypothesized capacity of iconic memory derived from full-report trials.\n\nA small variation in Sperling's partial report procedure which yielded similar results was the use of a visual bar marker instead of an auditory tone as the retrieval cue. In this modification, participants were presented with a visual display of 2 rows of 8 letters for 50 ms. The probe was a visual bar placed above or below a letter's position simultaneously with array offset. Participants had an average accuracy of 65% when asked to recall the designated letter.\n\nVarying the time between the offset of the display and the auditory cue allowed Sperling to estimate the time course of sensory memory. Sperling deviated from the original procedure by varying tone presentation from immediately after stimulus offset, to 150, 500, or 1000 ms. Using this technique, the initial memory for a stimulus display was found to decay rapidly after display offset. At approximately 1000 ms after stimulus offset, there was no difference in recall between the partial-report and whole report conditions. Overall, experiments using partial report provided evidence for a rapidly decaying sensory trace lasting approximately 1000 ms after the offset of a display\n\nThe effects of masking were identified by the use of a circle presented around a letter as the cue for recall. When the circle was presented before the visual stimulus onset or simultaneously with stimulus offset, recall matched that found when using a bar or tone. However, if a circle was used as a cue 100 ms \"after\" stimulus offset, there was decreased accuracy in recall. As the delay of circle presentation increased, accuracy once again improved. This phenomenon was an example of metacontrast masking. Masking was also observed when images such as random lines were presented immediately after stimulus offset.\n", "id": "734667", "title": "Iconic memory"}
{"url": "https://en.wikipedia.org/wiki?curid=1196714", "text": "Memory-prediction framework\n\nThe memory-prediction framework is a theory of brain function created by Jeff Hawkins and described in his 2004 book \"On Intelligence\". This theory concerns the role of the mammalian neocortex and its associations with the hippocampi and the thalamus in matching sensory inputs to stored memory patterns and how this process leads to predictions of what will happen in the future.\n\nThe theory is motivated by the observed similarities between the brain structures (especially neocortical tissue) that are used for a wide range of behaviours available to mammals. The theory posits that the remarkably uniform \"physical\" arrangement of cortical tissue reflects a single principle or algorithm which underlies all cortical information processing. The basic processing principle is hypothesized to be a feedback/recall loop which involves both cortical and extra-cortical participation (the latter from the thalamus and the hippocampi in particular).\n\nThe memory-prediction framework provides a unified basis for thinking about the adaptive control of complex behavior. Although certain brain structures are identified as participants in the core 'algorithm' of prediction-from-memory, these details are less important than the set of principles that are proposed as basis for all high-level cognitive processing.\n\nThe central concept of the memory-prediction framework is that bottom-up inputs are matched in a hierarchy of recognition, and evoke a series of top-down expectations encoded as potentiations. These expectations interact with the bottom-up signals to both analyse those inputs and generate predictions of subsequent expected inputs. Each hierarchy level remembers frequently observed temporal sequences of input patterns and generates labels or 'names' for these sequences. When an input sequence matches a memorized sequence at a given layer of the hierarchy, a label or 'name' is propagated up the hierarchy – thus eliminating details at higher levels and enabling them to learn higher-order sequences. This process produces increased invariance at higher levels. Higher levels predict future input by matching partial sequences and projecting their expectations to the lower levels. However, when a mismatch between input and memorized/predicted sequences occurs, a more complete representation propagates upwards. This causes alternative 'interpretations' to be activated at higher levels, which in turn generates other predictions at lower levels.\n\nConsider, for example, the process of vision. Bottom-up information starts as low-level retinal signals (indicating the presence of simple visual elements and contrasts). At higher levels of the hierarchy, increasingly meaningful information is extracted, regarding the presence of lines, regions, motions, etc. Even further up the hierarchy, activity corresponds to the presence of specific objects – and then to behaviours of these objects. Top-down information fills in details about the recognized objects, and also about their expected behaviour as time progresses.\n\nThe sensory hierarchy induces a number of differences between the various layers. As one moves up the hierarchy, representations have increased:\n\nThe relationship between sensory and motor processing is an important aspect of the basic theory. It is proposed that the motor areas of the cortex consist of a behavioural hierarchy similar to the sensory hierarchy, with the lowest levels consisting of explicit motor commands to musculature and the highest levels corresponding to abstract prescriptions (e.g. 'resize the browser'). The sensory and motor hierarchies are tightly coupled, with behaviour giving rise to sensory expectations and sensory perceptions driving motor processes.\n\nFinally, it is important to note that all the memories in the cortical hierarchy have to be learnt – this information is not pre-wired in the brain. Hence, the process of extracting this representation from the flow of inputs and behaviours is theorized as a process that happens continually during cognition.\n\nHawkins has extensive training as an electrical engineer. Another way to describe the theory (hinted at in his book) is as a learning hierarchy of feed forward stochastic state machines. In this view, the brain is analyzed as an encoding problem, not too dissimilar from future-predicting error-correction codes. The hierarchy is a hierarchy of abstraction, with the higher level machines' states representing more abstract conditions or events, and these states predisposing lower-level machines to perform certain transitions. The lower level machines model limited domains of experience, or control or interpret sensors or effectors. The whole system actually controls the organism's behavior. Since the state machine is \"feed forward\", the organism responds to future events predicted from past data. Since it is hierarchical, the system exhibits behavioral flexibility, easily producing new sequences of behavior in response to new sensory data. Since the system learns, the new behavior adapts to changing conditions.\n\nThat is, the evolutionary purpose of the brain is to predict the future, in admittedly limited ways, so as to change it.\n\nThe hierarchies described above are theorized to occur primarily in mammalian neocortex. In particular, neocortex is assumed to consist of a large number of columns (as surmised also by Vernon Benjamin Mountcastle from anatomical and theoretical considerations). Each column is attuned to a particular feature at a given level in a hierarchy. It receives bottom-up inputs from lower levels, and top-down inputs from higher levels. (Other columns at the same level also feed into a given column, and serve mostly to inhibit the activation exclusive representations.) When an input is recognized – that is, acceptable agreement is obtained between the bottom-up and top-down sources – a column generates outputs which in turn propagate to both lower and higher levels.\n\nThese processes map well to specific layers within mammalian cortex. (The cortical layers should not be confused with different levels of the processing hierarchy: all the layers in a single column participate as one element in a single hierarchical level). Bottom-up input arrives at layer 4 (L4), whence it propagates to L2 and L3 for recognition of the invariant content. Top-down activation arrives to L2 and L3 via L1 (the mostly axonal layer that distributes activation locally across columns). L2 and L3 compare bottom up and top-down information, and generate either the invariant 'names' when sufficient match is achieved, or the more variable signals that occur when this fails. These signals are propagated up the hierarchy (via L5) and also down the hierarchy (via L6 and L1).\n\nTo account for storage and recognition of \"sequences\" of patterns, a combination of two processes is suggested. The nonspecific thalamus acts as a 'delay line' – that is, L5 activates this brain area, which re-activates L1 after a slight delay. Thus, the output of one column generates L1 activity, which will coincide with the input to a column which is temporally subsequent within a sequence. This time ordering operates in conjunction with the higher-level identification of the sequence, which does not change in time; hence, activation of the sequence representation causes the lower-level components to be predicted one after the other. (Besides this role in sequencing, the thalamus is also active as sensory waystation – these roles apparently involve distinct regions of this anatomically non-uniform structure.)\n\nAnother anatomically diverse brain structure which is hypothesized to play an important role in hierarchical cognition is the hippocampus. It is well known that damage to both hippocampi impairs the formation of long-term declarative memory; individuals with such damage are unable to form new memories of episodic nature, although they can recall earlier memories without difficulties and can also learn new skills. In the current theory, the hippocampi are thought of as the top level of the cortical hierarchy; they are specialized to retain memories of events that propagate all the way to the top. As such events fit into predictable patterns, they become memorizable at lower levels in the hierarchy. (Such movement of memories down the hierarchy is, incidentally, a general prediction of the theory.) Thus, the hippocampi continually memorize 'unexpected' events (that is, those not predicted at lower levels); if they are damaged, the entire process of memorization through the hierarchy is compromised.\n\nThe memory-prediction framework explains a number of psychologically salient aspects of cognition. For example, the ability of experts in any field to effortlessly analyze and remember complex problems within their field is a natural consequence of their formation of increasingly refined conceptual hierarchies. Also, the procession from 'perception' to 'understanding' is readily understandable as a result of the matching of top-down and bottom-up expectations. Mismatches, in contrast, generate the exquisite ability of biological cognition to detect unexpected perceptions and situations. (Deficiencies in this regard are a common characteristic of current approaches to artificial intelligence.)\n\nBesides these subjectively satisfying explanations, the framework also makes a number of testable predictions. For example, the important role that prediction plays throughout the sensory hierarchies calls for anticipatory neural activity in certain cells throughout sensory cortex. In addition, cells that 'name' certain invariants should remain active throughout the presence of those invariants, even if the underlying inputs change. The predicted patterns of bottom-up and top-down activity – with former being more complex when expectations are not met – may be detectable, for example by functional magnetic resonance imaging (fMRI).\n\nAlthough these predictions are not highly specific to the proposed theory, they are sufficiently unambiguous to make verification or rejection of its central tenets possible. See \"On Intelligence\" for details on the predictions and findings.\n\nBy design, the current theory builds on the work of numerous neurobiologists, and it may be argued that most of these ideas have already been proposed by researchers such as Grossberg and Mountcastle. On the other hand, the novel separation of the conceptual mechanism (i.e., bidirectional processing and invariant recognition) from the biological details (i.e., neural layers, columns and structures) lays the foundation for abstract thinking about a wide range of cognitive processes.\n\nThe most significant limitation of this theory is its current lack of detail. For example, the concept of invariance plays a crucial role; Hawkins posits \"name cells\" for at least some of these invariants. (See also Neural ensemble#Encoding for grandmother neurons which perform this type of function, and mirror neurons for a somatosensory system viewpoint.) But it is far from obvious how to develop a mathematically rigorous definition, which will carry the required conceptual load across the domains presented by Hawkins. Similarly, a complete theory will require credible details on both the short-term dynamics and the learning processes that will enable the cortical layers to behave as advertised.\n\nIBM is implementing Hawkins' model.\n\nThe memory-prediction theory claims a common algorithm is employed by all regions in the neocortex. The theory has given rise to a number of software models aiming to simulate this common algorithm using a hierarchical memory structure. The year in the list below indicates when the model was last updated.\n\nThe following models use belief propagation or belief revision in singly connected Bayesian networks.\n\n\n\n\n", "id": "1196714", "title": "Memory-prediction framework"}
{"url": "https://en.wikipedia.org/wiki?curid=732493", "text": "Visual short-term memory\n\nIn the study of vision, visual short-term memory (VSTM) is one of three broad memory systems including iconic memory and long-term memory. VSTM is a type of short-term memory, but one limited to information within the visual domain.\n\nThe term VSTM refers in a theory-neutral manner to the non-permanent storage of visual information over an extended period of time. The visuospatial sketchpad is a VSTM subcomponent within the theoretical model of working memory proposed by Alan Baddeley. Whereas iconic memories are fragile, decay rapidly, and are unable to be actively maintained, visual short-term memories are robust to subsequent stimuli and last over many seconds. VSTM is distinguished from long-term memory, on the other hand, primarily by its very limited capacity.\n\nThe introduction of stimuli which were hard to verbalize, and unlikely to be held in long-term memory, revolutionized the study of VSTM in the early 1970s (Cermak, 1971; Phillips, 1974; Phillips & Baddeley, 1971). The basic experimental technique used required observers to indicate whether two matrices (Phillips, 1974; Phillips & Baddeley, 1971), or figures (Cermak, 1971), separated by a short temporal interval, were the same. The finding that observers were able to report that a change had occurred, at levels significantly above chance, indicated that they were able to encode aspect of the first stimulus in a purely visual store, at least for the period until the presentation of the second stimulus. However, as the stimuli used were complex, and the nature of the change relatively uncontrolled, these experiments left open various questions, such as: (1) whether only a subset of the perceptual dimensions comprising a visual stimulus are stored (e.g., spatial frequency, luminance, or contrast); (2) whether perceptual dimensions are maintained in VSTM with greater fidelity than others; and (3) the nature by which these dimensions are encoded (i.e., are perceptual dimensions encoded within separate, parallel channels, or are all perceptual dimensions stored as a single bound entity within VSTM?).\n\nMuch effort has been dedicated to investigating the capacity limits of VSTM. In a typical change-detection task, observers are presented with two arrays, composed of a number of stimuli. The two arrays are separated by a short temporal interval, and the task of observers is to decide if the first and second arrays are identical, or whether one item differs across the two displays (e.g., Luck & Vogel, 1997). Performance is critically dependent on the number of items in the array. While performance is generally almost perfect for arrays of one or two items, correct responses invariably decline in a monotonic fashion as more items are added. Different theoretical models have been put forward to explain limits on VSTM storage, and distinguishing between them remains an active area of research.\n\nA prominent class of model proposes that observers are limited by the total number of items which can be encoded, either because the capacity of VSTM itself is limited (e.g., Cowan, 2001; Luck & Vogel, 1997; Pashler, 1988). This type of model has obvious similarities to urn models used in probability theory (see, for example, Mendenhall, 1967). In essence, an urn model assumes that VSTM is restricted in storage capacity to only a few items, \"k\" (often estimated to lie in the range of three-to-five in adults, though fewer in children (Riggs, McTaggart & Simpson, 2006)). The probability that a suprathreshold change will be detected is simply the probability that the change element is encoded in VSTM (i.e., \"k\"/\"N\"). This capacity limit has been linked to the posterior parietal cortex, the activity of which initially increases with the number of stimuli in the arrays, but saturates at higher set-sizes (Todd & Marois, 2004). Although urn models are used commonly to describe performance limitations in VSTM (e.g., Luck & Vogel, 1997; Pashler, 1988; Sperling, 1960), it is only recently that the actual structure of items stored has been considered. Luck and colleagues have reported a series of experiments designed specifically to elucidate the structure of information held in VSTM (Luck & Vogel, 1997). This work provides evidence that items stored in VSTM are coherent objects, and not the more elementary features of which those objects are composed.\n\nAn alternative framework has more recently been put forward by Wilken and Ma (2004) who suggest that apparent capacity limitations in VSTM are caused by a monotonic decline in the quality of the internal representations stored (i.e., monotonic increase in noise) as a function of set-size. In this conception capacity limitations in memory are not caused by a limit on the number of things that can be encoded, but by a decline in the quality of the representation of each thing as more things are added to memory. In their 2004 experiments, they varied color, spatial frequency, and orientation of objects stored in VSTM using a signal detection theory approach (see also the closely related work by Palmer, 1990). The participants were asked to report differences between the visual stimuli presented to them in consecutive order. The investigators found that different stimuli were encoded independently and in parallel, and that the major factor limiting report performance was neuronal noise (which is a function of visual set-size).\n\nUnder this framework, the key limiting factor on working memory performance is the precision with which visual information can be stored, not the number of items that can be remembered. Further evidence for this theory was obtained by Bays and Husain (2008) using a discrimination task. They showed that, unlike a \"slot\" model of VSTM, a signal-detection model could account both for discrimination performance in their study and previous results from change detection tasks (e.g. Luck and Vogel, 1997). These authors proposed that VSTM is a flexible resource, shared out between elements of a visual scene—items that receive more resource are stored with greater precision. In support of this, they showed that increasing the salience of one item in a memory array led to that item being recalled with increased resolution, but at the cost of reducing resolution of storage for the other items in the display.\n\nPsychophysical experiments suggest that information is encoded in VSTM across multiple parallel channels, each channel associated with a particular perceptual attribute (Magnussen, 2000). Within this framework, a decrease in an observer's ability to detect a change with increasing set-size can be attributed to two different processes: (1) if decisions are made across different channels, decreases in performance are typically small, and consistent with decreases expected when making multiple independent decisions (Greenlee & Thomas, 1993; Vincent & Regan, 1995); (2) if multiple decisions are made within the same channel, the decrease in performance is much greater than expected on the basis of increased decision-noise alone, and is attributed to interference caused by multiple decisions within the same perceptual channel (Magnussen & Greenlee, 1997).\n\nHowever, the Greenlee-Thomas model (Greenlee & Thomas, 1993) suffers from two failings as a model for the effects of set-size in VSTM. First, it has only been empirically tested with displays composed of one or two elements. It has been shown repeatedly in various experimental paradigms that set-size effects differ for displays composed of a relatively small number of elements (i.e., 4 items or less), and those associated with larger displays (i.e., more than 4 items). The Greenlee-Thomas (1993) model offers no explanation for why this might be so. Second, while Magnussen, Greenlee, and Thomas (1997) are able to use this model to predict that greater interference will be found when dual decisions are made within the same perceptual dimension, rather than across different perceptual dimensions, this prediction lacks quantitative rigor, and is unable to accurately anticipate the size of the threshold increase, or give a detailed explanation of its underlying causes.\n\nIn addition to the Greenlee-Thomas model (Greenlee & Thomas, 1993), there are two other prominent approaches for describing set-size effects in VSTM. These two approaches can be referred to as sample size models (Palmer, 1990), and urn models (e.g., Pashler, 1988). They differ from the Greenlee-Thomas (1993) model by: (1) ascribing the root cause of set-size effects to a stage prior to decision making; and (2) making no theoretical distinction between decisions made in the same, or across different, perceptual dimensions.\n\nA recent study has obtained evidence that could support the existence of an intermediate visual store with characteristics of both iconic memory and VSTM (Sligte et al., 2008). This intermediate store is proposed to have high capacity (up to 15 items) and prolonged memory trace duration (up to 4 seconds). It coexists with VSTM but unlike it visual stimuli can overwrite the contents of its visual store (Pinto et al., 2013). Further research suggests an involvement of visual area V4 (Sligte et al., 2009).\n\nVSTM is thought to be the visual component of the working memory system, and as such it is used as a buffer for temporary information storage during the process of naturally occurring tasks. But what naturally occurring tasks actually require VSTM? Most work on this issue has focused on the role of VSTM in bridging the sensory gaps caused by saccadic eye movements. These sudden shift of gaze typically occur 2–4 times per second, and vision is briefly suppressed while the eyes are moving. Thus, the visual input consists of a series of spatially shifted snapshots of the overall scene, separated by brief gaps. Over time, a rich and detailed long-term memory representation is constructed from these brief glimpses of the input, and VSTM is thought to bridge the gaps between these glimpses and to allow the relevant portions of one glimpse to be aligned with the relevant portions of the next glimpse. Both spatial and object VSTM systems may play important roles in the integration of information across eye movements.\n\n\n", "id": "732493", "title": "Visual short-term memory"}
{"url": "https://en.wikipedia.org/wiki?curid=10245224", "text": "Concurrent overlap\n\nIn human memory research, concurrent overlap, or task appropriate processing, is a type of processing overlap between an activity engaged in before the prospective memory is to be remembered and a cue that directs attention towards the prospective memory. It is prospective memory specific and is distinct from sequential overlap, or transfer-appropriate processing, which occurs in both retrospective and prospective memory and is defined as the overlap in processing the to-be-remembered memory between planning (or study in retrospective memory) and test times.\n", "id": "10245224", "title": "Concurrent overlap"}
{"url": "https://en.wikipedia.org/wiki?curid=1764639", "text": "Levels-of-processing effect\n\nThe levels-of-processing effect, identified by Fergus I. M. Craik and Robert S. Lockhart in 1972, describes memory recall of stimuli as a function of the depth of mental processing. Deeper levels of analysis produce more elaborate, longer-lasting, and stronger memory traces than shallow levels of analysis. Depth of processing falls on a shallow to deep continuum. Shallow processing (e.g., processing based on phonemic and orthographic components) leads to a fragile memory trace that is susceptible to rapid decay. Conversely, deep processing (e.g., semantic processing) results in a more durable memory trace.\n\nThis theory contradicts the multi-store Atkinson-Shiffrin memory model which represents memory strength as being continuously variable, the assumption being that rehearsal always improves long-term memory. They argued that rehearsal that consists simply of repeating previous analyses (maintenance rehearsal) doesn't enhance long-term memory.\n\nIn a study from 1975 (Craik and Tulving) participants were given a list of 60 words. Each word was presented along with three questions. The participant had to answer one of them. Those three questions were in one of three categories. One category of questions was about how the word was presented visually (\"Is the word shown in \"italics\"?\"). The second category of questions was about the phonemic qualities of the word (\"Does the word begin with the sound 'bee'?\"). The third category of questions was presented so that the reader was forced to think about the word within a certain context. (\"Can you meet one in the street [a friend]\"?) The result of this study showed that the words which contained deep processing (the latter) were remembered better.\n\nFamiliarity, transfer-appropriate processing, the self-reference effect, and the explicit nature of a stimulus modify the levels-of-processing effect by manipulating mental processing depth factors.\n\nA stimulus will have a higher recall value if it is highly compatible with preexisting semantic structures (Craik, 1972). According to semantic network theories, this is because such a stimulus will have many connections to other encoded memories, which are activated based on closeness in semantic network structure. This activation increases cognitive analysis, increasing the strength of the memory representation. The familiarity modifier has been tested in implicit memory experiments, where subjects report false memories when presented with related stimuli.\n\nSpecificity of processing describes the increased recall value of a stimulus when presented in the method with which it was inputed. For example, auditory stimuli (spoken words and sounds) have the highest recall value when spoken, and visual stimuli have the highest recall value when a subject is presented with images. In writing tasks, words are recalled most effectively with semantic cues (asking for words with a particular meaning) if they are encoded semantically (self-generated by the subject as being related to a particular meaning). Words are recalled most effectively with data-driven cues (word completion) if they are read, rather than generated by a subject.\n\nLevels of processing have been an integral part of learning about memory. The self-reference effect describes the greater recall capacity for a particular stimulus if it is related semantically to the subject. This can be thought of as a corollary of the familiarity modifier, because stimuli specifically related to an event in a person's life will have widespread activation in that person's semantic network. For example, the recall value of a personality trait adjective is higher when subjects are asked whether the trait adjective applies to them than when asked whether trait adjective has a meaning similar to another trait.\n\nImplicit memory tests, in contrast with explicit memory tests, measure the recall value of a particular stimulus based on later performance on stimulus-related tasks. During these tasks, the subject does not explicitly recall the stimulus, but the previous stimulus still affects performance. For example, in a word-completion implicit memory task, if a subject reads a list containing the word \"dog\", the subject provides this word more readily when asked for three-letter words beginning in \"d\". The levels-of-processing effect is only found for explicit memory tests. One study found that word completion tasks were unaffected by levels of semantic encodings achieved using three words with various levels of meaning in common. Another found that typical level-of-processing effects are reversed in word completion tasks; subjects recalled pictures pairs more completely if they were shown a word representing a picture rather than asked to rate a picture for pleasantness (semantic encoding). Typical level-of-processing theory would predict that picture encodings would create deeper processing than lexical encoding.\n\n\"Memory over the short term and the long term has been thought to differ in many ways in terms of capacity, the underlying neural substrates, and the types of processes that support performance.\"\n\nWe especially remember information if we relate it to ourselves.\nDamage to the hippocampus produces an inability to form or retrieve new long-term memories, but the ability to maintain and reproduce a small subset of information over the short term is typically preserved.\n\nDifferent sensory modes, by their nature, involve different depths of processing, generally producing higher recall value in certain senses than others. However, there is significant room for the modifiers mentioned earlier to affect levels-of-processing to be activated within each sensory mode.\n\nVisual input creates the strongest recall value of all senses, and also allows the widest spectrum of levels-of-processing modifiers. It is also one of the most widely studied. Within visual studies, pictures have been shown to have a greater recall value than words – the picture superiority effect. However, semantic associations have the reverse effect in picture memories appear to be reversed to those in other memories. When logical details are stressed, rather than physical details, an image's recall value becomes lower. When comparing orthographic (capitalization, letter and word shape), phonological (word sound) and semantic (word meaning) encoding cues, the highest levels of recall were found with the meanings of the words, followed by their sounds and finally the written and shape-based cues were found to generate the least ability to stimulate recall.\n\nAuditory stimuli follow conventional levels-of-processing rules, although are somewhat weaker in general recall value when compared with vision. Some studies suggest that auditory weakness is only present for explicit memory (direct recall), rather than implicit memory. When test subjects are presented with auditory versus visual word cues, they only perform worse on directed recall of a spoken word versus a seen word, and perform about equally on implicit free-association tests. Within auditory stimuli, semantic analysis produces the highest levels of recall ability for stimuli. Experiments suggest that levels-of-processing on the auditory level is directly correlated with neural activation.\n\nTactile memory representations are similar in nature to visual representations, although there is not enough data to reliably compare the strength of the two kinds of stimuli. One study suggests that there is a difference in mental processing level due to innate differences between visual and tactile stimuli representations. In this study, subjects were presented with an object in both visual and tactile form (a subject is shown a sphere but cannot touch it, and later is given a similar sphere to only hold and not view). Subjects had more trouble identifying size difference in visual fields than using tactile feedback. A suggestion for the lower level of size processing in visual fields is that it results from the high variance in viewed object size due to perspective and distance.\n\nOdor memory is weaker than visual memory, achieving a successful identification rate of only 70-80% of visual memory. Levels-of-processing effects have been found within odor memory if subjects are asked to \"visualize\" smells and associate them with a particular picture. Subjects who perform this task have a different recall value on explicit memory tests than subjects who memorize smells using self-chosen methods. The difference in recall value, however, depends on the subject, and the subject's ability to form images from odors. Attributing verbal attributes to odors has similar effects. Semantic processing of odors (e.g. attributing the \"mud\" odor to \"smell like a puddle\") has found to have the most positive effects on recall.\n\nSeveral brain imaging studies using positron emission tomography and functional magnetic resonance imaging techniques have shown that higher levels of processing correlate with more brain activity and activity in different parts of the brain than lower levels. For example, in a lexical analysis task, subjects showed activity in the left inferior prefrontal cortex only when identifying whether the word represented a living or nonliving object, and not when identifying whether or not the word contained an \"a\". Similarly, an auditory analysis task showed increased activation in the left inferior prefrontal cortex when subjects performed increasingly semantic word manipulations. Synaptic aspects of word recognition have been correlated with the left frontal operculum and the cortex lining the junction of the inferior frontal and inferior precentral sulcus. The self-reference effect also has neural correlates with a region of the medial prefrontal cortex, which was activated in an experiment where subjects analyzed the relevance of data to themselves. Specificity of processing is explained on a neurological basis by studies that show brain activity in the same location when a visual memory is encoded and retrieved, and lexical memory in a different location. Visual memory areas were mostly located within the bilateral extrastriate visual cortex.\n\nLevels-of-processing effects interact in various ways with mental disorders. In particular, levels-of-processing effects appear to be strengthened in patients with age-related memory degradation, selectively strengthened in panic disorder patients, unaffected in Alzheimer's disease patients, and reversed in autistic patients.\n\nMemory encoding strength derived from higher levels-of-processing appears to be conserved despite other losses in memory function with age. Several studies show that, in older individuals, the ability to process semantically in contrast with non-semantically is improved by this disparity. Neural imaging studies show decreased left-prefrontal cortex activity when words and images are presented to older subjects than with younger subjects, but roughly equal activity when assessing semantic connections.\n\nPanic disorders appear to modify levels-of-processing by increasing ability to recall words with threatening meanings over positive and neutral words. In one study, both implicit (free recall) and explicit (memory of emotional aspects) memorization of word lists were enhanced by threatening meanings in such patients.\n\nModern studies show an increased effect of levels-of-processing in Alzheimer patients. Specifically, there is a significantly higher recall value for semantically encoded stimuli over physically encoded stimuli. In one such experiment, subjects maintained a higher recall value in words chosen by meaning over words selected by numerical order.\n\nIn autistic patients, levels-of-processing effects are reversed in that semantically presented stimuli have a lower recall value than physically presented stimuli. In one study, phonological and orthographic processing created higher recall value in word list-recall tests. Other studies have explicitly found non-semantically processed stimuli to be more accurately processed by autistic patients than in non-autistic patients. No clear conclusions have been drawn as to the cause of this oddity.\n", "id": "1764639", "title": "Levels-of-processing effect"}
{"url": "https://en.wikipedia.org/wiki?curid=10269587", "text": "Echoic memory\n\nEchoic memory is the sensory memory register specific to auditory information (sounds). The sensory memory for sounds that people have just perceived is the form of echoic memory. Unlike visual memory, in which our eyes can scan the stimuli over and over, the auditory stimuli cannot be scanned over and over. Overall, echoic memories are stored for slightly longer periods of time than iconic memories (visual memories). Auditory stimuli are received by the ear one at a time before they can be processed and understood. For instance, hearing the radio is very different from reading a magazine. A person can only hear the radio once at a given time, while the magazine can be read over and over again. It can be said that the echoic memory is like a \"holding tank\" concept, because a sound is unprocessed (or held back) until the following sound is heard, and only then can it be made meaningful. This particular sensory store is capable of storing large amounts of auditory information that is only retained for a short period of time (3–4 seconds). This echoic sound resonates in the mind and is replayed for this brief amount of time shortly after the presentation of auditory stimuli. Echoic memory encrypts only moderately primitive aspects of the stimuli, for example pitch, which specifies localization to the non-association brain regions.\n\nShortly after George Sperling's partial report studies of the visual sensory memory store, researchers began investigating its counterpart in the auditory domain. The term echoic memory was coined in 1967 by Ulric Neisser to describe this brief representation of acoustic information. It was initially studied using similar partial report paradigms to those utilized by Sperling; however, modern neuropsychological techniques have enabled the development of estimations of the capacity, duration, and location of the echoic memory store. Using Sperling's model as an analogue, researchers continue to apply his work to the auditory sensory store using partial and whole report experiments. They found that the echoic store has a duration of up to 4 seconds. However, different durations have been proposed for the existing echo once the hearing signal has been presented. Guttman and Julesz suggested that it may last approximately one second or less, while Eriksen and Johnson suggested that it can take up to 10 seconds.\n\nBaddeley's model of working memory consists of the visuospatial sketchpad which is related to iconic memory, and a phonological loop which attends to auditory information processing in two ways. The first is a phonological store which has the capacity to retain information for 3–4 seconds before decay, which is a much longer duration than iconic memory (which is less than 1000ms). The second is a sub-vocal rehearsal process to keep refreshing the memory trace by the using one's \"inner voice\". However, this model fails to provide a detailed description of the relationship between the initial sensory input and ensuing memory processes.\n\nA short-term memory model proposed by Nelson Cowan attempts to address this problem by describing a verbal sensory memory input and storage in more detail. It suggests a pre-attentive sensory storage system that can hold a large amount of accurate information over a short period of time and consists of an initial phase input of 200-400ms and a secondary phase that transfers the information into a more long term memory store to be integrated into working memory that starts to decay after 10-20s.\n\nFollowing Sperling's (1960) procedures on iconic memory tasks, future researchers were interested in testing the same phenomenon for the auditory sensory store. Echoic memory is measured by behavioural tasks where participants are asked to repeat a sequence of tones, words, or syllables that were presented to them, usually requiring attention and motivation. The most famous partial report task was conducted by presenting participants with an auditory stimulus in the left, right, and both ears simultaneously. Then they were asked to report spatial location and category name of each stimulus. Results showed that spatial location was far easier to recall than semantic information when inhibiting information from one ear over the other. Consistent with results on iconic memory tasks, performance on the partial report conditions were far superior to the whole report condition. In addition, a decrease in performance was observed as the interstimulus interval(ISI) (length of time between presentation of the stimulus and recall) increased.\n\nAuditory backward recognition masking (ABRM) is one of the most successful tasks in studying audition. It involves presenting participants with a brief target stimulus, followed by a second stimulus (the mask) after an ISI. The amount of time the auditory information is available in memory is manipulated by the length of the ISI. Performance as indicated by accuracy of target information increases as the ISI increased to 250 ms. The mask doesn't affect the amount of information obtained from the stimulus, but it acts as interference for further processing.\n\nA more objective, independent task capable of measuring auditory sensory memory that does not require focused attention are mismatch negativity (MMN) tasks, which record changes in activation in the brain by use of electroencephalography (EEG).\nThis records elements of auditory event-related potentials (ERP) of brain activity elicited 150-200ms after a stimulus. This stimulus is an unattended, infrequent, \"oddball\" or deviant stimulus presented among a sequence of standard stimuli, thereby comparing the deviant stimulus to a memory trace.\n\nAuditory sensory memory has been found to be stored in the primary auditory cortex contralateral to the ear of presentation. This echoic memory storage involves several different brain areas, due to the different processes it is involved in. The majority of brain regions involved are located in the prefrontal cortex (PFC) as this is where the executive control is located, and is responsible for attentional control. The phonological store and the rehearsal system appear to be a left-hemisphere based memory system as increased brain activity has been observed in these areas. The major regions involved are the left posterior ventrolateral prefrontal cortex (VLPFC), the left premotor cortex (PMC), and the left posterior parietal cortex (PPC). Within the VLPFC, Broca's area is the main location responsible for verbal rehearsal and the articulatory process. The dorsal PMC is used in rhythmic organization and rehearsal, and finally the PPC shows a role in localizing objects in space.\n\nThe cortical areas in the brain believed to be involved with auditory sensory memory exhibited by MMN response have not been localized specifically. However results have shown comparative activation in the superior temporal gyrus (STG) and in the inferior temporal gyrus (ITG).\n\nAge-related increases in activation within the neural structures responsible for echoic memory have been observed showing that with age comes increased proficiency in the processing auditory sensory information.\n\nFindings of a (MMN) study, also suggest that the duration of auditory sensory memory increases with age, significantly between the ages of two and six years old from 500-5000ms. Children 2 years of age exhibited an MMN response in ISI between 500ms and 1000ms. Children 3 years old have a MMN response from 1 to 2 seconds, 4 year olds over 2 seconds, and 6 year old children from 3 to 5 seconds. These developmental and cognitive changes and that occur at a young age, and extends into adulthood until eventually decreasing again at old age.\n\nResearchers have found shortened echoic memory duration in former late talkers (LT's), children with Precordial catch syndrome (PCS), and oral clefts, with information decaying before 2000 ms. However this reduced echoic memory is not predictive for language difficulties in adulthood.\n\nIn a study, it was found that when words were presented to both younger subjects and adult subjects, that the younger subjects out perform the adult subject as the rate in which the words are presented is increased\n\nAffect echoic memory capacity seems to be independent of age.\n\nChildren with deficits in auditory memory have been shown to have developmental language disorders. These problems are difficult to assess since performance could be due to their inability to understand a given task, rather than a problem with their memory.\n\nPeople with attributed unilateral damage to the dorsolateral prefrontal cortex and temporal-parietal cortex after experiencing a stroke were measured using the a MMN test. For the control group the MMN amplitude was largest in the right hemisphere regardless if the tone was presented in the right or left ear.\n\nMMN was greatly reduced for temporal-parietal damaged patients when the auditory stimulus was presented to the contralateral ear of the lesion side of the brain. This adheres to the theory of auditory sensory memory being stored in the contralateral auditory cortex of ear presentation. Further research on stroke victims with a reduced auditory memory store has shown that listening to daily music or audio books improved their echoic memory. This shows a positive effect of music in neural rehabilitation after brain damage.\n", "id": "10269587", "title": "Echoic memory"}
{"url": "https://en.wikipedia.org/wiki?curid=579359", "text": "Episodic memory\n\nEpisodic memory is the memory of autobiographical events (times, places, associated emotions, and other contextual who, what, when, where, why knowledge) that can be explicitly stated or conjured. It is the collection of past personal experiences that occurred at a particular time and place. For example, if one remembers the party on his or her 6th birthday, this is an episodic memory. They allow an individual to figuratively travel back in time to remember the event that took place at that particular time and place.\n\nSemantic and episodic memory together make up the category of declarative memory, which is one of the two major divisions of memory – the other is implicit memory. The term \"episodic memory\" was coined by Endel Tulving in 1972. He was referring to the distinction between knowing and remembering. Knowing is more factual (semantic) whereas remembering is a feeling that is located in the past (episodic).\n\nTulving has seminally defined three key properties of episodic memory recollection. These are a subjective sense of time (or mental time travel), connection to the self, and autonoetic consciousness. Autonoetic consciousness refers to a special kind of consciousness that accompanies the act of remembering which enables an individual to be aware of the self in a subjective time. Aside from Tulving, others named the important aspects of recollection which includes visual imagery, narrative structure, retrieval of semantic information and the feelings of familiarity.\n\nEvents that are recorded into episodic memory may trigger episodic learning, i.e. a change in behavior that occurs as a result of an event. For example, a fear of dogs after being bitten by a dog is a result of episodic learning.\n\nOne of the main components of episodic memory is the process of recollection. Recollection is a process that elicits the retrieval of contextual information pertaining to a specific event or experience that has occurred.\n\nThere are essentially nine properties of episodic memory that collectively distinguish it from other types of memory. Other types of memory may exhibit a few of these properties, but only episodic memory has all nine:\n\n\nThe formation of new episodic memories requires the medial temporal lobe, a structure that includes the hippocampus. Without the medial temporal lobe, one is able to form new procedural memories (such as playing the piano) but cannot remember the events during which they happened (See the hippocampus and memory).\n\nThe prefrontal cortex (and in particular the right hemisphere) is also involved in the formation of new episodic memories (also known as episodic encoding). Patients with damage to the prefrontal cortex can learn new information, but tend to do so in a disordered fashion. For example, they might show normal recognition of an object they had seen in the past, but fail to recollect when or where it had been viewed. Some researchers believe that the prefrontal cortex helps organize information for more efficient storage, drawing upon its role in executive function. Others believe that the prefrontal cortex underlies semantic strategies which enhance encoding, such as thinking about the meaning of the study material or rehearsing it in working memory.\n\nResearchers do not agree about how long episodic memories are stored in the hippocampus. Some researchers believe that episodic memories always rely on the hippocampus. Others believe the hippocampus only stores episodic memories for a short time, after which the memories are consolidated to the neocortex. The latter view is strengthened by recent evidence that neurogenesis in the adult hippocampus may ease the removal of old memories and increase the efficiency of forming new memories.\n\nEndel Tulving originally described episodic memory as a record of a person's experience that held temporally dated information and spatio-temporal relations. A feature of episodic memory that Tulving later elaborates on is that it allows an agent to imagine traveling back in time. A current situation may cue retrieval of a previous episode, so that context that colours the previous episode is experienced at the immediate moment. The agent is provided with a means of associating previous feelings with current situations. Semantic memory, on the other hand, is a structured record of facts, concepts, and skills that we have acquired. Semantic information is derived from accumulated episodic memory. Episodic memory can be thought of as a \"map\" that ties together items in semantic memory. For example, all encounters with how a \"dog\" looks and sounds will make up the semantic representation of that word. All episodic memories concerning a dog will then reference this single semantic representation of \"dog\" and, likewise, all new experiences with the dog will modify the single semantic representation of that dog.\n\nTogether, semantic and episodic memory make up our declarative memory. They each represent different parts of context to form a complete picture. As such, something that affects episodic memory can also affect semantic memory. For example, anterograde amnesia, from damage of the medial temporal lobe, is an impairment of declarative memory that affects both episodic and semantic memory operations. Originally, Tulving proposed that episodic and semantic memory were separate systems that competed with each other in retrieval. However, this theory was rejected when Howard and Kahana completed experiments on latent semantic analysis (LSA) that supported the opposite. Instead of an increase in semantic similarity when there was a decrease in the strength of temporal associations, the two worked together so semantic cues on retrieval were strongest when episodic cues were strong as well.\n\nEpisodic memory emerges at approximately 3 to 4 years of age. Activation of specific brain areas (mostly the hippocampus) seems to be different between younger (aged 23–39) and older people (aged 67–80) upon episodic memory retrieval. Older people tend to activate both their left and right hippocampus, while younger people activate only the left one.\n\nThe relationship between emotion and memory is complex, but generally, emotion tends to increase the likelihood that an event will be remembered later and that it will be remembered vividly. Flashbulb memory is one example of this. An example of this would be an experience such as a close family member dying or the Christmas that you got the exact toy you wanted as a kid. The experience holds so much emotional significance that it is encoded as an extremely vivid, almost picture-perfect memory. However, whether the vividness of the flashbulb memory is due to a virtual \"flash\" that occurs because of the emotional experience has been hotly contested. Flashbulb memories may occur because of our propensity to rehearse and retell those highly emotional events, which strengthens the memory\n<McCloskey, Wibble & Cohen, 1988>.\n\nIn healthy adults, longterm visual episodic memory can be enhanced specifically through administration of the Acetylcholine esterase inhibitor Donepezil, whereas verbal episodic memory can be improved in persons with the val/val genotype of the val158met polymorphism through administration of the CNS penetrant specific catecholamine-O-methyltransferase inhibitor Tolcapone. Furthermore, episodic memory is enhanced through AZD3480, a selective agonist at the neuronal alpha4beta2 nicotinic receptor, which is developed by the company Targacept. Currently, there are several other products developed by several companies—including new catecholamine-O-methyltransferase inhibitors with fewer side effects—that aim for improving episodic memory. A recent placebo controlled study found that DHEA, which is a functional cortisol antagonist, improves episodic memory in healthy young men (Alhaj et al. 2006).\n\nA 2015 meta-analysis of high quality evidence found that therapeutic doses of amphetamine and methylphenidate improve performance on working memory, episodic memory, and inhibitory control tests in normal healthy adults.\n\n\nTulving (1983) proposed that to meet the criteria of episodic memory, evidence of conscious recollection must be provided. Demonstrating episodic memory in the absence of language, and thus in non-human animals, is impossible, because there are no agreed upon non-linguistic behavioral indicators of conscious experience (Griffiths et al., 1999).\nThis idea was first challenged by Clayton and Dickinson in their work with the western scrub jay (\"Aphelocoma californica\"). They were able to demonstrate that these birds may possess an episodic-like memory system as they found that they remember where they cached different food types, and discriminately recovered them depending on the perishability of the item and time that elapsed since caching. Thus, scrub-jays appear to remember the \"what-where-and-when\" of specific past caching events. The authors argued that such performance meets the behavioral criteria for episodic memory, but referred to the ability as \"episodic-like\" memory because the study did not address the phenomenological aspects of episodic memory.\nAfter a study done by the University of Edinburgh (2006), hummingbirds were the first animal to demonstrate two of the aspects of episodic memory—the ability to recall where certain flowers were located and how recently they were visited. Other studies have demonstrated this type of memory in different animal species, such as dogs, rats, honey bees, and primates.\n\nThe ability of animals to encode and retrieve past experiences relies on the circuitry of the medial temporal lobe, a structure including the hippocampus. Animal lesion studies have provided significant findings related to the importance of particular brain structures in episodic-like memory. For example, hippocampal lesions have severely impacted all three components (what, where, and when) in animals, suggesting that the hippocampus is responsible for detecting novel events, stimuli, and places when forming new memories and on retrieving that information later on.\nDespite similar neural areas and evidence from experiments, some scholars remain cautious about comparisons to human episodic memory (Suddendorf & Busby, 2003). Purported episodic-like memory often seems fixed to a particular domain or could be explained in terms of procedural or semantic memory. The problem may be better tractable by studying episodic memory's adaptive counterpart: the capacity to flexibly imagine future events. However, a recent experiment addressed one of Suddendorf and Busby (2003)'s specific criticisms (the Bischof-Köhler hypothesis, which states that nonhuman animals can only take actions based on immediate needs, as opposed to future needs). Correia and colleagues demonstrated that western scrub-jays can selectively cache different types of foods depending on which type of food they will desire at a future time, offering strong evidence against the Bischof-Köhler hypothesis by demonstrating that scrub-jays can flexibly adjust their behavior based on past experience of desiring a particular food. Similarities and differences between humans and other animals are currently much debated.\n\nAn autobiographical memory is a personal representation of general or specific events and personal facts. Autobiographical memory also refers to memory of a person's history. An individual does not remember exactly everything that has happened in one's past. Memory is constructive, where previous experience affects how we remember events and what we end up recalling from memory. Autobiographical memory is constructive and reconstructed as an evolving process of history. A person's autobiographical memory is fairly reliable; although, the reliability of autobiographical memories is questionable because of memory distortions.\n\nAutobiographical memories can differ for special periods of life. People recall few personal events from the first years of their lives. The loss of these first events is called childhood or infantile amnesia. People tend to recall many personal events from adolescence and early adulthood. This effect is called the reminiscence bump. Additionally, people recall many personal events from their previous few years. For adolescents and young adults the reminiscence bump and the recent events can coincide.\n\nIt is known that autobiographical memories initially are stored as episodic memories, but it is currently unknown if autobiographical memories are the same as episodic memories or if the autobiographical memories become converted to semantic memories with time.\n\n\nEpisodic memories can be stored in autoassociative neural networks (e.g., a Hopfield network) if the stored representation includes information on the spatiotemporal context in which an item was studied.\n\n\n", "id": "579359", "title": "Episodic memory"}
{"url": "https://en.wikipedia.org/wiki?curid=713455", "text": "Memory augmentation\n\nMemory augmentation is the process by which one's ability to retain information is increased. The retrieval of memory has been proven to be faulty in the human brain—it is partially inaccurate and not totally reliable (see more: Recovered memory.) A study conducted by students of the Information Science Department in Nara, Japan sought to measure how able memory is to augment. They used a computer system, the \"Ubiquitous Memories,\" to demonstrate if the technology aided to augmentation better than other methods: notes with a pen and paper, portraits used in a previous trial experiment, and just plain human memory. The results were that the Ubiquitous Memories aided in retrieving memory, and made less mistakes than the other methods.\n\nThe Ubiquitous memories system contains two advantages:\n\nIn order to select an object, the system must detect the object under the two conditions: 1:Dense/dust-covered object; a dense object is an object that is one of thickly gathered objects or one of piled object; a dust- covered object represents an object is not used often(2007).\n\n\"6 Procedures illustrate the Ubiquitous memories system:\"\n\n\n", "id": "713455", "title": "Memory augmentation"}
{"url": "https://en.wikipedia.org/wiki?curid=13366874", "text": "Transfer-appropriate processing\n\nTransfer-appropriate processing, also referred to as TAP, is a type of state-dependent memory specifically showing that memory performance is not only determined by the depth of processing (where associating meaning with information strengthens the memory; see levels-of-processing effect), but by the relationship between how information is initially encoded and how it is later retrieved.\n\nMemory will be best when the processes engaged in during encoding match those engaged in during retrieval. Transfer-appropriate processing (TAP) argues that to have memory successfully recalled there needs to be a successful encoding process. There has been an argument among cognitive psychologists that suggests that the encoding process and retrieval processes are substantially similar. In an experiment that tested TAP researchers found this argument to ring true. They found that successful memory retrieval backs up the encoding process, which therefore has a similar effect on both the retrieval and encoding function. This experiment also pointed out that there are certain variables to consider when looking at TAP because they greatly limit the effectiveness of the retrieval and encoding processes. They believed that to change TAP into a broader form, you would have to question whether the two processing forms actually coincide. Also, TAP is an information-processing action that occurs in two stages; the first includes the procedures that should manipulate the information that coincides with the task activity, and the second stage focuses on the experience that the task activity created. Meaning, that we do not process stimuli all at one time, but instead break it down into a series of responses.\n\n\nIn 1972, Fergus I. M. Craik and Robert S. Lockhart completed studies that went against the idea of multistore theories and were in favor of levels of processing when it comes to the human memory. Craik and Lockhart's studies were some of the first studies completed dealing with Transfer-Appropriate Processing, which is now popular because of their ideas. \nCraik and Lockhart explained that the theory of multistore had very little evidence when it came to capacity, coding and retention. Instead, they proposed that memory involves level of processing. They concluded that we are always building from what we already know through our senses, patterns, and stimulus. Craik and Lockhart completed 10 experiments where participants processed different words by answering questions about them. Depending on the word, the response could be shallow or deep. After this section of the experiment was complete, participants were asked to randomly recall words. They were able to conclude that participants remembered positive and deeper responses more easily. Next, Craik continued his work with Endel Tulving in 1975. They tested subjects individually for perception and speed. Participants had a word revealed to them for 200 ms. through a tachistoscope. Before exposure, questions were asked about the word. These questions were meant to create shallow or deep reactions about the words for the participants. After this was complete, the participants were then asked questions about the words. After these random questions, the participants were asked to recall the words. It was assumed that deeper level questions would be recalled more often. Through four separate experiments, Craik and Tulving found this to be true.\n\nThis phenomenon has been shown by various experiments:\n\nAlthough this theory has many experiments backing up its reliability, many researchers are questioning the levels of processing that TAP seems to fall into. The levels of processing have been under speculation for the fact that they seem untestable and unfalsifiable. They argue that these processing effects are \"circular\" in the sense that deep processing can be considered as just better remembering. They believe that much of the questionability of the processing effects lies between the encoding specificity principle and TAP. The researchers argue that these processing systems function much like Darwin's natural selection theory in that the \"fitness\" of a species and the \"depth of processing\" in the levels of processing cannot fully predict the final outcome, meaning the survival and retrievability of the species or the information processed. They have found that TAP is still vulnerable to this same type of circularity because it lacks a precise and definite definition. Basically, TAP can only be identified as happening only AFTER retrieval has occurred. Roediger and Gallo argue that after 30 years of research, they still cannot identify why or how we get the typical levels-of-processing effect. However, they still believe that even with these doubts that memory retrieval can be studied and subjected to experiments with \"specified\" retrieval conditions. Therefore, the levels-of-processing effect that TAP falls under supports that the \"greater survival\" of deep processing most likely occurs, which means that if they had any doubts about transfer-appropriate processing, they should consider the fact that retrieval has more of a range than a semantic processing theory would support, and more than likely thrive and survive.\n\nAn example of TAP can be compared to the theory of natural selection presented by Darwin in the section above. This means that if a certain species is \"fitter\" than the other species, then that fitter species is more likely to continue to adapt to future environmental situations. Lockhart, who refers to this phenomenon, suggests that if a rabbit and a koala were compared that a rabbit would thrive and survive in many environments whereas the koala has worked itself into a \"narrow ecological niche\". This means the rabbit would excel at surviving because it has a wider range of flexible qualities. Of course it could be argued that there would be certain areas that the koala would thrive in, but they are not as numerous as the survival qualities of the rabbit.\n\n", "id": "13366874", "title": "Transfer-appropriate processing"}
{"url": "https://en.wikipedia.org/wiki?curid=2402111", "text": "Cryptomnesia\n\nCryptomnesia occurs when a forgotten memory returns without it being recognized as such by the subject, who believes it is something new and original. It is a memory bias whereby a person may falsely recall generating a thought, an idea, a tune, or a joke, not deliberately engaging in plagiarism but rather experiencing a memory as if it were a new inspiration.\n\nThe first documented instance of cryptomnesia occurred in 1874 with the medium Stainton Moses.\n\nThe word was first used by the psychiatrist Théodore Flournoy, in reference to the case of medium Hélène Smith (Catherine-Élise Müller) to suggest the high incidence in psychism of \"latent memories on the part of the medium that come out, sometimes greatly disfigured by a subliminal work of imagination or reasoning, as so often happens in our ordinary dreams.\"\n\nCarl Gustav Jung treated the subject in his thesis \"On the Psychology and Pathology of So-Called Occult Phenomena\" (1902) and in an article, \"Cryptomnesia\" (1905), suggested the phenomenon in Friedrich Nietzsche's \"Thus Spoke Zarathustra\". The idea was studied or mentioned by Géza Dukes, Sándor Ferenczi and Wilhelm Stekel as well as by Sigmund Freud in speaking of the originality of his inventions.\n\nIn the first empirical study of cryptomnesia, people in a group took turns generating category examples (e.g., kinds of birds: parrot, canary, etc.). They were later asked to create new exemplars in the same categories that were not previously produced, and also to recall which words they had personally generated. People inadvertently plagiarized about 3–9% of the time either by regenerating another person's thought or falsely recalling someone's thought as their own. Similar effects have been replicated using other tasks such as word search puzzles and in brainstorming sessions.\n\nResearch has distinguished between two kinds of cryptomnesia, though they are often studied together. The distinction between these two types of plagiarism is in the underlying memory bias responsible—specifically, is it the thought that is forgotten, or the thinker? The first type of bias is one of familiarity. The plagiarizer regenerates an idea that was presented earlier, but believes the idea to be an original creation. The idea that is reproduced could be another's idea, or one's own from a previous time. B. F. Skinner describes his own experience of self-plagiarism:\n\nThe second type of cryptomnesia results from an error of authorship whereby the ideas of others are remembered as one's own. In this case, the plagiarizer correctly recognizes that the idea is from an earlier time, but falsely remembers having been the origin for the idea (or, having lost the specific memory of encountering it in print or conversation, assumes that it \"came to\" the plagiarizer as an original idea).\nVarious terms have been coined to distinguish these two forms of plagiarism — occurrence forgetting vs. source forgetting and generation errors vs. recognition errors. The two types of cryptomnesia appear to be independent: no relationship has been found between error rates and the two types are precipitated by different causes.\n\nCryptomnesia is more likely to occur when the ability to properly monitor sources is impaired. For example, people are more likely to falsely claim ideas as their own when they were under high cognitive load at the time they first considered the idea. Plagiarism increases when people are away from the original source of the idea, and decreases when participants are specifically instructed to pay attention to the origin of their ideas. False claims are also more prevalent for ideas originally suggested by persons of the same sex, presumably because the perceptual similarity of the self to a same-sex person exacerbates source confusion. In other studies it has been found that the timing of the idea is also important: if another person produces an idea immediately before the self produces an idea, the other's idea is more likely to be claimed as one's own, ostensibly because the person is too busy preparing for their own turn to properly monitor source information. \n\nAs explained by Carl Jung,\nin \"Man and His Symbols,\" \"An author may be writing steadily to a preconceived plan, working out an argument or developing the line of a story, when he suddenly runs off at a tangent. Perhaps a fresh idea has occurred to him, or a different image, or a whole new sub-plot. If you ask him what prompted the digression, he will not be able to tell you. He may not even have noticed the change, though he has now produced material that is entirely fresh and apparently unknown to him before. Yet it can sometimes be shown convincingly that what he has written bears a striking similarity to the work of another author — a work that he believes he has never seen.\"\n\nJorge Luis Borges's story, \"Pierre Menard, Author of the Quixote,\" is a meta-fictive enactment of cryptomnesia. This work is written in the form of a review or literary critical piece about (the non-existent) Pierre Menard. It begins with a brief introduction and a listing of all of Menard's work.\n\nBorges's \"review\" describes this 20th-century French writer (Menard) who has made an effort to go further than mere \"translation\" of Don Quixote, but to immerse himself so thoroughly as to be able to actually \"re-create\" it, line for line, in the original 16th century Spanish. Thus, Pierre Menard is often used to raise questions and discussion about the nature of accurate translation or, in this case, the hermeneutics of cryptomnesia.\n\nJung gives the following example in \"Man and His Symbols\". Friedrich Nietzsche's book \"Thus Spoke Zarathustra\" includes an almost word for word account of an incident also included in a book published about 1835, half a century before Nietzsche wrote. This is considered to be neither purposeful plagiarism nor pure coincidence: Nietzsche's sister confirmed that he had indeed read the original account when he was 11 years old; and Nietzsche's youthful intellectual prowess, his later cognitive degeneration due to neurosyphilis, and his accompanying psychological deterioration (specifically, his increasing grandiosity as manifested in his later behavior and writings) together strengthen the likelihood that he happened to commit the passage to memory upon initially reading it and later, after having lost his memory of encountering it, assumed that his own mind had created it.\n\nIn some cases, the line between cryptomnesia and zeitgeist (compare the concept of multiple discovery in science) may be somewhat hazy. Readers of Lord Byron's closet drama \"Manfred\" noted a strong resemblance to Goethe's \"Faust\". In a review published in 1820, Goethe wrote, \"Byron's tragedy \"Manfred\" was to me a wonderful phenomenon, and one that closely touched me. This singular intellectual poet has taken my Faustus to himself, and extracted from it the strangest nourishment for his hypochondriac humour. He has made use of the impelling principles in his own way, for his own purposes, so that no one of them remains the same; and it is particularly on this account that I cannot enough admire his genius.\" Byron was apparently thankful for the compliment; however, he claimed that he had\nnever read \"Faustus\".\n\nJ. M. Barrie, the creator of Peter Pan, was aware of the occurrence of cryptomnesia. In \"Peter and Wendy\" Wendy sews Peter's shadow back on and this makes him very happy but he immediately thinks he has attached the shadow himself:\n\n“How clever I am,” he crowed rapturously, “oh, the cleverness of me!”\n\nPeter exhibits a number of other clinically accurate peculiarities of memory suggesting that Barrie regarded Peter's behavior as a memory disorder rather than self-centredness.\n\nHelen Keller compromised her and her teacher's credibility with an incident of cryptomnesia which was misinterpreted as plagiarism. \"The Frost King\", which Keller wrote out of buried memories of a fairy tale read to her four years previously, left Keller a nervous wreck, and unable to write fiction for the rest of her life.\n\nRobert Louis Stevenson refers to an incident of cryptomnesia that took place during the writing of \"Treasure Island\", and that he discovered to his embarrassment several years afterward:\n\nThe precedent in United States copyright law, since 1976, has been to treat alleged cryptomnesia no differently from deliberate plagiarism. The seminal case is \"Bright Tunes Music v. Harrisongs Music,\" where the publisher of \"He's So Fine,\" written and composed by Ronald Mack, demonstrated to the court that George Harrison borrowed substantial portions of his song \"My Sweet Lord\" from \"He's So Fine.\" The Court imposed damages despite a claim that the copying was subconscious. The ruling was upheld by the Second Circuit in \"ABKCO Music v. Harrisongs Music,\" and the case \"Three Boys Music v. Michael Bolton,\" upheld by the Ninth Circuit, affirmed the principle.\n\nIn 1987, Australian author Colleen McCullough published a novella, \"The Ladies of Missalonghi\". Critics alleged that she had plagiarised \"The Blue Castle\", a 1926 novel by L. M. Montgomery. McCullough acknowledged having read Montgomery's works in her youth, but attributed the similarities to subconscious recollection.\n\nIn \"Interpretation and Overinterpretation\", Umberto Eco describes the rediscovery of an antique book among his large collection, which was eerily similar to the pivotal object in his novel \"The Name of the Rose\".\n\n\n", "id": "2402111", "title": "Cryptomnesia"}
{"url": "https://en.wikipedia.org/wiki?curid=1673339", "text": "Hyperfocus\n\nHyperfocus is an intense form of mental concentration or visualization that focuses consciousness on a subject, topic, or task. In some individuals, various subjects or topics may also include daydreams, concepts, fiction, the imagination, and other objects of the mind. Hyperfocus on a certain subject can cause side-tracking away from assigned or important tasks.\n\nHyperfocus may bear a relationship to the concept of flow. In some circumstances both flow and hyperfocus can be an aid to achievement, but in other circumstance or situations, the same focus and behavior could be a liability, distracting from the task at hand. However, unlike hyperfocus, \"flow\" is often described in more glowing terms, suggesting they are not two sides of the same condition under contrasting circumstance or intellect.\n\nHyperfocus may in some cases also be symptomatic of a psychiatric condition. In these cases it is more commonly and accurately referred to as perseveration - an inability or impairment in switching tasks or activities (\"set-shifting\"), or desisting from mental or physical response repetition (gestures, words, thoughts) despite absence or cessation of a stimulus, and which is not excessive in terms of quantity but are apparently both functionless and involve a narrow range of behaviours, and are not better described as stereotypy (a highly repetitive idiosyncratic behaviour).\n\nConditions associated with perseveration include neurodevelopmental disorders, particularly those considered to be on the autism spectrum (especially Asperger syndrome), and attention deficit hyperactivity disorder (ADHD). In the latter, it is informally but probably incorrectly called \"hyperfocus\" and may be a coping mechanism or a symptom of self-regulation impairment–as well as people who are both intellectually gifted and suffer a learning disability who may have either or both of hyperfocus and perseverative behaviours. Other conditions involving dysfunction or disregulation within the frontal lobe could also theoretically have similar effects.\n\nIt is typical for individuals with ADHD to say they 1), cannot focus on boring things and 2), can only focus on stimulating things, and \"that focus is often extreme\". Thus it is both a concentration deficit and over-concentration, or generically: \"hyperfocus.\" More concisely, some types of ADHD are a difficulty in \"directing\" one's attention, (an executive function of the frontal lobe), \"not\" a lack of attention. Glickman & Dodd (1998) found that adults with self-reported ADHD scored higher than normal adults on self-reported ability to hyperfocus on \"urgent tasks\", such as last-minute projects or preparations. Adults in the ADHD group were uniquely able to postpone eating, sleeping and other personal needs and stay absorbed in the \"urgent task\" for an extended time.\n\nClinical conditions unlikely to be confused with hyperfocus often involve repetition of thoughts or behaviors, such as obsessive–compulsive disorder (OCD), trauma, and some cases of traumatic brain injury.\n\n\n", "id": "1673339", "title": "Hyperfocus"}
{"url": "https://en.wikipedia.org/wiki?curid=18374763", "text": "Active recall\n\nActive recall is a principle of efficient learning, which claims the need to actively stimulate memory during the learning process. It contrasts with passive review, in which the learning material is processed passively (e.g. by reading, watching, etc.). For example, reading a text about George Washington, with no further action, is a passive review. Answering the question \"Who was the first US President?\", is active recall. \n\nActive recall exploits the psychological testing effect and is very efficient in consolidating long-term memory. \n\nA study done by J.D. Karpicke and H.L. Roediger, III (2008) lent support to the idea that practicing information retrieval is integral to learning. They had college students study 40 pairs of foreign language words on flash cards. One group learned the words by going through the deck of cards each time until they could recall all the words. The other group's subjects dropped a card whenever they successfully recalled its paired word on the reverse side. Both groups alternated between study and test trials. Furthermore, half of the subjects were tested on the entire list during each test trial, while the other half were only tested on words they failed to recall on previous test trials. The results of a follow-up test on the entire list a week later clearly showed that those who were tested on the entire list during learning were able to recall a greater percentage of the word pairs (~80% as opposed to ~30% for the partial-list tested subjects). Results didn't depend on how the students studied (entire list or only unrecalled pairs), only how they were tested. The authors concluded that more rigorous testing leads to better retrieval in the future.\n\nKarpicke and Janell R. Blunt (2011) followed up in this finding and questioned whether elaborative studying with concept mapping or retrieval-heavy studying was more effective. 200 Subjects who had studied various scientific concepts using more retrieval techniques did 50% better than the other group when tested a week later on their comprehension and ability to infer. Retrieval-heavy studiers performed better than concept-mappers in every measured way, even on questions requiring the creation of concept maps. Thus, they concluded that retrieval techniques aid learning more than elaborative studying. Karpicke believes the next step is to discover better ways to use retrieval in learning.\n\nMcDaniel et al. (2009) came up with the 3R (read-recite-review) method for learning from textbooks. They conducted two experiments that compared the 3R strategy to rereading and note-taking ones. Their results from one of the experiments showed that 3R improved both immediate and delayed (one week) free recall of information. The other one involved more complexity, and its results showed 3R studiers did better than those who reread and as well as note-takers, though the note-takers studied for longer than the 3R group.\n\nThus there is much support that active recall is better than rereading text for enhancing learning. In fact, Karpicke, et al. (2009) believe that students get \"illusions of competence\" from rereading their notes and textbook. One reason for this illusion is that the text contains all the information, so it is easy to glance over it and feel as if it is known well, when that is not the case at all. Better put: in the text, the cue and corresponding target are both present, which is not the case during testing. The results of their study showed that retrieval as a study strategy is rare among students. They prefer to reread instead. \n\nSome critics of active recall claim that using retrieval techniques only improves learning a specific response. However, Karpicke et al. (2009) and Butler (2010) proved that at the very least, information is better remembered. \n\n", "id": "18374763", "title": "Active recall"}
{"url": "https://en.wikipedia.org/wiki?curid=20494159", "text": "Negative transfer (memory)\n\nNegative transfer is a behavioral psychology term that refers to the interference of the previous knowledge with new learning. It relates to the experience with one set of events could hurt performance on related tasks. It also describes the pattern of error in animal learning and behavior. It occurs when a learned, previously adaptive response to one stimulus interferes with the acquisition of an adaptive response to a novel stimulus that is similar to the first.\n\nA common example is switching from a manual transmission vehicle to an automatic transmission vehicle. The adaptive response series in a standard vehicle when it reaches 10 miles per hour is to step on the clutch, shift gears, and step on the accelerator. This previously adaptive response is incompatible with the proper response in an automatic transmission, confirming how prior experience can limit the ability to function efficiently in new settings.\n\nNegative transfer can also correspond with positive transfer and the errors that come with negative transfer can sometimes be made at faster rates. Although positive transfer is more likely than negative transfer, error rates can be much higher when negative transfer does occur than if no previously learned behavior had existed.\n\nFor example, if a mariner already knows how to operate navigation equipment and if negative transfer occurs, he or she may display more errors while learning to operate new equipment than a mariner who has never operated navigation equipment. This is because the old mariner is using previous knowledge that may have transferred negatively and now is having to forget the prior inaccurate knowledge and try to learn new equipment the correct way. This is because the learning that was transferred can not properly apply to the new experience. Another example of this might be the way an experience of the same kind has different protocol for different situations. If a mariner has already learned how to quickly react to an alarm, the same mariner may react to the same alarm on new equipment even more quickly (positive transfer on reaction time) than without already knowing how to react. Although the reaction may happen very quickly, it may be to push the wrong alarm switch located in the same position as the old alarm switch (negative transfer on error rate). In essence, the old behavior contributes to making errors at a faster rate on the new behavior.\n\nA common test for negative transfer is the AB-AC list learning paradigm from the verbal learning research of the 1950s and 1960s. In this paradigm, two lists of paired associates are learned in succession, and if the second set of associations (List 2) constitutes a modification of the first set of associations (List 1), negative transfer results and thus the learning rate of the second list is slower than the first list.\n\nAn elementary example of transfer learning would be, for example, in dealing with subject–verb agreement: When you have a singular subject you have to add and \"s\" on the end of the verb and when you have a plural subject you do not put an \"s\" on the end of the verb. However, when they are the subjects \"I\" and \"You\", they do not follow this rule. Therefore, if students take the previously learned lesson of subject–verb agreement and have not learned the exception properly they are going to be adding \"s\" on the end of the verbs in their sentences, which is incorrect. This is an example of negative transfer because they have taken what they have learned from one set of rules and have applied it to a similar experience but in the wrong way.\n\nStudies have shown that not only by reevaluating the situation and its errors but by comparing them to others who have attempted the same task allows for deeper insight into how to correct ones ability or knowledge of a situation. By having the ability to compare oneself to another individuals performance adjustments can be better internalized and negative effects can be calculated and seen before they are even made. This allows for the error rate to slow down with the positive transfer rate to increase. When applying what has been learned to another similar task, one can now take what has been learned along with the comparisons of what others have accomplished and apply it more accurately. It would almost be like getting five chances to learn from one exercise when in actuality you only attempted it once.\n\nAccording to research done by Magda Osman, without external normative standards, such as making a comparison with another’s learning experiences, self-perceptions of the knowledge and control ability of self-conditions lead to negative self-assessments. Moreover, along with poor self-efficacy judgments, individuals also undervalued the relevancy of previously gained knowledge in assisting them in the transfer task. The reason for this is because individuals overcompensate in their error detection and correction, which leads them to ignore rather than transfer relevant prior information. As a consequence this has negative effects on their performance because they have failed to utilize prior relevant knowledge.\n\nRemember to consider the negative learnings that can transfer over from prior experiences when introducing something new. Address the negatives to help extinguish the error rates that might occur. Luckily, while negative transfer is a real and often problematic phenomenon of learning, it is of much less concern to education than positive transfer. Negative transfer typically causes trouble only in the early stages of learning a new domain. With experience, learners correct for the effects of negative transfer.\n\n", "id": "20494159", "title": "Negative transfer (memory)"}
{"url": "https://en.wikipedia.org/wiki?curid=465526", "text": "Excitatory postsynaptic potential\n\nIn neuroscience, an excitatory postsynaptic potential (EPSP) is a postsynaptic potential that makes the postsynaptic neuron more likely to fire an action potential. This temporary depolarization of postsynaptic membrane potential, caused by the flow of positively charged ions into the postsynaptic cell, is a result of opening ligand-gated ion channels. These are the opposite of inhibitory postsynaptic potentials (IPSPs), which usually result from the flow of \"negative\" ions into the cell or positive ions \"out\" of the cell. EPSPs can also result from a decrease in outgoing positive charges, while IPSPs are sometimes caused by an increase in positive charge outflow. The flow of ions that causes an EPSP is an excitatory postsynaptic current (EPSC).\n\nEPSPs, like IPSPs, are graded (i.e. they have an additive effect). When multiple EPSPs occur on a single patch of postsynaptic membrane, their combined effect is the sum of the individual EPSPs. Larger EPSPs result in greater membrane depolarization and thus increase the likelihood that the postsynaptic cell reaches the threshold for firing an action potential.\n\nEPSPs in living cells are caused chemically. When an active presynaptic cell releases neurotransmitters into the synapse, some of them bind to receptors on the postsynaptic cell. Many of these receptors contain an ion channel capable of passing positively charged ions either into or out of the cell (such receptors are called ionotropic receptors). At excitatory synapses, the ion channel typically allows sodium into the cell, generating an excitatory postsynaptic current. This depolarizing current causes an increase in membrane potential, the EPSP.\n\nThe neurotransmitter most often associated with EPSPs is the amino acid glutamate, and is the main excitatory neurotransmitter in the central nervous system of vertebrates. Its ubiquity at excitatory synapses has led to it being called \"the\" excitatory neurotransmitter. In some invertebrates, glutamate is the main excitatory transmitter at the neuromuscular junction. In the neuromuscular junction of vertebrates, EPP (end-plate potentials) are mediated by the neurotransmitter acetylcholine, which (along with glutamate) is one of the primary transmitters in the central nervous system of invertebrates.\nAt the same time, GABA is the most common neurotransmitter associated with IPSPs in the brain. \nHowever, classifying neurotransmitters as such is technically incorrect, as there are several other synaptic factors that help determine a neurotransmitter's excitatory or inhibitory effects.\n\nThe release of neurotransmitter vesicles from the presynaptic cell is probabilistic. In fact, even without stimulation of the presynaptic cell, a single vesicle will occasionally be released into the synapse, generating miniature EPSPs \n(mEPSPs). Bernard Katz pioneered the study of these mEPSPs at the neuromuscular junction (often called miniature end-plate potentials) in 1951, revealing the quantal nature of synaptic transmission. \"Quantal size\" can then be defined as the synaptic response to the release of neurotransmitter from a single vesicle, while \"quantal content\" is the number of effective vesicles released in response to a nerve impulse. \"Quantal analysis\" refers to the methods used to deduce, for a particular synapse, how many quanta of transmitter are released and what the average effect of each quantum is on the target cell, measured in terms of amount of ions flowing (charge) or change in the membrane potential.\n\nEPSPs are usually recorded using intracellular electrodes. The extracellular signal from a single neuron is extremely small and thus next to impossible to record in the human brain. However, in some areas of the brain, such as the hippocampus, neurons are arranged in such a way that they all receive synaptic inputs in the same area. Because these neurons are in the same orientation, the extracellular signals from synaptic excitation don't cancel out, but rather add up to give a signal that can easily be recorded with a field electrode. This extracellular signal recorded from a population of neurons is the field potential. In studies of hippocampal long-term potentiation (LTP), figures are often given showing the field EPSP (fEPSP) in stratum radiatum of CA1 in response to Schaffer collateral stimulation. This is the signal seen by an extracellular electrode placed in the layer of apical dendrites of CA1 pyramidal neurons. The Schaffer collaterals make excitatory synapses onto these dendrites, and so when they are activated, there is a current sink in stratum radiatum: the field EPSP. The voltage deflection recorded during a field EPSP is negative-going, while an intracellularly recorded EPSP is positive-going. This difference is due to the relative flow of ions (primarily the sodium ion) into the cell, which, in the case of the field EPSP is away from the electrode, while for an intracellular EPSPs it is towards the electrode. After a field EPSP, the extracellular electrode may record another change in electrical potential named the population spike which corresponds to the population of cells firing action potentials (spiking). In other regions than CA1 of the hippocampus, the field EPSP may be far more complex and harder to interpret as the source and sinks are far less defined. In regions such as the striatum, neurotransmitters such as dopamine, acetylcholine, GABA and others may also be released and further complicate the interpretation.\n\n\n", "id": "465526", "title": "Excitatory postsynaptic potential"}
{"url": "https://en.wikipedia.org/wiki?curid=22761510", "text": "Lateral intraparietal cortex\n\nThe lateral intraparietal cortex (area LIP) is found in the intraparietal sulcus of the brain. This area is most likely involved in eye movement, as electrical stimulation evokes saccades (quick movements) of the eyes. It is also thought to contribute to working memory associated with guiding eye movement, examined using a delayed saccade task described below:\n\n\nNeurons in area LIP have been shown to start responding with the initial presentation of the stimulus. The neurons keep responding through the delay period until the fixation point is removed. As the neural response stops, the saccadic eye movement starts and the animal soon focuses on the exact location of the previously shown target. There is also evidence for neurons firing for saccadic responses in the two-alternative forced choice task. The conclusion of this task experiment is that neurons in area LIP store information (the location of the target) useful for guiding the saccadic movement; that is, this area of the cortex shows modality-specific working memory.\n\nAreas showing specificity for other modalities have been located.\n", "id": "22761510", "title": "Lateral intraparietal cortex"}
{"url": "https://en.wikipedia.org/wiki?curid=17995", "text": "Long-term memory\n\nLong-term memory (LTM) is the stage of the Atkinson–Shiffrin memory model where informative knowledge is held indefinitely. It is defined in contrast to short-term and working memory, that persist for only about 18 to 30 seconds. Long-term memory is commonly labelled as explicit memory (declarative), as well as episodic memory, semantic memory, autobiographical memory, and implicit memory (procedural memory).\n\nAccording to Miller, whose paper in 1956 popularized the theory of the \"magic number seven\", short-term memory is limited to a certain number of chunks of information, while long-term memory has a limitless store.\n\nAccording to the dual store memory model proposed by Richard C. Atkinson and Richard Shiffrin in 1968, memories can reside in the short-term \"buffer\" for a limited time while they are simultaneously strengthening their associations in long-term memory. When items are first presented, they enter short-term memory, but due to its limited space, as new items enter, older ones are pushed out. However, each time an item in short-term memory is rehearsed, it is strengthened in long-term memory. Similarly, the longer an item stays in short-term memory, the stronger its association becomes in long-term memory.\n\nIn 1974 Baddeley and Hitch proposed an alternative theory of short-term memory: Baddeley's model of working memory. According to this theory, short-term memory is divided into different slave systems for different types of input items, and there is an executive control supervising what items enter and exit those systems. The slave systems include the phonological loop, the visuo-spatial sketchpad, and the episodic buffer (later added by Baddeley).\n\nLong-term memory encodes information semantically for storage, as researched by Baddeley. In vision, the information needs to enter working memory before it can be stored into long-term memory. This is evidenced by the fact that the speed with which information is stored into long-term memory is determined by the amount of information that can be fit, at each step, into visual working memory. In other words, the larger the capacity of working memory for certain stimuli, the faster will these materials be learned.\n\nSynaptic Consolidation is the process by which items are transferred from short-term to long-term memory. Within the first minutes or hours after acquisition, the engram (memory trace) is encoded within synapses, becoming resistant (though not immune) to interference from outside sources.\n\nAs long-term memory is subject to fading in the natural forgetting process, maintenance rehearsal (several recalls/retrievals of memory) may be needed to preserve long term memories. Individual retrievals can take place in increasing intervals in accordance with the principle of spaced repetition. This can happen quite naturally through reflection or deliberate recall (also known as recapitulation), often dependent on the perceived importance of the material. Using testing methods as a form of recall can lead to the testing effect, which aids long term memory through information retrieval and feedback.\n\nSome theories consider sleep to be an important factor in establishing well-organized long-term memories. \"(See also sleep and learning.)\" Sleep plays a key function in the consolidation of new memories.\n\nAccording to Tarnow's theory, long-term memories are stored in dream format (reminiscent of the Penfield & Rasmussen’s findings that electrical excitations of cortex give rise to experiences similar to dreams). During waking life an executive function interprets long-term memory consistent with reality checking . It is further proposed in the theory that the information stored in memory, no matter how it was learned, can affect performance on a particular task without the subject being aware that this memory is being used. Newly acquired declarative memory traces are believed to be reactivated during NonREM sleep to promote their hippocampo-neocortical transfer for long-term storage. Specifically new declarative memories are better remembered if recall follows Stage II non-rapid eye movement sleep. The reactivation of memories during sleep can lead to lasting synaptic changes within certain neural networks. It is the high spindle activity, low oscillation activity, and delta wave activity during NREM sleep that helps to contribute to declarative memory consolidation. In learning before sleep spindles are redistributed to neuronally active upstates within slow oscillations. Sleep spindles are thought to induce synaptic changes and thereby contribute to memory consolidation during sleep. Here, we examined the role of sleep in the object-place recognition task, a task closely comparable to tasks typically applied for testing human declarative memory: It is a one-trial task, hippocampus-dependent, not stressful and can be repeated within the same animal. Sleep deprivation reduces vigilance or arousal levels, affecting the efficiency of certain cognitive functions such as learning and memory.\n\nThe theory that sleep benefits memory retention is not a new idea. It has been around since Ebbinghaus's experiment on forgetting in 1885. More recently studies have been done by Payne and colleagues and Holtz and colleagues. In Payne and colleague's experiment participants were randomly selected and split into two groups. Both groups were given semantically related or unrelated word pairs, but one group was given the information at 9am and the other group received theirs at 9pm. Participants were then tested on the word pairs at one of three intervals 30 minutes, 12 hours, or 24 hours later. It was found that participants who had a period of sleep between the learning and testing sessions did better on the memory tests. This information is similar to other results found by previous experiments by Jenkins and Dallenbach (1924). It has also been found that many domains of declarative memory are affected by sleep such as emotional memory, semantic memory, and direct encoding.\n\nHoltz found that not only does sleep affect consolidation of declarative memories, but also procedural memories. In this experiment fifty adolescent participants were taught either word pairs (which represents declarative memory) and a finger tapping task (procedural memory) at one of two different times of day. What they found was that the procedural finger tapping task was best encoded and remembered directly before sleep, but the declarative word pairs task was better remembered and encoded if learned at 3 in the afternoon.\n\nThe brain does not store memories in one unified structure, as might be seen in a computer's hard disk drive. Instead, different types of memory are stored in different regions of the brain. Long-term memory is typically divided up into two major headings: explicit memory and implicit memory.\n\nExplicit memory (declarative memory) refers to all memories that are consciously available. These are encoded by the hippocampus, entorhinal cortex, and perirhinal cortex, but consolidated and stored elsewhere. The precise location of storage is unknown, but the temporal cortex has been proposed as a likely candidate. Research by Meulemans and Van der Linden (2003) found that amnesiac patients with damage to the medial temporal lobe performed more poorly on explicit learning tests than did healthy controls. However, these same amnesiac patients performed at the same rate as healthy controls on implicit learning tests. This implies that the medial temporal lobe is heavily involved in explicit learning, but not in implicit learning.\n\nDeclarative memory has three major subdivisions:\n\nEpisodic memory refers to memory for specific events in time, as well as supporting their formation and retrieval. Some examples of episodic memory would be remembering someone's name and what happened at your last interaction with each other. Experiments conducted by Spaniol and colleagues indicated that older adults have worse episodic memories than younger adults because episodic memory requires context dependent memory.\n\nSemantic memory refers to knowledge about factual information, such as the meaning of words. Semantic memory is independent information such as information remembered for a test. In contrast with episodic memory, older adults and younger adults do not show much of a difference in semantic memory, presumably because semantic memory does not depend on context memory.\n\nAutobiographical memory refers to knowledge about events and personal experiences from an individual's own life. Though similar to episodic memory, it differs in that it contains only those experiences which directly pertain to the individual, from across their lifespan. Conway and Pleydell-Pearce (2000) argue that this is one component of the self-memory system.\n\nImplicit memory (procedural memory) refers to the use of objects or movements of the body, such as how exactly to use a pencil, drive a car, or ride a bicycle. This type of memory is encoded and it is presumed stored by the striatum and other parts of the basal ganglia. The basal ganglia is believed to mediate procedural memory and other brain structures and is largely independent of the hippocampus. Research by Manelis, Hanson, and Hanson (2011) found that the reactivation of the parietal and occipital regions was associated with implicit memory. Procedural memory is considered non-declarative memory or unconscious memory which includes priming and non-associative learning.\nThe first part of nondeclarative memory (implicit memory) involves priming. Priming occurs when you do something faster after you have already done that activity, such as writing or using a fork.\nOther categories of memory may also be relevant to the discussion of long-term memory. For example:\n\nEmotional memory, the memory for events that evoke a particularly strong emotion, is a domain that can involve both declarative and procedural memory processes. Emotional memories are consciously available, but elicit a powerful, unconscious physiological reaction. Research indicates that the amygdala is extremely active during emotional situations, and acts with the hippocampus and prefrontal cortex in the encoding and consolidation of emotional events.\n\nWorking memory is not part of long-term memory, but is important for long-term memory to function. Working memory holds and manipulates information for a short period of time, before it is either forgotten or encoded into long-term memory. Then, in order to remember something from long-term memory, it must be brought back into working memory. If working memory is overloaded it can affect the encoding of long-term memory. If one has a good working memory they may have a better long-term memory encoding.\n\nMinor everyday slips and lapses of memory are fairly commonplace, and may increase naturally with age, when ill, or when under stress. Some women may experience more memory lapses following the onset of the menopause.\nIn general, more serious problems with memory occur due to traumatic brain injury or neurodegenerative disease.\n\nThe majority of findings about memory have been the result of studies that lesioned specific brain regions in rats or primates, but some of the most important work has been the result of accidental or inadvertent brain trauma. The most famous case in recent memory studies is the case study of HM, who had parts of his hippocampus, parahippocampal cortices, and surrounding tissue removed in an attempt to cure his epilepsy. His subsequent total anterograde amnesia and partial retrograde amnesia provided the first evidence for the localization of memory function, and further clarified the differences between declarative and procedural memory.\n\nMany neurodegenerative diseases can cause memory loss. Some of the most prevalent (and, as a consequence, most intensely researched) include Alzheimer's disease, dementia, Huntington's disease, multiple sclerosis, Parkinson's disease, and schizophrenia. None act specifically on memory; instead, memory loss is often a casualty of generalized neuronal deterioration. Currently, these illnesses are irreversible, but research into stem cells, psychopharmacology, and genetic engineering holds much promise.\n\nThose with Alzheimer's disease generally display symptoms such as getting momentarily lost on familiar routes, placing possessions in inappropriate locations and distortions of existing memories or completely forgetting memories. Researchers have often used the Deese–Roediger–McDermott paradigm (DRM) to study the effects of Alzheimer's disease on memory. The DRM paradigm presents a list of words such as doze, pillow, bed, dream, nap, etc., with a theme word that is not presented. In this case the theme word would have been sleep. Alzheimer's disease patients are more likely to recall the theme word as being part of the original list than healthy adults. There is a possible link between longer encoding time and increased false memory in LTM. The patients end up relying on the gist of information instead of the specific words themselves. Alzheimer's leads to an uncontrolled inflammatory response brought on by extensive amyloid depostion in the brain, which leads to cell death in the brain. This gets worse over time and eventually leads to cognitive decline, after the loss of memory. Pioglitazone may improve cognitive impairments, including memory loss and may help protect long-term and visiospatial memory from neurodegenerative disease.\n\nParkinson's disease patients have problems with cognitive performance; these issues resemble what is seen in frontal lobe patients and can often lead to dementia. It is thought that Parkinson's disease is caused by degradation of the dopaminergic mesocorticolimbic projection originating from the ventral tegmental area. It has also been indicated that the hippocampus plays an important role in episodic and spatial (parts of LTM) memory and Parkinson's disease patients have abnormal hippocampuses resulting in abnormal functioning of LTM. L-dopa injections are often used to try to relieve Parkinson's disease symptoms as well as behavioral therapy.\n\nSchizophrenia patients have trouble with attention and executive functions which in turn affects long-term memory consolidation and retrieval. They cannot encode or retrieve temporal information properly, which causes them to select inappropriate social behaviors. They cannot effectively use the information they possess. The prefrontal cortex, where schizophrenia patients have structural abnormalities, is involved with the temporal lobe and also affects the hippocampus, which causes their difficulty in encoding and retrieving temporal information (including long-term memory).\n\nLong-term memory, unlike short-term memory, is dependent upon the synthesis of new proteins. This occurs within the cellular body, and concerns the particular transmitters, receptors, and new synapse pathways that reinforce the communicative strength between neurons. The production of new proteins devoted to synapse reinforcement is triggered after the release of certain signaling substances (such as calcium within hippocampal neurons) in the cell. In the case of hippocampal cells, this release is dependent upon the expulsion of magnesium (a binding molecule) that is expelled after significant and repetitive synaptic signaling. The temporary expulsion of magnesium frees NMDA receptors to release calcium in the cell, a signal that leads to gene transcription and the construction of reinforcing proteins. For more information, see long-term potentiation (LTP).\n\nOne of the newly synthesized proteins in LTP is also critical for maintaining long-term memory. This protein is an autonomously active form of the enzyme protein kinase C (PKC), known as PKMζ. PKMζ maintains the activity-dependent enhancement of synaptic strength and inhibiting PKMζ erases established long-term memories, without affecting short-term memory or, once the inhibitor is eliminated, the ability to encode and store new long-term memories is restored.\n\nAlso, BDNF is important for the persistence of long-term memories.\n\nThe long-term stabilization of synaptic changes is also determined by a parallel increase of pre- and postsynaptic structures such as axonal bouton, dendritic spine and postsynaptic density.\nOn the molecular level, an increase of the postsynaptic scaffolding proteins PSD-95 and Homer1c has been shown to correlate with the stabilization of synaptic enlargement.\n\nThe cAMP response element-binding protein (CREB) is a transcription factor which is believed to be important in consolidating short-term to long-term memories, and which is believed to be downregulated in Alzheimer's disease.\n\nA couple of studies have had results that contradict the dual-store memory model. Studies showed that in spite of using distractors, there was still both a recency effect for a list of items and a contiguity effect.\n\nAnother study revealed that how long an item spends in short-term memory is not the key determinant in its strength in long-term memory. Instead, whether the participant actively tries to remember the item while elaborating on its meaning determines the strength of its store in long-term memory.\n\nAn alternative theory is that there is only one memory store with associations among items and their contexts. In this model, the context serves as a cue for retrieval, and the recency effect is greatly caused by the factor of context. Immediate and delayed free-recall will have the same recency effect because the relative similarity of the contexts still exist. Also, the contiguity effect still occurs because contiguity also exists between similar contexts.\n\n\n\n", "id": "17995", "title": "Long-term memory"}
{"url": "https://en.wikipedia.org/wiki?curid=465517", "text": "Inhibitory postsynaptic potential\n\nAn inhibitory postsynaptic potential (IPSP) is a kind of synaptic potential that makes a postsynaptic neuron less likely to generate an action potential. The opposite of an inhibitory postsynaptic potential is an excitatory postsynaptic potential (EPSP), which is a synaptic potential that makes a postsynaptic neuron \"more\" likely to generate an action potential. IPSPs can take place at all chemical synapses, which use the secretion of neurotransmitters to create cell to cell signalling. Inhibitory presynaptic neurons release neurotransmitters that then bind to the postsynaptic receptors; this induces a change in the permeability of the postsynaptic neuronal membrane to particular ions. An electric current that changes the postsynaptic membrane potential to create a more negative postsynaptic potential is generated, i.e. the postsynaptic membrane potential becomes more negative than the resting membrane potential, and this is called hyperpolarisation. In order for an action potential to be generated, depolarisation of the postsynaptic membrane needs to occur, where the membrane potential becomes more positive than resting membrane potential. Therefore, hyperpolarisation of the postsynaptic membrane makes it less likely for depolarisation to sufficiently occur to generate an action potential in the postsynaptic neurone. \n\nDepolarization can also occur due to an IPSP if the reverse potential is between the resting threshold and the action potential threshold. Another way to look at inhibitory postsynaptic potentials is that they are also a chloride conductance change in the neuronal cell because it decreases the driving force. This is because, if the neurotransmitter released into the synaptic cleft causes an increase in the permeability of the postsynaptic membrane to chloride ions by binding to ligand-gated chloride ion channels and causing them to open, then chloride ions, which are in greater concentration in the synaptic cleft, will diffuse into the postsynaptic neurone. As these are negatively charged ions, hyperpolarisation results, making it less likely for an action potential to be generated in the postsynaptic neurone. Microelectrodes can be used to measure postsynaptic potentials at either excitatory or inhibitory synapses.\n\nIn general, a postsynaptic potential is dependent on the type and combination of receptor channel, reverse potential of the postsynaptic potential, action potential threshold voltage, ionic permeability of the ion channel, as well as the concentrations of the ions in and out of the cell; this determines if it is excitatory or inhibitory. IPSPs always want to keep the membrane potential more negative than the action potential threshold and can be seen as a \"transient hyperpolarization\". EPSPs and IPSPs compete with each other at numerous synapses of a neuron; this determines whether or not the action potential at the presynaptic terminal will regenerate at the postsynaptic membrane. Some common neurotransmitters involved in IPSPs are GABA and glycine.\n\nThis system IPSPs can be temporally summed with subthreshold or suprathreshold EPSPs to reduce the amplitude of the resultant postsynaptic potential. Equivalent EPSPs (positive) and IPSPs (negative) can cancel each other out when summed. The balance between EPSPs and IPSPs is very important in the integration of electrical information produced by inhibitory and excitatory synapses.\n\nThe size of the neuron can also affect the inhibitory postsynaptic potential. Simple temporal summation of postsynaptic potentials occurs in smaller neurons, whereas in larger neurons larger numbers of synapses and ionotropic receptors as well as a longer distance from the synapse to the soma enables the prolongation of interactions between neurons.\n\nGABA is a very common neurotransmitter used in IPSPs in the adult mammalian brain and retina. GABA receptors are pentamers most commonly composed of three different subunits (α, β, γ), although several other subunits (δ,ε, θ, π, ρ) and conformations exist. The open channels are selectively permeable to chloride or potassium ions (depending on the type of receptor) and allow these ions to pass through the membrane. If the electrochemical potential of the ion is more negative than that of the action potential threshold then the resultant conductance change that occurs due to the binding of GABA to its receptors keeps the postsynaptic potential more negative than the threshold and decreases the probability of the postsynaptic neuron completing an action potential. Glycine molecules and receptors work much in the same way in the spinal cord, brain, and retina.\n\nThere are two types of inhibitory receptors:\n\nIonotropic receptors (also known as ligand-gated ion channels) play an important role in inhibitory postsynaptic potentials. A neurotransmitter binds to the extracellular site and opens the ion channel that is made up of a membrane-spanning domain that allows ions to flow across the membrane inside the postsynaptic cell. This type of receptor produces very fast postsynaptic actions within a couple of milliseconds of the presynaptic terminal receiving an action potential. These channels influence the amplitude and time-course of postsynaptic potentials as a whole. Ionotropic GABA receptors are used in binding for various drugs such as barbiturates (Phenobarbital, pentobarbital), steroids, and picrotoxin. Benzodiazepines (Valium) bind to the α and δ subunits of GABA receptors in order to improve GABAergic signaling. Alcohol also modulates ionotropic GABA receptors.\n\nMetabotropic receptors, or G-protein-coupled receptors, do not use ion channels in their structure; they, instead, consist of an extracellular domain that binds to a neurotransmitter and an intracellular domain that binds to G-protein. This begins the activation of the G-protein, which then releases itself from the receptor and interacts with ion channels and other proteins to open or close ion channels through intracellular messengers. They produce slow postsynaptic responses (from milliseconds to minutes) and can be activated in conjunction with ionotropic receptors to create both fast and slow postsynaptic potentials at one particular synapse. Metabotropic GABA receptors, heterodimers of R1 and R2 subunits, use potassium channels instead of chloride. They can also block calcium ion channels in order to hyperpolarize postsynaptic cells.\n\nThere are many applications of inhibitory postsynaptic potentials to the real world. Drugs that affect the actions of the neurotransmitter can treat neurological and psychological disorders through different combinations of types of receptors, G-proteins, and ion channels in postsynaptic neurons.\n\nFor example, studies researching opioid receptor-mediated receptor desensitizing and trafficking in the locus cereleus of the brain are being performed. When a high concentration of agonist is applied for an extended amount of time (fifteen minutes or more), hyperpolarization peaks and then decreases. This is significant because it is a prelude to tolerance; the more opioids one needs for pain the greater the tolerance of the patient. These studies are important because it helps us to learn more about how we deal with pain and our responses to various substances that help treat pain. By studying our tolerance to pain, we can develop more efficient medications for pain treatment.\n\nIn addition, research is being performed in the field of dopamine neurons in the ventral tegmental area, which deals with reward, and the substantia nigra, which is involved with movement and motivation. Metabotropic responses occur in dopamine neurons through the regulation of the excitability of cells. Opioids inhibit GABA release; this decreases the amount of inhibition and allows them to fire spontaneously. Morphine and opioids relate to inhibitory postsynaptic potentials because they induce disinhibition in dopamine neurons.\n\nIPSPs can also be used to study the input-output characteristics of an inhibitory forebrain synapse used to further study learned behavior, to be specific song learning in birds in a study performed at the University of Washington. Poisson trains of unitary IPSPs were induced at a high frequency to reproduce postsynaptic spiking in the medial portion of the dorsalateral thalamic nucleus without any extra excitatory inputs. This shows an excess of thalamic GABAergic activation. This is important because spiking timing is needed for proper sound localization in the ascending auditory pathways. Songbirds use GABAergic calyceal synaptic terminals and a calcyx-like synapse such that each cell in the dorsalateral thalamic nucleus receives at most two axon terminals from the basal ganglia to create large postsynaptic currents.\n\nInhibitory postsynaptic potentials are also used to study the basal ganglia of amphibians to see how motor function is modulated through its inhibitory outputs from the striatum to the tectum and tegmentum. Visually guided behaviors may be regulated through the inhibitory striato-tegmental pathway found in amphibians in a study performed at the Baylor College of Medicine and the Chinese Academy of Sciences. The basal ganglia in amphibians is very important in receiving visual, auditory, olfactory, and mechansensory inputs; the disinhibitory striato-protecto-tectal pathway is important in prey-catching behaviors of amphibians. When the ipsilateral striatum of an adult toad was electrically stimulated, inhibitory postsynaptic potentials were induced in binocular tegmental neurons, which affects the visual system of the toad.\n\nInhibitory postsynaptic potentials can be inhibited themselves through a signaling process called \"depolarized-induced suppression of inhibition (DSI)\" in CA1 pyramidal cells and cerebellar Purkinje cells. In a laboratory setting step depolarizations the soma have been used to create DSIs, but it can also be achieved through synaptically induced depolarization of the dendrites. DSIs can be blocked by ionotropic receptor calcium ion channel antagonists on the somata and proximal apical dendrites of CA1 pyramidal cells. Dendritic inhibitory postsynaptic potentials can be severely reduced by DSIs through direct depolarization.\n\nAlong these lines, inhibitory postsynaptic potentials are useful in the signaling of the olfactory bulb to the olfactory cortex. EPSPs are amplified by persistent sodium ion conductance in external tufted cells. Low-voltage activated calcium ion conductance enhances even larger EPSPs. The hyperpolarization activated nonselective cation conductance decreases EPSP summation and duration and they also change inhibitory inputs into postsynaptic excitation. IPSPs come into the picture when the tufted cells membranes are depolarized and IPSPs then cause inhibition. At resting threshold IPSPs induce action potentials. GABA is responsible for much of the work of the IPSPs in the external tufted cells.\n\nAnother interesting study of inhibitory postsynaptic potentials looks at neuronal theta rhythm oscillations that can be used to represent electrophysiological phenomena and various behaviors. Theta rhythms are found in the hippocampus and GABAergic synaptic inhibition helps to modulate them. They are dependent on IPSPs and started in either CA3 by muscarinic acetylcholine receptors and within C1 by the activation of group I metabotropic glutamate receptors. When interneurons are activated by metabotropic acetylcholine receptors in the CA1 region of rat hippocampal slices, a theta pattern of IPSPs in pyramidal cells occurs independent of the input. This research also studies DSIs, showing that DSIs interrupt metabotropic acetylcholine-initiated rhythm through the release of endocannabinoids. An endocannabinoid-dependent mechanism can disrupt theta IPSPs through action potentials delivered as a burst pattern or brief train. In addition, the activation of metabotropic glutamate receptors removes any theta IPSP activity through a G-protein, calcium ion–independent pathway.\n\nInhibitory postsynaptic potentials have also been studied in the Purkinje cell through dendritic amplification. The study focused in on the propagation of IPSPs along dendrites and its dependency of ionotropic receptors by measuring the amplitude and time-course of the inhibitory postsynaptic potential. The results showed that both compound and unitary inhibitory postsynaptic potentials are amplified by dendritic calcium ion channels. The width of a somatic IPSP is independent of the distance between the soma and the synapse whereas the rise time increases with this distance. These IPSPs also regulate theta rhythms in pyramidal cells.\nOn the other hand, inhibitory postsynaptic potentials are depolarizing and sometimes excitatory in immature mammalian spinal neurons because of high concentrations of intracellular chloride through ionotropic GABA or glycine chloride ion channels. These depolarizations activate voltage-dependent calcium channels. They later become hyperpolarizing as the mammal matures. To be specific, in rats, this maturation occurs during the perinatal period when brain stem projects reach the lumbar enlargement. Descending modulatory inputs are necessary for the developmental shift from depolarizing to hyperpolarizing inhibitory postsynaptic potentials. This was studied through complete spinal cord transections at birth of rats and recording IPSPs from lumbar motoneurons at the end of the first week after birth.\n\nGlutamate, an excitatory neurotransmitter, is usually associated with excitatory postsynaptic potentials in synaptic transmission. However, a study completed at the Vollum Institute at the Oregon Health Sciences University demonstrates that glutamate can also be used to induce inhibitory postsynaptic potentials in neurons. This study explains that metabotropic glutamate receptors feature activated G proteins in dopamine neurons that induce phosphoinositide hydrolysis. The resultant products bind to inositol triphosphate (IP3) receptors through calcium ion channels. The calcium comes from stores and activate potassium conductance, which causes a pure inhibition in the dopamine cells. The changing levels of synaptically released glutamate creates an excitation through the activation of ionotropic receptors, followed by the inhibition of metabotropic glutamate receptors.\n\n", "id": "465517", "title": "Inhibitory postsynaptic potential"}
{"url": "https://en.wikipedia.org/wiki?curid=21312310", "text": "Remember versus know judgements\n\nThere is evidence suggesting that different processes are involved in remembering something versus knowing whether it is familiar. It appears that \"remembering\" and \"knowing\" represent relatively different characteristics of memory as well as reflect different ways of using memory.\n\nTo remember is the conscious \"recollection\" of many vivid contextual details, such as \"when\" and \"how\" the information was learned. Remembering utilizes episodic memory and requires a deeper level of processing (e.g. undivided attention) than knowing. Errors in recollection may be due to source-monitoring errors that prevent an individual from remembering where exactly a piece of information was received. On the other hand, source monitoring may be very effective in aiding the retrieval of episodic memories. Remembering is a knowledge-based and conceptually-driven form of processing that can be influenced by many things. It is relevant to note that under this view both kinds of judgments are characteristics of individuals and thus any distinctions between the two are correlational, not causal, events.\n\nTo know is a feeling (unconscious) of \"familiarity\". It is the sensation that the item has been seen before, but not being able to pin down the reason why. Knowing simply reflects the familiarity of an item without recollection. Knowing utilizes semantic memory that requires perceptually based, data-driven processing. Knowing is the result of shallow maintenance rehearsal that can be influenced by many of the same aspects as semantic memory.\n\nRemember and know responses are quite often differentiated by their functional correlates in specific areas in the brain. For instance, during \"remember\" situations it is found that there is greater EEG activity than \"knowing\", specifically, due to an interaction between frontal and posterior regions of the brain. It is also found that the hippocampus is differently activated during recall of \"remembered\" (vs. familiar) stimuli. On the other hand, items that are only \"known\", or seem familiar, are associated with activity in the rhinal cortex.\n\nThe remember-know paradigm began its journey in 1985 from the mind of Endel Tulving. He suggested that there are only two ways in which an individual can access their past. For instance, we can recall what we did last night by simply traveling back in time through memory and episodically imagining what we did (remember) or we can know something about our past such as a phone number, but have no specific memory of where the specific memory came from (know). Recollection is based on the episodic memory system, and familiarity is based on the semantic memory system. Tulving argued that the remember-know paradigm could be applied to all aspects of recollection.\n\nIn 1988 the application of the paradigm was refined to a set of instructions that could elicit reliable judgments from subjects that can be found using many variables. The remember-know paradigm has changed the way in which researchers can study memory tasks and has had implications on what were originally considered purely \"episodic\" memories, which can now be thought of as a combination of both remembering and knowing or episodic and semantic.\n\nRemembering and knowing have been linked to dual-component theories, as well as unitary trace-strength/signal detection models.\n\nEpisodic and semantic memory give rise to two different states of consciousness, autonoetic and noetic, which influence two kinds of subjective experience, remembering and knowing, respectively. Autonoetic consciousness refers to the ability of recovering the episode in which an item originally occurred. In noetic consciousness, an item is familiar but the episode in which it was first encountered is absent and cannot be recollected. Remembering involves retrieval from episodic memory and knowing involves retrieval from semantic memory.\n\nIn his SPI model, Tulving stated that encoding into episodic and semantic memory is serial, storage is parallel, and retrieval is independent. By this model, events are first encoded in semantic memory before being encoded in episodic memory; thus, both systems may have an influence on the recognition of the event.\n\nThe original high-threshold model held that recognition is a probabilistic process. It is assumed that there is some probability that previously studied items will exceed a memory threshold. If an item exceeds the threshold then it is in a discrete memory state. If an item does not exceed the threshold then it is not remembered, but it may still be endorsed as old on the basis of a random guess. According to this model, a test item is either recognized (i.e., it falls above a threshold) or it is not (i.e., it falls below a threshold), with no degrees of recognition occurring between these extremes. Only target items can generate an above-threshold recognition response because only they appeared on the list. The lures, along with any targets that are forgotten, fall below threshold, which means that they generate no memory signal whatsoever. For these items, the participant has the option of declaring them to be new (as a conservative participant might do) or guessing that some of them are old (as a more liberal participant might do). False alarms in this model reflect memory-free guesses that are made to some of the lures. \nThis simple and intuitively appealing model yields the once widely used correction for guessing formula, and it predicts a linear receiver operating characteristic (ROC). An ROC is simply a plot of the hit rate versus the false alarm rate for different levels of bias. A typical ROC is obtained by asking participants to supply confidence ratings for their recognition memory decisions. Several pairs of hit and false alarm rates can then be computed by accumulating ratings from different points on the confidence scale (beginning with the most confident responses). The high-threshold model of recognition memory predicts that a plot of the hit rate versus the false alarm rate (i.e., the ROC) will be linear it also predicts that the z-ROC will be curvilinear. \n\nThe dual-process account states that recognition decisions are based on the processes of recollection and familiarity. Recollection is a conscious, effortful process in which specific details of the context in which an item was encountered are retrieved. Familiarity is a relatively fast, automatic process in which one gets the feeling the item has been encountered before, but the context in which it was encountered is not retrieved. According to this view, remember responses reflect recollections of past experiences and know responses are associated with recognition on the basis of familiarity.\n\nAccording to this theory, recognition decisions are based on the strength of a memory trace in reference to a certain decision threshold. A memory that exceeds this threshold is perceived as old, and trace that does not exceed the threshold is perceived as new. According to this theory, remember and know responses are products of different degrees of memory strength. There are two criteria on a decision axis; a point low on the axis is associated with a know decision, and a point high on the axis is associated with a remember decision. If memory strength is high, individuals make a \"remember\" response, and if memory strength is low, individuals make a \"know\" response.\n\nProbably the strongest support for the use of signal detection theory in recognition memory came from the analysis of ROCs. An ROC is the function that relates the proportion of correct recognitions (hit rate) to the proportion of incorrect recognitions (false-alarm rate). \n\nSignal-detection theory assumed a preeminent position in the field of recognition memory in large part because its predictions about the shape of the ROC were almost always shown to be more accurate than the predictions of the intuitively plausible high-threshold model. More specifically, the signal-detection model, which assumes that memory strength is a graded phenomenon (not a discrete, probabilistic phenomenon) predicts that the ROC will be curvilinear, and because every recognition memory ROC analyzed between 1958 and 1997 was curvilinear, the high-threshold model was abandoned in favor of signal-detection theory. Although signal-detection theory predicts a curvilinear ROC when the hit rate is plotted against the false alarm rate, it predicts a linear ROC when the hit and false alarm rates are converted to z scores (yielding a z-ROC). \n\n“The predictive power of the signal detection modem seems to rely on know responses being related to transient feelings of familiarity without conscious recollection, rather than Tulving’s (1985) original definition of know awareness.\n\nThe dual-process signal-detection/high-threshold theory tries to reconcile dual-process theory and signal-detection theory into one main theory. This theory states that recollection is governed by a threshold process, while familiarity is not. Recollection is a high-threshold process (i.e., recollection either occurs or does not occur), whereas familiarity is a continuous variable that is governed by an equal-variance detection model. On a recognition test, item recognition is based on recollection if the target item has exceeded threshold, producing an \"old\" response. If the target item does not reach threshold, the individual must make an item recognition decision based on familiarity. According to this theory, an individual makes a \"remember\" response when recollection has occurred. A know response is made when recollection has not occurred, and the individual must decide whether they recognize the target item solely on familiarity.\nThus, in this model, the participant is thought to resort to familiarity as a backup process whenever recollection fails to occur.\n\nIn the past, it was suggested that remembering is associated with conceptual processing and knowing is associated with perceptual processing. However, recent studies have reported that there are some conceptual factors that influence knowing and some perceptual factors that influence remembering. Findings suggest that regardless of perceptual or conceptual factors, distinctiveness of processing at encoding is what affects remembering, and fluency of processing is what affects knowing. Remembering is associated with distinctiveness because it is seen as an effortful, consciously controlled process. Knowing, on the other hand, depends on fluency as it is more automatic and reflexic and requires much less effort.\n\nItems of low-frequency are generally better recognized and receive more remember responses than high-frequency items. In a study, 96 words consisting of 48 low-frequency words and 48 high-frequency words were chosen by a psycholinguistic database. There were two alternate study lists, each consisting of 24 low-frequency words and 24 high-frequency words. Half of the subjects received one study list, while the other half of the participants received the other. The recognition test, which involved all 96 words, required participants to first acknowledge whether the target item was old or new; if the item was considered old, participants were further asked to distinguish whether the item was remembered (they could recollect the context in which it had been studied) or known (the item seemed familiar but they couldn't recollect contextual details). The results of this experiment were that low-frequency words received many more remember responses than high-frequency words. Since remember words are affected by distinctiveness, this makes sense; low-frequency words are experienced less than high-frequency words which makes them more distinctive. Also, there seemed to be no significant difference in the number of know responses made for low-frequency words and high-frequency words.\n\nItems which are generated by a person receive more remember responses than items which are read, seen, or heard by a person. In addition, the generation of images to words enhances remember responses. In one study, all participants were asked to study a list of 24 common pairs of opposites, 12 had to be generated and 12 were read. The generated pairs required participants to generate them in the context of a given rule. The recognition test consisted of 48 words, 24 targets and 24 distractors. Participants were asked if items were old or new; if participants replied \"old\", they were then asked whether they remembered (could recollect contextual details of when it was studied) the pair or if they knew (recognized it but recollection was absent) the pair. Generation effects were seen in remember responses; items which were generated received more remember responses than read items. However, generation effects were not seen in know responses.\n\nRemember responses depend on the amount of attention available during learning. Divided attention at learning has a negative impact on remember responses. A study was done which consisted of 72 target words which were divided into two study lists. Half of the participants were required to study the list in an undivided attention condition and half of the subjects studied the list in a divided attention condition. In the divided attention condition, subjects had to study the list while listening to and reporting high, low, or medium tone sequences. The recognition test consisted of participants deciding whether items were old or new; if items were deemed old, participants were then required to say whether items were remembered or known. It was found that the divided attention condition impaired the level of correct remember responses; however, the know responses seemed unaffected.\n\nWhen more detailed, elaborate encoding and associations are made, more remember responses are reported than know responses. The opposite occurs with shallow, surface encoding which results in fewer remember responses.\n\nThe primacy effect is related to enhanced remembering. In a study, a free recall test was conducted on some lists of words and no test on other lists of words prior to a recognition test. They found that testing led to positive recency effects for remembered items; on the other hand, with no prior test, negative recency effects occurred for remembered items. Thus, both primary and recency effects can be seen in remember responses.\n\nMasked recognition priming is a manipulation which is known to increase perceptual fluency. Since know responses increase with increased fluency of processing, masked recognition priming enhances know responses.\n\nBriefly presenting a prime prior to test causes an increase in fluency of processing and an associated increase in feeling of familiarity. Short duration primes tend to enhance know responses. In contrast to briefly presented primes, primes which are presented for long durations are said to disrupt processing fluency as the prime saturates the representation of the target word. Thus, longer duration primes tend to have a negative impact on know responses.\n\nKnow responses are enhanced when stimuli modality match during study and test; therefore, shifting the modality of a stimulus has been found to negatively impact know responses.\n\nKnowing is influenced by the fluency in which an item is processed, regardless of whether information is conceptual or perceptual. Know responses are enhanced by manipulations which increase perceptual and conceptual fluency. For example, masked repetition priming, modality match during study and test, and the use of easy word-fragments in word-fragment recall are all perceptual manipulations which increase know responses. An example of a conceptual manipulation which enhances know responses is when a prime item is semantically related to a target item. Manipulations which increase processing fluency do not seem to affect remember responses.\n\nNormal aging tends to disrupt remember responses to a greater extent than know responses. This decrease in remember responses is associated with poor encoding and frontal lobe dysfunction. It has been found that older individuals fail to use elaborative encoding in comparison to younger individuals. In addition to poor encoding, older individuals tend to have problems with retrieving information that is highly specific because they are less effective at controlling their retrieval processes. It is difficult for older individuals to constrain retrieval processes to the context of the specific item that is to be retrieved.\n\nWhen words are used as stimuli, more remember responses and fewer know responses are produced in comparison to when nonwords are used as stimuli.\n\nGradual presentation of stimuli causes an increase in familiarity and thus an increase in associated know responses; however, gradual presentation causes a decrease in remember responses.\n\nThe amygdala plays an important role during encoding and retrieval of emotional information. It has been found that although negative and positive items are remembered or known to the same extent, the processes involved in remembering and knowing differs with emotional valence.\n\nActivity in the orbitofrontal and ventrolateral prefrontal cortex are associated with remembering for both positive and negative items. When it comes to remembering, it has been suggested that negative items may be remembered with more detail in comparison to positive or neutral items; support for this has been found in the temporo-occipital regions, which showed activity specific to negative items that were \"remembered\". A study found that in addition to being remembered in more detail, negative items also tended to be remembered for longer durations than positive items.\n\nActivity in the mid-cingulate gyrus, the inferior parietal lobe, and the superior frontal lobe are all associated with knowing for both positive and negative items. These regions are said to be involved in the retrieval of both semantic and episodic information. It has been suggested that the encoding of items which people forget details for or items which are forgotten as a whole are associated with these regions. This forgetting has to do with retrieval-related processes being active at the same time as encoding-related processes. Thus, the process of retrieval may come at the expense of encoding vivid details of the item.\n\nIn addition, disproportionate activity along the cingulate gyrus, within the parietal lobe, and in the prefrontal cortex is associated with the encoding of \"known\" positive items. This increased activity may cause the trade-off between retrieval-related processes and encoding-related processes to occur more significantly for positive items. This supports the idea that when people are in a positive mood, they have a more holistic, general thought process and disregard details.\n\nThe functional account of remembering states that remember responses are determined by the context in which they're made; in general, recollection is based on the type of info that was encouraged by the deeper level processing task. Remember responses occur when retrieved information allow subjects to finish a memory test. The same item may elicit a remember response or a know response, depending on the context in which it is found.\n\nIn the expectancy heuristic, items that reach beyond a level of distinctiveness (the likelihood an item would later be recognized in a recognition test) elicit a remember response. Items that do not reach this level of distinctiveness elicit a know response. The level of distinctiveness is determined by the context in which items are studied and/or tested. In a given context, there is an expected level of distinctiveness; in contexts where subjects expect many distinct items, participants give fewer remember responses than when they expect few distinct items. Therefore, remember responses are affected by the expected strength of distinctiveness of items in a given context.\n\nIn addition, context can affect remember and know responses while not affecting recollection and familiarity processes. Remember and know responses are subjective decisions that can be affected by underlying memory processes. While changing recollection and familiarity processes can influence remember and know judgments, context can affect how items are weighted for remember-know decisions.\n\nAccording to the distinctiveness-fluency model, items are seen as distinct when they exceed a level of memorability and items are seen as fluent when they do not reach this level but give a feeling of familiarity. Distinct items are usually unusual in comparison to the context in which they're found. Therefore, distinct items generally elicit a remember response, and fluent items elicit a know response.\n\nIn this study, the presence of source memory was utilized to estimate the extent to which episodic details were recalled; feelings of familiarity were accompanied by retrieval of partial contextual details, which are considered sufficient for an accurate source decision but not for a recollection response. Subjects that remembered the stimuli were able to differentiate the corresponding source correctly. The findings were consistent with the idea that \"remember\" responses, unlike \"know\" responses, are accompanied by memory for episodic detail, and that the loss of memory for episodic detail over time parallels the conversion of \"remember\" responses to \"know\" responses.\n\nIn the preceding task, participants are given a list of items to study in the primary learning phase. Subsequently, during the recognition stage, participants are asked to make a decision about whether presented test items existed in the previously studied list. If the participant responds \"yes\", they are asked why they recognized the specified item. From this, a conclusion was obtained based on whether the item was remembered or simply known.\n\nPrimarily, experimenters recorded eye movements while participants studied a series of photos. Individuals were then involved in a recognition task in which their eye movements were recorded for the second time. From the previous tasks, it was discovered that eye fixations, maintaining a visual gaze on a single location, were more clustered for remembering rather than knowing tasks. This suggests that remembering is associated with encoding a specific salient component of an item whereas recognition is activated by an augmented memory for this part of the stimulus.\n\nIn the above experiment, participants were presented with a list of 100 familiar words and were asked to read them aloud while simultaneously trying to remember each one. Subsequent to this, participants were asked to make a recognition decision based on the number of \"yes\" responses that were accompanied by some recollective experience. The results demonstrate the differing relationships between the \"yes\" and \"no\" conditions and \"remember\" and \"know\" memory performance. The outcome confirms that although familiarity and recollection may involve different processes, the remember/know exemplar does not probe them directly.\n\nIn the previous study, two different remember-know paradigms are explored. The first is the \"remember-first method\" in which a remember response is solicited prior to a know response for non-remembered items. Secondly, a trinary paradigm, in which a single response judges the \"remember vs. know\" and \"new\" alternatives is investigated. Participants are asked to subjectively decide whether their response within these studies is attributed to a recollection of specific details, \"remembering\", or familiarity \"knowing\". In the presently discussed experiment, \"remember\" and \"know\" responses generally depend on a single strength variable.\n\nRemembering (recollection) accesses memory for separate contextual details (e.g. screen location and font size); i.e. involves retrieval of a particular context configuration.\n\nThis model uses the idea that remembering and knowing each represent different levels of confidence. In this sense remember/know judgments are viewed as quantitatively different judgments that vary along the same continuum. Subjects place their \"know\" and \"remember\" judgments on a continuum of strength. When people are very confident about recognizing an item, they assign it a \"remember\" response, and when they are less confident about a response it is labelled as a \"know\" response. A potential problem with this model is that it lacks explanatory power; it may be difficult to determine where the criteria should be placed on the continuum.\n\nThe remember-know paradigm has had great usage in clinical studies. Using this paradigm, researchers can look into the mechanisms of neuro-biological functions as well as social aspects of disorders and illnesses that plague humans. Recognition memory has been linked to advancements in the understanding of schizophrenia, epilepsy and even explaining simple autobiographical memory loss as we grow older.\n\nThe remember-know paradigm has been used to settle the debate over whether the hippocampus plays a critical role in familiarity of objects. Studies done with patients suffering from epilepsy suggest that the hippocampus plays a critical role in the familiarity of objects. A study was conducted using the remember-know distinction to understand this idea of familiarity and whether it is in fact the hippocampus that plays this critical role. This study found that the hippocampus is essentially a system based on familiarity. The hippocampus actually suppresses any sort of arousal response that would normally occur if the stimuli were novel. It is almost as though familiarity is a qualitative characteristic just as is colour or loudness.\n\nThe remember-know paradigm with epilepsy patients to distinguish whether a stimulus (picture of a face) was familiar. Patients that were found to have right temporal lobe epilepsy showed relatively lower face recognition response than those with left temporal lobe epilepsy due to damage of secondary sensory regions (including fusiform gyrus) in the brain's right hemisphere, which is responsible for perception and encoding (esp. face memory).\n\nA remember-know paradigm was used to test whether patients with schizophrenia would exhibit abnormalities in conscious recollection due to a deterioration of frontal memory processes that are involved in encoding/retrieval of memories as well as executive functions linked to reality monitoring and decision making. Using the \"remember-know\" paradigm, participants first identify stimuli that they previously studied. If an item is identified as a known stimulus, the participants are asked to distinguish whether they can remember aspects of the original presentation of the identified item (remember response) or whether they know that the item was on the study list, but have no episodic memory of specifically learning it. \n\nResults showed that patients with schizophrenia exhibit a deficit in conscious recollection for auditory stimuli. These findings, when considered together with remember/know data collected from the same set of patients for olfactory and visual recognition memory, support proposals that the abnormalities in conscious recollection stem from a breakdown in central processes rather than domain-specific processes. This study depended greatly on the remember-know paradigm to test for conscious recollection differences in these patients.\n\nThe remember-know paradigm has been used in studies that focus on the idea of a reminiscence bump and the age effects on autobiographical memory. Previous studies suggested old people had more \"know\" than \"remember\" and it was also found that younger individuals often excelled in the \"remember\" category but lacked in the \"know\". \n\nA specific study used the remember-know paradigm while asking elderly people about their episodic memories from their university graduation. They were asked to determine whether their self reported memories were \"remembered\" or \"known\". It was hypothesized that reminiscence component of elderly adults' autobiographical recall would be strong for \"remember\" responses, but less so for \"know\" responses. It was also expected that recent memories would hold the opposite effect, that those individuals would be better at \"know\" responses than with \"remember\" responses. \n\nResults showed that there was good retention after reminiscence bump and equal \"remember\" to \"know\" responses were reported. It was concluded that autobiographical memories were tied to both episodic and semantic memories. These results are important to demonstrate that aging is not accompanied by a decline in episodic memory due to a reliance on semantic memory as previously thought. The remember-know distinction was integral in achieving these results as well as understanding the ways in which autobiographical memory works and the prevalence of the reminiscence bump. The findings of Rybash are supported by other research.\n\nThe tip-of-the-tongue state is the phenomenon that occurs when people fail to recall information but still feel as if they are close to retrieving it from memory. In this sense an individual feels as if they \"know\" but cannot \"remember\" the actual information desired. It is a frustrating but common problem that typically occurs for individuals about once a week, is frequent among nouns and is typically resolved on its own. The occurrence of the tip-of-the-tongue state increases with age throughout adulthood. Such a feeling is indication that remembering will occur or is about to occur.\n\nThe knew-it-all-along effect is a variant of the hindsight bias. It is the tendency for people to misremember and think that they knew more in the past than they actually did. In such situations it is difficult for us to remember what it was like when we did not have an understanding of something. For example, one might have a hard time teaching a concept to another individual because they can not remember what it is like to not understand the material.\n\nHindsight bias is the phenomenon where people tend to view events as more predictable than they really are. This occurs because one's current knowledge influences the recollection of previous beliefs. In this phenomenon, what someone \"knows\" is affecting what they \"remember\". This inaccurate assessment of reality after it has occurred is also referred to as \"creeping determinism\". The hindsight bias has been found among a number of domains such as historical events, political elections and the outcome of sporting events. The hindsight bias is a common phenomenon that occurs regularly among individuals in everyday life and can be generated in a laboratory setting to help increase the understanding of memory and specifically memory distortions.\n", "id": "21312310", "title": "Remember versus know judgements"}
{"url": "https://en.wikipedia.org/wiki?curid=5506325", "text": "Memory inhibition\n\nIn psychology, memory inhibition is the ability \"not\" to remember irrelevant information. The scientific concept of memory inhibition should not be confused with everyday uses of the word \"inhibition\". Scientifically speaking, memory inhibition is a type of cognitive inhibition, which is the stopping or overriding of a mental process, in whole or in part, with or without intention.\n\nMemory inhibition is a critical component of an effective memory system. While some memories are retained for a lifetime, most memories are forgotten. According to evolutionary psychologists, forgetting is adaptive because it facilitates selectivity of rapid, efficient recollection. For example, a person trying to remember where they parked their car would not want to remember every place they have ever parked. In order to remember something, therefore, it is essential not only to activate the relevant information, but also to inhibit irrelevant information. \n\nThere are many memory phenomena that seem to involve inhibition, although there is often debate about the distinction between interference and inhibition.\n\nIn the early days of psychology, the concept of inhibition was prevalent and influential (e.g., Breese, 1899; Pillsbury, 1908; Wundt, 1902). These psychologists applied the concept of inhibition (and interference) to early theories of learning and forgetting. Starting in 1894, German scientists Muller and Shumann conducted empirical studies that demonstrated how learning a second list of items interfered with memory of the first list. Based on these experiments, Muller argued that the process of attention was based on facilitation. Arguing for a different explanation, Wundt (1902) claimed that selective attention was accomplished by the active inhibition of unattended information, and that to attend to one of several simultaneous stimuli, the others had to be inhibited. American Psychologist Walter Pillsbury combined Muller and Wundt's arguments, claiming that attention both facilitates information that is wanted and inhibits information that is unwanted.\n\nIn the face of behaviorism during the late 1920s through the 1950s, and through the early growth of cognitive psychology in the late 1950s and early 1960s, inhibition largely disappeared as a theory. Instead, classical interference theory dominated memory research until as late as 1960. By the early 1970s, however, classical interference theory began to decline due to its reliance on associationism, to its inability to explain the facts of interference or how interference applies to everyday life, and to newly published reports on proactive and retroactive inhibition.\n\nSince the mid-1980s, there has been a renewed interest in understanding the role of inhibition in cognition. Research on a wide variety of psychological processes, including attention, perception, learning and memory, psycholinguistics, cognitive development, aging, learning disabilities, and neuropsychology, suggests that resistance to interference (which implies capacity for inhibition) is an important part of cognition.\n\nMore recently, researchers suggest that the hippocampus plays a role in the regulation of disliked and competing memories, and fMRI studies have shown hippocampus activity during inhibition processes.\n\nThe \"part-set cuing effect\" was initially discovered by Slamecka (1968), who found that providing a portion of to-be-remembered items as test cues\noften impairs retrieval of the remaining un-cued items compared with performance in a no-cue (free-recall) control condition. Such an effect is intriguing because\nnormally cues are expected to aid recall (e.g., Tulving & Pearlstone,\n1966). A prominent figure in retrieval-based inhibition research, Henry L. Roediger III was another one of the first psychologists to propose the idea that retrieving an item reduces the subsequent accessibility of other stored items. Becoming aware of the part-set cueing effect reduces the effect, such that relearning part of a set of previously learned associations can improve recall of the non-relearned associations.\n\nUsing inhibition to explain memory processes began with the work of Hasher and Zacks (1988), which focused on the cognitive costs associated with aging and bridging the attention-memory gap. Hasher and Zacks found that older adults show impairments on tasks that require inhibiting irrelevant information in working memory, and these impairments may lead to problems in a variety of contexts.\n\nAnderson and Spellman's model of retrieval-induced forgetting suggests that when items compete during retrieval, an inhibitory process will serve to suppress those competitors. For instance, retrieval of one meaning for a word (e.g. the verb meaning of the word \"sock\") will tend to inhibit the dominant meaning of that word (e.g. the noun meaning of \"sock\"). In 1995, Anderson and Spellman conducted a three-phase study using their retrieval-induced forgetting model to demonstrate unlearning as inhibition.\n\nAnderson and Spellman observed that items that shared a semantic relationship with practiced information was less recallable. Using the example from above, recall of items related to practiced information, including \"tomato\" and \"strawberry\" was lower than recall for \"cracker\", even though strawberry is part of a different pair. This finding suggests that associative competition by explicit category cue is not the only factor in retrieval difficulty. They theorized that the brain suppresses, or inhibits, non-practiced attributes. This explains why an item that is very similar to tomato, but not from the same pair, also exhibits decreased recall rate.\n\nDuring the recovered memory debate of the 1990s, cognitive psychologists were dubious about whether specific memories could be repressed. One stumbling block was that repression had not been demonstrated in a research study. In 2001, researchers Anderson and Green claimed to have found laboratory evidence of suppression. They trained their participants with a list of unrelated word pairs (such as ordeal-roach), so they could respond with the second member of the pair (roach) when they saw the other member (ordeal). The more frequently participants had tried to not think about a particular word, the less likely they were to retrieve it on a final memory test. This impairment even occurred when participants were given an \"independent probe\" test, i.e. given a similar category (insect) instead of the original cue (roach), and asked to fill in the blank on the memory test: insect-r_____. According to Anderson and Green, the fact that participants had a decreased ability to recall items they were told to forget strongly supports the existence of an inhibitory control mechanism and the idea that people have the ability to suppress unwanted memories.\n\nThough Anderson & Green's (2001) results have been replicated several times, a group of prominent psychology researchers using the same methodology as the original study were unable to replicate even the basic result (Bulevich, Roediger, Balota, & Butler, 2006). They determined that suppression is not a robust experimental phenomenon in the think/no-think paradigm and suggested that Anderson and Green's findings could be explained by retroactive interference, or simply thinking about X when told to \"not think\" about Y.\n\nAmnesia, the forgetting of important personal information, usually occurs because of disease or injury to the brain, while Psychogenic amnesia, which involves a loss of personal identity and has psychological causes, is rare. Nonetheless, a range of studies have concluded that at least 10% of physical and sexual abuse victims forget the abuse. Some studies claim that the rate of delayed recall of many forms of traumatic experiences (including natural disasters, kidnapping, torture and more) averages among studies at approximately 15%, with the highest rates resulting from child sexual abuse, military combat, and witnessing a family member murdered. A 1996 interview survey of 711 women reported that forgetting and later remembering childhood sexual abuse is not uncommon; more than a quarter of the respondents who reported abuse also reported forgetting the abuse for some period of time and then recalling it on their own. Of those who reported abuse, less than 2% reported that the recall of the abuse was assisted by a therapist or other professional. Other studies show that people who have experienced trauma usually remember it, not forget it. (McNally, 2001) found that women who report having either repressed or recovered memories of childhood sexual abuse have no worse memory for trauma cue words than women who have never been sexually abused. Similarly, McNally (1998) found that women who were sexually abused as children and who developed PTSD as a result of their abuse will not have any more trouble recalling trauma related words than healthy adult survivors of childhood sexual abuse or women who were never abused as children.\n\nAlthough the rate of recall of previously forgotten traumatic events was shown by Elliot and Briere (1996) to be unaffected by whether or not the victim had a history of being in psychotherapy, individuals who report repressed memories are more susceptible to producing false memories than individuals who could always recall the memory. Williams found that among women with confirmed histories of sexual abuse, approximately 38% did not recall the abuse 17 years later, especially when it was perpetrated by someone familiar to them. Hopper cites several studies which indicate that some abuse victims will have intervals of complete amnesia for their abuse. Peer reviewed and clinical studies have documented the existence of recovered memory; one website lists 43 legal cases where an individual whose claim to have recovered a repressed memory has been accepted by a court. Traumatic amnesia, which allegedly involves the forgetting of specific traumatic events for long periods of time, is highly controversial, as is repression, the psychodynamic explanation of traumatic amnesia. Because these concepts lack good empirical support, psychological scientists are skeptical about the validity of \"recovered memories\", and argue that some therapists, through suggestive techniques, have (un)knowingly encouraged false memories of victimization.\n\nThe idea that subjects can actively inhibit a memory has many critics. MacLeod (2003) challenged the idea of inhibition in cognitive control, arguing that inhibition can be attributed to conflict resolution, which is the error-prone act of choosing between two similar values that do not necessarily have the same pair. Re-examine the pairs from above: Food-Cracker, Food-Strawberry, Red-Tomato, and Red-Blood. Memory inhibition theories suggest that recall of strawberry decreases when recall of tomato decreases because tomato's attributes are inhibited when red-blood is learned. MacLeod argues that inhibition does not take place, but instead is the result of confusion between similar word-pairs like food-tomato and red-strawberry that can lead to errors. This is different from tomato's attributes being inhibited. \"In most cases where inhibitory mechanisms have been offered to explain cognitive performance,\" explains MacLeod, \"non-inhibitory mechanisms can accomplish the same goal (p.203).\"\n\n\n", "id": "5506325", "title": "Memory inhibition"}
{"url": "https://en.wikipedia.org/wiki?curid=273177", "text": "Human memory process\n\nNumerous theoretical accounts of memory have differentiated memory for facts and memory for context. Psychologist Endel Tulving (1972; 1983) further defined these two declarative memory conceptions of explicit memory (in which information is consciously registered and recalled) into semantic memory wherein general world knowledge not tied to specific events is stored and episodic memory involving the storage of context-specific information about personal experiences (i.e. time, location, and surroundings of personal knowledge). Conversely, implicit memory (non declarative) involves perhaps unconscious registration (lack of awareness during encoding), yet definite unconscious recollection. Skills and habits, priming, and classical conditioning all utilize implicit memory.\n\nAn essential aspect of episodic memory includes date and time encoding in the subject's past. For such processing, the details surrounding the memory (where, when, and with whom the experience took place) must be preserved and are necessary for an episodic memory to form, otherwise the memory would be semantic. For instance, one may possess an episodic memory of John F. Kennedy's assassination, including the fact that he was watching Walter Cronkite announce that Kennedy had been murdered. However, if the contextual details of this event were lost, remaining would be a semantic memory that John F. Kennedy was assassinated. The ability to recall episodic information concerning a memory has been termed source monitoring, and is subject to distortion that may lead to source amnesia.\n\n", "id": "273177", "title": "Human memory process"}
{"url": "https://en.wikipedia.org/wiki?curid=533317", "text": "Decay theory\n\nDecay theory proposes that memory fades due to the mere passage of time. Information is therefore less available for later retrieval as time passes and memory, as well as memory strength, wears away. When we learn something new, a neurochemical \"memory trace\" is created. However, over time this trace slowly disintegrates. Actively rehearsing information is believed to be a major factor counteracting this temporal decline. It is widely believed that neurons die off gradually as we age, yet some older memories can be stronger than most recent memories. Thus, decay theory mostly affects the short-term memory system, meaning that older memories (in long-term memory) are often more resistant to shocks or physical attacks on the brain. It is also thought that the passage of time alone cannot cause forgetting, and that decay theory must also take into account some processes that occur as more time passes.\n\nThe term \"decay theory\" was first coined by Edward Thorndike in his book \"The Psychology of Learning\" in 1914. This simply states that if a person does not access and use the memory representation they have formed the memory trace will fade or decay over time. This theory was based on the early memory work by Hermann Ebbinghaus in the late 19th century. The decay theory proposed by Thorndike was heavily criticized by McGeoch and his interference theory. This led to the abandoning of the decay theory, until the late 1950s when studies by John Brown and the Petersons showed evidence of time based decay by filling the retention period by counting backwards in threes from a given number. This led to what is known as the Brown–Peterson paradigm. The theory was again challenged, this time a paper by Keppel and Underwood who attributed the findings to proactive interference. Studies in the 1970s by Reitman tried reviving the decay theory by accounting for certain confounds criticized by Keppel and Underwood. Roediger quickly found problems with these studies and their methods. Harris made an attempt to make a case for decay theory by using tones instead of word lists and his results are congruent making a case for decay theory. In addition, McKone used implicit memory tasks as opposed to explicit tasks to address the confound problems. They provided evidence for decay theory, however, the results also interacted with interference effects. One of the biggest criticisms of decay theory is that it cannot be explained as a mechanism and that is the direction that the research is headed.\n\nResearchers disagree about whether memories fade as a function of the mere passage of time (as in decay theory) or as a function of interfering succeeding events (as in interference theory). Often, evidence tends to favour interference related decay over temporal decay, yet this varies depending on the specific memory system taken into account.\n\nWithin the short-term memory system, evidence favours an interference theory of forgetting, based on various researchers' manipulation of the amount of time between a participant's retention and recall stages finding little to no effect on how many items they are able to remember. Looking solely at verbal short-term memory within studies that control against participants' use of rehearsal processes, a very small temporal decay effect coupled with a much larger interference decay effect can be found. No evidence for temporal decay in verbal short-term memory has been found in recent studies of serial recall tasks. Regarding the word-length effect in short-term memory, which states that lists of longer word are harder to recall than lists of short words, researchers argue that interference plays a larger role due to articulation duration being confounded with other word characteristics.\n\nBoth theories are equally argued in working memory. One situation in which this shows considerable debate is within the complex-span task of working memory, where a complex task is alternated with the encoding of to-be-remembered items. It is either argued that the amount of time taken to perform this task or the amount of interference this task involves cause decay. A time-based resource-sharing model has also been proposed, stating that temporal decay occurs once attention is switched away from whatever information is to be remembered, and occupied by processing of the information. This theory gives more credit to the active rehearsal of information, as refreshing items to be remembered focuses attention back on the information to be remembered in order for it to be better processed and stored in memory. As processing and maintenance are both crucial components of working memory, both of these processes need to be taken into account when determining which theory of forgetting is most valid. Research also suggests that information or an event's salience, or importance, may play a key role. Working memory may decay in proportion to information or an event's salience. This means that if something is more meaningful to an individual, that individual may be less likely to forget it quickly.\n\nThese inconsistencies may be found due to the difficulty with conducting experiments that focus solely on the passage of time as a cause of decay, ruling out alternative explanations. However, a close look at the literature regarding decay theory will reveal inconsistencies across several studies and researchers, making it difficult to pinpoint precisely which indeed plays the larger role within the various systems of memory. It could be argued that both temporal decay and interference play an equally important role in forgetting, along with motivated forgetting and retrieval failure theory.\n\nRevisions in decay theory are being made in research today. The theory is simple and intuitive, but also problematic. Decay theory has long been rejected as a mechanism of long term forgetting. Now, its place in short term forgetting is being questioned. The simplicity of the theory works against it in that supporting evidence always leaves room for alternative explanations. Researchers have had much difficulty creating experiments that can pinpoint decay as a definitive mechanism of forgetting. Current studies have always been limited in their abilities to establish decay due to confounding evidence such as attention effects or the operation of interference.\n\nThe future of decay theory, according to Nairne (2002), should be the development of hybrid theories that incorporate elements of the standard model while also assuming that retrieval cues play an important role in short term memory. By broadening the view of this theory, it will become possible to account for the inconsistencies and problems that have been found with decay to date.\n\nAnother direction of future research is to tie decay theory to sound neurological evidence. As most current evidence for decay leaves room for alternate explanations, studies indicating a neural basis for the idea of decay will give the theory new solid support. Jonides et al. (2008) found neural evidence for decay in tests demonstrating a general decline in activation in posterior regions over a delay period. Though this decline was not found to be strongly related to performance, this evidence is a starting point in making these connections between decay and neural imaging. \nA model proposed to support decay with neurological evidence places importance on the firing patterns of neurons over time. The neuronal firing patterns that make up the target representation fall out of synchrony over time unless they are reset. The process of resetting the firing patterns can be looked at as rehearsal, and in absence of rehearsal, forgetting occurs. This proposed model needs to be tested further to gain support, and bring firm neurological evidence to the decay theory.\n", "id": "533317", "title": "Decay theory"}
{"url": "https://en.wikipedia.org/wiki?curid=26346988", "text": "Verbal memory\n\nVerbal memory is a term used in cognitive psychology that refers to memory of words and other abstractions involving language.\n\nVerbal encoding refers to the interpretation of verbal stimuli. Verbal encoding appears to be strongly left-lateralized in the medial temporal lobe of the human brain; however, its functional neuroanatomy can vary between individuals.\n\nVerbal recall refers to the recollection of verbal information. Although left-lateralization is typically associated with language, studies suggest that symmetrical bi-lateralization of language in the brain is advantageous to verbal recall.\n\n", "id": "26346988", "title": "Verbal memory"}
{"url": "https://en.wikipedia.org/wiki?curid=533281", "text": "Interference theory\n\nInterference theory is a theory regarding human memory. Interference occurs in learning when there is an interaction between the new material and transfer effects of past learned behavior, memories or thoughts that have a negative influence in comprehending the new material. Bringing to memory old knowledge has the effect of impairing both the speed of learning and memory performance. \nThere are two main kinds of interference: \nThe main assumption of interference theory is that the stored memory is intact but unable to be retrieved due to competition created by newly acquired information.\n\nJohn A. Bergström is credited as conducting the first study regarding interference in 1892. His experiment was similar to the Stroop task and required subjects to sort two decks of card with words into two piles. When the location was changed for the second pile, sorting was slower, demonstrating that the first set of sorting rules interfered with learning the new set. German psychologists continued in the field with Georg Elias Müller and Pilzecker in 1900 studying retroactive interference. To the confusion of Americans at a later date, Müller used \"associative Hemmung\" (inhibition) as a blanket term for retroactive and proactive inhibition.\n\nThe next major advancement came from American psychologist Benton J. Underwood in 1957. Underwood revisited the classic Ebbinghaus learning curve and found that much of the forgetting was due to interference from previously learned materials \n\nIn 1924, James J. Jenkins and Karl Dallenbach showed that everyday experiences can interfere with memory with an experiment that resulted in retention being better over a period of sleep than over the same amount of time devoted to activity. The United States again made headway in 1932 with John A. McGeoch suggesting that decay theory should be replaced by an interference theory. The most recent major paradigm shift came when Underwood proposed that proactive inhibition is more important or meaningful than retroactive inhibition in accounting for forgetting.\n\nProactive interference is the \"forgetting [of information] due to interference from the traces of events or learning that occurred prior to the materials to be remembered\". Proactive interference occurs when, in any given context, past memories inhibit an individual's full potential to retain new memories. It has been hypothesized that forgetting working memories would be non-existent if not for proactive interference.\n\nProactive interference build up occurs with memories being learned in similar contexts. It is also associated with poorer list discrimination, which occurs when participants are asked to judge whether an item has appeared on a previously learned list. If the items or pairs to be learned are conceptually related to one another, then proactive interference has a greater effect. Delos Wickens discovered that proactive interference build up is released when there is a change to the category of items being learned, leading to increased processing in short term memory.\n\nThe leading experimental technique for studying proactive interference in the brain is the \"recent-probes\" task, in which participants must commit a given set of items to memory and they are asked to recall a specific item indicated by a probe. Using the recent-probes task and fMRIs, the brain mechanisms involved in the resolution of proactive interference have been identified as the ventrolateral prefrontal cortex and the left anterior prefrontal cortex.\n\nResearchers have studied the joint influence of proactive and retroactive interference using a list of items to be remembered. As expected, recall was hampered by increasing the number items in a given list. Proactive interference also affected learning when dealing with multiple lists. Researchers had participants learn a list of 10 paired adjectives. The experimenters would consider a list to be learned if the participant could correctly recall eight of the ten items. After two days, participants could recall close to 70% of the items. However, those asked to memorize a new list the day after learning the first one had a recall of only 40%. Those who learned a third list recalled 25% of the items. Therefore, Proactive interference affected the correct recall of the last list learned, because of the previous one, or two. In terms of forgetting, the effect of Proactive interference was supported by further studies using different methods. The effect of proactive interference was reduced when the test was immediate and when the new target list was obviously different from the previously learned lists.\n\nSpan performance refers to working memory capacity. It is hypothesized that span performance is limited in language comprehension, problem solving, and memory. Proactive Interference affects susceptibility to span performance limitations, as span performance in later experimental trials were worse than performance in earlier trials. With single tasks, proactive interference had less effect on participants with high working memory spans than those with low ones. With dual tasks, both types were similarly susceptible.\n\nTo differ, others have tried to investigate the relation of proactive interference when cued to forget. Turvey and Wittlinger designed an experiment to examine the effects of cues such as \"not to remember\" and \"not to recall\" with currently learned material. While \"not to remember\" had a significant effect in reducing proactive interference, cued to \"not to recall\" previously encoded and stored information did not significantly reduce the effect. Therefore, these associated cues do not directly control the potential effect of proactive interference on short term memory span.\n\nProactive interference has shown an effect during the learning phase in terms of stimuli at the acquisition and retrieval stages with behavioral tasks for humans, as found by Castro, Ortega and Matute. With 106 participants, they investigated two main questions: if two cues are learned as predictors of the same outcome (one after the other), would the second-cue outcome association be retarded? And secondly, once the second association is fully learned, will there still be an effect on subsequent trials? The research, as predicted, showed retardation and impairment in associations, due to the effect of Proactive Interference.\n\nRetroactive interference (RI) is a phenomenon that occurs when newly learned information interferes with and impedes the recall of previously learned information. RI is a result of decreased recall of the primary studied functions due to the learning and recall of succeeding functions. RI is a classic paradigm that was first officially termed by Muller. These memory research pioneers demonstrated that filling the retention interval (defined as the amount of time that occurs between the initial learning stage and the memory recall stage) with tasks and material caused significant interference effects with the primary learned items.\n\nAs compared to proactive interference, retroactive interference may have larger effects because of the fact that there is not only competition involved, but also unlearning.\n\nBriggs (1954) study modeled McGeoch’s work on interference by setting the stage for a classic design of retroactive interference. In his study participants were asked to learn 12 paired associates to a criterion of 100%. To ensure parsimony, these pairs can be labeled as A-B-, A-B-…A-B (also called AB/AC paradigm). Briggs used a \"modified free recall\" technique by asking participants to recall an item when cued with B. Over multiple anticipation trials, participants learned B items through the prompt of B items. After perfecting Ai- B learning, participants were given a new list of paired associates to learn; however B items were replaced with C items (now given a list of A-C-, A-C-…A-C). As the learning of A-C pairs increased, the learning of A-B pairs decreased. Eventually recalling the Ci items exceeded the recall of the B items, representing the phenomenon of retroactive interference.\nA significant part of Briggs (1954) study was that once participants were tested after a delay of 24 hours the Bi responses spontaneously recovered and exceeded the recall of the Ci items. Briggs explained the spontaneous recovery illustration as an account of A-B items competing with A-C items or, as McGeoch would define it: \"a resultant [of] momentary dominance\".\n\nJ.M. Barnes and B.J. Underwood (1959) expanded Briggs (1954) study by implementing a similar procedure. The main difference in this study, however, was that unlike Briggs (1954) \"modified free recall\" (MFR) task where participants gave one item responses, Barnes and Underwood asked participants to give both List 1 and List 2 responses to each cued recall task. Participants’ ability to recall both items was termed \"modified modified free recall\" (MMFR) technique. Equivocally to Briggs (1954) results, RI occurred when C recalled responses gradually came to exceed B responses. Barnes and Underwood argued that because there was \"unlimited recall time\" to produce multiple item responses, the fact that A-C responses still trumped A-B responses represented an account of unlearning.\n\nThe phenomenon of retroactive interference is highly significant in the study of memory as it has sparked a historical and ongoing debate in regards to whether the process of forgetting is due to the interference of other competing stimuli, or rather the unlearning of the forgotten material. The important conclusion one may gain from RI is that \"forgetting is not simply a failure or weakness of the memory system\" (Bjork, 1992), but rather an integral part of our stored knowledge repertoire. Although modern cognitive researchers continue to debate the actual causes of forgetting (e.g., competition vs. unlearning), retroactive interference implies a general understanding that additional underlying processes play a role in memory.\n\nA standard explanation for the cause of RI is Competition. New associations compete with older associations and the more recent association would win out making it impossible to remember earlier associations. Spontaneous Recovery in MFR supports the claim of competition since after a rest period participants spontaneously remembered original pair associations that they were not able to remember right after the second test.\n\nThe associative unlearning hypothesis explains RI by saying that new associations replace the old associations in memory causing the participant to forget the initial associations. Barnes and Underwood argued that A-C responses still outnumbering A-B responses after the delay period supports the Associative Unlearning Hypothesis over Competition.\n\nRetroactive Interference has been localized to the left anterior ventral prefrontal cortex by magnetoencephalography (MEG) studies investigating Retroactive Interference and working memory in elderly adults. The study found that adults 55–67 years of age showed less magnetic activity in their prefrontal cortices than the control group. Executive control mechanisms are located in the frontal cortex and deficits in working memory show changes in the functioning of this brain area.\n\nRetroactive Interference has also been investigated using pitch perception as the learning medium. The researcher found that the presentation of subsequent stimuli in succession causes a decrease in recalled accuracy. Massaro found that the presentation of successive auditory tones, confused perceptual short term memory, causing Retroactive Interference as the new tone inhibits the retrieval of previously heard tones.\n\nWohldmann, Healey and Bourne found that Retroactive Interference also affects retention of motor movements. Researchers found that retroactive interference affects the performance of old motor movements when newly acquired motor movements are practiced. Physical practice of newly executed motor movements decreased the retention and recall of previously learnt movements. Despite the retroactive interference noted by Wohldmann et al., researchers noted that mental practice decreased the amount of retroactive interference, suggesting that mental practice is more flexible and durable over time. This study of the superiority effect of physical practice is similar to the Word Superiority Effect made famous by Cattell.\n\nRetroactive Interference increases when the items are similar, therefore increasing association between them as shown by spreading activation. Barnes and Underwood found that when participants in the experimental condition were presented with two similar word lists, the recollection of the first word list decreased with the presentation of the second word list. This finding contrasts the control condition as they had little Retroactive Inference when asked to recall the first word list after a period of unrelated activity.\n\nOutput Interference occurs when the initial act of recalling specific information interferes with the retrieval of the original information. An example scenario in which Output Interference might occur would be if one had created a list of items to purchase at a grocery store, but then neglected to take the list when leaving home. The act of remembering a couple items on that list decreases the probability of remembering the other items on that list.\n\nHenry L. Roediger III and Schmidt found that the act of retrieval can serve as the source of the failing to remember, using multiple experiments that tested the recall of categorized and paired associative lists. Three experiments were carried out where subjects were first presented with category lists and then asked to recall the items in the list after being shown the category name as a cue. The further the test position from the category resulted in a decline of the recall of words. A fourth experiment revealed that only recent items were present in output interference in paired associative lists.\n\nSmith found that if categories with corresponding items were successfully recalled, a systematic decline would occur when recalling the items in a category across the output sequence. He conducted multiple experiments to determine the input conditioned necessary to produce Output Interference. In his first experiment word recall per category was greater at 60 sec than 30 sec when taking the last input category out to prevent recency effect. In his second experiment he changed the instructions, words used, and nature of the test for retention, and showed with recognition procedure, there was Output Interference but the effect was limited to the first three output positions. Even if retrieving items is necessary for recall, it is not crucial to performance in a recognition tack. Recall of the organized information from long-term memory had a negative effect on the following item recalled. In long-term memory, Smith suggests that Output Interference has effects on extra-core material, which is represented as contextual information, rather than core material, which is highly available as a result of organization. Both short and long term memories are centralized to the hippocampus and the amygdala.\n\nIn both short-term memory and long-term memory Smith measured output interference in three age groups (aged 20–39, 40-59, 60–80 years). The results of recall performance revealed significant differences due to age where the older group recalled fewer items than the middle group who recalled fewer items than the youngest group. Overall Smith concluded that memory decline appears with increased age with long-term memory forgetting rather than short-term memory forgetting and short-term memory was unaffected by age. However output interference was unable to explain the memory deficit seen in older subject.\n\nRecent research of adult’s free recall and cognitive triage displayed similar findings of recall performance being poorer in older adults compared to younger adults. Although it was also indicated that older adults had an increased susceptibility to output interference compared to younger adults and the difference increased as additional items were recalled.\n\nDecay theory outlines that memories weaken over time despite consolidation and storing. This is to say that although you remember a specific detail, over time you may have greater difficulty retrieving the detail you encoded. It has been suggested that the time interval between encoding and retrieval determines the accuracy of recall.\n\nA practical example of decay theory is seen in the financial sector. If you open a bank account and not deposit or withdraw money from the account, after a period of time the bank will render the account dormant. The owner of the account then has to reopen the account for it to remain active. The bank account (the memory) is rendered dormant (the memory weakened) over time if there is not activity on the account (if the memory is not retrieved after a period of time).\n\nDecay theory is similar to interference theory in the way that old memories are lost over time. Memories are lost in Decay Theory by the passing of time. In Interference Theory, memories are lost due to newly acquired memories. Both Decay and Interference Theories are involved in psychological theories of forgetting.\n\nDecay and interference theory differ in that Interference Theory has a second stimulus that impedes the retrieval of the first stimulus. Decay Theory is caused by time itself. Decay Theory is a passive method of forgetting as no interference is produced. Interference Theory is an active process because the act of learning new information directly impedes the recollection of previously stored information.\n\nDual task interference is a kind of interference that occurs when two tasks are attempted simultaneously. Harold Pashler from McMaster University in Hamilton, Ontario, Canada wrote a paper summing up the theoretical approaches to dual task interference. The basis of his research looked at, when one attempts two or more tasks at the same time, why in some cases is one successful in completing their task and in other cases not.\n\nPashler proposed that the brain contains one mental entity to where all tasks must be carried out. A real-life example of this could be going to the dentist; the only place to have cavities filled is at a dentist’s office. When the brain is attempting to complete two tasks, both tasks are present in the same mind area and compete for processing ability and speed. This relates to interference theory as the tasks compete. Interference theory says that the learning of new information decreases the retrieval of older information and this is true in dual task interference. The dominant task of the two inhibits the other task from completion. It is presumed that the dominant task would be a new task as a previously accomplished task would already be stored in memory. The new task would then successfully be completed as more mind effort is required to complete a novel task and the previously completed task would not be completed as the new task dominated the mental capacity. Just as Interference Theory states, the completion of new tasks inhibits the completion of previously completed tasks due to capacity sharing.\n\nCross talk is the communication between sensory inputs, processing and the thoughts of the individual. The theory is that if two processes are being activated and they are not similar in any way (making cookies and going on vacation), the brain will be confused as separate cognitive areas are being activated and there is conflicting communication between the two. Contrastingly, if the two processes are similar (making cookies and pouring milk), there will be less crosstalk and a more productive and uninterrupted cognitive processing.\n\nCrosstalk is used by engineers to discuss the degradation of communication channels due to context dependence.\n\nNavon and Miller claim that Dual Task Interference is caused by outcome conflict which is a result of one task producing, \"outputs, throughputs, or side effects that are harmful to the processing of the [other task]\". This is basically the concept of Interference Theory. The thoughts, outputs and side effects of one task either effect the previous or subsequent recall.\n\nThe performance of Stroop and Simon tasks were monitored on 10 healthy young adults using magnetic resonance image (MRI) scanning. Functional images were acquired at specific time intervals during each subject's scan. Brain activation during the Stroop and Simon task was remarkably similar including anterior cingulate, supplementary motor cortex, visual association cortex, inferior temporal cortex, inferior parietal cortex, inferior frontal cortex, dorsolateral prefrontal cortex, and caudate nuclei. Interference effects in the Stroop and Simon tasks activate similar brain regions at similar time distributions.\n\nIt has been demonstrated that recall will be lower when consumers have afterwards seen an ad for a competing brand in the same product class. Exposure to later similar advertisements does not cause interference for consumers when brands are rated on purchasing likelihood. This shows that information processing objective can moderate the effects of interference of competitive advertising. Competitive brand advertising not only interferes with consumer recall of advertising in the past but also interferes with learning new distinctive brand information in the future.\n\nRepetition improves brand name recall when presented alone. When competitive advertising was presented it was shown that repetition provided no improvement in brand name recall over a single exposure. The competitive ads interfered with the added learning from repetition. However, when target brand name was shown using varying ad executions interference was reduced. Presenting ads in multi modalities (visual, auditory) will reduce possible interference because there are more associations or paths to cue recall than if only one modality had been used. This is the principle of multimedia learning. Also, interference is increased when competing ads are presented in the same modality. Therefore, by presenting ads in multiple modalities the chance that the target brand has unique cues is increased.\n\n\n", "id": "533281", "title": "Interference theory"}
{"url": "https://en.wikipedia.org/wiki?curid=26685741", "text": "Sleep and memory\n\nThe relationship between sleep and memory has been postulated and studied since at least the early 19th century. Memory, the cognitive process whereby experiences, learning and recognition are recalled, is a product of brain plasticity, the structural changes within synapses that create associations between stimuli. Stimuli are encoded within milliseconds; however, the long-term maintenance of memories can take additional minutes, days, or even years to fully consolidate and become a stable memory (more resistant to change or interference). Therefore, the formation of a specific memory occurs rapidly, but the evolution of a memory is often an ongoing process.\n\nMemory processes have been shown to be stabilized and enhanced (sped up and/or integrated) and memories better consolidated by nocturnal sleep and even daytime naps. Certain sleep stages have been demonstrated as improving an individual's memory, though this is task-specific. Generally, declarative memories are believed to be enhanced by slow-wave sleep, while non-declarative memories are enhanced by rapid eye movement (REM) sleep, although there are some inconsistencies among experimental results. The effect of sleep on memory, especially as it pertains to the human brain, is an active field of research in neurology, psychology, and related disciplines.\nIn 1801, David Hartley first postulated that dreaming altered the associative planetary links within the brain during dreaming periods of dreams. The idea that sleep had a mentally restorative effect, sorting out and consolidating memories and ideas, was intellectually acceptable by the end of the 19th century. In ‘Peter and Wendy’, J.M. Barrie wrote \"‘It is the nightly custom of every good mother after her children are asleep to rummage in their minds and put things straight for next morning, repacking into their proper places the many articles that have wandered during the day. …When you wake in the morning, the naughtinesses and evil passions with which you went to bed have been folded up small and placed at the bottom of your mind; and on the top, beautifully aired, are spread out your prettier thoughts, ready for you to put on.’\" The stories of Peter Pan take place in a mental world and contain many allusions to aspects of cognitive psychology, some of which predate their formal scientific investigation.\n\nThe first semi-multiple-systematic study of the connection between sleep and memory was conducted in 1924 by Jenkins and Dallenbach, for the purpose of testing Hermann Ebbinghaus' memory decay theory. Their results showed that memory retention was much better after a period of sleep compared to the same time interval spent awake. It was not until 1953, however, when sleep was delineated into rapid eye movement sleep and non-rapid eye movement sleep, that studies focusing on the effect of specific sleep stages on memory were conducted. As behavioral characteristics of the effects of sleep and memory are becoming increasingly understood and supported, researchers are turning to the weakly understood neural basis of sleep and memory.\n\nSleep progresses in a cyclical fashion through five stages. Four of these stages are collectively referred to as \"non-rapid eye movement\" (NREM) sleep whereas the last cycle is a rapid eye movement period. A cycle takes approximately 90–110 minutes to complete. Wakefulness is found through EEG measures to be characterized by \"beta waves\" which are the highest in frequency and lowest in amplitude and tend to move inconsistently due to the vast amount of stimuli a person encounters while awake. \nDuring the first half of the night, the largest portion of sleep is spent as SWS, but as the night progresses SWS stages decrease in length while REM stages increase.\n\nStabilization of a memory is the anchoring a memory in place, a weak connection is established. Stabilization of procedural memories can even occur during waking hours, suggesting that specific non-declarative tasks are enhanced in the absence of sleep.\nWhen memories are said to be enhanced, however, the connection is strengthened by rehearsal as well as connecting it to other related memories thereby making the retrieval more efficient. Whereas stabilization of non-declarative memories can be seen to occur during a wakeful state, \"enhancement\" of these sensory and motor memories has most been found to occur during nocturnal sleep.\n\nBrain activity that occurs during sleep is assessed in two ways: Use-dependency, and Experience-dependency. \nUse-dependent brain activity is a result of the neuronal usage that occurred during the previous waking hours. Essentially it is neuronal regeneration, activity that occurs whether you have learnt anything new or not.\n\nExperience-dependent brain activity is a result of a new situation, environment, or learned task or fact that has taken place in the pre-sleep period. This is the type of brain activity that denotes memory consolidation/enhancement.\n\nIt is often hard to distinguish between the two in an experimental setting because the setting alone is a new environment. This new environment would be seen in the sleeping brain activity along with the newly learned task. To avoid this, most experimenters insist participants spend one day in the experimental condition before testing begins so the setting is not novel once the experiment begins. This ensures the collected data for experience-dependent brain activity is purely from the novel task.\n\nConsolidation of a memory is a process that takes an initially unstable representation and encodes it in a more sturdy, effective and efficient manner. In this new state, the memory is less susceptible to interference. There are essentially three phases of memory consolidation and all are thought to be facilitated by sleep or not sleep:\n\nReconsolidation of a memory involves the retrieval of an already consolidated memory (explicit or implicit), into short-term or working memory. Here it is brought into a labile state where subsequent information can 'interfere' with what is currently in memory, therefore altering the memory. This is known as retroactive interference, and is an extremely significant issue for court and eye witness testimonies.\n\nResearchers approach the study of sleep and memory from different angles. Some studies measure the effects of sleep deprivation \"after\" a novel task is taught (the subject learns the task and is sleep deprived afterwards). This is referred to as \"post-training sleep deprivation\". Conversely, other experiments have been conducted that measure the effects of sleep deprivation \"before\" a task has been taught (the subject is sleep-deprived and then learns a task). This is referred to as \"pre-training sleep deprivation\".\n\nThis is the processing of memories out of conscious awareness. After you are finished reading something, for example this very article about sleep and memory, your brain continues to process the information even as you accomplish other tasks like playing a game of soccer. This \"offline\" processing likewise occurs in your sleep.\n\n\n\nNeuroimaging can be classified into two categories, both used in varying situations depending on what type of information is needed. Structural imaging deals predominately with the structure of the brain (computed tomography) while functional imaging deals more heavily with metabolic processes in regards to anatomical functioning (positron emission tomography, functional magnetic resonance imaging). In recent years, the relationship between sleep and memory processes had been aided by the development of such neuroimaging techniques.\n\nPositron emission tomography (PET) is used in viewing a functional processes of the brain (or other body parts). A Positron-emitting radionuclide is injected into the blood stream and emits gamma rays which are detected by an imaging scanner. Computer analysis then allows for a 3-dimensional reconstruction of the brain region or body part of interest.\n\nFunctional magnetic resonance imaging (fMRI) is a type of brain imaging that measures the change of oxygen in the blood due to the activity of neurons. The resulting data can be visualized as a picture of the brain with colored representations of activation.\n\nAlthough this may be seen as similar to neuroimaging techniques, molecular measures help to enhance areas of activation that would otherwise be indecipherable to neuroimaging. One such technique that aids in both the temporal and visual resolution of fMRI is the blood-oxygen-level dependent (BOLD) response. Changes in the BOLD response can be seen when there is differing levels of activation in suspected areas of functioning. Energy is supplied to the brain in the form glucose and oxygen (which is transferred by hemoglobin). The blood supply is consistently regulated so that areas of activation receive higher amounts of energy compared to areas that are less activated. In positron emission tomography, the use of radionuclides (isotopes with short half lives) facilitates visual resolution. These radionuclides are attached to glucose, water and ammonia so that easy absorption into the activated brain areas is accomplished. Once these radioactive tracers are injected into the bloodstream, the efficiency and location of chemical processes can be observed using PET.\n\nThe main method of measuring sleep in humans is polysomnography (PSG). For this method, participants often must come into a lab where researchers can use PSG to measure things such as total sleep time, sleep efficiency, wake after sleep onset, and sleep fragmentation. PSG can monitor various body functions including brain activity (electroencephalography), eye movement (electrooculography), muscle movement (electromyography), and heart rhythm (electrocardiography).\n\nElectroencephalography (EEG) is a procedure that records electrical activity along the scalp. This procedure cannot record activity from individual neurons, but instead measures the overall average electrical activity in the brain.\n\nElectrooculography (EOG) measures the difference in electrical potential between the front and the back of the eye. This does not measure a response to individual visual stimuli, but instead measures general eye movement.\n\nElectromyography (EMG) is used to records the electrical activity of skeletal muscles. A device called an electromyograph measures the electrical potential of muscle cells to monitor muscle movement.\n\nElectrocardiography (ECG or EKG) measures the electrical depolarization of the heart muscles using various electrodes placed near the chest and limbs. This measure of depolarization can be used to monitor heart rhythm.\n\nActigraphy is a common and minimally invasive way to measure sleep architecture. Actigraphy has only one method of recording, movement. This movement can be analyzed using different actigraphic programs. As such, an actigraph can often be worn similarly to a watch, or around the waist as a belt. Because it is minimally invase and relatively inexpensive, this method allows for recordings outside of a lab setting and for many days at a time. But, actigraphy often over estimates sleep time (de Souza 2003 and Kanady 2011).\n\nMost studies point to the specific deficits in declarative memories that form pre or post REM sleep deprivation. Conversely, deficits in non-declarative memory occur pre or post NREM sleep deprivation. This is the stage specific enhancement theory. There is also a proposed dual-step memory hypothesis suggesting that optimal learning occurs when the memory trace is initially processed in SWS and then REM sleep. Support for this is shown in many experiments where memory improvement is greater with either SWS or REM sleep compared to sleep deprivation, but memory is even more accurate when the sleep period contains both SWS and REM sleep.\n\nDeclarative memory is the memory for conscious events. There are two types of declarative memory: episodic and semantic. Episodic memory is for remembering experiences whereas semantic memory is remembering specific facts.\n\nTemporal memory consists of remembering \"when\" a specific memory has occurred. In a study participants were placed in 4 groups; two control groups either given caffeine or a placebo and two groups that were sleep deprived for 36 hours either given caffeine or a placebo. The task used to measure temporal memory consisted of discriminating between recent and less recent face presentations. A set of twelve unfamiliar faces were presented sequentially every 10 seconds. A self-ordered pointing task was used afterwards for 5 minutes to prevent rehearsal and to keep tired participants occupied. This required them to mark any new items seen (either nouns or abstract shapes) presented on 12 sheets. A second set was presented, followed by another self-ordered pointing task, and then a random sequence of 48 faces either containing previously presented faces or new ones were shown to the participant. They were asked if they recognized the faces and whether they were from the first or second set. Results indicate that sleep deprivation does not significantly affect recognition of faces, but does produce a significant impairment of temporal memory (discriminating which face belonged to which set). Caffeine was found to have a greater effect on the sleep deprived group as compared to the placebo group deprived of sleep but still performed worse than both control groups. Sleep deprivation was also found to increase beliefs of being correct, especially if they were wrong. Brain imaging studies of those sleep deprived found that the greatest reduction in metabolic rate is in the prefrontal cortex.\n\nA blood-oxygen-level dependent (BOLD) fMRI was used in a study by Drummond et al. to measure the brain's response to verbal learning following sleep deprivation. An fMRI recorded brain activity during a verbal learning task of participants either having a normal night of sleep or those deprived of 34.7 (± 1.2) hours of sleep. The task alternated between a baseline condition of determining whether nouns were upper or lower case and an experimental condition of memorizing a list of nouns. The results of the study indicate that performance is significantly worse on free recall of the list of nouns when sleep deprived (an average of 2.8 ± 2 words) compared to having a normal night of sleep (4.7 ± 4 words). In terms of brain regions activated, the left prefrontal cortex, premotor cortex, and temporal lobes were found to be activated during the task in the rested state and discrete regions of the prefrontal cortex were even more activated during the task in the sleep deprived state. As well, the bilateral parietal lobe, left middle frontal gyrus, and right interior frontal gyrus were found to be activated for those sleep deprived. The implication of these findings are that the brain can initially compensate for the effects of sleep deprivation while maintaining partially intact performance, which declines with an increasing time-on-task. This initial compensation may be found in the bilateral regions of both frontal and parietal lobes and the activation of the prefrontal cortex is significantly correlated with sleepiness.\n\nCerebral activation during performance on three cognitive tasks (verbal learning, arithmetic, and divided attention) were compared after both normal sleep and 35 hours of total sleep deprivation (TSD) in a study by Drummond and Brown. Use of fMRI measured these differences in the brain. In the verbal learning task, fMRI indicated the regions involved in both verbal learning and memorization. The results found that both TSD and a normal night of sleep showed a significant response in the prefrontal cortex and following TSD displayed a response of additional areas which included other prefrontal areas, bilateral inferior parietal lobule and superior parietal lobes. Increases in sleepiness also correlated with activation of two ventral prefrontal regions and a correlation between a greater activation in bilateral parietal lobes (which include language areas) and lower levels of impairment on free recall were also found following TSD. In the arithmetic task normal sleep showed the expected activation in the bilateral prefrontal and parietal working memory regions but following TSD only showed activation in the left superior parietal lobe and the left premotor cortex in response, with no new areas to compensate (as was found in verbal learning). Increased sleepiness was also correlated with activation in a ventral prefrontal region, but only one region. The divided attention task combined both verbal learning and the arithmetic task. fMRI indicated that cerebral response after TSD is similar to that of the verbal learning task (specifically the right prefrontal cortex, bilateral parietal lobes, and cingulate gyrus showing the strongest response). The implication of this finding is that additional brain regions activated after both verbal learning and divided attention tasks following TSD represent a cerebral compensatory response to lacking sleep. For example, there is a decline in response of the left temporal lobes during both tasks which is involved in different learning tasks during a rested state but involvement of the left inferior parietal lobe in short-term verbal memory storage following TSD suggests that this region might compensate. No new areas for the arithmetic task may suggest that it relies heavily on working memory so compensation is not possible, in comparison to tasks such as verbal learning which rely less on working memory.\n\nSlow wave sleep occurs during stages 3 and 4 of the sleep process. Slow wave activity is increased by as much as 25% after implicit learning and time spent in this sleep stage has been shown to improve performance of the implicitly learned task after sleep.\n\nResearchers used rats in order to investigate the effects of novel tactile objects on the long-term evolution of the major rodent forebrain loops essential in species-specific behaviours, including such structures as the hippocampus, putamen, neocortex and the thalamus. The rats were monitored but not bothered for 48–96 hours, allowing normal wake-sleep cycles to occur. At some point four novel tactile objects were placed in the four corners of the rat's cage. They were all very different from one another and they were there for a total of one hour. The brain activity during this hour was used as a baseline or template to compare. Data analysis implied that the neural assemblies during SWS correlated significantly more with templates than during waking hours or REM sleep. As well, these post-learning, post-SWS reverberations lasted 48 hours, much longer than the duration of novel object learning (one hour), indicating long term potentiation. Further analysis on a neuron to neuron base showed no subset of neurons (brain structure) to be responsible for the reverberations or anti-reverberations (patterns of activity significantly more dissimilar than novel stimulation templates). Another difference noticed was the highest correlation peaks in SWS corresponded with the lowest rate of neuron firing in the forebrain, opposite that of REM sleep and waking where rate of firing is the highest. It is hypothesized that this is due to interference from other incoming stimuli during waking periods. In SWS there is no incoming stimuli so the novel experience can be replayed, without interruption.\n\nA study by Peigneux \"et al.\", (2004) noted that the firing sequences in the hippocampal ensembles during spatial learning are also active during sleep, which shows that post training sleep has a role in processing spatial memories. This study was done to prove that the same hippocampal areas are activated in humans during route learning in a virtual town, and are reactivated during subsequent slow wave sleep (SWS). In order to monitor this activation, experimenters used PET scans and fMRI to use cerebral blood flow as a marker of synaptic activity. The finding noted that the amount of hippocampal activation during slow-wave sleep positively correlated to the improvement on the virtual tour task the following day, which indicates that hippocampal activity during sleep correlates with the improvement in memory performance. These findings prove that learning-dependent modulation in hippocampal activity while sleeping shows processing of the previously learned episodic and spatial memory traces. This modulation of the hippocampus leads to plastic changes in the brain and ultimately an improvement in performance. The results of this study showed that spatial memory traces are processed in humans while they are in NREM sleep. It showed a reaction of the hippocampal formation during SWS, after a declarative spatial memory task. Experimenters also found, that in humans, there is experience dependent modulations of activity during NREM sleep in the hippocampal regions, but not during REM sleep after learning. The evidence from this study was substantial to its hypothesis that the information learned while awake, is altered, and strengthened while humans are sleeping.\n\nIn this study, two groups of participants took part in a two night counterbalanced study. Two tasks were learned by all participants between 10:00-10:30pm. The declarative task was a paired-associate word list of 40 German semantically related word pairs. The non-declarative task was a mirror-tracing task. At 11:00pm all participants were put on a two-hour infusion of either physostigmine or a placebo. Physostigmine is an acetylcholinesterase inhibitor; it is a drug that inhibits the breakdown of the inhibitory neurotransmitter Acetylcholine, thereby allowing it to remain active longer in the synapses. The sleep group was put to bed while the other group stayed awake. Testing of both tasks took place at 2:45am, 30 minutes after the sleep group had been woken up; a sleep which had been rich in slow-wave sleep (SWS). Results showed that the increased ACh negatively affected recall memory (declarative task), in the sleep condition compared to participants given the placebo. Specifically, recall after sleep for the placebo group showed an increase of 5.2 ± 0.8 words compared to an increase of only 2.1 ± 0.6 words when participants were given the acetylcholinesterase inhibitor. Conversely, neither speed nor accuracy declined in the non-declarative mirror task when participants were given physostigmine, and neither task performance was affected in the wake groups when physotigmine was administered. This suggests that the purpose of ACh suppression during SWS allows for hippocampus –dependent declarative memory consolidation; high levels of ACh during SWS blocks memory replay on a hippocampal level.\n\nSleep spindles are short and intense bursts of neurons firing in sync, occurring in the thalamo-cortical networks. These peak late in the night and are defining characteristics of stage two sleep. Sleep spindles are thought to aid in information consolidating during sleep and have been shown to increase after training on a motor task.\n\nA study, using 49 rats indicated the increase of sleep spindles during slow-wave sleep following learning. It gave evidence to the increase of spindle frequency during non-REM sleep following paired associate of motor-skill learning tasks. Using an EEG, sleep spindles were detected and showed to be present only during slow-wave sleep. Beginning with a preliminary study, rats underwent six hours of monitored sleep, after a period of learning. Results showed that during the first hour following learning, there was the most evident effect on learning-modulated sleep spindle density. However, this increase in spindle density was not dependent on the training condition. In other words, there was an increase in spindles regardless of how the rats were trained. EEG patterns showed a significant difference in the density of sleep spindles compared to the density of a control group of rats, who did not undergo any training before their sleep spindles were measured. This effect of increased spindle density only lasted for the first hour into sleep following training, and then disappeared within the second hour into sleep.\n\nIn a study by Fischer and Born, 2009, previous knowledge of monetary reward and post-training sleep are proven to be significant predictors of overall finger sequence tapping performance. Subjects were presented with two different finger sequence tasks that would have to be replicated at a later time. The subjects were told that there would be a reward offered for improvement upon a specific finger tapping sequence task. A control group was not given any knowledge of a reward. The subjects were separated further by allowing a sleep period between initial training and final testing for one group while another group faced a wake retention interval. It was concluded that the group that received both information about reward as well as being able to sleep displayed the highest increase in performance on both finger tapping sequences. Knowledge of reward without sleep and sleep without knowledge of reward were both significant contributors to improved performance. In all cases sleep was determined to have an advantageous effect on overall performance when compared to groups that underwent a twelve-hour wake retention period.\n\nNon-declarative memory is memory gained from previous experiences that is unconsciously applied to everyday scenarios. Non-declarative memory is essential for the performance of learned skills and habits, for example, running or cooking a favourite meal. There are three types of non-declarative memories: implicit memory (unconscious memory, priming), instrumental memory (classical conditioning), and procedural memory (automatic skill memory).\n\nExtracellular signal-related kinases, also known as classical MAP kinase, are a group of protein kinases located in neurons. These proteins are activated or deactivated by phosphorylation (adding of a phosphate group using ATP), in response to neurotransmitters and growth factors. This can result in subsequent protein to protein interactions and signal transductions (neurotransmitters or hormones transmit to cells), which ultimately controls all cellular processes including gene transcription and cell cycles (important in learning and memory). A study tested four groups of rats in the Morris Water Maze, two groups in the spatial task (hidden platform) and two groups in the non-spatial task (visible platform.) The effects of six hours of total sleep deprivation (TSD) were assessed for the experimental group (one spatial group, one non-spatial group) in both tasks. Six hours after the TSD period (or sleep period for controls), the groups of rats were trained on either task then tested 24 hours later. In addition, the levels of total ERK phosphorylation (ERK 1 and ERK 2), protein phosphate 1 (PP1), and MAPK phosphatase 2 (latter two both involved in dephosphorylation) were assessed by decapitating four other groups of mice, (two sleep deprived and two non-sleep deprived), and removing their hippocampuses after the six hours of TSD, or two hours after TSD (eight hours total). Results showed that TSD did not impair learning of the spatial task, but it did impair memory. With regards to the non-spatial task, learning again was no different in the TSD; however, memory in the TSD group was actually slightly better, although not quite significantly. Analysis of the hippocampus showed that TSD significantly decreased the levels of total ERK phosphorylation by about 30%. TSD did not affect proteins in the cortex which indicates that the decreases in ERK levels were due to impaired signal transduction in the hippocampus. In addition, neither PP1 or MAPK phosphatase 2 levels were increased suggesting that the decreases in ERK were not due to dephosphorylation but instead a result of TSD. Therefore, it is proposed that TSD has aversive effects on the cellular processes (ERK: gene transcription etc.), underlying sleep-dependent memory plasticity.\n\nREM sleep is known for its vivid creations and similarity to the bioelectric outputs of a waking person. This stage of sleep is characterized by muscle atonia, fast but low voltage EEG and, as the name suggests, rapid eye movement. It is difficult to attribute memory gains to a single stage of sleep when it may be the entire sleep cycle that is responsible for memory consolidation. Recent research conducted by Datta et al. used an avoidance task followed by a post-training REM sleep period to examine changes in P waves affecting reprocessing of recently acquired stimuli. It was found that not only were the P waves increased during post-training sleep but also the density of the waves. These findings may imply that P waves during REM sleep may help to activate critical forebrain and cortical structures dealing with memory consolidation. In a Hennevin et al. study, 1989, the mesencephalic reticular formation (MRF) was given light electrical stimulation, during REM sleep, which is known to have an advantageous effect for learning when applied after training. The rats in the experiment were trained to run a maze in search of a food reward. One group of rats was given non-awakening MRF electrical stimulations after each of their maze trials compared to a control group which did not receive any electrical stimulation. It was noticed that the stimulated rats performed significantly better in respect to error reduction. These findings imply that dynamic memory processes occur both during training as well as during post-training sleep. Another study by Hennevin et al. (1998) conditioned rats to fear a noise that is associated with a subsequent foot shock. The interesting part of the experiment is that fear responding to the noise (measured in the amygdala) was observed when the noise was presented during REM sleep. This was compared to a group of pseudo-conditioned rats who did not display the same amygdalar activation during post-training sleep. This would suggest that neural responding to previously salient stimuli is maintained even during REM sleep. There is no shortage of research conducted on the effects that REM sleep has on the working brain but consistency in the findings is what plagues recent research. There is no guarantee as to what functions REM sleep may perform for our bodies and brains but modern research is always expanding and assimilating new ideas to further our understanding of such processes.\n\nIn animals, the appearance of ponto-geniculo-occipital waves (PGO waves) is related to that of the bioelectric outputs of rapid eye movements. These waves are most clearly seen during the transition from non-REM to REM sleep. Although these phasic waves are observed in many portions of the animal brain, they are most noticeable in the pons, lateral geniculate bodies, and the occipital cortex. Peigneux et al., 2006, reported that the lateral geniculate nucleus and occipital cortex display higher levels of activity during REM sleep than during wakefulness. This would add to the theory that activation in these areas is similar to PGO wave activation in animals. Pontine waves are commonly seen in animals as a mechanism to help facilitate learning and memory consolidation. An improvement on task performance was seen to be a result of increased P waves between REM sleep sessions. In a study using post learning REM sleep deprivation the effects of stimulating the P wave generator (located in the pontine tegmentum) of a rat were observed. Two groups of rats underwent an avoidance learning task and then allowed a sleep period while another group of rats were deprived sleep. When comparing the two groups the sleep deprived rats showed a significant deficit in learning from having not undergone REM sleep. In another rat group, the P wave generator was stimulated using a carbachol injection and the rats then underwent a sleep deprivation stage. When these rats were again tested on their learning it was shown that activation of the P wave generator during sleep deprivation resulted in normal learning being achieved. This would point to the fact that the activation of P waves, even without REM sleep, was enough to enhance the memory processes that would not normally have happened.\n\nFaces are an important part of one's social life. To be able to recognize, respond and act towards a person requires unconscious memory encoding and retrieval processes. Facial stimuli are processed in the fusiform gyrus (occipito-temporal brain area) and this processing is an implicit function representing a typical form of implicit memory. REM sleep has been seen to be more beneficial to implicit visuospatial memory processes, rather than slow-wave sleep which is crucial for explicit memory consolidation. REM sleep is known for its visual experiences, which may often include detailed depictions of the human countenance. A recognition task was used to gauge familiarity with a previously shown sequence of faces after a subsequent period of REM sleep. It was seen that the fusiform gyrus was active during training, the REM sleep period, and the recognition task as well. It is hypothesized that brain mechanisms during REM sleep, as well as pure repetition priming, can account for the implicit recognition of the previously shown faces.\n\nPrevious research has shown REM sleep to reactivate cortical neural assemblies post-training on a serial reaction time task (SRT), in other words REM sleep replays the processing that occurred while one learnt an implicit task in the previous waking hours. However, control subjects did not complete a SRT task, thus researchers could not assume the reactivation of certain networks to be a result of the implicitly learned sequence/grammar as it could simply be due to elementary visuomotor processing which was obtained in both groups. To answer this question the experiment was redone and another group was added who also took part in the SRT task. They experienced no sequence to the SRT task (random group), whereas the experimental group did experience a sequence (probabilistic group), although without conscious awareness. Results of PET scans indicate that bilateral cuneus were significantly more activated during SRT practice as well as post-training REM sleep in the Probabilistic group than the Random group. In addition, this activation was significantly increased during REM sleep versus the SRT task. This suggests that specific brain regions are specifically engaged in the post-processing of sequential information. This is further supported by the fact that regional CBF (rCBF) during post-training REM sleep are modulated by the level of high-order, but not low-order learning obtained prior to sleep. Therefore, brain regions that take part in a learning process are modulated by both the sequential structure of the learned material (increased activation in cuneus), and the amount of high-order learning (rCBF).\n\nThe effects of REM sleep deprivation (RSD) on neurotrophic factors, specifically nerve growth factor (NGF) and brain-derived neurotrophic factor (BDNF), were assessed in 2000 by Sie et el. Neurotrophins are proteins found in the brain and periphery that aid in the survival, functioning and generation of neurons; this is an important element in the synaptic plasticity process, the underlying neurochemical foundation in forming memories. Sei et al., inserted electrodes into the skulls of seven pairs of rats to measure Electroencephalogram (EEG), and inserted wire into the neck muscles of the rats to measure Electromyogram (EMG), a technique used to measure the amount of muscle activity. Half the rats experienced a six-hour REM sleep deprivation period, while the other half experienced a six-hour sleep period, containing all sleep cycles. Results showed that in the rats in the REM sleep deprivation group showed decreased level of brain-derived neurotrophic factor in the cerebellum (coordination, motor learning) and brainstem (sensory and motor ascending pathway), conversely the hippocampus (long-term memory, spatial navigation), showed decreases in nerve growth factor levels. BDNF protein has been shown to be necessary for procedural learning (form of non-declarative memory). Since procedural learning has also exhibited consolidation and enhancement under REM sleep, it is proposed that the impairment of procedural learning tasks is due to the lack of BDNF proteins in the cerebellum and brainstem during RSD. In regards to NGF, the basal forebrain (production and distribution of AcH in the brain), more specifically the medial septal area, sends cholinergic (excitatory in hippocampus) and GABAinergic (inhibitory) neurotransmitters through fibres to the hippocampus target cells. These target cells then secrete NGF which plays a key role in the physiological state of the hippocampus and its functions. It has been noted that REM sleep increases the secretion of NGF, therefore it has been proposed that during RSD cholinergic activity decreases leading to a decrease in NGF and impairment in procedural learning.\nWalker and Stickgold hypothesized that after initial memory acquisition, sleep reorganizes memory representation at a macro-brain systems level. Their experiment consisted of two groups; the night-sleep group was taught a motor sequence block tapping task at night, put to sleep and then retested 12 hours later. The day-wake group was taught the same task in the morning and tested 12 hours later with no intervening sleep. FMRI was used to measure brain activity during retest. Results indicated significantly fewer errors/sequence in the night-sleep group compared to the day wake group. FMRI output for the night-sleep group indicated increased activation in the right primary motor cortex/M1/Prefrontal Gyrus (contra lateral to the hand they were block tapping with), right anterior medial prefrontal lobe, right hippocampus (long-term memory, spatial memory), right ventral striatum (olfactory tubercle, nucleus accumbens), as well as regions of the cerebellum (lobules V1, V11). In the day-wake group, fMRI showed \"decreased\" signal activation bilaterally in the parietal cortices (integrates multiple modalities), in addition to the left insular cortex (regulation of homeostasis), left temporal pole (most anterior of temporal cortex), and the left inferior fronto-polar cortex. Previous investigations have shown that signal increases indicate brain plasticity. The increased signal activity seen in M1 after sleep corresponds to increased activity in this area seen during practice; however, an individual must practice for longer periods than they would have to sleep in order to obtain the same level of M1 signal increases. Therefore, it is suggested that sleep enhances the cortical representation of motor tasks by brain system expansion, as seen by increased signal activity.\n\nConsidered to be a mental workspace enabling temporary storage and retrieval of information, working memory is crucial to problem-solving and analysis of different situations. Working memory capacity is a measure of the number of mental processing functions one is able to perform consecutively. Increases in one's working memory capacity can be accomplished with a strategy known as chunking. Aritake et al. conducted a finger sequence tapping experiment in which the subjects were shown coloured dots in sequence on a monitor corresponding to buttons on their keyboard. When a colour was shown the subject had to react by pressing the right colour on the keyboard. The subjects were separated into three groups. Group one continually trained with no periods of sleep. Group two was trained and retested over ten hours of wakefulness followed by eight hours of sleep and final testing. The third group was trained at ten pm, followed by an eight-hour sleep. This group was then tested the following morning and again later in the same day. Results displayed that wakefulness was an insignificant predictor of performance improvement, unless followed by a period of sleep. Groups that were allowed a post training sleep period, regardless of its time in reference to training, was beneficial to the improvement on the finger tapping sequences. The initial working memory capacity of the groups averaged three to four units. In groups two and three the working memory capacity was increased to an average of 5–6 units. It was proposed that sleep-dependent improvements may contribute to overall improvement in working memory capacity leading to improved fluid intelligence.\n\nSleep deprivation, whether it is total sleep deprivation or partial sleep deprivation, can impair working memory in measures of memory, speed of cognitive processing, attention and task switching. Casement et al. found that when subjects were asked to recognize digits displayed on a screen by typing them on a keypad, the working memory speed of subjects whose sleep was restricted to four hours a night (approximately 50% of their normal sleep amount) were 58% slower than control groups who were allowed their full eight hours of sleep.\n\nThe brain is an ever-changing, plastic, model of information sharing and processing. In order for the brain to incorporate new experiences into a refined schema it has to undergo specific modifications to consolidate and assimilate all new information. Synaptic plasticity can be described as the changing in strength between two related neurons. Neuroplasticity is most clearly seen in the instances of REM sleep deprivation during brain maturation. Regional brain measurements in neo-natal REM sleep deprived rats displayed a significant size reduction in areas such as the cerebral cortex and the brain stem. The rats were deprived during critical periods after birth and thus anatomical size reduction is observed. Using a pursuit task (used to test visuomotor capabilities) in combination with an fMRI, Maquet et al., 2003, found that increases in activation were seen in the supplementary eye field and right dentate nucleus of subjects who were allowed to sleep as compared to sleep deprived individuals. The right superior temporal sulcus was also noticed to have higher activation levels. When functional connectivity was analyzed it was found that the dentate nucleus was more closely involved with the functions of the superior temporal sulcus. The results suggest that performance on the pursuit task relies on the subject's ability to comprehend appropriate movement patterns in order for recreation of the optimal movements. Sleep deprivation was found to interrupt the slow processes that lead to learning of this procedural skill and alter connectivity changes that would have normally been seen after a night of rest. Neuroplasticity has been thoroughly researched over the past few decades and results have shown that significant changes that occur in our cortical processing areas have the power to modulate neuronal firing to both new and previously experienced stimuli.\n\nThe changes in quantity of a certain neurotransmitter as well as how the post-synaptic terminal responds to this change are underlying mechanisms of brain plasticity. During sleep there are remarkable changes in modulatory neurotransmitters throughout the brain.Acetylcholine is an excitatory neurotransmitter that is seen to increase to near waking levels during REM sleep while compared to lower levels during slow-wave sleep. Evidence has shown that functioning of the hippocampus dependent memory system (episodic memory and autobiographical memory) is directly affected by cholinergic changes throughout the wake-sleep cycle. High levels of ACh would promote information attained during wakefulness to be stored in the hippocampus. This is accomplished by suppressing previous excitatory connections while facilitating encoding without interference from previously stored information. During NREM sleep, and especially slow-wave sleep, low levels of Ach would cause the release of this suppression and allow for spontaneous recovery of hippocampal neurons resulting in the facilitation of memory consolidation.\n\nRecently, approximately one hundred genes whose brain expression is increased during periods of sleep have been found. A similar number of genes were found to promote gene expression during wakefulness. These sets of genes are related to different functional groups which may promote different cellular processes. The genes expressed during wakefulness may perform numerous duties including energy allocation, synaptic excitatory neurotransmission, high transcriptional activity and synaptic potentiation in learning of new information. There was a sleep related increase in processes that involve the synthesis and maintenance of the synapse. Such processes include membrane trafficking, synaptic vesicle recycling, myelin structural protein formation, and cholesterol and protein synthesis. In a different study it was found that there was a sleep related increase in calmodulin-dependent protein kinase IV that has been specifically involved in synaptic depression and in the consolidation of long-term memory. These findings encourage an association between sleep and different aspects of neural plasticity.\n\nThe impact of daytime naps was looked at by Walker and Stickgold (2005). The experimental group was given a 60-90 minute afternoon nap (one full cycle), after a motor skills task learned that morning, while the control group received no nap. The nap group improved 16% when tested after their nap, while the no-nap group made no significant improvements. However, it seemed to all even out after that same nights sleep; the no-nap group improved 24% and the nap group improved only 7% more for a total of 23%, virtually identical. With regards to motor skills learning, naps seem to only speed up skill enhancement, not increase the amount of enhancement.\n\nMuch like motor skills learning, verbal skills learning increased after a daytime nap period. Researchers Mednick and colleagues have shown that if a visual skills task [find task] is taught in the morning and repeatedly tested throughout the day, individuals will actually become worse at the task. The individuals that were allowed a 30-60 minute nap seemed to gain stabilization of the skill as no deterioration occurred. If allowed a 60-90 minute nap (REM sleep and slow-wave sleep), individuals displayed enhancement. Unlike the motor task, enhancement was not suppressed during the nocturnal sleep if the individual had napped earlier. In the situation of visual skill learning, naps have been shown to prevent wakeful deterioration and even enhance learning above and beyond enhancement occurring in nocturnal sleep.\n\nShift workers who work throughout the night have been known to have far more accidents as opposed to daytime workers. This can be attributed to several factors, including fewer staff and fatigue; however, part of the problem may be the workers poor working memory and poor performance skills due to poor memory consolidation. Both implicitly learned tasks and explicitly learned tasks improve by roughly 20% after a full nights sleep. Without an adequate nights sleep between learning a new task and performance of that task, performance fails to improve. Shift workers who are not given an adequate amount of sleep, particularly in the NREM stage, between learning and performance of a task will not perform as well as workers who maintain a standard sleep routine.\n\nSleep often becomes deregulated in the elderly and can lead to or exacerbate preexisting memory decline.\n\nThe positive correlation between sleep and memory breaks down with aging. In general, older adults suffer from decreased sleep efficiency. The amount of time and density of REM sleep and SWS decreases with age. Consequently, it is common that the elderly receive no increase in memory after a period of rest.\n\nTo combat this, donepezil has been tested in healthy elderly patients where it was shown to increase time spent in REM sleep and improve next day memory recall.\n\nPatients with Alzheimer's disease experience more sleep disruption than the healthy elderly. Studies have shown that in patients with Alzheimer's disease, there is a decrease in fast spindles. It has also been reported that spindle density the night before a memory test correlate positively with accuracy on an immediate recall task. A positive correlation between time spent in SWS and next day autobiographical memory recall has also been reported in Alzheimer's patients.\n\n", "id": "26685741", "title": "Sleep and memory"}
{"url": "https://en.wikipedia.org/wiki?curid=26685721", "text": "Methods used to study memory\n\nThe study of memory incorporates research methodologies from neuropsychology, human development and animal testing using a wide range of species. The complex phenomenon of memory is explored by combining evidence from many areas of research. New technologies, experimental methods and animal experimentation have led to an increased understanding of the workings of memory.\n\nIt is usually desirable to study memory in humans because we have the ability to subjectively describe experiences, and have the intellect to perform complex and indirect tests of memory. Lesion studies allow us to reduce the neural mechanisms of memory, and results from finely constructed psychological tests can help us make inferences about how memory works. Neuropsychologists attempt to show that specific behavioural deficits are associated with specific sites of brain damage. The famous case of HM, a man who had both his medial temporal lobes removed resulting in profound amnesia, illustrates how brain damage can tell us a lot about the inner workings of memory. One of the fundamental problems with studying human patients who have already acquired brain damage is the lack of experimental control. Comparisons usually have to be made between individuals; exact lesion location and individual differences cannot be controlled for.\n\nHermann Ebbinghaus began the scientific study of human memory with this treatise \"On Memory\" is 1885. Ebbinghaus experimented on himself by testing his own ability to memorize lists of randomly arranged syllables presented at regular pace of 2.5 syllables per second. He would record how long it took him to memorize a list of syllables and also how quickly he would lose the memorization. With this data, he traced learning and forgetting curves. Ebbinghaus also collected data on his ability to memorize at different times of the day and under different conditions. His work later influenced G.E. Müller who continued the tradition of lists of items to conduct memory experiments on human subjects and using behavioral data to develop models of memory. The two most common types of memory studied using these methods are recognition and recall.\n\nRecognition memory is the ability to judge whether or not the cued item was previously presented on the list usually with a yes or no response. This memory is akin to the type of memory used for police line-ups. The particular task described used to be called \"item recognition\". Scientists study the rates of hits (correct \"yes\" responses) related to the rates of false alarms (incorrect \"yes\" responses) with statistical analysis to develop theories on how the recognition judgement is stored and retrieved. These theories are then developed into memory models.\n\nRecall memory is the ability to retrieve stored information either by being cued by a particular associated item or without an associated cue. The first is called cued recall and the second is called free recall. In cued recall the participant studies a list of paired items and then is presented one half of those pairs and must recall the associated other half. A common additional task is to have the participant learn a new set of associations with the cued items and study the amount of interference from prior association occurs. These experiments help to develop theories on how we learn associations and what conditions affect that learning.\nFree recall does not use item pairs. Instead participants study a list of items and then recall that list in the order that they retrieve the items from memory. In these experiments the data is drawn from the order in which items are recalled and the inter-response times. This data is used to develop models of memory storage and retrieval.\n\nHumans are extremely dependent on memory for survival as we are dependent on our ability to identify and remember a wide range of material in order to learn and function, which is why the capability for memory is developed at a very young age. Memory in children is displayed in simpler ways than in adults due to lack of verbal communication and mental capabilities, and therefore the testing methods are similar but are modified to suit age specific abilities.\n\nObservational and experimental methods are used to test children’s memory by documenting either their physical actions, emotional (facial) response or attention/ focus, depending on the age (ability) of the child and the type of task provided. An older child can provide answers to both verbal and non-verbal tests to more complex tasks in a more detailed and accurate way and therefore produce more direct data. The change of methods according to age is necessary for several reasons particularly because memory ability expands at certain stages of childhood and can be influenced by biological advances and environmental experience.\n\nFrom birth, children use observational (imitation) and auditory learning styles with increasing ability to remember complex sequences and concepts. The rapid growth in ability to retain information is partly due to the increase of myelin, which increases the speed of impulses between neurons, especially in the visual and auditory cortexes which become myelinated very early in the development processes. The amount of myelination has a direct effect on the speed of information-processing and in turn the speed and strength at which we are able to remember things. Understanding of contexts and relationships as well as memory strategies such as chunking and learning of schemas can also influence the strength of memories and therefore must be considered when choosing a method of testing.\n\nA well known study used to show early signs of recall memory, examines 3 month old infants behavior with mobiles. The experiment entailed tying a string on a colourful mobile to the infant’s foot, so that kicking would cause the mobile to move, pleasing the baby. After the initial training, it was shown that a week later the infants kicked in order to produce movement. Two weeks later the infant resumed the kicking behavior after a short reminder session where they watched the mobile moving (unattached to their foot). This experiment showed a recognition of the mobile, as well as recall (after one week), and cued recall (after 2 weeks). Recognition and recall are two essential aspects of memory; these are particularly useful in children since verbal reporting of memory may not be available yet or as reliable when testing very young participants. At the age of approximately 9 months some infants are able to reproduce some simple actions they observe up to 24 hours after witnessing them. The reproduction of behaviors such as choosing one object over another or repeatedly placing an object in a certain spot is a type of situational memory test used to identify the child's level of memory capability. Deferred imitation, the ability to reproduce behaviour without cuing, can be seen and tested near the age of 18– 24 months. Children become able to replicate more complex events with greater detail, from memory.\n\nJean Piaget, a child development psychologist, conducted a study testing the cognitive and memory abilities of children around 2 years of age. These tests were conducted using objects presented to the child followed by their removal from sight. This causes very young infants to believe the object no longer exists. At approximately 8–12 months, children will look for the missing object and this display shows memory, as well as a comprehension that it still exists when it is not directly seen. This led to the theory of object permanence which demonstrates the stage at which memory and cognitive development have reached the level of mental representation.\n\nA child as early as 7 days old can shows signs of facial expression imitation such as tongue or lip protrusion and opening of their mouth. There is some debate as to whether this is a voluntary or reflexive action though the ability to imitate demonstrates the infants ability to encode the image and imitate it. Evidence of facial memory over longer delays can be seen in children as early as 2–3 weeks old. Changes in behavior such as crying less and smiling more shows evidence for recognition of a familiar face.\n\nRecognition can also be shown with habituation, the process of attending a familiar stimuli less in preference for a new one. This can be seen as early as 5 months of age in several forms including auditory and visual identification. In a study of 8-10 month infants, both familiar objects/faces and newly presented ones were introduced at different intervals and time delays. Looking time and first looks were recorded, and the results indicated habituation and memory recall.\n\nAccuracy of memory is an important factor when studying memory in children as it has been demonstrated that children’s memory more susceptible to suggesting and implanting of false memories than adults. In a study on preschoolers, using a questionnaire method in a yes/no and multiple choice format, the result of forcing an answer in response to a controlled situation differed according to question style. The children showed a preference to saying ‘yes’ to ‘no’ and showed an equal preference for the multiple choice (when neither were correct), neither are an ideal test for memory accuracy in children. The Criterion-Based Content Analysis (CBCA) is designed to decipher truthful from false memories in children and is a part of the Statement Validity Analysis (SVA) which consists of structured interviews, systematic analysis of verbal statements and statement validity checklist. This is a comprehensive protocol that tests the reliability of a child witness’s memory of an event or situation which relies on the assumption that false memories are of weaker quality then truthful and accurate memories.\n\nRecognition memory in a legal setting is also different in children than in adults. In a meta-analysis study a line-up style test was used for a child witness to identify a culprit among other suspects. The results show that children over the age of 5 were able to identify a culprit (when the culprit was present) at a comparable rate to adults but children up to age 14 produced far greater number of false identifications when the culprit was not present. It was speculated by Pozzulo & Lindsay that false positives were caused by an inability to produce correct absolute judgments (recall) while relative judgments (recognition) are more successful.\n\nCognitive neuroscience aims to reduce cognition to its neural basis using new technologies such as fMRI, repetitive transcranial magnetic stimulation (rTMS) and Magnetoencephalography (MEG) as well as older methods such as Positron emission tomography (PET) and Electroencephalography (EEG) studies. Due to the correlational designs used in fMRI, many scientists have coined this up and coming field as the new phrenology in the sense that techniques such as fMRI rely heavily on complex statistics. Type 1 errors can lead scientists to draw premature and incorrect causal relationships if improper designs are used.\n\nHuman subject research must be carefully designed as to not have any adverse effects on the subject, and not impose on their rights as a human being. Neurological procedures that intentionally lesion the brain are illegal, and therefore those who have already obtained brain damage must be studied. The study of brain damaged patients has its drawbacks, however case studies of brain injured patients has greatly improved our understanding of the neural basis of memory.\n\nAll subjects (or their legal representative) must have a clear understanding of the experimental procedure, including any potential dangers to themself, and be in a right state of mind to express consent to the researcher before any tests can be started.\n\nThe American Psychological Association uses a strict ethical code concerning research in psychology.\n\nA major complication that is raised in memory research is its fallible nature in humans. Having the ability to recall memories does not necessarily mean they are accurate. Our ability to store and process what is going on around us relies on memory being a constructive, fallible process. The technologies explained above may show areas of activation associated with certain behaviors, but without any idea of lesion location, it is difficult to pinpoint exactly what part of the brain relates to which behavioral deficits observed. Neuropsychologists have created various tasks designed to assess specific types of memory so that inferences about lesion location can be drawn from poor performances on these tests. Neuropsychological tests can aid us in understanding specific types of memory associated with specific sites of brain damage.\n\nAn assessment of visual-spatial memory involves mimicking a researcher as he/she taps nine identical spatially separated blocks. The sequence starts out simple, usually using two blocks, but becomes more complex until the subject's performance suffers. This number is known as the Corsi Span, and averages about 5 for normal human subjects. An fMRI study involving subjects undergoing this test revealed that while the sequence length increases, general brain activity remains the same. So while humans may show encoding difficulty, this is not related to overall brain activation. Whether able to perform the task well or not the ventrolateral prefrontal cortex is highly involved. Corsi blocks tasks with a normal forward order requires support from the visuospatial skech pad, but not from the phonological loop. When the sequence to be recalled becomes longer than three or four items, central executive resources are used\n\nThe study of memory has greatly benefited from experimentation with animals. Current ethical guidelines state that using animals for scientific purposes is only acceptable when the harm (physical or psychological) done to animals is outweighed by the benefits of the research. Keeping this in mind, we can use research techniques on animals that would not necessarily be performed on humans.\n\nOur understanding of memory has benefited greatly from animal research. Animals brains can be selectively lesioned using surgical, or neurotoxic methods and be assessed before and after the experimental manipulation. Groundbreaking new technology has allowed for genetic manipulation and the creation of knockout mice. Scientists genetically engineer these mice to lack functionally or behaviourally important alleles or have missing or altered gene sequences. Careful observation of behaviors and phenotypes may help uncover the neural substrates of memory and prove to be an essential tool for the study of gene-behaviour interactions.\n\nAnimals can be selectively bred for certain behavioural characteristics, such as having strong maze-solving skills. Although the genetic underpinnings of observable phenotypes are extremely difficult to show, behavioural characteristics can be selectively bred. The difficulty lies in the subjective determination of the phenotype. How can one show that an animal has a good memory? There are many confounds when breeding for behaviours, however if animals can be selected for memory and learning ability, maybe we can learn something about how genes contribute to memory systems.\n\nFunctional magnetic resonance imaging (fMRI) has intriguing implications for the study of memory in humans, however it can also be used in animal models. fMRI can be used to assess brain functionality in monkeys in the context of a variety of behavioral tasks. Structural MRI can be used to examine the extent and location of brain lesions, so that behavioral abnormalities observed can be directly linked to specific brain structures. High-resolution fMRI can help locate and assess the functionality of large neural networks so that these regions can be further studied using more traditional electrophysiological recording devices.\n\nSingle-unit recording directly measures action potentials and can be used in a variety of animals. This technique employed in the macaque monkey led to the discovery of the mirror neuron which has major implications for learning, memory and the perception of self. Single cell recordings can be used to record the activity of single neurons while animals perform tasks related to memory.\n\nCortical cooling is a relatively new technique that allows for a temporary disruption of function in a chosen area of the brain. Due to the use of large, surgically implanted devices this technique is generally used on primates, however the use of small cryotip implants has implications for use in rats. This exciting new technology allows for reversible lesions, which removes variations between animals so that better causal relationships can be shown. Animals can act as their own controls and this is ideal for any neuroscientific research.\n\nThe neurotoxic lesion technique has proven to be essential for studying the neural basis of behavior and memory in animals. Neurotoxins can be used to selectively damage very specific nerve tracts in the brain. Pertaining to the study of memory, areas of interest include the hippocampus, the rhinal cortex, the pre-frontal cortex, and the frontal cortex. The neurotoxic lesion technique uses neurotoxins such as ibotenic acid to selectively disrupt or kill specific neural tracts in any of the areas described above. The animal is anaesthetized and immobilized so that stereotaxic instruments can be used to drill holes in specific locations of the skull. Scientists can then carefully administer microinjections of neurotoxins to damage only the neurons in specific brain areas. This technique allows for the selective lesioning of areas of interest and leaves surrounding supportive tissue unaffected.\n\nResearch animals are a good substitute for humans because similar principles are assumed to underlie basic mechanisms of brain function. In order to fully understand human memory, cross-species comparisons in neuropsychology are used so that multiple variables can be manipulated. The more experimental control we have, the easier it is to eliminate confounding variables, and better causal conclusions can be made.\n\nAlthough there is no single \"ideal\" animal model of a human, for each problem of interest there is an animal upon which it can be most conveniently studied. For example, the study of spatial memory has benefitted greatly from experimentation and observation of food caching birds. To study auditory learning and memory, songbirds can be used. To study more complex systems such as motor learning, object recognition, short term memory and working memory, often primates such as the macaque monkey are used because of their large brains and more sophisticated intelligence. Small rodents can be used to study aversive conditioning and emotional memory, and contextual/spatial memory. To reduce memory and learning to its genetic basis, mice can be genetically modified and studied. Generally animal studies depend on the principles of positive reinforcement, aversion techniques and Pavlovian conditioning. This type of research is extremely useful and has shed much light on learning and memory in humans.\n\nScientific reductionism has pushed our understanding of memory closer to the neural level. In order to broaden our understanding, we need to draw conclusions from converging evidence. Studying memory in animals such as birds, rodents, and primates is difficult because scientists can only study and quantify observable behaviors. Animal research relies on carefully constructed methodologies, and these are species specific.\n\nThe textbook \"Fundamentals of Human Neuropsychology\" by Kolb and Whishaw describes some designs used to study memory in the macaque monkey. Elizabeth Murray and her colleagues trained monkeys to reach through the bars of their cage after a brief delay in order to displace objects under which a reward may be located. During the brief delay the monkey had to use either object recognition memory, or contextual memory to remember where the reward was located. Object recognition is tested with a matching-to-sample task where the monkey had to remember visual characteristics of the object in order to obtain the reward. Alternatively, in the non-matching-to-sample design the monkey must remember the location of the previously seen object. The monkey must then use context and spatial memory in order to correctly displace an object in the same location as previous, in order to obtain the food reward. These two tasks can be used to differentiate between object recognition memory and contextual memory. Murray and her colleagues were able to show that hippocampal lesions impaired contextual memory whereas rhinal cortex lesions impaired object recognition memory. This experimental design allowed for the dissociation of two mutually exclusive brain regions devoted to specific types of memory.\n\nIn experiments with the macaque monkey, Earl Miller and his colleagues used the delayed matching to sample (DMS) task to assess working memory in monkeys. The monkey was required to fixate on a computer screen while coloured images were displayed serially for 0.5 seconds, and separated by a one-second delay. The first image shown was the sample, and the monkey was trained to pull a lever when the sample object was shown a second time. In this experiment single-cell recordings were taken from the prefrontal cortex, an area thought to be involved with working memory. In order to verify the location of the recording device, MRI and stereotaxic instruments were used. The ability to use single cell recordings solely for experimental purposes is exclusive to animal testing and has greatly increased our understanding of memory systems.\n\nComparisons of the neuroanatomy of caching and non-caching birds has shed light on the neural basis of spatial memory. David Sherry and his colleagues devised experiments using the displacement of landmarks, to show that caching birds rely on spatial memory and landmark cues to find their caches, and local changes in cache appearance had no effect on food retrieval. They were able to show that changes in neurogenesis are directly related to food storing behavior. Food caching behavior reaches a maximum in August, continues through the winter and declines in the spring. They showed that the amount of neurons added to the hippocampus was also at a max during August and winter, and declined into the spring. Using selective hippocampal lesions, cause-effect relationships were shown. Lesions disrupted cache retrieval and other spatial behavior, however had no effect on actual food caching. The hippocampus has been implicated as a key structure for spatial memory in humans, and studying behavior in food caching birds and other animals has been extremely influential on this view. Sherry and his colleagues were also able to show that food-caching birds have much larger hippocampus’ than non food-caching birds. Without techniques such as cell labeling, neuronal staining and careful post mortem analysis, these comparison studies would be impossible.\n\nSongbirds are excellent animal models for studying learning and auditory memory. These birds have highly developed vocal organs, which give them the ability to create diverse and elaborate birdsongs. Neurotoxic lesions can be used to study how specific brain structures are key for this type of learning and scientists can also manipulate the environment in which these birds are raised. In addition, with functional MRI the response to different auditory stimuli has been studied in vivo. In experiments with songbirds, using these two methodologies has led to very interesting discoveries about auditory learning and memory. Much like humans, birds have a critical period when they must be exposed to adult birdsong. The cognitive systems of vocal production and auditory recognition parallel those in humans, and experiments have shown that there are critical brain structures for these processes.\n\nRodents are small mammals capable of learning, displaying complex behaviors, and are relatively inexpensive to raise. They are an ideal animal to use for the study of memory. The assessment of learning and memory in rodents has been employed in scientific research for a long time, and there are many experimental methods used. Generally tests of spatial memory employ maze designs where the rodent must run a maze in order to receive a food reward. Learning can be shown when the rodent reduces its average number of errors or wrong turns. Aversive techniques such as the Morris Water Maze can also be used to study spatial memory. The rat is placed in murky water surrounded by sheer walls containing spatial cues. Learning is shown when the rat swims a more direct route to the obscured platform. Small rodents can also be easily conditioned using taste aversion or odor aversion techniques. Performing neurotoxic lesions in these conditioned rodents is an excellent way to study the neural basis of aversion learning and memory.\n\nPsychologists who work with animals assume that the things they learn can be applied to the human brain. Memory is a complex system that relies on interactions between many distinct parts of the brain. In order to fully understand memory, researchers must cumulate evidence from human, animal, and developmental research in order to make broad theories about how memory works. Intraspecies comparisons are key. Rats for example display extremely complex behaviours and most of the structures in their brain parallel those in humans. Due to their relatively simple organization, slugs can be useful for studying how neurons interconnect to produce observable behaviour. Fruit flies are useful for studying gene-behaviour interactions because many generations with genetic alterations can be quickly bred in the laboratory.\n", "id": "26685721", "title": "Methods used to study memory"}
{"url": "https://en.wikipedia.org/wiki?curid=26685627", "text": "Motivated forgetting\n\nMotivated forgetting is a theorized psychological behavior in which people may forget unwanted memories, either consciously or unconsciously. It is an example of defence mechanism, since these are unconscious or conscious coping techniques used to reduce anxiety arising from unacceptable or potentially harmful impulses thus it can be defence mechanism in some ways. Defence mechanisms are not to be confused with conscious coping strategies.\n\nThought suppression is a method in which people protect themselves by blocking the recall of these anxiety-arousing memories. For example, if something reminds a person of an unpleasant event, his or her mind may steer towards unrelated topics. This could induce forgetting without being generated by an intention to forget, making it a motivated action. There are two main classes of motivated forgetting: \"psychological repression\" is an unconscious act, while \"thought suppression\" is a conscious form of excluding thoughts and memories from awareness.\n\nNeurologist Jean-Martin Charcot was the first to do research into hysteria as a psychological disorder in the late 19th century. Sigmund Freud, Joseph Breuer, and Pierre Janet continued with the research that Charcot began on hysteria. These three psychologists determined that hysteria was an intense emotional reaction to some form of severe psychological disturbance, and they proposed that incest and other sexual traumas were the most likely cause of hysteria. The treatment that Freud, Breuer, and Pierre agreed upon was named the \"talking cure\" and was a method of encouraging patients to recover and discuss their painful memories. During this time, Janet created the term dissociation which is referred to as a lack of integration amongst various memories. He used dissociation to describe the way in which traumatizing memories are stored separately from other memories.\n\nThe publication of Freud's famous paper, \"The Aetiology of Hysteria\", in 1896 led to much controversy regarding the topic of these traumatic memories. Freud stated that neuroses were caused by repressed sexual memories, which suggested that incest and sexual abuse must be common throughout upper and middle class Europe. The psychological community did not accept Freud's ideas, and years passed without further research on the topic.\n\nIt was during World War I and World War II that interest in memory disturbances was piqued again. During this time, many cases of memory loss appeared among war veterans, especially those who had experienced shell shock. Hypnosis and drugs became popular for the treatment of hysteria during the war. The term post traumatic stress disorder (PTSD) was introduced upon the appearance of similar cases of memory disturbances from veterans of the Korean War. Forgetting, or the inability to recall a portion of a traumatic event, was considered a key factor for the diagnosis of PTSD.\n\nAnn Burgess and Lynda Holmstrom looked into trauma related memory loss in rape victims during the 1970s. This began a large outpouring of stories related to childhood sexual abuse. It took until 1980 to determine that memory loss due to all severe traumas was the same set of processes.\n\nThe idea of motivated forgetting began with the philosopher Friedrich Nietzsche in 1894. Nietzshe and Sigmund Freud had similar views on the idea of repression of memories as a form of self-preservation. Nietzsche wrote that man must forget in able to move forward. He stated that this process is active, in that we forget specific events as a defense mechanism.\n\nThe False Memory Syndrome Foundation (FMSF) was created in 1992 as a response to the large number of memories claimed to be recovered. The FMSF was created to oppose the idea that memories could be recovered using specific techniques; instead, its members believed that the \"memories\" were actually confabulations created through the inappropriate use of techniques such as hypnosis.\n\nThere are many theories which are related to the process of motivated forgetting.\n\nThe main theory, the \"motivated forgetting theory\", suggests that people forget things because they either do not want to remember them or for another particular reason. Painful and disturbing memories are made unconscious and very difficult to retrieve, but still remain in storage. Retrieval Suppression is one way in which we are able to stop the retrieval of unpleasant memories using cognitive control. This theory was tested by Anderson and Green using the Think/No-Think paradigm.\n\nThe \"decay theory\" is another theory of forgetting which refers to the loss of memory over time. When information enters memory, neurons are activated. These memories are retained as long as the neurons remain active. Activation can be maintained through rehearsal or frequent recall. If activation is not maintained, the memory trace fades and decays. This usually occurs in short term memory. The decay theory is a controversial topic amongst modern psychologists. Bahrick and Hall disagree with the decay theory. They have claimed that people can remember algebra they learnt from school even years later. A refresher course brought their skill back to a high standard relatively quick. These findings suggest that there may be more to the theory of trace decay in human memory.\n\nAnother theory of motivated forgetting is \"interference theory\", which posits that subsequent learning can interfere with and degrade a person's memories. This theory was tested by giving participants ten nonsense syllables. Some of the participants then slept after viewing the syllables, while the other participants carried on their day as usual. The results of this experiment showed that people who stayed awake had a poor recall of the syllables, while the sleeping participants remembered the syllables better. This could have occurred due to the fact that the sleeping subjects had no interference during the experiment, while the other subjects did. There are two types of interference; proactive interference and retroactive interference. Proactive interference occurs when you are unable to learn a new task due to the interference of an old task that has already been learned. Research has been done to show that students who study similar subjects at the same time often experience interference. Retroactive interference occurs when you forget a previously learnt task due to the learning of a new task.\n\nThe Gestalt theory of forgetting, created by Gestalt psychology, suggests that memories are forgotten through distortion. This is also called false memory syndrome. This theory states that when memories lack detail, other information is put in to make the memory a whole. This leads to the incorrect recall of memories.\n\nThe term recovered memory, also known in some cases as a false memory, refers to the theory that some memories can be repressed by an individual and then later recovered. Recovered memories are often used as evidence in a case where the defendant is accused of either sexual or some other form of child abuse, and recently recovered a repressed memory of the abuse. This has created much controversy, and as the use of this form of evidence rises in the courts, the question has arisen as to whether or not recovered memories actually exist. In an effort to determine the factuality of false memories, several laboratories have developed paradigms in order to test whether or not false repressed memories could be purposefully implanted within a subject. As a result, the verbal paradigm was developed. This paradigm dictates that if someone is presented a number of words associated with a single non-presented word, then they are likely to falsely remember that word as presented.\n\nSimilar to the verbal paradigm is fuzzy-trace theory, which dictates that one encodes two separate things about a memory: the actual information itself and the semantic information surrounding it (or the gist). If we are given a series of semantic information surrounding a false event, such as time and location, then we are more likely to falsely remember an event as occurring. Tied to that is Source Monitoring Theory, which, among other things, dictates that emotionally salient events tend to increase the power of the memory that forms from said event. Emotion also weakens our ability to remember the source from the event. Source monitoring is centralized to the anterior cingulate cortex.\n\nRepressed memory therapy has come under heavy criticism as it is said that it follows very similar techniques that are used to purposefully implant a memory in an adult. These include: asking questions on the gist of an event, creating imagery about said gist, and attempting to discover the event from there. This, when compounded with the fact that most repressed memories are emotionally salient, the likelihood of source confusion is high. One might assume that a child abuse case one heard about actually happened to one, remembering it with the imagery established through the therapy.\n\nThe idea of psychological repression was developed in 1915 as an automatic defensive mechanism based on Sigmund Freud's psychoanalytic model in which people subconsciously push unpleasant or intolerable thoughts and feelings into their unconscious.\n\nWhen situations or memories occur that we are unable to cope with, we push them away. It is a primary ego defence mechanism that many psychotherapists readily accept. There have been numerous studies which have supported the psychoanalytic theory that states that murder, childhood trauma and sexual abuse can be repressed for a period of time and then recovered in therapy.\n\nRepressed memories can influence behavior unconsciously, manifesting themselves in our discussions, dreams, and emotional reactions. An example of repression would include a child who is abused by a parent, who later has no recollection of the events, but has trouble forming relationships. Freud suggested psychoanalysis as a treatment method for repressed memories. The goal of treatment was to bring repressed memories, fears and thoughts back to the conscious level of awareness.\n\nThought suppression is referred to as the conscious and deliberate efforts to curtail one's thoughts and memories. Suppression is goal-directed and it includes conscious strategies to forget, such as intentional context shifts. For example, if someone is thinking of unpleasant thoughts, ideas that are inappropriate at the moment, or images that may instigate unwanted behaviors, they may try to think of anything else but the unwanted thought in order to push the thought out of consciousness.\n\nIn order to suppress a thought, one must (a) plan to suppress the thought and (b) carry out that plan by suppressing all other manifestations of the thought, including the original plan. Thought suppression seems to entail a state of knowing and not knowing all at once. It can be assumed that thought suppression is a difficult and even time consuming task. Even when thoughts are suppressed, they can return to consciousness with minimal prompting. This is why suppression has also been associated with obsessive-compulsive disorder.\n\nSuppression encompasses the term directed forgetting, also known as intentional forgetting. This term refers to forgetting which is initiated by a conscious goal to forget. Intentional forgetting is important at the individual level: suppressing an unpleasant memory of a trauma or a loss that is particularly painful.\n\nThe directed forgetting paradigm is a psychological term meaning that information can be forgotten upon instruction. There are two methods of the directed forgetting paradigm; item method and list method. In both methods, the participants are instructed to forget some items, the to-be-forgotten items and the to-be-remembered items.\n\nIn the item method of directed forgetting, participants are presented with a series of random to-be-remembered and to-be-forgotten items. After each item an instruction is given to the participant to either remember it, or forget it. After the study phase, when participants are told to remember or to forget subsets of the items, the participants are given a test of all the words presented. The participants were unaware that they would be tested on the to-be-forgotten items. The recall for the to-be-forgotten words are often significantly impaired compared to the to-be-remembered words. The directed forgetting effect has also been demonstrated on recognition tests. For this reason researchers believe that the item method affects episodic encoding.\n\nIn the list method procedure, the instructions to forget are given only after half of the list has been presented. These instructions are given once in the middle of the list, and once at the end of the list. The participants are told that the first list they had to study was just a practice list, and to focus their attention on the upcoming list. After the participants have conducted the study phase for the first list, a second list is presented. A final test is then given, sometimes for only the first list and other times for both lists. The participants are asked to remember all the words they studied. When participants are told they are able to forget the first list, they remember less in this list and remember more in the second list. List method directed forgetting demonstrates the ability to intentionally reduce memory retrieval. To support this theory, researchers did an experiment in which they asked participants to record in a journal 2 unique events that happened to them each day over a 5 day period. After these 5 days the participants were asked to either remember or forget the events on these days. They were then asked to repeat the process for another 5 days, after which they were told to remember all the events in both weeks, regardless of earlier instructions. The participants that were part of the forget group had worse recall for the first week compared to the second week.\n\nThere are two theories that can explain directed forgetting: retrieval inhibition hypothesis and context shift hypothesis. The Retrieval Inhibition Hypothesis states that the instruction to forget the first list hinders memory of the list-one items. This hypothesis suggests that directed forgetting only reduces the retrieval of the unwanted memories, not causing permanent damage. If we intentionally forget items, they are difficult to recall but are recognized if the items are presented again. The Context Shift Hypothesis suggests that the instructions to forget mentally separate the to-be-forgotten items. They are put into a different context from the second list. The subject's mental context changes between the first and second list, but the context from the second list remains. This impairs the recall ability for the first list.\n\nMotivated forgetting encompasses the term psychogenic amnesia which refers to the inability to remember past experiences of personal information, due to psychological factors rather than biological dysfunction or brain damage\n\nPsychogenic amnesia is not part of Freud's theoretical framework. The memories still exist buried deeply in the mind, but could be resurfaced at any time on their own or from being exposed to a trigger in the person's surroundings. Psychogenic amnesia is generally found in cases where there is a profound and surprising forgetting of chunks of one's personal life, whereas motivated forgetting includes more day-to-day examples in which people forget unpleasant memories in a way that would not call for clinical evaluation.\n\nPsychogenic fugue, a form of psychogenic amnesia, is a in which people forget their personal history, including who they are, for a period of hours to days following a trauma. A history of depression as well as stress, anxiety or head injury could lead to fugue states. When the person recovers they are able to remember their personal history, but they have amnesia for the events that took place during the fugue state.\n\nMotivated forgetting occurs as a result of activity that occurs within the prefrontal cortex. This was discovered by testing subjects while taking a functional MRI of their brain. The prefrontal cortex is made up of the anterior cingulate cortex, the intraparietal sulcus, the dorsolateral prefrontal cortex, and the ventrolateral prefrontal cortex. These areas are also associated with stopping unwanted actions, which confirms the hypothesis that the suppression of unwanted memories and actions follow a similar inhibitory process. These regions are also known to have executive functions within the brain.\n\nThe anterior cingulate cortex has functions linked to motivation and emotion. The intraparietal sulcus possesses functions that include coordination between perception and motor activities, visual attention, symbolic numerical processing, visuospatial working memory, and determining the intent in the actions of other organisms. The dorsolateral prefrontal cortex plans complex cognitive activities and processes decision making.\n\nThe other key brain structure involved in motivated forgetting is the hippocampus, which is responsible for the formation and recollection of memories. When the process of motivated forgetting is engaged, meaning that we actively attempt to suppress our unwanted memories, the prefrontal cortex exhibits higher activity than baseline, while suppressing hippocampal activity at the same time. It has been proposed that the executive areas which control motivation and decision-making lessen the functioning of the hippocampus in order to stop the recollection of the selected memories that the subject has been motivated to forget.\n\nMotivated forgetting has been a crucial aspect of psychological study relating to such traumatizing experiences as rape, torture, war, natural disasters, and homicide. Some of the earliest documented cases of memory suppression and repression relate to veterans of the Second World War. The number of cases of motivated forgetting was high during war times, mainly due to factors associated with the difficulties of trench life, injury, and shell shock. At the time that many of these cases were documented, there were limited medical resources to deal with many of these soldier's mental well-being. There was also a lesser understanding of the aspects of memory suppression and repression.\n\nThe repression of memories was the prescribed treatment by many doctors and psychiatrists, and was deemed effective for the management of these memories. Unfortunately, many soldier's traumas were much too vivid and intense to be dealt with in this manner, as described in the journal of Dr. Rivers. One soldier, who entered the hospital after losing consciousness due to a shell explosion, is described as having a generally pleasant demeanor. This was disrupted by his sudden onsets of depression occurring approximately every 10 days. This intense depression, leading to suicidal feelings, rendered him unfit to return to war. It soon became apparent that these symptoms were due to the patient's repressed thoughts and apprehensions about returning to war. Dr. Smith suggested that this patient face his thoughts and allow himself to deal with his feelings and anxieties. Although this caused the soldier to take on a significantly less cheery state, he only experienced one more minor bout of depression.\n\nMany cases of motivated forgetting have been reported in regards to recovered memories of childhood abuse. Many cases of abuse, particularly those performed by relatives or figures of authority, can lead to memory suppression and repression of varying amounts of time. One study indicates that 31% of abuse victims were aware of at least some forgetting of their abuse and a collaboration of seven studies has shown that one eighth to one quarter of abuse victims have periods of complete unawareness (amnesia) of the incident or series of events. There are many factors associated with forgetting abuse including: younger age at onset, threats/intense emotions, more types of abuse, and increased number of abusers. Cued recovery has been shown in 90% of cases, usually with one specific event triggering the memory. For example, the return of incest memories have been shown to be brought on by television programs about incest, the death of the perpetrator, the abuse of the subject's own child, and seeing the site of abuse. In a study by Herman and Schatzow, confirming evidence was found for the same proportion of individuals with continuous memories of abuse as those individuals who had recovered memories. 74% of cases from each group were confirmed. Cases of Mary de Vries and Claudia show examples of confirmed recovered memories of sexual abuse.\n\nMotivated forgetting and repressed memories have become a very controversial issue within the court system. Courts are currently dealing with historical cases, in particular a relatively new phenomenon known as historic child sexual abuse (HCSA). HCSA refers to allegations of child abuse having occurred several years prior to the time at which they are being prosecuted.\n\nUnlike most American states, Canada, the United Kingdom, Australia and New Zealand have no statute of limitations to limit the prosecution of historical offenses. Therefore, legal decision-makers in each case need to evaluate the credibility of allegations that may go back many years. It is nearly impossible to provide evidence for many of these historical abuse cases. It is therefore extremely important to consider the credibility of the witness and accused in making a decision regarding guiltiness of the defendant.\n\nOne of the main arguments against the credibility of historical allegations, involving the retrieval of repressed memories, is found in false memory syndrome. False memory syndrome claims that through therapy and the use of suggestive techniques clients mistakenly come to believe that they were sexually abused as children.\n\nIn the United States, the statute of limitations requires that legal action be taken within three to five years of the incident of interest. Exceptions are made for minors, where the child has until they reach eighteen years of age.\n\nThere are many factors related to the age at which child abuse cases may be presented. These include bribes, threats, dependency on the abuser, and ignorance of the child to their state of harm. All of these factors may lead a person who has been harmed to require more time to present their case. As well, as seen in the case below of Jane Doe and Jane Roe, time may be required if memories of the abuse have been repressed or suppressed. In 1981, the statute was adjusted to make exceptions for those individuals who were not consciously aware that their situation was harmful. This rule was called the discovery rule. This rule is to be used by the court as deemed necessary by the Judge of that case.\n\nSevere cases of trauma may lead to psychogenic amnesia, or the loss of all memories occurring around the event.\n", "id": "26685627", "title": "Motivated forgetting"}
{"url": "https://en.wikipedia.org/wiki?curid=27241889", "text": "1-2-AX working memory task\n\nThe 1-2-AX working memory task is a task which requires working memory to be solved. It can be used as a test case for learning algorithms to test their ability to remember some old data. This task can be used to demonstrate the working memory abilities of algorithms like PBWM or Long short-term memory.\n\nIt is an extension of the A-X version of the continuous performance task.\n\nThe input of the task is a sequence of the numbers/letters \"1\", \"2\", \"A\", \"X\", \"B\", \"Y\". And additional \"C\" and \"Z\" which should be ignored.\n\nThe output is a sequence of the letters \"L\" and \"R\" — one letter for each input letter (except for \"C\" or \"Z\").\n\nThe output \"R\" should be returned if and only if there is a matching of any trailing part of the input sequence to the regular expression \"1[AXBYCZ]*A[CZ]*X\" or \"2[AXBYCZ]*B[CZ]*Y\".\n\nOtherwise (except for \"C\" or \"Z\"), an \"L\" should be returned.\n\nIn other words, \"C\" and \"Z\" are completely ignored. The sequence \"A-X\" or \"B-Y\" is accepted (with an \"R\") depending if the most recent number was a \"1\" or a \"2\". Otherwise, an \"L\" is returned.\n\n\nTo solve this task, an algorithm must be able to both remember the last number \"1\" or \"2\" and the last letter \"A\" or \"B\" independently. We refer to this memory as the working memory. This memory must persist all other input.\n\nIn addition, the algorithm must be able to strip out and ignore the letters \"C\" and \"Z\".\n\nFor traditional computer models, both requirements are easy to solve. Here is some Python code (kind of pseudo code but works) where the function \"nextOutput\" gets one single number/letter as input and returns either a letter or nothing. \"nextOutputs\" is there for convenience to operate on a whole sequence.\n\nSimilarly, this task can be solved in a straightforward way by a finite state machine with 7 states (call them \"---\", \"1--\", \"2--\", \"1A-\", \"2B-\", \"1AX\", \"2BY\").\n\nThis task is much more difficult for neural networks. For simple feedforward neural networks, this task is not solveable because feedforward networks don't have any working memory.\n\nAfter all, including working memory into neural networks is a difficult task. There have been several approaches like PBWM or Long short-term memory which have working memory. This 1-2-AX task is good task for these models and both are able to solve the task.\n", "id": "27241889", "title": "1-2-AX working memory task"}
{"url": "https://en.wikipedia.org/wiki?curid=4450450", "text": "Suspense\n\nSuspense is a feeling of fascination and excitement mixed with apprehension, tension, and anxiety developed from an unpredictable, mysterious, and rousing source of entertainment. The term most often refers to an audience's perceptions in a dramatic work. Suspense is not exclusive to fiction. It may operate whenever there is a perceived suspended drama or a chain of cause is left in doubt, with tension being a primary emotion felt as part of the situation.\n\nIn the kind of suspense described by film director Alfred Hitchcock, an audience experiences suspense when they expect something bad to happen and have (or believe they have) a superior perspective on events in the drama's hierarchy of knowledge, yet they are powerless to intervene to prevent it from happening. Films having a lot of suspense belong in the thriller genre.\n\nIn broader definition of suspense, this emotion arises when someone is aware of his lack of knowledge about the development of a meaningful event; thus, suspense is a combination of anticipation and uncertainty dealing with the obscurity of the future. In terms of narrative expectations, it may be contrasted with mystery or curiosity and surprise. Suspense could however be some small event in a person's life, such as a child anticipating an answer to a request they've made, such as, \"May I get the kitty?\" Therefore, suspense may be experienced to different degrees.\n\nThe typical suspense consists of having some real danger looming and a ray of hope. If there is no hope, the audience will feel despair. \n\nThe two common outcomes are:\n\nSome authors have tried to explain the \"paradox of suspense\", namely: a narrative tension that remains effective even when uncertainty is neutralized, because repeat audiences know exactly how the story resolves (see Gerrig 1989, Walton 1990, Yanal 1996, Brewer 1996, Baroni 2007). Some theories assume that true repeat audiences are extremely rare because, in reiteration, we usually forget many details of the story and the interest arises due to these holes of memory (see Brewer); others claim that uncertainty remains even for often told stories because, during the immersion in the fictional world, we forget fictionally what we know factually (Walton) or because we expect fictional worlds to look like the real world, where exact repetition of an event is impossible (Gerrig).\n\nThe position of Yanal is more radical and postulates that narrative tension that remains effective in true repetition should be clearly distinguished from genuine suspense, because uncertainty is part of the definition of suspense. Baroni (2007: 279-295) proposes to name \"rappel\" this kind of suspense whose excitement relies on the ability of the audience to anticipate perfectly what is to come, a precognition that is particularly enjoyable for children dealing with well-known fairy tales. Baroni adds that another kind of suspense without uncertainty can emerge with the occasional contradiction between what the reader knows about the future (cognition) and what he desires (volition), especially in tragedy, when the protagonist eventually dies or fails (\"suspense par contradiction\").\n\nIn thrillers, suspense is the key element authors use to leave the reader or viewer hanging, trying to figure out what will happen next. The effect is specially strong when the work ends without actually revealing what happens next in the storyline.\n\nSuspense is what gives a person the \"on-edge\" feeling. Suspense builds in order to make those final moments, no matter how short, the most memorable. The suspense in a story just keeps the person hooked into reading or watching more until the climax is reached, and the thrill and amusement of the suspension finally come to a close.\n\nThe tension doesn't have to be in the form of the villain stalking the hero. It can be much simpler, much less dramatic, but still make the person keep reading or watching. Suspense is about conflict, about the obstacles between the hero and their goal.\n\n\n", "id": "4450450", "title": "Suspense"}
{"url": "https://en.wikipedia.org/wiki?curid=236809", "text": "Recall (memory)\n\nRecall in memory refers to the mental process of retrieval of information from the past. Along with encoding and storage, it is one of the three core processes of memory. There are three main types of recall: free recall, cued recall and serial recall. Psychologists test these forms of recall as a way to study the memory processes of humans and animals.\nTwo main theories of the process of recall are the two-stage theory and the theory of encoding specificity.\n\nThe \"two-stage theory\" states that the process of recall begins with a search and retrieval process, and then a decision or recognition process where the correct information is chosen from what has been retrieved. In this theory, recognition only involves the latter of these two stages, or processes, and this is thought to account for the superiority of the recognition process over recall. Recognition only involves one process in which error or failure may occur, while recall involves two. However, recall has been found to be superior to recognition in some cases, such as a failure to recognize words that can later be recalled.\n\nThe theory of \"encoding specificity\" finds similarities between the process of recognition and that of recall. The \"encoding specificity principle\" states that memory utilizes information from the memory trace, or the situation in which it was learned, and from the environment in which it is retrieved. In other words, memory is improved when information available at encoding is also available at retrieval. For example, if one is to learn about a topic and study it in a specific location, but take their exam in a different setting, they would not have had as much of a successful memory recall as if they were in the location that they learned and studied the topic in. Encoding specificity helps to take into account context cues because of its focus on the retrieval environment, and it also accounts for the fact recognition may not always be superior to recall.\n\nPhilosophical questions regarding how people acquire knowledge about their world spurred the study of memory and learning. Recall is a major part of the study of memory and often comes into play in all research. For this reason, the main studies on memory in general will also provide a history to the study of recall.\nIn 1885, Hermann Ebbinghaus created nonsense syllables, combinations of letters that do not follow grammatical rules and have no meaning, to test his own memory. He would memorize a list of nonsense syllables and then test his recall of that list over varying time periods. He discovered that memory loss occurred rapidly over the first few hours or days, but showed a more steady, gradual decline over subsequent days, weeks, and months. Furthermore, Ebbinghaus discovered that multiple learning, over-learning, and spacing study times increased retention of information. Ebbinghaus' research influenced much of the research conducted on memory and recall throughout the twentieth century.\n\nFrederic Bartlett was a prominent researcher in the field of memory during the mid-twentieth century. He was a British experimental psychologist who focused on the mistakes people made when recalling new information. One of his well-known works was \"Remembering: A Study in Experimental and Social Psychology\", which he published in 1932. He is well known for his use of North American Native folk tales, including \"The War of the Ghosts\". He would provide participants in his study with an excerpt from a story and then asked them to recall it as accurately as they could. Retention intervals would vary from directly after reading the story to days later. Bartlett found that people strive for meaning, by attempting to understand the overall meaning of the story. Since the folk tale included supernatural elements, people would rationalize them to make them fit better with their own culture. Ultimately, Bartlett argued that the mistakes that the participants made could be attributed to schematic intrusions. Their current sets of knowledge intruded on their accurately recalling the folk tale.\n\nIn the 1950s there was a change in the overall study of memory that has come to be known as the cognitive revolution. This included new theories on how to view memory, often likening it to a computer processing model. Two important books influenced the revolution: \"Plans and Structures of Behavior\" by George Miller, Eugene Galanter, and Karl H. Pribram in 1960 and \"Cognitive Psychology\" by Ulric Neisser in 1967. Both provided arguments for an information-processing view of the human mind. Allen Newell and Herbert A. Simon constructed computer programs that simulated the thought processes people go through when solving different kinds of problems.\n\nIn the 1960s, interest in short-term memory (STM) increased. Before the 1960s, there was very little research that studied the workings of short-term memory and rapid memory loss. Lloyd and Margaret Peterson observed that when people are given a short list of words or letters and then are distracted and occupied with another task for few seconds, their memory for the list is greatly decreased.\nAtkinson and Shiffrin (1973) created the short term memory model, which became the popular model for studying short term memory.\n\nThe next major development in the study of memory recall was Endel Tulving's proposition of two kinds of memory: episodic and semantic. Tulving described episodic memory as a memory about a specific event that occurred at a particular time and place, for example what you got for your 10th birthday. Semantic memories are abstract words, concepts, and rules stored in long-term memory. Furthermore, Endel Tulving devised the encoding specificity principle in 1983, which explains the importance of the relation between the encoding of information and then recalling that information. To explain further, the encoding specificity principle means that a person is more likely to recall information if the recall cues match or are similar to the encoding cues.\n\nThe 1960s also saw a development in the study of visual imagery and how it is recalled. This research was led by Allan Paivio, who found that the more image-arousing a word was the more likely it would be recalled in either free recall or paired associates.\n\nThere has been a considerable amount of research into the workings of memory, and specifically recall since the 1980s. The previously mentioned research was developed and improved upon, and new research was and still is being conducted.\n\nFree recall describes the process in which a person is given a list of items to remember and then is tested by being asked to recall them in any order. Free recall often displays evidence of primacy and recency effects. Primacy effects are displayed when the person recalls items presented at the beginning of the list earlier and more often. The recency effect is when the person recalls items presented at the end of the list earlier and more often.\n\nCued recall is when a person is given a list of items to remember and is then tested with cues to remember material. Researchers have used this procedure to test memory. Participants are given pairs, usually of words, A1-B1, A2-B2...An-Bn (n is the number of pairs in a list) to study. Then the experimenter gives the participant a word to cue the participant to recall the word with which it was originally paired. The word presentation can either be visual or auditory.\n\nThere are two basic experimental methods used to conduct cued recall, the study-test method and the anticipation method. In the study-test method participants study a list of word pairs presented individually. Immediately after or after a time delay, participants are tested in the study phase of the experiment on the word pairs just previously studied. One word of each pair is presented in a random order and the participant is asked to recall the item with which it was originally paired. The participant can be tested for either forward recall, Ai is presented as a cue for Bi, or backward recall, Bi is presented as a cue for Ai. In the anticipation method, participants are shown Ai and are asked to anticipate the word paired with it, Bi. If the participant cannot recall the word, the answer is revealed. During an experiment using the anticipation method, the list of words is repeated until a certain percentage of Bi words are recalled.\n\nThe learning curve for cued recall increases systematically with the number of trials completed. This result has caused a debate about whether or not learning is all-or-none. One theory is that learning is incremental and that the recall of each word pair is strengthened with repetition. Another theory suggests that learning is all-or-none, that is one learns the word pair in a single trial and memory performance is due to the average learned pairs, some of which are learned on earlier trials and some on later trials. To examine the validity of these theories researchers have performed memory experiments. In one experiment published in 1959, experimental psychologist Irvin Rock and colleague Walter Heimer of the University of Illinois had both a control group and an experimental group learn pairs of words. The control group studied word pairs that were repeated until the participants learned all the word pairs. In the experimental group, the learned pairs remained in the list while unlearned pairs were substituted with recombinations of previous words. Rock believed that associations between two items would be strengthened if learning were incremental even when pairs are not correctly recalled. His hypothesis was that the control group would have a higher correct recall probability than the experimental group. He thought that repetition would increase the strength of the word pair until the strength reaches a threshold needed to produce an overt response. If learning were all or none, then the control group and the experimental group should learn the word pairs at the same rate. Rock found experimentally there was little difference in learning rates between the two groups. However, Rock's work did not settle the controversy because in his experiment he rearranged replaced word pairs that could be either easier or harder to learn than the original words in the word- digit pair. In further experiments that addressed the question, there were mixed results. The incremental learning hypothesis is supported by the notion that awhile after Ai-Bi pairs are learned, the recall time to recall Bi decreases with continued learning trails.\n\nAnother theory that can be tested using cued recall is symmetry of forward and backward recall. Forward recall is generally assumed to be easier than backward recall, i.e. forward recall is stronger than backward recall. This is generally true for long sequences of word or letters such as the alphabet. In one view, the independent associations hypothesis, the strength of forward and backward recall are hypothesized to be independent of each other. To confirm this hypothesis, Dr. George Wolford tested participants' forward and backward recall and found that forward and backward recall are independent of each other. The probability of correct forward recall was .47 for word pair associations and the probability of correct backward recall of word pair associations was .25. However, in another view, the associative symmetry hypothesis, the strengths of forward and backward recall are about equal and highly correlated. In S.E Asch from Swathmore College and S. M Ebenholtz's experiment, participants learned pairs of nonsense syllables by anticipation recall. After reaching a certain threshold of learning, the participants were tested by free recall to determine all pairs and single items they could remember. These researchers found that backward association was greatly weaker than forward association. However, when the availability of forward and backward recall were basically the same, there was little difference between forward and backward recall. Some scientists including Asch and Ebenholtz believe in the independent association hypothesis think that the equal strengths of forward and backward recall are compatible with their hypothesis because forward and backward recall could be independent but with equal strengths. However associative symmetry theorists interpreted the data to mean that the results fit their hypothesis.\n\nAnother study done using cued recall found that learning occurs during test trials. Mark Carrier and Pashler (1992) found that the group with a study-only phase makes 10% more errors than the group with a test-study phase. In the study-only phase, participants were given Ai-Bi, where Ai was an English word and Bi was a Siberian Eskimo Yupik word. In the test study phase, participants first attempted to recall Bi given Ai as a cue then they were shown Ai-Bi pair together. This result suggests that after participants learn something, testing their memory with mental operations helps later recall. The act of recalling instead of restudying creates new and longer lasting connection between Ai and Bi. This phenomenon is commonly referred to as the testing effect.\n\nAnother study showed that when lists are tested immediately after study, the last couple of pairs are remembered best. After a five-second delay, the recall of recently studied words diminishes. However, word pairs at the beginning of a list still show better recall. Moreover, in a longer list, the absolute number of word pairs recalled is greater but in a shorter list of word pairs, the percentage of word pairs recalled is greater.\n\nSometimes, when recalling word pairs, there is an intrusion. An intrusion is an error that participants make when they attempt to recall a word based on a cue of that word pair. Intrusions tend to have either semantic attributes in common with the correct word not recalled or have been previously studied in another word pair on the current list or a previously studied list or were close in time to the cue item. When two items are similar, an intrusion may occur. Professor Kahana and Marieke Vugt at the University of Pennsylvania examined the effects of face similarity for face-name associations. In the first experiment, they wanted to determine if performance of recall would vary with the number of faces in the study set that were similar to the cue face. Faces were similar if the radius of the faces were within a range. The number of faces within a radius is called a neighborhood density. They found that the recall of a name to face exhibited a lower accuracy and slower reaction time for faces with a greater neighborhood density. The more similarity that two faces have, the greater the probability for interference between the two faces. When cued with face A, name B may be recalled if face A and B are similar, which would signify that an intrusion has occurred. The probability of correct recall came from the number of faces that had other similar faces.\n\nSerial recall is the ability to recall items or events in the order in which they occurred. The ability of humans to store items in memory and recall them is important to the use of language. Imagine recalling the different parts of a sentence, but in the wrong order. The ability to recall in serial order has been found not only in humans, but in a number of non-human primate species and some non-primates. Imagine mixing up the order of phonemes, or meaningful units of sound, in a word so that \"slight\" becomes \"style.\" Serial-order also helps us remember the order of events in our lives, our autobiographical memories. Our memory of our past appears to exist on a continuum on which more recent events are more easily remembered in order.\n\nSerial recall in long-term memory (LTM) differs from serial recall in short-term memory (STM). To store a sequence in LTM, the sequence is repeated over time until it is represented in memory as a whole, rather than as a series of items. In this way, there is no need to remember the relationships between the items and their original positions. In STM, immediate serial recall (ISR) has been thought to result from one of two mechanisms. The first refers to ISR as a result of associations between the items and their positions in a sequence, while the second refers to associations between items. These associations between items are referred to as chaining, and is an unlikely mechanism, according to research. Position-item relationships do not account for recency and primacy effects, or the phonological similarity effect. The Primacy Model moves away from these two assumptions, suggesting that ISR results from a gradient of activation levels where each item has a particular level of activation that corresponds to its position. Research has supported the fact that immediate serial recall performance is much better when the list is homogenous (of the same semantic category) than when they are heterogeneous (of different semantic category). This suggests that semantic representations are beneficial to immediate serial recall performance. Short-term serial recall is also affected by similar-sounding items, as recall is lower (remembered more poorly) than items that do not sound alike. This is true when lists are tested independently (when comparing two separate lists of similar-sounding and not similar-sounding items) as well as when tested using a mixed list. Alan Baddeley first reported such an experiment in which items within a list were either mutually dissimilar or highly similar.\n\nThere is evidence indicating that rhythm is highly sensitive to competing motor production. Actions such as paced finger tapping can have an effect on recall as the disruptive impact of paced finger tapping, but lack of consistent effect of paced irrelevant sound, is indicative of motor feedback from the tapping task disrupting rehearsal and storage.\n\nSeven different effects are generally seen in serial recall studies with humans:\n\n\n\n\n\n\n\n\n\nThe anterior cingulate cortex, globus pallidus, thalamus, and cerebellum show higher activation during recall than during recognition which suggests that these components of the cerebello-frontal pathway play a role in recall processes that they do not in recognition. Although recall and recognition are considered separate processes, it should be noted that they are both most likely constitute components of distributed networks of brain regions. \n\nAccording to neuroimaging data, PET studies on recall and recognition have consistently found increases in regional cerebral blood flow (RCBF) in the following six brain regions: (1) the prefrontal cortex, particularly on the right hemisphere; (2) the hippocampal and parahippocampal regions of the medial temporal lobe; (3) the anterior cingulate cortex; (4) the posterior midline area that includes posterior cingulate, retrosplenial (see retrosplenial region), precuneus, and cuneus regions; (5) the inferior parietal cortex, especially on the right hemisphere; and (6) the cerebellum, particularly on the left.\n\nThe specific role of each of the six main regions in episodic retrieval is still unclear, but some ideas have been suggested. The right prefrontal cortex has been related to retrieval attempt; the medial temporal lobes to conscious recollection; the anterior cingulate to response selection; the posterior midline region to imagery; the inferior parietal to awareness of space; and the cerebellum to self-initiated retrieval .\n\nIn recent research, a group of subjects was faced with remembering a list of items and then measured when trying to recall said items. The evoked potentials and hemodynamic activity measured during encoding were found to exhibit reliable differences between subsequently recalled and not recalled items. This effect has been termed the subsequent memory effect (SME). This difference in these specific brain regions determines whether or not an item is recalled. A study by Fernandez et al. has shown that the differences that predict recall appear both as a negative deflection in the rhinal cortex of an event-related potential (ERP) 400 ms after stimulus exposure, and as a positive hippocampal ERP beginning 800 ms after stimulus onset. This means that recall only occurs if these two brain regions (rhinal cortex and hippocampus) are activated in synchrony.\n\nThe effect of attention on memory recall has surprising results. It seems that the only time attention largely affects memory is during the encoding phase. During this phase, performing a parallel task can severely impair retrieval success. It is believed that this phase requires much attention to properly encode the information at hand, and thus a distractor task does not allow proper input and reduces the amount of information learned.\n\nHuman's attention on words is impacted by emotion grasping vocabulary. Negative and positive words are better recalled than neutral words that are spoken. Many different ways that attention is focused on hearing what the speaker has to say is the inflection of the presenter's voice in a sad, content, frustrated sound or in the use of words that are close to the hear. A study was conducted to observe if the use of emotional vocabulary was a key receptor of recall memory. The groups were put into the same lecture halls and given the same speakers, but the results came back to determine that the inflection and word choice recalled by the listeners concluded that emotional words, phrases, and sounds are more memorable than neutral speakers.\n\nRecall memory is linked with instincts and mechanisms in order to remember how an event happened to learn from it or avoid the agitator, connections are made with emotions. For instance, if a speaker is very calm and neutral, the effectiveness of encoding memory is very low and listeners get the gist of what the speaker is discussing. On the other hand, if a speaker is shouting and/or using emotionally driven words, listeners tend to remember key phrases and the meaning of the speech. This is full access of the fight or flight mechanism all people have functioning in the brain, but based on what triggers this mechanism will lead to better recall of it. People tend to focus their attention on cues that are loud, very soft, or something unusual. This makes the auditory system pick up the differences in regular speaking and meaningful speech, when something significant is spoken in the discussion people home in on the message at that part of the speech but tend to lose the other part of the discussion. Our brains sense differences in speech and when those differences occur the brain encodes that part of speech into memory and the information can be recalled for future reference.\n\nMotivation is a factor that encourages a person to perform and succeed at the task at hand. In an experiment done by Roebers, Moga and Schneider (2001), participants were placed in either forced report, free report or free report plus incentive groups. In each group, they found that the amount of correct information recalled did not differ, yet in the group where participants were given an incentive they had higher accuracy results. This means that presenting participants with an encouragement to provide correct information motivates them to be more precise. However, this is only true if the perception is that success is providing correct information. When it is believed that success is the completion of the task rather than the accuracy of that completion, the number of responses is higher, yet its accuracy is lowered. This shows that the results are dependent on how success is defined to the participant. In the referred experiment, the participants that were placed in the forced response group had the lowest overall accuracy; they had no motivation to provide accurate responses and were forced to respond even when they were unsure of the answer. Another study done by Hill RD, Storandt M, and Simeone C tested the impact of memory skills training and external reward on free recall of serial word lists. Effects similar to those reported in the previous study were seen in children—in contrast to older learners.\n\nIn the absence of interference, there are two factors at play when recalling a list of items: the recency and the primacy effects. The recency effect occurs when the short-term memory is used to remember the most recent items, and the primacy effect occurs when the long-term memory has encoded the earlier items. The recency effect can be eliminated if there is a period of interference between the input and the output of information extending longer than the holding time of short-term memory (15–30 seconds). This occurs when a person is given subsequent information to recall preceding the recall of the initial information. The primacy effect, however, is not affected by the interference of recall. The elimination of the last few items from memory is due to the displacement of these items from short term memory, by the distracting task. As they have not been recited and rehearsed, they are not moved into long-term memory and are thus lost. A task as simple as counting backwards can change memory recall; however an empty delay interval has no effect. This is because the person can continue to rehearse the items in their working memory to be remembered without interference. Cohen (1989) found that there is better recall for an action in the presence of interference if that action is physically performed during the encoding phase. It has also been found that recalling some items can interfere and inhibit the recall of other items. Another stream of thought and evidence suggests that the effects of interference on recency and primacy are relative, determined by the ratio rule (retention interval to inter item presentation distractor rate) and they exhibit time-scale invariance.\n\nContext-dependency effects on recall are typically interpreted as evidence that the characteristics of the environment are encoded as part of the memory trace and can be used to enhance retrieval of the other information in the trace. In other words, you can recall more when the environments are similar in both the learning and recall phases. Context cues appear to be important in the retrieval of newly learned meaningful information. In a classic study by Godden and Baddelley (1975), they demonstrated that deep-sea divers recalled their training more effectively when trained underwater, rather than being trained on land. An academic application would be that students may perform better on exams by studying in silence, because exams are usually done in silence.\n\nState-dependent retrieval is demonstrated when material learned under one State is best recalled in that same state. A study by Carter and Cassady (1998) showed this effect with antihistamine. In other words, if you study while on hay fever tablets, then you will recall more of what you studied if you test yourself while on antihistamines in comparison to testing yourself while not on antihistamines after having studied on antihistamines.\n\nA study by Block and Ghoneim (2000) found that, relative to a matched group of healthy, non-drug-using controls, heavy marijuana use is associated with small but significant impairments in memory retrieval.cannabis induces loss of internal control and cognitive impairment, especially impairment of attention and memory, for the duration of the intoxication period.\n\nStimulants, such as cocaine, amphetamines or caffeine are known to improve recall in humans. However, the effect of prolonged use of stimulants on cognitive functioning is very different from the impact on one-time users. Some researchers have found stimulant use to lower recall rates in humans after prolonged usage. The axons, dendrites, and neurons wear out in many cases. Current research illustrates a paradoxical effect. The few exceptions undergo mental hypertrophy. Methylenedioxymethamphetamine (MDMA) users are found to exhibit difficulties encoding information into long-term memory, display impaired verbal learning, are more easily distracted, and are less efficient at focusing attention on complex tasks. The degree of executive impairment increases with the severity of use, and the impairments are relatively long-lasting. Chronic cocaine users display impaired attention, learning, memory, reaction time and cognitive flexibility. Whether or not stimulants have a positive or negative effect on recall depends on how much is used and for how long.\n\nConsistently, females perform better than males on episodic memory tasks including delayed recall and recognition. However, males and females do not differ on working, immediate and semantic memory tasks. Neuro-psychological observations suggest that, in general, previous injuries cause greater deficits in females than in males. It has been proposed that the gender differences in memory performance reflect underlying differences in the strategies used to process information, rather than anatomical differences. However, gender differences in cerebral asymmetry received support from morphometric studies showing a greater leftward asymmetry in males than in females, meaning that men and women use each side of their brain to a different extent. There is also evidence for a negative recall bias in women, which means females in general are more likely than males to recall their mistakes. In an eyewitness study by Dan Yarmey in 1991, he found that women were significantly more accurate than men in accuracy of recall for weight of suspects.\n\nThere has been much research on whether eating prior to a cognitive recall test can affect cognitive functioning. One example was a study of the effect of breakfast timing on selected cognitive functions of elementary school students. Their results found that children who ate breakfast at school scored notably higher on most of the cognitive tests than did students who ate breakfast at home and also children who did not eat breakfast at all.\n\nIn a study of women experiencing Premenstrual Syndrome, they were either given a placebo beverage or a carbohydrate-rich one. The patients were tested at home; their moods, cognitive performance, and food craving were measured before the consumption of the beverage and 30, 90, and 180 minutes after consumption. The results showed that the carbohydrate-rich beverage significantly decreased self-reported depression, anger, confusion, and carbohydrate craving 90 to 180 minutes after consumption. Memory word recognition also improved significantly.\n\nStudies have indicated that children who are inactive have poor health, but they also have poor cognitive health also. With low fitness there is a relationship to decreased cognitive functioning; for instance there are different types of cognitive problems like perception, memory, cognitive control, and there is lower academic achievement. Many tests have been conducted to identify what exactly is the reduction when children do not have physical activity. One test selected children to be in two different groups, one group was physically active the other group was not. After a while of monitoring the children the researchers tested the children in learning and recall memory to see what they were retaining and to observe the difference if available of low physical activity versus high physical activity. The results came back indicating that the children without physical activity have a later recall process than the children with physical activity. The learning part of the experiment was equally distributed on both spectrums for each group, but recall memory was the only variable that did not match both of the groups. \n\nThere is barely any recalled memory in cases of fear and trauma exposure, brain injury, post-traumatic stress disorder, pain, or anxiety. Recall memory is very limited, since the only memory people have that suffer from these problems is the flash backs of what happened when the event took place. People can only recall the memory that happened on that day when they hear or see something that brings the memory into existence. They cannot recall how they felt or what they saw, but through images or audio people can recall that tragic event. For example, the day of September 11, 2001, first responders remember the day and what it was like; but the feelings they could not recall. The only way to recall the feelings they had were when sirens of police vehicles, fire trucks, and ambulances drove by their house they feel the exact feelings that were in effect on that day. \n\nThe phenomenological account of recall is referred to as metacognition, or \"knowing about knowing\". This includes many states of conscious awareness known as feeling-of-knowing states, such as the tip-of-the-tongue state. It has been suggested that metacognition serves a self-regulatory purpose whereby the brain can observe errors in processing and actively devote resources to resolving the problem. It is considered an important aspect of cognition that can aid in the development of successful learning strategies that can also be generalized to other situations.\n\nA key technique in improving and helping recall memory is to take advantage of Mnemonic devices and other cognitive strategies. Mnemonic devices are a type of cognitive strategy that enables individuals to memorize and recall new information in an easier fashion, rather than just having to remember a list of information that is not related to one another. An example of mnemonic devices are PEMDAS or Please Excuse My Dear Aunt Sally; this is a device for arithmetic when solving equations that have parenthesis, exponents, multiplication, division, addition, or subtraction and what order to do each calculation. Words or an acronym can stand for a process that individuals need to recall. The benefits of using these types of strategies to perform tasks are that encoding becomes more organized and it is easier to remember and process information. Also this device reduces the need of intentional resources at the point of retrieval, which means that recall does not need outside sources helping an individual remember what happened yesterday. Cognitive strategies can leverage semantic connections that will allow the brain to process and work more efficiently than just having to process the information as whole parts. By using the strategies the information becomes related to each other and the information sticks. \nAnother type of device people use to help their recall memory become efficient chunking. Chunking is the process of breaking down numbers into smaller units to remember the information or data, this helps recall numbers and math facts. An example of this chunking process is a telephone number; this is chunked with three digits, three digits, then four digits. People read them off as such when reciting a phone number to another person. There has been research done about these techniques and an institution tested two groups of people to see if these types of devices work well for real people, the results came back determining a significant performance difference between the group who did not use cognitive strategies and the group who did. The group using the techniques immediately performed better than the other group and when taking a pre-test and post-test the results indicated that the group using the techniques improved while the other group did not.\n\n\"Main page: Tip of the tongue\"\n\nA tip of the tongue (TOT) state refers to the perception of a large gap between the identification or knowledge of a specific subject and being able to recall descriptors or names involving said subject. This phenomenon is also referred to as 'presque vu', a French term meaning \"almost seen\". There are two prevalent perspectives of TOT states: the psycholinguistic perspective and the metacognitive perspective.\n\nPsycholinguistics views TOT states as a failure of retrieval from lexical memory (see Cohort Model) being cued by semantic memory (facts). Since there is an observed increase in the frequency of TOT states with age, there are two mechanisms within psycholinguistics that could account for the TOT phenomenon. The first is the degradation of lexical networks with age, where degrading connections between the priming of knowledge and vocabulary increases difficulty of successfully retrieving a word from memory. The second suggests that the culmination of knowledge, experience, and vocabulary with age results in a similar situation where many connections between a diverse vocabulary and diverse knowledge also increases the difficulty of successful retrieval of a word from memory.\n\nThe metacognitive perspective views TOT states simply as the awareness felt when such an event occurs and the perception of the experience involved. Mainly being aware of a TOT state can result in the rapid devotion of cognitive resources to resolving the state and successfully retrieving the word from memory. Such an explanation leaves much to be desired; however, the psycholinguistic perspective and the metacognitive perspective on TOT states are not mutually exclusive and both are used to observe TOT states in a laboratory setting.\n\nAn incubation effect can be observed in TOT states, where the passage of time alone can influence the resolution of the state and result in successful recall. Also, the presence of a TOT state is a good predictor that the problem can be resolved correctly, although this has been shown to occur more frequently with older-young-adults than young-adults or seniors. This is evidence for both the metacognitive perspective as well as the psycholinguistic perspective. It demonstrates the devotion of resources to searching memory, a source of cumulative information, for the desired correct information, and it also shows that we are aware of what information we know or do not know. This is why the current debate between the psycholinguistic view of TOTs as retrieval failure and the metacognitive view of TOTs as a tool for learning continues.\n\nSimilar phenomena include Déjà vu (Already seen), Jamais vu (Never Seen), and Déjà entendu (Already Heard).\nThese occur rarely and are more prevalent in patients with traumatic head injuries, and brain disorders including epilepsy.\n\nUntil recently, research on this phenomenon has been relatively rare, with only two types of involuntary memory retrieval identified: involuntary autobiographical memory retrieval, and involuntary semantic memory retrieval. Both of these phenomena can be considered emergent aspects of otherwise normal and quite efficient cognitive processes.\n\"Involuntary autobiographical memory\" (IAM) retrieval occurs spontaneously as the result of sensory cues as well as internal cues, such as thought or intention. These cues influence us in our day-to-day lives by constantly and automatically activating unconscious memories through priming. It has been demonstrated in many studies that our specific goals and intentions will most frequently result in the retrieval of related IAM, while the second most frequent IAM retrievals result from physical cues in the surrounding context. Autobiographical memories that are unrelated to any specific cues, whether internal or external, are the least frequent to occur. It has been suggested that in this case, an error in self-regulation of memory has occurred that results in an unrelated autobiographical memory reaching the conscious mind. These findings are consistent with metacognition as the third type of experience is often identified as the most salient one.\n\nInvoluntary semantic memory retrieval (ISM), or \"semantic-popping\", occurs in the same fashion as IAM retrieval. However, the elicited memory is devoid of personal grounding and often considered trivial, such as a random word, image, or phrase. ISM retrieval can occur as a result of spreading activation, where words, thoughts, and concepts activate related semantic memories continually. When enough related memories are primed that an interrelated concept, word, thought, or image \"pops\" into consciousness and you are unaware of the extent of its relatedness within your memory. Spreading activation is thought to build over a period of many hours, days, or even weeks before a random semantic memory \"pops\".\n\n\"Main page: False memory syndrome\"\n\nFalse memories result from persistent beliefs, suggestions via authority figures, or statements of false information. Repeated exposure to these stimuli influence the reorganization of a person's memory, affecting its details, or implanting vivid false accounts of an event. This is usually accounted for by source-monitoring error, where a person can recall specific facts, but cannot correctly identify the source of that knowledge because of apparent loss of the association between the episodic (specific experience, or source) and semantic (concept-based, or gist) accounts of the stored knowledge. An example of this is cryptomnesia, or inadvertent plagiarism, where one duplicates a work that they have previously encountered believing it to be their original idea. False memories can also be accounted for by the generation effect, which is an observable phenomenon where repeated exposure to a belief, suggestion, or false information is better remembered with each subsequent generation. This can be seen with the misinformation effect, where an eye-witness account of an event can be influenced by a bystander account of the same event, or by suggestion via an authority figure. It is also believed to influence the recovery of repressed shocking or abusive memories in patients under hypnosis, where the recovered memory, although possibly a vivid account, could be entirely false, or have specific details influenced as the result of persistent suggestion by the therapist.\n\nRetrograde amnesia is typically the result of physical or psychological trauma which manifests itself as the inability to remember information preceding the traumatic event. It is usually accompanied by some type of anterograde amnesia, or inability to acquire new knowledge. Focal retrograde amnesia (FRA), sometimes known as functional amnesia, refers to the presence of retrograde amnesia while knowledge acquisition remains intact (no anterograde amnesia). Memory for how to use objects and perform skills (implicit memory) may remain intact while specific knowledge of personal events or previously learned facts (explicit memory) become inaccessible or lost. Amnesia can result from a number of different causes, including encephalitis, severe traumatic brain injury, vitamin B deficiency as seen in Korsakoff's Syndrome, and psychotic episodes, or by witnessing an emotionally traumatic event (Dissociative amnesia). Dysfunction of the temporal and frontal lobes have been observed in many cases of focal retrograde amnesia, whether metabolic or the result of lesions. However, this evidence only appears to correlate with the symptoms of retrograde amnesia as cases have been observed where patients suffering from minor concussions, showing no visible brain damage, develop FRA. It has been suggested that FRA could represent a variety of different disorders, cognitive deficits, or conditions that result in disproportionate loss of explicit memory, hence Disproportionate Retrograde Amnesia.\n\nThe Face Advantage allows information and memories to be recalled easier through the presentation of a person's face rather than a person's voice. Faces and voices are very similar stimuli that reveal similar information and result in similar processes of memory recall. During face perception, there are three stages of memory recall that include recognition, followed by the remembering of semantic memory and episodic memory, and finally name recall. The Face Advantage is shown through an experiment where participants are presented with faces and voices of unfamiliar faces and recognizable celebrity faces. The stimuli are presented with a between-group design. The participants are asked to say if the face or voice is familiar. If the answer is yes, they are asked to recall semantic and episodic memories and finally the name of the face or voice. It was much easier for those presented with a celebrity's face to recall information than for those presented with a voice. The results show that in the second stage of face perception when memories are recalled, information is recalled faster and more accurate after a face is perceived, and slower, less accurate and with less detail after a voice is perceived. A possible explanation is that the connections between face representations and semantic and episodic memory are stronger than that of voices.\n\nMemory phenomena are rich sources of storylines and novel situations in popular media. Two phenomena that appear regularly are total recall abilities and amnesia.\n\nThe Argentinean author, Jorge Luis Borges wrote the short story \"Funes the Memorious\" in 1944. It depicts the life of Ireneo Funes, a fictional character who falls off his horse and experiences a head injury. After this accident, Funes has total recall abilities. He is said to recall an entire day with no mistakes, but this feat of recall takes him an entire day to accomplish. It is said that Borges was ahead of his time in his description of memory processes in this story, as it was not until the 1950s and research on the patient HM that some of what the author describes began to be understood. A more recent instance of total recall in literature is found in \nis in Stieg Larsson's books \"The Girl with the Dragon Tattoo\", in which the lead character, Lisbeth Salander remembers anything she reads, indicating she has total recall ability. Another example is in Dan Brown's books \"The Da Vinci Code\" and \"Angels & Demons\", in which the main character, Dr. Robert Langdon, a religious iconography and symbology professor at Harvard University, has almost total recall ability. In \"The Curious Incident of the Dog in the Nighttime\" by Mark Haddon, the main character, Christopher Boone, is a 15-year-old autistic boy with total recall abilities.\n\nTotal recall is also popular in television. It can be seen in Season 4 of the television show \"Criminal Minds\", in which the character Dr. Spencer Reid claims to have total recall ability. Agent Fox Mulder from the television show \"The X-Files\" has a photographic memory, a popular term for total recall. Also, the character of hospital resident Lexie Grey on the television show \"Grey's Anatomy\" has total recall ability.\n\nAmnesia the damage or disruption of memory processes is a very popular subject in movies since 1915. Although its portrayal is usually inaccurate, there are some exceptions. \"Memento\" (2000) is said to be inspired by the condition of the famous amnesic patient known as HM. The main character Leonard suffers from anterograde amnesia after a traumatic attack in which his wife dies. He maintains his identity and shows very little retrograde amnesia. He also displays some of the daily memory problems that are experiences by most amnesics, such as forgetting names or where he is going. Another fairly accurate portrayal of memory disturbances is the non-human character Dory in \"Finding Nemo\" (2003). This fish, like Leonard, shows memory problems faced by most amnesics where she forgets names, has difficulty storing and recalling information, and often forgets what she is doing, or why she is doing something.\n\nMovies tend to show amnesia as a result of head injury from accidents or attacks. The loss of identity and autobiographical memory shown in \"Santa Who?\" (2000) in which Santa suffers from amnesia that destroys his identity and memory of himself is very unlikely in the real world. This is also portrayed in \"The Bourne Identity\" (2002) and \"The Bourne Supremacy\" (2004) where the main character forgets he is a trained assassin. Another misrepresentation of the reality of memory loss in the movies can be seen in Clean Slate (1994) and 50 First Dates (2004) where the characters are able to encode memory during the day but lose all memory of that day at night, while sleeping.\n\nMovies often restore victim's memory through a second trauma, or through a kind of cued recall when they revisit familiar places or see familiar objects. The phenomenon of the second trauma can be seen in \"Singing in the Dark\" (1956) where the victim experiences the onset of amnesia because of the trauma of the Holocaust, but memory is restored with a blow to the head. Although neurosurgery is often the cause of amnesia, it is seen as a solution in some movies, including \"Deluxe Annie\" (1918) and \"Rascals\" (1938).\n\nMemory erasure is portrayed in \"Eternal Sunshine of the Spotless Mind\" (2004) and in the \"Men in Black\" movies. \"Men in Black\" features a device to erase the potentially harmful memories of extraterrestrial interactions in members of the general public. \"Eternal Sunshine of the Spotless Mind\" describes a process that targets and erases memories of interpersonal relationships the patients would rather forget so that they are no longer able to recall the experience. In \"Paycheck\" (2003) and \"Total Recall\" (1990) memory suppression is used to control and the characters are able to overcome the attempts and recall pieces of their memory.\n\nBy repeating (or recalling [?]) an item over and over again, memory can improve. This process is also known as rehearsal.\n\nRetrieval-induced forgetting is a process by which retrieving an item from long-term memory impairs subsequent recall of related items.\n\n", "id": "236809", "title": "Recall (memory)"}
{"url": "https://en.wikipedia.org/wiki?curid=487921", "text": "Long-term depression\n\nLong-term depression (LTD), in neurophysiology, is an activity-dependent reduction in the efficacy of neuronal synapses lasting hours or longer following a long patterned stimulus. LTD occurs in many areas of the CNS with varying mechanisms depending upon brain region and developmental progress. LTD in the hippocampus and cerebellum have been the best characterized, but there are other brain areas in which mechanisms of LTD are understood. LTD has also been found to occur in different types of neurons that release various neurotransmitters, however, the most common neurotransmitter involved in LTD is L-glutamate. L-glutamate acts on the N-methyl-D- asparate receptors (NMDARs), α-amino-3-hydroxy-5-methylisoxazole-4-propionic acid receptors (AMPARs), kainate receptors (KARs) and metabotropic glutamate receptors (mGluRs) during LTD. It can result from strong synaptic stimulation (as occurs in the cerebellar Purkinje cells) or from persistent weak synaptic stimulation (as in the hippocampus). Long-term potentiation (LTP) is the opposing process to LTD; it is the long-lasting increase of synaptic strength. In conjunction, LTD and LTP are factors affecting neuronal synaptic plasticity. LTD is thought to result mainly from a decrease in postsynaptic receptor density, although a decrease in presynaptic neurotransmitter release may also play a role. Cerebellar LTD has been hypothesized to be important for motor learning. However, it is likely that other plasticity mechanisms play a role as well. Hippocampal LTD may be important for the clearing of old memory traces. \nHippocampal/cortical LTD can be dependent on NMDA receptors, metabotropic glutamate receptors (mGluR), or endocannabinoids. The result of the underlying-LTD molecular mechanism is the phosphorylation of AMPA glutamate receptors and their elimination from the surface of the parallel fiber-Purkinje cell (PF-PC) synapse.\n\nLTD is one of several processes that serves to selectively weaken specific synapses in order to make constructive use of synaptic strengthening caused by LTP. This is necessary because, if allowed to continue increasing in strength, synapses would ultimately reach a ceiling level of efficiency, which would inhibit the encoding of new information.\n\nIt is highly important for neurons to maintain a variable range of neuronal output. If synapses were only reinforced by positive feedback, they would eventually come to the point of complete inactivity or too much activity. To prevent neurons from becoming static, there are two regulatory forms of plasticity that provide negative feedback: metaplasticity and scaling. Metaplasticity is expressed as a change in the capacity to provoke subsequent synaptic plasticity, including LTD and LTP. The Bienenstock, Cooper and Munro model (BCM model) proposes that a certain threshold exists such that a level of postsynaptic response below the threshold leads to LTD and above it leads to LTP. BCM theory further proposes that the level of this threshold depends upon the average amount of postsynaptic activity. Scaling has been found to occur when the strength of all of a neuron’s excitatory inputs are scaled up or down. LTD and LTP coincide with metaplasticity and synaptic scaling to maintain proper neuronal network function.\n\nLong-term depression can be described as either homosynaptic plasticity or heterosynaptic plasticity. Homosynaptic LTD is restricted to the individual synapse that is activated by a low frequency stimulus. In other words, this form of LTD is activity-dependent, because the events causing the synaptic weakening occur at the same synapse that is being activated. Homosynaptic LTD is also associative in that it correlates the activation of the postsynaptic neuron with the firing of the presynaptic neuron. Heterosynaptic LTD, in contrast, occurs at synapses that are not potentiated or are inactive. The weakening of a synapse is independent of the activity of the presynaptic or postsynaptic neurons as a result of the firing of a distinct modulatory interneuron. Thus, this form of LTD impacts synapses nearby those receiving action potentials.\n\nLTD affects hippocampal synapses between the Schaffer collaterals and the CA1 pyramidal cells. LTD at the Schaffer collateral-CA1 synapses depends on the timing and frequency of calcium influx. LTD occurs at these synapses when Schaffer collaterals are stimulated repetitively for extended time periods (10–15 minutes) at a low frequency (approximately 1 Hz). Depressed excitatory postsynaptic potentials (EPSPs) result from this particular stimulation pattern. The magnitude of calcium signal in the postsynaptic cell largely determines whether LTD or LTP occurs; LTD is brought about by small, slow rises in postsynaptic calcium levels. When Ca entry is below threshold, it leads to LTD. The threshold level in area CA1 is on a sliding scale that depends on the history of the synapse. If the synapse has already been subject to LTP, the threshold is raised, increasing the probability that a calcium influx will yield LTD. In this way, a \"negative feedback\" system maintains synaptic plasticity. Activation of NMDA-type glutamate receptors, which belong to a class of ionotropic glutamate receptors (iGluRs), is required for calcium entry into the CA1 postsynaptic cell. Change in voltage provides a graded control of postsynaptic Ca by regulating NMDAR-dependent Ca influx, which is responsible for initiating LTD.\n\nWhile LTP is in part due to the activation of protein kinases, which subsequently phosphorylate target proteins, LTD arises from activation of calcium-dependent phosphatases that dephosphorylate the target proteins. Selective activation of these phosphatases by varying calcium levels might be responsible for the different effects of calcium observed during LTD. The activation of postsynaptic phosphatases causes internalization of synaptic AMPA receptors (also a type of iGluRs) into the postsynaptic cell by clathrin-coated endocytosis mechanisms, thereby reducing sensitivity to glutamate released by Schaffer collateral terminals.\n\nLTD occurs at synapses in cerebellar Purkinje neurons, which receive two forms of excitatory input, one from a single climbing fiber and one from hundreds of thousands of parallel fibers. LTD decreases the efficacy of parallel fiber synapse transmission, though, according to recent findings, it also impairs climbing fiber synapse transmission. Both parallel fibers and climbing fibers must be simultaneously activated for LTD to occur. With respect to calcium release however, it is best if the parallel fibers are activated a few hundred milliseconds before the climbing fibres. In one pathway, parallel fiber terminals release glutamate to activate AMPA and metabotropic glutamate receptors in the postsynaptic Purkinje cell. When glutamate binds to the AMPA receptor, the membrane depolarizes. Glutamate binding to the metabotropic receptor activates phospholipase C (PLC) and produces diacylglycerol (DAG) and inositol triphosphate (IP3) second messengers. In the pathway initiated by activation of climbing fibers, calcium enters the postsynaptic cell through voltage-gated ion channels, raising intracellular calcium levels. Together, DAG and IP3 augment the calcium concentration rise by targeting IP3-sensitive receptors triggering release of calcium from intracellular stores as well as protein kinase C (PKC) activation (which is accomplished jointly by calcium and DAG). PKC phosphorylates AMPA receptors, which promotes their dissociation from scaffold proteins in the post-synaptic membrane and subsequent internalization. With the loss of AMPA receptors, the postsynaptic Purkinje cell response to glutamate release from parallel fibers is depressed. Calcium triggering in the cerebellum is a critical mechanism involved in long-term depression. Parallel fibre terminals and climbing fibres work together in a positive feedback loop for invoking high calcium release.\n\nFurther research has determined calcium's role in long-term depression induction. While other mechanisms of long-term depression are being investigated, calcium's role in LTD is a defined and well understood mechanism by scientists. High calcium concentrations in the post-synaptic Purkinje cells is a necessity for the induction of long-term depression. There are several sources of calcium signaling that elicit LTD: climbing fibres and parallel fibres which converge onto Purkinje cells. Calcium signaling in the post-synaptic cell involved both spatial and temporal overlap of climbing fibre induced calcium release into dendrites as well as parallel fibre induced mGluRs and IP3 mediated calcium release. In the climbing fibres, AMPAR-mediated depolarization induces a regenerative action potential that spreads to the dendrites, which is generated by voltage-gated calcium channels. Paired with PF-mediated mGluR1 activation results in LTD induction. In the parallel fibres, GluRs are activated by constant activation of the parallel fibres which indirectly induces the IP3 to bind to its receptor (IP3) and activate calcium release from intracellular storage. In calcium induction, there is a positive feedback loop to regenerate calcium for long-term depression.Climbing and parallel fibres must be activated together to depolarize the Purkinje cells while activating mGlur1s. Timing is a critical component to CF and PF as well, a better calcium release involves PF activation a few hundred milliseconds before CF activity.\n\nThere is a series of signaling cascades, MAPK, in the cerebellum that plays a critical role in cerebellum LTD. The MAPK cascade is important in information processing within neurons and other various types of cells. The cascade includes MAPKKK, MAPKK, and MAPK. Each is dual phosphorylated by the other, MAPKKK dual phosphorylates MAPKK and in turn dual phosphorylates MAPK. There is a positive feedback loop that results from a simultaneous input of signals from PF-CF and increases DAG and Ca in Purkinje dendritic spines. Calcium and DAG activate conventional PKC (cPKC), which then activates MAPKKK and the rest of the MAPK cascade. Activated MAPK and Ca activate PLA2, AA and cPKC creating a positive feedback loop. Induced cPKC phosphorylates AMPA receptors and are eventually removed form the postsynaptic membrane via endocytosis. The timescale is for this process is approximately 40 minutes. overall, the magnitude of the LTD correlates with AMPAR phosphorylation.\n\nThe mechanisms of LTD differ in the two subregions of the striatum. LTD is induced at corticostriatal medium spiny neuron synapses in the dorsal striatum by a high frequency stimulus coupled with postsynaptic depolarization, coactivation of dopamine D1 and D2 receptors and group I mGlu receptors, lack of NMDA receptor activation, and endocannabinoid activation.\n\nIn the prelimbic cortex of the striatum, three forms or LTD have been established. The mechanism of the first is similar to CA1-LTD: a low frequency stimulus induces LTD by activation of NMDA receptors, with postsynaptic depolarization and increased postsynaptic calcium influx. The second is initiated by a high frequency stimulus and is arbitrated by presynaptic mGlu receptor 2 or 3, resulting in a long term reduction in the involvement of P/Q-type calcium channels in glutamate release. The third form of LTD requires endocannabinoids, activation of mGlu receptors and repetitive stimulation of glutamatergic fibers (13 Hz for ten minutes), resulting in a long term decrease in presynaptic glutamate release. It is proposed that LTD in GABAergic striatal neurons leads to a long term decrease in inhibitory effects on the basal ganglia, influencing the storage of motor skills.\n\nLong-term depression has also been observed in the visual cortex, and it is proposed to be involved in ocular dominance. Recurring low-frequency stimulation of layer IV of the visual cortex or the white matter of the visual cortex causes LTD in layer III. In this form of LTD, low-frequency stimulation of one pathway results in LTD only for that input, making it homosynaptic. This type of LTD is similar to that found in the hippocampus, because it is triggered by a small elevation in postsynaptic calcium ions and activation of phosphatases. LTD has also been found to occur in this fashion in layer II. A different mechanism is at work in the LTD that occurs in layer V. In layer V, LTD requires low frequency stimulation, endocannabinoid signaling, and activation of presynaptic NR2B-containing NMDA receptors.\n\nIt has been found that paired-pulse stimulation (PPS) induces a form of homosynaptic LTD in the superficial layers of the visual cortex when the synapse is exposed to carbachol (CCh) and norepinephrine (NE).\n\nThe magnitude of this LTD is comparable to that which results from low frequency stimulation, but with fewer stimulation pulses (40 PPS for 900 low frequency stimulations). It is suggested that the effect of NE is to control the gain of NMDA receptor-dependent homosynaptic LTD. Like norepinephrine, acetylcholine is proposed to control the gain of NMDA receptor-dependent homosynaptic LTD, but it is likely to be a promoter of additional LTD mechanisms as well.\n\nThe neurotransmitter serotonin is involved in LTD induction in the prefrontal cortex (PFC). The serotonin system in the PFC plays an important role in regulating cognition and emotion. Serotonin, in cooperation with a group I metabotropic glutamate receptor (mGluR) agonist, facilitates LTD induction through augmentation of AMPA receptor internalization. This mechanism possibly underlies serotonin's role in the control of cognitive and emotional processes that synaptic plasticity in PFC neurons mediates.\n\nComputational models predict that LTD creates a gain in recognition memory storage capacity over that of LTP in the perirhinal cortex, and this prediction is confirmed by neurotransmitter receptor blocking experiments. It is proposed that there are multiple memory mechanisms in the perirhinal cortex. The exact mechanisms are not completely understood, however pieces of the mechanisms have been deciphered. Studies suggest that one perirhinal cortex LTD mechanism involves NMDA receptors and group I and II mGlu receptors 24 hours after the stimulus. The other LTD mechanism involves acetylcholine receptors and kainate receptors at a much earlier time, about 20 to 30 minutes after stimulus.\n\nEndocannabinoids affect long-lasting plasticity processes in various parts of the brain, serving both as regulators of pathways and necessary retrograde messengers in specific forms of LTD. In regard to retrograde signaling, cannabinoid receptors function widely throughout the brain in presynaptic inhibition. Endocannabinoid retrograde signaling has been shown to effect LTD at corticostriatal synapses and glutamatergic synapses in the prelimbic cortex of the nucleus accumbens (NAc), and it is also involved in spike-timing-dependent LTD in the visual cortex. Endocannabinoids are implicated in LTD of inhibitory inputs (LTDi) within the basolateral nucleus of the amygdala (BLA) as well as in the stratum radiatum of the hippocampus. Additionally, endocannabinoids play an important role in regulating various forms of synaptic plasticity. They are involved in inhibition of LTD at parallel fiber Purkinje neuron synapses in the cerebellum and NMDA receptor-dependent LTD in the hippocampus.\n\nSpike timing-dependent plasticity (STDP) refers to the timing of presynaptic and postsynaptic action potentials. STDP is a form of neuroplasticity in which a millisecond-scale change in the timing of presynaptic and postsynaptic spikes will cause differences in postsynaptic Ca signals, inducing either LTP or LTD. LTD occurs when postsynaptic spikes precede presynaptic spikes by up to 20-50 ms. Whole-cell patch clamp experiments \"in vivo\" indicate that post-leading-pre spike delays elicit synaptic depression.\nLTP is induced when neurotransmitter release occurs 5-15 ms before a back-propagating action potential, whereas LTD is induced when the stimulus occurs 5-15 ms after the back-propagating action potential. There is a plasticity window: if the presynaptic and postsynaptic spikes are too far apart (i.e., more than 15 ms apart), there is little chance of plasticity. The possible window for LTD is wider than that for LTP – although it is important to note that this threshold depends on synaptic history.\n\nWhen postsynaptic action potential firing occurs prior to presynaptic afferent firing, both presynaptic endocannabinoid (CB1) receptors and NMDA receptors are stimulated at the same time. Postsynaptic spiking alleviates the Mg block on NMDA receptors. The postsynaptic depolarization will subside by the time an EPSP occurs, enabling Mg to return to its inhibitory binding site. Thus, the influx of Ca in the postsynaptic cell is reduced. CB1 receptors detect postsynaptic activity levels via retrograde endocannabinoid release.\n\nSTDP selectively enhances and consolidates specific synaptic modifications (signals), while depressing global ones (noise). This results in a sharpened signal-to-noise ratio in human cortical networks that facilitates the detection of relevant signals during information processing in humans.\n\nLong-term depression has long been hypothesized to be an important mechanism behind motor learning and memory. Cerebellar LTD is thought to lead to motor learning, and hippocampal LTD is thought to contribute to the decay of memory. However, recent studies have found that hippocampal LTD may not act as the reverse of LTP, but may instead contribute to spatial memory formation. Although LTD is now well characterized, these hypotheses about its contribution to motor learning and memory remain controversial.\n\nStudies have connected deficient cerebellar LTD with impaired motor learning. In one study, metabotropic glutamate receptor 1 mutant mice maintained a normal cerebellar anatomy but had weak LTD and consequently impaired motor learning. However the relationship between cerebellar LTD and motor learning has been seriously challenged. A study on rats and mice proved that normal motor learning occurs while LTD of Purkinje cells is prevented by (1R-1-benzo thiophen-5-yl-2[2-diethylamino)-ethoxy] ethanol hydrochloride (T-588). Likewise, LTD in mice was disrupted using several experimental techniques with no observable deficits in motor learning or performance. These taken together suggest that the correlation between cerebellar LTD and motor learning may have been illusory.\n\nStudies on rats have made a connection between LTD in the hippocampus and memory. In one study, rats were exposed to a novel environment, and homosynaptic plasticity (LTD) in CA1 was observed. After the rats were brought back to their initial environment, LTD activity was lost. It was found that if the rats were exposed to novelty, the electrical stimulation required to depress synaptic transmission was of lower frequency than without novelty. When the rat was put in a novel environment, acetylcholine was released in the hippocampus from the medial septum fiber, resulting in LTD in CA1. Therefore, it has been concluded that acetylcholine facilitates LTD in CA1.\n\nLTD has been correlated with spatial learning in rats, and it is crucial in forming a complete spatial map. It suggested that LTD and LTP work together to encode different aspects of spatial memory.\n\nNew evidence suggests that LTP works to encode space, whereas LTD works to encode the features of space. Specifically, it is accepted that encoding of experience takes place on a hierarchy. Encoding of new space is the priority of LTP, while information about orientation in space could be encoded by LTD in the dentate gyrus, and the finer details of space could be encoded by LTD in the CA1.\n\nThe addictive property of cocaine is believed to occur in the nucleus accumbens (NAc). After chronic cocaine use, the amount of AMPA receptors relative to NMDA receptors decreases in the medium spiny neurons in the NAc shell. This decrease in AMPA receptors is thought to occur through the same mechanism as NMDR-dependent LTD, because this form of plasticity is reduced after cocaine use. During the period of cocaine use, the mechanisms of LTD artificially occur in the NAc. As a consequence, the amount of AMPA receptors is ramped up in the NAc neurons during withdrawal. This is possibly due to homeostatic synaptic scaling. This increase in AMPA receptors causes a hyperexcitability in the NAc neurons. The effect of this hyperexcitability is thought to be an increase in the amount of GABA release from the NAc on the ventral tegmental area (VTA), making the dopaminergic neurons in the VTA less likely to fire, and thus resulting in the symptoms of withdrawal.\n\nResearch on the role of LTD in neurological disorders such as Alzheimer's disease (AD) is ongoing. It has been suggested that a reduction in NMDAR-dependent LTD may be due to changes not only in postsynaptic AMPARs but also in NMDARs, and these changes are perhaps present in early and mild forms of Alzheimer-type dementia.\n\nAdditionally, researchers have recently discovered a new mechanism (which involves LTD) linking soluble amyloid beta protein (Aβ) with the synaptic injury and memory loss related to AD. While Aβ's role in LTD regulation has not been clearly understood, it has been found that soluble Aβ facilitates hippocampal LTD and is mediated by a decrease in glutamate recycling at hippocampal synapses. Excess glutamate is a proposed contributor to the progressive neuronal loss involved in AD. Evidence that soluble Aβ enhances LTD through a mechanism involving altered glutamate uptake at hippocampal synapses has important implications for the initiation of synaptic failure in AD and in types of age-related Aβ accumulation. This research provides a novel understanding of the development of AD and proposes potential therapeutic targets for the disease. Further research is needed to understand how soluble amyloid beta protein specifically interferes with glutamate transporters.\n\nThe mechanism of long-term depression has been well characterized in limited parts of the brain. However, the way in which LTD affects motor learning and memory is still not well understood. Determining this relationship is presently one of the major focuses of LTD research.\n\nNeurodegenerative diseases research remains inconclusive as to the mechanisms that triggers the degeneration in the brain. New evidence demonstrates there are similarities between the apoptotic pathway and LTD which involves the phosphorylation/activation of GSK3β. NMDAR-LTD(A) contributes to the elimination of excess synapses during development. This process is downregulated after synapses have stabilized, and is regulated by GSK3β. During neurodegeneration, there is the possibility that there is deregulation of GSK3β resulting in 'synaptic prunning'. If there is excess removal of synapses, this illustrates early signs of neurodegeration and a link between apoptosis and neurodegeneration diseases.\n\n\n", "id": "487921", "title": "Long-term depression"}
{"url": "https://en.wikipedia.org/wiki?curid=1873056", "text": "Prospective memory\n\nProspective memory is a form of memory that involves remembering to perform a planned action or recall a planned intention at some future point in time. Prospective memory tasks are common in daily life and range from the relatively simple to extreme life-or-death situations. Examples of simple tasks include remembering to put the toothpaste cap back on, remembering to reply to an email, or remembering to return a rented movie. Examples of highly important situations include a patient remembering to take medication or a pilot remembering to perform specific safety procedures during a flight.\n\nIn contrast to prospective memory, retrospective memory involves remembering people, events, or words that have been encountered in the past. Whereas retrospective memory requires only the recall of past events, prospective memory requires the exercise of retrospective memory at a time that has not yet occurred. Prospective memory is thus considered a form of memory for the future. Retrospective memory involves the memory of what we know, containing informational content; prospective memory focuses on when to act, rather than focusing on informational content. There is some evidence demonstrating the role of retrospective memory in the successful execution of prospective memory, but this role seems to be relatively small.\n\nThere are two types of prospective memory: event-based and time-based prospective memory. Event-based prospective memory involves remembering to perform certain actions when specific circumstances occur. For example, driving past the local library cues the remembrance of the need to return an overdue book. Time-based prospective memory involves remembering to perform an action at a particular point in time. For example, seeing that it is 10:00 PM acts as a cue to watch a favorite television show.\n\nResearch performed by Sellen et al. (1997) compared event-based and time-based cues on prospective memory tasks. The experimenters gave participants a place (event-based cue) and a time (time-based cue) and were told to press a button each time those cues appeared during the study. It was found that performance on event-based tasks was better than performance on time-based tasks, even when participants took more time to think about their responses. The difference in task performance between the two types of prospective memory suggests that the intended action was better triggered by external cues of the event-based task than internal cues of the time-based task. External cues, as opposed to internal cues, act as a prompt for better performance, making it easier to complete event-based tasks.\n\nMcDaniel et al. (2004) further distinguished event-based prospective memory into immediate-execute tasks and delayed-execute tasks. Immediate-execute tasks involve a response as soon as a particular cue is noticed, while delayed-execute tasks involve delays between the perception of the relevant cue and the performance of the intended action. Delayed-execute tasks more commonly occur in real life when circumstances of a situation prevent intermediate action once the cue has been perceived. Research was performed by McDaniel et al. (2004), in which participants completed tasks involving various delays and interruptions between cues and responses. It was demonstrated that correct performance suffered when there was a delay or interruption during a task. However, it was further shown that the use of reminders for participants eliminated the effects of the interruption task.\n\nThere is great interest about the possible mechanisms and resources that underlie the workings of prospective memory.\n\nThe preparatory attentional and memory (PAM) theory proposes two types of processes involved in successful prospective memory performance. The first component of this theory involves a monitoring process that begins when a person constructs an intention that is then maintained until it is performed. This monitoring component involves a capacity-consuming process, similar to those used when maintaining attention, because there is a need for the intention to be stored and maintained in memory. The second component involves the use of elements of retrospective memory processes. These elements are used to differentiate between the wanted prospective memory intention and unwanted thoughts, in an attempt to keep focus on the goal and not the other options surrounding it. Retrospective memory is also used to remember specifically what intention is supposed to be performed in the future, and the monitoring process is needed to be able to remember to perform this action at the correct condition or time.\n\nAccording to this theory, prospective memory should be enhanced when complete attention is given to the desired task than when attention is divided among multiple tasks. Research conducted by McDaniel et al. (1998) attempted to prove that prospective memory performance is better on focused tasks as opposed to those where attention is split. Subjects completed a prospective memory task in either a condition where full attention was given or a condition where attention was divided on other tasks. The results were consistent with the PAM theory, showing that participants' prospective memory performance was better with full attention.\n\nHowever, there is a lot of scepticism that the rather complex mechanisms of the PAM theory are required for all, sometimes mundane, prospective memory tasks. In research by Reese and Cherry (2002), participants formed an intention to act in the future, but were interrupted prior to acting on their intention when the cue was present. When participants were asked their thoughts at the moment of interruption, only 2% reported that they were thinking of the original intention. This demonstrated evidence against the PAM theory, that there is constant maintenance from the time of constructing the intention to acting upon it at the right circumstance.\n\nFurther research conducted by Einstein and McDaniel in 1990, found that subjects during prospective memory tasks reported that their intention often \"popped\" into mind, instead of being constantly monitored and consciously maintained. Another theory was proposed in 2000, called the reflexive-associative theory, which states that when people create an intention for a prospective memory task, they make an association between the target cue and the intended action. Later when the target cue occurs, the automatic associative-memory system triggers the retrieval of the intended action and brings it back into conscious awareness. Therefore, as long as the target cue occurs, the association previously made will initiate the retrieval of the intended action, regardless of whether the intention is in consciousness.\n\nAnother theory that has been used to explain the mechanisms of prospective memory is the multi-process model proposed by McDaniel and Einstein (2005). This theory states that prospective memory retrieval does not always need an active monitoring process but can occur spontaneously (i.e., the occurrence of a cue can cause the intention to be retrieved, even when no preparatory attentional processes are engaged). Therefore, multiple processes can be used for successful prospective memory. Further, it was believed that it would be maladaptive to rely solely on active monitoring because it requires a lot of attentional resources. This may potentially interfere with other forms of processing that are required for different tasks during the retention interval.\nProspective memory cues will lead to spontaneous retrieval of an intention when at least one of four conditions is met: the cue and target action are highly associated with each other, the cue is salient, the other processes performed during the period between cue and action of the prospective memory task direct attention to relevant cue features (e.g., task appropriate processing), or the intended action is simple. Further research has found that although many aspects of prospective memory tasks are automatic, they do involve a small amount of processing. An experiment conducted by Einstein et al. (2005) found that some participants performed slower on a filler task when performing a prospective memory task at the same time. Even though some of the participants did not engage in active monitoring, they showed nearly the same rate of success on the task, demonstrating the use of multiple processes for prospective memory performance.\n\nAs prospective memory involves remembering and fulfilling an intention, it requires episodic memory, declarative memory, and retrospective memory, followed by supervisory executive functions. All of these are controlled by the frontal lobe which is situated at the front of the cerebral hemisphere.\n\nStudies using positron emission tomography (PET) trace a slight increase in blood flow to the frontal lobe in participants completing prospective memory tasks involving remembering a planned action, while performing other tasks. During these procedures, sites of brain activation include the prefrontal cortex, specifically the right dorsolateral, ventrolateral, and medial regions, as well as the median frontal lobe. The prefrontal cortex is responsible for holding the intention in consciousness and suppressing other internal thoughts. The median frontal lobe keeps attention focused on the planned action instead of the other tasks.\n\nThe prefrontal cortex is involved mainly in event-based as opposed to time-based prospective memory. Cheng et al. (2008) had participants with lesions in the prefrontal cortex perform event-based and time-based prospective memory tasks. They found that performance was impaired in the event-based tasks, which use event cues to trigger intentions, but not in the time-based tasks which use time cues to trigger intentions.\n\nOther lesion studies have also shown the use of the frontal lobe in remembering and focusing on intentions. Burgess et al. (2000) studied patients with lesions to areas in the frontal lobe such as Brodmann's area 10, finding that these patients failed to follow instructions and switch attention during prospective memory tasks.\n\nThe parietal lobe is typically involved in processing sensory information and is situated in the superior region of the brain.\n\nFor prospective memory, the parietal lobe is important for recognizing cues that trigger an intended action, especially when the cues are visual or spatial. The parietal lobe is also responsible for maintaining attention on the intended action and inhibiting other activities during performance. Studies using PET have shown that the parietal lobe is activated when participants engage in prospective memory tasks involving visual information such as remembering a series of numbers. Activation of the parietal lobe is also evident in studies using magnetoencephalography (MEG) which traces electric activity of the brain.\n\nHarrington et al. (1998) found that neural areas ranging from the inferior parietal cortex to the frontal gyri are involved in temporal monitoring during time-based prospective memory tasks. Patients with damage to these areas of the brain had difficulty judging duration and frequency of auditory tones that were presented. Keeping track of information over time is important for prospective memory, remembering intentions to perform in the future.\n\nMuch of the limbic system, which contains primitive brain structures relating to emotion and motivation, are involved in memory.\n\nMethods that test prospective memory require the distinction between retrospective memory, which is remembering information, and prospective memory, which is remembering information for the future. Prospective memory requires retrospective memory because one must remember the information itself in order to act in the future. For example, remembering to buy groceries after work (prospective memory) requires the ability to remember what type of groceries are needed (retrospective memory). While prospective memory and retrospective memory are connected, they are distinguishable. This makes it possible to separate these two processes during tests.\n\nProspective memory tasks can be used in a variety of ways to assess prospective memory. Firstly, results from these tasks can directly assess prospective memory. Furthermore, these tasks can be performed while experimenters use PET, magnetic resonance imaging (MRI), or MEG to monitor brain activation. Finally, these tasks can be followed by questionnaires about prospective memory. Combining different assessments can confirm or deny experimental findings, making sure that conclusions about prospective memory are accurate. All tasks can assess individual stages of prospective memory such as the formation or execution of an intention, or access prospective memory as a whole by looking at overall performance.\n\nTechnological assessments were created in order to more appropriately evaluate prospective memory by combining real life intentions with experimental control.\n\nThere is an increasing amount of research on the effect of age on prospective memory where typical studies compare groups of people from different ages. A study by Smith et al. (2010) comparing event-based prospective memory in schoolchildren (7–10 years old) and young adults found that adults had better memory performance. Another study by Kvavilashvili et al. (2009) comparing time-based prospective memory among young adults (18- to 30-year-olds), young-old adults (60- to 75-year-olds) and old-old adults (76- to 90-year-olds) showed that young adults had better performance. Event-based prospective memory was further compared between young-old and old-old adults and findings were that young-old adults performed better than old-old adults. These studies suggested that there is continual improvement of prospective memory from childhood into young adulthood but that a decline begins in late adulthood.\n\nA study comparing prospective memory of non-psychotic first-degree relatives of patients with schizophrenia and healthy participant showed that the relatives performed significantly worse on time-based and event-based prospective memory tasks. Since schizophrenia has a heritable component, this suggested that genetics may play a role in affecting prospective memory.\n\nMany diseases and disorders negatively affect prospective memory, as well as source memory, item recognition, and temporal order memory. The effects range from mild cognitive impairments to more detrimental impairments such as early onset dementia.\n\nThe effect of pregnancy on prospective memory is still under current study. Rendell et al. (2008) tested the prospective memory of 20 pregnant women in the laboratory. There were no significant differences observed between pregnant and non-pregnant women for event-based prospective memory tasks, but there were clear hindrances in performance for pregnant women in time-based prospective memory tasks such as a job deadline. Pregnant women are more likely to remember to perform an intention after the cue has already passed. Further, women tested a few months after giving birth were found to forget intentions entirely. Both these findings may be related to stress encountered during pregnancy or child rearing and lack of sleep.\n\nEmotional target cues have been shown to eliminate age differences in prospective memory. For older participants, emotional prospective memory cues were better remembered than neutral cues. Whether the cues are positive or negative, strong emotional attachment makes the cue more self-relevant and easier to remember. For example, an aversive picture of a snake biting a person or a positive picture of a dog licking its owner are easier to remember because they evoke emotional responses, as opposed to a neutral picture of an animal that does not evoke an emotional response. It is speculated that the amygdala and hippocampus may play a role in this emotionally enhanced memory effect.\n\nIn a study by Kliegel et al. (2008), it was shown that motivational state affected performance in two age groups (three-year-olds and five-year-olds) completing the same prospective memory task. There was no difference for the two age groups when motivation was high but performance of the three years old was reduced when motivation was low. If a person considers a task to be unimportant or is affected by fatigue, they will not be motivated to remember the intention. Less attention will be given to relevant cues and the memory is more likely to be forgotten. Therefore, prospective memory can be enhanced by avoiding low motivational states.\n\nVarious studies have reported that 50-80% of all everyday memories are, at least in part, related to prospective memory. Prospective memory is crucial for normal functioning since people form future intentions and remember to carry out past intentions on a daily basis. Numerous aspects of daily life require prospective memory, ranging from ordinary activities such as remembering where to meet a friend, to more important tasks such as remembering what time to take medication.\n\nThere is a complicated relationship between prospective memory and time management skills which include making lists, scheduling activities, and avoiding interruptions. Studies have not identified distinct cause and effect relationships between prospective memory and time management, but many consistent correlations have been observed. For example, people who reported better prospective memory according to the Prospective and Retrospective Memory Questionnaire (PRMQ) also indicated a higher likelihood of setting goals and priorities and being more organized. There may be a cyclical effect between prospective memory and time management: better memory may lead to better organization, and better organization may further lead to better memory.\n\nAviation controllers are often occupied with multiple tasks at the same time, and hazardous effects can occur when prospective memory fails. In an accident in 1991, a tower controller in an airport forgot a step in a simple procedure and that led to two planes crashing into each other, killing a number of passengers and crew. An analysis of over 1300 fatal aviation accidents from 1950-2009 showed that the majority were due to pilot error: 50% attributed to pilot error, 6% due to non-pilot human error, 22% to mechanical failure, 12% to bad weather, 9% to sabotage, and 1% to other causes.\n\nThe nursing environment is full of event-based and time-based prospective memory tasks. Simple tasks such as remembering to order a drug or calling patient's family and remembering when to switch shifts are just some examples of a nurse's reliance on prospective memory. It is surprising that not much research has been done concerning the importance of prospective memory in nurses since they face many life-threatening tasks.\n\nProspective memory is required to remember when to take oral contraceptive pills. A study performed by Matter and Meier (2008) showed that women who self-reported higher prospective memory ability were more satisfied with oral contraceptive use and experienced lower stress levels. Having better memory makes it is easier for these women to remind themselves to take their contraceptives at the required time of the day.\n\nWith advancements in technology, Smartphones can serve as prospective memory aids. Electronic calendars are of great use in time-based prospective memory tasks and recently they have been shown to also cue event-based tasks. The iPhone, as well as phones using the Android operating system, can track the user's location using the phone's Global Positioning System (GPS) and send reminders based on the current location. For example, when a parent is near their children's school, the phone can send a reminder for them to pick up their children after school.\n\nProspective memory has been implicated in the steering cognition model of how children coordinate their attention and response to learning tasks in school. Walker and Walker showed that pupils able to adjust their prospective memory most accurately for different curriculum learning tasks in maths, science and English were more effective learners than pupils whose prospective memory was fixed or inflexible.\n\nAttempts to find wanted or missing individuals through public alert systems sometimes make use of a type of event based prospective memory called prospective person memory. In prospective person memory, a picture of a wanted or missing person is presented to the public with instructions to report any sightings of the individual to authorities. Field experiments show that prospective person memory is often quite poor.\n", "id": "1873056", "title": "Prospective memory"}
