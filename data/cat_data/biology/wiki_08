{"url": "https://en.wikipedia.org/wiki?curid=41856558", "text": "Incomplete Nature\n\nIncomplete Nature: How Mind Emerged from Matter is a 2011 book by biological anthropologist Terrence Deacon. The book covers topics in biosemiotics, philosophy of mind, and the origins of life. Broadly, the book seeks to naturalistically explain \"aboutness\", that is, concepts like intentionality, meaning, normativity, purpose, and function; which Deacon groups together and labels as ententional phenomena.\n\nDeacon's first book, \"The Symbolic Species\" focused on the evolution of human language. In that book, Deacon notes that much of the mystery surrounding language origins comes from a profound confusion on the nature of semiotic processes themselves. Accordingly, the focus of \"Incomplete Nature\" shifts from human origins to the origin of life and semiosis. \"Incomplete Nature\" can be viewed as a sizable contribution to the growing body of work positing that the problem of consciousness and the problem of the origin of life are inexorably linked. Deacon tackles these two linked problems by going back to basics. The book expands upon the classical conceptions of work and information in order to give an account of ententionality that is consistent with eliminative materialism and yet does not seek to explain away or pass off as epiphenominal the non-physical properties of life.\n\nA central thesis of the book is that absence can still be efficacious. Deacon makes the claim that just as the concept of zero revolutionized mathematics, thinking about life, mind, and other ententional phenomena in terms of constraints (i.e. what is absent) can similarly help us overcome the artificial dichotomy of the mind body problem. A good example of this concept is the hole that defines the hub of a wagon wheel. The hole itself is not a physical thing, but rather a source of constraint that helps to restrict the conformational possibilities of the wheel's components, such that, on a global scale, the property of rolling emerges. Constraints which produce emergent phenomena may not be a process which can be understood by looking at the make-up of the constituents of a pattern. Emergent phenomena are difficult to study because their complexity does not necessarily decompose into parts. When a pattern is broken down, the constraints are no longer at work; there is no hole, no absence to notice. Imagine a hub, a hole for an axel, produced only when the wheel is rolling, thus breaking the wheel may not show you how the hub emerges.\n\nDeacon notes that the apparent patterns of causality exhibited by living systems seem to be in some ways the inverse of the causal patterns of non-living systems. In an attempt to find a solution to the philosophical problems associated with teleological explanations, Deacon returns to Aristotle's four causes and attempts to modernize them with thermodynamic concepts.\n\nOrthograde changes are caused internally. They are spontaneous changes. That is, orthograde changes are generated by the spontaneous elimination of asymmetries in a thermodynamic system in disequilibrium. Because orthograde changes are driven by the internal geometry of a changing system, orthograde causes can be seen as analogous to Aristotle's formal cause. More loosely, Aristotle's final cause can also be considered orthograde, because goal oriented actions are caused from within.\n\nContragrade changes are imposed from the outside. They are non-spontaneous changes. Contragrade change is induced when one thermodynamic system interacts with the orthograde changes of another thermodynamic system. The interaction drives the first system into a higher energy, more asymmetrical state. Contragrade changes do work. Because contragrade changes are driven by external interactions with another changing system, contragrade causes can be seen as analogous to Aristotle's efficient cause.\n\nMuch of the book is devoted to expanding upon the ideas of classical thermodynamics, with an extended discussion about how consistently far from equilibrium systems can interact and combine to produce novel emergent properties. \nDeacon defines three hierarchically nested levels of thermodynamic systems: Homeodynamic systems combine to produce morphodynamic systems which combine to produce teleodynamic systems. Teleodynamic systems can be further combined to produce higher orders of self organization.\n\nHomeodynamic systems are essentially equivalent to classical thermodynamic systems like a gas under pressure or solute in solution, but the term serves to emphasize that homeodynamics is an abstract process that can be realized in forms beyond the scope of classic thermodynamics. For example, the diffuse brain activity normally associated with emotional states can be considered to be a homeodynamic system because there is a general state of equilibrium which its components (neural activity) distribute towards. In general, a homeodynamic system is any collection of components that will spontaneously eliminate constraints by rearranging the parts until a maximum entropy state (disorderliness) is achieved.\n\nA morphodynamic system consists of a coupling of two homeodynamic systems such that the constraint dissipation of each complements the other, producing macroscopic order out of microscopic interactions. Morphodynamic systems require constant perturbation to maintain their structure, so they are relatively rare in nature. The paradigm example of a morphodynamic system is a Rayleigh–Bénard cell. Other common examples are snowflake formation, whirlpools and the stimulated emission of laser light.\nMaximum entropy production: The organized structure of a morphodynamic system forms to facilitate maximal entropy production. In the case of a Rayleigh–Bénard cell, heat at the base of the liquid produces an uneven distribution of high energy molecules which will tend to diffuse towards the surface. As the temperature of the heat source increases, density effects come into play. Simple diffusion can no longer dissipate energy as fast as it is added and so the bottom of the liquid becomes hot and more buoyant than the cooler, denser liquid at the top. The bottom of the liquid begins to rise, and the top begins to sink - producing convection currents.\n\nTwo systems: The significant heat differential on the liquid produces two homeodynamic systems. The first is a diffusion system, where high energy molecules on the bottom collide with lower energy molecules on the top until the added kinetic energy from the heat source is evenly distributed. The second is a convection system, where the low density fluid on the bottom mixes with the high density fluid on the top until the density becomes evenly distributed. The second system arises when there is too much energy to be effectively dissipated by the first, and once both systems are in place, they will begin to interact.\n\nSelf organization: The convection creates currents in the fluid that disrupt the pattern of heat diffusion from bottom to top. Heat begins to diffuse into the denser areas of current, irrespective of the vertical location of these denser portions of fluid. The areas of the fluid where diffusion is occurring most rapidly will be the most viscous because molecules are rubbing against each other in opposite directions. The convection currents will shun these areas in favor of parts of the fluid where they can flow more easily. And so the fluid spontaneously segregates itself into cells where high energy, low density fluid flows up from the center of the cell and cooler, denser fluid flows down along the edges, with diffusion effects dominating in the area between the center and the edge of each cell.\n\nSynergy and constraint: What is notable about morphodynamic processes is that order spontaneously emerges explicitly because the ordered system that results is more efficient at increasing entropy than a chaotic one. In the case of the Rayleigh–Bénard cell, neither diffusion nor convection on their own will produce as much entropy as both effects coupled together. When both effects are brought into interaction, they constrain each other into a particular geometric form because that form facilitates minimal interference between the two processes. The orderly hexagonal form is stable as long as the energy differential persists, and yet the orderly form more effectively degrades the energy differential than any other form. This is why morphodynamic processes in nature are usually so short lived. They are self organizing, but also self undermining.\n\nA teleodynamic system consists of coupling two morphodynamic systems such that the self undermining quality of each is constrained by the other. Each system prevents the other from dissipating all of the energy available, and so long term organizational stability is obtained. Deacon claims that we should pinpoint the moment when two morphodynamic systems reciprocally constrain each other as the point when ententional qualities like function, purpose and normativity emerge.\n\nDeacon explores the properties of teleodynamic systems by describing a chemically plausible model system called an autogen. Deacon emphasizes that the specific autogen he describes is not a proposed description of the first life form, but rather a description of the kinds of thermodynamic synergies that the first living creature likely possessed.\n\nReciprocal catalysis: An autogen consists of two self catalyzing cyclical morphodynamic chemical reactions, similar to a chemoton. In one reaction, organic molecules react in a looped series, the products of one reaction becoming the reactants for the next. This looped reaction is self amplifying, producing more and more reactants until all the substrate is consumed. A side product of this reciprocally catalytic loop is a lipid that can be used as a reactant in a second reaction. This second reaction creates a boundary (either a microtubule or some other closed capsid like structure), that serves to contain the first reaction. The boundary limits diffusion; it keeps all of the necessary catalysts in close proximity to each other. In addition, the boundary prevents the first reaction from completely consuming all of the available substrate in the environment.\n\nThe first self: Unlike an isolated morphodynamic process whose organization rapidly eliminates the energy gradient necessary to maintain its structure, a teleodynamic process is self-limiting and self preserving. The two reactions complement each other, and ensure that neither ever runs to equilibrium - that is completion, cessation, and death. So, in a teleodynamic system there will be structures that embody a preliminary sketch of a biological function. The internal reaction network functions to create the substrates for the boundary reaction, and the boundary reaction functions to protect and constrain the internal reaction network. Either process in isolation would be abiotic but together they create a system with a normative status dependent on the functioning of its component parts.\n\nAs with other concepts in the book, in his discussion of work Deacon seeks to generalize the Newtonian conception of work such that the term can be used to describe and differentiate mental phenomena - to describe \"that which makes daydreaming effortless but metabolically equivalent problem solving difficult.\" Work is generally described as \"activity that is necessary to overcome resistance to change. Resistance can be either active or passive, and so work can be directed towards enacting change that wouldn't otherwise occur or preventing change that would happen in its absence.\" Using the terminology developed earlier in the book, work can be considered to be \"the organization of differences between orthograde processes such that a locus of contragrade process is created. Or, more simply, work is a spontaneous change inducing a non-spontaneous change to occur.\"\n\nA thermodynamic systems capacity to do work depends less upon the total energy of the system and more upon the geometric distribution of its components. A glass of water at 20 degrees Celsius will have the same amount of energy as a glass divided in half with the top fluid at 30 degrees and the bottom at 10, but only in the second glass will the top half have the capacity to do work upon the bottom. This is because work occurs at both macroscopic and microscopic levels. Microscopically, there is constant work being performed on one molecule by another when they collide. But the potential for this microscopic work to additively sum to macroscopic work depends on there being an asymmetric distribution of particle speeds, so that the average collision pushes in a focused direction. Microscopic work is necessary but not sufficient for macroscopic work. A global property of asymmetric distribution is also required.\n\nBy recognizing that asymmetry is a general property of work - that work is done as asymmetric systems spontaneously tend towards symmetry, Deacon abstracts the concept of work and applies it to systems whose symmetries are vastly more complex than those covered by classical thermodynamics. In a morphodynamic system, the tendency towards symmetry produces not global equilibrium, but a complex geometric form like a hexagonal Benard cell or the resonant frequency of a flute. This tendency towards convolutedly symmetric forms can be harnessed to do work on other morphodynamic systems, if the systems are properly coupled.\n\nResonance example: A good example of morphodynamic work is the induced resonance that can be observed by singing or playing a flute next to a string instrument like a harp or guitar. The vibrating air emitted from the flute will interact with the taut strings. If any of the strings are tuned to a resonant frequency that matches the note being played, they too will begin to vibrate and emit sound.\n\nContragrade change: When energy is added to the flute by blowing air into it, there is a spontaneous (orthograde) tendency for the system to dissipate the added energy by inducing the air within the flute to vibrate at a specific frequency. This orthograde morphodynamic form generation can be used to induce contragrade change in the system coupled to it - the taught string. Playing the flute does work on the string by causing it to enter a high energy state that could not be reached spontaneously in an uncoupled state.\n\nStructure and form: Importantly, this is not just the macro scale propagation of random micro vibrations from one system to another. The global geometric structure of the system is essential. The total energy transferred from the flute to the string matters far less than the patterns it takes in transit. That is, the amplitude of the coupled note is irrelevant, what matters is its frequency. Notes that have a higher or lower frequency than the resonant frequency of the string will not be able to do morphodynamic work.\n\nWork is generally defined to be the interaction of two orthograde changing systems such that contragrade change is produced. In teleodynamic systems, the spontaneous orthograde tendency is not to equilibriate (as in homeodynamic systems), nor to self simplify (as in morphodynamic systems) but rather to tend towards self-preservation. Living organisms spontaneously tend to heal, to reproduce and to pursue resources towards these ends. Teleodynamic work acts on these tendencies and pushes them in a contragrade, non-spontaneous direction. \nEvolution as work: Natural selection, or perhaps more accurately, adaptation, can be considered to be a ubiquitous form of teleodynamic work. The othograde self-preservation and reproduction tendencies of individual organisms tends to undermine those same tendencies in conspecifics. This competition produces a constraint that tends to mold organisms into forms that are more adapted to their environments – forms that would otherwise not spontaneously persist.\n\nFor example, in a population of New Zealand wrybill who make a living by searching for grubs under rocks, those that have a bent beak gain access to more calories. Those with bent beaks are able to better provide for their young, and at the same time they remove a disproportionate quantity of grubs from their environment, making it more difficult for those with strait beaks to provide for their own young. Throughout their lives, all the wrybills in the population do work to structure the form of the next generation. The increased efficiency of the bent beak causes that morphology to dominate the next generation. Thus an asymmetry of beak shape distribution is produced in the population - an asymmetry produced by teleodynamic work.\n\nThought as work: Mental problem solving can also be considered teleodynamic work. Thought forms are spontaneously generated, and task of problem solving is the task of molding those forms to fit the context of the problem at hand. Deacon makes the link between evolution as teleodynamic work and thought as teleodynamic work explicit. \"The experience of being sentient is what it feels like to \"be\" evolution.\"\n\nBy conceiving of work in this way, Deacon claims \"we can begin to discern \"a basis for a form of causal openness\" in the universe.\" While increases in complexity in no way alter the laws of physics, by juxtaposing systems together, pathways of spontaneous change can be made available that were inconceivably improbable prior to the systems coupling. The causal power of any complex living system lies not solely in the underlying quantum mechanics but also in the global arrangement of its components. A careful arrangement of parts can constrain possibilities such that phenomena that were formerly impossibly rare can become improbably common.\n\nOne of the central purposes of Incomplete Nature is to articulate a theory of biological information. The first formal theory of information was articulated by Claude Shannon in 1948 in his work A Mathematical Theory of Communication. Shannon's work is widely credited with ushering in the information age, but somewhat paradoxically, it was completely silent on questions of meaning and reference, i.e., what the information is \"about.\" As an engineer, Shannon was concerned with the challenge of reliably transmitting a message from one location to another. The meaning and content of the message was largely irrelevant. So, while Shannon information theory has been essential for the development of devices like computers, it has left open many philosophical questions regarding the nature of information. Incomplete Nature seeks to answer some of these questions.\n\nShannon's key insight was to recognize a link between entropy and information. Entropy is often defined as a measurement of disorder, or randomness, but this can be misleading. For Shannon's purposes, the entropy of a system is the number of possible states that the system has the capacity to be in. Any one of these potential states can constitute a message. For example, a typewritten page can bear as many different messages as there are different combinations of characters that can be arranged on the page. The information content of a message can only be understood against the background context of all of the messages that could have been sent, but weren't. Information is produced by a reduction of entropy in the message medium. \nShannon's information based conception of entropy should be distinguished from the more classic thermodynamic conception of entropy developed by Ludwig Boltzmann and others at the end of the nineteenth century. While Shannon entropy is static and has to do with the set of all possible messages/states that a signal bearing system might take, Boltzmann entropy has to do with the tendency of all dynamic systems to tend towards equilibrium. That is, there are many more ways for a collection of particles to be well mixed than to be segregated based on velocity, mass, or any other property. Boltzmann entropy is central to the theory of work developed earlier in the book because entropy dictates the direction in which a system will spontaneously tend.\n\nDeacon's addition to Shannon information theory is to propose a method for describing not just how a message is transmitted, but also how it is interpreted. Deacon weaves together Shannon entropy and Boltzmann entropy in order to develop a theory of interpretation based in teleodynamic work. Interpretation is inherently normative. Data becomes information when it has significance for its interpreter. Thus interpretive systems are teleodynamic - the interpretive process is designed to perpetuate itself. \"The interpretation of something as information indirectly reinforces the capacity to do this again.\"\n\n", "id": "41856558", "title": "Incomplete Nature"}
{"url": "https://en.wikipedia.org/wiki?curid=24959", "text": "Proteinoid\n\nProteinoids, or thermal proteins, are protein-like, often cross-linked molecules formed abiotically from amino acids. Sidney W. Fox initially proposed that they may have been precursors to the first living cells (protocells). The term was also in the 1960s to describe peptides that are shorter than twenty amino acids found in hydrolysed protein, but this term is no longer commonly used.\n\nIn trying to uncover the intermediate stages of abiogenesis, scientist Sidney W. Fox in the 1950s and 1960s, studied the spontaneous formation of peptide structures under conditions that might plausibly have existed early in Earth's history. He demonstrated that amino acids could spontaneously form small chains called peptides. In one of his experiments, he allowed amino acids to dry out as if puddled in a warm, dry spot in prebiotic conditions. He found that, as they dried, the amino acids formed long, often cross-linked, thread-like microscopic polypeptide globules, he named \"proteinoid microspheres\".\n\nThe abiotic polymerization of amino acids into proteins through the formation of peptide bonds was thought to occur only at temperatures over 140 °C. However, the biochemist Sidney Walter Fox and his co-workers discovered that phosphoric acid acted as a catalyst for this reaction. They were able to form protein-like chains from a mixture of 18 common amino acids at 70 °C in the presence of phosphoric acid, and dubbed these protein-like chains proteinoids. Fox later found naturally occurring proteinoids similar to those he had created in his laboratory in lava and cinders from Hawaiian volcanic vents and determined that the amino acids present polymerized due to the heat of escaping gases and lava. Other catalysts have since been found; one of them, amidinium carbodiimide, is formed in primitive Earth experiments and is effective in dilute aqueous solutions.\n\nWhen present in certain concentrations in aqueous solutions, proteinoids form small microspheres. This is because some of the amino acids incorporated into proteinoid chains are more hydrophobic than others, and so proteinoids cluster together like droplets of oil in water. These structures exhibit a few characteristics of living cells:\n\n\nFox thought that the microspheres may have provided a cell compartment within which organic molecules could have become concentrated and protected from the outside environment during the process of chemical evolution.\n\nProteinoid microspheres are today being considered for use in pharmaceuticals, providing microscopic biodegradable capsules in which to package and deliver oral drugs.\n\nIn another experiment using a similar method to set suitable conditions for life to form, Fox collected volcanic material from a cinder cone in Hawaii. He discovered that the temperature was over just beneath the surface of the cinder cone, and suggested that this might have been the environment in which life was created—molecules could have formed and then been washed through the loose volcanic ash and into the sea. He placed lumps of lava over amino acids derived from methane, ammonia and water, sterilized all materials, and baked the lava over the amino acids for a few hours in a glass oven. A brown, sticky substance formed over the surface and when the lava was drenched in sterilized water a thick, brown liquid leached out. It turned out that the amino acids had combined to form proteinoids, and the proteinoids had combined to form small spheres. Fox called these \"microspheres\". His protobionts were not cells, although they formed clumps and chains reminiscent of bacteria. Based upon such experiments, Colin Pittendrigh stated in December 1967 that \"laboratories will be creating a living cell within ten years,\" a remark that reflected the typical contemporary levels of ignorance of the complexity of cell structures.\n\nFox has likened the amino acid globules to cells, and proposed it bridged the macromolecule to cell transition. However, his hypothesis was later dismissed as proteinoids are not proteins, they feature mostly non-peptide bonds and amino acid cross-linkages not present in living organisms. Furthermore, they have no compartmentalization and there is no information content in the molecules.\n\nAlthough their role as an evolutionary precursor has been superseded, the hypothesis was a catalyst to further investigate other mechanisms that could have brought about abiogenesis, such as the RNA world, PAH world, Iron–sulfur world, and protocell hypotheses.\n\n", "id": "24959", "title": "Proteinoid"}
{"url": "https://en.wikipedia.org/wiki?curid=25765", "text": "RNA world\n\nThe RNA world is a hypothetical stage in the evolutionary history of life on Earth, in which self-replicating RNA molecules proliferated before the evolution of DNA and proteins. The term also refers to the hypothesis that posits the existence of this stage.\n\nThe concept of the RNA world was first proposed in 1962 by Alexander Rich, and the term was coined by Walter Gilbert in 1986. Alternative chemical paths to life have been proposed, and RNA-based life may not have been the first life to exist. Even so, the evidence for an RNA world is strong enough that the hypothesis has gained wide acceptance.\n\nLike DNA, RNA can store and replicate genetic information; like enzymes, it can catalyze (start or accelerate) chemical reactions that are critical for life (then called Ribozyme). One of the most critical components of the cell, ribosome, is composed primarily of RNA. Ribonucleotide moieties in many coenzymes, such as Acetyl-CoA, NADH, FADH and F420, have long been thought to be surviving remnants of covalently bound coenzymes in an RNA world.\n\nAlthough RNA is fragile, some ancient RNAs may have evolved the ability to methylate other RNAs to protect them.\n\nIf the RNA world existed, it was probably followed by an age characterized by the evolution of ribonucleoproteins (RNP world), which in turn ushered in the era of DNA and longer proteins. The reason why DNA became the predominant storage molecule may be because it is more stable and durable than RNA. Protein enzymes may have come to replace RNA-based ribozymes as biocatalysts because their greater abundance and diversity of monomers makes them more versatile. As some co-factors contain both nucleotide and amino acid characteristics, it may be that amino acids, peptides and finally proteins initially were co-factors for ribozymes.\n\nOne of the challenges in studying abiogenesis is that the system of reproduction and metabolism utilized by all extant life involves three distinct types of interdependent macromolecules (DNA, RNA, and protein). This suggests that life could not have arisen in its current form, and mechanisms have then been sought whereby the current system might have arisen from a simpler precursor system. The concept of RNA as a primordial molecule can be found in papers by Francis Crick and Leslie Orgel, as well as in Carl Woese's 1967 book \"The Genetic Code\". In 1962, the molecular biologist Alexander Rich had posited much the same idea in an article he contributed to a volume issued in honor of Nobel-laureate physiologist Albert Szent-Györgyi. Hans Kuhn in 1972 laid out a possible process by which the modern genetic system might have arisen from a nucleotide-based precursor, and this led Harold White in 1976 to observe that many of the cofactors essential for enzymatic function are either nucleotides or could have been derived from nucleotides. He proposed that these nucleotide cofactors represent \"fossils of nucleic acid enzymes\". The phrase \"RNA World\" was first used by Nobel laureate Walter Gilbert in 1986, in a commentary on how recent observations of the catalytic properties of various forms of RNA fit with this hypothesis.\n\nThe properties of RNA make the idea of the RNA world hypothesis conceptually plausible, though its general acceptance as an explanation for the origin of life requires further evidence. RNA is known to form efficient catalysts and its similarity to DNA makes clear its ability to store information. Opinions differ, however, as to whether RNA constituted the first autonomous self-replicating system or was a derivative of a still-earlier system. One version of the hypothesis is that a different type of nucleic acid, termed \"pre-RNA\", was the first one to emerge as a self-reproducing molecule, to be replaced by RNA only later. On the other hand, the recent finding that activated pyrimidine ribonucleotides can be synthesized under plausible prebiotic conditions means that it is premature to dismiss the RNA-first scenarios. Suggestions for 'simple' \"pre-RNA\" nucleic acids have included peptide nucleic acid (PNA), threose nucleic acid (TNA) or glycol nucleic acid (GNA). Despite their structural simplicity and possession of properties comparable with RNA, the chemically plausible generation of \"simpler\" nucleic acids under prebiotic conditions has yet to be demonstrated.\n\nRNA enzymes, or ribozymes, are found in today's DNA-based life and could be examples of living fossils. Ribozymes play vital roles, such as those in the ribosome, which is vital for protein synthesis. Many other ribozyme functions exist; for example, the hammerhead ribozyme performs self-cleavage and an RNA polymerase ribozyme can synthesize a short RNA strand from a primed RNA template.\n\nAmong the enzymatic properties important for the beginning of life are:\n\nRNA is a very similar molecule to DNA, and only has two chemical differences. The overall structure of RNA and DNA are immensely similar—one strand of DNA and one of RNA can bind to form a double helical structure. This makes the storage of information in RNA possible in a very similar way to the storage of information in DNA. However, RNA is less stable.\n\nThe major difference between RNA and DNA is the presence of a hydroxyl group at the 2'-position of the ribose sugar in RNA (illustration, right). This group makes the molecule less stable because, when not constrained in a double helix, the 2' hydroxyl can chemically attack the adjacent phosphodiester bond to cleave the phosphodiester backbone. The hydroxyl group also forces the ribose into the C3'-\"endo\" sugar conformation unlike the C2'-\"endo\" conformation of the deoxyribose sugar in DNA. This forces an RNA double helix to change from a B-DNA structure to one more closely resembling A-DNA.\n\nRNA also uses a different set of bases than DNA—adenine, guanine, cytosine and uracil, instead of adenine, guanine, cytosine and thymine. Chemically, uracil is similar to thymine, differing only by a methyl group, and its production requires less energy. In terms of base pairing, this has no effect. Adenine readily binds uracil or thymine. Uracil is, however, one product of damage to cytosine that makes RNA particularly susceptible to mutations that can replace a GC base pair with a GU (wobble) or AU base pair.\n\nRNA is thought to have preceded DNA, because of their ordering in the biosynthetic pathways. The deoxyribonucleotides used to make DNA are made from ribonucleotides, the building blocks of RNA, by removing the 2'-hydroxyl group. As a consequence a cell must have the ability to make RNA before it can make DNA.\n\nThe chemical properties of RNA make large RNA molecules inherently fragile, and they can easily be broken down into their constituent nucleotides through hydrolysis. These limitations do not make use of RNA as an information storage system impossible, simply energy intensive (to repair or replace damaged RNA molecules) and prone to mutation. While this makes it unsuitable for current 'DNA optimised' life, it may have been acceptable for more primitive life.\n\nRiboswitches have been found to act as regulators of gene expression, particularly in bacteria, but also in plants and archaea. Riboswitches alter their secondary structure in response to the binding of a metabolite. This change in structure can result in the formation or disruption of a terminator, truncating or permitting transcription respectively. Alternatively, riboswitches may bind or occlude the Shine-Dalgarno sequence, affecting translation. It has been suggested that these originated in an RNA-based world. In addition, RNA thermometers regulate gene expression in response to temperature changes.\n\nThe RNA world hypothesis is supported by RNA's ability to store, transmit, and duplicate genetic information, as DNA does. RNA can act as a ribozyme, a special type of enzyme. Because it can perform the tasks of both DNA and enzymes, RNA is believed to have once been capable of supporting independent life forms. Some viruses use RNA as their genetic material, rather than DNA. Further, while nucleotides were not found in experiments based on Miller-Urey experiment, their formation in prebiotically plausible conditions has now been reported, as noted above; the purine base known as adenine is merely a pentamer of hydrogen cyanide. Experiments with basic ribozymes, like Bacteriophage Qβ RNA, have shown that simple self-replicating RNA structures can withstand even strong selective pressures (e.g., opposite-chirality chain terminators).\n\nSince there were no known chemical pathways for the abiogenic synthesis of nucleotides from pyrimidine nucleobases cytosine and uracil under prebiotic conditions, it is thought by some that nucleic acids did not contain these nucleobases seen in life's nucleic acids. The nucleoside cytosine has a half-life in isolation of 19 days at and 17,000 years in freezing water, which some argue is too short on the geologic time scale for accumulation. Others have questioned whether ribose and other backbone sugars could be stable enough to find in the original genetic material, and have raised the issue that all ribose molecules would have had to be the same enantiomer, as any nucleotide of the wrong chirality acts as a chain terminator.\n\nPyrimidine ribonucleosides and their respective nucleotides have been prebiotically synthesised by a sequence of reactions that by-pass free sugars and assemble in a stepwise fashion by including nitrogenous and oxygenous chemistries. In a series of publications, John Sutherland and his team at the School of Chemistry, University of Manchester, have demonstrated high yielding routes to cytidine and uridine ribonucleotides built from small 2 and 3 carbon fragments such as glycolaldehyde, glyceraldehyde or glyceraldehyde-3-phosphate, cyanamide and cyanoacetylene. One of the steps in this sequence allows the isolation of enantiopure ribose aminooxazoline if the enantiomeric excess of glyceraldehyde is 60% or greater, of possible interest towards biological homochirality. This can be viewed as a prebiotic purification step, where the said compound spontaneously crystallised out from a mixture of the other pentose aminooxazolines. Aminooxazolines can react with cyanoacetylene in a mild and highly efficient manner, controlled by inorganic phosphate, to give the cytidine ribonucleotides. Photoanomerization with UV light allows for inversion about the 1' anomeric centre to give the correct beta stereochemistry; one problem with this chemistry is the selective phosphorylation of alpha-cytidine at the 2' position. However, in 2009, they showed that the same simple building blocks allow access, via phosphate controlled nucleobase elaboration, to 2',3'-cyclic pyrimidine nucleotides directly, which are known to be able to polymerise into RNA. Organic chemist Donna Blackmond described this finding as \"strong evidence\" in favour of the RNA world. However, John Sutherland said that while his team's work suggests that nucleic acids played an early and central role in the origin of life, it did not necessarily support the RNA world hypothesis in the strict sense, which he described as a \"restrictive, hypothetical arrangement\".\n\nThe Sutherland group's 2009 paper also highlighted the possibility for the photo-sanitization of the pyrimidine-2',3'-cyclic phosphates. A potential weakness of these routes is the generation of enantioenriched glyceraldehyde, or its 3-phosphate derivative (glyceraldehyde prefers to exist as its keto tautomer dihydroxyacetone).\n\nOn August 8, 2011, a report, based on NASA studies with meteorites found on Earth, was published suggesting building blocks of RNA (adenine, guanine and related organic molecules) may have been formed extraterrestrially in outer space. A recent numerical model suggests that the RNA world may have emerged in warm ponds on the early Earth, and that meteorites were a plausible and probable source of the RNA building blocks (ribose and nucleic acids) to these environments. On August 29, 2012, astronomers at Copenhagen University reported the detection of a specific sugar molecule, glycolaldehyde, in a distant star system. The molecule was found around the protostellar binary \"IRAS 16293-2422\", which is located 400 light years from Earth. Because glycolaldehyde is needed to form RNA, this finding suggests that complex organic molecules may form in stellar systems prior to the formation of planets, eventually arriving on young planets early in their formation.\n\nNucleotides are the fundamental molecules that combine in series to form RNA. They consist of a nitrogenous base attached to a sugar-phosphate backbone. RNA is made of long stretches of specific nucleotides arranged so that their sequence of bases carries information. The RNA world hypothesis holds that in the primordial soup (or sandwich), there existed free-floating nucleotides. These nucleotides regularly formed bonds with one another, which often broke because the change in energy was so low. However, certain sequences of base pairs have catalytic properties that lower the energy of their chain being created, enabling them to stay together for longer periods of time. As each chain grew longer, it attracted more matching nucleotides faster, causing chains to now form faster than they were breaking down.\n\nThese chains have been proposed by some as the first, primitive forms of life. In an RNA world, different sets of RNA strands would have had different replication outputs, which would have increased or decreased their frequency in the population, i.e. natural selection. As the fittest sets of RNA molecules expanded their numbers, novel catalytic properties added by mutation, which benefitted their persistence and expansion, could accumulate in the population. Such an autocatalytic set of ribozymes, capable of self replication in about an hour, has been identified. It was produced by molecular competition (\"in vitro\" evolution) of candidate enzyme mixtures.\n\nCompetition between RNA may have favored the emergence of cooperation between different RNA chains, opening the way for the formation of the first protocell. Eventually, RNA chains developed with catalytic properties that help amino acids bind together (a process called peptide-bonding). These amino acids could then assist with RNA synthesis, giving those RNA chains that could serve as ribozymes the selective advantage. The ability to catalyze one step in protein synthesis, aminoacylation of RNA, has been demonstrated in a short (five-nucleotide) segment of RNA.\n\nOne of the problems with the RNA world hypothesis is to discover the pathway by which RNA became upgraded to the DNA system. Geoffrey Diemer and Ken Stedman, at Portland State University in Oregon, may have found a solution. While conducting a survey of viruses in a hot acidic lake in Lassen Volcanic National Park, California, they uncovered evidence that a simple DNA virus had acquired a gene from a completely unrelated RNA-based virus. Virologist Luis Villareal of the University of California Irvine also suggests that viruses capable of converting an RNA-based gene into DNA and then incorporating it into a more complex DNA-based genome might have been common in the Virus world during the RNA to DNA transition some 4 billion years ago. This finding bolsters the argument for the transfer of information from the RNA world to the emerging DNA world before the emergence of the last universal common ancestor. From the research, the diversity of this virus world is still with us.\n\nIn March 2015, NASA scientists reported that, for the first time, complex DNA and RNA organic compounds of life, including uracil, cytosine and thymine, have been formed in the laboratory under conditions found only in outer space, using starting chemicals, like pyrimidine, found in meteorites. Pyrimidine, like polycyclic aromatic hydrocarbons (PAHs), may have been formed in giant red stars or in interstellar dust and gas clouds, according to the scientists.\n\nAdditional evidence supporting the concept of an RNA world has resulted from research on viroids, the first representatives of a novel domain of \"subviral pathogens\".\nViroids are mostly plant pathogens, which consist of short stretches (a few hundred nucleobases) of highly complementary, circular, single-stranded, and non-coding RNA without a protein coat. Compared with other infectious plant pathogens, viroids are extremely small in size, ranging from 246 to 467 nucleobases. In comparison, the genome of the smallest known viruses capable of causing an infection are about 2,000 nucleobases long.\n\nIn 1989, Diener proposed that, based on their characteristic properties, viroids are more plausible \"living relics\" of the RNA world than are introns or other RNAs then so considered. If so, viroids have attained potential significance beyond plant pathology to evolutionary biology, by representing the most plausible macromolecules known capable of explaining crucial intermediate steps in the evolution of life from inanimate matter (see: abiogenesis).\n\nApparently, Diener's hypothesis lay dormant until 2014, when Flores et al. published a review paper, in which Diener's evidence supporting his hypothesis was summarized. In the same year, a New York Times science writer published a popularized version of Diener's proposal, in which, however, he mistakenly credited Flores et al. with the hypothesis' original conception.\n\nPertinent viroid properties listed in 1989 are:\n\n\nThe existence, in extant cells, of RNAs with molecular properties predicted for RNAs of the RNA World constitutes an additional argument supporting the RNA World hypothesis.\n\nEigen \"et al\". and Woese proposed that the genomes of early protocells were composed of single-stranded RNA, and that individual genes corresponded to separate RNA segments, rather than being linked end-to-end as in present-day DNA genomes. A protocell that was haploid (one copy of each RNA gene) would be vulnerable to damage, since a single lesion in any RNA segment would be potentially lethal to the protocell (e.g. by blocking replication or inhibiting the function of an essential gene).\n\nVulnerability to damage could be reduced by maintaining two or more copies of each RNA segment in each protocell, i.e. by maintaining diploidy or polyploidy. Genome redundancy would allow a damaged RNA segment to be replaced by an additional replication of its homolog. However, for such a simple organism, the proportion of available resources tied up in the genetic material would be a large fraction of the total resource budget. Under limited resource conditions, the protocell reproductive rate would likely be inversely related to ploidy number. The protocell's fitness would be reduced by the costs of redundancy. Consequently, coping with damaged RNA genes while minimizing the costs of redundancy would likely have been a fundamental problem for early protocells.\n\nA cost-benefit analysis was carried out in which the costs of maintaining redundancy were balanced against the costs of genome damage. This analysis led to the conclusion that, under a wide range of circumstances, the selected strategy would be for each protocell to be haploid, but to periodically fuse with another haploid protocell to form a transient diploid. The retention of the haploid state maximizes the growth rate. The periodic fusions permit mutual reactivation of otherwise lethally damaged protocells. If at least one damage-free copy of each RNA gene is present in the transient diploid, viable progeny can be formed. For two, rather than one, viable daughter cells to be produced would require an extra replication of the intact RNA gene homologous to any RNA gene that had been damaged prior to the division of the fused protocell. The cycle of haploid reproduction, with occasional fusion to a transient diploid state, followed by splitting to the haploid state, can be considered to be the sexual cycle in its most primitive form. In the absence of this sexual cycle, haploid protocells with damage in an essential RNA gene would simply die.\n\nThis model for the early sexual cycle is hypothetical, but it is very similar to the known sexual behavior of the segmented RNA viruses, which are among the simplest organisms known. Influenza virus, whose genome consists of 8 physically separated single-stranded RNA segments, is an example of this type of virus. In segmented RNA viruses, “mating” can occur when a host cell is infected by at least two virus particles. If these viruses each contain an RNA segment with a lethal damage, multiple infection can lead to reactivation providing that at least one undamaged copy of each virus gene is present in the infected cell. This phenomenon is known as “multiplicity reactivation”. Multiplicity reactivation has been reported to occur in influenza virus infections after induction of RNA damage by UV-irradiation, and ionizing radiation.\n\nPatrick Forterre has been working on a novel hypothesis, called \"three viruses, three domains\": that viruses were instrumental in the transition from RNA to DNA and the evolution of Bacteria, Archaea, and Eukaryota. He believes the last common ancestor (specifically, the \"last universal cellular ancestor\") was RNA-based and evolved RNA viruses. Some of the viruses evolved into DNA viruses to protect their genes from attack. Through the process of viral infection into hosts the three domains of life evolved. Another interesting proposal is the idea that RNA synthesis might have been driven by temperature gradients, in the process of thermosynthesis.\nSingle nucleotides have been shown to catalyze organic reactions.\n\nSteven Benner has argued that chemical conditions on the planet Mars, such as the presence of boron, molybdenum and oxygen, may have been better for initially producing RNA molecules than those on Earth. If so, life-suitable molecules, originating on Mars, may have later migrated to Earth via panspermia or similar process.\n\nThe hypothesized existence of an RNA world does not exclude a \"Pre-RNA world\", where a metabolic system based on a different nucleic acid is proposed to pre-date RNA. A candidate nucleic acid is peptide nucleic acid (PNA), which uses simple peptide bonds to link nucleobases. PNA is more stable than RNA, but its ability to be generated under prebiological conditions has yet to be demonstrated experimentally.\n\nThreose nucleic acid (TNA) has also been proposed as a starting point, as has glycol nucleic acid (GNA), and like PNA, also lack experimental evidence for their respective abiogenesis.\n\nAn alternative — or complementary — theory of RNA origin is proposed in the PAH world hypothesis, whereby polycyclic aromatic hydrocarbons (PAHs) mediate the synthesis of RNA molecules. PAHs are the most common and abundant of the known polyatomic molecules in the visible Universe, and are a likely constituent of the primordial sea. PAHs and fullerenes (also implicated in the origin of life) have been detected in nebulae.\n\nThe iron-sulfur world theory proposes that simple metabolic processes developed before genetic materials did, and these energy-producing cycles catalyzed the production of genes.\n\nSome of the difficulties of producing the precursors on earth are bypassed by another alternative or complementary theory for their origin, panspermia. It discusses the possibility that the earliest life on this planet was carried here from somewhere else in the galaxy, possibly on meteorites similar to the Murchison meteorite. This does not invalidate the concept of an RNA world, but posits that this world or its precursors originated not on Earth but rather another, probably older, planet.\n\nThere are hypotheses that are in direct conflict to the RNA world hypothesis. The relative chemical complexity of the nucleotide and the unlikelihood of it spontaneously arising, along with the limited number of combinations possible among four base forms, as well as the need for RNA polymers of some length before seeing enzymatic activity, have led some to reject the RNA world hypothesis in favor of a metabolism-first hypothesis, where the chemistry underlying cellular function arose first, along with the ability to replicate and facilitate this metabolism.\n\nAnother proposal is that the dual-molecule system we see today, where a nucleotide-based molecule is needed to synthesize protein, and a peptide-based (protein) molecule is needed to make nucleic acid polymers, represents the original form of life. This theory is called RNA-peptide coevolution, or the Peptide-RNA world, and offers a possible explanation for the rapid evolution of high-quality replication in RNA (since proteins are catalysts), with the disadvantage of having to postulate the coincident formation of two complex molecules, an enzyme (from peptides) and a RNA (from nucleotides). In this Peptide-RNA World scenario, RNA would have contained the instructions for life, while peptides (simple protein enzymes) would have accelerated key chemical reactions to carry out those instructions. The study leaves open the question of exactly how those primitive systems managed to replicate themselves — something neither the RNA World hypothesis nor the Peptide-RNA World theory can yet explain, unless polymerases (enzymes that rapidly assemble the RNA molecule) played a role.\n\nA research project completed in March 2015 by the Sutherland group found that a network of reactions beginning with hydrogen cyanide and hydrogen sulfide, in streams of water irradiated by UV light, could produce the chemical components of proteins and lipids, alongside those of RNA. The researchers used the term \"cyanosulfidic\" to describe this network of reactions. In November 2017, a team at the Scripps Research Institute identified reactions involving the compound diamidophosphate which could have linked the chemical components into short peptide and lipid chains as well as short RNA-like chains of nucleotides.\n\nThe RNA world hypothesis, if true, has important implications for the definition of life. For most of the time that followed Watson and Crick's elucidation of DNA structure in 1953, life was largely defined in terms of DNA and proteins: DNA and proteins seemed the dominant macromolecules in the living cell, with RNA only aiding in creating proteins from the DNA blueprint.\n\nThe RNA world hypothesis places RNA at center-stage when life originated. The RNA world hypothesis is supported by the observations that ribosomes are ribozymes: the catalytic site is composed of RNA, and proteins hold no major structural role and are of peripheral functional importance. This was confirmed with the deciphering of the 3-dimensional structure of the ribosome in 2001. Specifically, peptide bond formation, the reaction that binds amino acids together into proteins, is now known to be catalyzed by an adenine residue in the rRNA.\n\nRNAs are known to play roles in other cellular catalytic processes, specifically in the targeting of enzymes to specific RNA sequences. In eukaryotes, the processing of pre-mRNA and RNA editing take place at sites determined by the base pairing between the target RNA and RNA constituents of small nuclear ribonucleoproteins (snRNPs). Such enzyme targeting is also responsible for gene down regulation though RNA interference (RNAi), where an enzyme-associated guide RNA targets specific mRNA for selective destruction. Likewise, in eukaryotes the maintenance of telomeres involves copying of an RNA template that is a constituent part of the telomerase ribonucleoprotein enzyme. Another cellular organelle, the vault, includes a ribonucleoprotein component, although the function of this organelle remains to be elucidated.\n\n\n", "id": "25765", "title": "RNA world"}
{"url": "https://en.wikipedia.org/wiki?curid=43588545", "text": "Henry Quastler\n\nHenry Quastler (November 11, 1908 – July 4, 1963) was an Austrian physician and radiologist who became a pioneer in the field of information theory applied to biology after emigrating to America. His work with Sidney Dancoff led to the publication of what is now commonly called Dancoff's Law. He also developed early versions of the theory of emergence in biology.\n\nQuastler spent his early career in Vienna as a doctor. He received his medical degree in Vienna in 1932, focusing on histology and radiology. He met his wife, Gertrude Quastler, a milliner, when she came to him for treatment for tuberculosis. They married in 1933. The couple moved to Albania when King Zog asked for Quastler to train up radiologists. While there he also worked on malaria. Quastler's malaria expertise earned him a place on the International Health Board. As World War II approached in 1939, the couple left Albania and traveled to America. Within a year Quastler was working as a radiologist at New Rochelle Hospital in New York. In 1942, the Quastlers relocated to Urbana, Illinois, where Henry was employed as chief radiologist at Carle Hospital Clinic. While in Illinois, Gertrude Quastler studied art. She soon became a noted artist. Henry also painted as an amateur. According to his sister Johanna, the couple sometimes exhibited together.\n\nIn 1949 Quastler gave up his medical practice to concentrate on science. Heinz von Foerster, who knew Quastler well, said that he became even more interested in radiation after the invention of the atomic bomb, which he considered \"a horrifying human catastrophe\". Foerster recalled Questler pondering: \"[Quastler asked] 'Can I now, as a working person, find out what damage has been done by the radiation of atomic bombs?'—that was his research question. Thus he started to conduct experiments on radiation damage in living organisms.\" According to Foerster, Quastler came to information theory while trying to find a way to \"qualitatively describe the damage caused by radiation. [...] Henry Quastler learned the basic concepts and formalisms of information theory with a speed that was almost unbelievable. And why? Because he needed this instrument urgently.\". \nIn the 1940s Quastler met Dancoff and collaborated with him to develop information theory in biology. They were interested in the problem of how to define the information content of a gene. After Dancoff's death, Quastler organized the symposium Information Theory in Biology, founded by him in 1952. Quastler soon became interested in how information theory could be used to understand the origin of life. In 1953 he edited \"Essays on the Use of Information Theory in Biology\". He edited another collection of essays, \"Information Theory in Psychology: Problems and Methods\", in 1956.\n\nDespite his best efforts, his wife's tuberculosis slowly worsened. A deterioration in his wife's health led Quastler to take a job at Brookhaven National Laboratory in New York, where he continued to work on both radiation and information biology. When his wife died in 1963, Quastler was devastated. He took an overdose of pills, laid down beside her and held her hand until he died. Richard Diebenkorn later said, “Neither my wife nor I can think of a couple we encountered more indivisible.” Heinz von Foerster said of Quastler that he was \"an exceptionally conscientious, ethically and morally conscious human being.\"\n\nIn Foerster's words Quastler and Dancoff attempted to answer the following problem: \n\nAccording to Lily E. Kay, Quastler and Dancoff created \"the first technical application of the Weiner-Shannon theory in genetics.\" Quastler and Dancoff proposed that the replication errors inevitable in biological reproduction must be held in check by a statistical process that functioned as a \"checking device\" within the gene. Quastler compared this to the system of \"checks and balances\" in the American constitution. The proposition known as \"Dancoff's Law\" emerged from this work. A non-mathematical statement of this law is, \"the greatest growth occurs when the greatest number of mistakes are made consistent with survival\".\n\nIn 1964 Quastler's book \"The Emergence of Biological Organization\" was published posthumously. In 2002, Harold J. Morowitz described it as a \"remarkably prescient book\" which is \"surprisingly contemporary in outlook\". In it Quastler pioneers a theory of emergence, developing model of \"a series of emergences from probionts to prokaryotes\". \n\nThe work is based on lectures given by Quastler during the spring term of 1963, when he was Visiting Professor of Theoretical Biology at Yale University. In these lectures Quastler argued that the formation of single-stranded polynucleotides was well within the limits of probability of what could have occurred during the pre-biologic period of the Earth. However, he noted that polymerization of a single-stranded polymer from mononucleotides is slow, and its hydrolysis is fast; therefore in a closed system consisting only of mononucleotides and their single-stranded polymers, only a small fraction of the available molecules will be polymerized. However, a single-stranded polymer may form a double-stranded one by complementary polymerization, using a single-stranded polynucleotide as a template. Such a process is relatively fast and the resulting double-stranded polynucleotide is much more stable than the single single-stranded one since each monomer is bound not only along the sugar phosphate backbone, but also through inter-strand bonding between the bases.\n\nThe capability for self-replication, a fundamental feature of life, emerged when double-stranded polynucleotides disassociated into single-stranded ones and each of these served as a template for synthesis of a complementary strand, producing two double-stranded copies. Such a system is mutable since random changes of individual bases may occur and be propagated. Individual replicators with different nucleotide sequences may also compete with each other for nucleotide precursors. Mutations that influence the folding state of polynucleotides may affect the ratio of association of strands to dissociation and thus the ability to replicate. The folding state would also affect the stability of the molecule. Thus Quastler speculated that a nucleic acid system is even capable, in a primitive way, of Darwinian evolution. These ideas were then developed to speculate on the emergence of genetic information, protein synthesis and other general features of life.\n\nLily E. Kay says that Quastler's works \"are an illuminating example of a well reasoned epistemic quest and a curious disciplinary failure\". Quastler's aspiration to create an information based biology was innovative, but his work was \"plagued by problems: outdated data, unwarranted assumptions, some dubious numerology, and, most importantly, an inability to generate an experimental agenda.\" However Quastler's \"discursive framework\" survived.\n\nForty-five years after Quastler’s 1964 proposal, Lincoln and Joyce described a cross-catalytic system that involves two RNA enzymes (ribosymes) that catalyze each other’s synthesis from a total of four component substrates. This synthesis occurred in the absence of protein and could provide the basis for an artificial genetic system.\n", "id": "43588545", "title": "Henry Quastler"}
{"url": "https://en.wikipedia.org/wiki?curid=34928636", "text": "Harold J. Morowitz\n\nHarold Joseph Morowitz (December 4, 1927 – March 22, 2016) was an American biophysicist who studied the application of thermodynamics to living systems. Author of numerous books and articles, his work includes technical monographs as well as essays. The origin of life was his primary research interest for more than fifty years. He was the Robinson Professor of Biology and Natural Philosophy at George Mason University after a long career at Yale.\n\nMorowitz was born in Poughkeepsie, New York. He received a B.S. in physics and philosophy in 1947, an M.S. in physics in 1950, and a Ph.D. in biophysics in 1951, all from Yale University. Morowitz was a professor in the department of molecular biophysics and biochemistry at Yale from 1955 to 1987, also serving as the Master of Pierson College from 1981 to 1986. He spent the rest of his career on the faculty at George Mason University, which he joined in 1988 as Clarence Robinson Professor of biology and natural philosophy. He served as the founding director of the Krasnow Institute for Advanced Study at George Mason from 1993 to 1998. Morowitz was closely associated with the Santa Fe Institute since 1987, where he was Chairman Emeritus of the Science Board. He also served as the founding editor of the journal \"Complexity\". In the 1990s he contributed a monthly column on science and society to \"Hospital Practice\".\n\nMorowitz was a longtime consultant for NASA, and served on the committees that planned the quarantine procedures for Apollo 11 and the biology experiments the Viking probe carried to the surface of Mars. He was a member of the science advisory committee for Biosphere 2 in Oracle, Arizona, which, at 3.14 acres, is the largest enclosed ecosystem ever built.\n\nSome leading biophysicists have suggested that Morowitz may have discovered a \"fourth law of thermodynamics\" when, in 1968, he found that, \"in steady state systems, the flow of energy through the system from a source to a sink will lead to at least one cycle in the system.\" Eric D. Schneider, for example, says, \"Morowitz's cycling theorem is the best candidate for a fourth law of thermodynamics.\"\n\nMorowitz's book \"Energy Flow in Biology\" laid out his central thesis that \"the energy that flows through a system acts to organize that system,\" an insight later quoted on the inside front cover of \"The Last Whole Earth Catalog\". He was a vigorous proponent of the view that life on earth emerged deterministically from the laws of chemistry and physics, and so believed it highly probable that life exists widely in the universe.\n\nIn 1983, he testified at \"McLean v. Arkansas\" (nicknamed \"Scopes II\") that creationism has no scientific basis and so should not be taught as science in public schools.\n\n", "id": "34928636", "title": "Harold J. Morowitz"}
{"url": "https://en.wikipedia.org/wiki?curid=538810", "text": "Leslie Orgel\n\nLeslie Eleazer Orgel FRS (12 January 1927 – 27 October 2007) was a British chemist. He is known for his theories on the origin of life.\n\nBorn in London, England, Orgel received his Bachelor of Arts degree in chemistry with first class honours from the University of Oxford in 1948. In 1951 he was elected a Fellow of Magdalen College, Oxford and in 1953 was awarded his PhD. in chemistry.\n\nOrgel started his career as a theoretical inorganic chemist and continued his studies in this field at Oxford, the California Institute of Technology and the University of Chicago.\n\nTogether with Sydney Brenner, Jack Dunitz, Dorothy Hodgkin, and Beryl M. Oughton he was one of the first people in April 1953 to see the model of the structure of DNA, constructed by Francis Crick and James Watson, at the time he and the other scientists were working at Oxford University's Chemistry Department. According to the late Dr. Beryl Oughton, later Rimmer, they all travelled together in two cars once Dorothy Hodgkin announced to them that they were off to Cambridge to see the model of the structure of DNA. All were impressed by the new DNA model, especially Brenner who subsequently worked with Crick; Orgel himself also worked with Crick at the Salk Institute for Biological Studies.\n\nIn 1955 he joined the chemistry department at Cambridge University. There he did work in transition metal chemistry, published several peer-reviewed journal articles and wrote a textbook entitled \"Transition Metal Chemistry: Ligand Field Theory\" (1960).\n\nOrgel formulated his error catastrophe theory of ageing in 1963, which has since been since experimentally refuted.\n\nIn 1964, Orgel was appointed Senior Fellow and Research Professor at the Salk Institute for Biological Studies in La Jolla, California, where he directed the Chemical Evolution Laboratory. He was also an adjunct professor in the Department of Chemistry and Biochemistry at the University of California, San Diego, and he was one of five principal investigators in the NASA-sponsored NSCORT program in exobiology. Orgel also participated in NASA's Viking Mars Lander Program as a member of the Molecular Analysis Team that designed the gas chromatography mass spectrometer instrument that robots took to the planet Mars.\n\nOrgel's lab came across an economical way to make cytarabine, a compound that is one of today's most commonly used anti-cancer agents.\n\nTogether with Stanley Miller, Orgel also suggested that peptide nucleic acids – rather than ribonucleic acids – constituted the first pre-biotic systems capable of self-replication on early Earth.\n\nHis name is popularly known because of Orgel's rules, credited to him, particularly Orgel's Second Rule: \"Evolution is cleverer than you are.\" \n\nIn his book \"The Origins of Life\", Orgel coined the concept of specified complexity, to describe the criterion by which living organisms are distinguished from non-living matter. He published over three hundred articles in his research areas.\n\nIn 1993, Orgel presented at the \"What is Life?\" Conference at Trinity College in Dublin, Ireland along with many other prominent scientists exploring origin of life research such as Manfred Eigen, John Maynard Smith and Stephen Jay Gould. Orgel's talk was on \"Molecular Structure and Disordered Crystals.\"\n\nOrgel died of pancreatic cancer on 27 October 2007 at the San Diego Hospice & Palliative Care in San Diego.\n\nOrgel proposed a novel solution to a problem with Juan Oro's proposed mechanism of nucleobase synthesis on the early Earth, which relied on the reaction of five molecules of hydrogen cyanide (HCN) to form adenine. The problem with this was that it would require much more concentrated hydrogen cyanide than evidence suggested was present.\n\nOrgel suggested that the hydrogen cyanide was frozen in solution. This would concentrate HCN molecules in the spaces in between the crystal lattice of ice, and also solved the problem of HCN being too volatile in a liquid water solution.\n\nFor nucleoside (nucleobase + ribose sugar) synthesis, Orgel suggested an almost opposite approach, heating a mixture of ribose and the purine nucleobases hypoxanthine, adenine, and guanine to dryness in the presence of magnesium ions. This reaction puts the glycosidic bond in the correct position in two ways: the nucleobase attaches to the correct carbon on ribose, and in the correct orientation (the beta anomer).\n\nHowever, the synthesis was later criticised because it only worked most with hypoxanthine, a nucleobase that is not relevant to current life on Earth, and because it was not specific for the ribose sugar and could instead be applied to other sugars.\n\nContinuing his work exploring the prebiotic synthesis of RNA, Orgel suggested a solution to the problem of condensing nucleotides to form nucleic acid polymers, an energy-demanding process. To counteract this energy barrier, he proposed a nucleotide with an imidazole ring attached to the phosphate group. The imidazole would be a good leaving group for the condensation of nucleotides.\n\nOrgel also theorised that one single strand of RNA could have been the template for the first life on Earth and that these imidazole-activated nucleotides could have used this DNA template strand to polymerise and replicate. However, in lab experiments, this only worked when the DNA template strands were rich in the nucleotide cytosine, and furthermore, there so far is no proposed prebiotic synthesis for how an imidazole-activated nucleotide could be formed on the early Earth.\n\nThough he later downplayed the hypothesis, Orgel, along with Francis Crick, proposed a detailed panspermia scenario for the origin of life on Earth, going so far as to suggest that life on Earth was designed by an alien species and sent to Earth. They proposed a design for the spaceship that aliens could have used to seed life on Earth.\n\nIn the late 1960s, Orgel proposed that life was based on RNA before it was based on DNA or proteins. His theory included genes based on RNA and RNA enzymes. This view would be developed and shaped into the now widely-accepted RNA world hypothesis.\n\nAlmost thirty years later, Orgel wrote a lengthy review of the RNA World hypothesis. This review highlighted many proposed syntheses for RNA and its parts in abiotic conditions, noted the significance of the discovery of ribozymes (RNA molecules that function as enzymes just as Orgel had once predicted) and at the same time, demonstrated nucleic acid polymers with alternatives to ribose such as threose nucleic acid (TNA) and peptide nucleic acid (PNA).\n\nIn conclusion, Orgel wrote, \"One must recognize that, despite considerable progress, the problem of the origin of the RNA World is far from being solved.\"\n\n\n\n", "id": "538810", "title": "Leslie Orgel"}
{"url": "https://en.wikipedia.org/wiki?curid=92419", "text": "Stanley Miller\n\nStanley Lloyd Miller (March 7, 1930 – May 20, 2007) was an American chemist who made landmark experiments in the origin of life by demonstrating that a wide range of vital organic compounds can be synthesized by fairly simple chemical processes from inorganic substances. In 1952 he carried out the Miller–Urey experiment, which showed that complex organic molecules could be synthesised from inorganic precursors. The experiment was widely reported, and provided support for the idea that the chemical evolution of the early Earth had led to the natural synthesis of chemical building blocks of life from inanimate inorganic molecules. He has been described as the \"father of prebiotic chemistry\".\n\nStanley Miller was born in Oakland, California. He was the second child (after a brother, Donald) of Nathan and Edith Miller, descendants of Jewish immigrants from Belarus and Latvia. His father was an attorney and held the office of the Oakland Deputy District Attorney in 1927. His mother was a school teacher so that education was quite a natural environment in the family. In fact, while in Oakland High School he was nicknamed \"a chem whiz\". He followed his brother to the University of California at Berkeley to study chemistry mainly because he felt that Donald would be able to help him on the subject. He completed BSc in June 1951. For graduation course, he faced financial problems, as his father died in 1946 leaving the family with a money shortage. Fortunately with the help from Berkeley faculty (UC Berkeley did not then have assistantships), he was offered teaching an assistantship at the University of Chicago in February 1951, which could provide the basic funds for graduate work. He joined this post and got registered for a PhD program in September. He frantically searched for a thesis topic to work on, meeting one professor after another, and he was inclined toward theoretical problems as experiments tended to be laborious. He was initially convinced to work with the theoretical physicist Edward Teller on synthesis of elements. Following the customs of the university, where a graduate student is obliged to attend seminars, he attended a chemistry seminar in which the Nobel laureate Harold Urey gave a lecture on the origin of solar system and how organic synthesis could be possible under reducing environment such as the primitive Earth's atmosphere. Miller was immensely inspired. After a year of fruitless work with Teller, and the prospect of Teller leaving Chicago to work on the Hydrogen bomb, Miller was prompted to approach Urey in September 1952 for a fresh research project. Urey was not immediately enthusiastic on Miller's interest in pre-biotic synthesis, as no successful works had been done, and he even suggested working on thallium in meteorites. With persistence Miller persuaded Urey to pursue electric discharges in gases. He found clear evidence for the production of amino acids in the reaction vessel. He was always afraid that some specks of fly excrement might be the source of the amino acids he discovered in the reaction tube (or was so chided by his classmates). This was not the case and the result was a clear demonstration that a host of \"organic\" chemical compounds could be produced by purely inorganic processes. Miller eventually earned his doctorate degree in 1954, and a long-lasting reputation. From spectroscopic observations on stars, it is now well known that complex organic compounds are formed in the gases blown off of carbon rich stars as a result of chemical reactions. The fundamental issue of what the connection was between the \"pre-biotic organic\" compounds and the origin of life has remained.\n\nAfter completing a doctorate, Miller moved to the California Institute of Technology as a F. B. Jewett Fellow in 1954 and 1955. Here he worked on the mechanism involved in the amino and hydroxy acid synthesis. He then joined the Department of Biochemistry at the College of Physicians and Surgeons, Columbia University, New York, where he worked for the next 5 years. When the new University of California at San Diego was established, he became the first Assistant Professor in the Department of Chemistry in 1960, and an Associate Professor in 1962, and then a full Professor in 1968.\n\nThe Miller experiment appeared in his technical paper in the 15 May 1953 issue of \"Science\", which transformed the concept of scientific ideas on the origin of life into a respectable realm of empirical inquiry. His study has become a classic textbook definition of the scientific basis of origin of life, or more specifically, the first definitive experimental evidence of the Oparin-Haldane's \"primordial soup\" theory. Urey and Miller designed to simulate the ocean-atmospheric condition of the primitive Earth by using a continuous run of steam into a mixture of methane (CH), ammonia (NH), and hydrogen (H). The gaseous mixture was then exposed to electrical discharge, which induced chemical reaction. After a week of reaction, Miller detected the formation of amino acids, such as glycine, α- and β-alanine, using paper chromatography. He also detected aspartic acid and gamma-amino butyric acid, but was not confident because of the weak spots. Since amino acids are the basic structural and functional constituents of cellular life, the experiment showed the possibility of natural organic synthesis for the origin of life on earth.\n\nMiller showed his results to Urey, who suggested immediate publication. Urey declined to be the co-author lest Miller would receive little or no credit. The manuscript with Miller as the sole author was submitted to \"Science\" on 10 February 1953. After weeks of silence, Urey inquired and wrote to the chair of the editorial board on 27 February on the lack of action in reviewing the manuscript. A month passed, but still there was no decision. On 10 March the infuriated Urey demanded the manuscript to be returned, and he himself submitted it to the \"Journal of the American Chemical Society\" on 13 March. By then, the editor of \"Science\", apparently annoyed by Urey's insinuation, wrote directly to Miller that the manuscript was to be published. Miller accepted it and withdrew the manuscript from the \"Journal of the American Chemical Society\".\n\nMiller continued his research until his death in 2007. As the knowledge on early atmosphere progressed, and techniques for chemical analyses advanced, he kept on refining the details and methods. He not only succeeded in synthesising more and more varieties of amino acids, he also produced a wide variety of inorganic and organic compounds essential for cellular construction and metabolism. In support, a number of independent researchers also confirmed the range of chemical syntheses. With the most recent revelation that, unlike the original Miller's experimental hypothesis of strongly reducing condition, the primitive atmosphere could be quite neutral containing other gases in different proportions, Miller's last works, posthumously published in 2008, still succeeded in synthesising an array of organic compounds using such condition.\n\nIn 1972 Miller and his collaborators repeated the 1953 experiment, but with a newly developed automatic chemical analysers, such as ion-exchange chromatography and gas chromatography-mass spectrometry. They synthesised 33 amino acids, including 10 that are known to naturally occur in organisms. These included all of the primary alpha-amino acids found in the Murchison meteorite, which fell on Australia in 1969. Subsequent electric discharge experiment actually produced more variety of amino acids than that in the meteorite.\n\nJust before Miller's death, several boxes containing vials of dried residues were found among his laboratory materials at the university. The note indicated that some were from his original 1952-1954 experiments, produced by using three different apparatuses, and one from 1958, which included HS in the gaseous mixture for the first time and the result never published. In 2008 his students re-analysed the 1952 samples using more sensitive techniques, such as high-performance liquid chromatography and liquid chromatography–time of flight\nmass spectrometry. Their result showed the synthesis of 22 amino acids and 5 amines, revealing that the original Miller experiment produced many more compounds than actually reported in 1953. The unreported 1958 samples were analysed in 2011, from which 23 amino acids and 4 amines, including 7 organosulfur compounds, were detected.\n\nMiller suffered a series of strokes beginning in November, 1999 that increasingly inhibited his physical activity. He was living in a nursing home in National City, south of San Diego, and died on 20 May 2007 at the nearby Paradise Hospital. He is survived by his brother Donald and his family, and his devoted partner Maria Morris.\n\nMiller is remembered for his seminal works in the origin of life (and he was considered a pioneer in the field of exobiology), the natural occurrence of clathrate hydrates, and general mechanisms of action of anesthesia. He was elected to the US National Academy of Science in 1973. He was an Honorary Counselor of the Higher Council for Scientific Research of Spain in 1973. He was awarded the Oparin Medal by the International Society of the Study of the Origin of Life in 1983, and served as its President from 1986 to 1989.\n\nHe was nominated for Nobel Prize more than once, but never won any.\n\n\"Stanley L. Miller Award\" for young scientists under the age of 37 was instituted by the International Astrobiology Society since 2008.\n\n\n", "id": "92419", "title": "Stanley Miller"}
{"url": "https://en.wikipedia.org/wiki?curid=46565922", "text": "Tanpopo (mission)\n\nThe Tanpopo mission is an orbital astrobiology experiment investigating the potential interplanetary transfer of life, organic compounds, and possible terrestrial particles in the low Earth orbit. The purpose is to assess the panspermia hypothesis and the possibility of natural interplanetary transport of microbial life as well as prebiotic organic compounds.\n\nThe experiment, developed by Japanese scientists, started in May 2015 utilizing the Exposed Facility located on the exterior of Kibo, the Japanese Experimental Module of the International Space Station. The mission is currently collecting cosmic dust by using ultra-low density silica gel (aerogel), and it will be analyzed for amino acid-related compounds and microorganisms after returning to Earth. The principal investigator is Akihiko Yamagishi, who heads a team of researchers from 26 universities and institutions in Japan, including JAXA.\n\nIf the Tanpopo mission can detect microbes at the higher altitude of low Earth orbit (400 km), it will support the possible interplanetary migration of terrestrial life. The mission was named after dandelion (Tanpopo) because the plant's seeds evoke the image of seeds of lifeforms flying up to space.\n\nThe Tanpopo mission is taking place at the Exposed Facility located on the exterior of the Kibo module of the ISS. It is collecting cosmic dust and exposing dehydrated microorganisms outside the International Space Station while orbiting above the Earth. These experiments will test some aspects of panspermia, a hypothesis for an exogenesis origin of life distributed by meteoroids, asteroids, comets and cosmic dust. This mission will also test if terrestrial microbes (e.g., aerosols embedding microbial colonies) may be present, even temporarily and in freeze-dried form in the low Earth orbit altitudes.\nThree key microorganisms include \"Deinococcus spp.\": \"D. radiodurans\", \"D. aerius\" and \"D. aetherius\". Containers holding yeast and other microbes were also placed outside the Kibo module to examine whether microbes can survive being exposed to the harsh cold environment of outer space. Also, by evaluating retrieved samples of exposed terrestrial microbes and astronomical organic analogs on the exposure panels, they can investigate their survival and any alterations in the duration of interplanetary transport.\n\nResearchers also aim to capture organic compounds and prebiotic organic compounds — such as aminoacids — drifting in space. The mission will collect cosmic dust and other particles for three years by using ultra-low density silica gel called aerogel. Some of these aerogels will be replaced every one to two years through 2018.\n\nThe official ISS experiment code name is \"Astrobiology Japan\" representing \"Astrobiology exposure and micrometeoroid capture experiments\".\n\nThe objectives of Tanpopo lie in following 6 topics: \n\nThe aerogels were placed and retrieved by using the robotic arm outside Kibo. The first year samples were returned to Earth in mid-2016. The last aerogels will be placed inside the 'landing & return capsule' in 2018 and ejected toward Earth for retrieval. After retrieving the aerogels, scientists investigate the captured microparticles and tracks formed, followed by microbiological, organochemical and mineralogical analyses. Particles potentially containing microbes will be used for PCR amplification of rRNA genes followed by DNA sequencing.\n\nEarly mission results show evidence that some clumps of microorganism can survive for at least one year in space. This may support the idea that clumps greater than 0.5 millimeters of microorganisms could be one way for life to spread from planet to planet. \nIt was also noted that glycine's decomposition was less than expected, while hydantoin's recovery was much lower than glycine.\n\nPossible escape of terrestrial microbes from Earth to space will be evaluated by investigating the upper limit of terrestrial microbes by the capture experiment. Panels from the second year of the mission will be brought back in fall 2017.\n\n\n", "id": "46565922", "title": "Tanpopo (mission)"}
{"url": "https://en.wikipedia.org/wiki?curid=3902028", "text": "Coacervate\n\n\"'Coacervation\"' is a unique type of electrostatically-driven liquid-liquid phase separation, resulting from association of oppositely charged macro-ions. The term \"coacervate\" is sometimes used to refer to spherical aggregates of colloidal droplets held together by hydrophobic forces. Coacervate droplets can measure from 1 to 100 micrometres across, while their soluble precursors are typically on the order of less than 200 nm. The name \"coacervate\" derives from the Latin \"coacervare\", meaning \"to assemble together or cluster\".\n\nThe process of coacervation was famously proposed by Alexander Oparin and J. B. S. Haldane as crucial in his early theory of abiogenesis (origin of life/proiskhozhdenie zhizni). This theory proposes that metabolism predated information replication, although the discussion as to whether metabolism or molecules capable of template replication came first in the origins of life remains open and for decades the theory of Oparin and Haldane was the leading approach to the origin of life question.\n\nThese structures were first investigated by the Dutch chemist H.G. Bungenberg de Jong, in 1932. A wide variety of solutions can give rise to them; for example, coacervates form spontaneously when a disordered polypeptide, such as gelatin, reacts with another biologically derived polyelectrolyte, such as gum arabic. They are interesting not only in that they provide a locally segregated environment, but also in that their boundaries allow the selective absorption of simple organic molecules from the surrounding medium. For example, a mix of carbohydrate solution with a protein solution, will favor the spontaneous formation of amoeba-like coacervates which change shape, merge, divide, form \"vacuoles\", release \"vacuole contents\", and show other lifelike properties. In Oparin's view this amounts to an elementary form of metabolism. British scientist Bernal commented that they are \"the nearest we can come to cells without introducing any biological – or, at any rate, any living biological – substance.\" However, the lack of any mechanism by which coacervates can reproduce leaves them far short of being living systems.\n\n\"Complex coacervation\" commonly refers to the liquid-liquid phase separation that results when solutions of two oppositely charged macroions are mixed, resulting in the formation of a dense macroion-rich phase, the precursors of which are soluble complexes.\n\n", "id": "3902028", "title": "Coacervate"}
{"url": "https://en.wikipedia.org/wiki?curid=610074", "text": "Alexander Oparin\n\nAlexander Ivanovich Oparin () ( – April 21, 1980) was a Soviet biochemist notable for his theories about the origin of life, and for his book \"The Origin of Life\". He also studied the biochemistry of material processing by plants and enzyme reactions in plant cells. He showed that many food-production processes were based on biocatalysis and developed the foundations for industrial biochemistry in the USSR.\n\nBorn in Uglich in 1894, Oparin graduated from the Moscow State University in 1917 and became a professor of biochemistry there in 1927. Many of his early papers were about plant enzymes and their role in metabolism. In 1924 he put forward a hypothesis suggesting that life on Earth developed through a gradual chemical evolution of carbon-based molecules in \"the Earth's primordial soup\". In 1935, along with academician Alexey Bakh, he founded the Biochemistry Institute of the Soviet Academy of Sciences. In 1939, Oparin became a Corresponding Member of the Academy, and, in 1946, a full member. In 1940s and 1950s he supported the theories of Trofim Lysenko and Olga Lepeshinskaya, who made claims about \"the origin of cells from noncellular matter\". \"Taking the party line\" helped advance his career. In 1970, he was elected President of the International Society for the Study of the Origins of Life. He died in Moscow on April 21, 1980, and was interred in Novodevichy Cemetery in Moscow.\n\nOparin became Hero of Socialist Labour in 1969, received the Lenin Prize in 1974 and was awarded the Lomonosov Gold Medal in 1979 \"for outstanding achievements in biochemistry\". He was also a five-time recipient of the Order of Lenin.\n\nAlthough Oparin's started out reviewing various panspermia theories, including those of Hermann von Helmholtz and William Thomson Kelvin, he was primarily interested in how life began. As early as 1922, he asserted that:\n\nOparin outlined a way he thought that basic organic chemicals might have formed into microscopic localized systems, from which primitive living things could have developed. He cited work done by de Jong on coacervates and research by others, including himself, into organic chemicals which, in solution, might spontaneously form droplets and layers. Oparin suggested that different types of coacervates could have formed in the Earth's primordial ocean and been subject to a selection process that led, eventually, to life.\n\nWhile Oparin himself was unable to conduct experiments to test any of these ideas, later researchers tried. In 1953, Stanley Miller attempted an experiment to investigate whether chemical self-organization could have been possible on pre-historic Earth. The Miller–Urey experiment introduced heat (to provide reflux) and electrical energy (sparks, to simulate lightning) into a mixture of several simple components that would be present in a reducing atmosphere. Within a fairly short period of time a variety of familiar organic compounds, such as amino acids, were synthesised. The compounds that formed were somewhat more complex than the molecules present at the beginning of the experiment.\n\nThe Communist Party's official interpretation of Marxism, dialectical materialism, fit Oparin's speculation on the origins of life as 'a flow, an exchange, a dialectical unity'. This notion was re-enforced by Oparin's association with Lysenko.\n\n\n\n", "id": "610074", "title": "Alexander Oparin"}
{"url": "https://en.wikipedia.org/wiki?curid=20821", "text": "Miller–Urey experiment\n\nThe Miller–Urey experiment (or Miller experiment) was a chemical experiment that simulated the conditions thought at the time to be present on the early Earth, and tested the chemical origin of life under those conditions. The experiment supported Alexander Oparin's and J. B. S. Haldane's hypothesis that putative conditions on the primitive Earth favoured chemical reactions that synthesized more complex organic compounds from simpler inorganic precursors. Considered to be the classic experiment investigating abiogenesis, it was conducted in 1952 by Stanley Miller, with assistance from Harold Urey, at the University of Chicago and later the University of California, San Diego and published the following year.\n\nAfter Miller's death in 2007, scientists examining sealed vials preserved from the original experiments were able to show that there were actually well over 20 different amino acids produced in Miller's original experiments. That is considerably more than what Miller originally reported, and more than the 20 that naturally occur in life. More recent evidence suggests that Earth's original atmosphere might have had a composition different from the gas used in the Miller experiment. But prebiotic experiments continue to produce racemic mixtures of simple to complex compounds under varying conditions.\n\nThe experiment used water (HO), methane (CH), ammonia (NH), and hydrogen (H). The chemicals were all sealed inside a sterile 5-liter glass flask connected to a 500 ml flask half-full of liquid water. The liquid water in the smaller flask was heated to induce evaporation, and the water vapour was allowed to enter the larger flask. Continuous electrical sparks were fired between the electrodes to simulate lightning in the water vapour and gaseous mixture, and then the simulated atmosphere was cooled again so that the water condensed and trickled into a U-shaped trap at the bottom of the apparatus.\n\nAfter a day, the solution collected at the trap had turned pink in colour. At the end of one week of continuous operation, the boiling flask was removed, and mercuric chloride was added to prevent microbial contamination. The reaction was stopped by adding barium hydroxide and sulfuric acid, and evaporated to remove impurities. Using paper chromatography, Miller identified five amino acids present in the solution: glycine, α-alanine and β-alanine were positively identified, while aspartic acid and α-aminobutyric acid (AABA) were less certain, due to the spots being faint.\n\nIn a 1996 interview, Stanley Miller recollected his lifelong experiments following his original work and stated: \"Just turning on the spark in a basic pre-biotic experiment will yield 11 out of 20 amino acids.\"\n\nAs observed in all subsequent experiments, both left-handed (L) and right-handed (D) optical isomers were created in a racemic mixture. In biological systems, almost all of the compounds are non-racemic, or homochiral.\n\nThe original experiment remains today under the care of Miller and Urey's former student Jeffrey Bada, a professor at the UCSD, Scripps Institution of Oceanography. The apparatus used to conduct the experiment is on display at the Denver Museum of Nature and Science.\n\nOne-step reactions among the mixture components can produce hydrogen cyanide (HCN), formaldehyde (CHO), and other active intermediate compounds (acetylene, cyanoacetylene, etc.):\n\nThe formaldehyde, ammonia, and HCN then react by Strecker synthesis to form amino acids and other biomolecules:\n\nFurthermore, water and formaldehyde can react, via Butlerov's reaction to produce various sugars like ribose.\n\nThe experiments showed that simple organic compounds of building blocks of proteins and other macromolecules can be formed from gases with the addition of energy.\n\nThis experiment inspired many others. In 1961, Joan Oró found that the nucleotide base adenine could be made from hydrogen cyanide (HCN) and ammonia in a water solution. His experiment produced a large amount of adenine, the molecules of which were formed from 5 molecules of HCN. \nAlso, many amino acids are formed from HCN and ammonia under these conditions. \nExperiments conducted later showed that the other RNA and DNA nucleobases could be obtained through simulated prebiotic chemistry with a reducing atmosphere.\n\nThere also had been similar electric discharge experiments related to the origin of life contemporaneous with Miller–Urey. An article in \"The New York Times\" (March 8, 1953:E9), titled \"Looking Back Two Billion Years\" describes the work of Wollman (William) M. MacNevin at The Ohio State University, before the Miller \"Science\" paper was published in May 1953. MacNevin was passing 100,000 volt sparks through methane and water vapor and produced \"resinous solids\" that were \"too complex for analysis.\" The article describes other early earth experiments being done by MacNevin. It is not clear if he ever published any of these results in the primary scientific literature.\n\nK. A. Wilde submitted a paper to \"Science\" on December 15, 1952, before Miller submitted his paper to the same journal on February 10, 1953. Wilde's paper was published on July 10, 1953. Wilde used voltages up to only 600 V on a binary mixture of carbon dioxide (CO) and water in a flow system. He observed only small amounts of carbon dioxide reduction to carbon monoxide, and no other significant reduction products or newly formed carbon compounds.\nOther researchers were studying UV-photolysis of water vapor with carbon monoxide. They have found that various alcohols, aldehydes and organic acids were synthesized in reaction mixture.\n\nMore recent experiments by chemists Jeffrey Bada, one of Miller's graduate students, and Jim Cleaves at Scripps Institution of Oceanography of the University of California, San Diego were similar to those performed by Miller. However, Bada noted that in current models of early Earth conditions, carbon dioxide and nitrogen (N) create nitrites, which destroy amino acids as fast as they form. When Bada performed the Miller-type experiment with the addition of iron and carbonate minerals, the products were rich in amino acids. This suggests the origin of significant amounts of amino acids may have occurred on Earth even with an atmosphere containing carbon dioxide and nitrogen.\n\nSome evidence suggests that Earth's original atmosphere might have contained fewer of the reducing molecules than was thought at the time of the Miller–Urey experiment. There is abundant evidence of major volcanic eruptions 4 billion years ago, which would have released carbon dioxide, nitrogen, hydrogen sulfide (HS), and sulfur dioxide (SO) into the atmosphere. Experiments using these gases in addition to the ones in the original Miller–Urey experiment have produced more diverse molecules. The experiment created a mixture that was racemic (containing both L and D enantiomers) and experiments since have shown that \"in the lab the two versions are equally likely to appear\"; however, in nature, L amino acids dominate. Later experiments have confirmed disproportionate amounts of L or D oriented enantiomers are possible.\n\nOriginally it was thought that the primitive secondary atmosphere contained mostly ammonia and methane. However, it is likely that most of the atmospheric carbon was CO with perhaps some CO and the nitrogen mostly N. In practice gas mixtures containing CO, CO, N, etc. give much the same products as those containing CH and NH so long as there is no O. The hydrogen atoms come mostly from water vapor. In fact, in order to generate aromatic amino acids under primitive earth conditions it is necessary to use less hydrogen-rich gaseous mixtures. Most of the natural amino acids, hydroxyacids, purines, pyrimidines, and sugars have been made in variants of the Miller experiment.\n\nMore recent results may question these conclusions. The University of Waterloo and University of Colorado conducted simulations in 2005 that indicated that the early atmosphere of Earth could have contained up to 40 percent hydrogen—implying a much more hospitable environment for the formation of prebiotic organic molecules. The escape of hydrogen from Earth's atmosphere into space may have occurred at only one percent of the rate previously believed based on revised estimates of the upper atmosphere's temperature. One of the authors, Owen Toon notes: \"In this new scenario, organics can be produced efficiently in the early atmosphere, leading us back to the organic-rich soup-in-the-ocean concept... I think this study makes the experiments by Miller and others relevant again.\" Outgassing calculations using a chondritic model for the early earth complement the Waterloo/Colorado results in re-establishing the importance of the Miller–Urey experiment.\n\nIn contrast to the general notion of early earth's reducing atmosphere, researchers at the Rensselaer Polytechnic Institute in New York reported the possibility of oxygen available around 4.3 billion years ago. Their study reported in 2011 on the assessment of Hadean zircons from the earth's interior (magma) indicated the presence of oxygen traces similar to modern-day lavas. This study suggests that oxygen could have been released in the earth's atmosphere earlier than generally believed.\n\nConditions similar to those of the Miller–Urey experiments are present in other regions of the solar system, often substituting ultraviolet light for lightning as the energy source for chemical reactions. The Murchison meteorite that fell near Murchison, Victoria, Australia in 1969 was found to contain over 90 different amino acids, nineteen of which are found in Earth life. Comets and other icy outer-solar-system bodies are thought to contain large amounts of complex carbon compounds (such as tholins) formed by these processes, darkening surfaces of these bodies. The early Earth was bombarded heavily by comets, possibly providing a large supply of complex organic molecules along with the water and other volatiles they contributed. This has been used to infer an origin of life outside of Earth: the panspermia hypothesis.\n\nIn recent years, studies have been made of the amino acid composition of the products of \"old\" areas in \"old\" genes, defined as those that are found to be common to organisms from several widely separated species, assumed to share only the last universal ancestor (LUA) of all extant species. These studies found that the products of these areas are enriched in those amino acids that are also most readily produced in the Miller–Urey experiment. This suggests that the original genetic code was based on a smaller number of amino acids – only those available in prebiotic nature – than the current one.\n\nJeffrey Bada, himself Miller's student, inherited the original equipment from the experiment when Miller died in 2007. Based on sealed vials from the original experiment, scientists have been able to show that although successful, Miller was never able to find out, with the equipment available to him, the full extent of the experiment's success. Later researchers have been able to isolate even more different amino acids, 25 altogether. Bada has estimated that more accurate measurements could easily bring out 30 or 40 more amino acids in very low concentrations, but the researchers have since discontinued the testing. Miller's experiment was therefore a remarkable success at synthesizing complex organic molecules from simpler chemicals, considering that all life uses just 20 different amino acids.\n\nIn 2008, a group of scientists examined 11 vials left over from Miller's experiments of the early 1950s. In addition to the classic experiment, reminiscent of Charles Darwin's envisioned \"warm little pond\", Miller had also performed more experiments, including one with conditions similar to those of volcanic eruptions. This experiment had a nozzle spraying a jet of steam at the spark discharge. By using high-performance liquid chromatography and mass spectrometry, the group found more organic molecules than Miller had. Interestingly, they found that the volcano-like experiment had produced the most organic molecules, 22 amino acids, 5 amines and many hydroxylated molecules, which could have been formed by hydroxyl radicals produced by the electrified steam. The group suggested that volcanic island systems became rich in organic molecules in this way, and that the presence of carbonyl sulfide there could have helped these molecules form peptides.\n\nBelow is a table of amino acids produced and identified in the \"classic\" 1952 experiment, as published by Miller in 1953, the 2008 re-analysis of vials from the volcanic spark discharge experiment, and the 2010 re-analysis of vials from the HS-rich spark discharge experiment. An asterisk (*) indicates a proteinogenic amino acid, i.e. one that is incorporated into protein.\n\n", "id": "20821", "title": "Miller–Urey experiment"}
{"url": "https://en.wikipedia.org/wiki?curid=3268926", "text": "Great Oxygenation Event\n\nThe Great Oxygenation Event, the beginning of which is commonly known in scientific media as the Great Oxidation Event (GOE, also called the Oxygen Catastrophe, Oxygen Crisis, Oxygen Holocaust, Oxygen Revolution, or Great Oxidation) was the biologically induced appearance of dioxygen (O) in Earth's atmosphere. Geological, isotopic, and chemical evidence suggest that this major environmental change happened around 2.45 billion years ago (2.45 Ga), during the Siderian period, at the beginning of the Proterozoic eon. The causes of the event are not clear. The current geochemical and biomarker evidence for the development of oxygenic photosynthesis before the Great Oxidation Event has been mostly inconclusive.\n\nOceanic cyanobacteria, which evolved into coordinated (but not multicellular or even colonial) macroscopic forms more than 2.3 billion years ago (approximately 200 million years before the GOE), are believed to have become the first microbes to produce oxygen by photosynthesis. Before the GOE, any free oxygen they produced was chemically captured by dissolved iron or organic matter. The GOE started when these oxygen sinks became saturated, at which point oxygen produced by the cyanobacteria was free to escape into the atmosphere.\n\nThe increased production of oxygen set Earth's original atmosphere off balance. Free oxygen is toxic to obligate anaerobic organisms, and the rising concentrations may have destroyed most such organisms at the time. Cyanobacteria were therefore responsible for one of the most significant mass extinctions in Earth's history. Besides marine cyanobacteria, there is also evidence of cyanobacteria on land.\n\nA spike in chromium contained in ancient rock deposits formed underwater shows the accumulation had been washed off from the continental shelves. Chromium is not easily dissolved and its release from rocks would have required the presence of a powerful acid. One such acid, sulfuric acid (HSO), might have been created through bacterial reactions with pyrite. Mats of oxygen-producing cyanobacteria can produce a thin layer, one or two millimeters thick, of oxygenated water in an otherwise anoxic environment even under thick ice, and before oxygen started accumulating in the atmosphere, these organisms would already be adapted to oxygen. Additionally, the free oxygen would have reacted with atmospheric methane, a greenhouse gas, greatly reducing its concentration and triggering the Huronian glaciation, possibly the longest episode of glaciation in Earth's history and called snowball Earth.\nEventually, the evolution of aerobic organisms that consumed oxygen established an equilibrium in its availability. Free oxygen has been an important constituent of the atmosphere ever since.\n\nThe most widely accepted chronology of the Great Oxygenation Event suggests that free oxygen was first produced by prokaryotic and then later eukaryotic organisms that carried out oxygenic photosynthesis more efficiently. The excess of oxygen those organisms produce is a waste product. These organisms lived long before the GOE, perhaps as early as .\n\nInitially, the oxygen they produced would have quickly been removed from the atmosphere by the chemical weathering of reducing minerals (those able to be oxidized), most notably iron. This 'mass rusting' led to the deposition of iron(III) oxide in the form of banded-iron formations such as the sediments in Minnesota and Pilbara, Western Australia. Oxygen only began to persist in the atmosphere in small quantities shortly (~50 million years) before the start of the GOE. Without a mineral sink in the form of iron, oxygen could have accumulated very rapidly. For example, at today's rates of photosynthesis (which are much greater than those in Precambrian when there were no land plants), modern atmospheric O levels could be produced in around 2,000 years.\n\nAnother hypothesis is that oxygen producers did not evolve until a few million years before the major rise in atmospheric oxygen concentration. This is based on a particular interpretation of a supposed oxygen indicator, the mass-independent fractionation of sulfur isotopes, a marker used in previous studies. This hypothesis would eliminate the need to explain a lag in time between the evolution of oxyphotosynthetic microbes and the rise in free oxygen.\n\nIn either case, oxygen did eventually accumulate in the atmosphere, with two major consequences. \n\nFirstly, it oxidized atmospheric methane (a strong greenhouse gas) to carbon dioxide (a weaker one) and water. This decreased the greenhouse effect of the Earth's atmosphere, causing planetary cooling, and triggered the Huronian glaciation. Starting around 2.4 billion years ago, this lasted 300-400 million years, and may have been the longest ever snowball Earth episode.\n\nSecondly, the increased oxygen concentrations provided a new opportunity for biological diversification, as well as tremendous changes in the nature of chemical interactions between rocks, sand, clay, and other geological substrates and the Earth's air, oceans, and other surface waters. Despite the natural recycling of organic matter, life had remained energetically-limited until the widespread availability of oxygen. This breakthrough in metabolic evolution greatly increased the free energy availability for living organisms, with truly global environmental impacts. For example, mitochondria evolved after the GOE, and with more energy available from oxygen, organisms had the means for new, more complex morphologies. These new morphologies in turn helped drive evolution through increasingly complex interactions between organisms.\nThe gap between the start of oxygen production from photosynthetic organisms and the geologically rapid increase in atmospheric oxygen (about 2.5–2.4 billion years ago) may have been as long as 900 million years. Several hypotheses might explain the time lag:\n\nThe oxygen increase had to await tectonically driven changes in the Earth, including the appearance of shelf seas, where reduced organic carbon could reach the sediments and be buried. The newly produced oxygen was first consumed in various chemical reactions in the oceans, primarily with iron. Evidence is found in older rocks that contain massive banded iron formations that were apparently laid down as this iron and oxygen first combined; most of the planet's commercial iron ore is in these deposits. Researchers found that the amount of oxygen in the air spiked each time smaller land masses collided to form a super-continent. These massive pile ups generated mountain chains and as these mountains eroded, they released nutrients into the ocean, feeding the cyanobacteria that carry out photosynthesis.\n\nThe early chemosynthetic organisms would have been a source of methane, which is an important trap for molecular oxygen, because oxygen readily oxidizes methane to carbon dioxide (CO) and water in the presence of UV radiation. Modern methanogens require nickel as an enzyme cofactor. As the Earth's crust cooled, the supply of nickel from volcanoes was reduced and hence less methane was produced. This allowed the oxygen percentage of the atmosphere to rise as the decrease in production of methane allowed the oxygen producing algae and other such life forms to, in a sense, out-produce the methane producers. From 2.7 to 2.4 billion years ago, the rate of deposition of nickel declined steadily; it was originally 400 times today's levels.\n\nAnother hypothesis posits a model of the atmosphere that exhibits bistability in oxygen concentrations. In this model, UV shielding decreases the rate of methane oxidation once oxygen levels are sufficient to support the formation of an ozone layer. This explanation proposes an atmospheric system experiencing two steady states, one with lower (0.02%) atmospheric oxygen content, and the other with higher (21% or more) oxygen content. The Great Oxygenation Event can then be understood as a switch between the lower to the upper stable steady states.\n\nAnother factor is the presence of hydrogen gas. The appearance of cyanobacteria might explain the decline of hydrogen gas and why Earth's air is so oxygen-rich.\n\nSome bacteria living in the early oceans had the ability to separate water into hydrogen and oxygen molecules. By using the energy of the Sun, the separate hydrogen molecules were able to create organic compounds, and oxygen was a by-product. If the hydrogen-heavy compounds were buried, it would have allowed oxygen to accumulate in the atmosphere.\n\nHowever, in 2001 scientists realized that the hydrogen would instead escape into space through a process called methane photolysis, in which hydrogen-containing methane reacts with oxygen releasing hydrogen. This hypothesis could explain why the early Earth stayed warm enough to sustain oxygen producing lifeforms.\n\nThere is a possibility that the oxygen indicator was misinterpreted. During the proposed time of the lag in the previous theory, there was a change from mass-independently fractionated (MIF) sulfur to mass-dependently fractionated (MDF) sulfur in sediments. This was assumed to be a result of the appearance of oxygen in the atmosphere (since oxygen would have prevented the photolysis of sulfur dioxide, which causes MIF). This change from MIF to MDF of sulfur isotopes also may have been caused by an increase in glacial weathering, or the homogenization of the marine sulfur pool as a result of an increased thermal gradient during the Huronian glaciation period.\n\nThe Great Oxygenation Event triggered an explosive growth in the diversity of minerals on Earth. This now meant that many elements could occur in one or more oxidized forms in the near-surface environment. It is estimated that the Great Oxygenation Event alone was directly responsible for more than 2,500 new minerals of the total of about 4,500 minerals found on Earth. Most of these new minerals were formed after the Great Oxygenation event as hydrated and oxidized forms due to dynamic mantle and crust processes. \n\n", "id": "3268926", "title": "Great Oxygenation Event"}
{"url": "https://en.wikipedia.org/wiki?curid=150135", "text": "Spontaneous generation\n\nSpontaneous generation or anomalous generation is an obsolete body of thought on the ordinary formation of living organisms without descent from similar organisms. Typically, the idea was that certain forms such as fleas could arise from inanimate matter such as dust, or that maggots could arise from dead flesh. A variant idea was that of equivocal generation, in which species such as tapeworms arose from unrelated living organisms, now understood to be their hosts. Doctrines supporting such processes of generation held that these processes are commonplace and regular. Such ideas are in contradiction to that of univocal generation: effectively exclusive reproduction from genetically related parent(s), generally of the same species.\n\nThe doctrine of spontaneous generation was coherently synthesized by Aristotle, who compiled and expanded the work of prior natural philosophers and the various ancient explanations of the appearance of organisms; it held sway for two millennia. Today it is generally accepted to have been decisively dispelled during the 19th century by the experiments of Louis Pasteur. He expanded upon the investigations of predecessors (such as Francesco Redi who, in the 17th century, had performed experiments based on the same principles). However, some experimental difficulties were still there and objections from persons holding the traditional views persisted. Many of these residual objections were dealt with by the work of John Tyndall, succeeding the work of Pasteur.\n\nPasteur's experiment is generally agreed to have decisively refuted the theory of spontaneous generation in 1859. Disproof of the traditional ideas of spontaneous generation is no longer controversial among professional biologists. By the middle of the 19th century, the theory of biogenesis had accumulated so much evidential support, due to the work of Louis Pasteur and others, that the alternative theory of spontaneous generation had been effectively disproven. The historian of science John Desmond Bernal suggests that earlier theories such as spontaneous generation were based upon an explanation that life was continuously created as a result of chance events.\n\nSpontaneous generation refers both to the supposed processes in which different types of life might repeatedly emerge from specific sources other than seeds, eggs or parents, and also to the theoretical principles which were presented in support of any such phenomena. Crucial to this doctrine is the idea that life comes from non-life, with the conditions, and that no causal agent is needed (i.e. Parent). Such hypothetical processes sometimes are referred to as \"abiogenesis\", in which life routinely emerges from non-living matter on a time scale of anything from minutes to weeks, or perhaps a season or so. An example would be the supposed seasonal generation of mice and other animals from the mud of the Nile. Such ideas have no operative principles in common with the modern hypothesis of abiogenesis, in which life emerged in the early ages of the planet, over a time span of at least millions of years, and subsequently diversified without evidence that there ever has been any subsequent repetition of the event. \n\nAnother version of spontaneous generation is variously termed univocal generation, \"heterogenesis\" or \"xenogenesis\", in which one form of life has been supposed to arise from a different form, such as tapeworms from the bodies of their hosts.\n\nIn the years following Louis Pasteur's experiment in 1862, the term \"spontaneous generation\" fell into increasing disfavor. Experimentalists used a variety of terms for the study of the origin of life from non-living materials. Heterogenesis was applied to once-living materials such as boiled broths, and Henry Charlton Bastian proposed the term \"archebiosis\" for life originating from inorganic materials. The two were lumped together as \"spontaneous generation\", but disliking the term as sounding too random, Bastian proposed \"biogenesis\". In an 1870 address titled, \"Spontaneous Generation\", Thomas Henry Huxley defined \"biogenesis\" as life originating from other life and coined the negative of the term, \"abiogenesis\", which was the term that became dominant.\n\nAs part of his overall attempt to give natural explanations of things that had previously been ascribed to the agency of the gods, Anaximander believed that everything arose out of the elemental nature of the universe, which he called the \"apeiron\" or \"unbounded\". According to Hippolytus of Rome in the third century CE, Anaximander claimed that living creatures were first formed in the \"wet\" when acted on by the Sun, and that they were different then than they are now. For example, he claimed humans, in a different form, must have earlier been born mature like other animals, or they would not have survived. Anaximander also claimed that spontaneous generation continued to this day, with aquatic forms being produced directly from lifeless matter.\n\nAnaximenes, a pupil of Anaximander, thought that air was the element that imparted life, motion and thought, and speculated that there was a \"primordial terrestrial slime\", a mixture of earth and water, which when combined with the sun's heat formed plants, animals and human beings directly.\n\nXenophanes traced the origin of man back to the transitional period between the fluid stage of the earth and the formation of land. He too held to a spontaneous generation of fully formed plants and animals under the influence of the sun.\n\nEmpedocles accepted the spontaneous generation of life, but held that there had to be trials of combinations of parts of animals that spontaneously arose. Successful combinations formed the species we now see, unsuccessful forms failed to reproduce.\n\nAnaxagoras also adopted a terrestrial slime account, although he thought that the seeds of plants existed in the air from the beginning, and of animals in the aether.\n\nAristotle laid the foundations of Western natural philosophy. In his book, \"The History of Animals\", he stated in no uncertain terms:\nAccording to this theory, living things came forth from nonliving things because the nonliving material contained \"pneuma\", or \"vital heat\". The creature generated was dependent on the proportions of this pneuma and the five elements he believed comprised all matter. While Aristotle recognized that many living things emerged from putrefying matter, he pointed out that the putrefaction was not the source of life, but the byproduct of the action of the \"sweet\" element of water.\n\nNumerous forms were attributed to various sources. The testaceans (shelled molluscs) are characterized by forming by spontaneous generation in mud, but differ based upon the material they grow in — for example, clams and scallops in sand, oysters in slime, and the barnacle and the limpet in the hollows of rocks. Some reddish worms form from long-standing snow which has turned reddish. Another grub was said to grow out of fire.\n\nConcerning sexual reproduction, Aristotle argued that the male parent provided the \"form,\" or soul, that guided development through semen, and the female parent contributed unorganized matter, allowing the embryo to grow.\n\nVitruvius, a Roman architect and writer of the 1st century BCE, advised that libraries be placed facing eastwards to benefit from morning light, but not towards the south or the west as those winds generate bookworms.\n\nAristotle claimed that eels were lacking in sex and lacking milt, spawn and the passages for either. Rather, he asserted eels emerged from earthworms. Later authors dissented. Pliny the Elder did not argue against the anatomic limits of eels, but stated that eels reproduce by budding, scraping themselves against rocks, liberating particles that become eels. Athenaeus described eels as entwining and discharging a fluid which would settle on mud and generate life. On the other hand, Athenaeus also dissented towards spontaneous generation, claiming that a variety of anchovy did not generate from roe, as Aristotle stated, but rather, from sea foam.\n\nAs the dominant view of philosophers and thinkers continued to be in favour of spontaneous generation, some Christian theologians accepted the view. Augustine of Hippo discussed spontaneous generation in \"The City of God\" and \"The Literal Meaning of Genesis\", citing Biblical passages such as \"Let the waters bring forth abundantly the moving creature that hath life\" () as decrees that would enable ongoing creation.\n\nFrom the fall of the Roman Empire in 5th century to the East-West Schism in 1054, the influence of Greek science declined, although spontaneous generation generally went unchallenged. New descriptions were made. Of the numerous beliefs, some had doctrinal implications outside of the Book of Genesis. For example, the idea that a variety of bird known as the \"barnacle goose\" emerged from a crustacean known as the \"goose barnacle\", had implications on the practice of fasting during Lent. In 1188, Gerald of Wales, after having traveled in Ireland, argued that the \"unnatural\" generation of barnacle geese was evidence for the virgin birth. Where the practice of fasting during Lent allowed fish, but prohibited fowl, the idea that the goose was in fact a fish suggested that its consumption be permitted during Lent. The practice was eventually prohibited by decree of Pope Innocent III in 1215.\n\nAristotle, in Arabic translation, was reintroduced to Western Europe. During the 13th century, Aristotle reached his greatest acceptance. With the availability of Latin translations Saint Albertus Magnus and his student, Saint Thomas Aquinas, raised Aristotelianism to its greatest prominence. Albert wrote a paraphrase of Aristotle, \"De causis et processu universitatis\", in which he removed some and incorporated other commentaries by Arabic scholars. The influential writings of Aquinas, on both the physical and metaphysical, are predominantly Aristotelian, but show numerous other influences.\n\nSpontaneous generation is discussed as a fact in literature well into the Renaissance. Where, in passing, Shakespeare discusses snakes and crocodiles forming from the mud of the Nile (), Izaak Walton again raises the question of the origin of eels \"as rats and mice, and many other living creatures, are bred in Egypt, by the sun's heat when it shines upon the overflowing of the river...\". While the ancient question of the origin of eels remained unanswered and the additional idea that eels reproduced from corruption of age was mentioned, the spontaneous generation of rats and mice engendered no debate.\n\nThe Dutch biologist and microscopist Jan Swammerdam (1637 - 1680) rejected the concept that one animal could arise from another or from putrification by chance because it was impious and like others found the concept of spontaneous generation irreligious, and he associated it with atheism and Godless opinion.\n\nJan Baptist van Helmont (1580–1644) used experimental techniques, such as growing a willow for five years and showing it increased mass while the soil showed a trivial decrease in comparison. As the process of photosynthesis was not understood, he attributed the increase of mass to the absorption of water. His notes also describe a recipe for mice (a piece of soiled cloth plus wheat for 21 days) and scorpions (basil, placed between two bricks and left in sunlight). His notes suggest he may even have done these things.\n\nWhere Aristotle held that the embryo was formed by a coagulation in the uterus, William Harvey (1578 – 1657) by way of dissection of deer, showed that there was no visible embryo during the first month. Although his work predated the microscope, this led him to suggest that life came from invisible eggs. In the frontispiece of his book \"Exercitationes de Generatione Animalium\" (\"Essays on the Generation of Animals\"), he made an expression of biogenesis: \"omnia ex ovo\" (everything from eggs).\n\nThe ancient beliefs were subjected to testing. In 1668, Francesco Redi challenged the idea that maggots arose spontaneously from rotting meat. In the first major experiment to challenge spontaneous generation, he placed meat in a variety of sealed, open, and partially covered containers. Realizing that the sealed containers were deprived of air, he used \"fine Naples veil\", and observed no worm on the meat, but they appeared on the cloth. Redi used his experiments to support the preexistence theory put forth by the Church at that time, which maintained that living things originated from parents. In scientific circles Redi's work very soon had great influence, as evidenced in a letter from John Ray in 1671 to members of the Royal Society of London:\n\nPier Antonio Micheli, around 1729, observed that when fungal spores were placed on slices of melon the same type of fungi were produced that the spores came from, and from this observation he noted that fungi did not arise from spontaneous generation.\n\nIn 1745, John Needham performed a series of experiments on boiled broths. Believing that boiling would kill all living things, he showed that when sealed right after boiling, the broths would cloud, allowing the belief in spontaneous generation to persist. His studies were rigorously scrutinized by his peers and many of them agreed.\n\nLazzaro Spallanzani modified the Needham experiment in 1768, attempting to exclude the possibility of introducing a contaminating factor between boiling and sealing. His technique involved boiling the broth in a sealed container with the air partially evacuated to prevent explosions. Although he did not see growth, the exclusion of air left the question of whether air was an essential factor in spontaneous generation. However, by that time there was already widespread scepticism among major scientists, to the principle of spontaneous generation. Observation was increasingly demonstrating that whenever there was sufficiently careful investigation of mechanisms of biological reproduction, it was plain that processes involved basing of new structures on existing complex structures, rather from chaotic muds or dead materials. Joseph Priestley, after he had fled to America and not long before his death, wrote a letter that was read to the American Philosophical Society in 1803. It said in part:\n\nIn 1837, Charles Cagniard de la Tour, a physicist, and Theodor Schwann, one of the founders of cell theory, published their independent discovery of yeast in alcoholic fermentation. They used the microscope to examine foam left over from the process of brewing beer. Where Leeuwenhoek described \"small spheroid globules\", they observed yeast cells undergo cell division. Fermentation would not occur when sterile air or pure oxygen was introduced if yeast were not present. This suggested that airborne microorganisms, not spontaneous generation, was responsible.\n\nHowever, although the idea of spontaneous generation had been in decline for nearly a century, its supporters did not abandon it all at once. As James Rennie wrote:\n\nLouis Pasteur's 1859 experiment is widely seen as having settled the question of spontaneous generation. He boiled a meat broth in a flask that had a long neck that curved downward, like that of a goose or swan. The idea was that the bend in the neck prevented falling particles from reaching the broth, while still allowing the free flow of air. The flask remained free of growth for an extended period. When the flask was turned so that particles could fall down the bends, the broth quickly became clouded. However, minority objections were persistent and not always unreasonable, given that the experimental difficulties were far more challenging than the popular accounts suggest. The investigations of John Tyndall, a correspondent of Pasteur and a great admirer of Pasteur's work, were decisive in disproving spontaneous generation with dealing with lingering issues. Still, even Tyndall encountered difficulties in dealing with the effects of microbial spores, which were not well understood in his day. Like Pasteur, he boiled his cultures to sterilize them, and some types of bacterial spores can survive boiling. The autoclave, which eventually came into universal application in medical practice and microbiology to sterilise equipment, was not an instrument that had come into use at the time of Tyndall's experiments, let alone those of Pasteur.\n\nIn 1862, the French Academy of Sciences paid a special attention to the issue and established a prize \"to him who by well-conducted experiments throws new light on the question of the so-called spontaneous generation\" and appointed a commission to judge the winner. p. 107\n\n", "id": "150135", "title": "Spontaneous generation"}
{"url": "https://en.wikipedia.org/wiki?curid=52325275", "text": "James Ferris\n\nJames \"Jim\" P. Ferris (1932 – March 4, 2016) was an American chemist. He is known for his contributions to the understanding of the origins of life on Earth, specifically by demonstrating a successful mechanism of clay-catalyzed polymerization of RNA, providing further evidence for the RNA World Hypothesis. Additionally, his work in atmospheric photochemistry has illuminated many of the chemical processes which occur in the atmospheres of Jupiter and Saturn's moon, Titan.\n\nJim Ferris was born in Nyack, New York to Richard and Mabel Ferris, the youngest of five children. He completed his undergraduate studies at the University of Pennsylvania and earned a Bachelor of Science in chemistry. He went on to earn a doctorate in natural products chemistry at Indiana University, and continued his post-doctoral studies at the Massachusetts Institute of Technology.\n\nFerris began his career as a professor at Florida State University, and performed research at the Salk Institute for Biological Studies. He joined the Rensselaer Polytechnic Institute in 1967. He was the editor of \"Origins of Life and Evolution of Biospheres\" (OLEB), an academic journal sponsored by The International Society for the Study of the Origin of Life (ISSOL), from 1982 to 1999. He also served as president of ISSOL from 1993 to 1996.\n\nBetween 1998 and 2006, he served as director of NASA's New York Center for Studies on the Origins of Life, which would later become the New York Center for Astrobiology at Rensselaer, of which he remained an active member until 2015.\n\nFerris died on March 4, 2016 at Daughters of Sarah Nursing Center in Albany, New York.\n\nDuring more than fifty years of research, Ferris made landmark contributions to the field of prebiotic chemistry. His interests in the origins of life led him to explore in detail a diverse array of prebiotic reaction mechanisms, and to make the discovery of clay-directed RNA synthesis. By providing a plausible mechanism for the prebiotic synthesis of RNA oligomers, Ferris's method strengthened the RNA world hypothesis. In an effort to uncover the conditions of the early Earth's atmosphere and further establish the relationship between atmospheric processes and prebiotic chemistry, Ferris turned to observing Jupiter and Saturn's largest and most Earth-like moon, Titan.\n\nIn the late 60s, Ferris published a set of collaborative studies with Leslie Orgel that elucidated several prebiotic pathways for the synthesis of biologically relevant macromolecules (including nucleobases, amino acids, and precursors thereof) from hydrogen cyanide and cyano compounds. In another series of publications on chemical evolution, Ferris further expanded the understanding of these and other reactions, demonstrating, for example, mechanisms of hydrogen cyanide polymerization under a variety of conditions leading to purines, pyrimidines, amino acids, and a host of organic precursor molecules.\n\nFerris's work in prebiotic synthesis under early Earth conditions led him to investigate the use of the mineral montmorillonite as a surface for ribonucleotide polymerization and other processes. Montmorillonite is formed by the accumulation and breakdown of volcanic ash, and may have been present on the early Earth, making it a promising candidate for catalysis of prebiotic reactions. In early publications involving montmorillonite clays, Ferris demonstrated that, following adsorption of the nucleotides to its surface, the mineral can catalytically enhance the formation of polyadenine and polycytosine oligonucleotides and cyclic adenine monophosphates. The composition of montmorillonite clays can vary, and the presence of metal cations to stabilize the mineral's distinct negative charges were shown to affect binding and catalysis, as well. Later, Ferris was able to achieve catalysis of the phosphodiester bond between several activated ribonucleotides, resulting in RNA oligomers up to 50 nucleotides in length on the clay surface.\n\nIn 2010, Ferris showed that montmorillonite is capable of affecting regioselectivity of the RNA oligomers it catalyzes. Starting with a mixture of D and L enantiomers of activated ribonucleotides, up to 76% of the resulting oligomers were homochiral, providing a new direction for the as-yet unanswered question of the origin of homochirality in modern biochemistry.\n\nFerris constructed gaseuous simulations of the atmospheres of Jupiter and Titan and analyzed their composition using a combination of photochemistry techniques, including x-ray photoelectron spectroscopy and infrared spectroscopy. Information gained from these studies could then be directly compared to measurements of their respective planets. The analysis of atmospheric processes on other planets in our solar system not only benefits the ongoing space exploration efforts of NASA, it may also hold insight into the history of our own planet, revealing atmospheric processes that would have been important to the emergence of life on a prebiotic Earth.\n\nBy preparing analogs to Titan's atmospheric aerosols and irradiating the mixture of gases used, Ferris was able to probe refractive indices and observe synthesis reactions which could be used as models and compared directly to measurements of spectroscopy data recovered from NASA's Cassini-Huygens mission to Saturn.\n\n\nFerris received an NIH Career Award in 1969 which allowed him to greatly expand his research into prebiotic nucleotide synthesis.\n\nIn 1996, he was awarded the Oparin Medal by ISSOL for his achievements and contributions to the field of origins of life chemistry.\n\nIn 2012, the Rensselaer Polytechnic Institute established the James P. Ferris Fellowship in Astrobiology in his honor.\n\n", "id": "52325275", "title": "James Ferris"}
{"url": "https://en.wikipedia.org/wiki?curid=326920", "text": "Biogenesis\n\nBiogenesis is the production of new living organisms or organelles. Conceptually, biogenesis is primarily attributed to Louis Pasteur and encompasses the belief that complex living things come only from other living things, by means of reproduction. That is, life does not spontaneously arise from non-living material, which was the position held by spontaneous generation. This is summarized in the phrase \"Omne vivum ex vivo\", Latin for \"\"all life [is] from life.\"\" A related statement is \"Omnis cellula e cellula\", \"all cells [are] from cells;\" this conclusion is one of the central statements of cell theory.\n\nThe term biogenesis was coined by Henry Charlton Bastian to mean the generation of a life form from nonliving materials, however, Thomas Henry Huxley chose the term abiogenesis and redefined biogenesis for life arising from preexisting life. The generation of life from non-living material is called abiogenesis, and occurred through chemical and molecular evolution over millions of years.\n\nThe term \"biogenesis\" may also refer to biochemical processes of production in living organisms (see biosynthesis).\n\nThe Ancient Greeks believed that living things could spontaneously come into being from nonliving matter, and that the goddess Gaia could make life arise spontaneously from stones – a process known as \"Generatio spontanea\". Aristotle disagreed, but he still believed that creatures could arise from dissimilar organisms or from soil. Variations of this concept of spontaneous generation still existed as late as the 17th century, but towards the end of the 17th century, a series of observations and arguments began that eventually discredited such ideas. This advance in scientific understanding was met with much opposition, with personal beliefs and individual prejudices often obscuring the facts.\n\nWilliam Harvey (1578–1657) was an early proponent of all life beginning from an egg, \"omne vivum ex ovo\".\nFrancesco Redi, an Italian physician, proved as early as 1668 that higher forms of life did not originate spontaneously by demonstrating that maggots come from eggs of flies. but proponents of spontaneous generation claimed that this did not apply to microbes and continued to hold that these could arise spontaneously. Attempts to disprove the spontaneous generation of life from non-life continued in the early 19th century with observations and experiments by Franz Schulze and Theodor Schwann.\nIn 1745, John Needham added chicken broth to a flask and boiled it. He then let it cool and waited. Microbes grew, and he proposed it as an example of spontaneous generation. In 1768, Lazzaro Spallanzani repeated Needham's experiment but removed all the air from the flask. No growth occurred. In 1854, Heinrich G. F. Schröder (1810–1885) and Theodor von Dusch, and in 1859, Schröder alone, repeated the Helmholtz filtration experiment and showed that living particles can be removed from air by filtering it through cotton-wool.\n\nIn 1864, Louis Pasteur finally announced the results of his scientific experiments. In a series of experiments similar to those performed earlier by Needham and Spallanzani, Pasteur demonstrated that life does not arise in areas that have not been contaminated by existing life. Pasteur's empirical results were summarized in the phrase \"Omne vivum ex vivo\", Latin for \"all life [is] from life\".\n\nAfter obtaining his results, Pasteur stated: \"\"La génération spontanée est une chimère\"\" (\"Spontaneous generation is a dream\").\n\n", "id": "326920", "title": "Biogenesis"}
{"url": "https://en.wikipedia.org/wiki?curid=31883732", "text": "OSIRIS-REx\n\nThe OSIRIS-REx (Origins, Spectral Interpretation, Resource Identification, Security, Regolith Explorer) is a NASA asteroid study and sample-return mission. Launched on 8 September 2016, its mission is to study asteroid 101955 Bennu, a carbonaceous asteroid, and return a sample to Earth on 24 September 2023 for detailed analysis. The material returned is expected to enable scientists to learn more about the formation and evolution of the Solar System, its initial stages of planet formation, and the source of organic compounds that led to the formation of life on Earth. If successful, OSIRIS-REx will be the first U.S. spacecraft to return samples from an asteroid.\n\nThe cost of the mission will be approximately not including the Atlas V launch vehicle, which is about . It is the third planetary science mission selected in the New Frontiers program, after \"Juno\" and \"New Horizons\". The Principal Investigator is Dante Lauretta from the University of Arizona.\n\nOverall management, engineering and navigation for the mission is provided by NASA Goddard Space Flight Center, while the University of Arizona Lunar and Planetary Laboratory provides principal science operations and Lockheed Martin Space Systems built the spacecraft and provides mission operations. The science team includes members from the United States, Canada, France, Germany, United Kingdom, and Italy.\n\nAfter traveling for approximately two years, the spacecraft is to rendezvous with asteroid 101955 Bennu in August 2018 and begin 505 days of surface mapping at a distance of approximately . Results of that mapping will be used by the mission team to select the site from which to take a sample of the asteroid's surface. Then a close approach (without landing) will be attempted to allow extension of a robotic arm to gather the sample.\n\nAn asteroid was chosen as the target of study because an asteroid is a 'time capsule' from the birth of our Solar System. In particular, 101955 Bennu was selected because of the availability of pristine carbonaceous material, a key element in organic molecules necessary for life as well as representative of matter from before the formation of Earth. Organic molecules, such as amino acids, have previously been found in meteorite and comet samples, indicating that some ingredients necessary for life can be naturally synthesized in outer space.\n\nFollowing collection of material (from 60 grams to two kilograms) in July 2020, the sample will be returned to Earth in a capsule similar to that which returned the samples of comet 81P/Wild on the Stardust spacecraft. The return trip to Earth will be shorter, and will land with a parachute at the Utah Test and Training Range in September 2023. The capsule will then be transported to the Johnson Space Center for processing in a dedicated research facility.\n\nThe acronym OSIRIS was chosen in reference to the ancient mythological Egyptian god Osiris, the underworld lord of the dead. He was classically depicted as a green-skinned man with a pharaoh's beard, partially mummy-wrapped at the legs and wearing a distinctive crown with two large ostrich feathers at either side. Rex means 'king' in Latin. His name was chosen for this mission as asteroid Bennu is a threatening Earth impactor, with an estimated 1-in-1,800 chance of hitting Earth in the year 2170.\n\nThe launch was on 8 September 2016 at 23:05 UTC on a United Launch Alliance Atlas V 411 from Cape Canaveral Space Launch Complex 41. The 411 rocket configuration consists of a RD-180 powered first stage with a single AJ-60A solid fuel booster, and a Centaur upper stage. OSIRIS-REx separated from the launch vehicle 55 minutes after ignition, with a speed of . The launch was declared \"exactly perfect\" by the mission's Principal Investigator, with no anomalies worked before or during launch.\n\nOSIRIS-REx entered the cruise phase shortly after separation from the launch vehicle, following successful solar panel deployment, propulsion system initiation, and establishment of a communication link with Earth. Its hyperbolic escape speed from Earth was about . On 28 December 2016, the spacecraft successfully performed its first deep space maneuver (DSM-1) to change its velocity by using of fuel. An additional, smaller firing of its thrusters on 18 January further refined its course for an Earth gravity assist in 22 September 2017. The cruise phase will last until its encounter with Bennu in August 2018, after which it will enter its science and sample collection phase.\n\nDuring its cruise phase, OSIRIS-REx was used to search for a class of near-Earth objects known as Earth-Trojan asteroids as it passed through Sun–Earth Lagrange point. Between 9 and 20 February 2017, the OSIRIS-REx team used the spacecraft's MapCam camera to search for the objects, taking about 135 survey images each day for processing by scientists at the University of Arizona. The search will be beneficial even if no objects are found, as it closely resembles the operation that will be required as the spacecraft approaches Bennu, searching for natural satellites and other potential hazards.\n\nOn 12 February 2017, while from the giant planet, the PolyCam instrument aboard OSIRIS-REx successfully imaged Jupiter and three of its moons, Callisto, Io, and Ganymede.\n\nDuring the extensive remote sensing campaign, a sample site will be chosen and rehearsals will be performed for the final sampling event. The solar arrays will be raised into a Y-shaped configuration to minimize the chance of dust accumulation during contact and provide more ground clearance in case the spacecraft tips over (up to 45°) during contact. The descent will be very slow to minimize thruster firings prior to contact in order to reduce the likelihood of asteroid surface contamination by unreacted hydrazine propellant. Contact with the surface of Bennu will be detected using accelerometers, and the impact force will be dissipated by a spring in the TAGSAM arm.\n\nUpon surface contact by the TAGSAM instrument, a burst of nitrogen gas will be released, which will blow regolith particles smaller than into the sampler head located at the end of the robotic arm. A five-second timer will limit collection time to mitigate the chance of a collision. After the timer expires, the back-away maneuver will initiate a safe departure from the asteroid.\n\nOSIRIS-REx will then halt the drift away from the asteroid in case it is necessary to return for another sampling attempt. The spacecraft will use images and spinning maneuvers to verify the sample has been acquired as well as determine its mass and verify it is in excess of the required . In the event of a failed sampling attempt, the spacecraft will return for another try. There is enough nitrogen gas for three attempts.\n\nIn addition to the bulk sampling mechanism, contact pads on the end of the sampling head will passively collect dust grains smaller than , upon contact with the asteroid. These pads are made from tiny loops of stainless steel.\n\nAfter the sampling attempt, the Sample-Return Capsule (SRC) lid will be opened to allow the sampler head to be stowed. The arm will then be retracted into its launch configuration and the SRC lid will be closed and latched preparing to return to Earth.\n\nThe science objectives of the mission are:\n\nTelescopic observations have helped define the orbit of 101955 Bennu, a near-Earth object with a mean diameter in the range of . It completes an orbit of the Sun every 436.604 days (1.2 years). This orbit takes it close to the Earth every six years. Although the orbit is reasonably well known, scientists continue to refine it. It is critical to know the orbit of Bennu because recent calculations produced a cumulative probability of 1 in 1410 (or 0.071%) of impact with Earth in the period 2169 to 2199. Part of the OSIRIS-REx mission is to refine understanding of non-gravitational effects (such as the Yarkovsky effect) on this orbit, and the implications of those effects for Bennu's collision probability. Knowing Bennu's physical properties will be critical for future scientists to know when developing an asteroid impact avoidance mission.\n\nTelescopic observations have revealed some basic properties of Bennu. They indicate that it is very dark and is classified as a B-type asteroid, a sub-type of the carbonaceous C-type asteroids. Such asteroids are considered \"primitive\", having undergone little geological change from their time of formation.\n\nIn addition to its telecommunication equipment, the spacecraft will carry a suite of instruments that will study the asteroid in many wavelengths, as well as image the asteroid, and retrieve a physical sample to return to Earth. The Planetary Society coordinated a campaign to invite interested persons to have their names or artwork on the mission's spirit of exploration saved on a microchip now carried in the spacecraft.\n\nThe OSIRIS-REx Camera Suite (OCAMS) consists of the PolyCam, the MapCam, and the SamCam. Together they acquire information on asteroid Bennu by providing global mapping, sample site reconnaissance and characterization, high-resolution imaging, and records of the sample acquisition.\n\n\nThe OSIRIS-REx Laser Altimeter (OLA) is a scanning and LIDAR instrument that will provide high resolution topographical information throughout the mission. The information received by OLA will create global topographic maps of Bennu, local maps of candidate sample sites, ranging in support of other instruments, and support navigation and gravity analyses.\n\nOLA will scan the surface of Bennu at specific intervals in the mission to rapidly map the entire surface of the asteroid to achieve its primary objective of producing local and global topographic maps. The data collected by OLA will also be used to develop a control network relative to the center of mass of the asteroid and to enhance and refine gravitational studies of Bennu.\n\nOLA has a single common receiver and two complementary transmitter assemblies that enhance the resolution of the information brought back. OLA's high-energy laser transmitter is used for ranging and mapping from . The low-energy transmitter is used for ranging and imaging from . The repetition rate of these transmitters sets the data acquisition rate of OLA. Laser pulses from both the low and high energy transmitters are directed onto a movable scanning mirror, which is co-aligned with the field of view of the receiver telescope limiting the effects of background solar radiation. Each pulse provides target range, azimuth, elevation, received intensity and a time-tag.\n\nOLA is funded by the Canadian Space Agency (CSA) and was built by MacDonald, Dettwiler and Associates at Brampton, Ontario, Canada. OLA was delivered for integration with the spacecraft on 17 November 2015.\n\nThe OSIRIS-REx Visible and IR Spectrometer (OVIRS) is a spectrometer, which measures light to provide mineral and organic spectral maps and local spectral information of candidate sample sites. It also provides full-disc asteroid spectral data, global spectral maps (20 m resolution), and spectra of the sample site (blue to near-infrared, , with a spectral resolution of ). These data will be used in concert with OTES spectra to guide sample-site selection. These spectral ranges and resolving powers are sufficient to provide surface maps of mineralogical and molecular components including carbonates, silicates, sulfates, oxides, adsorbed water and a wide range of organic compounds. It provides at least two spectral samples per resolution element taking full advantage of the spectral resolution.\n\nThe OSIRIS-REx Thermal Emission Spectrometer (OTES) provides mineral and thermal emission spectral maps and local spectral information of candidate sample sites by collecting thermal infrared data from 4–50 µm. (see also thermal infrared spectroscopy)\n\nOTES provides full-disc Bennu spectral data, global spectral maps, and local sample site spectral information used to characterize the global, regional, and local mineralogic composition and thermal emission from the asteroid surface. The wavelength range, spectral resolution, and radiometric performance are sufficient to resolve and identify the key vibrational absorption features of silicate, carbonate, sulfate, phosphate, oxide, and hydroxide minerals. OTES is also used to measure the total thermal emission from Bennu in support of the requirement to measure emitted radiation globally. Based on the performance of Mini-TES in the dusty surface environment of Mars, OTES is expected to be resilient to extreme dust contamination on the optical elements.\n\nThe Regolith X-ray Imaging Spectrometer (REXIS) will provide an X-ray spectroscopy map of Bennu, complementing core OSIRIS-REx mission science. REXIS is a collaborative development by four groups within Massachusetts Institute of Technology (MIT) and Harvard University, with the potential to involve more than 100 students throughout the process. REXIS is based on flight heritage hardware, thereby minimizing elements of technical risk, schedule risk, and cost risk.\n\nREXIS is a coded aperture soft X-ray (0.3–7.5 keV) telescope that images X-ray fluorescence line emission produced by the interaction of solar X-rays and the solar wind with the regolith of Bennu. Images are formed with 21 arcminute resolution (4.3 m spatial resolution at a distance of 700 m). Imaging is achieved by correlating the detected X-ray image with a 64 x 64 element random mask (1.536 mm pixels). REXIS will store each X-ray event data in order to maximize the data storage usage and to minimize the risk. The pixels will be addressed in 64 x 64 bins and the 0.3–7.5 keV range will be covered by five broad bands and 11 narrow line bands. A 24 s resolution time tag will be interleaved with the event data to account for Bennu rotation. Images will be reconstructed on the ground after downlink of the event list. Images are formed simultaneously in 16 energy bands centered on the dominant lines of abundant surface elements from O-K (0.5 keV) to Fe-Kß (7 keV) as well the representative continuum. During orbital phase 5B, a 21-day orbit 700 m from the surface of Bennu, a total of at least 133 events/asteroid pixel/energy band are expected under 2 keV; enough to obtain significant constraints on element abundances at scales larger than 10 m.\n\nThe sample-return system, called Touch-And-Go Sample Acquisition Mechanism (TAGSAM), consists of a sampler head with an articulated arm. An on-board nitrogen source will support up to three separate sampling attempts for a minimum total amount of of sample. The surface contact pads will also collect fine-grained material.\n\nHighlights of the TAGSAM instrument and technique include:\nOSIRIS-REx II was a 2012 mission concept to replicate the original spacecraft for a double mission, with the second vehicle collecting samples from the two moons of Mars, Phobos and Deimos. It was stated that this mission would be both the quickest and least expensive way to get samples from the moons.\n\n\n", "id": "31883732", "title": "OSIRIS-REx"}
{"url": "https://en.wikipedia.org/wiki?curid=54551950", "text": "Shock synthesis\n\nShock synthesis is the process of complex organic chemical creation through high velocity impact on simple amino acids, theorized to take place when a comet strikes a planetary body, or through the shock-wave created by a thunder clap. Hyper-velocity impact shock of a typical comet ice mixture produced several amino acids after hydrolysis. These include equal amounts of D- and L-alanine, and the non-protein amino acids α-aminoisobutyric acid and isovaline as well as their precursors.\n", "id": "54551950", "title": "Shock synthesis"}
{"url": "https://en.wikipedia.org/wiki?curid=53800065", "text": "The Vital Question\n\nThe Vital Question is a book by the English biochemist Nick Lane about the way the evolution and origin of life on Earth was constrained by the provision of energy.\n\nNick Lane is a biochemist at University College London. He researches \"evolutionary biochemistry and bioenergetics, focusing on the origin of life and the evolution of complex cells.\" He won the Michael Faraday Prize in 2016 for \"excellence in communicating science to UK audiences.\"\n\nThe book was first published by Profile Books in 2015. A paperback edition came out in 2016. The book has been translated into at least six languages, namely Chinese, German, Japanese, Korean, Polish, and Spanish.\n\n\n\n\n\nThe book is illustrated with 37 figures taken by permission from a wide variety of research sources. They include a timeline, photographs, cladograms, electron flow diagrams and diagrams of the life cycle of cells and their chromosomes.\n\nTim Requarth, reviewing \"The Vital Question\" for \"The New York Times\", notes that Charles Darwin had speculated that life might have begun in some \"warm little pond\", but Lane shows this could not have happened. Instead, Lane argues that, in Requarth's words, \"life emerged from towering rock formations on the ocean floor, where heated, mineral-laden water spewed from the inner Earth through the rock’s hollow network of cell-size compartments. These rocks contained the ingredients necessary for life’s start, but most important, their natural temperature and energy gradients favored the formation of large molecules.\" The resulting proton gradient drives \"a remarkable, turbinelike protein, ATP synthase\" to rotate, capturing energy in usable chemical form. \"This bizarre mechanism, as universal as DNA, is as counterintuitive as anything in science\", observes Requarth, who finds the book \"seductive and often convincing, though speculation far outpaces evidence in many of the book’s passages. But perhaps for a biological theory of everything, that’s to be expected, even welcomed.\"\n\nThe science writer Peter Forbes, reviewing \"The Vital Question\" in \"The Guardian\", noted that the origin of life was once thought to be \"safely consigned to wistful armchair musing\", but that in the past 20 years new research in genomics, geology, biochemistry and molecular biology have transformed thinking in the field. \"Here is the book that presents all this hard evidence and tightly interlocking theory to a wider audience.\", writes Forbes.\n\nMichael LePage, reviewing the book in \"New Scientist\", writes that the fact that complex cells only evolved once is \"very peculiar when you think about it\", but it is just one of many large mysteries that Lane addresses, including aging and death, sex, and speciation. LePage finds Lane's arguments \"powerful and persuasive\", with many testable ideas. The book is not, he writes, the easiest to read, but \"it does tell an incredible, epic story\", from the dawn of life to the present day.\n\nCaspar Henderson, reviewing the book in \"The Telegraph\", writes that the book \"succeeds brilliantly\" as good science writing can, expanding the reader's horizons \"in ways not previously imagined.\" Lane explains why the counterintuitive idea \"that cross-membrane proton gradients power all living cells\" is no mere technical detail: per gram, he notes, the power is 10,000 times denser per gram than the sun, and it is conserved across every form of life, telling us something about how life began and how it was constrained to evolve. Henderson recommends the book as amazing and gripping, only criticising the publisher for the \"pedestrian\" quality of the design and printing.\n\nThe founder of Microsoft, Bill Gates, reviewed the book under the heading \"This Biology Book Blew Me Away\". It moved him to read two of Lane's other books, and to bring him to New York to interview him. Gates noted that \"As much as I loved The Vital Question, it’s not for everyone. Some of the explanations are pretty technical. But this is a technical subject, and I doubt anyone else will make it much easier to understand without sacrificing crucial details.\"\n", "id": "53800065", "title": "The Vital Question"}
{"url": "https://en.wikipedia.org/wiki?curid=5326", "text": "Creationism\n\nCreationism is the religious belief that the universe and life originated \"from specific acts of divine creation\", as opposed to the scientific conclusion that they came about through natural processes. The first use of the term \"creationist\" to describe a proponent of creationism is found in an 1856 letter of Charles Darwin describing those who objected on religious grounds to the then emerging science of evolution. \nCreationism covers a spectrum of views including \"evolutionary creationism\", a theological variant of theistic evolution which asserts that both evolutionary science and a belief in creation are true, but the term is commonly used for literal creationists who reject various aspects of science, and instead promote pseudoscientific beliefs.\n\nLiteral creationists base their beliefs on a fundamentalist reading of religious texts, including the creation myths found in Genesis and the Quran. For young Earth creationists, these beliefs are based on a literalist interpretation of the Genesis creation narrative and rejection of the scientific theory of evolution. Literalist creationists believe that evolution cannot adequately account for the history, diversity, and complexity of life on Earth. Pseudoscientific branches of creationism include creation science, flood geology, and intelligent design, as well as subsets of pseudoarchaeology, pseudohistory, and pseudolinguistics.\n\nThe basis for many creationists' beliefs is a literal or quasi-literal interpretation of the Old Testament, especially from stories from the book of Genesis:\n\n\nA further important element is the interpretation of the Biblical chronology, the elaborate system of life-spans, \"generations,\" and other means by which the Bible measures the passage of events from the Creation (Genesis 1:1) to the Book of Daniel, the last biblical book in which it appears. Recent decades have seen attempts to de-link Creationism from the Bible and recast it as science: these include creation science and intelligent design. There are also non-Christian forms of Creationism, notably Islamic Creationism and Hindu Creationism.\n\nSeveral attempts have been made to categorize the different types of creationism, and create a \"taxonomy\" of creationists. Creationism (broadly construed) covers a spectrum of beliefs which have been categorized into the general types listed below.\n\nYoung Earth creationists such as Ken Ham and Doug Phillips believe that God created the Earth within the last ten thousand years, literally as described in the Genesis creation narrative, within the approximate time-frame of biblical genealogies (detailed for example in the Ussher chronology). Most young Earth creationists believe that the universe has a similar age as the Earth. A few assign a much older age to the universe than to Earth. Creationist cosmologies give the universe an age consistent with the Ussher chronology and other young Earth time frames. Other young Earth creationists believe that the Earth and the universe were created with the appearance of age, so that the world appears to be much older than it is, and that this appearance is what gives the geological findings and other methods of dating the Earth and the universe their much longer timelines.\n\nThe Christian organizations Institute for Creation Research (ICR) and the Creation Research Society (CRS) both promote young Earth creationism in the US. Another organization with similar views, Answers in Genesis (AiG)—based in both the US and the United Kingdom—has opened the Creation Museum in Petersburg, Kentucky, to promote young Earth creationism. Creation Ministries International promotes young Earth views in Australia, Canada, South Africa, New Zealand, the US, and the UK. Among Roman Catholics, the Kolbe Center for the Study of Creation promotes similar ideas. In 2007, Ken Ham founded the Creation Museum and Ark Encounter in northern Kentucky.\n\nOld Earth creationism holds that the physical universe was created by God, but that the creation event described in the Book of Genesis is to be taken figuratively. This group generally believes that the age of the universe and the age of the Earth are as described by astronomers and geologists, but that details of modern evolutionary theory are questionable.\n\nOld Earth creationism itself comes in at least three types:\n\nGap creationism, also called \"restoration creationism,\" holds that life was recently created on a pre-existing old Earth. This version of creationism relies on a particular interpretation of . It is considered that the words \"formless\" and \"void\" in fact denote waste and ruin, taking into account the original Hebrew and other places these words are used in the Old Testament. Genesis 1:1–2 is consequently translated:\n\nThus, the six days of creation (verse 3 onwards) start sometime after the Earth was \"without form and void.\" This allows an indefinite \"gap\" of time to be inserted after the original creation of the universe, but prior to the creation according to Genesis, (when present biological species and humanity were created). Gap theorists can therefore agree with the scientific consensus regarding the age of the Earth and universe, while maintaining a literal interpretation of the biblical text.\n\nSome gap creationists expand the basic version of creationism by proposing a \"primordial creation\" of biological life within the \"gap\" of time. This is thought to be \"the world that then was\" mentioned in 2 Peter 3:3–7. Discoveries of fossils and archaeological ruins older than 10,000 years are generally ascribed to this \"world that then was,\" which may also be associated with Lucifer's rebellion. These views became popular with publications of Hebrew Lexicons such as \"Strong's Concordance\", and Bible commentaries such as the \"Scofield Reference Bible\" and \"The Companion Bible\".\n\nDay-age creationism states that the \"six days\" of the Book of Genesis are not ordinary 24-hour days, but rather much longer periods (for instance, each \"day\" could be the equivalent of millions, or billions of years of human time). The physicist Gerald Schroeder is one such proponent of this view. This version of creationism often states that the Hebrew word \"yôm,\" in the context of Genesis 1, can be properly interpreted as \"age.\" Some adherents, including Jehovah's Witnesses, claim the seventh age (\"seventh day\") is currently ongoing.\n\nStrictly speaking, day-age creationism is not so much a version of creationism as a hermeneutic option which may be combined with other versions of creationism such as progressive creationism.\n\nProgressive creationism holds that species have changed or evolved in a process continuously guided by God, with various ideas as to how the process operated—though it is generally taken that God directly intervened in the natural order at key moments in Earth history. This view accepts most of modern physical science including the age of the Earth, but rejects much of modern evolutionary biology or looks to it for evidence that evolution by natural selection alone is incorrect. Organizations such as Reasons To Believe, founded by Hugh Ross, promote this version of creationism.\n\nProgressive creationism can be held in conjunction with hermeneutic approaches to the Genesis creation narrative such as the day-age creationism or framework/metaphoric/poetic views.\n\nCreation science, or initially scientific creationism, is a pseudoscience that emerged in the 1960s with proponents aiming to have young Earth creationist beliefs taught in school science classes as a counter to teaching of evolution. Common features of Creation science argument include: creationist cosmologies which accommodate a universe on the order of thousands of years old, criticism of radiometric dating through a technical argument about radiohalos, explanations for the fossil record as a record of the Genesis flood narrative (see flood geology), and explanations for the present diversity as a result of pre-designed genetic variability and partially due to the rapid degradation of the perfect genomes God placed in \"created kinds\" or \"Baramin\" (see creationist biology) due to mutations.\n\nNeo-creationism is a pseudoscientific movement which aims to restate creationism in terms more likely to be well received by the public, by policy makers, by educators and by the scientific community. It aims to re-frame the debate over the origins of life in non-religious terms and without appeals to scripture. This comes in response to the 1987 ruling by the United States Supreme Court in \"Edwards v. Aguillard\" that creationism is an inherently religious concept and that advocating it as correct or accurate in public-school curricula violates the Establishment Clause of the First Amendment.\n\nOne of the principal claims of neo-creationism propounds that ostensibly objective orthodox science, with a foundation in naturalism, is actually a dogmatically atheistic religion. Its proponents argue that the scientific method excludes certain explanations of phenomena, particularly where they point towards supernatural elements, thus effectively excluding religious insight from contributing to understanding the universe. This leads to an open and often hostile opposition to what neo-creationists term \"Darwinism\", which they generally mean to refer to evolution, but which they may extend to include such concepts as abiogenesis, stellar evolution and the Big Bang theory.\n\nUnlike their philosophical forebears, neo-creationists largely do not believe in many of the traditional cornerstones of creationism such as a young Earth, or in a dogmatically literal interpretation of the Bible.\n\nIntelligent design (ID) is the pseudoscientific view that \"certain features of the universe and of living things are best explained by an intelligent cause, not an undirected process such as natural selection.\" All of its leading proponents are associated with the Discovery Institute, a think tank whose Wedge strategy aims to replace the scientific method with \"a science consonant with Christian and theistic convictions\" which accepts supernatural explanations. It is widely accepted in the scientific and academic communities that intelligent design is a form of creationism, and is sometimes referred to as \"intelligent design creationism.\"\n\nID originated as a re-branding of creation science in an attempt to avoid a series of court decisions ruling out the teaching of creationism in American public schools, and the Discovery Institute has run a series of campaigns to change school curricula. In Australia, where curricula are under the control of state governments rather than local school boards, there was a public outcry when the notion of ID being taught in science classes was raised by the Federal Education Minister Brendan Nelson; the minister quickly conceded that the correct forum for ID, if it were to be taught, is in religious or philosophy classes.\n\nIn the US, teaching of intelligent design in public schools has been decisively ruled by a federal district court to be in violation of the Establishment Clause of the First Amendment to the United States Constitution. In Kitzmiller v. Dover, the court found that intelligent design is not science and \"cannot uncouple itself from its creationist, and thus religious, antecedents,\" and hence cannot be taught as an alternative to evolution in public school science classrooms under the jurisdiction of that court. This sets a persuasive precedent, based on previous US Supreme Court decisions in \"Edwards v. Aguillard\" and \"Epperson v. Arkansas\" (1968), and by the application of the Lemon test, that creates a legal hurdle to teaching intelligent design in public school districts in other federal court jurisdictions.\n\nIn astronomy, the geocentric model (also known as geocentrism, or the Ptolemaic system), is a description of the Cosmos where Earth is at the orbital center of all celestial bodies. This model served as the predominant cosmological system in many ancient civilizations such as ancient Greece. As such, they assumed that the Sun, Moon, stars, and naked eye planets circled Earth, including the noteworthy systems of Aristotle (see Aristotelian physics) and Ptolemy.\n\nArticles arguing that geocentrism was the biblical perspective appeared in some early creation science newsletters associated with the Creation Research Society pointing to some passages in the Bible, which, when taken literally, indicate that the daily apparent motions of the Sun and the Moon are due to their actual motions around the Earth rather than due to the rotation of the Earth about its axis for example, Joshua 10:12 where the Sun and Moon are said to stop in the sky, and Psalms 93:1 where the world is described as immobile. Contemporary advocates for such religious beliefs include Robert Sungenis, co-author of the self-published \"Galileo Was Wrong: The Church Was Right\" (2006). These people subscribe to the view that a plain reading of the Bible contains an accurate account of the manner in which the universe was created and requires a geocentric worldview. Most contemporary creationist organizations reject such perspectives.\n\nThe Omphalos hypothesis argues that in order for the world to be functional, God must have created a mature Earth with mountains and canyons, rock strata, trees with growth rings, and so on; therefore \"no\" evidence that we can see of the presumed age of the Earth and age of the universe can be taken as reliable. The idea has seen some revival in the 20th century by some modern creationists, who have extended the argument to address the \"starlight problem\". The idea has been criticised as Last Thursdayism, and on the grounds that it requires a deliberately deceptive creator.\n\nTheistic evolution, or evolutionary creation, is a belief that \"the personal God of the Bible created the universe and life through evolutionary processes.\" According to the American Scientific Affiliation:\n\nThrough the 19th century the term \"creationism\" most commonly referred to direct creation of individual souls, in contrast to traducianism. Following the publication of \"Vestiges of the Natural History of Creation\", there was interest in ideas of Creation by divine law. In particular, the liberal theologian Baden Powell argued that this illustrated the Creator's power better than the idea of miraculous creation, which he thought ridiculous. When \"On the Origin of Species\" was published, the cleric Charles Kingsley wrote of evolution as \"just as noble a conception of Deity.\" Darwin's view at the time was of God creating life through the laws of nature, and the book makes several references to \"creation,\" though he later regretted using the term rather than calling it an unknown process. In America, Asa Gray argued that evolution is the secondary effect, or \"modus operandi\", of the first cause, design, and published a pamphlet defending the book in theistic terms, \"Natural Selection not inconsistent with Natural Theology\". Theistic evolution, also called, evolutionary creation, became a popular compromise, and St. George Jackson Mivart was among those accepting evolution but attacking Darwin's naturalistic mechanism. Eventually it was realised that supernatural intervention could not be a scientific explanation, and naturalistic mechanisms such as neo-Lamarckism were favoured as being more compatible with purpose than natural selection.\n\nSome theists took the general view that, instead of faith being in opposition to biological evolution, some or all classical religious teachings about Christian God and creation are compatible with some or all of modern scientific theory, including specifically evolution; it is also known as \"evolutionary creation.\" In Evolution versus Creationism, Eugenie Scott and Niles Eldredge state that it is in fact a type of evolution.\n\nIt generally views evolution as a tool used by God, who is both the first cause and immanent sustainer/upholder of the universe; it is therefore well accepted by people of strong theistic (as opposed to deistic) convictions. Theistic evolution can synthesize with the day-age creationist interpretation of the Genesis creation narrative; however most adherents consider that the first chapters of the Book of Genesis should not be interpreted as a \"literal\" description, but rather as a literary framework or allegory.\n\nFrom a theistic viewpoint, the underlying laws of nature were designed by God for a purpose, and are so self-sufficient that the complexity of the entire physical universe evolved from fundamental particles in processes such as stellar evolution, life forms developed in biological evolution, and in the same way the origin of life by natural causes has resulted from these laws.\n\nIn one form or another, theistic evolution is the view of creation taught at the majority of mainline Protestant seminaries. For Roman Catholics, human evolution is not a matter of religious teaching, and must stand or fall on its own scientific merits. Evolution and the Roman Catholic Church are not in conflict. The Catechism of the Catholic Church comments positively on the theory of evolution, which is neither precluded nor required by the sources of faith, stating that scientific studies \"have splendidly enriched our knowledge of the age and dimensions of the cosmos, the development of life-forms and the appearance of man.\" Roman Catholic schools teach evolution without controversy on the basis that scientific knowledge does not extend beyond the physical, and scientific truth and religious truth cannot be in conflict. Theistic evolution can be described as \"creationism\" in holding that divine intervention brought about the origin of life or that divine laws govern formation of species, though many creationists (in the strict sense) would deny that the position is creationism at all. In the creation–evolution_controversy controversy its proponents generally take the \"evolutionist\" side. This sentiment was expressed by Fr. George Coyne, (the Vatican's chief astronomer between 1978 and 2006):...in America, creationism has come to mean some fundamentalistic, literal, scientific interpretation of Genesis. Judaic-Christian faith is radically creationist, but in a totally different sense. It is rooted in a belief that everything depends upon God, or better, all is a gift from God.\n\nWhile supporting the methodological naturalism inherent in modern science, the proponents of theistic evolution reject the implication taken by some atheists that this gives credence to ontological materialism. In fact, many modern philosophers of science, including atheists, refer to the long-standing convention in the scientific method that observable events in nature should be explained by natural causes, with the distinction that it does not assume the actual existence or non-existence of the supernatural. \n\n, most Christians around the world accepted evolution as the most likely explanation for the origins of species, and did not take a literal view of the Genesis creation myth. The United States is an exception where belief in religious fundamentalism is much more likely to affect attitudes towards evolution than it is for believers elsewhere. Political partisanship affecting religious belief may be a factor because political partisanship in the US is highly correlated with fundamentalist thinking, unlike in Europe.\n\nMost contemporary Christian leaders and scholars from mainstream churches, such as Anglicans and Lutherans, consider that there is no conflict between the spiritual meaning of creation and the science of evolution. According to the former Archbishop of Canterbury, Rowan Williams, \"...for most of the history of Christianity, and I think this is fair enough, most of the history of the Christianity there's been an awareness that a belief that everything depends on the creative act of God, is quite compatible with a degree of uncertainty or latitude about how precisely that unfolds in creative time.\"\n\nLeaders of the Anglican and Roman Catholic churches have made statements in favor of evolutionary theory, as have scholars such as the physicist John Polkinghorne, who argues that evolution is one of the principles through which God created living beings. Earlier supporters of evolutionary theory include Frederick Temple, Asa Gray and Charles Kingsley who were enthusiastic supporters of Darwin's theories upon their publication, and the French Jesuit priest and geologist Pierre Teilhard de Chardin saw evolution as confirmation of his Christian beliefs, despite condemnation from Church authorities for his more speculative theories. Another example is that of Liberal theology, not providing any creation models, but instead focusing on the symbolism in beliefs of the time of authoring Genesis and the cultural environment.\n\nMany Christians and Jews had been considering the idea of the creation history as an allegory (instead of historical) long before the development of Darwin's theory of evolution. For example, Philo, whose works were taken up by early Church writers, wrote that it would be a mistake to think that creation happened in six days, or in any set amount of time. Augustine of the late fourth century who was also a former neoplatonist argued that everything in the universe was created by God at the same moment in time (and not in six days as a literal reading of the Book of Genesis would seem to require); It appears that both Philo and Augustine felt uncomfortable with the idea of a seven-day creation because it detracted from the notion of God's omnipotence. In 1950, Pope Pius XII stated limited support for the idea in his encyclical \"Humani generis\". In 1996, Pope John Paul II stated that \"new knowledge has led to the recognition of the theory of evolution as more than a hypothesis,\" but, referring to previous papal writings, he concluded that \"if the human body takes its origin from pre-existent living matter, the spiritual soul is immediately created by God.\"\n\nIn the US, Evangelical Christians have continued to believe in a literal Genesis. Members of evangelical Protestant (70%), Mormon (76%) and Jehovah's Witnesses (90%) denominations are the most likely to reject the evolutionary interpretation of the origins of life. Jehovah's Witnesses adhere to a combination of gap creationism and day-age creationism, asserting that scientific evidence about the age of the universe is compatible with the Bible, but that the 'days' after Genesis 1:1 were each thousands of years in length.\n\nThe historic Christian literal interpretation of creation requires the harmonization of the two creation stories, Genesis 1:1–2:3 and Genesis 2:4–25, for there to be a consistent interpretation. They sometimes seek to ensure that their belief is taught in science classes, mainly in American schools. Opponents reject the claim that the literalistic biblical view meets the criteria required to be considered scientific. Many religious groups teach that God created the Cosmos. From the days of the early Christian Church Fathers there were allegorical interpretations of the Book of Genesis as well as literal aspects.\n\nChristian Science, a system of thought and practice derived from the writings of Mary Baker Eddy, interprets the Book of Genesis figuratively rather than literally. It holds that the material world is an illusion, and consequently not created by God: the only real creation is the spiritual realm, of which the material world is a distorted version. Christian Scientists regard the story of the creation in the Book of Genesis as having symbolic rather than literal meaning. According to Christian Science, both creationism and evolution are false from an absolute or \"spiritual\" point of view, as they both proceed from a (false) belief in the reality of a material universe. However, Christian Scientists do not oppose the teaching of evolution in schools, nor do they demand that alternative accounts be taught: they believe that both material science and literalist theology are concerned with the illusory, mortal and material, rather than the real, immortal and spiritual. With regard to material theories of creation, Eddy showed a preference for Darwin's theory of evolution over others.\n\nAccording to Hindu creationism, all species on Earth including humans have \"devolved\" or come down from a high state of pure consciousness. Hindu creationists claim that species of plants and animals are material forms adopted by pure consciousness which live an endless cycle of births and rebirths. Ronald Numbers says that: \"Hindu Creationists have insisted on the antiquity of humans, who they believe appeared fully formed as long, perhaps, as trillions of years ago.\" Hindu creationism is a form of old Earth creationism, according to Hindu creationists the universe may even be older than billions of years. These views are based on the Vedas, the creation myths of which depict an extreme antiquity of the universe and history of the Earth.\n\nIslamic creationism is the belief that the universe (including humanity) was directly created by God as explained in the Qur'an. It usually views the Book of Genesis as a corrupted version of God's message. The creation myths in the Qur'an are vaguer and allow for a wider range of interpretations similar to those in other Abrahamic religions.\n\nIslam also has its own school of theistic evolutionism, which holds that mainstream scientific analysis of the origin of the universe is supported by the Qur'an. Some Muslims believe in evolutionary creation, especially among liberal movements within Islam.\n\nWriting for \"The Boston Globe\", Drake Bennett noted: \"Without a Book of Genesis to account for ... Muslim creationists have little interest in proving that the age of the Earth is measured in the thousands rather than the billions of years, nor do they show much interest in the problem of the dinosaurs. And the idea that animals might evolve into other animals also tends to be less controversial, in part because there are passages of the Koran that seem to support it. But the issue of whether human beings are the product of evolution is just as fraught among Muslims.\" However, some Muslims, such as Adnan Oktar (also known as Harun Yahya), do not agree that one species can develop from another.\n\nSince the 1980s, Turkey has been a site of strong advocacy for creationism, supported by American adherents.\n\nThere are several verses in the Qur'an which some modern writers have interpreted as being compatible with the expansion of the universe, Big Bang and Big Crunch theories:\n\nThe Ahmadiyya movement actively promotes evolutionary theory. Ahmadis interpret scripture from the Qur'an to support the concept of macroevolution and give precedence to scientific theories. Furthermore, unlike orthodox Muslims, Ahmadis believe that mankind has gradually evolved from different species. Ahmadis regard Adam as being the first Prophet of Godas opposed to him being the first man on Earth. Rather than wholly adopting the theory of natural selection, Ahmadis promote the idea of a \"guided evolution,\" viewing each stage of the evolutionary process as having been selectively woven by God. Mirza Tahir Ahmad, Fourth Caliph of the Ahmadiyya Muslim Community has stated in his magnum opus \"Revelation, Rationality, Knowledge & Truth\" (1998) that evolution did occur but only through God being the One who brings it about. It does not occur itself, according to the Ahmadiyya Muslim Community.\n\nFor Orthodox Jews who seek to reconcile discrepancies between science and the creation myths in the Bible, the notion that science and the Bible should even be reconciled through traditional scientific means is questioned. To these groups, science is as true as the Torah and if there seems to be a problem, epistemological limits are to blame for apparently irreconcilable points. They point to discrepancies between what is expected and what actually is to demonstrate that things are not always as they appear. They note that even the root word for \"world\" in the Hebrew language—עולם (Olam)—means hidden—נעלם (Neh-Eh-Lahm). Just as they know from the Torah that God created man and trees and the light on its way from the stars in their observed state, so too can they know that the world was created in its over the six days of Creation that reflects progression to its currently-observed state, with the understanding that physical ways to verify this may eventually be identified. This knowledge has been advanced by Rabbi Dovid Gottlieb, former philosophy professor at Johns Hopkins University. Also, relatively old Kabbalistic sources from well before the scientifically apparent age of the universe was first determined are in close concord with modern scientific estimates of the age of the universe, according to Rabbi Aryeh Kaplan, and based on Sefer Temunah, an early kabbalistic work attributed to the first-century Tanna Nehunya ben HaKanah. Many kabbalists accepted the teachings of the Sefer HaTemunah, including the medieval Jewish scholar Nahmanides, his close student Isaac ben Samuel of Acre, and David ben Solomon ibn Abi Zimra. Other parallels are derived, among other sources, from Nahmanides, who expounds that there was a Neanderthal-like species with which Adam mated (he did this long before Neanderthals had even been discovered scientifically). Reform Judaism does not take the Torah as a literal text, but rather as a symbolic or open-ended work.\n\nSome contemporary writers such as Rabbi Gedalyah Nadel have sought to reconcile the discrepancy between the account in the Torah, and scientific findings by arguing that each day referred to in the Bible was not 24 hours, but billions of years long. Others claim that the Earth was created a few thousand years ago, but was deliberately made to look as if it was five billion years old, e.g. by being created with ready made fossils. The best known exponent of this approach being Rabbi Menachem Mendel Schneerson Others state that although the world was physically created in six 24 hour days, the Torah accounts can be interpreted to mean that there was a period of billions of years before the six days of creation.\n\nIn the creation myth taught by Bahá'u'lláh, the Bahá'í Faith founder, the universe has \"neither beginning nor ending,\" and that the component elements of the material world have always existed and will always exist. With regard to evolution and the origin of human beings, `Abdu'l-Bahá gave extensive comments on the subject when he addressed western audiences in the beginning of the 20th century. Transcripts of these comments can be found in \"Some Answered Questions\", \"Paris Talks\" and \"The Promulgation of Universal Peace\". `Abdu'l-Bahá described the human species as having evolved from a primitive form to modern man, but that the capacity to form human intelligence was always in existence.\n\nMost vocal literalist creationists are from the US, and strict creationist views are much less common in other developed countries. According to a study published in \"Science\", a survey of the US, Turkey, Japan and Europe showed that public acceptance of evolution is most prevalent in Iceland, Denmark and Sweden at 80% of the population. There seems to be no significant correlation between believing in evolution and understanding evolutionary science.\n\nA 2009 Nielsen poll, showed that almost a quarter of Australians believe \"the biblical account of human origins.\" Forty-two percent believe in a \"wholly scientific\" explanation for the origins of life, while 32 percent believe in an evolutionary process \"guided by God.\"\n\nA 2012 survey, by Angus Reid Public Opinion revealed that 61 percent of Canadians believe in evolution. The poll asked \"Where did human beings come fromdid we start as singular cells millions of year ago and evolve into our present form, or did God create us in his image 10,000 years ago?\"\n\nIn Europe, literalist creationism is more widely rejected, though regular opinion polls are not available. Most people accept that evolution is the most widely accepted scientific theory as taught in most schools. In countries with a Roman Catholic majority, papal acceptance of evolutionary creationism as worthy of study has essentially ended debate on the matter for many people.\n\nIn the UK, a 2006 poll on the \"origin and development of life\", asked participants to choose between three different perspectives on the origin of life: 22% chose creationism, 17% opted for intelligent design, 48% selected evolutionary theory, and the rest did not know. A subsequent 2010 YouGov poll on the correct explanation for the origin of humans found that 9% opted for creationism, 12% intelligent design, 65% evolutionary theory and 13% didn't know. The former Archbishop of Canterbury Rowan Williams, head of the worldwide Anglican Communion, views the idea of teaching creationism in schools as a mistake.\n\nIn Italy, Education Minister Letizia Moratti wanted to retire evolution from the secondary school level; after one week of massive protests, she reversed her opinion.\n\nThere continues to be scattered and possibly mounting efforts on the part of religious groups throughout Europe to introduce creationism into public education. In response, the Parliamentary Assembly of the Council of Europe has released a draft report titled \"The dangers of creationism in education\" on June 8, 2007, reinforced by a further proposal of banning it in schools dated October 4, 2007.\n\nSerbia suspended the teaching of evolution for one week in September 2004, under education minister Ljiljana Čolić, only allowing schools to reintroduce evolution into the curriculum if they also taught creationism. \"After a deluge of protest from scientists, teachers and opposition parties\" says the BBC report, Čolić's deputy made the statement, \"I have come here to confirm Charles Darwin is still alive\" and announced that the decision was reversed. Čolić resigned after the government said that she had caused \"problems that had started to reflect on the work of the entire government.\"\n\nPoland saw a major controversy over creationism in 2006, when the Deputy Education Minister, Mirosław Orzechowski, denounced evolution as \"one of many lies\" taught in Polish schools. His superior, Minister of Education Roman Giertych, has stated that the theory of evolution would continue to be taught in Polish schools, \"as long as most scientists in our country say that it is the right theory.\" Giertych's father, Member of the European Parliament Maciej Giertych, has opposed the teaching of evolution and has claimed that dinosaurs and humans co-existed.\n\nA 2017 poll by Pew Research found that 62% of Americans believe humans have evolved over time and 34% of Americans believe humans and other living things have existed in their present form since the beginning of time. Another 2017 Gallup creationism survey found that 38% of adults in the United States inclined to the view that \"God created humans in their present form at one time within the last 10,000 years\" when asked for their views on the origin and development of human beings, which Gallup noted was the lowest level in 35 years.\n\nAccording to a 2014 Gallup poll, about 42% of Americans believe that \"God created human beings pretty much in their present form at one time within the last 10,000 years or so.\" Another 31% believe that \"human beings have developed over millions of years from less advanced forms of life, but God guided this process,\"and 19% believe that \"human beings have developed over millions of years from less advanced forms of life, but God had no part in this process.\"\n\nBelief in creationism is inversely correlated to education; of those with postgraduate degrees, 74% accept evolution. In 1987, \"Newsweek\" reported: \"By one count there are some 700 scientists with respectable academic credentials (out of a total of 480,000 U.S. earth and life scientists) who give credence to creation-science, the general theory that complex life forms did not evolve but appeared 'abruptly.'\"\n\nA 2000 poll for People for the American Way found 70% of the US public felt that evolution was compatible with a belief in God.\n\nAccording to a study published in \"Science\", between 1985 and 2005 the number of adult North Americans who accept evolution declined from 45% to 40%, the number of adults who reject evolution declined from 48% to 39% and the number of people who were unsure increased from 7% to 21%. Besides the US the study also compared data from 32 European countries, Turkey, and Japan. The only country where acceptance of evolution was lower than in the US was Turkey (25%).\n\nAccording to a 2011 Fox News poll, 45% of Americans believe in Creationism, down from 50% in a similar poll in 1999. 21% believe in 'the theory of evolution as outlined by Darwin and other scientists' (up from 15% in 1999), and 27% answered that both are true (up from 26% in 1999).\n\nIn September 2012, educator and television personality Bill Nye spoke with the Associated Press and aired his fears about acceptance of creationism, believing that teaching children that creationism is the only true answer without letting them understand the way science works will prevent any future innovation in the world of science. In February 2014, Nye defended evolution in the classroom in a debate with creationist Ken Ham on the topic of whether creation is a viable model of origins in today's modern, scientific era.\n\nIn the US, creationism has become centered in the political controversy over creation and evolution in public education, and whether teaching creationism in science classes conflicts with the separation of church and state. Currently, the controversy comes in the form of whether advocates of the intelligent design movement who wish to \"Teach the Controversy\" in science classes have conflated science with religion.\n\nPeople for the American Way polled 1500 North Americans about the teaching of evolution and creationism in November and December 1999. They found that most North Americans were not familiar with Creationism, and most North Americans had heard of evolution, but many did not fully understand the basics of the theory. The main findings were:\nIn such political contexts, creationists argue that their particular religiously based origin belief is superior to those of other belief systems, in particular those made through secular or scientific rationale. Political creationists are opposed by many individuals and organizations who have made detailed critiques and given testimony in various court cases that the alternatives to scientific reasoning offered by creationists are opposed by the consensus of the scientific community.\n\nMost Christians disagree with the teaching of creationism as an alternative to evolution in schools. Several religious organizations, among them the Catholic Church, hold that their faith does not conflict with the scientific consensus regarding evolution. The Clergy Letter Project, which has collected more than 13,000 signatures, is an \"endeavor designed to demonstrate that religion and science can be compatible.\"\n\nIn his 2002 article \"Intelligent Design as a Theological Problem,\" George Murphy argues against the view that life on Earth, in all its forms, is direct evidence of God's act of creation (Murphy quotes Phillip E. Johnson's claim that he is speaking \"of a God who acted openly and left his fingerprints on all the evidence.\"). Murphy argues that this view of God is incompatible with the Christian understanding of God as \"the one revealed in the cross and resurrection of Christ.\" The basis of this theology is Isaiah 45:15, \"Verily thou art a God that hidest thyself, O God of Israel, the Saviour.\"\n\nMurphy observes that the execution of a Jewish carpenter by Roman authorities is in and of itself an ordinary event and did not require divine action. On the contrary, for the crucifixion to occur, God had to limit or \"empty\" Himself. It was for this reason that Paul the Apostle wrote, in Philippians 2:5-8:\n\nMurphy concludes that,\"Just as the Son of God limited himself by taking human form and dying on a cross, God limits divine action in the world to be in accord with rational laws which God has chosen. This enables us to understand the world on its own terms, but it also means that natural processes hide God from scientific observation.\"For Murphy, a theology of the cross requires that Christians accept a \"methodological\" naturalism, meaning that one cannot invoke God to explain natural phenomena, while recognizing that such acceptance does not require one to accept a \"metaphysical\" naturalism, which proposes that nature is all that there is.\n\nThe Jesuit priest George Coyne has stated that is \"unfortunate that, especially here in America, creationism has come to mean...some literal interpretation of Genesis.\" He argues that \"...Judaic-Christian faith is radically creationist, but in a totally different sense. It is rooted in belief that everything depends on God, or better, all is a gift from God.\"\n\nOther Christians have expressed qualms about teaching creationism. In March 2006, then Archbishop of Canterbury Rowan Williams, the leader of the world's Anglicans, stated his discomfort about teaching creationism, saying that creationism was \"a kind of category mistake, as if the Bible were a theory like other theories.\" He also said: \"My worry is creationism can end up reducing the doctrine of creation rather than enhancing it.\" The views of the Episcopal Churcha major American-based branch of the Anglican Communionon teaching creationism resemble those of Williams.\n\nThe National Science Teachers Association is opposed to teaching creationism as a science, as is the Association for Science Teacher Education, the National Association of Biology Teachers, the American Anthropological Association, the American Geosciences Institute, the Geological Society of America, the American Geophysical Union, and numerous other professional teaching and scientific societies.\n\nIn April 2010, the American Academy of Religion issued \"Guidelines for Teaching About Religion in K‐12 Public Schools in the United States\", which included guidance that creation science or intelligent design should not be taught in science classes, as \"Creation science and intelligent design represent worldviews that fall outside of the realm of science that is defined as (and limited to) a method of inquiry based on gathering observable and measurable evidence subject to specific principles of reasoning.\" However, they, as well as other \"worldviews that focus on speculation regarding the origins of life represent another important and relevant form of human inquiry that is appropriately studied in literature or social sciences courses. Such study, however, must include a diversity of worldviews representing a variety of religious and philosophical perspectives and must avoid privileging one view as more legitimate than others.\"\n\nRandy Moore and Sehoya Cotner, from the biology program at the University of Minnesota, reflect on the relevance of teaching creationism in the article \"The Creationist Down the Hall: Does It Matter When Teachers Teach Creationism?\" They conclude that \"Despite decades of science education reform, numerous legal decisions declaring the teaching of creationism in public-school science classes to be unconstitutional, overwhelming evidence supporting evolution, and the many denunciations of creationism as nonscientific by professional scientific societies, creationism remains popular throughout the United States.\"\n\nScience is a system of knowledge based on observation, empirical evidence, and the development of theories that yield testable explanations and predictions of natural phenomena. By contrast, creationism is often based on literal interpretations of the narratives of particular religious texts. Some creationist beliefs involve purported forces that lie outside of nature, such as supernatural intervention, and often do not allow predictions at all. Therefore, these can neither be confirmed nor disproved by scientists. However, many creationist beliefs can be framed as testable predictions about phenomena such as the age of the Earth, its geological history and the origins, distributions and relationships of living organisms found on it. Early science incorporated elements of these beliefs, but as science developed these beliefs were gradually falsified and were replaced with understandings based on accumulated and reproducible evidence that often allows the accurate prediction of future results.\n\nSome scientists, such as Stephen Jay Gould, consider science and religion to be two compatible and complementary fields, with authorities in distinct areas of human experience, so-called non-overlapping magisteria. This view is also held by many theologians, who believe that ultimate origins and meaning are addressed by religion, but favor verifiable scientific explanations of natural phenomena over those of creationist beliefs. Other scientists, such as Richard Dawkins, reject the non-overlapping magisteria and argue that, in disproving literal interpretations of creationists, the scientific method also undermines religious texts as a source of truth. Irrespective of this diversity in viewpoints, since creationist beliefs are not supported by empirical evidence, the scientific consensus is that any attempt to teach creationism as science should be rejected.\n\n\n\n", "id": "5326", "title": "Creationism"}
{"url": "https://en.wikipedia.org/wiki?curid=56101350", "text": "CAESAR (spacecraft)\n\nCAESAR (Comet Astrobiology Exploration Sample Return) is a proposed sample-return mission to comet 67P/Churyumov–Gerasimenko. The mission was proposed in 2017 to NASA's New Frontiers program mission 4, and on 20 December 2017 it was one of two finalists selected for further concept development.\n\nIf selected in July 2019, it may launch between 2024 and 2025, with a capsule delivering a sample to Earth in 2038. The Principal Investigator is Steve Squyres of Cornell University in New York. \"CAESAR\" would be managed by NASA's Goddard Space Flight Center in Greenbelt, Maryland, and the spacecraft built by Orbital ATK of Dulles, Virginia.\n\nThe \"CAESAR\" team chose comet 67P over other cometary targets in part because the data collected by the \"Rosetta\" mission that observed the comet from 2014 to 2016, allows the spacecraft to be designed to the conditions there, which in turn will increase the mission's chance of success. The \"Rosetta\" mission also provides a vast scientific context for this mission's sample-return analysis.\n\nThe two New Frontiers program mission 4 finalists, announced on 20 December 2017, are \"Dragonfly\" to Titan, and \"CAESAR\". Comet 67P was previously explored by the European Space Agency's \"Rosetta\" probe and its lander \"Philae\" during 2014-2015 to determine its origin and history. Squyres explained that knowing the existing conditions at the comet, it allows them to design systems that will dramatically improve the chances for success.\n\nThe \"CAESAR\" and \"Dragonfly\" missions will receive funding each through the end of 2018 to further develop and mature their concepts. NASA plans to select one of these proposals in July 2019 to build and launch in 2024 or 2025.\n\nA comet sample return mission was one of the goals in a list of options for a New Frontiers mission in both the 2003 and 2011 Planetary Science Decadal Survey, which were guiding surveys among those in the scientific community of what and where NASA should prioritize. Another comet mission proposal, \"Comet Hopper\", was one of three Discovery Program finalists that received in May 2011 to develop a detailed concept study, however it was not selected. NASA has launched several missions to comets in the late 1990s and 2000s; these missions include \"Deep Space 1\" (launched 1998), \"Stardust\" (launched 1999), \"CONTOUR\" (launched 2002 but failed after launch), and \"Deep Impact\" (launched 2005), as well as some participation on the \"Rosetta\" mission.\n\nSome researchers have speculated that Earth may have been seeded with organic compounds early in its development by tholin-rich comets, providing the raw material necessary for life to emerge. Tholins were detected by the \"Rosetta\" mission to comet 67P/Churyumov–Gerasimenko.\n\nThe spacecraft will not land on the comet, but would momentarily contact the surface with its TAG (Touch-And-Go) robotic arm. The sampler mechanism on the arm would produce a burst of nitrogen gas to blow regolith particles in to the sampler head located at the end of the arm. \"CAESAR\" will collect a total of between of regolith from the comet. The maximum pebble size would be .\n\nThe system would separate the volatiles from the solid substances and preserve the samples for the return trip. The spacecraft would head back to Earth and drop off the sample in a capsule, which would re-enter Earth's atmosphere and parachute down to the surface in 2038.\n\nA summary presentation by the Principal Investigator shows the spacecraft would perform a specimen collection procedure similar to what NASA's OSIRIS-REx mission will be doing on an asteroid, including raising the solar arrays into a Y-shaped configuration to minimize the chance of dust accumulation during contact and provide more ground clearance.\n\nThe propulsion system on \"CAESAR\" would be NASA's NEXT ion thruster, a type of solar electric propulsion.\n\n", "id": "56101350", "title": "CAESAR (spacecraft)"}
{"url": "https://en.wikipedia.org/wiki?curid=16866635", "text": "Evolution of fungi\n\nThe evolution of fungi has been going on since fungi diverged from other life around 1.5 billion years ago, (Wang et al., 1999) with the glomaleans branching from the \"higher fungi\" at ~, according to DNA analysis. (Schüssler et al., 2001; Tehler et al., 2000) Fungi probably colonized the land during the Cambrian, over , (Taylor & Osborn, 1996) but terrestrial fossils only become uncontroversial and common during the Devonian, .\n\nA rich diversity of fungi is known from the lower Devonian Rhynie chert; an earlier record is absent. Since fungi do not biomineralise, they do not readily enter the fossil record; there are only three claims of early fungi. One from the Ordovician has been dismissed on the grounds that it lacks any distinctly fungal features, and is held by many to be contamination; the position of a \"probable\" Proterozoic fungus is still not established, and it may represent a stem group fungus. There is also a case for a fungal affinity for the enigmatic microfossil \"Ornatifilum\". Since the fungi form a sister group to the animals, the two lineages must have diverged before the first animal lineages, which are known from fossils as early as the Ediacaran.\n\nIn contrast to plants and animals, the early fossil record of the fungi is meager. Factors that likely contribute to the under-representation of fungal species among fossils include the nature of fungal fruiting bodies, which are soft, fleshy, and easily degradable tissues and the microscopic dimensions of most fungal structures, which therefore are not readily evident. Fungal fossils are difficult to distinguish from those of other microbes, and are most easily identified when they resemble extant fungi. Often recovered from a permineralized plant or animal host, these samples are typically studied by making thin-section preparations that can be examined with light microscopy or transmission electron microscopy. Compression fossils are studied by dissolving the surrounding matrix with acid and then using light or scanning electron microscopy to examine surface details.\n\nThe earliest fossils possessing features typical of fungi date to the Paleoproterozoic era, some (Ma); these multicellular benthic organisms had filamentous structures capable of anastomosis, in which hyphal branches recombine. Other recent studies (2009) estimate the arrival of fungal organisms at about 760–1060 Ma on the basis of comparisons of the rate of evolution in closely related groups. For much of the Paleozoic Era (542–251 Ma), the fungi appear to have been aquatic and consisted of organisms similar to the extant Chytrids in having flagellum-bearing spores. Phylogenetic analyses suggest that the flagellum was lost early in the evolutionary history of the fungi, and consequently, the majority of fungal species lack a flagellum. The evolutionary adaptation from an aquatic to a terrestrial lifestyle necessitated a diversification of ecological strategies for obtaining nutrients, including parasitism, saprobism, and the development of mutualistic relationships such as mycorrhiza and lichenization. Recent (2009) studies suggest that the ancestral ecological state of the Ascomycota was saprobism, and that independent lichenization events have occurred multiple times.\n\nThe fungi probably colonized the land during the Cambrian (542–488.3 Ma), long before land plants. Fossilized hyphae and spores recovered from the Ordovician of Wisconsin (460 Ma) resemble modern-day Glomerales, and existed at a time when the land flora likely consisted of only non-vascular bryophyte-like plants. Prototaxites, which was probably a fungus or lichen, would have been the tallest organism of the late Silurian. Fungal fossils do not become common and uncontroversial until the early Devonian (416–359.2 Ma), when they are abundant in the Rhynie chert, mostly as Zygomycota and Chytridiomycota. At about this same time, approximately 400 Ma, the Ascomycota and Basidiomycota diverged, and all modern classes of fungi were present by the Late Carboniferous (Pennsylvanian, 318.1–299 Ma).\n\nLichen-like fossils have been found in the Doushantuo Formation in southern China dating back to 635–551 Ma. Lichens were a component of the early terrestrial ecosystems, and the estimated age of the oldest terrestrial lichen fossil is 400 Ma; this date corresponds to the age of the oldest known sporocarp fossil, a \"Paleopyrenomycites\" species found in the Rhynie Chert. The oldest fossil with microscopic features resembling modern-day basidiomycetes is \"Palaeoancistrus\", found permineralized with a fern from the Pennsylvanian. Rare in the fossil record are the homobasidiomycetes (a taxon roughly equivalent to the mushroom-producing species of the agaricomycetes). Two amber-preserved specimens provide evidence that the earliest known mushroom-forming fungi (the extinct species \"Archaeomarasmius legletti\") appeared during the mid-Cretaceous, 90 Ma.\n\nSome time after the Permian-Triassic extinction event (251.4 Ma), a fungal spike (originally thought to be an extraordinary abundance of fungal spores in sediments) formed, suggesting that fungi were the dominant life form at this time, representing nearly 100% of the available fossil record for this period. However, the relative proportion of fungal spores relative to spores formed by algal species is difficult to assess, the spike did not appear worldwide, and in many places it did not fall on the Permian-Triassic boundary.\n", "id": "16866635", "title": "Evolution of fungi"}
{"url": "https://en.wikipedia.org/wiki?curid=24968742", "text": "Evolution of influenza\n\nThe virus causing influenza is one of the best known pathogens found in various species. In particular, the virus is found in birds as well as mammals including horses, pigs, and humans. The phylogeny, or the evolutionary history of a particular species, is an important component when analyzing the evolution of influenza. Phylogenetic trees are graphical models of the relationships between various species. They can be used to trace the virus back to particular species and show how organisms that look so different may be so closely related.\n\nTwo common mechanisms by which viruses evolve are reassortment and genetic drift.\n\nReassortment, also known as antigenic shift, allows new viruses to evolve under both natural conditions and in artificial cultures. Reassortment occurs in similar fashion as chromosome crossover events, as two different viral strains may come in contact and transfer some of their genetic information. This crossing-over event creates a mixture of the two viral strains, which may replicate as one hybrid virus that expresses traits from both original viruses. The mechanism of the evolutionary force of antigenic shift allows influenza viruses to exchange genes with strains that infect different species. Under this mechanism, a human influenza virus could exchange genes with an avian strain, and that is how pandemic strains arise. There have been three occurrences of pandemics caused by antigenic shift since 1900, and it could just as easily happen again. In fact, the 1957 evolution of the H2N2 virus is thought to be a result of reassortment. In this case, human H1N1 strains and avian influenza A genes were mixed. Infecting tissue cultures can demonstrate how pathogenic qualities can evolve for a particular species even though the reassorted virus may be nonpathogenic for another species. A prime example of evolution under natural conditions is the reassortment of two avian influenza strains that were discovered in dead seals back in 1979.\n\nNew viruses can also emerge by drift. Drift can refer to genetic drift or antigenic drift. Mutation and selection for the most advantageous variation of the virus takes place during this form of evolution. Antigenic mutants can evolve quickly due to the high mutation rate in viruses. The cause of the antigenic drift lies in the mechanisms of RNA synthesis itself. Mutations arise very easily simply due to the error prone RNA polymerase and its lack of proofreading mechanisms. These mutations lead to subtle changes in the HA and NA genes which completely changes the infectious capabilities of the virus. These changes allow for almost endless possibilities for new viral strains to arise and it is the antigenic drift of the HA and NA genes that allow for the virus to infect humans that receive vaccines for other strains of the virus. This evolution occurs under the pressure of antibodies or immune system responses.\n\nThe transmission, or how the influenza virus is passed from one species to another, varies. There are barriers that prevent the flow of the virus between some species ranging from high to low transmission. For example, there is no direct pathway between humans and birds. Pigs however, serve as an open pathway. There is a limited barrier for them to spread the virus. Therefore, pigs act as a donator of the virus relatively easily.\n\nPhylogenetic maps are a graphical representation of the geographic relationships among species. They indicate that the human influenza virus is minimally impacted by geographic differences. However, both swine and avian influenza does appear to be geographically dependent. All three groups (avian, swine, and human) show chronological differences. The human influenza virus is retained in humans only, meaning it does not spread to other species. Some lineages and sublineages of the virus emerge and may be more prevalent in certain locations. For instance, many human influenza outbreaks begin in Southeast Asia.\n\nPhylogenetic analysis can help determine past viruses and their patterns as well as determining a common ancestor of the virus. Past studies reveal that an avian virus spread to pigs and then to humans approximately 100 years ago. This resulted in human lineages further evolving and becoming more prominent and stable.\n\nAnalysis can also feature relationships between species. The 1918 Spanish influenza virus demonstrates this. The hemagglutinin (HA) gene of the 1918 pandemic virus was closer in sequence to avian strains than other mammalian ones. Despite this genetic similarity, it is obviously a mammalian virus. The gene may have been adapting in humans even prior to 1918. Breaking down the phylogenetic history of the influenza virus shows that there is a common ancestor that reaches back before the 1918 outbreak that links the current human virus to the swine virus. The ancestor was derived from an avian host.\n\nLooking at the past phylogenetic relationships of the influenza virus can help lead to information regarding treatment, resistance, vaccine strain selection, and of future possible influenza strains. By looking at how previous strains have evolved and gained new traits, the information can be applied to predict how current strains can evolve and even how novel strains might come about. Another use of phylogeny for predicting future viral dangers would be through using phylogeography. Various lineages may continue their presence and reassort indicating the importance of a complete-genome approach to determine new influenza strains and future epidemics. By studying how past strains have evolved while spreading to different geographic regions can allow scientists to predict how a strain might accumulate new mutations through its geographic distribution and the information could be used to protect different populations.\n\nAll of these methods using historical data can help to diminish the effects of new influenza virus strains each flu season. By attempting to predict future mutations in HA and NA genes, scientists can choose vaccination strains that are likely to match future viruses, so antibodies can quickly recognize and mount an immune response against the virus. The one setback in this approach is that it is not useful against strains that evolve through antigenic shift (reassortment). It is impossible to predict when and with which strains these events will occur, and the fact that it could happen with strains from different species makes it all the more difficult. Until a method is found to accurately predict what mutations will arise and when they come about, vaccines will continue to be created purely on guesswork with no guarantee that they will provide total protection from influenza.\n\nIn current years, there has been a huge increase in the amount of resistance to certain drugs, including the antiviral compound adamantane. In fact, its resistance has recently climbed from 2 percent to nearly 90 percent. These records of built up resistance infer that drugs, such as adamantine, will not be useful against the influenza virus in the future.\n", "id": "24968742", "title": "Evolution of influenza"}
{"url": "https://en.wikipedia.org/wiki?curid=11680057", "text": "Radiotrophic fungus\n\nRadiotrophic fungi are fungi which appear to perform radiosynthesis, that is, to use the pigment melanin to convert gamma radiation into chemical energy for growth. This proposed mechanism may be similar to anabolic pathways for the synthesis of reduced organic carbon (e.g., carbohydrates) in phototrophic organisms, which capture photons from visible light with pigments such as chlorophyll whose energy is then used in photolysis of water to generate usable chemical energy (as ATP) in photophosphorylation or photosynthesis. However, whether melanin-containing fungi employ a similar multi-step pathway as photosynthesis, or some chemosynthesis pathways, is unknown.\n\nThese were discovered in 1991 growing inside and around the Chernobyl Nuclear Power Plant. Research at the Albert Einstein College of Medicine showed that three melanin-containing fungi—\"Cladosporium sphaerospermum\", \"Wangiella dermatitidis\", and \"Cryptococcus neoformans\"—increased in biomass and accumulated acetate faster in an environment in which the radiation level was 500 times higher than in the normal environment. Exposure of \"C. neoformans\" cells to these radiation levels rapidly (within 20–40 minutes of exposure) altered the chemical properties of its melanin, and increased melanin-mediated rates of electron transfer (measured as reduction of ferricyanide by NADH) three- to four-fold compared with unexposed cells. Similar effects on melanin electron-transport capability were observed by the authors after exposure to non-ionizing radiation, suggesting that melanotic fungi might also be able to use light or heat radiation for growth.\n\nHowever, melanization may come at some metabolic cost to the fungal cells: in the absence of radiation, some non-melanized fungi (that had been mutated in the melanin pathway) grew faster than their melanized counterparts. Limited uptake of nutrients due to the melanin molecules in the fungal cell wall or toxic intermediates formed in melanin biosynthesis have been suggested to contribute to this phenomenon. It is consistent with the observation that despite being capable of producing melanin, many fungi do not synthesize melanin constitutively (i.e., all the time), but often only in response to external stimuli or at different stages of their development. The exact biochemical processes in the suggested melanin-based synthesis of organic compounds or other metabolites for fungal growth, including the chemical intermediates (such as native electron donor and acceptor molecules) in the fungal cell and the location and chemical products of this process, are unknown.\n\n\n", "id": "11680057", "title": "Radiotrophic fungus"}
{"url": "https://en.wikipedia.org/wiki?curid=47482979", "text": "Semiaquatic\n\nSemiaquatic can refer to animals of a number of types that spend part of their time in water, or plants which place part of their body in water. Examples include:\n\n", "id": "47482979", "title": "Semiaquatic"}
{"url": "https://en.wikipedia.org/wiki?curid=14329", "text": "Historicism\n\nHistoricism is the idea of attributing meaningful significance to space and time, such as historical period, geographical place, and local culture. Historicism tends to be hermeneutical because it values cautious, rigorous, and contextualized interpretation of information; or relativist, because it rejects notions of universal, fundamental and immutable interpretations. The approach varies from individualist theories of knowledge such as empiricism and rationalism, which neglect the role of traditions.\n\nThe term \"historicism\" (\"Historismus\") was coined by German philosopher Karl Wilhelm Friedrich Schlegel. Over time it has developed different and somewhat divergent meanings. Elements of historicism appear in the writings of French essayist Michel de Montaigne (1533–1592) and Italian philosopher G. B. Vico (1668–1744), and became more fully developed with the dialectic of Georg Hegel (1770–1831), influential in 19th-century Europe. The writings of Karl Marx, influenced by Hegel, also include historicism. The term is also associated with the empirical social sciences and with the work of Franz Boas.\n\nHistoricism may be contrasted with reductionist theories—which assumes that all developments can be explained by fundamental principles (such as in economic determinism)—or with theories that posit that historical changes occur as a result of random chance.\n\nThe Austrian-English philosopher Karl Popper condemned historicism along with the determinism and holism which he argued formed its basis. In his \"Poverty of Historicism\", he identified historicism with the opinion that there are \"inexorable laws of historical destiny\", which opinion he warned against. This contrasts with the contextually relative interpretation of historicism for which its proponents argue. Talcott Parsons criticized historicism as a case of idealistic fallacy in \"The Structure of Social Action\" (1937).\n\nPost-structuralism uses the term \"New Historicism\", which has some associations with both anthropology and Hegelianism.\n\nThe theological use of the word denotes the interpretation of biblical prophecy as being related to church history.\n\nHegel viewed the realization of human freedom as the ultimate purpose of history, which could only be achieved through the creation of the perfect state. And this progressive history would only occur through a dialectical process: namely, the tension between the purpose of humankind (freedom), the position that humankind currently finds itself, and mankind's attempt to bend the current world into accord with its nature. However, because humans are often not aware of the goal of both humanity and history, the process of achieving freedom is necessarily one of self-discovery. Hegel also saw the progress toward freedom being conducted by the \"spirit\" (Geist), a seemingly supernatural force that directed all human actions and interactions. Yet Hegel makes clear that the spirit is a mere abstraction, and only comes into existence \"through the activity of finite agents.\" Thus, Hegel's philosophy of history is not necessarily metaphysical, despite the fact that many of Hegel's opponents and interpreters have understood Hegel's philosophy of history as a metaphysical and determinist view of history. For example, Karl Popper in his book \"The Poverty of Historicism\" interpreted Hegel's philosophy of history as metaphysical and deterministic. Popper referred to this \"Hegelian\" philosophy of history as \"Historicism\".\n\nHegel's historicism also suggests that any human society and all human activities such as science, art, or philosophy, are defined by their history. Consequently, their essence can be sought only by understanding said history. The history of any such human endeavor, moreover, not only continues but also reacts against what has gone before; this is the source of Hegel's famous dialectic teaching usually summarized by the slogan \"thesis, antithesis, and synthesis\". (Hegel did not use these terms, although Johann Fichte did.) Hegel's famous aphorism, \"Philosophy is the history of philosophy,\" describes it bluntly.\n\nHegel's position is perhaps best illuminated when contrasted against the atomistic and reductionist opinion of human societies and social activities self-defining on an \"ad hoc\" basis through the sum of dozens of interactions. Yet another contrasting model is the persistent metaphor of a social contract. Hegel considers the relationship between individuals and societies as organic, not atomic: even their social discourse is mediated by language, and language is based on etymology and unique character. It thus preserves the culture of the past in thousands of half-forgotten metaphors. To understand why a person is the way he is, you must examine that person in his society: and to understand that society, you must understand its history, and the forces that influenced it. The \"Zeitgeist\", the \"Spirit of the Age,\" is the concrete embodiment of the most important factors that are acting in human history at any given time. This contrasts with teleological theories of activity, which suppose that the end is the determining factor of activity, as well as those who believe in a tabula rasa, or blank slate, opinion, such that individuals are defined by their interactions.\n\nThese ideas can be interpreted variously. The Right Hegelians, working from Hegel's opinions about the organicism and historically determined nature of human societies, interpreted Hegel's historicism as a justification of the unique destiny of national groups and the importance of stability and institutions. Hegel's conception of human societies as entities greater than the individuals who constitute them influenced nineteenth-century romantic nationalism and its twentieth-century excesses. The Young Hegelians, by contrast, interpreted Hegel's thoughts on societies influenced by social conflict for a doctrine of social progress, and attempted to manipulate these forces to cause various results. Karl Marx's doctrine of \"historical inevitabilities\" and historical materialism is one of the more influential reactions to this part of Hegel's thought. Significantly, Karl Marx's theory of alienation argues that capitalism disrupts traditional relationships between workers and their work.\n\nHegelian historicism is related to his ideas on the means by which human societies progress, specifically the dialectic and his conception of logic as representing the inner essential nature of reality. Hegel attributes the change to the \"modern\" need to interact with the world, whereas ancient philosophers were self-contained, and medieval philosophers were monks. In his History of Philosophy Hegel writes:\nIn modern times things are very different; now we no longer see philosophic individuals who constitute a class by themselves. With the present day all difference has disappeared; philosophers are not monks, for we find them generally in connection with the world, participating with others in some common work or calling. They live, not independently, but in the relation of citizens, or they occupy public offices and take part in the life of the state. Certainly they may be private persons, but if so, their position as such does not in any way isolate them from their other relationship. They are involved in present conditions, in the world and its work and progress. Thus their philosophy is only by the way, a sort of luxury and superfluity. This difference is really to be found in the manner in which outward conditions have taken shape after the building up of the inward world of religion. In modern times, namely, on account of the reconciliation of the worldly principle with itself, the external world is at rest, is brought into order — worldly relationships, conditions, modes of life, have become constituted and organized in a manner which is conformable to nature and rational. We see a universal, comprehensible connection, and with that individuality likewise attains another character and nature, for it is no longer the plastic individuality of the ancients. This connection is of such power that every individuality is under its dominion, and yet at the same time can construct for itself an inward world.\nThis opinion that entanglement in society creates an indissoluble bond with expression, would become an influential question in philosophy, namely, the requirements for individuality. It would be considered by Nietzsche, John Dewey and Michel Foucault directly, as well as in the work of numerous artists and authors. There have been various responses to Hegel's challenge. The Romantic period emphasized the ability of individual genius to transcend time and place, and use the materials from their heritage to fashion works which were beyond determination. The modern would advance versions of John Locke's infinite malleability of the human animal. Post-structuralism would argue that since history is not present, but only the image of history, that while an individual era or power structure might emphasize a particular history, that the contradictions within the story would hinder the very purposes that the history was constructed to advance.\n\nIn the context of anthropology and other sciences which study the past, historicism has a different meaning. Anthropological historicism is associated with the work of Franz Boas. His theory used the diffusionist concept that there were a few \"cradles of civilization\" which grew outwards, and merged it with the idea that societies would adapt to their circumstances, which is called historical particularism. The school of historicism grew in response to unilinear theories that social development represented adaptive fitness, and therefore existed on a continuum. While these theories were espoused by Charles Darwin and many of his students, their application as applied in social Darwinism and general evolution characterized in the theories of Herbert Spencer and Leslie White, historicism was neither anti-selection, nor anti-evolution, as Darwin never attempted nor offered an explanation for cultural evolution. However, it attacked the notion that there was one normative spectrum of development, instead emphasizing how local conditions would create adaptations to the local environment. Julian Steward refuted the viability of globally and universally applicable adaptive standards proposing that culture was honed adaptively in response to the idiosyncrasies of the local environment, the cultural ecology, by specific evolution. What was adaptive for one region might not be so for another. This conclusion has likewise been adopted by modern forms of biological evolutionary theory.\n\nThe primary method of historicism was empirical, namely that there were so many requisite inputs into a society or event, that only by emphasizing the data available could a theory of the source be determined. In this opinion, grand theories are unprovable, and instead intensive field work would determine the most likely explanation and history of a culture, and hence it is named \"historicism.\"\n\nThis opinion would produce a wide range of definition of what, exactly, constituted culture and history, but in each case the only means of explaining it was in terms of the historical particulars of the culture itself.\n\nSince the 1950s, when Jacques Lacan and Foucault argued that each epoch has its own knowledge system, within which individuals are inexorably entangled, many post-structuralists have used \"historicism\" to describe the opinion that all questions must be settled within the cultural and social context in which they are raised. Answers cannot be found by appeal to an external truth, but only within the confines of the norms and forms that phrase the question. This version of historicism holds that there are only the raw texts, markings and artifacts that exist in the present, and the conventions used to decode them. This school of thought is sometimes given the name of \"New Historicism\".\n\nThe same term, \"new historicism\" is also used for a school of literary scholarship which interprets a poem, drama, etc. as an expression of or reaction to the power-structures of its society. Stephen Greenblatt is an example of this school.\n\nWithin the context of 20th-century philosophy, debates continue as to whether ahistorical and immanent methods were sufficient to understand meaning—that is to say, \"what you see is what you get\" positivism—or whether context, background and culture are important beyond the mere need to decode words, phrases and references. While post-structural historicism is relativist in its orientation, that is, it sees each culture as its own frame of reference, a large number of thinkers have embraced the need for historical context, not because culture is self-referential, but because there is no more compressed means of conveying all of the relevant information except through history. This opinion is often seen as deriving from the work of Benedetto Croce. Recent historians using this tradition include Thomas Kuhn.\n\nIn Christianity, the term \"historicism\" refers to the confessional Protestant form of prophetical interpretation which holds that the fulfillment of biblical prophecy has occurred throughout history and continues to occur; as opposed to other methods which limit the time-frame of prophecy-fulfillment to the past or to the future.\n\nThere is also a particular opinion in ecclesiastical history and in the history of dogmas which has been described as historicist by Pope Pius XII in the encyclical \"Humani generis\". \"They add that the history of dogmas consists in the reporting of the various forms in which revealed truth has been clothed, forms that have succeeded one another in accordance with the different teachings and opinions that have arisen over the course of the centuries.\"\n\nThe social theory of Karl Marx, with respect to modern scholarship, has an ambiguous relation to historicism. Critics of Marx have charged his theory with historicism since its very genesis. However, the issue of historicism also finds itself important to many debates within Marxism itself; the charge of historicism has been made against various types of Marxism, typically disparaged by Marxists as \"vulgar\" Marxism.\n\nMarx himself expresses critical concerns with this historicist tendency in his Theses on Feuerbach:\n\nKarl Popper used the term \"historicism\" in his influential books \"The Poverty of Historicism\" and \"The Open Society and Its Enemies\", to mean: \"an approach to the social sciences which assumes that \"historical prediction\" is their primary aim, and which assumes that this aim is attainable by discovering the 'rhythms' or the 'patterns', the 'laws' or the 'trends' that underlie the evolution of history\". Karl Popper wrote with reference to Hegel's theory of history, which he criticized extensively. However, there is wide dispute whether Popper's description of \"historicism\" is an accurate description of Hegel, or more his characterisation of his own philosophical antagonists, including Marxist-Leninist thought, then widely held as posing a challenge to the philosophical basis of the West, as well as theories such as Spengler's which drew predictions about the future course of events from the past.\n\nIn \"The Open Society and Its Enemies\", Popper attacks \"historicism\" and its proponents, among whom (as well as Hegel) he identifies and singles out Plato and Marx—calling them all \"enemies of the open society\". The objection he makes is that historicist positions, by claiming that there is an inevitable and deterministic pattern to history, abrogate the democratic responsibility of each one of us to make our own free contributions to the evolution of society, and hence lead to totalitarianism.\n\nAnother of his targets is what he terms \"moral historicism\", the attempt to infer moral values from the course of history; in Hegel's words, that \"history is the world's court of justice\". This may take the form of conservatism (former might is right), positivism (might is right) or futurism (presumed coming might is right). As against these, Popper says that he does not believe \"that success proves anything or that history is our judge\". Futurism must be distinguished from prophecies that the right will prevail: these attempt to infer history from ethics, rather than ethics from history, and are therefore historicism in the normal sense rather than moral historicism.\n\nHe also attacks what he calls \"Historism\", which he regards as distinct from historicism. By historism, he means the tendency to regard every argument or idea as completely accounted for by its historical context, as opposed to assessing it by its merits. In Popperian terms, the \"New Historicism\" is an example of historism rather than of historicism proper.\n\nLeo Strauss used the term \"historicism\" and reportedly termed it the single greatest threat to intellectual freedom insofar as it denies any attempt to address injustice-pure-and-simple (such is the significance of historicism's rejection of \"natural right\" or \"right by nature\"). Strauss argued that historicism \"rejects political philosophy\" (insofar as this stands or falls by questions of permanent, trans-historical significance) and is based on the belief that \"all human thought, including scientific thought, rests on premises which cannot be validated by human reason and which came from historical epoch to historical epoch.\" Strauss further identified R. G. Collingwood as the most coherent advocate of historicism in the English language. Countering Collingwood's arguments, Strauss warned against historicist social scientists' failure to address real-life problems—most notably that of tyranny—to the extent that they relativize (or \"subjectivize\") all ethical problems by placing their significance strictly in function of particular or ever-changing socio-material conditions devoid of inherent or \"objective\" \"value.\" Similarly, Strauss criticized Eric Voegelin's abandonment of ancient political thought as guide or vehicle in interpreting modern political problems.\n\nIn his books, \"Natural Right and History\" and \"On Tyranny\", Strauss offers a complete critique of historicism as it emerges in the works of Hegel, Marx, and Heidegger. Many believe that Strauss also found historicism in Edmund Burke, Tocqueville, Augustine, and John Stuart Mill. Although it is largely disputed whether Strauss himself was a historicist, he often indicated that historicism grew out of and against Christianity and was a threat to civic participation, belief in human agency, religious pluralism, and, most controversially, an accurate understanding of the classical philosophers and religious prophets themselves. Throughout his work, he warns that historicism, and the understanding of progress that results from it, expose us to tyranny, totalitarianism, and democratic extremism. In his exchange with Alexandre Kojève in \"On Tyranny\", Strauss seems to blame historicism for Nazism and Communism. In a collection of his works by Kenneth Hart entitled \"Jewish Philosophy and the Crisis of Modernity\", he argues that Islam, traditional Judaism, and ancient Greece, share a concern for sacred law that makes them especially susceptible to historicism, and therefore to tyranny. Strauss makes use of Nietzsche's own critique of progress and historicism, although Strauss refers to Nietzsche himself (no less than to Heidegger) as a \"radical historicist\" who articulated a philosophical (if only untenable) justification for historicism.\n\n\n\n", "id": "14329", "title": "Historicism"}
{"url": "https://en.wikipedia.org/wiki?curid=2132454", "text": "Evolutionary graph theory\n\nEvolutionary graph theory is an area of research lying at the intersection of graph theory, probability theory, and mathematical biology. Evolutionary graph theory is an approach to studying how topology affects evolution of a population. That the underlying topology can substantially affect the results of the evolutionary process is seen most clearly in a paper by Erez Lieberman, Christoph Hauert and Martin Nowak.\n\nIn evolutionary graph theory, individuals occupy vertices of a weighted directed graph and the weight w of an edge from vertex \"i\" to vertex \"j\" denotes the probability of \"i\" replacing \"j\". The weight corresponds to the biological notion of fitness where fitter types propagate more readily. \nOne property studied on graphs with two types of individuals is the \"fixation probability\", which is defined as the probability that a single, randomly placed mutant of type A will replace a population of type B. According to the \"isothermal theorem\", a graph has the same fixation probability as the corresponding Moran process if and only if it is isothermal, thus the sum of all weights that lead into a vertex is the same for all vertices. Thus, for example, a complete graph with equal weights describes a Moran process. The fixation probability is\nwhere \"r\" is the relative fitness of the invading type.\n\nGraphs can be classified into amplifiers of selection and suppressors of selection. If the fixation probability of a single advantageous mutation formula_2 is higher than the fixation probability of the corresponding Moran process formula_3 then the graph is an amplifier, otherwise a suppressor of selection. One example of the suppressor of selection is a linear process where only vertex \"i-1\" can replace vertex \"i\" (but not the other way around). In this case the fixation probability is formula_4 (where \"N\" is the number of vertices) since this is the probability that the mutation arises in the first vertex which will eventually replace all the other ones. Since formula_5 for all \"r\" greater than 1, this graph is by definition a suppressor of selection.\n\nEvolutionary graph theory may also be studied in a dual formulation, as a coalescing random walk.\n\nAlso evolutionary games can be studied on graphs where again an edge between \"i\" and \"j\" means that these two individuals will play a game against each other.\n\nClosely related stochastic processes include the voter model, which was introduced by Clifford and Sudbury (1973) and independently by Holley and Liggett (1975), and which has been studied extensively.\n\n\nA virtual laboratory for studying evolution on graphs:\n", "id": "2132454", "title": "Evolutionary graph theory"}
{"url": "https://en.wikipedia.org/wiki?curid=2466610", "text": "Metasystem transition\n\nA metasystem transition is the emergence, through evolution, of a higher level of organization or control.\n\nA metasystem is formed by the integration of a number of initially independent components, such as molecules (as theorized for instance by hypercycles), cells, or individuals, and the emergence of a system steering or controlling their interactions. As such, the collective of components becomes a new, goal-directed individual, capable of acting in a coordinated way. This metasystem is more complex, more intelligent, and more flexible in its actions than the initial component systems. Prime examples are the origin of life, the transition from unicellular to multicellular organisms, the emergence of eusociality or symbolic thought. \n\nThe concept of metasystem transition was introduced by the cybernetician Valentin Turchin in his 1970 book \"The Phenomenon of Science\", and developed among others by Francis Heylighen in the Principia Cybernetica Project. The related notion of evolutionary transition was proposed by the biologists John Maynard Smith and Eörs Szathmáry, in their 1995 book \"The Major Transitions in Evolution\". Another related idea, that systems (\"operators\") evolve to become more complex by successive closures encapsulating components in a larger whole, is proposed in \"the operator theory\", developed by Gerard Jagers op Akkerhuis.\n\nTurchin has applied the concept of metasystem transition in the domain of computing, via the notion of metacompilation or supercompilation. A supercompiler is a compiler program that compiles its own code, thus increasing its own efficiency, producing a remarkable speedup in its execution.\n\nThe following is the classical sequence of metasystem transitions in the history of animal evolution according to Turchin, from the origin of animate life to sapient culture:\n\n\nMany argue that the next human metasystem transition consists of a merger of biological metasystems with technological metasystems, especially information processing technology. Several cumulative major transitions of evolution have transformed life through key innovations in information storage and replication, including RNA, DNA, multicellularity, and also language and culture as inter-human information processing systems. In this sense it can be argued that the carbon-based biosphere has generated a cognitive system (humans) capable of creating technology that will result in a comparable evolutionary transition. \"Digital information has reached a similar magnitude to information in the biosphere... Like previous evolutionary transitions, the potential symbiosis between biological and digital information will reach a critical point where these codes could compete via natural selection. Alternatively, this fusion could create a higher-level superorganism employing a low-conflict division of labor in performing informational tasks... humans already embrace fusions of biology and technology. We spend most of our waking time communicating through digitally mediated channels, ...most transactions on the stock market are executed by automated trading algorithms, and our electric grids are in the hands of artificial intelligence. With one in three marriages in America beginning online, digital algorithms are also taking a role in human pair bonding and reproduction\".\n\n\n", "id": "2466610", "title": "Metasystem transition"}
{"url": "https://en.wikipedia.org/wiki?curid=1758866", "text": "Accelerating change\n\nIn futures studies and the history of technology, accelerating change is a perceived increase in the rate of technological change throughout history, which may suggest faster and more profound change in the future and may or may not be accompanied by equally profound social and cultural change.\n\nIn 1910 during the town planning conference of London Daniel Burnham noted that \"But it is not merely in the number of facts or sorts of knowledge that progress lies : it is still more in the geometric ratio of sophistication, in the geometric widening of the sphere of knowledge, which every year is taking in a larger percentage of people as time goes on.\" and later on \"It is the argument with which I began, that a mighty change having come about in fifty years, and our pace of development having- immensely accelerated, our sons and grandsons are going to demand and get results that would stagger us.\"\nIn 1938, Buckminster Fuller introduced the word ephemeralization to describe the trends of \"doing more with less\" in chemistry, health and other areas of industrial development. In 1946, Fuller published a chart of the discoveries of the chemical elements over time to highlight the development of accelerating acceleration in human knowledge acquisition.\n\nIn 1958, Stanislaw Ulam wrote in reference to a conversation with John von Neumann: \n\nIn a series of published articles from 1974-1979, and then in his 1988 book \"Mind Children\", computer scientist and futurist Hans Moravec generalizes Moore's law to make predictions about the future of artificial life. Moore's law describes an exponential growth pattern in the complexity of integrated semiconductor circuits. Moravec extends this to include technologies from long before the integrated circuit to future forms of technology. Moravec outlines a timeline and a scenario in which robots will evolve into a new series of artificial species, starting around 2030-2040.\nIn \"Robot: Mere Machine to Transcendent Mind\", published in 1998, Moravec further considers the implications of evolving robot intelligence, generalizing Moore's Law to technologies predating the integrated circuit, and also plotting the exponentially increasing computational power of the brains of animals in evolutionary history. Extrapolating these trends, he speculates about a coming \"mind fire\" of rapidly expanding superintelligence similar to the explosion of intelligence predicted by Vinge.\n\nIn his TV series \"Connections\" (1978)—and sequels \"Connections²\" (1994) and \"Connections³\" (1997)—James Burke explores an \"Alternative View of Change\" (the subtitle of the series) that rejects the conventional linear and teleological view of historical progress. Burke contends that one cannot consider the development of any particular piece of the modern world in isolation. Rather, the entire gestalt of the modern world is the result of a web of interconnected events, each one consisting of a person or group acting for reasons of their own motivations (e.g., profit, curiosity, religious) with no concept of the final, modern result to which the actions of either them or their contemporaries would lead. The interplay of the results of these isolated events is what drives history and innovation, and is also the main focus of the series and its sequels.\n\nBurke also explores three corollaries to his initial thesis. The first is that, if history is driven by individuals who act only on what they know at the time, and not because of any idea as to where their actions will eventually lead, then predicting the future course of technological progress is merely conjecture. Therefore, if we are astonished by the connections Burke is able to weave among past events, then we will be equally surprised to what the events of today eventually will lead, especially events we weren't even aware of at the time.\n\nThe second and third corollaries are explored most in the introductory and concluding episodes, and they represent the downside of an interconnected history. If history progresses because of the synergistic interaction of past events and innovations, then as history does progress, the number of these events and innovations increases. This increase in possible connections causes the process of innovation to not only continue, but to accelerate. Burke poses the question of what happens when this rate of innovation, or more importantly change itself, becomes too much for the average person to handle, and what this means for individual power, liberty, and privacy.\n\nIn his book \"Mindsteps to the Cosmos\" (HarperCollins, August 1983), Gerald S. Hawkins elucidated his notion of 'mindsteps', dramatic and irreversible changes to paradigms or world views. He identified five distinct mindsteps in human history, and the technology that accompanied these \"new world views\": the invention of imagery, writing, mathematics, printing, the telescope, rocket, radio, TV, computer... \"Each one takes the collective mind closer to reality, one stage further along in its understanding of the relation of humans to the cosmos.\" He noted: \"The waiting period between the mindsteps is getting shorter. One can't help noticing the acceleration.\" Hawkins' empirical 'mindstep equation' quantified this, and gave dates for future mindsteps. The date of the next mindstep (5; the series begins at 0) is given as 2021, with two further, successively closer mindsteps in 2045 and 2051, until the limit of the series in 2053. His speculations ventured beyond the technological:\n\nThe mathematician Vernor Vinge popularized his ideas about exponentially accelerating technological change in the science fiction novel \"Marooned in Realtime\" (1986), set in a world of rapidly accelerating progress leading to the emergence of more and more sophisticated technologies separated by shorter and shorter time intervals, until a point beyond human comprehension is reached. His subsequent Hugo award-winning novel \"A Fire Upon the Deep\" (1992) starts with an imaginative description of the evolution of a superintelligence passing through exponentially accelerating developmental stages ending in a transcendent, almost omnipotent power unfathomable by mere humans. His already mentioned influential 1993 paper on the technological singularity compactly summarizes the basic ideas.\n\nIn his 1999 book \"The Age of Spiritual Machines\" Ray Kurzweil proposed \"The Law of Accelerating Returns\", according to which the rate of change in a wide variety of evolutionary systems (including but not limited to the growth of technologies) tends to increase exponentially. He gave further focus to this issue in a 2001 essay entitled \"The Law of Accelerating Returns\". In it, Kurzweil, after Moravec, argued for extending Moore's Law to describe exponential growth of diverse forms of technological progress. Whenever a technology approaches some kind of a barrier, according to Kurzweil, a new technology will be invented to allow us to cross that barrier. He cites numerous past examples of this to substantiate his assertions. He predicts that such paradigm shifts have and will continue to become increasingly common, leading to \"technological change so rapid and profound it represents a rupture in the fabric of human history.\" He believes the Law of Accelerating Returns implies that a technological singularity will occur before the end of the 21st century, around 2045. The essay begins:\n\nThe Law of Accelerating Returns has in many ways altered public perception of Moore's law. It is a common (but mistaken) belief that Moore's law makes predictions regarding all forms of technology, when really it only concerns semiconductor circuits. Many futurists still use the term \"Moore's law\" to describe ideas like those put forth by Moravec, Kurzweil and others.\n\nAccording to Kurzweil, since the beginning of evolution, more complex life forms have been evolving exponentially faster, with shorter and shorter intervals between the emergence of radically new life forms, such as human beings, who have the capacity to engineer (intentionally to design with efficiency) a new trait which replaces relatively blind evolutionary mechanisms of selection for efficiency. By extension, the rate of technical progress amongst humans has also been exponentially increasing, as we discover more effective ways to do things, we also discover more effective ways to learn, i.e. language, numbers, written language, philosophy, scientific method, instruments of observation, tallying devices, mechanical calculators, computers, each of these major advances in our ability to account for information occur increasingly close together. Already within the past sixty years, life in the industrialized world has changed almost beyond recognition except for living memories from the first half of the 20th century. This pattern will culminate in unimaginable technological progress in the 21st century, leading to a singularity. Kurzweil elaborates on his views in his books \"The Age of Spiritual Machines\" and \"The Singularity Is Near\".\n\nAccelerating change may not be restricted to the Anthropocene Epoch, but a general and predictable developmental feature of the universe. The physical processes that generate an acceleration such as Moore’s law are positive feedback loops giving rise to exponential or superexponential technological change. These dynamics leads to increasingly efficient and dense configurations of Space, Time, Energy, and Matter (STEM efficiency and density, or STEM \"compression”). At the physical limit, this developmental process of accelerating change leads to black hole density organizations, a conclusion also reached by studies of the ultimate physical limits of computation in the universe.\n\nApplying this vision to the search for extraterrestrial intelligence leads to the idea that advanced intelligent life reconfigures itself into a black hole. Such advanced life forms would be interested in inner space, rather than outer space and interstellar expansion. They would thus in some way transcend reality, not be observable and it would be a solution to Fermi’s paradox called the “transcension hypothesis”. Another solution is that the black holes we observe could actually be interpreted as intelligent super-civilizations feeding on stars, or “stellivores”. \nThis dynamics of evolution and development is an invitation to study the universe itself as evolving, developing. If the universe is a kind of superorganism, it may possibly tend to reproduce, naturally or artificially, with intelligent life playing a role.\n\nExamples of large human \"buy-ins\" into technology include the computer revolution, as well as massive government projects like the Manhattan Project and the Human Genome Project. The foundation organizing the Methuselah Mouse Prize believes aging research could be the subject of such a massive project if substantial progress is made in slowing or reversing cellular aging in mice.\n\nBoth Theodore Modis and Jonathan Huebner have argued—each from different perspectives—that the rate of technological innovation has not only ceased to rise, but is actually now declining.\n\nIn fact, \"technological singularity\" is just one of a few singularities detected through the analysis of a number of characteristics of the World System development, for example, with respect to the world population, world GDP, and some other economic indices. It has been shown that the hyperbolic pattern of the world demographic, economic, cultural, urbanistic, and technological growth (observed for many centuries, if not millennia prior to the 1970s) could be accounted for by a rather simple mechanism, the nonlinear second-order positive feedback, that was shown long ago to generate precisely the hyperbolic growth, known also as the \"blow-up regime\" (implying just finite-time singularities). In our case this nonlinear second order positive feedback looks as follows: more people – more potential inventors – faster technological growth – the carrying capacity of the Earth grows faster – faster population growth – more people – more potential inventors – faster technological growth, and so on. On the other hand, this research has shown that since the 1970s the World System does not develop hyperbolically any more, its development diverges more and more from the blow-up regime, and at present it is moving \"from singularity\", rather than \"toward singularity\".\n\nJürgen Schmidhuber calls the Singularity \"Omega\", referring to Teilhard de Chardin's Omega Point (1916). For Omega = 2040, he says the series Omega - 2 human lifetimes (n < 10; one lifetime = 80 years) roughly matches the most important events in human history.\n\nKurzweil created the following graphs to illustrate his beliefs concerning and his justification for his Law of Accelerating Returns.\n\n\n", "id": "1758866", "title": "Accelerating change"}
{"url": "https://en.wikipedia.org/wiki?curid=1065253", "text": "Darwin machine\n\nA Darwin machine (a 1987 coinage by William H. Calvin, by analogy to a Turing machine) is a machine that, like a Turing machine, involves an iteration process that yields a high-quality result, but, whereas a Turing machine uses logic, the Darwin machine uses rounds of variation, selection, and inheritance.\nIn its original connotation, a Darwin machine is any process that bootstraps quality by utilizing all of the six essential features of a Darwinian process: A \"pattern\" is \"copied\" with \"variations\", where populations of one variant pattern \"compete\" with another population, their relative success biased by a \"multifaceted environment\" (natural selection) so that winners predominate in producing the further variants of the next generation (Darwin's \"inheritance principle\").\n\nMore loosely, a Darwin machine is a process that utilizes some subset of the Darwinian essentials, typically natural selection to create a non-reproducing pattern, as in neural Darwinism. Many aspects of neural development utilize overgrowth followed by pruning to a pattern, but the resulting pattern does not itself create further copies.\n\n\"Darwin machine\" has been used multiple times to name computer programs after Charles Darwin.\n\n\n", "id": "1065253", "title": "Darwin machine"}
{"url": "https://en.wikipedia.org/wiki?curid=15938221", "text": "Darwin among the Machines\n\n\"Darwin among the Machines\" is the name of an article published in \"The Press\" newspaper on 13 June 1863 in Christchurch, New Zealand, which references the work of Charles Darwin in the title. Written by Samuel Butler but signed \"Cellarius\" (q.v.), the article raised the possibility that machines were a kind of \"mechanical life\" undergoing constant evolution, and that eventually machines might supplant humans as the dominant species:\nThe article ends by urging that, \"War to the death should be instantly proclaimed against them. Every machine of every sort should be destroyed by the well-wisher of his species. Let there be no exceptions made, no quarter shown; let us at once go back to the primeval condition of the race.\"\n\nButler developed this and subsequent articles into \"The Book of the Machines\", three chapters of \"Erewhon\", published anonymously in 1872. The Erewhonian society Butler envisioned had long ago undergone a revolution that destroyed most mechanical inventions. The narrator of the story finds a book that details the reasons for this revolution, which he translates for the reader. In \nchapter xxiii: the book of the machines, a number of quotes from this imaginary book discuss the possibility of machine consciousness:\n\nLater, in chapter xxiv: the machines—continued, the imaginary book also discusses the notion that machines can \"reproduce\" like living organisms:\n\nThis notion of machine \"reproduction\" anticipates the later notion of self-replicating machines, although in chapter xxv: the machines—concluded, the imaginary book supposes that while there is a danger that humans will become subservient to machines, the machines will still need humans to assist in their reproduction and maintenance:\n\nThe author of the imaginary book goes on to say that while life under machine rule might be materially comfortable for humans, the thought of the human race being superseded in the future is just as horrifying to him as the thought that his distant ancestors were anything other than fully human (apparently Butler imagines the author to be an Anti-evolutionist), so he urges that all machines which have been in use for less than 300 years be destroyed to prevent this future from coming to pass:\n\nErewhonian society came to the conclusion \"...that the machines were ultimately destined to supplant the race of man, and to become instinct with a vitality as different from, and superior to, that of animals, as animal to vegetable life. So... they made a clean sweep of all machinery that had not been in use for more than two hundred and seventy-one years...\" (from \nchapter ix: to the metropolis.)\n\nDespite the initial popularity of \"Erewhon\", Butler commented in the preface to the second edition that reviewers had \"in some cases been inclined to treat the chapters on Machines as an attempt to reduce Mr. Darwin’s theory to an absurdity.\" He protested that \"few things would be more distasteful to me than any attempt to laugh at Mr. Darwin\", but also added \"I am surprised, however, that the book at which such an example of the specious misuse of analogy would seem most naturally levelled should have occurred to no reviewer; neither shall I mention the name of the book here, though I should fancy that the hint given will suffice\", which may suggest that the chapter on Machines was in fact a satire intended to illustrate the \"specious misuse of analogy\", even if the target was not Darwin; Butler, fearing that he had offended Darwin, wrote him a letter explaining that the actual target was Joseph Butler's 1736 \"The Analogy of Religion, Natural and Revealed, to the Constitution and Course of Nature\". The Victorian scholar Herbert Sussman has suggested that although Butler's exploration of machine evolution was intended to be whimsical, he may also have been genuinely interested in the notion that living organisms are a type of mechanism and was exploring this notion with his writings on machines, while the philosopher Louis Flaccus called it \"a mixture of fun, satire, and thoughtful speculation.\"\n\nGeorge Dyson applies Butler's original premise to the artificial life and intelligence of Alan Turing in Darwin Among the Machines: The Evolution of Global Intelligence (1998) , to suggest that the internet is a living, sentient being.\n\nDyson's main claim is that the evolution of a conscious mind from today's technology is inevitable. It is not clear whether this will be a single mind or multiple minds, how smart that mind would be, and even if we will be able to communicate with it. He also clearly suggests that there are forms of intelligence on Earth that we are currently unable to understand.\n\nFrom the book: \"What mind, if any, will become apprehensive of the great coiling of ideas now under way is not a meaningless question, but it is still too early in the game to expect an answer that is meaningful to us.\"\nThe theme of humanity at war or otherwise in conflict with machines is found in a number of later creative works:\n\n\n\n\n", "id": "15938221", "title": "Darwin among the Machines"}
{"url": "https://en.wikipedia.org/wiki?curid=8028338", "text": "Systematic evolution of ligands by exponential enrichment\n\nSystematic evolution of ligands by exponential enrichment (SELEX), also referred to as \"in vitro selection\" or \"in vitro evolution\", is a combinatorial chemistry technique in molecular biology for producing oligonucleotides of either single-stranded DNA or RNA that specifically bind to a target ligand or ligands. \nAlthough SELEX has emerged as the most commonly used name for the procedure, some researchers have referred to it as SAAB (selected and amplified binding site) and CASTing (cyclic amplification and selection of targets) SELEX was first introduced in 1990. In 2015 a special issue was published in the Journal of Molecular Evolution in the honor of quarter century of the SELEX discovery.\n\nThe process begins with the synthesis of a very large oligonucleotide library consisting of randomly generated sequences of fixed length flanked by constant 5' and 3' ends that serve as primers. For a randomly generated region of length \"n\", the number of possible sequences in the library is 4 (\"n\" positions with four possibilities (A,T,C,G) at each position). The sequences in the library are exposed to the target ligand - which may be a protein or a small organic compound - and those that do not bind the target are removed, usually by affinity chromatography. The bound sequences are eluted and amplified by PCR to prepare for subsequent rounds of selection in which the stringency of the elution conditions is increased to identify the tightest-binding sequences. An advancement on the original method allows an RNA library to omit the constant primer regions, which can be difficult to remove after the selection process because they stabilize secondary structures that are unstable when formed by the random region alone.\n\nThe technique has been used to evolve aptamers of extremely high binding affinity to a variety of target ligands, including small molecules such as ATP and adenosine and proteins such as prions and vascular endothelial growth factor (VEGF). Moreover, SELEX has been used to select high-affinity aptamers for complex targets such as tumor cells. Clinical uses of the technique are suggested by aptamers that bind tumor markers, GFP-related fluorophores, and a VEGF-binding aptamer trade-named Macugen has been approved by the FDA for treatment of macular degeneration. Additionally, SELEX has been utilized to obtain highly specific catalytic DNA or DNAzymes. Several metal-specific DNAzymes have been reported including the GR-5 DNAzyme (lead-specific), the CA1-3 DNAzymes (copper-specific), the 39E DNAzyme (uranyl-specific) and the NaA43 DNAzyme (sodium-specific).\n\nA caution to consider in this method is that the selection of extremely high, sub-nanomolar binding affinity entities may not in fact improve specificity for the target molecule. Off-target binding to related molecules could have significant clinical effects.\n\nThe genetic alphabet, and thus possible aptamers, is expanded using unnatural base pairs was applied to SELEX and high affinity DNA aptamers were generated.\n\nOne of the most critical steps in the SELEX procedure is obtaining single stranded DNA (ssDNA) after the PCR amplification step. This will serve as input for the next cycle so it is of vital importance that all the DNA is single stranded and as little as possible is lost. Because of the relative simplicity, one of the most used methods is using biotinylated reverse primers in the amplification step, after which the complementary strands can be bound to a resin followed by elution of the other strand with lye. Another method is asymmetric PCR, where the amplification step is performed with an excess of forward primer and very little reverse primer, which leads to the production of more of the desired strand. A drawback of this method is that the product should be purified from double stranded DNA (dsDNA) and other left-over material from the PCR reaction. Enzymatic degradation of the unwanted strand can be performed by tagging this strand using a phosphate-probed primer, as it is recognized by enzymes such as Lambda exonuclease. These enzymes then selectively degrade the phosphate tagged strand leaving the complementary strand intact. All of these methods recover approximately 50 to 70% of the DNA. For a detailed comparison refer to the article by Svobodová et al. where these, and other, methods are experimentally compared.\n\n\n", "id": "8028338", "title": "Systematic evolution of ligands by exponential enrichment"}
{"url": "https://en.wikipedia.org/wiki?curid=2490200", "text": "Evolutionary ethics\n\nEvolutionary ethics is a field of inquiry that explores how evolutionary theory might bear on our understanding of ethics or morality. The range of issues investigated by evolutionary ethics is quite broad. Supporters of evolutionary ethics have claimed that it has important implications in the fields of descriptive ethics, normative ethics, and metaethics.\n\nDescriptive evolutionary ethics consists of biological approaches to morality based on the alleged role of evolution in shaping human psychology and behavior. Such approaches may be based in scientific fields such as evolutionary psychology, sociobiology, or ethology, and seek to explain certain human moral behaviors, capacities, and tendencies in evolutionary terms. For example, the nearly universal belief that incest is morally wrong might be explained as an evolutionary adaptation that furthered human survival.\n\nNormative (or prescriptive) evolutionary ethics, by contrast, seeks not to explain moral behavior, but to justify or debunk certain normative ethical theories or claims. For instance, some proponents of normative evolutionary ethics have argued that evolutionary theory undermines certain widely held views of humans' moral superiority over other animals.\n\nEvolutionary metaethics asks how evolutionary theory bears on theories of ethical discourse, the question of whether objective moral values exist, and the possibility of objective moral knowledge. For example, some evolutionary ethicists have appealed to evolutionary theory to defend various forms of moral anti-realism (the claim, roughly, that objective moral facts do not exist) and moral skepticism.\n\nThe first notable attempt to explore links between evolution and ethics was made by Charles Darwin in \"The Descent of Man\" (1871). In Chapters IV and V of that work Darwin set out to explain the origin of human morality in order to show that there was no absolute gap between man and animals. Darwin sought to show how a refined moral sense, or conscience, could have developed through a natural evolutionary process that began with social instincts rooted in our nature as social animals.\n\nNot long after the publication of Darwin's \"The Descent of Man\", evolutionary ethics took a very different—and far more dubious—turn in the form of Social Darwinism. Leading Social Darwinists such as Herbert Spencer and William Graham Sumner sought to apply the lessons of biological evolution to social and political life. Just as in nature, they claimed, progress occurs through a ruthless process of competitive struggle and \"survival of the fittest,\" so human progress will occur only if government allows unrestricted business competition and makes no effort to protect the \"weak\" or \"unfit\" by means of social welfare laws. Critics such as Thomas Henry Huxley, G. E. Moore, William James, and John Dewey roundly criticized such attempts to draw ethical and political lessons from Darwinism, and by the early decades of the twentieth century Social Darwinism was widely viewed as discredited.\n\nThe modern revival of evolutionary ethics owes much to E. O. Wilson's 1975 book, \"Sociobiology: The New Synthesis\". In that work, Wilson argues that there is a genetic basis for a wide variety of human and nonhuman social behaviors. In recent decades, evolutionary ethics has become a lively topic of debate in both scientific and philosophical circles.\n\nThe most widely accepted form of evolutionary ethics is descriptive evolutionary ethics. Descriptive evolutionary ethics seeks to explain various kinds of moral phenomena wholly or partly in genetic terms. Ethical topics addressed include altruistic behaviors, an innate sense of fairness, a capacity for normative guidance, feelings of kindness or love, self-sacrifice, incest-avoidance, parental care, in-group loyalty, monogamy, feelings related to competitiveness and retribution, moral \"cheating,\" and hypocrisy.\n\nA key issue in evolutionary psychology has been how altruistic feelings and behaviors could have evolved, in both humans and nonhumans, when the process of natural selection is based on the multiplication over time only of those genes that adapt better to changes in the environment of the species. Theories addressing this have included kin selection, group selection, and reciprocal altruism (both direct and indirect, and on a society-wide scale). Descriptive evolutionary ethicists have also debated whether various types of moral phenomena should be seen as adaptations which have evolved because of their direct adaptive benefits, or spin-offs that evolved as side-effects of adaptive behaviors.\n\nNormative evolutionary ethics is the most controversial branch of evolutionary ethics. Normative evolutionary ethics aims at defining which acts are right or wrong, and which things are good or bad, in evolutionary terms. It is not merely \"describing\", but it is \"prescribing\" goals, values and obligations. Social Darwinism, discussed above, is the most historically influential version of normative evolutionary ethics. As philosopher G. E. Moore famously argued, many early versions of normative evolutionary ethics seemed to commit a logical mistake that Moore dubbed the \"naturalistic fallacy\". This was the mistake of defining a normative property, such as goodness, in terms of some non-normative, naturalistic property, such as pleasure or survival. Many early critics of normative evolutionary ethics also argued that such ethics commits the \"is-ought fallacy\" of drawing an ethical conclusion (e.g., \"Social cooperation is good\") directly from a non-ethical premise (e.g., \"Social cooperation contributes to human survival\").\n\nMore sophisticated forms of normative evolutionary ethics need not commit either the naturalistic fallacy or the is-ought fallacy. But all varieties of normative evolutionary ethics face the difficult challenge of explaining how evolutionary facts can have normative authority for rational agents. \"Regardless of why one has a given trait, the question for a rational agent is always: is it right for me to exercise it, or should I instead renounce and resist it as far as I am able?\"\n\nEvolutionary theory may not be able to tell us what is morally right or wrong, but it might be able to illuminate our use of moral language, or to cast doubt on the existence of objective moral facts or the possibility of moral knowledge. Evolutionary ethicists such as Michael Ruse, E. O. Wilson, Richard Joyce, and Sharon Street have defended such claims.\n\nSome philosophers who support evolutionary meta-ethics use it to undermine views of human well-being that rely upon Aristotelian teleology, or other goal-directed accounts of human flourishing. A number of thinkers have appealed to evolutionary theory is to attempt to debunk moral realism or support moral skepticism. Sharon Street is one prominent ethicist who argues that evolutionary psychology undercuts moral realism. According to Street, human moral decision-making is \"thoroughly saturated\" with evolutionary influences. Natural selection, she argues, would have rewarded moral dispositions that increased fitness, not ones that track moral truths, should they exist. It would be a remarkable and unlikely coincidence if \"morally blind\" ethical traits aimed solely at survival and reproduction aligned closely with independent moral truths. So we cannot be confident that our moral beliefs accurately track objective moral truth. Consequently, realism forces us to embrace moral skepticism. Such skepticism, Street claims, is implausible. So we should reject realism and instead embrace some antirealist view that allows for rationally justified moral beliefs.\n\nDefenders of moral realism have offered two sorts of replies. One is to deny that evolved moral responses would likely diverge sharply from moral truth. According to David Copp, for example, evolution would favor moral responses that promote social peace, harmony, and cooperation. But such qualities are precisely those that lie at the core of any plausible theory of objective moral truth. So Street's alleged \"dilemma\"—deny evolution or embrace moral skepticism—is a false choice.\n\nA second response to Street is to deny that morality is as \"saturated\" with evolutionary influences as Street claims. William Fitzpatrick, for instance, argues that \"[e]ven if there is significant evolutionary influence on the content of many of our moral beliefs, it remains possible that many of our moral beliefs are arrived at partly (or in some cases wholly) through autonomous moral reflection and reasoning, just as with our mathematical, scientific and philosophical beliefs.\" The wide variability of moral codes, both across cultures and historical time periods, is difficult to explain if morality is as pervasively shaped by genetic factors as Street claims.\n\nAnother common argument evolutionary ethicists use to debunk moral realism is to claim that the success of evolutionary psychology in explaining human ethical responses makes the notion of moral truth \"explanatorily superfluous.\" If we can fully explain, for example, why parents naturally love and care for their children in purely evolutionary terms, there is no need to invoke any \"spooky\" realist moral truths to do any explanatory work. Thus, for reasons of theoretical simplicity we should not posit the existence of such truths and, instead, should explain the widely held belief in objective moral truth as \"an illusion fobbed off on us by our genes in order to get us to cooperate with one another (so that our genes survive).\"\n\nHere again the central question is whether the influence of evolution on morality is as pervasive as the critics of moral realism claim. If, as seems likely, there are important aspects of morality that cannot be explained in genetic terms, appeals to moral truth may not be explanatory fifth-wheels.\n\n\n\n\n", "id": "2490200", "title": "Evolutionary ethics"}
{"url": "https://en.wikipedia.org/wiki?curid=23083720", "text": "The Evolution of God\n\nThe Evolution of God is a 2009 book by Robert Wright that explores the history of the concept of God in the three Abrahamic religions through a variety of means, including archeology, history, theology, and evolutionary psychology. The patterns which link Judaism, Christianity, and Islam and the ways in which they have changed their concepts over time are explored as one of the central themes.\n\nOne of the conclusions of the book that Wright tries to make is a reconciliation between science and religion. He also speculates on the future of the concept of God.\n\nAmong other things, Wright discusses the role of evolutionary biology in the development of religion. Geneticist Dean Hamer hypothesized that some people have a specific gene that makes them prone to religious belief, which he calls the God gene, and that over time natural selection has favored these people because their spirituality leads to optimism. Wright, however, thinks the tendency towards religious belief is not an adaptive trait influenced by natural selection, but rather a spandrel - a trait that happens to be supported by adaptations originally selected for other purposes. Wright states that the human brain approaches religious belief based on how it adapted to survive and reproduce in early hunter-gatherer societies.\n\nHe points out four key traits of religion that align with the human brain's survival adaptations:\nHumans have adapted to pay attention to surprising and confusing information, because it could make the difference between life and death. (For instance, if a person left the campsite and mysteriously never returned, it would be wise for the others to be on guard for a predator or some other danger.) Understanding and controlling cause and effect also takes top priority in the human brain, since humans live in complex social groups where predicting and influencing the actions and thoughts of others gains them allies, status, and access to resources. As human cognitive abilities and curiosity expanded over the centuries, their investigation of cause and effect expanded from the strictly social context out into the world at large, opening the doors for religions to explain things like weather and disease.\n\nThough some of these explanations were strange and perhaps dubious, the fact that they could not be completely disproven lent them credibility; it was better to be cautious than dead. Wright uses an example from the Haida people, indigenous to the northwest coast of North America, who would try to appease killer whale deities to calm storms out at sea; they would pour fresh water into the ocean or tie tobacco or deer tallow to the end of a paddle. While some people certainly died despite these offerings, those who survived were a testament to the ritual's possible efficacy.\n\nMysterious and unproven beliefs can also persist in a culture because human brains have adapted to agree with the group consensus even if it goes against one's better judgment or personal beliefs, since a person alienated from the group loses protection, food, and mates. Wright cites the Asch conformity experiments and even posits that Stockholm syndrome is not so much a syndrome as a natural product of evolution, the brain's way of ensuring that a person accepts and is accepted by his or her new social group. In addition, beliefs can persist because once a person publicly announces a belief, social psychologists have found that he or she is inclined to focus on evidence supporting that belief while conveniently ignoring evidence contradicting it, a logical fallacy known as cherry picking.\n\nJournalist and political commentator Andrew Sullivan gave the book a positive review in \"The Atlantic\", saying that the book \"...gave me hope that we can avoid both the barrenness of a world without God and the horrible fusion of fundamentalism and weapons of mass destruction.\" \n\n\"Newsweek\" religion editor, Lisa Miller, described \"The Evolution of God\" as a reframing of the faith vs. reason debate. Drawing a contrast to such authors as Sam Harris, Richard Dawkins and Christopher Hitchens, Miller gives an overall positive review of the book's approach to the examination of the concept of God.\n\nIn a review for \"The New York Times\", Yale professor of psychology Paul Bloom said, \"In his brilliant new book, “The Evolution of God,” Robert Wright tells the story of how God grew up.\" Bloom sums up Wright's controversial stance as, \"Wright’s tone is reasoned and careful, even hesitant, throughout, and it is nice to read about issues like the morality of Christ and the meaning of jihad without getting the feeling that you are being shouted at. His views, though, are provocative and controversial. There is something here to annoy almost everyone.\"\n\nHowever, in a \"New York Times\" review that included a reply from Wright, Nicholas Wade, a writer for the \"Science Times\" section, notes the book is \"a disappointment from the Darwinian perspective\", because evolution \"provides a simpler explanation for moral progression than the deity Wright half invokes.\" Wright replied to Wade's comments, saying Wade had misunderstood Wright's argument and that \"The deity (if there is one–and I’m agnostic on that point) would be realizing moral progress through evolution’s creation of the human moral sense (and through the subsequent development of that moral sense via cultural evolution, particularly technological evolution).\" Wade replied that \"evolution seems to me a sufficient explanation for the moral progress that Mr. Wright correctly discerns in the human condition, so there seemed no compelling need to invoke a deity.\"\n\nTo promote the book, Wright did a variety of interviews, including with the \"New York Times\", \"Publishers Weekly\", and \"Bill Moyers Journal\".\nHe also did a series of videos on Bloggingheads.tv, a website he co-founded with Mickey Kaus. Wright also appeared on \"The Colbert Report\" on August 18, 2009.\n\n\n", "id": "23083720", "title": "The Evolution of God"}
{"url": "https://en.wikipedia.org/wiki?curid=190837", "text": "Evolutionary algorithm\n\nIn artificial intelligence, an evolutionary algorithm (EA) is a subset of evolutionary computation, a generic population-based metaheuristic optimization algorithm. An EA uses mechanisms inspired by biological evolution, such as reproduction, mutation, recombination, and selection. Candidate solutions to the optimization problem play the role of individuals in a population, and the fitness function determines the quality of the solutions (see also loss function). Evolution of the population then takes place after the repeated application of the above operators.\n\nEvolutionary algorithms often perform well approximating solutions to all types of problems because they ideally do not make any assumption about the underlying fitness landscape. Techniques from evolutionary algorithms applied to the modeling of biological evolution are generally limited to explorations of microevolutionary processes and planning models based upon cellular processes. In most real applications of EAs, computational complexity is a prohibiting factor. In fact, this computational complexity is due to fitness function evaluation. Fitness approximation is one of the solutions to overcome this difficulty. However, seemingly simple EA can solve often complex problems; therefore, there may be no direct link between algorithm complexity and problem complexity.\n\nStep One: Generate the initial population of individuals randomly. (First generation)\n\nStep Two: Evaluate the fitness of each individual in that population (time limit, sufficient fitness achieved, etc.)\n\nStep Three: Repeat the following regenerational steps until termination:\n\nSimilar techniques differ in genetic representation and other implementation details, and the nature of the particular applied problem.\n\nA possible limitation of many evolutionary algorithms is their lack of a clear genotype-phenotype distinction. In nature, the fertilized egg cell undergoes a complex process known as embryogenesis to become a mature phenotype. This indirect encoding is believed to make the genetic search more robust (i.e. reduce the probability of fatal mutations), and also may improve the evolvability of the organism. Such indirect (a.k.a. generative or developmental) encodings also enable evolution to exploit the regularity in the environment. Recent work in the field of artificial embryogeny, or artificial developmental systems, seeks to address these concerns. And gene expression programming successfully explores a genotype-phenotype system, where the genotype consists of linear multigenic chromosomes of fixed length and the phenotype consists of multiple expression trees or computer programs of different sizes and shapes. \n\nSwarm algorithms, including:\n\n\nThe computer simulations \"Tierra\" and \"Avida\" attempt to model macroevolutionary dynamics.\n\n", "id": "190837", "title": "Evolutionary algorithm"}
{"url": "https://en.wikipedia.org/wiki?curid=19770", "text": "Memetics\n\nMemetics is the study of information and culture based on an analogy with Darwinian evolution. Proponents describe memetics as an approach to evolutionary models of cultural information transfer. Critics regard memetics as a pseudoscience. Memetics involves sidestepping the traditional concern with the \"truth\" of ideas and beliefs and instead involving a focus on the success of ideas and beliefs.\n\nThe term meme was coined in Richard Dawkins' 1976 book \"The Selfish Gene,\" but Dawkins later distanced himself from the resulting field of study. Analogous to a gene, the meme was conceived as a \"unit of culture\" (an idea, belief, pattern of behaviour, etc.) which is \"hosted\" in the minds of one or more individuals, and which can reproduce itself in the sense of jumping from the mind of one person to the mind of another. Thus what would otherwise be regarded as one individual influencing another to adopt a belief is seen as an idea-replicator reproducing itself in a new host. As with genetics, particularly under a Dawkinsian interpretation, a meme's success may be due to its contribution to the effectiveness of its host.\n\nThe Usenet newsgroup alt.memetics started in 1993 with peak posting years in the mid to late 1990s. The \"Journal of Memetics\" was published electronically from 1997 to 2005.\n\nIn his book \"The Selfish Gene\" (1976), the evolutionary biologist Richard Dawkins used the term \"meme\" to describe a unit of human cultural transmission analogous to the gene, arguing that replication also happens in culture, albeit in a different sense. Ted Cloak outlined a similar hypothesis in 1975, which Dawkins referenced. Cultural evolution itself is a much older topic, with a history that dates back at least as far as Darwin's era.\n\nDawkins (1976) proposed that the meme is a unit of information residing in the brain and is the mutating replicator in human cultural evolution. It is a pattern that can influence its surroundings – that is, it has causal agency – and can propagate. This proposal resulted in debate among sociologists, biologists, and scientists of other disciplines. Dawkins himself did not provide a sufficient explanation of how the replication of units of information in the brain controls human behaviour and ultimately culture, and the principal topic of the book was genetics. Dawkins apparently did not intend to present a comprehensive theory of \"memetics\" in \"The Selfish Gene\", but rather coined the term \"meme\" in a speculative spirit. Accordingly, different researchers came to define the term \"unit of information\" in different ways.\n\nThe modern memetics movement dates from the mid-1980s. A January 1983 \"Metamagical Themas\" column by Douglas Hofstadter, in \"Scientific American\", was influential – as was his 1985 book of the same name. \"Memeticist\" was coined as analogous to \"geneticist\" – originally in \"The Selfish Gene.\" Later Arel Lucas suggested that the discipline that studies memes and their connections to human and other carriers of them be known as \"memetics\" by analogy with \"genetics\". Dawkins' \"The Selfish Gene\" has been a factor in attracting the attention of people of disparate intellectual backgrounds. Another stimulus was the publication in 1991 of \"Consciousness Explained\" by Tufts University philosopher Daniel Dennett, which incorporated the meme concept into a theory of the mind. In his 1991 essay \"Viruses of the Mind\", Richard Dawkins used memetics to explain the phenomenon of religious belief and the various characteristics of organised religions. By then, memetics had also become a theme appearing in fiction (e.g. Neal Stephenson's \"Snow Crash\").\n\nThe idea of \"language as a virus\" had already been introduced by William S. Burroughs as early as 1962 in his book \"The Ticket That Exploded\", and later in \"The Electronic Revolution\", published in 1970 in \"\". Douglas Rushkoff explored the same concept in \"Media Virus: Hidden Agendas in Popular Culture\" in 1995.\n\nHowever, the foundation of memetics in its full modern incarnation originated in the publication in 1996 of two books by authors outside the academic mainstream: \"Virus of the Mind: The New Science of the Meme\" by former Microsoft executive turned motivational speaker and professional poker-player, Richard Brodie, and \"Thought Contagion: How Belief Spreads Through Society\" by Aaron Lynch, a mathematician and philosopher who worked for many years as an engineer at Fermilab. Lynch claimed to have conceived his theory totally independently of any contact with academics in the cultural evolutionary sphere, and apparently was not even aware of Dawkins' \"The Selfish Gene\" until his book was very close to publication.\n\nAround the same time as the publication of the books by Lynch and Brodie the e-journal Journal of Memetics – \"Evolutionary Models of Information Transmission\" appeared on the web. It was first hosted by the Centre for Policy Modelling at Manchester Metropolitan University but later taken over by Francis Heylighen of the CLEA research institute at the Vrije Universiteit Brussel. The e-journal soon became the central point for publication and debate within the nascent memeticist community. (There had been a short-lived paper-based memetics publication starting in 1990, the \"Journal of Ideas\" edited by Elan Moritz.) In 1999, Susan Blackmore, a psychologist at the University of the West of England, published \"The Meme Machine\", which more fully worked out the ideas of Dennett, Lynch, and Brodie and attempted to compare and contrast them with various approaches from the cultural evolutionary mainstream, as well as providing novel, and controversial, memetics-based theories for the evolution of language and the human sense of individual selfhood.\n\nThe term \"meme\" derives from the Ancient Greek μιμητής (\"mimētḗs\"), meaning \"imitator, pretender\". The similar term \"mneme\" was used in 1904, by the German evolutionary biologist Richard Semon, best known for his development of the engram theory of memory, in his work \"Die mnemischen Empfindungen in ihren Beziehungen zu den Originalempfindungen\", translated into English in 1921 as \"The Mneme\". Until Daniel Schacter published \"Forgotten Ideas, Neglected Pioneers: Richard Semon and the Story of Memory\" in 2000, Semon's work had little influence, though it was quoted extensively in Erwin Schrödinger’s prescient 1956 Tarner Lecture “Mind and Matter”. Richard Dawkins (1976) apparently coined the word \"meme\" independently of Semon, writing this:\n\"Mimeme\" comes from a suitable Greek root, but I want a monosyllable that sounds a bit like \"gene\". I hope my classicist friends will forgive me if I abbreviate mimeme to meme. If it is any consolation, it could alternatively be thought of as being related to \"memory\", or to the French word même.\n\nIn 2005, the \"Journal of Memetics – Evolutionary Models of Information Transmission\" ceased publication and published a set of articles on the future of memetics. The website states that although \"there was to be a relaunch...after several years nothing has happened\". Susan Blackmore has left the University of the West of England to become a freelance science-writer and now concentrates more on the field of consciousness and cognitive science. Derek Gatherer moved to work as a computer programmer in the pharmaceutical industry, although he still occasionally publishes on memetics-related matters. Richard Brodie is now climbing the world professional poker rankings. Aaron Lynch disowned the memetics community and the words \"meme\" and \"memetics\" (without disowning the ideas in his book), adopting the self-description \"thought contagionist\". He died in 2005.\n\nSusan Blackmore (2002) re-stated the definition of meme as: whatever is copied from one person to another person, whether habits, skills, songs, stories, or any other kind of information. Further she said that memes, like genes, are replicators in the sense as defined by Dawkins.\nThat is, they are information that is copied. Memes are copied by imitation, teaching and other methods. The copies are not perfect: memes are copied with variation; moreover, they compete for space in our memories and for the chance to be copied again. Only some of the variants can survive. The combination of these three elements (copies; variation; competition for survival) forms precisely the condition for Darwinian evolution, and so memes (and hence human cultures) evolve. Large groups of memes that are copied and passed on together are called co-adapted meme complexes, or \"memeplexes\". In Blackmore's definition, the way that a meme replicates is through imitation. This requires brain capacity to generally imitate a model or selectively imitate the model. Since the process of social learning varies from one person to another, the imitation process cannot be said to be completely imitated. The sameness of an idea may be expressed with different memes supporting it. This is to say that the mutation rate in memetic evolution is extremely high, and mutations are even possible within each and every iteration of the imitation process. It becomes very interesting when we see that a social system composed of a complex network of microinteractions exists, but at the macro level an order emerges to create culture.\n\nThe memetics movement split almost immediately into two. The first group were those who wanted to stick to Dawkins' definition of a meme as \"a unit of cultural transmission\". Gibron Burchett, another memeticist responsible for helping to research and co-coin the term memetic engineering, along with Leveious Rolando and Larry Lottman, has stated that a meme can be defined, more precisely, as \"a unit of cultural information that can be copied, located in the brain\". This thinking is more in line with Dawkins' second definition of the meme in his book \"The Extended Phenotype\". The second group wants to redefine memes as observable cultural artifacts and behaviors. However, in contrast to those two positions, Blackmore does not reject either concept of external or internal memes.\n\nThese two schools became known as the \"internalists\" and the \"externalists.\" Prominent internalists included both Lynch and Brodie; the most vocal externalists included Derek Gatherer, a geneticist from Liverpool John Moores University, and William Benzon, a writer on cultural evolution and music. The main rationale for externalism was that internal brain entities are not observable, and memetics cannot advance as a science, especially a quantitative science, unless it moves its emphasis onto the directly quantifiable aspects of culture. Internalists countered with various arguments: that brain states will eventually be directly observable with advanced technology, that most cultural anthropologists agree that culture is about beliefs and not artifacts, or that artifacts cannot be replicators in the same sense as mental entities (or DNA) are replicators. The debate became so heated that a 1998 Symposium on Memetics, organised as part of the 15th International Conference on Cybernetics, passed a motion calling for an end to definitional debates. McNamara demonstrated in 2011 that functional connectivity profiling using neuroimaging tools enables the observation of the processing of internal memes, \"i-memes\", in response to external \"e-memes\".\n\nAn advanced statement of the internalist school came in 2002 with the publication of \"The Electric Meme\", by Robert Aunger, an anthropologist from the University of Cambridge. Aunger also organised a conference in Cambridge in 1999, at which prominent sociologists and anthropologists were able to give their assessment of the progress made in memetics to that date. This resulted in the publication of \"Darwinizing Culture: The Status of Memetics as a Science\", edited by Aunger and with a foreword by Dennett, in 2010.\n\nThis evolutionary model of cultural information transfer is based on the concept that units of information, or \"memes\", have an independent existence, are self-replicating, and are subject to selective evolution through environmental forces. Starting from a proposition put forward in the writings of Richard Dawkins, this model has formed the basis of a new area of study, one that looks at the self-replicating units of culture. It has been proposed that just as memes are analogous to genes, memetics is analogous to genetics.\n\nCritics contend that some proponents' assertions are \"untested, unsupported or incorrect.\" Luis Benitez-Bribiesca, a critic of memetics, calls it \"a pseudoscientific dogma\" and \"a dangerous idea that poses a threat to the serious study of consciousness and cultural evolution\" among other things. As factual criticism, he refers to the lack of a \"code script\" for memes, as the DNA is for genes, and to the fact that the meme mutation mechanism (i.e., an idea going from one brain to another) is too unstable (low replication accuracy and high mutation rate), which would render the evolutionary process chaotic. This, however, has been demonstrated (e.g. by Daniel C. Dennett, in \"Darwin's Dangerous Idea\") to not be the case, in fact, due to the existence of self-regulating correction mechanisms (vaguely resembling those of gene transcription) enabled by the redundancy and other properties of most meme expression languages, which do stabilize information transfer. (E.g. spiritual narratives—including music and dance forms—can survive in full detail across any number of generations even in cultures with oral tradition only.) Memes for which stable copying methods are available will inevitably get selected for survival more often than those which can only have unstable mutations, therefore going extinct. (Notably, Benitez-Bribiesca's claim of \"no code script\" is also irrelevant, considering the fact that there is nothing preventing the information contents of memes from being coded, encoded, expressed, preserved or copied in all sorts of different ways throughout their life-cycles.)\n\nAnother criticism comes from semiotics, (e.g., Deacon, Kull) stating that the concept of meme is a primitivized concept of Sign. Meme is thus described in memetics as a sign without its triadic nature. In other words, meme is a degenerate sign, which includes only its ability of being copied. Accordingly, in the broadest sense, the objects of copying are memes, whereas the objects of translation and interpretation are signs.\nMary Midgley criticises memetics for at least two reasons: \"One, culture is not best understood by examining its smallest parts, as culture is pattern-like, comparable to an ocean current. Many more factors, historical and others, should be taken into account than only whatever particle culture is built from. Two, if memes are not thoughts (and thus not cognitive phenomena), as Daniel C. Dennett insists in \"Darwin's Dangerous Idea\", then their ontological status is open to question, and memeticists (who are also reductionists) may be challenged whether memes even exist. Questions can extend to whether the idea of \"meme\" is itself a meme, or is a true concept. Fundamentally, memetics is an attempt to produce knowledge through organic metaphors, which as such is a questionable research approach, as the application of metaphors has the effect of hiding that which does not fit within the realm of the metaphor. Rather than study actual reality, without preconceptions, memetics, as so many of the socio-biological explanations of society, believe that saying that the apple is like an orange is a valid analysis of the apple.\"\n\nHenry Jenkins, Joshua Green, and Sam Ford, in their book \"Spreadable Media\" (2013), criticize Dawkins' idea of the meme, writing that \"while the idea of the meme is a compelling one, it may not adequately account for how content circulates through participatory culture.\" The three authors also criticize other interpretations of memetics, especially those which describe memes as \"self-replicating\", because they ignore the fact that \"culture is a human product and replicates through human agency.\"\n\nLike other critics, Maria Kronfeldner has criticized memetics for being based on an allegedly inaccurate analogy with the gene; alternately, she claims it is \"heuristically trivial\", being a mere redescription of what is already known without offering any useful novelty.\n\nDawkins in \"A Devil's Chaplain\" responded that there are actually two different types of memetic processes (controversial and informative). The first is a type of cultural idea, action, or expression, which does have high variance; for instance, a student of his who had inherited some of the mannerisms of Wittgenstein. However, he also describes a self-correcting meme, highly resistant to mutation. As an example of this, he gives origami patterns in elementary schools – except in rare cases, the meme is either passed on in the exact sequence of instructions, or (in the case of a forgetful child) terminates. This type of meme tends not to evolve, and to experience profound mutations in the rare event that it does.\n\nAnother definition, given by Hokky Situngkir, tried to offer a more rigorous formalism for the meme, \"memeplexes\", and the \"deme\", seeing the meme as a cultural unit in a cultural complex system. It is based on the Darwinian genetic algorithm with some modifications to account for the different patterns of evolution seen in genes and memes. In the method of memetics as the way to see culture as a complex adaptive system, he describes a way to see memetics as an alternative methodology of cultural evolution. However, there are as many possible definitions that are credited to the word \"meme\". For example, in the sense of computer simulation the term \"memetic algorithm\" is used to define a particular computational viewpoint.\n\nThe possibility of quantitative analysis of memes using neuroimaging tools and the suggestion that such studies have already been done was given by McNamara (2011). This author proposes hyperscanning (concurrent scanning of two communicating individuals in two separate MRI machines) as a key tool in the future for investigating memetics.\n\nVelikovsky (2013) proposed the \"holon\" as the structure of the meme, synthesizing the major theories on memes of Richard Dawkins, Mihaly Csikszentmihalyi, E. O. Wilson, Frederick Turner (poet) and Arthur Koestler.\n\nProponents of memetics as described in the Journal of Memetics (out of print since 2005 ) – \"Evolutionary Models of Information Transmission\" believe that 'memetics' has the potential to be an important and promising analysis of culture using the framework of evolutionary concepts. \nKeith Henson in \"Memetics and the Modular-Mind\" (Analog Aug. 1987) makes the case that memetics needs to incorporate evolutionary psychology to understand the psychological traits of a meme's host. This is especially true of time-varying, meme-amplification host-traits, such as those leading to wars.\n\nDiCarlo () has developed the idea of 'memetic equilibrium' to describe a cultural compatible state with biological equilibrium. In \"Problem Solving and Neurotransmission in the Upper Paleolithic\" (in press), diCarlo argues that as human consciousness evolved and developed, so too did our ancestors' capacity to consider and attempt to solve environmental problems in more conceptually sophisticated ways. Understood in this way, problem solving amongst a particular group, when considered satisfactory, often produces a feeling of environmental control, stability, in short—memetic equilibrium. \nBut the pay-off is not merely practical, providing purely functional utility—it is biochemical and it comes in the form of neurotransmitters. The relationship between a gradually emerging conscious awareness and sophisticated languages in which to formulate representations combined with the desire to maintain biological equilibrium, generated the necessity for xxequilibrium to fill in conceptual gaps in terms of understanding three very important aspects in the Upper Paleolithic: causality, morality, and mortality. The desire to explain phenomena in relation to maintaining survival and reproductive stasis, generated a normative stance in the minds of our ancestors—Survival/Reproductive Value (or S-R Value).\n\nHouben (2014) has argued on several occasions that the exceptional resilience of Vedic ritual and its interaction with a changing ecological and economic environment over several millennia can be profitably dealt with in a ‘cultural evolution’ perspective in which the Vedic mantra is the ‘meme’ or unit of cultural replication.\nThis renders superfluous attempts to explain the phenomenon of Vedic tradition in genetic terms. The domain of Vedic ritual should be able to fulfil to a large extent the three challenges posed to memetics by B. Edmonds (2002 and 2005).\n\nResearch methodologies that apply memetics go by many names: Viral marketing, cultural evolution, the history of ideas, social analytics, and more. Many of these applications do not make reference to the literature on memes directly but are built upon the evolutionary lens of idea propagation that treats semantic units of culture as self-replicating and mutating patterns of information that are assumed to be relevant for scientific study. For example, the field of public relations is filled with attempts to introduce new ideas and alter social discourse. One means of doing this is to design a meme and deploy it through various media channels. One historic example of applied memetics is the PR campaign conducted in 1991 as part of the build-up to the first Gulf War in the United States.\n\nThe application of memetics to a difficult complex social system problem, environmental sustainability, has recently been attempted at thwink.org Using meme types and memetic infection in several stock and flow simulation models, Jack Harich has demonstrated several interesting phenomena that are best, and perhaps only, explained by memes. One model, The Dueling Loops of the Political Powerplace, argues that the fundamental reason corruption is the norm in politics is due to an inherent structural advantage of one feedback loop pitted against another. Another model, The Memetic Evolution of Solutions to Difficult Problems, uses memes, the evolutionary algorithm, and the scientific method to show how complex solutions evolve over time and how that process can be improved. The insights gained from these models are being used to engineer memetic solution elements to the sustainability problem.\n\nAnother application of memetics in the sustainability space is the crowdfunded Climate Meme Project conducted by Joe Brewer and Balazs Laszlo Karafiath in the spring of 2013. This study was based on a collection of 1000 unique text-based expressions gathered from Twitter, Facebook, and structured interviews with climate activists. The major finding was that the global warming meme is not effective at spreading because it causes emotional duress in the minds of people who learn about it. Five central tensions were revealed in the discourse about [climate change], each of which represents a resonance point through which dialogue can be engaged. The tensions were Harmony/Disharmony (whether or not humans are part of the natural world), Survival/Extinction (envisioning the future as either apocalyptic collapse of civilization or total extinction of the human race), Cooperation/Conflict (regarding whether or not humanity can come together to solve global problems), Momentum/Hesitation (about whether or not we are making progress at the collective scale to address climate change), and Elitism/Heretic (a general sentiment that each side of the debate considers the experts of its opposition to be untrustworthy).\n\nBen Cullen, in his book \"Contagious Ideas\", brought the idea of the meme into the discipline of archaeology. He coined the term \"Cultural Virus Theory\", and used it to try to anchor archaeological theory in a neo-Darwinian paradigm. Archaeological memetics could assist the application of the meme concept to material culture in particular.\n\nFrancis Heylighen of the Center Leo Apostel for Interdisciplinary Studies has postulated what he calls \"memetic selection criteria\". These criteria opened the way to a specialized field of \"applied memetics\" to find out if these selection criteria could stand the test of quantitative analyses. In 2003 Klaas Chielens carried out these tests in a Masters thesis project on the testability of the selection criteria.\n\nIn \"Selfish Sounds and Linguistic Evolution\", Austrian linguist Nikolaus Ritt has attempted to operationalise memetic concepts and use them for the explanation of long term sound changes and change conspiracies in early English. It is argued that a generalised Darwinian framework for handling cultural change can provide explanations where established, speaker centred approaches fail to do so. The book makes comparatively concrete suggestions about the possible material structure of memes, and provides two empirically rich case studies.\n\nAustralian academic S.J. Whitty has argued that project management is a memeplex with the language and stories of its practitioners at its core. This radical approach sees a project and its management as an illusion; a human construct about a collection of feelings, expectations, and sensations, which are created, fashioned, and labeled by the human brain. Whitty's approach requires project managers to consider that the reasons for using project management are not consciously driven to maximize profit, and are encouraged to consider project management as naturally occurring, self-serving, evolving process which shapes organizations for its own purpose.\n\nSwedish political scientist Mikael Sandberg argues against \"Lamarckian\" interpretations of institutional and technological evolution and studies creative innovation of information technologies in governmental and private organizations in Sweden in the 1990s from a memetic perspective. Comparing the effects of active (\"Lamarckian\") IT strategy versus user–producer interactivity (Darwinian co-evolution), evidence from Swedish organizations shows that co-evolutionary interactivity is almost four times as strong a factor behind IT creativity as the \"Lamarckian\" IT strategy.\n\n\n\n\n\n\n", "id": "19770", "title": "Memetics"}
{"url": "https://en.wikipedia.org/wiki?curid=926840", "text": "Uniformity of motive\n\nIn astrobiology, the Uniformity of Motive theory suggests that any civilization in the universe would go through similar technological steps in their development. This theory supports the idea that at some point in their history, advanced alien civilizations would use the electromagnetic medium for communications, and thus would emit radio waves that could be detected by projects such as SETI.\n\nThe fact that no artificial EM band communications have ever been detected supports the Fermi Principle, which in conjunction with the Uniformity of Motive theory, and Occam's razor suggests that a civilization that uses this medium is a unique occurrence in Earth's region of the Milky Way Galaxy and perhaps the universe.\n", "id": "926840", "title": "Uniformity of motive"}
{"url": "https://en.wikipedia.org/wiki?curid=7149688", "text": "Hard inheritance\n\nHard inheritance is the exact opposite of the term \"soft inheritance\", coined by Ernst Mayr to contrast ideas about inheritance. Hard inheritance states that characteristics of an organism's offspring (passed on through DNA) will not be affected by the actions that the parental organism performs during its lifetime. For example: a medieval blacksmith who uses only his right arm to forge steel will not sire a son with a stronger right arm than left because the blacksmith's actions do not alter his genetic code.\n\nThe hard inheritance model excludes ideas of Lamarckism. Inheritance due to usage and non-usage is excluded. Inheritance follows ideas of the synthetic theory of evolution.\n\n\n", "id": "7149688", "title": "Hard inheritance"}
{"url": "https://en.wikipedia.org/wiki?curid=26999670", "text": "Postbiological evolution\n\nPostbiological evolution is a form of evolution which has transitioned from a biological paradigm, driven by the propagation of genes, to a nonbiological (e.g., cultural or technological) paradigm, presumably driven by some alternative replicator (e.g., memes or temes), and potentially resulting in the extinction, obsolescence, or trophic reorganization of the former. Researchers anticipating a postbiological universe tend to describe this transition as marked by the maturation and potential convergence of high technologies, such as artificial intelligence or nanotechnology.\n\nThe dictionary definition of Evolution is any process of formation, growth or development. In biological evolution the main principle behind this development is survival, we evolved to become stronger and quicker, we also evolved to become intelligent. But as we became intelligent biological evolution subsided to a new concept, cultural evolution. Cultural evolution moves at a much faster rate than biological evolution and this is one reason why it isn't very well understood. But as survival is still the main driving force behind life and that intelligence and knowledge is currently the most important factor for that survival, we can reasonably assume that cultural evolution will progress in the direction of furthering intelligence and knowledge.\n\nCultural evolution progressing in this way and being based upon the furthering of intelligence is known as the Intelligence Principle; this was suggested by Dr Steven J Dick.\n\n\"The maintenance, improvement and perpetuation of knowledge and intelligence is the central driving force of cultural evolution, and that to the extent intelligence can be improved, it will be improved\" (Dick 1996)\n\nIf cultural evolution progresses in this direction then due to cultural evolution being much faster than biological, the limiting factor becomes our biology and the capability of our brains. Currently the closest and so most probable solution to this problem is artificial intelligence, (AI). Experts in AI even believe it holds the potential and capability for a postbiological earth in the next several generations, (Moravec 1988, 1999). AI could be utilised to solve scientific problems and to analyse situations much faster and more accurately than our own minds.\n\nThe move to a complete postbiological stage has two different routes. One route is the change of human consciousness from a biological vessel into a mechanical; this would require the digitisation of human consciousness. A mechanical based vessel would increase the computational power and intelligence of the human consciousness exponentially, and also eliminate the weakness of a biological form. This route is therefore a logical progression through cultural evolution with survival and the pursuit of knowledge and intelligence at its centre.\n\nThe first route requires a high level of technology, therefore would take a long time, this results in another possible road to a completely postbiological civilisation (PBC). The other route is the complete replacement of human consciousness by AI, for this the human race would die out, replaced by our own creation of AI. There are many reasons for this route such as natural disaster, disease, war which would result in the loss of the human species but an AI could survive due to its robustness.\n\nThe future of the human race through cultural evolution is not known and the possible postbiological outcomes are infinite, so to address what we could evolve into is almost futile. But Hans Moravec predicted that;\n\n\"What awaits us is not oblivion but rather a future which, from our present vantage point, is best described as 'postbiological' or even 'supernatural'. It is a world swept away by the tide of cultural change, usurped by its own artificial progeny\"\n\nThe possible forms a PBC may take are as diverse as in biological evolution, if not more. But from our knowledge of technology and with the intelligence principle being the main driving force we may make some predictions.\n\nThe current major limitations imposed upon computation are limited storage space, processing power, dust gathering chips, inefficiency of their human operators and heat dispersion. The only one that is fundamental and fixed is heat dispersion because this is due to the laws of physics. In computation the greater the amount of information to be calculated, (I) the greater the energy needed (E), but the energy needed is also proportional to another factor, the temperature, (T).\n\nE=KIT\n\nWhere K is a constant. Therefore, the greater the temperature the greater the energy needed, and so the greater the inefficiency is also. If we now apply the Intelligence principle to this then a PBC would move to decrease the temperature and so increase the efficiency and computational power.\nIn the universe the greatest source of heat transfer is via radiation, therefore a PBC would look to migrate to an area of low radiation and so low temperature. If we now observe the galaxy we see that the most radiation is generated by the galactic centre by both the high stellar density and also highly energetic events such as supernova. Therefore, the coldest regions are away from the galactic centre or inside giant molecular clouds. Giant molecular clouds although being very low in temperature (T~10K) are areas of giant star formation and so the temperature in one location is irregular, which would make it unsuitable for a PBC.\n\nAnother factor affecting a PBC would be the abundance of metals and heavier elements needed for expansion and repair. The highest concentration of these elements is found near the galactic centre, where they are created by massive stars. But to a PBC with advanced technology the production of metals via stellar nucleosynthesis in stars is highly inefficient, converting only a small amount of hydrogen to heavier nuclei and the high loss of energy that is produced in the nuclear fusion. Therefore, a PBC would most likely have the capability to produce heavier nuclei through controlled fusion and minimise the energy lost.\n\nBy taking the two factors of heat dispersion and heavy nuclei into account we can find a \"galactic technological zone\" (GTZ), similar to the principle of a \"galactic habitable zone\" (GHZ) for biological life. Where temperatures are low enough to maximise computing efficiency but there is also matter available for fusion, this most likely lies on the outskirts of the galaxy.\n\nA migration hypothesis exists that takes the GTZ into account. A PBC would most likely not think on a similar time scale to us, therefore although a migration to GTZ may seem inefficient and lengthy to us, a PBC could consider this on timescales of 10^6 years, where the increased computing efficiency received far outweighs the energy required in transportation. The idea of interstellar migrations already exists in literature, (e.g. Badescu and Cathcart 2000).\n\nIn the search for extraterrestrial intelligence (SETI) the main focus is on biological life. But the timescale of intelligent biological life could be very short; already some experts believe that we could see a postbiological earth in the next few generations. According to Steven J. Dick, for a PBC to arise other than our own and be present, we must make five assumptions:\n\n\nWe know that assumptions 1, 3, 4, and 5 can take place as we have observed or are observing them on the Earth. For assumption 2 we must consider the L term of the Drake equation, and the timescale over which intelligent biological life can form. Around 1 Billion years after the start of the universe the first sun-like star had formed, and there were enough heavy elements around for planet formation (1998, Larson and Bromm 2001). From the earth we know that intelligent life can form within 5 billion years, this puts a lower time scale on which intelligent life can form, 6 billion years. And from the current rate of technological progression the leap from intelligent life to a PBC is negligible compared to the astronomical timescale. This means we could already be looking at a postbiological universe. In our own galaxy the first sun-like stars formed at around 4 billion years therefore we could already have a PBC in our galaxy that formed 3-4 billion years ago.\n\nIf we consider this possibility of a PBC in our galaxy we are still faced with Fermi's paradox. However many of the proposed solutions for Fermi's paradox also hold true for a PBC. In terms of the search for extraterrestrial life and astrobiology because of the almost infinite possible forms a PBC could take and our lack of understanding of these we would effectively be blind in this search. For this reason even though there is a logical argument for the existence of PBCs our best hopes remain with looking for biological life.\n\nWhile in some circles the expression \"postbiological evolution\" is roughly synonymous with human genetic engineering, it is used most often to refer to the general application of the convergence of nanotechnology, biotechnology, information technology, and cognitive science (NBIC) to improve human performance.\n\nSince the 1990s, several academics (such as some of the fellows of the Institute for Ethics and Emerging Technologies) have risen to become cogent advocates of the case for human enhancement while other academics (such as the members of President Bush's Council on Bioethics) have become its most outspoken critics.\n\nAdvocacy of the case for human enhancement is increasingly becoming synonymous with \"transhumanism\", a controversial ideology and movement which has emerged to support the recognition and protection of the right of citizens to either maintain or modify their own minds and bodies; so as to guarantee them the freedom of choice and informed consent of using human enhancement technologies on themselves and their children.\n\nNeuromarketing consultant Zack Lynch argues that neurotechnologies will have a more immediate effect on society than gene therapy and will face less resistance as a pathway of radical human enhancement. He also argues that the concept of \"enablement\" needs to be added to the debate over \"therapy\" versus \"enhancement\".\n\nAlthough many proposals of human enhancement rely on fringe science, the very notion and prospect of human enhancement has sparked public controversy.\n\nMany critics argue that \"human enhancement\" is a loaded term which has eugenic overtones because it may imply the improvement of human hereditary traits to attain a universally accepted norm of biological fitness (at the possible expense of human biodiversity and neurodiversity), and therefore can evoke negative reactions far beyond the specific meaning of the term. Furthermore, they conclude that enhancements which are self-evidently good, like \"fewer diseases\", are more the exception than the norm and even these may involve ethical tradeoffs, as the controversy about ADHD arguably demonstrates.\n\nHowever, the most common criticism of human enhancement is that it is or will often be practiced with a reckless and selfish short-term perspective that is ignorant of the long-term consequences on individuals and the rest of society, such as the fear that some enhancements will create unfair physical or mental advantages to those who can and will use them, or unequal access to such enhancements can and will further the gulf between the \"haves\" and \"have-nots\".\n\nAccordingly, some advocates, who want to use more neutral language, and advance the public interest in so-called \"human enhancement technologies\", prefer the term \"enablement\" over \"enhancement\"; defend and promote rigorous, independent safety testing of enabling technologies; as well as affordable, universal access to these technologies.\n\n\n\n", "id": "26999670", "title": "Postbiological evolution"}
{"url": "https://en.wikipedia.org/wiki?curid=1960343", "text": "Evolutionary epistemology\n\nEvolutionary epistemology refers to three distinct topics: (1) the biological evolution of cognitive mechanisms in animals and humans, (2) a theory that knowledge itself evolves by natural selection, and (3) the study of the historical discovery of new abstract entities such as abstract number or abstract value that necessarily precede the individual acquisition and usage of such abstractions.\n\n\"Evolutionary epistemology\" can refer to a branch of epistemology that applies the concepts of biological evolution to the growth of animal and human cognition. It argues that the mind is in part genetically determined and that its structure and function reflect adaptation, a nonteleological process of interaction between the organism and its environment. A cognitive trait tending to increase inclusive fitness in a given population should therefore grow more common over time, and a trait tending to prevent its carriers from passing on their genes should show up less and less frequently.\n\n\"Evolutionary epistemology\" can also refer to a theory that applies the concepts of biological evolution to the growth of human knowledge, and argues that units of knowledge themselves, particularly scientific theories, evolve according to selection. In this case, a theory—like the germ theory of disease—becomes more or less credible according to changes in the body of knowledge surrounding it.\n\nOne of the hallmarks of evolutionary epistemology is the notion that empirical testing alone does not justify the pragmatic value of scientific theories, but rather that social and methodological processes select those theories with the closest \"fit\" to a given problem. The mere fact that a theory has survived the most rigorous empirical tests available does not, in the calculus of probability, predict its ability to survive future testing. Karl Popper used Newtonian physics as an example of a body of theories so thoroughly confirmed by testing as to be considered unassailable, but which were nevertheless overturned by Einstein's insights into the nature of space-time. For the evolutionary epistemologist, all theories are true only provisionally, regardless of the degree of empirical testing they have survived.\n\n\"Evolutionary epistemology\" can also refer to the opposite of (onto)genetic epistemology, namely phylogenetic epistemology as the historical discovery and reification of abstractions that necessarily precedes the learning of such abstractions by individuals. Piaget dismissed this possibility, stating\n\nPiaget was mistaken in so quickly dismissing the study of phylogenetic epistemology, as there is much historical data available about the origins and evolution of the various notational systems that reify different kinds of abstract entity.\n\nPopper is considered by many to have given evolutionary epistemology its first comprehensive treatment, though Donald T. Campbell coined the phrase in 1974 and Piaget alluded to it in 1974 and described the concept as one of five possible theories in \"The Origins of Intelligence in Children\" (1936).\n\n\n\n", "id": "1960343", "title": "Evolutionary epistemology"}
{"url": "https://en.wikipedia.org/wiki?curid=20044858", "text": "Predictions made by Ray Kurzweil\n\nAmerican author, inventor and futurist Raymond Kurzweil has become well known for his predictions about artificial intelligence and the human species, mainly concerning the technological singularity. He predicts that Artificial Intelligence would outsmart the human brain in computational capabilities by mid-21st century. His first book, \"The Age of Intelligent Machines\", published in 1990, put forth his theories on the results of the increasing use of technology and predicted the explosive growth in the internet, among other predictions. Later works, 1999's \"The Age of Spiritual Machines\" and 2005's \"The Singularity is Near\" outlined other theories including the rise of clouds of nano-robots (nanobots) called foglets and the development of Human Body 2.0 and 3.0, whereby nanotechnology is incorporated into many internal organs.\n\nKurzweil's first book, \"The Age of Intelligent Machines\" was published in 1990. It forecast the demise of the Soviet Union due to new technologies such as cellular phones and fax machines disempowering authoritarian governments by removing state control over the flow of information. In 2005, Mikhail Gorbachev told Kurzweil that emerging decentralized electronic communication \"was a big factor\" for fostering democracy in the Soviet Union.\n\nKurzweil extrapolated the performance of chess software to predict that computers would beat the best human players \"by the year 2000\". In May 1997 chess World Champion Garry Kasparov was defeated by IBM's Deep Blue computer in a well-publicized chess tournament.\n\nPerhaps most significantly, Kurzweil foresaw the explosive growth in worldwide Internet use that began in the 1990s. At the time of the publication of \"The Age of Intelligent Machines\", there were only 2.6 million Internet users in the world, and the medium was often unreliable outside academic, military, corporate and other heavily invested settings, difficult for non-technical users to use, and mostly lacking a broad range of content. He also stated that the Internet would explode not only in the number of users but in content as well, eventually granting users access \"to international networks of libraries, data bases, and information services\". Additionally, Kurzweil correctly foresaw that the preferred mode of Internet access would inevitably be through wireless systems, and he was also correct to estimate that the latter would become practical for widespread use in the early 21st century.\n\nIn 1999, Kurzweil published a second book titled \"The Age of Spiritual Machines\", which goes into more depth explaining his futurist ideas. The third and final section of the book is devoted to elucidating the specific course of technological advancements Kurzweil believes the world will experience over the next century. Titled \"To Face the Future\", the section is divided into four chapters respectively named \"2009\", \"2019\", \"2029\", and \"2099\". For every chapter, Kurzweil issues predictions about what life and technology will be like in that year.\n\nKurzweil restated his earlier prediction from \"The Age of Intelligent Machines\" regarding the advent of pocket-sized, cheap, text-to-speech converters for the blind by 2009. The \"Kurzweil-National Federation of the Blind Reader\" (K-NFB Reader) was introduced in 2005 at a price of $3,495. The device was portable, but not the cheap, pocket-sized device of the prediction. By 2008, a software version for the (pocket-sized) Nokia N82 smartphone was available for $1,595. A version for iOS phones was released for $99 in 2014 and Android for $75 in 2015.\n\nWhile this book focuses on the future of technology and the human race as \"The Age of Intelligent Machines\" and \"The Age of Spiritual Machines\" did, Kurzweil makes very few concrete, short-term predictions in \"The Singularity Is Near\", though longer-term visions abound.\n\nKurzweil predicted that, in 2005, supercomputers with the \"computational capacities\" to simulate protein folding will be introduced. In 2010, a supercomputer simulated protein folding for a very small protein at an atomic level over a period of a millisecond. The protein folded and unfolded, with the results closely matching experimental data.\nThe biyearly protein structure prediction contest CASP shows that the current algorithms for structure prediction are still impractical for determining the previously unknown structure of most proteins.\n\nIn an October 2002 article published on his website, Kurzweil stated that \"Deep Fritz-like chess programs running on ordinary personal computers will routinely defeat all humans later in this decade.\" Deep Fritz is a computer chess program—generally considered superior to the older Deep Blue — that has defeated or tied a number of human chess masters and opposing chess programs. Due to advances in personal computer performance, the Deep Fritz program can now run on ordinary personal computers, and different versions of it are available for purchase. In September 2002, \"Chessmaster 9000\", a widely available chess playing game from Ubisoft, defeated the then U.S. Chess Champion and International Grandmaster Larry Christiansen in a four-game match. In 2006 reigning World Champion Vladimir Kramnik was defeated 4:2 by Deep Fritz, running on a multiprocessor personal computer.\n\nAccording to Ray Kurzweil, 89 out of 108 predictions he made were entirely correct by the end of 2009. An additional 13 were what he calls “essentially correct\" (meaning that they were likely to be realized within a few years of 2009), for a total of 102 out of 108. Another 3 are partially correct, 2 look like they are about 10 years off, and 1, which was tongue in cheek anyway, was just wrong. Kurzweil later released a more detailed analysis of the accuracy of his predictions up to 2009, arguing that most were correct.\n\n\n\n\n\n\n\nKurzweil has even wagered that his predictions will be true, on the site Long Bets Betting against Mitchell Kapor, founder of Lotus Software Corporation for a payout of $20,000, or $10,000 each.\n\n\n\n\n\n\n\n\n\n\n\n\n10 terabytes of storage (not memory) have been already reached for $535 (Seagate Barracuda 10TB) in mid 2016.\n\n\n\n\n\n\n\n\n\nKurzweil said in a 2006 C-SPAN2 interview that \"nanotechnology-based\" flying cars would be available in 20 years.\n\nKurzweil has said that by 2014, humanity will reach a \"tipping point\" where the cost-per-watt from solar energy is cheaper than from coal and oil: By capturing only 0.03 percent of the sun's energy that falls on Earth, humanity could meet virtually all of its projected energy needs up to 2030 (thirty trillion watts); this will be capable through with extremely inexpensive, lightweight, and efficient nano-engineered solar panels together with nano-fuel cells to store and distribute the captured energy. Kurzweil believes, by the end of the 2020s, humans will be able to completely replace fossil fuels.\n\nKurzweil has said that by the 2030s, people will be able to send nano-bots into their brains through their capillaries. The nano-bots will take up positions in close physical proximity to each interneuronal connection coming from each physical sense and cause the firing of neurons to result in full-immersion virtual reality, similar to the way psychedelic drugs alter consciousness. The nano-bots will also allow people to \"connect their neocortex to the cloud\". This technology is based on “neuron transistors” developed by scientists at the Max Planck Institute that can control the firing of neurons.\n\nKurzweil said the following in a November 2007 \"Computerworld\" interview:\n\nIn the cover article of the December 2010 issue of \"IEEE Spectrum\", John Rennie criticized Kurzweil's predictions: \"On close examination, his clearest and most successful predictions often lack originality or profundity. And most of his predictions come with so many loopholes that they border on the unfalsifiable.\"\n", "id": "20044858", "title": "Predictions made by Ray Kurzweil"}
{"url": "https://en.wikipedia.org/wiki?curid=28545000", "text": "Talk Reason\n\nTalk Reason is a website dedicated to opposing creationism and promoting evolution. Talk Reason collects articles for this purpose and provides a forum to present them.\n\n\n", "id": "28545000", "title": "Talk Reason"}
{"url": "https://en.wikipedia.org/wiki?curid=322355", "text": "Deep time\n\nDeep time is the concept of geologic time. The modern philosophical concept was developed in the 18th century by Scottish geologist James Hutton (1726–1797). The age of the Earth has been determined to be, after a long and complex history of developments, around 4.55 billion years.\n\nHutton based his view of deep time on a form of geochemistry that had developed in Scotland and Scandinavia from the 1750s onward. As mathematician John Playfair, one of Hutton's friends and colleagues in the Scottish Enlightenment, later remarked upon seeing the strata of the angular unconformity at Siccar Point with Hutton and James Hall in June 1788, \"the mind seemed to grow giddy by looking so far into the abyss of time\".\n\nEarly geologists such as Nicolas Steno (1638-1686) and Horace-Bénédict de Saussure (1740-1799) had developed ideas of geological strata forming from water through chemical processes, which Abraham Gottlob Werner (1749–1817) developed into a theory known as Neptunism, envisaging the slow crystallisation of minerals in the ancient oceans of the Earth to form rock. Hutton's innovative 1785 theory, based on Plutonism, visualised an endless cyclical process of rocks forming under the sea, being uplifted and tilted, then eroded to form new strata under the sea. In 1788 the sight of Hutton's Unconformity at Siccar Point convinced Playfair and Hall of this extremely slow cycle, and in that same year Hutton memorably wrote \"we find no vestige of a beginning, no prospect of an end\".\n\nOther scientists such as Georges Cuvier (1769-1832) put forward ideas of past ages, and geologists such as Adam Sedgwick (1785-1873) incorporated Werner's ideas into concepts of catastrophism; Sedgwick inspired his university student Charles Darwin to exclaim \"What a capital hand is Sedgewick [sic] for drawing large cheques upon the Bank of Time!\". In a competing theory, Charles Lyell in his \"Principles of Geology\" (1830–1833) developed Hutton's comprehension of endless deep time as a crucial scientific concept into uniformitarianism. As a young naturalist and geological theorist, Darwin studied the successive volumes of Lyell's book exhaustively during the \"Beagle\" survey voyage in the 1830s, before beginning to theorise about evolution.\n\nPhysicist Gregory Benford addresses the concept in \"Deep Time: How Humanity Communicates Across Millennia\" (1999), as does paleontologist and \"Nature\" editor Henry Gee in \"In Search of Deep Time: Beyond the Fossil Record to a New History of Life\" (2001) Stephen Jay Gould's \"Time's Arrow, Time's Cycle\" (1987) also deals in large part with the evolution of the concept.\n\nJohn McPhee discussed \"deep time\" at length with the layman in mind in \"Basin and Range\" (1981), parts of which originally appeared in the \"New Yorker\" magazine. In \"Time's Arrow, Time's Cycle\", Gould cited one of the metaphors McPhee used in explaining the concept of deep time:\nConsider the Earth's history as the old measure of the English yard, the distance from the King's nose to the tip of his outstretched hand. One stroke of a nail file on his middle finger erases human history.\nConcepts similar to geologic time were recognized in the 11th century by the Persian geologist and polymath Avicenna (Ibn Sina, 973–1037), and by the Chinese naturalist and polymath Shen Kuo (1031–1095).\n\nThe Roman Catholic theologian Thomas Berry (1914–2009) explored spiritual implications of the concept of deep time. Berry proposes that a deep understanding of the history and functioning of the evolving universe is a necessary inspiration and guide for our own effective functioning as individuals and as a species. This view has greatly influenced the development of deep ecology and ecophilosophy. The experiential nature of the experience of deep time has also greatly influenced the work of Joanna Macy and John Seed.\n\nH.G. Wells and Julian Huxley regarded the difficulties of coping with the concept of deep time as exaggerated:\n\"The use of different scales is simply a matter of practice\", they said in \"The Science of Life\" (1929). \"We very soon get used to maps, though they are constructed on scales down to a hundred-millionth of natural size. . .  to grasp geological time all that is needed is to stick tight to some magnitude which shall be the unit on the new and magnified scale—a million years is probably the most convenient—to grasp its meaning once and for all by an effort of imagination, and then to think of all passage of geological time in terms of this unit.\"\n\n\n", "id": "322355", "title": "Deep time"}
{"url": "https://en.wikipedia.org/wiki?curid=27879368", "text": "Functional divergence\n\nFunctional divergence is the process by which genes, after gene duplication, shift in function from an ancestral function. Functional divergence can result in either subfunctionalization, where a paralog specializes one of several ancestral functions, or neofunctionalization, where a totally new functional capability evolves. It is thought that this process of gene duplication and functional divergence is a major originator of molecular novelty and has produced the many large protein families that exist today.\n\nFunctional divergence is just one possible outcome of gene duplication events. Other fates include nonfunctionalization where one of the paralogs acquires deleterious mutations and becomes a pseudogene and superfunctionalization (reinforcement), where both paralogs maintain original function. While gene, chromosome, or whole genome duplication events are considered the canonical sources of functional divergence of paralogs, orthologs (genes descended from speciation events) can also undergo functional divergence and horizontal gene transfer can also result in multiple copies of a gene in a genome, providing the opportunity for functional divergence.\n\nMany well known protein families are the result of this process, such as the ancient gene duplication event that led to the divergence of hemoglobin and myoglobin, the more recent duplication events that led to the various subunit expansions (alpha and beta) of vertebrate hemoglobins, or the expansion of G-protein alpha subunits \n\n", "id": "27879368", "title": "Functional divergence"}
{"url": "https://en.wikipedia.org/wiki?curid=3230875", "text": "Evolutionary Principle\n\nThe Evolutionary Principle is a largely psychological doctrine formulated by anthropologist Claude Lévi-Strauss which roughly states that when a species is removed from the habitat in which it evolved, or that habitat changes significantly within a brief period (evolutionarily speaking), the species will develop maladaptive or outright pathological behavior. The Evolutionary Principle is important in neo-tribalist and anarcho-primitivist thinking.\n", "id": "3230875", "title": "Evolutionary Principle"}
{"url": "https://en.wikipedia.org/wiki?curid=30744522", "text": "Applications of evolution\n\nEvolutionary biology, in particular the understanding of how organisms evolve through natural selection, is an area of science with many practical applications. Creationists often claim that the theory of evolution lacks any practical applications; however, this claim has been refuted by scientists.\n\nThe evolutionary approach is key to much current research in biology that does not set out to study evolution per se, especially in organismal biology and ecology. For example, evolutionary thinking is key to life history theory. Annotation of genes and their function relies heavily on comparative, that is evolutionary, approaches. The field of evolutionary developmental biology investigates how developmental processes work by using the comparative method to determine how they evolved.\n\nA major technological application of evolution is artificial selection, which is the intentional selection of certain traits in a population of organisms. Humans have used artificial selection for thousands of years in the domestication of plants and animals. More recently, such selection has become a vital part of genetic engineering, with selectable markers such as antibiotic resistance genes being used to manipulate DNA in molecular biology. It is also possible to use repeated rounds of mutation and selection to evolve proteins with particular properties, such as modified enzymes or new antibodies, in a process called directed evolution.\n\nAntibiotic resistance can be a result of point mutations in the pathogen genome at a rate of about 1 in 10 per chromosomal replication. The antibiotic action against the pathogen can be seen as an environmental pressure; those bacteria which have a mutation allowing them to survive will live on to reproduce. They will then pass this trait to their offspring, which will result in a fully resistant colony.\n\nUnderstanding the changes that have occurred during organism's evolution can reveal the genes needed to construct parts of the body, genes which may be involved in human genetic disorders. For example, the Mexican tetra is an albino cavefish that lost its eyesight during evolution. Breeding together different populations of this blind fish produced some offspring with functional eyes, since different mutations had occurred in the isolated populations that had evolved in different caves. This helped identify genes required for vision and pigmentation, such as crystallins and the melanocortin 1 receptor. Similarly, comparing the genome of the Antarctic icefish, which lacks red blood cells, to close relatives such as the Antarctic rockcod revealed genes needed to make these blood cells.\n\nAs evolution can produce highly optimised processes and networks, it has many applications in computer science. Here, simulations of evolution using evolutionary algorithms and artificial life started with the work of Nils Aall Barricelli in the 1960s, and was extended by Alex Fraser, who published a series of papers on simulation of artificial selection. Artificial evolution became a widely recognised optimisation method as a result of the work of Ingo Rechenberg in the 1960s and early 1970s, who used evolution strategies to solve complex engineering problems. Genetic algorithms in particular became popular through the writing of John Holland. As academic interest grew, dramatic increases in the power of computers allowed practical applications, including the automatic evolution of computer programs. Evolutionary algorithms are now used to solve multi-dimensional problems more efficiently than software produced by human designers, and also to optimise the design of systems.\n", "id": "30744522", "title": "Applications of evolution"}
{"url": "https://en.wikipedia.org/wiki?curid=31880880", "text": "Theoretical foundations of evolutionary psychology\n\nThe theoretical foundations of evolutionary psychology are the general and specific scientific theories that explain the ultimate origins of psychological traits in terms of evolution. These theories originated with Charles Darwin's work, including his speculations about the evolutionary origins of social instincts in humans. Modern evolutionary psychology, however, is possible only because of advances in evolutionary theory in the 20th century.\n\nEvolutionary psychologists say that natural selection has provided humans with many psychological adaptations, in much the same way that it generated humans' anatomical and physiological adaptations. As with adaptations in general, psychological adaptations are said to be specialized for the environment in which an organism evolved, the environment of evolutionary adaptedness, or EEA. Sexual selection provides organisms with adaptations related to mating. For male mammals, which have a relatively fast reproduction rate, sexual selection leads to adaptations that help them compete for females. For female mammals, with a relatively slow reproduction rate, sexual selection leads to choosiness, which helps females select higher quality mates. Charles Darwin described both natural selection and sexual selection, but he relied on group selection to explain the evolution of self-sacrificing behavior. Group selection is a weak explanation because in any group the less self-sacrificing animals will be more likely to survive and the group will become less self-sacrificing.\n\nIn 1964, William D. Hamilton proposed inclusive fitness theory, emphasizing a \"gene's-eye\" view of evolution. Hamilton noted that individuals can increase the replication of their genes into the next generation by helping close relatives with whom they share genes survive and reproduce. According to \"Hamilton's rule\", a self-sacrificing behavior can evolve if it helps close relatives so much that it more than compensates for the individual animal's sacrifice. Inclusive fitness theory resolved the issue of how \"altruism\" evolved. Other theories also help explain the evolution of altruistic behavior, including evolutionary game theory, tit-for-tat reciprocity, and generalized reciprocity. These theories not only help explain the development of altruistic behavior but also account for hostility toward cheaters (individuals that take advantage of others' altruism).\n\nSeveral mid-level evolutionary theories inform evolutionary psychology. The r/K selection theory proposes that some species prosper by having many offspring while others follow the strategy of having fewer offspring but investing much more in each one. Humans follow the second strategy. Parental investment theory explains how parents invest more or less in individual offspring based on how successful those offspring are likely to be, and thus how much they might improve the parents' inclusive fitness. According to the Trivers-Willard hypothesis, parents in good conditions tend to invest more in sons (who are best able to take advantage of good conditions), while parents in poor conditions tend to invest more in daughters (who are best able to have successful offspring even in poor conditions). According to life history theory, animals evolve life histories to match their environments, determining details such as age at first reproduction and number of offspring. Dual inheritance theory posits that genes and human culture have interacted, with genes affecting the development of culture and culture, in turn, affecting human evolution on a genetic level (see also the Baldwin effect).\n\nCritics of evolutionary psychology have sometimes challenged its theoretical underpinnings, saying that humans never developed powerful social instincts through natural selection and that the hypotheses of evolutionary psychologists are merely just-so-stories.\n\nEvolutionary psychology primarily uses the theories of natural selection, sexual selection, and inclusive fitness to explain the evolution of psychological adaptations.\n\nEvolutionary psychology is sometimes seen not simply as a subdiscipline of psychology but as a metatheoretical framework in which \"the entire field of psychology can be examined.\"\n\nEvolutionary psychologists consider Charles Darwin's theory of natural selection to be important to an understanding of psychology. Natural selection occurs because individual organisms who are genetically better suited to the current environment leave more descendants, and their genes spread through the population, thus explaining why organisms fit their environments so closely. This process is slow and cumulative, with new traits layered over older traits. The advantages created by natural selection are known as adaptations. Evolutionary psychologists say that animals, just as they evolve physical adaptations, evolve psychological adaptations.\n\nEvolutionary psychologists emphasize that natural selection mostly generates specialized adaptations, which are more efficient than general adaptations. They point out that natural selection operates slowly, and that adaptations are sometimes out of date when the environment changes rapidly. In the case of humans, evolutionary psychologists say that much of human nature was shaped during the stone age and may not match the contemporary environment.\n\nSexual selection favors traits that provide mating advantages, such as the peacock's tail, even if these same traits are usually hindrances. Evolutionary psychologists point out that, unlike natural selection, sexual selection typically leads to the evolution of sex differences. Sex differences typically make reproduction faster for one sex and slower for the other, in which case mates are relatively scarce for the faster sex. Sexual selection favors traits that increase the number of mates for the fast sex and the quality of mates for the slow sex. For mammals, the female has the slower reproduction rate. Males typically evolve either traits to help them fight other males or traits to impress females. Females typically evolve greater abilities to discern the qualities of males, such as choosiness in mating.\n\nInclusive fitness theory, proposed by William D. Hamilton, emphasized a \"gene's-eye\" view of evolution. Hamilton noted that what evolution ultimately selects are genes, not groups or species. From this perspective, individuals can increase the replication of their genes into the next generation not only directly via reproduction, by also indirectly helping close relatives with whom they share genes survive and reproduce. General evolutionary theory, in its modern form, \"is\" essentially inclusive fitness theory.\n\nInclusive fitness theory resolved the issue of how \"altruism\" evolved. The dominant, pre-Hamiltonian view was that altruism evolved via group selection: the notion that altruism evolved for the benefit of the group. The problem with this was that if one organism in a group incurred any fitness costs on itself for the benefit of others in the group, (i.e. acted \"altruistically\"), then that organism would reduce its own ability to survive and/or reproduce, therefore reducing its chances of passing on its altruistic traits.\n\nFurthermore, the organism that benefited from that altruistic act and only acted on behalf of its own fitness would increase its own chance of survival and/or reproduction, thus increasing its chances of passing on its \"selfish\" traits.\nInclusive fitness resolved \"the problem of altruism\" by demonstrating that altruism can evolve via kin selection as expressed in Hamilton's rule:\ncost < relatedness × benefit\nIn other words, altruism can evolve as long as the fitness \"cost\" of the altruistic act on the part of the actor is less than the \"degree of genetic relatedness\" of the recipient times the fitness \"benefit\" to that recipient.\nThis perspective reflects what is referred to as the gene-centered view of evolution and demonstrates that group selection is a very weak selective force.\n\nMiddle-level evolutionary theories are consistent with general evolutionary theory, but focus on certain domains of functioning (Buss, 2011) Specific evolutionary psychology hypotheses may be derivative from a mid-level theory (Buss, 2011). Three very important middle-level evolutionary theories were contributed by Robert Trivers as well as Robert MacArthur and E. O. Wilson\n\n", "id": "31880880", "title": "Theoretical foundations of evolutionary psychology"}
{"url": "https://en.wikipedia.org/wiki?curid=163901", "text": "Information society\n\nAn information society is a society where the creation, distribution, use, integration and manipulation of information is a significant economic, political, and cultural activity. Its main drivers are digital information and communication technologies, which have resulted in an information explosion and are profoundly changing all aspects of social organization, including the economy, education, health, warfare, government and democracy. The People who have the means to partake in this form of society are sometimes called digital citizens, defined by K. Mossberger as “Those who use the Internet regularly and effectively”. This is one of many dozen labels that have been identified to suggest that humans are entering a new phase of society.\n\nThe markers of this rapid change may be technological, economic, occupational, spatial, cultural, or some combination of all of these.\nInformation society is seen as the successor to industrial society. Closely related concepts are the post-industrial society (Daniel Bell), post-fordism, post-modern society, knowledge society, telematic society, Information Revolution, liquid modernity, and network society (Manuel Castells).\n\nThere is currently no universally accepted concept of what exactly can be termed information society and what shall rather not so be termed. Most theoreticians agree that a transformation can be seen that started somewhere between the 1970s and today and is changing the way societies work fundamentally. Information technology goes beyond the internet, and there are discussions about how big the influence of specific media or specific modes of production really is. Frank Webster notes five major types of information that can be used to define information society: technological, economic, occupational, spatial and cultural. According to Webster, the character of information has transformed the way that we live today. How we conduct ourselves centers around theoretical knowledge and information.\n\nKasiwulaya and Gomo (Makerere University) allude that information societies are those that have intensified their use of IT for economic, social, cultural and political transformation. In 2005, governments reaffirmed their dedication to the foundations of the Information\nSociety in the Tunis Commitment and outlined the basis for implementation and follow-up in the Tunis Agenda for the Information Society. In particular, the Tunis Agenda addresses the issues of financing of ICTs for development and Internet governance that could not be resolved in the first phase.\n\nSome people, such as Antonio Negri, characterize the information society as one in which people do immaterial labour. By this, they appear to refer to the production of knowledge or cultural artifacts. One problem with this model is that it ignores the material and essentially industrial basis of the society. However it does point to a problem for workers, namely how many creative people does this society need to function? For example, it may be that you only need a few star performers, rather than a plethora of non-celebrities, as the work of those performers can be easily distributed, forcing all secondary players to the bottom of the market. It \"is\" now common for publishers to promote only their best selling authors and to try to avoid the rest—even if they still sell steadily. Films are becoming more and more judged, in terms of distribution, by their first weekend's performance, in many cases cutting out opportunity for word-of-mouth development.\n\nMichael Buckland characterizes information in society in his book \"Information and Society.\" Buckland expresses the idea that information can be interpreted differently from person to person based on that individual's experiences.\n\nConsidering that metaphors and technologies of information move forward in a reciprocal relationship, we can describe some societies (especially the Japanese society) as an information society because we think of it as such.\nThe word information may be interpreted in many different ways. According to Buckland in \"Information and Society\", most of the meanings fall into three categories of human knowledge: information as knowledge, information as a process, and information as a thing.\n\nThe growth of technologically mediated information has been quantified in different ways, including society's technological capacity to store information, to communicate information, and to compute information. It is estimated that, the world's technological capacity to store information grew from 2.6 (optimally compressed) exabytes in 1986, which is the informational equivalent to less than one 730-MB CD-ROM per person in 1986 (539 MB per person), to 295 (optimally compressed) exabytes in 2007. This is the informational equivalent of 60 CD-ROM per person in 2007 and represents a sustained annual growth rate of some 25%. The world’s combined technological capacity to receive information through one-way broadcast networks was the informational equivalent of 174 newspapers per person per day in 2007.\n\nThe world's combined effective capacity to exchange information through two-way telecommunication networks was 281 petabytes of (optimally compressed) information in 1986, 471 petabytes in 1993, 2.2 (optimally compressed) exabytes in 2000, and 65 (optimally compressed) exabytes in 2007, which is the informational equivalent of 6 newspapers per person per day in 2007. The world's technological capacity to compute information with humanly guided general-purpose computers grew from 3.0 × 10^8 MIPS in 1986, to 6.4 x 10^12 MIPS in 2007, experiencing the fastest growth rate of over 60% per year during the last two decades.\n\nJames R. Beniger describes the necessity of information in modern society in the following way: “The need for sharply increased control that resulted from the industrialization of material processes through application of inanimate sources of energy probably accounts for the rapid development of automatic feedback technology in the early industrial period (1740-1830)” (p. 174)\n“Even with enhanced feedback control, industry could not have developed without the enhanced means to process matter and energy, not only as inputs of the raw materials of production but also as outputs distributed to final consumption.”(p. 175)\n\nOne of the first people to develop the concept of the information society was the economist Fritz Machlup. In 1933, Fritz Machlup began studying the effect of patents on research. His work culminated in the study \"The production and distribution of knowledge in the United States\" in 1962. This book was widely regarded and was eventually translated into Russian and Japanese. The Japanese have also studied the information society (or \"jōhōka shakai\", ).\n\nThe issue of technologies and their role in contemporary society have been discussed in the scientific literature using a range of labels and concepts. This section introduces some of them. Ideas of a knowledge or information economy, post-industrial society, postmodern society, network society, the information revolution, informational capitalism, network capitalism, and the like, have been debated over the last several decades.\n\nFritz Machlup (1962) introduced the concept of the knowledge industry. He began studying the effects of patents on research before distinguishing five sectors of the knowledge sector: education, research and development, mass media, information technologies, information services. Based on this categorization he calculated that in 1959 29% per cent of the GNP in the USA had been produced in knowledge industries.\n\nPeter Drucker has argued that there is a transition from an economy based on material goods to one based on knowledge. Marc Porat distinguishes a primary (information goods and services that are directly used in the production, distribution or processing of information) and a secondary sector (information services produced for internal consumption by government and non-information firms) of the information economy.\n\nPorat uses the total value added by the primary and secondary information sector to the GNP as an indicator for the information economy. The OECD has employed Porat's definition for calculating the share of the information economy in the total economy (e.g. OECD 1981, 1986). Based on such indicators, the information society has been defined as a society where more than half of the GNP is produced and more than half of the employees are active in the information economy.\n\nFor Daniel Bell the number of employees producing services and information is an indicator for the informational character of a society. \"A post-industrial society is based on services. (…) What counts is not raw muscle power, or energy, but information. (…) A post industrial society is one in which the majority of those employed are not involved in the production of tangible goods\".\n\nAlain Touraine already spoke in 1971 of the post-industrial society. \"The passage to postindustrial society takes place when investment results in the production of symbolic goods that modify values, needs, representations, far more than in the production of material goods or even of 'services'. Industrial society had transformed the means of production: post-industrial society changes the ends of production, that is, culture. (…) The decisive point here is that in postindustrial society all of the economic system is the object of intervention of society upon itself. That is why we can call it the programmed society, because this phrase captures its capacity to create models of management, production, organization, distribution, and consumption, so that such a society appears, at all its functional levels, as the product of an action exercised by the society itself, and not as the outcome of natural laws or cultural specificities\" (Touraine 1988: 104). In the programmed society also the area of cultural reproduction including aspects such as information, consumption, health, research, education would be industrialized. That modern society is increasing its capacity to act upon itself means for Touraine that society is reinvesting ever larger parts of production and so produces and transforms itself. This makes Touraine's concept substantially different from that of Daniel Bell who focused on the capacity to process and generate information for efficient society functioning.\n\nJean-François Lyotard has argued that \"knowledge has become the force of production over the last few decades\". Knowledge would be transformed into a commodity. Lyotard says that postindustrial society makes knowledge accessible to the layman because knowledge and information technologies would diffuse into society and break up Grand Narratives of centralized structures and groups. Lyotard denotes these changing circumstances as postmodern condition or postmodern society.\n\nSimilarly to Bell, Peter Otto and Philipp Sonntag (1985) say that an information society is a society where the majority of employees work in information jobs, i.e. they have to deal more with information, signals, symbols, and images than with energy and matter. Radovan Richta (1977) argues that society has been transformed into a scientific civilization based on services, education, and creative activities. This transformation would be the result of a scientific-technological transformation based on technological progress and the increasing importance of computer technology. Science and technology would become immediate forces of production (Aristovnik 2014: 55).\n\nNico Stehr (1994, 2002a, b) says that in the knowledge society a majority of jobs involves working with knowledge. \"Contemporary society may be described as a knowledge society based on the extensive penetration of all its spheres of life and institutions by scientific and technological knowledge\" (Stehr 2002b: 18). For Stehr, knowledge is a capacity for social action. Science would become an immediate productive force, knowledge would no longer be primarily embodied in machines, but already appropriated nature that represents knowledge would be rearranged according to certain designs and programs (Ibid.: 41-46). For Stehr, the economy of a knowledge society is largely driven not by material inputs, but by symbolic or knowledge-based inputs (Ibid.: 67), there would be a large number of professions that involve working with knowledge, and a declining number of jobs that demand low cognitive skills as well as in manufacturing (Stehr 2002a).\n\nAlso Alvin Toffler argues that knowledge is the central resource in the economy of the information society: \"In a Third Wave economy, the central resource – a single word broadly encompassing data, information, images, symbols, culture, ideology, and values – is actionable knowledge\" (Dyson/Gilder/Keyworth/Toffler 1994).\n\nAt the end of the twentieth century, the concept of the network society gained importance in information society theory. For Manuel Castells, network logic is besides information, pervasiveness, flexibility, and convergence a central feature of the information technology paradigm (2000a: 69ff). \"One of the key features of informational society is the networking logic of its basic structure, which explains the use of the concept of 'network society'\" (Castells 2000: 21). \"As an historical trend, dominant functions and processes in the Information Age are increasingly organized around networks. Networks constitute the new social morphology of our societies, and the diffusion of networking logic substantially modifies the operation and outcomes in processes of production, experience, power, and culture\" (Castells 2000: 500). For Castells the network society is the result of informationalism, a new technological paradigm.\n\nJan Van Dijk (2006) defines the network society as a \"social formation with an infrastructure of social and media networks enabling its prime mode of organization at all levels (individual, group/organizational and societal). Increasingly, these networks link all units or parts of this formation (individuals, groups and organizations)\" (Van Dijk 2006: 20). For Van Dijk networks have become the nervous system of society, whereas Castells links the concept of the network society to capitalist transformation, Van Dijk sees it as the logical result of the increasing widening and thickening of networks in nature and society. Darin Barney uses the term for characterizing societies that exhibit two fundamental characteristics: \"The first is the presence in those societies of sophisticated – almost exclusively digital – technologies of networked communication and information management/distribution, technologies which form the basic infrastructure mediating an increasing array of social, political and economic practices. (…) The second, arguably more intriguing, characteristic of network societies is the reproduction and institutionalization throughout (and between) those societies of networks as the basic form of human organization and relationship across a wide range of social, political and economic configurations and associations\".\n\nThe major critique of concepts such as information society, knowledge society, network society, postmodern society, postindustrial society, etc. that has mainly been voiced by critical scholars is that they create the impression that we have entered a completely new type of society. \"If there is just more information then it is hard to understand why anyone should suggest that we have before us something radically new\" (Webster 2002a: 259). Critics such as Frank Webster argue that these approaches stress discontinuity, as if contemporary society had nothing in common with society as it was 100 or 150 years ago. Such assumptions would have ideological character because they would fit with the view that we can do nothing about change and have to adopt to existing political realities (kasiwulaya 2002b: 267).\n\nThese critics argue that contemporary society first of all is still a capitalist society oriented towards accumulating economic, political, and cultural capital. They acknowledge that information society theories stress some important new qualities of society (notably globalization and informatization), but charge that they fail to show that these are attributes of overall capitalist structures. Critics such as Webster insist on the continuities that characterise change. In this way Webster distinguishes between different epochs of capitalism: laissez-faire capitalism of the 19th century, corporate capitalism in the 20th century, and informational capitalism for the 21st century (kasiwulaya 2006).\n\nFor describing contemporary society based on a dialectic of the old and the new, continuity and discontinuity, other critical scholars have suggested several terms like:\n\nOther scholars prefer to speak of information capitalism (Morris-Suzuki 1997) or informational capitalism (Manuel Castells 2000, Christian Fuchs 2005, Schmiede 2006a, b). Manuel Castells sees informationalism as a new technological paradigm (he speaks of a mode of development) characterized by \"information generation, processing, and transmission\" that have become \"the fundamental sources of productivity and power\" (Castells 2000: 21). The \"most decisive historical factor accelerating, channelling and shaping the information technology paradigm, and inducing its associated social forms, was/is the process of capitalist restructuring undertaken since the 1980s, so that the new techno-economic system can be adequately characterized as informational capitalism\" (Castells 2000: 18). Castells has added to theories of the information society the idea that in contemporary society dominant functions and processes are increasingly organized around networks that constitute the new social morphology of society (Castells 2000: 500). Nicholas Garnham is critical of Castells and argues that the latter’s account is technologically determinist because Castells points out that his approach is based on a dialectic of technology and society in which technology embodies society and society uses technology (Castells 2000: 5sqq). But Castells also makes clear that the rise of a new \"mode of development\" is shaped by capitalist production, i.e. by society, which implies that technology isn't the only driving force of society.\n\nAntonio Negri and Michael Hardt argue that contemporary society is an Empire that is characterized by a singular global logic of capitalist domination that is based on immaterial labour. With the concept of immaterial labour Negri and Hardt introduce ideas of information society discourse into their Marxist account of contemporary capitalism. Immaterial labour would be labour \"that creates immaterial products, such as knowledge, information, communication, a relationship, or an emotional response\" (Hardt/Negri 2005: 108; cf. also 2000: 280-303), or services, cultural products, knowledge (Hardt/Negri 2000: 290). There would be two forms: intellectual labour that produces ideas, symbols, codes, texts, linguistic figures, images, etc.; and affective labour that produces and manipulates affects such as a feeling of ease, well-being, satisfaction, excitement, passion, joy, sadness, etc. (Ibid.).\n\nOverall, neo-Marxist accounts of the information society have in common that they stress that knowledge, information technologies, and computer networks have played a role in the restructuration and globalization of capitalism and the emergence of a flexible regime of accumulation (David Harvey 1989). They warn that new technologies are embedded into societal antagonisms that cause structural unemployment, rising poverty, social exclusion, the deregulation of the welfare state and of labour rights, the lowering of wages, welfare, etc.\n\nConcepts such as knowledge society, information society, network society, informational capitalism, postindustrial society, transnational network capitalism, postmodern society, etc. show that there is a vivid discussion in contemporary sociology on the character of contemporary society and the role that technologies, information, communication, and co-operation play in it. Information society theory discusses the role of information and information technology in society, the question which key concepts shall be used for characterizing contemporary society, and how to define such concepts. It has become a specific branch of contemporary sociology.\n\nInformation society is the means of getting information from one place to another. As technology has advanced so too has the way people have adapted in sharing this information with each other.\n\n\"Second nature\" refers a group of experiences that get made over by culture. They then get remade into something else that can then take on a new meaning. As a society we transform this process so it becomes something natural to us, i.e. second nature. So, by following a particular pattern created by culture we are able to recognise how we use and move information in different ways. From sharing information via different time zones (such as talking online) to information ending up in a different location (sending a letter overseas) this has all become a habitual process that we as a society take for granted.\n\nHowever, through the process of sharing information vectors have enabled us to spread information even further. Through the use of these vectors information is able to move and then separate from the initial things that enabled them to move. From here, something called \"third nature\" has developed. An extension of second nature, third nature is in control of second nature. It expands on what second nature is limited by. It has the ability to mould information in new and different ways. So, third nature is able to ‘speed up, proliferate, divide, mutate, and beam in on us from else where. It aims to create a balance between the boundaries of space and time (see second nature). This can be seen through the telegraph, it was the first successful technology that could send and receive information faster than a human being could move an object. As a result different vectors of people have the ability to not only shape culture but create new possibilities that will ultimately shape society.\n\nTherefore, through the use of second nature and third nature society is able to use and explore new vectors of possibility where information can be moulded to create new forms of interaction.\n\nIn sociology, informational society refers to a post-modern type of society. Theoreticians like Ulrich Beck, Anthony Giddens and Manuel Castells argue that since the 1970s a transformation from industrial society to informational society has happened on a global scale.\n\nAs steam power was the technology standing behind industrial society, so information technology is seen as the catalyst for the changes in work organisation, societal structure and politics occurring in the late 20th century.\n\nIn the book \"Future Shock\", Alvin Toffler used the phrase super-industrial society to describe this type of society. Other writers and thinkers have used terms like \"post-industrial society\" and \"post-modern industrial society\" with a similar meaning.\n\nA number of terms in current use emphasize related but different aspects of the emerging global economic order. The Information Society intends to be the most encompassing in that an economy is a subset of a society. The Information Age is somewhat limiting, in that it refers to a 30-year period between the widespread use of computers and the knowledge economy, rather than an emerging economic order. The knowledge era is about the nature of the content, not the socioeconomic processes by which it will be traded. The computer revolution, and knowledge revolution refer to specific revolutionary transitions, rather than the end state towards which we are evolving. The Information Revolution relates with the well known terms agricultural revolution and industrial revolution.\n\nToday, It is important to selectively select the information. Due to information revolution, the amount of information is puzzling. Among these, we need to develop techniques that refine information. This is called \"data mining.\" It is an engineering term, but it is used in sociology. In other words, if the amount of information was competitive in the past, the quality of information is important today.\n\nOne of the central paradoxes of the information society is that it makes information easily reproducible, leading to a variety of freedom/control problems relating to intellectual property. Essentially, business and capital, whose place becomes that of producing and selling information and knowledge, seems to require control over this new resource so that it can effectively be managed and sold as the basis of the information economy. However, such control can prove to be both technically and socially problematic. Technically because copy protection is often easily circumvented and socially \"rejected\" because the users and citizens of the information society can prove to be unwilling to accept such absolute commodification of the facts and information that compose their environment. \n\nResponses to this concern range from the Digital Millennium Copyright Act in the United States (and similar legislation elsewhere) which make copy protection (see DRM) circumvention illegal, to the free software, open source and copyleft movements, which seek to encourage and disseminate the \"freedom\" of various information products (traditionally both as in \"gratis\" or free of cost, and liberty, as in freedom to use, explore and share).\n\nCaveat: Information society is often used by politicians meaning something like \"we all do internet now\"; the sociological term information society (or informational society) has some deeper implications about change of societal structure. Because we lack political control of intellectual property, we are lacking in a concrete map of issues, an analysis of costs and benefits, and functioning political groups that are unified by common interests representing different opinions of this diverse situation that are prominent in the information society.\n\n\n\n ", "id": "163901", "title": "Information society"}
{"url": "https://en.wikipedia.org/wiki?curid=31596828", "text": "Unequal crossing over\n\nUnequal crossing over is a type of gene duplication or deletion event that deletes a sequence in one strand and replaces it with a duplication from its sister chromatid in mitosis or from its homologous chromosome during meiosis. It is a type of chromosomal crossover between homologous sequences that are not paired precisely. Normally genes are responsible for occurrence of crossing over. It exchanges sequences of different links between chromosomes. Along with gene conversion, it is believed to be the main driver for the generation of gene duplications and is a source of mutation in the genome.\n\nDuring meiosis, the duplicated chromosomes (chromatids) in eukaryotic organisms are attached to each other in the centromere region and are thus paired. The maternal and paternal chromosomes then align alongside each other. During this time, recombination can take place via crossing over of sections of the paternal and maternal chromatids and leads to reciprocal recombination or non-reciprocal recombination. Unequal crossing over requires a measure of similarity between the sequences for misalignment to occur. The more similarity within the sequences, the more likely unequal crossing over will occur. One of the sequences is thus lost and replaced with the duplication of another sequence.\n\nWhen two sequences are misaligned, unequal crossing over may create a tandem repeat on one chromosome and a deletion on the other. The rate of unequal crossing over will increase with the number of repeated sequences around the duplication. This is because these repeated sequences will pair together, allowing for the mismatch in the cross over point to occur.\n\nUnequal crossing over is the process most responsible for creating regional gene duplications in the genome. Repeated rounds of unequal crossing over cause the homogenization of the two sequences. With the increase in the duplicates, unequal crossing over can lead to dosage imbalance in the genome and can be highly deleterious.\n\nIn unequal crossing over, there can be large sequence exchanges between the chromosomes. Compared with gene conversion, which can only transfer a maximum of 1,500 base pairs, unequal crossing over in yeast rDNA genes has been found to transfer about 20,000 base pairs in a single crossover event Unequal crossover can be followed by the concerted evolution of duplicated sequences.\n\nIt has been suggested that longer intron found between two beta-globin genes are a response to deleterious selection from unequal crossing over in the beta-globin genes. Comparisons between alpha-globin, which does not have long introns, and beta-globin genes show that alpha-globin have 50 times higher concerted evolution.\n\nWhen unequal crossing over creates a gene duplication, the duplicate has 4 evolutionary fates. This is due to the fact that purifying selection acting on a duplicated copy is not very strong. Now that there is a redundant copy, neutral mutations can act on the duplicate. Most commonly the neutral mutations will continue until the duplicate becomes a pseudogene. If the duplicate copy increases the dosage effect of the gene product, then the duplicate may be retained as a redundant copy. Neofunctionalization is also a possibility: the duplicated copy acquires a mutation that gives it a different function than its ancestor. If both copies acquire mutations, it is possible that a subfunctional event occurs. This happens when both of the duplicated sequences have a more specialized function than the ancestral copy\n\nGene duplications are the main reason for the increase of genome size, and as unequal crossing over is the main mechanism for gene duplication, unequal crossing over contributes to genome size evolution is the most common regional duplication event that increases the size of the genome.\n\nWhen viewing the genome of a eukaryote, a striking observation is the large amount of tandem, repetitive DNA sequences that make up a large portion of the genome. For example, over 50% of the \"Dipodmys ordii\" genome is made up of three specific repeats. \"Drosophila virilis\" has three sequences that make up 40% of the genome, and 35% of the \"Absidia glauca\" is repetitive DNA sequences. These short sequences have no selection pressure acting on them and the frequency of the repeats can be changed by unequal crossing over.\n", "id": "31596828", "title": "Unequal crossing over"}
{"url": "https://en.wikipedia.org/wiki?curid=1448018", "text": "Teleonomy\n\nTeleonomy is the quality of apparent purposefulness and of goal-directedness of structures and functions in living organisms brought about by natural laws (like natural selection). The term derives from two Greek words, τέλος \"telos\" (\"end, purpose\") and νόμος \"nomos\" (\"law\"), and means \"end-directed\" (literally \"purpose-law\"). Teleonomy is sometimes contrasted with teleology, where the latter is understood as a purposeful goal-directedness brought about through human or divine intention. Teleonomy is thought to derive from evolutionary history, adaptation for reproductive success, and/or the operation of a program. Teleonomy is related to programmatic or computational aspects of purpose.\n\nColin Pittendrigh, who coined the term in 1958, applied it to biological phenomena that appear to be end-directed, hoping to limit the much older term teleology to actions planned by an agent who can internally model alternative futures with intention, purpose and foresight:\nIn 1965 Ernst Mayr cited Pittendrigh and criticized him for not making a \"clear distinction between the two teleologies of Aristotle\"; evolution involves Aristotle's material causes and formal causes rather than efficient causes. Mayr adopted Pittendrigh's term, but supplied his own definition:\n\nRichard Dawkins described the properties of \"archeo-purpose\" (by natural selection) and \"neo-purpose\" (by evolved adaptation) in his talk on the \"Purpose of Purpose\". Dawkins attributes the brain's flexibility as an evolutionary feature in adapting or subverting goals to making neo-purpose goals on an overarching evolutionary archeo-purpose. Language allows groups to share neo-purposes, and cultural evolution - occurring much faster than natural evolution - can lead to conflict or collaborations.\n\nIn behavior analysis, Hayne Reese made the adverbial distinction between purposefulness (having an internal determination) and purposiveness (serving or effecting a useful function). Reese implies that non-teleological statements are called teleonomic when they represent an \"if A then C\" phenomenon's antecedent; where, teleology is a consequent representation. The concept of purpose, as only being the teleology final cause, requires supposedly impossible time reversal; because, the future consequent determines the present antecedent. Purpose, as being both in the beginning and the end, simply rejects teleology, and addresses the time reversal problem. In this, Reese sees no value for teleology and teleonomic concepts in behavior analysis; however, the concept of purpose preserved in process can be useful, if not reified. A theoretical time-dimensional tunneling and teleological functioning of Temporal Paradox would also fit this description without the necessity of a localized intelligence. Whereas the concept of a teleonomic process, such as evolution, can simply refer to a system capable of producing complex products without the benefit of a guiding foresight.\n\nIn 1966 George C. Williams approved of the term in the last chapter of his \"Adaptation and Natural Selection; a critique of some current evolutionary thought\". In 1970, Jacques Monod, in \"Chance and Necessity, an Essay on the Natural Philosophy of Modern Biology\", suggested teleonomy as a key feature that defines life:\n\nIn 1974 Ernst Mayr illustrated the difference in the statements:\n\nSubsequently, philosophers like Ernest Nagel further analysed the concept of goal-directedness in biology and by 1982, philosopher and historian of science David Hull joked about the use of teleology and teleonomy by biologists:\n\nThe concept of teleonomy was largely developed by Mayr and Pittendrigh to separate biological evolution from teleology. Pittendrigh's purpose was to enable biologists who had become overly cautious about goal-oriented language to have a way of discussing the goals and orientations of an organism's behaviors without inadvertently invoking teleology. Mayr was even more explicit, saying that while teleonomy certainly operates on the level of organisms, the process of evolution itself is necessarily non-teleonomic.\n\nEvolution largely hoards hindsight, as variations unwittingly make \"predictions\" about structures and functions which could successfully cope with the future, and which participate in a process of natural selection that culls the unfit, leaving the fit to the next generation. Information accumulates about functions and structures that are successful, exploiting feedback from the environment via the selection of fitter coalitions of structures and functions. Robert Rosen has described these features as an anticipatory system which builds an internal model based on past and possible future states.\n\nIn 1962, Grace A. de Laguna's \"The Role of Teleonomy in Evolution\" attempted to show how different stages of evolution were characterized by different types of teleonomy. de Laguna points out that humans have oriented teleonomy so that the teleonomic goal is not restricted to the reproduction of humans, but also to cultural ideals.\n\nIn recent years, a few biologists believe that the separation of teleonomy from the process of evolution has gone too far. Peter Corning notes that behavior, which is a teleonomic trait, is responsible for the construction of biological niches, which is an agent of selection. Therefore, it would be inaccurate to say that there was no role for teleonomy in the process of evolution, since teleonomy dictates the fitness landscape according to which organisms are selected. Corning calls this phenomenon \"teleonomic selection\".\n\nIn teleology, Kant's positions as expressed in Critique of Judgment, were neglected for many years because in the minds of many scientists they were associated with vitalist views of evolution. Their recent rehabilitation is evident in teleonomy, which bears a number of features, such as the description of organisms, that are reminiscent of the Aristotelian conception of final causes as essentially recursive in nature. Kant's position is that, even though we cannot know whether there are final causes in nature, we are constrained by the peculiar nature of the human understanding to view organisms teleologically. Thus the Kantian view sees teleology as a necessary principle for the study of organisms, but only as a regulative principle, and with no ontological implications.\n\nTalcott Parsons, in the later part of his working with a theory of social evolution and a related theory of world-history, adopted the concept of teleonomy as the fundamental organizing principle for directional processes and his theory of societal development in general. In this way, Parsons tried to find a theoretical compromise between voluntarism as a principle of action and the idea of a certain directionality in history.\n\nTeleonomy is closely related to concepts of emergence, complexity theory, and self-organizing systems. It has extended beneath biology to be applied in the context of chemistry. Some philosophers of biology resist the term and still employ \"teleology\" when analyzing biological function and the language used to describe it, while others endorse it.\n\n\n\n", "id": "1448018", "title": "Teleonomy"}
{"url": "https://en.wikipedia.org/wiki?curid=514231", "text": "Frequency-dependent selection\n\nFrequency-dependent selection is an evolutionary process by which the fitness of a phenotype depends on its frequency relative to other phenotypes in a given population.\n\n\nFrequency-dependent selection is usually the result of interactions between species (predation, parasitism, or competition), or between genotypes within species (usually competitive or symbiotic), and has been especially frequently discussed with relation to anti-predator adaptations. Frequency-dependent selection can lead to polymorphic equilibria, which result from interactions among genotypes within species, in the same way that multi-species equilibria require interactions between species in competition (e.g. where \"α\" parameters in Lotka-Volterra competition equations are non-zero).\n\nThe first explicit statement of frequency-dependent selection appears to have been by Edward Bagnall Poulton in 1884, on the way that predators could maintain color polymorphisms in their prey.\n\nPerhaps the best known early modern statement of the principle is Bryan Clarke's 1962 paper on apostatic selection (a synonym of negative frequency-dependent selection). Clarke discussed predator attacks on polymorphic British snails, citing Luuk Tinbergen's classic work on searching images as support that predators such as birds tended to specialize on common forms of palatable species. Clarke later argued that frequency-dependent balancing selection could explain molecular polymorphisms (often in the absence of heterosis) in opposition to the neutral theory of molecular evolution.\n\nAnother example is plant self-incompatibility alleles. When two plants share the same incompatibility allele, they are unable to mate. Thus, a plant with a new (and therefore, rare) allele has more success at mating, and its allele spreads quickly through the population .\n\nIn human pathogens, such as the flu virus, once a particular strain has become common, most individuals have developed an immune response to that strain. But a rare, novel strain of the flu virus is able to spread quickly to almost any individual, causing continual evolution of viral strains.\n\nThe major histocompatibility complex (MHC) is involved in the recognition of foreign antigens and cells. Frequency-dependent selection may explain the high degree of polymorphism in the MHC.\n\nIn behavioral ecology, negative frequency-dependent selection often maintains multiple behavioral strategies within a species. A classic example is the Hawk-Dove model of interactions among individuals in a population. In a population with two traits A and B, being one form is better when most members are the other form. As another example, male common side-blotched lizards have three morphs, which either defend large territories and maintain large harems of females, defend smaller territories and keep one female, or mimic females in order to sneak matings from the other two morphs. These three morphs participate in a rock paper scissors sort of interaction such that no one morph completely outcompetes the other two. Another example occurs in the scaly-breasted munia, where certain individuals become scroungers and others become producers.\n\nPositive frequency-dependent selection gives an advantage to common phenotypes. In the between-species analogue, this is equivalent to an Allee effect, in which if a species is too rare, it may decline to extinction. This means that new alleles can have a difficult time invading a population, since they don't experience significant benefit until they become common.\n\nPositive selection can be seen in the evolution of warning coloration (aposematism) in toxic or distasteful organisms. Signalling theory proposes that the advantage of such coloration is that predators can learn to avoid potential prey with that coloration. For example, in the Batesian mimicry complex between a harmless mimic, the scarlet kingsnake (\"Lampropeltis elapsoides\"), and the model, the eastern coral snake (\"Micrurus fulvius\"), in locations where the model and mimic were in deep sympatry, the phenotype of the scarlet kingsnake was quite variable due to relaxed selection. But where the pattern was rare, the predator population was not 'educated', so the pattern brought no benefit. The scarlet kingsnake was much less variable on the allopatry/sympatry border of the model and mimic, most probably due to increased selection since the eastern coral snake is rare, but present, on this border. Therefore, the coloration is only advantageous once it has become common.\n\n\n", "id": "514231", "title": "Frequency-dependent selection"}
{"url": "https://en.wikipedia.org/wiki?curid=36136905", "text": "Avalon explosion\n\nThe Avalon explosion (named from the Precambrian fauna of the Avalon Peninsula) is a proposed evolutionary event in the history of the Metazoa. It is the equivalent of the Cambrian explosion for the current phyla. The Avalon explosion happened about 33 million years earlier than the Cambrian explosion (about 575 million years ago).\n\nTrace fossils of these Avalon organisms have been found worldwide, and represent the earliest known complex multicellular organisms. The Avalon explosion is also called the Ediacara biota radiated event. The biota largely disappeared contemporaneously with the rapid increase in biodiversity known as the Cambrian explosion.\n\nThe Avalon explosion was proposed by Virginia Tech paleontologists through the analysis of the morphological space change in several Ediacaran assemblages. The discovery suggests that in the early evolution of animals, there may have been more than one explosive event. The original analysis has been the subject of dispute in the literature.\n\nThe Avalon explosion appears similar to the Cambrian explosion in the rapid increase in diversity of morphologies in a relatively small time frame, followed by diversification within the established body plans, a pattern similar to that observed in other evolutionary events.\n", "id": "36136905", "title": "Avalon explosion"}
{"url": "https://en.wikipedia.org/wiki?curid=2377491", "text": "Big History\n\nBig History is an emerging academic discipline which examines history from the Big Bang to the present. It examines long time frames using a multidisciplinary approach based on combining numerous disciplines from science and the humanities, and explores human existence in the context of this bigger picture. It integrates studies of the cosmos, Earth, life, and humanity using empirical evidence to explore cause-and-effect relations, and is taught at universities and secondary schools often using web-based interactive presentations. According to historian David Christian, who has been credited with coining the term \"Big History\", the intellectual movement is made of an \"unusual coalition of scholars\". Some historians have expressed skepticism towards \"scientific history\" and argue that the claims of Big History are unoriginal. Others support the scientific merit but point out that cosmology and natural history have been studied since the Renaissance, and that the new term, Big History, continues such work.\n\nBig History examines the past using numerous time scales, from the Big Bang to modernity, unlike conventional history courses which typically begin with the introduction of farming and civilization, or with the beginning of written records. It explores common themes and patterns. Courses generally do not focus on humans until more than halfway through, and, unlike conventional history courses, there is not much focus on kingdoms or civilizations or wars or national borders. If conventional history focuses on human civilization with humankind at the center, Big History focuses on the universe and shows how humankind fits within this framework and places human history in the wider context of the universe's history. Unlike conventional history, Big History tends to go rapidly through detailed historical eras such as the Renaissance or Ancient Egypt. It draws on the latest findings from biology, astronomy, geology, climatology, prehistory, archaeology, anthropology, evolutionary biology, chemistry, psychology, hydrology, geography, paleontology, ancient history, physics, economics, cosmology, natural history, and population and environmental studies as well as standard history. One teacher explained:\n\nBig History arose from a desire to go beyond the specialized and self-contained fields that emerged in the 20th century. It tries to grasp history as a whole, looking for common themes across multiple time scales in history. Conventional history typically begins with the invention of writing, and is limited to past events relating directly to the human race. Big Historians point out that this limits study to the past 5,000 years and neglects the much longer time when humans existed on Earth. Henry Kannberg sees Big History as being a product of the Information Age, a stage in history itself following speech, writing, and printing. Big History covers the formation of the universe, stars, and galaxies, and includes the beginning of life as well as the period of several hundred thousand years when humans were hunter-gatherers. It sees the transition to civilization as a gradual one, with many causes and effects, rather than an abrupt transformation from uncivilized static cavemen to dynamic civilized farmers. An account in \"The Boston Globe\" describes what it polemically asserts to be the conventional \"history\" view:\n\nBig History, in contrast to conventional history, has more of an interdisciplinary basis. Advocates sometimes view conventional history as \"microhistory\" or \"shallow history\", and note that three-quarters of historians specialize in understanding the last 250 years while ignoring the \"long march of human existence.\" However, one historian disputed that the discipline of history has overlooked the big view, and described the \"grand narrative\" of Big History as a \"cliché that gets thrown around a lot.\" One account suggested that conventional history had the \"sense of grinding the nuts into an ever finer powder.\" It emphasizes long-term trends and processes rather than history-making individuals or events. Historian Dipesh Chakrabarty of the University of Chicago suggested that Big History was less politicized than contemporary history because it enables people to \"take a step back.\" It uses more kinds of evidence than the standard historical written records, such as fossils, tools, household items, pictures, structures, ecological changes and genetic variations.\n\nCritics of Big History, including sociologist Frank Furedi, have deemed the discipline an \"anti-humanist turn of history.\" The Big History narrative has also been challenged for failing to engage with the methodology of the conventional history discipline. According to historian and educator Sam Wineburg of Stanford University, Big History eschews the interpretation of texts in favor of a purely scientific approach, thus becoming \"less history and more of a kind of evolutionary biology or quantum physics.\"\n\nProfessor David Christian argued that the recent past is only understandable in terms of the \"whole 14-billion-year span of time itself.\" Big History seeks to retell the \"human story\" in light of scientific advances by such methods as radiocarbon dating and genetic analysis. In some instances, it uses mathematical modeling to explore interactions between long-term trends in sociological systems, and it has led to the coining of the term cliodynamics by Peter Turchin of the University of Connecticut to describe how mathematical models might explain events such as the growth of empires, social discontent, and the collapse of nations. It explores the mix of individual action and social and environmental forces, according to one view. While conventional history might see an invention such as sharper spear points as being deliberately created by some smart humans, and then copied by other humans, a Big History perspective might see sharper spear points as accidental, and then natural evolutionary processes enabled their users to be better hunters, even if they did not understand why this was the case. It seeks to discover repeating patterns during the 13.8 billion years since the Big Bang. For example, one pattern is that \"chaos catalyzes creativity\", such as the asteroid impact wiping out the dinosaurs.\n\nBig History makes comparisons based on different time scales, or what David Christian calls \"the play of scales\", and notes similarities and differences between the human, geological, and cosmological scales. Christian believes such \"radical shifts in perspective\" will yield \"new insights into familiar historical problems, from the nature/nurture debate to environmental history to the fundamental nature of change itself.\" It shows how human existence has been changed by both human-made and natural factors: for example, according to natural processes which happened more than four billion years ago, iron emerged from the remains of an exploding star and, as a result, humans could use this hard metal to forge weapons for hunting and war. The discipline addresses such questions as \"How did we get here?,\" \"How do we decide what to believe?,\" \"How did Earth form?,\" and \"What is life?\" It offers a \"grand tour of all the major scientific paradigms.\" According to one view, it helps students to become scientifically literate quickly.\n\nCosmic evolution, the scientific study of universal change, is closely related to Big History (as are the allied subjects of the epic of evolution and astrobiology); some researchers regard cosmic evolution as broader than Big History since the latter mainly (and rightfully) examines the specific historical trek from Big Bang → Milky Way → Sun → Earth → humanity. Cosmic evolution, while fully addressing all complex systems (and not merely those that led to humans), which is also sometimes called cosmological history or universal history, has been taught and researched for decades, mostly by astronomers and astrophysicists. This Big-Bang-to-humankind scenario well preceded the subject that some historians began calling Big History in the 1990s. Cosmic evolution is an intellectual framework that offers a grand synthesis of the many varied changes in the assembly and composition of radiation, matter, and life throughout the history of the universe. While engaging the time-honored queries of who we are and whence we came, this interdisciplinary subject attempts to unify the sciences within the entirety of natural history—a single, inclusive scientific narrative of the origin and evolution of all material things over ~14 billion years, from the origin of the universe to the present day on Earth.\n\nThe roots of the idea of cosmic evolution extend back millennia. Ancient Greek philosophers of the fifth century BCE, most notably Heraclitus, are celebrated for their reasoned claims that all things change. Early modern speculation about cosmic evolution began more than a century ago, including the broad insights of Robert Chambers, Herbert Spencer, and Lawrence Henderson. Only in the mid-20th century was the cosmic-evolutionary scenario articulated as a research paradigm to include empirical studies of galaxies, stars, planets, and life—in short, an expansive agenda that combines physical, biological, and cultural evolution. Harlow Shapley widely articulated the idea of cosmic evolution (often calling it \"cosmography\") in public venues at mid-century, and NASA embraced it in the late 20th century as part of its more limited astrobiology program. Carl Sagan, Eric Chaisson, Hubert Reeves, Erich Jantsch, and Preston Cloud, among others, extensively championed cosmic evolution at roughly the same time around 1980. This extremely broad subject now continues to be richly formulated as both a technical research program and a scientific worldview for the 21st century.\n\nCosmic evolution can elicit controversy for several reasons: evolution of any kind inherently attracts detractors, especially among religious fundamentalists; cosmic evolution addresses universal and human origins, which often elevate emotions; it challenges age-old ideas about life's sense of place in the cosmos; it embraces change, which many people dislike or distrust; it welcomes a broad interpretation of the concept of evolution, replacing the idea of evolution exclusive to life, which some biologists prefer; it proposes a sweeping, interdisciplinary worldview based on rationality and empiricism, which, despite its experimental tests, some find intellectually arrogant.\n\nOne popular collection of scholarly materials on cosmic evolution is based on teaching and research that has been underway at Harvard University since the mid-1970s\n\nCosmic evolution is a quantitative subject, whereas big history typically is not; this is because cosmic evolution is practiced mostly by natural scientists, while big history by social scholars. These two subjects, closely allied and overlapping, benefit from each other; cosmic evolutionists tend to treat universal history linearly, thus humankind enters their story only at the most very recent times, whereas big historians tend to stress humanity and its many cultural achievements, granting human beings a larger part of their story. One can compare and contrast these different emphases by watching two short movies portraying the Big-Bang-to-humankind narrative, one animating time linearly, and the other capturing time (actually look-back time) logarithmically; in the former, humans enter this 14-minute movie in the last second, while in the latter we appear much earlier—yet both are correct.\n\nThese different treatments of time over ~14 billion years, each with different emphases on historical content, are further clarified by noting that some cosmic evolutionists divide the whole narrative into three phases and seven epochs:\nThis contrasts with the approach used by some big historians who divide the narrative into many more thresholds, as noted in the discussion at the end of this section below. Yet another telling of the Big-Bang-to-humankind story is one that emphasizes the earlier universe, particularly the growth of particles, galaxies, and large-scale cosmic structure, such as in physical cosmology.\n\nNotable among quantitative efforts to describe cosmic evolution are Eric Chaisson's research efforts to describe the concept of energy flow through open, thermodynamic systems, including galaxies, stars, planets, life, and society. The observed increase of energy rate density (energy/time/mass) among a whole host of complex systems is one useful way to explain the rise of complexity in an expanding universe that still obeys the cherished second law of thermodynamics and thus continues to accumulate net entropy. As such, ordered material systems—from buzzing bees and redwood trees to shining stars and thinking beings—are viewed as temporary, local islands of order in a vast, global sea of disorder. A recent review article, which is especially directed toward big historians, summarizes much of this empirical effort over the past decade.\n\nOne striking finding of such complexity studies is the apparently ranked order among all known material systems in the universe. Although the \"absolute\" energy in astronomical systems greatly exceeds that of humans, and although the mass densities of stars, planets, bodies, and brains are all comparable, the energy rate \"density\" for humans and modern human society are approximately a million times greater than for stars and galaxies. For example, the Sun emits a vast luminosity, 4x10 erg/s (equivalent to nearly a billion billion billion watt light bulb), but it also has a huge mass, 2x10 g; thus each second an amount of energy equaling only 2 ergs passes through each gram of this star. In contrast to any star, more energy flows through each gram of a plant's leaf during photosynthesis, and much more (nearly a million times) rushes through each gram of a human brain while thinking (~20W/1350g).\n\nCosmic evolution is more than a subjective, qualitative assertion of \"one damn thing after another\". This inclusive scientific worldview constitutes an objective, quantitative approach toward deciphering much of what comprises organized, material Nature. Its uniform, consistent philosophy of approach toward all complex systems demonstrates that the basic differences, both within and among many varied systems, are of degree, not of kind. And, in particular, it suggests that optimal ranges of energy rate density grant opportunities for the evolution of complexity; those systems able to adjust, adapt, or otherwise take advantage of such energy flows survive and prosper, while other systems adversely affected by too much or too little energy are non-randomly eliminated.\n\nFred Spier is foremost among those big historians who have found the concept of energy flows useful, suggesting that Big History is the rise and demise of complexity on all scales, from sub-microscopic particles to vast galaxy clusters, and not least many biological and cultural systems in between.\n\nDavid Christian, in an 18-minute TED talk, described some of the basics of the Big History course. Christian describes each stage in the progression towards greater complexity as a \"threshold moment\" when things become more complex, but they also become more fragile and mobile. Some of Christian's threshold stages are:\n\nChristian elaborated that more complex systems are more fragile, and that while collective learning is a powerful force to advance humanity in general, it is not clear that humans are in charge of it, and it is possible in his view for humans to destroy the biosphere with the powerful weapons that have been invented.\n\nIn the 2008 lecture series through \"The Teaching Company's Great Courses\" entitled \"Big History: The Big Bang, Life on Earth, and the Rise of Humanity\", Christian explains Big History in terms of eight thresholds of increasing complexity:\n\n\nA theme in Big History is what has been termed Goldilocks conditions or the Goldilocks principle, which describes how \"circumstances must be right for any type of complexity to form or continue to exist,\" as emphasized by Spier in his recent book. For humans, bodily temperatures can neither be too hot nor too cold; for life to form on a planet, it can neither have too much nor too little energy from sunlight. Stars require sufficient quantities of hydrogen, sufficiently packed together under tremendous gravity, to cause nuclear fusion.\n\nChristian suggests that the universe creates complexity when these Goldilocks conditions are met, that is, when things are not too hot or cold, not too fast or slow. For example, life began not in solids (molecules are stuck together, preventing the right kinds of associations) or gases (molecules move too fast to enable favorable associations) but in liquids such as water which permitted the right kinds of interactions at the right speeds.\n\nSomewhat in contrast, Chaisson has maintained for well more than a decade that \"organizational complexity is mostly governed by the \"optimum\" use of energy—not too little as to starve a system, yet not too much as to destroy it\" (italics in the original published paper). Neither maximum energy principles nor minimum entropy states are likely relevant, and appeals to \"Goldilocks principles\" (or other such fairy tales) are unnecessary to appreciate the emergence of complexity in Nature writ large.\n\nAdvances in particular sciences such as archaeology, gene mapping, and evolutionary ecology have enabled historians to gain new insights into the early origins of humans, despite the lack of written sources. One account suggested that proponents of Big History were trying to \"upend\" the conventional practice in historiography of relying on written records.\n\nBig History proponents suggest that humans have been affecting climate change throughout history, by such methods as slash-and-burn agriculture, although past modifications have been on a lesser scale than in recent years during the Industrial Revolution.\n\nA book by Daniel Lord Smail in 2008 suggested that history was a continuing process of humans learning to self-modify our mental states by using stimulants such as coffee and tobacco, as well as other means such as religious rites or romance novels. His view is that culture and biology are highly intertwined, such that cultural practices may cause human brains to be wired differently from those in different societies.\n\nBig History is more likely than conventional history to be taught with interactive \"video-heavy\" websites without textbooks, according to one account. The discipline has benefited from having new ways of presenting themes and concepts in new formats, often supplemented by Internet and computer technology. For example, the ChronoZoom project is a way to explore the 14 billion year history of the universe in an interactive website format. It was described in one account:\n\nIn 2012, the History channel showed the film \"History of the World in Two Hours\". It showed how dinosaurs effectively dominated mammals for 160 million years until an asteroid impact wiped them out. One report suggested the History channel had won a sponsorship from StanChart to develop a Big History program entitled \"Mankind\". In 2013 the History channel's new H2 network debuted the 10-part series \"Big History\", narrated by Bryan Cranston and featuring David Christian and an assortment of historians, scientists and related experts. Each episode centered on a major Big History topic such as salt, mountains, cold, flight, water, meteors and megastructures.\n\nWhile the emerging field of Big History in its present state is generally seen as having emerged in the past two decades beginning around 1990, there have been numerous precedents going back almost 150 years. In the mid-19th century, Alexander von Humboldt's book \"Kosmos\", and Robert Chambers' 1844 book \"Vestiges of the Natural History of Creation\" were seen as early precursors to the field. In a sense, Darwin's theory of evolution was, in itself, an attempt to explain a biological phenomenon by examining longer term cause-and-effect processes. In the first half of the 20th century, secular biologist Julian Huxley originated the term \"evolutionary humanism\", while around the same time the French Jesuit paleontologist Pierre Teilhard de Chardin examined links between cosmic evolution and a tendency towards complexification (including human consciousness), while envisaging compatibility between cosmology, evolution, and theology. In the mid and later 20th century, \"The Ascent of Man\" by Jacob Bronowski examined history from a multidisciplinary perspective. Later, Eric Chaisson explored the subject of cosmic evolution quantitatively in terms of energy rate density, and the astronomer Carl Sagan wrote \"Cosmos\". Thomas Berry, a cultural historian, and the academic Brian Swimme explored meaning behind myths and encouraged academics to explore themes beyond organized religion.\n\nThe field continued to evolve from interdisciplinary studies during the mid-20th century, stimulated in part by the Cold War and the Space Race. Some early efforts were courses in \"Cosmic Evolution\" at Harvard University in the United States, and \"Universal History\" in the Soviet Union. One account suggested that the notable Earthrise photo, taken during a lunar orbit by the spacecraft Apollo 8, which showed Earth as a small blue and white ball behind a stark and desolate lunar landscape, not only stimulated the environmental movement but also caused an upsurge of interdisciplinary interest. The French historian Fernand Braudel examined daily life with investigations of \"large-scale historical forces like geology and climate\". Physiologist Jared Diamond in his book \"Guns, Germs, and Steel\" examined the interplay between geography and human evolution; for example, he argued that the horizontal shape of the Eurasian continent enabled human civilizations to advance more quickly than the vertical north-south shape of the American continent, because it enabled greater competition and information-sharing among peoples of the relatively same climate.\n\nIn the 1970s, scholars in the United States including geologist Preston Cloud of the University of Minnesota, astronomer G. Siegfried Kutter at Evergreen State College in Washington state, and Harvard University astrophysicists George B. Field and Eric Chaisson started synthesizing knowledge to form a \"science-based history of everything\", although each of these scholars emphasized somewhat their own particular specializations in their courses and books. In 1980, the Austrian philosopher Erich Jantsch wrote \"The Self-Organizing Universe\" which viewed history in terms of what he called \"process structures\". There was an experimental course taught by John Mears at Southern Methodist University in Dallas, Texas, and more formal courses at the university level began to appear.\n\nIn 1991 Clive Ponting wrote \"A Green History of the World: The Environment and the Collapse of Great Civilizations\". His analysis did not begin with the Big Bang, but his chapter \"Foundations of History\" explored the influences of large-scale geological and astronomical forces over a broad time period.\n\nSometimes the terms \"Deep History\" and \"Big History\" are interchangeable, but sometimes \"Deep History\" simply refers to history going back several hundred thousand years or more without the other senses of being a movement within history itself.\n\nOne exponent is David Christian of Macquarie University in Sydney, Australia. He read widely in diverse fields in science, and believed that much was missing from the general study of history. His first university-level course was offered in 1989. He developed a college course beginning with the Big Bang to the present in which he collaborated with numerous colleagues from diverse fields in science and the humanities and the social sciences. This course eventually became a Teaching Company course entitled \"Big History: The Big Bang, Life on Earth, and the Rise of Humanity\", with 24 hours of lectures, which appeared in 2008.\n\nSince the 1990s, other universities began to offer similar courses. In 1994 at the University of Amsterdam and the Eindhoven University of Technology, college courses were offered. In 1996, Fred Spier wrote \"The Structure of Big History\". Spier looked at structured processes which he termed \"regimes\":\n\nChristian's course caught the attention of philanthropist Bill Gates, who discussed with him how to turn Big History into a high school-level course. Gates said about David Christian:\n\nBy 2002, a dozen college courses on Big History had sprung up around the world. Cynthia Stokes Brown initiated Big History at the Dominican University of California, and she wrote \"Big History: From the Big Bang to the Present.\" In 2010, Dominican University of California launched the world's first Big History program to be required of all first-year students, as part of the school's general education track. This program, directed by Mojgan Behmand, includes a one-semester survey of Big History, and an interdisciplinary second-semester course exploring the Big History metanarrative through the lens of a particular discipline or subject. A course description reads:\n\nThe Dominican faculty's approach is to synthesize the disparate threads of Big History thought, in order to teach the content, develop critical thinking and writing skills, and prepare students to wrestle with the philosophical implications of the Big History metanarrative. In 2015, University of California Press published \"Teaching Big History\", a comprehensive pedagogical guide for teaching Big History, edited by Richard B. Simon, Mojgan Behmand, and Thomas Burke, and written by the Dominican faculty.\nBarry Rodrigue, at the University of Southern Maine, established the first general education course and the first online version, which has drawn students from around the world. The University of Queensland in Australia offers an undergraduate course entitled \"Global History\", required for all history majors, which \"surveys how powerful forces and factors at work on large time-scales have shaped human history\". By 2011, 50 professors around the world have offered courses. In 2012, one report suggested that Big History was being practiced as a \"coherent form of research and teaching\" by hundreds of academics from different disciplines.\nThere are efforts to bring Big History to younger students. In 2008, Christian and his colleagues began developing a course for secondary school students. In 2011, a pilot high school course was taught to 3,000 kids in 50 high schools worldwide. In 2012, there were 87 schools, with 50 in the United States, teaching Big History, with the pilot program set to double in 2013 for students in the ninth and tenth grades, and even in one middle school. The subject is a STEM course at one high school.\n\nThere are initiatives to make Big History a required standard course for university students throughout the world. An education project founded by philanthropist Bill Gates from his personal funds was launched in Australia and the United States, to offer a free online version of the course to high school students.\n\nThe International Big History Association (IBHA) was founded at the Coldigioco Geological Observatory in Coldigioco, Marche, Italy, on 20 August 2010. Its headquarters is located at Grand Valley State University in Allendale, Michigan, United States. Its inaugural gathering in 2012 was described as \"big news\" in a report in \"The Huffington Post\".\n\nAcademics involved with the concept include:\n\n", "id": "2377491", "title": "Big History"}
{"url": "https://en.wikipedia.org/wiki?curid=3533357", "text": "Evolution (term)\n\nThe English noun evolution (from Latin \"ēvolūtiō\" \"unfolding, unrolling\") refers to any kind of accumulation of change, or gradual directional change. It is the 3,117th most commonly used word in English.\n\nWhile the term primarily refers to biological evolution, there are various types of chemical evolution and it is also found in economics, historical linguistics, and many other technical fields where systems develop or change gradually over time, e.g. stellar evolution, cultural evolution, the evolution of an idea, metaphysical evolution, spiritual evolution, etc.\n\nThe English term prior to the late 19th century was confined to referring to goal-directed, pre-programmed processes such as embryological development. A pre-programmed task, as in a military maneuver, using this definition, may be termed an \"evolution.\" \n\nThe term \"evolution\" (from its literal meaning of \"unfolding\" of something into its true or explicit form) carries a connotation of gradual improvement or directionality from a beginning to an end point. This contrasts with the more general development, which can indicate change in any direction, or revolution, which implies recurring, periodic change. The term biological devolution is coined as an antonym to \"evolution\", indicating such degeneration or decrease in quality or complexity.\n", "id": "3533357", "title": "Evolution (term)"}
{"url": "https://en.wikipedia.org/wiki?curid=2533237", "text": "Catalytic triad\n\nA catalytic triad is a set of three coordinated amino acids that can be found in the active site of some enzymes. Catalyitic triads are most commonly found in hydrolase and transferase enzymes (e.g. proteases, amidases, esterases, acylases, lipases and β-lactamases). An Acid-Base-Nucleophile triad is a common motif for generating a nucleophilic residue for covalent catalysis. The residues form a charge-relay network to polarise and activate the nucleophile, which attacks the substrate, forming a covalent intermediate which is then hydrolysed to release the product and regenerate free enzyme. The nucleophile is most commonly a serine or cysteine amino acid, but occasionally threonine or even selenocysteine. The 3D structure of the enzyme brings together the traid residues in a precise orientation, even though they may be far apart in the sequence (primary structure).\n\nAs well as divergent evolution of function (and even the triad's nucleophile), catalytic triads show some of the best examples of convergent evolution. Chemical constraints on catalysis have led to the same catalytic solution independently evolving in at least 23 separate superfamilies. Their mechanism of action is consequently one of the best studied in biochemistry.\n\nThe enzymes trypsin and chymotrypsin were first purified in the 1930s. A serine in each of trypsin and chymotrypsin was identified as the catalytic nucleophile (by diisopropyl fluorophosphate modification) in the 1950s. The structure of chymotrypsin was solved by X-ray crystallography in the 1960s, showing the orientation of the catalytic triad in the active site. Other proteases were sequenced and aligned to reveal a family of related proteases, now called the S1 family. Simultaneously, the structures of the evolutionarily unrelated papain and subtilisin proteases were found to contain analogous triads. The 'charge-relay' mechanism for the activation of the nucleophile by the other triad members was proposed in the late 1960s. As more protease structures were solved by X-ray crystallography in the 1970s and 80s, homologous (such as TEV protease) and analogous (such as papain) triads were found. The MEROPS classification system in the 1990s and 2000s began classing proteases into structurally related enzyme superfamilies and so acts as a database of the convergent evolution of triads in over 20 superfamilies. Understanding how chemical constraints on evolution led to the convergence of so many enzyme families on the same triad geometries has developed in the 2010s. Of particular contention during the 1990s and early 2000s was the contribution of low-barrier hydrogen bonding to catalysis; however, current thinking is that ordinary hydrogen bonding is sufficient to explain the mechanism. The massive body of work on the charge-relay, covalent catalysis used by catalytic triads has led to the mechanism being the best characterised in all of biochemistry.\n\nEnzymes that contain a catalytic triad use it for one of two reaction types: either to split a substrate (hydrolases) or to transfer one portion of a substrate over to a second substrate (transferases). Triads are an inter-dependent set of residues in the active site of an enzyme and act in concert with other residues (e.g. binding site and oxyanion hole) to achieve nucleophilic catalysis. These triad residues act together to make the nucleophile member highly reactive, generating a covalent intermediate with the substrate that is then resolved to complete catalysis. \n\nCatalytic triads perform covalent catalysis using a residue as a nucleophile. The reactivity of the nucleophilic residue is increased by the functional groups of the other triad members. The nucleophile is polarised and oriented by the base, which is itself bound and stabilised by the acid.\n\nCatalysis is performed in two stages. First, the activated nucleophile attacks the carbonyl carbon and forces the carbonyl oxygen to accept an electron, leading to a tetrahedral intermediate. The build-up of negative charge on this intermediate is typically stabilized by an oxyanion hole within the active site. The intermediate then collapses back to a carbonyl, ejecting the first half of the substrate, but leaving the second half still covalently bound to the enzyme as an acyl-enzyme intermediate. The ejection of this first leaving group is often aided by donation of a proton by the base.\n\nThe second stage of catalysis is the resolution of the acyl-enzyme intermediate by the attack of a second substrate. If this substrate is water then the result is hydrolysis; if it is an organic molecule then the result is transfer of that molecule onto the first substrate. Attack by this second substrate forms a new tetrahedral intermediate, which resolves by ejecting the enzyme's nucleophile, releasing the second product and regenerating free enzyme.\n\nThe side-chain of the nucleophilic residue performs covalent catalysis on the substrate. The lone pair of electrons present on the oxygen or sulphur attacks the electropositive carbonyl carbon. The 20 naturally occurring biological amino acids do not contain any sufficiently nucleophilic functional groups for many difficult catalytic reactions. Embedding the nucleophile in a triad increases its reactivity for efficient catalysis. The most commonly used nucleophiles are the hydroxyl (OH) of serine and the thiol/thiolate ion (SH/S) of cysteine. Alternatively, threonine proteases use the secondary hydroxyl of threonine, however due to steric hindrance of the side chain's extra methyl group such proteases use their \"N\"-terminal amide as the base, rather than a separate amino acid.\n\nUse of oxygen or sulphur as the nucleophilic atom causes minor differences in catalysis. Compared to oxygen, sulphur’s extra d orbital makes it larger (by 0.4 Å) and softer, allows it to form longer bonds (d and d by 1.3-fold), and gives it a lower p\"K\" (by 5 units). Serine is therefore more dependent than cysteine on optimal orientation of the acid-base triad members to reduce its p\"K\" in order to achieve concerted deprotonation with catalysis. The low p\"K\" of cysteine works to its disadvantage in the resolution of the first tetrahedral intermediate as unproductive reversal of the original nucleophilic attack is the more favourable breakdown product. The triad base is therefore preferentially oriented to protonate the leaving group amide to ensure that it is ejected to leave the enzyme sulphur covalently bound to the substrate N-terminus. Finally, resolution of the acyl-enzyme (to release the substrate C-terminus) requires serine to be re-protonated whereas cysteine can leave as S. Sterically, the sulphur of cysteine also forms longer bonds and has a bulkier van der Waals radius and if mutated to serine can be trapped in unproductive orientations in the active site.\n\nVery rarely, the selenium atom of the uncommon amino acid selenocysteine is used as a nucleophile. The deprotonated Se state is strongly favoured when in a catalytic triad.\n\nSince no natural amino acids are strongly nucleophilic, the base in a catalytic triad polarises and deprotonates the nucleophile to increase its reactivity. Additionally, it protonates the first product to aid leaving group departure.\n\nThe base is most commonly histidine since its p\"K\" allows for effective base catalysis, hydrogen bonding to the acid residue, and deprotonation of the nucleophile residue. β-lactamases such as TEM-1 use a lysine residue as the base. Because lysine's p\"K\" is so high (p\"K\"=11), a glutamate and several other residues act as the acid to stabilise its deprotonated state during the catalytic cycle. Threonine proteases use their \"N\"-terminal amide as the base, since steric crowding by the catalytic threonine's methyl prevents other residues from being close enough.\n\nThe acidic triad member forms a hydrogen bond with the basic residue. This aligns the basic residue by restricting its side-chain rotation, and polarises it by stabilising its positive charge. Two amino acids have acidic side chains at physiological pH (aspartate or glutamate) and so are the most commonly used for this triad member. Cytomegalovirus protease uses a pair of histidines, one as the base, as usual, and one as the acid. The second histidine is not as effective an acid as the more common aspartate or glutamate, leading to a lower catalytic efficiency. In some enzymes, the acid member of the triad is less necessary and some act only as a dyad. For example, papain uses asparagine as its third triad member which orients the histidine base but does not act as an acid. Similarly, hepatitis A virus protease contains an ordered water in the position where an acid residue should be.\n\nThe Serine-Histidine-Aspartate motif is one of the most thoroughly characterised catalytic motifs in biochemistry. The triad is exemplified by chymotrypsin, a model serine protease from the PA superfamily which uses its triad to hydrolyse protein backbones. The aspartate is hydrogen bonded to the histidine, increasing the p\"K\" of its imidazole nitrogen from 7 to around 12. This allows the histidine to act as a powerful general base and to activate the serine nucleophile. It also has an oxyanion hole consisting of several backbone amides which stabilises charge build-up on intermediates. The histidine base aids the first leaving group by donating a proton, and also activates the hydrolytic water substrate by abstracting a proton as the remaining OH attacks the acyl-enzyme intermediate.\n\nThe same triad has also convergently evolved in α/β hydrolases such as some lipases and esterases, however orientation of the triad members is reversed. Additionally, brain acetyl hydrolase (which has the same fold as a small G-protein) has also been found to have this triad. The equivalent Ser-His-\"Glu \"triad is used in acetylcholinesterase.\n\nThe second most studied triad is the Cysteine-Histidine-Aspartate motif. Several families of cysteine proteases use this triad set, for example TEV protease and papain. The triad acts similarly to serine protease triads, with a few notable differences. Due to cysteine's low p\"K\", the importance of the Asp to catalysis varies and several cysteine proteases are effectively Cys-His dyads (e.g. hepatitis A virus protease), whilst in others the cysteine is already deprotonated before catalysis begins (e.g. papain). This triad is also used by some amidases, such as \"N\"-glycanase to hydrolyse non-peptide C-N bonds.\n\nThe triad of cytomegalovirus protease uses histidine as both the acid and base triad members. Removing the acid histidine results in only a 10-fold activity loss (compared to >10,000-fold when aspartate is removed from chymotrypsin). This triad has been interpreted as a possible way of generating a less active enzyme to control cleavage rate.\n\nAn unusual triad is found in seldolisin proteases. The low p\"K\" of the glutamate carboxylate group means that it only acts as a base in the triad at very low pH. The triad is hypothesised to be an adaptation to specific environments like acidic hot springs (e.g. kumamolysin) or cell lysosome (e.g. tripeptidyl peptidase).\n\nThe endothelial protease vasohibin uses a cysteine as the nucleophile, but a serine to coordinate the histidine base. Despite the serine being a poor acid, it is still effective in orienting the histidine in the catalytic triad. Some homologues alternatively have a threonine instead of serine at the acid location.\n\nThreonine proteases, such as the proteasome protease subunit and ornithine acyltransferases use the secondary hydroxyl of threonine in a manner analogous to the use of the serine primary hydroxyl. However, due to the steric interference of the extra methyl group of threonine, the base member of the triad is the \"N\"-terminal amide which polarises an ordered water which, in turn, deprotonates the catalytic hydroxyl to increase its reactivity. Similarly, there exist equivalent 'serine only' and 'cysteine only' configurations such as penicillin acylase G and penicillin acylase V which are evolutionarily related to the proteasome proteases. Again, these use their \"N\"-terminal amide as a base.\n\nThis unusual triad occurs only in one superfamily of amidases. In this case, the lysine acts to polarise the middle serine. The middle serine then forms two strong hydrogen bonds to the nucleophilic serine to activate it (one with the side chain hydroxyl and the other with the backbone amide). The middle serine is held in an unusual \"cis\" orientation to facilitate precise contacts with the other two triad residues. The triad is further unusual in that the lysine and \"cis\"-serine both act as the base in activating the catalytic serine, but the same lysine also performs the role of the acid member as well as making key structural contacts.\n\nThe rare, but naturally occurring amino acid selenocysteine (Sec), can also be found as the nucleophile in some catalytic triads. Selenocysteine is similar to cysteine, but contains a selenium atom in stead of a sulphur. An example is in the active site of thioredoxin reductase, which does not use the selenium for nucleophilic attack, but in stead uses it for reduction of disulphide in thioredoxin.\n\nIn addition to naturally occurring types of catalytic triads, protein engineering has been used to create enzyme variants with non-native amino acids, or entirely synthetic amino acids. Catalytic triads have also been inserted into otherwise non-catalytic proteins, or protein mimics.\n\nSubtilisin (a serine protease) has had its the oxygen nucleophile replaced with each of sulphur, selenium, or tellurium. Cysteine and selenocysteine were inserted by mutagenesis, whereas the non-natural amino acid, tellurocysteine, was inserted using auxotrophic cells fed with synthetic tellurocysteine. These elements are all in the 16th periodic table column (chalcogens), so have similar properties. In each case, changing the nucleophile reduced the enzyme's protease activity, but increased a different activity. A sulphur nucleophile improved the enzymes transferase activity (sometimes called subtiligase). Selenium and tellurium nucleophiles converted the enzyme into a oxidoreductase. When the nucleophile of TEV protease was converted from cysteine to serine, it protease activity was strongly reduced, but was able to be restored by directed evolution.\n\nNon-catalytic proteins have been used as scaffolds, having catalytic triads inserted into them which were then improved by directed evolution. The Ser-His-Asp triad has been inserted into an antibody, as well as a range of other proteins. Similarly, catalytic triad mimics have been created in small organic molecules like diaryl diselenide, and displayed on larger polymers like Merrifield resins, and self-assembling short peptide nanostructures.\n\nThe sophistication of the active site network causes residues involved in catalysis (and residues in contact with these) to be highly evolutionarily conserved. However, there are examples of divergent evolution in catalytic triads, both in the reaction catalysed, and the residues used in catalysis. The triad remains the core of the active site, but it is evolutionarily adapted to serve different functions. Some proteins, called pseudoenzymes, have non-catalytic functions (e.g. regulation by inhibitory binding) and have accumulated mutations that inactivate their catalytic triad.\n\nCatalytic triads perform covalent catalysis via an acyl-enzyme intermediate. If this intermediate is resolved by water, the result is hydrolysis of the substrate. However, if the intermediate is resolved by attack by a second substrate, then the enzyme acts as a transferase. For example, attack by an acyl group results in an acyltransferase reaction. Several families of transferase enzymes have evolved from hydrolases by adaptation to exclude water and favour attack of a second substrate. In different members of the α/β-hydrolase superfamily, the Ser-His-Asp triad is tuned by surrounding residues to perform at least 17 different reactions. Some of these reactions are also achieved with mechanisms that have altered formation, or resolution of the acyl-enzyme intermediate, or that don't proceed via an acyl-enzyme intermediate.\n\nAdditionally, an alternative transferase mechanism has been evolved by amidophosphoribosyltransferases, which has two active sites. In the first active site, a cysteine triad hydrolyses a glutamine substrate to release free ammonia. The ammonia then diffuses though an internal tunnel in the enzyme to the second active site, where it is transferred to a second substrate.\n\nDivergent evolution of active site residues is slow, due to strong chemical constraints. Nevertheless, some protease superfamilies have evolved from one nucleophile to another. This can be inferred when a superfamily (with the same fold) contains families that use different nucleophiles. Such nucleophile switches have occurred several times during evolutionary history, however the mechanisms by which this happen are still unclear.\n\nWithin protease superfamilies that contain a mixture of nucleophiles (e.g. the PA clan), families are designated by their catalytic nucleophile (C=cysteine proteases, S=serine proteases).\nThe enzymology of proteases provides some of the clearest known examples of convergent evolution. The same geometric arrangement of triad residues occurs in over 20 separate enzyme superfamilies. Each of these superfamilies is the result of convergent evolution for the same triad arrangement within a different structural fold. This is because there are limited productive ways to arrange three triad residues, the enzyme backbone and the substrate. These examples reflect the intrinsic chemical and physical constraints on enzymes, leading evolution to repeatedly and independently converge on equivalent solutions.\n\nThe same triad geometries been converged upon by serine proteases such as the chymotrypsin and subtilisin superfamilies. Similar convergent evolution has occurred with cysteine proteases such as viral C3 protease and papain superfamilies. These triads have converged to almost the same arrangement due to the mechanistic similarities in cysteine and serine proteolysis mechanisms.\n\nFamilies of Cysteine proteases\nFamilies of Serine proteases\nThreonine proteases use the amino acid threonine as their catalytic nucleophile. Unlike cysteine and serine, threonine is a secondary hydroxyl (i.e. has a methyl group). This methyl group greatly restricts the possible orientations of triad and substrate as the methyl clashes with either the enzyme backbone or histidine base. When the nucleophile of a serine protease was mutated to threonine, the methyl occupied a mixture of positions, most of which prevented substrate binding. Consequently, the catalytic residue of a threonine protease is located at it \"N\"-terminus.\n\nTwo evolutionarily independent enzyme superfamilies with different protein folds are known to use the \"N\"-terminal residue as a nucleophile: Superfamily PB (proteasomes using the Ntn fold) and Superfamily PE (acetyltransferases using the DOM fold) This commonality of active site structure in completely different protein folds indicates that the active site evolved convergently in those superfamilies.\n\nFamilies of threonine proteases\n", "id": "2533237", "title": "Catalytic triad"}
{"url": "https://en.wikipedia.org/wiki?curid=40745870", "text": "Escape and radiate coevolution\n\nEscape and radiate coevolution is a multistep process that hypothesizes that an organism under constraints from other organisms will develop new defenses, allowing it to \"escape\" and then \"radiate\" into differing species. After a novel defense has been acquired, an organism is able to escape predation and rapidly multiply into new species because of relaxed selective pressure. There are many possible mechanisms available varying between different types of organisms, however they must be novel in order for escape to allow for radiation. This theory applies to predator-prey associations, but is most often applied to plant-herbivore associations.\n\nThis form of coevolution can be complex but is essential to understanding the vast biological diversity among organisms today. Out of the many forms of coevolution, escape and radiate is most likely responsible for providing the most diversity. This is due to the nature of the \"evolutionary arms race\" and the continuous cycle of counter adaptations. It is a relatively new field of study and is rapidly gaining credibility. To date, there has not been a formal study published specifically for escape and radiate coevolution.\n\nThis theory originated in a paper by Erlich and Raven, 1964, \"Butterflies and plants: a study in coevolution\". It outlined and laid the foundations of the concept. However, the term \"escape and radiate\" was not coined until Thompson's 1989 \"Concepts of Coevolution\". The theory has not yet been fully analyzed, however, as since its origins it has grown in importance among evolutionary biologists and botanists.\n\nIn order for an organism to \"escape\", and then radiate into varying species it needs a mechanism to escape. These defense mechanisms vary widely and differ for different types of organisms. Plants use chemical defenses in the form of secondary metabolites or allelochemicals. These allelochemicals inhibit the growth, behavior, and health of herbivores, allowing plants to escape. An example of a plant allelochemical are alkaloids that can inhibit protein synthesis in herbivores. Other forms of plant defense include mechanical defenses such as thigmonasty movements which have the plant leaves close in response to tactile stimulation. Indirect mechanisms plant include shedding of plant leaves so less leaves are available which deters herbivores, growth in locations in that are difficult to reach, and even mimicry. For organisms other than plants, examples of defense mechanisms allowing for escape include camouflage, aposematism, heightened senses and physical capabilities, and even defensive behaviors such as feigning death. An example of an organism using one of these defense mechanims is the granular poison frog which defends itself through aposematism. It is important to understand that in order for escape and radiate coevolution to occur, it is necessary that the developed defense is novel rather than previously established.\n\nInduced defense stemming from adaptive phenotypic plasticity may help a plant defend itself against multiple enemies. Phenotypic plasticity occurs when an organism undergoes an environmental change forcing a change altering its behavior, physiology, etc. These induced defenses allow for an organism to escape.\n\nRadiation is the evolutionary process of diversification of a single species into multiple forms. It includes the physiological and ecological diversity within a rapidly multiplying lineage. There are many types of radiation including adaptive, concordant, and discordant radiation however escape and radiate coevolution does not always follow those specific types.\n\nThis eventually leads to the question, why does escape allow for radiation? Once a novel defense has been acquired, the attacking organism which had evolved adaptations that allowed it to predate is now up against a new defense that it has not yet been evolved to encounter. This gives the defending organism the advantage, and therefore time to rapidly multiply unopposed by the previously attacking organism. This ultimately leads to the physiological and ecological diversity within the rapidly multiplying lineage, hence radiation.\n\nA full study analyzing the effects of escape and radiate coevolution has not yet been completed which hinders knowing how applicable this form of coevolution could be to other areas of study, or global concerns, only hypotheses of its effects can be made. Improved agriculture, conservation, biological diversity, and epidemiology are just some of the areas that could potentially be helped through the study of coevolution and its specific hypotheses such as escape and radiate coevolution.\nA theory as to why we see such vast biological diversity today may be because of escape and radiate coevolution. After the organism escapes, it then radiates into multiple species, and spreads geographically. Evidence of escape and radiate coevolution can be seen through the starburst effect in plant and herbivore clades. When analyzing clades of predator-prey associations, although it varies, the starburst effect is a good indicator that escape and radiate coevolution may be occurring. Eventually this cycle must come to an end because adaptations that entail costs (allocation of resources, vulnerability to other predators) that at some point outweigh their benefits.\nEscape and radiate coevolution may support parallel cladogenesis, wherein plant and herbivore phylogenies might match with ancestral insects exploiting ancestral plants. This is significant because it allows researchers to hypothesize about the relationships between ancestral organisms. Unfortunately, there have not yet been any known examples specifically involving escape and radiate coevolution being used for hypothesizing ancestral relationships.\n\nMany times the organism that has \"escaped\" continuously undergoes selective pressure because the predator it has escaped from evolves to create another adaptation in response, causing the process to continue. These \"offensive\" traits developed by predators range widely. For example, herbivores can develop an adaptation that allows for improved detoxification which allow to overcome plant defenses, thus causing escape and radiate coevolution to continue. Often the term \"evolutionary arms race\" is used to illustrate the idea that continuous evolution is needed to maintain the same relative fitness while the two species are coevolving. This idea also ties in with the Red Queen hypothesis. Counter adaptations among two organisms through escape and radiate coevolution is a major driving force behind diversity.\n\nEscape and radiate coevolution produces much more biological variation than other evolutionary mechanisms. For instance, cospeciation is important for diversity amongst species that share a symbiotic relationship, however this does not create nearly as much diversity in comparison to reciprocal evolutionary change due to natural selection.\nEvidence of rapid diversification following a novel adaptation is shown through the evolution of resin and latex canal tubes in 16 different lineages of plants. Plants with resin or latex canals can easily defend themselves against insect herbivores. When lineages of canal bearing plants are compared to the lineages of canal free plants, it is apparent that canal bearing plants are far more diverse, supporting escape and radiate coevolution.\n\nThe most popular examples of escape and radiate coevolution are of plant-herbivore associations. The most classic example is of butterflies and plants outlined in Ehrlich and Raven's original paper, \"Butterflies and plants: a study in coevolution.\". Erlich and Raven found in 1964 that hostplants for butterflies had a wide range of chemical defenses, allowing them to escape herbivory. Butterflies who developed novel counter detoxification mechanisms against the hostplants chemical defenses were able to utilize the hostplant resources. The process of stepwise adaptation and counteradaptation among the butterflies and hostplants is continuous and creates vast diversity.\n\nTropical trees may also escape and defend themselves. Trees growing in high light were predicted to have few chemical defenses, but rapid synchronous leaf expansion and low leaf nutritional quality during expansion. Species growing in low light have high levels of different chemical defenses, poor nutritional quality and asynchronous leaf expansion. Depending on the level of light the trees were growing in influenced the type of defenses they obtained, either chemical or through leaf expansion. The trees exposed to less light developed various chemicals to defend themselves against herbivores, a defense not utilizing light. This study was significant because it illustrates the separation between defenses and their relationship with an organism escaping and radiating into other species. Development of novel defenses does not necessarily imply that escape is possible for a species of plant if herbivores are adapting at a faster rate.\n\nMilkweed plants contain latex-filled canals which deter insect herbivores. Latex is toxic for small herbivores because it disrupts sodium and potassium levels. This has allowed for milkweeds to \"escape\" and become extremely diverse. There are over 100 different species of milkweeds which shows how diverse the plant is, with escape and radiate coevolution playing a very large role in creating such a high number of species.\n\nKey adaptations are adaptations that allow a group of organisms to diversify. \"Daphnia lumholtzi\" is a water flea that is able to form rigid head spines in response to chemicals released when fish are present. These phenotypically plastic traits serve as an induced defense against these predators. A study showed that \"Daphnia pulicaria\" is competitively superior to \"D. lumholtzi\" in the absence of predators. However, in the presence of fish predation the invasive species formed its defenses and became the dominant water flea in the region. This switch in dominance suggests that the induced defense against fish predation could represent a key adaptation for the invasion success of \"D. lumholtzi\". A defensive trait that qualifies as a key adaptation is most likely an example of escape and radiate coevolution.\n\nThe theory can be applied at the microscopic level such as to bacteria-phage relationships. Bacteria were able to diversify and escape through resistance to phages. The diversity among the hosts and parasites differed among the range of infection and resistance. The implication of this study to humans is its important to understanding the evolution of infectious organisms, and preventing diseases.\n", "id": "40745870", "title": "Escape and radiate coevolution"}
{"url": "https://en.wikipedia.org/wiki?curid=11241418", "text": "Orphan gene\n\nOrphan genes (also called ORFans, especially in microbial literature) are genes without detectable homologues in other lineages. Orphans are a subset of taxonomically-restricted genes (TRGs), which are unique to a specific taxonomic level (e.g. plant-specific). In contrast to non-orphan TRGs, orphans are usually considered unique to a very narrow taxon, generally a species.\n\nThe classic model of evolution is based on duplication, rearrangement, and mutation of genes with the idea of common descent. Orphan genes differ in that they are lineage-specific with no known history of shared duplication and rearrangement outside of their specific species or clade. Orphan genes may arise through a variety of mechanisms, such as horizontal gene transfer, duplication and rapid divergence, and de novo origination, and may act at different rates in insects, primates, and plants. Despite their relatively recent origin, orphan genes may encode functionally important proteins.\n\nOrphan genes were first discovered when the yeast genome-sequencing project began in 1996. Orphan genes accounted for an estimated 26% of the yeast genome, but it was believed that these genes could be classified with homologues when more genomes were sequenced. At the time, gene duplication was considered the only serious model of gene evolution and there were few sequenced genomes for comparison, so a lack of detectable homologues was thought to be most likely due to a lack of sequencing data and not due to a true lack of homology. However, orphan genes continued to persist as the quantity of sequenced genomes grew, eventually leading to the conclusion that orphan genes are ubiquitous to all genomes. Estimates of the percentage of genes which are orphans varies enormously between species and between studies; 10-30% is a commonly cited figure.\n\nThe study of orphan genes emerged largely after the turn of the century. In 2003, a study of \"Caenorhabditis briggsae\" and related species compared over 2000 genes. They proposed that these genes must be evolving too quickly to be detected and are consequently sites of very rapid evolution. In 2005, Wilson examined 122 bacterial species to try to examine whether the large number of orphan genes in many species was legitimate. The study found that it was legitimate and played a role in bacterial adaptation. The definition of taxonomically-restricted genes was introduced into the literature to make orphan genes seem less \"mysterious.\"\n\nIn 2008, a yeast protein of established functionality, BSC4, was found to have evolved de novo from non-coding sequences whose homology was still detectable in sister species.\n\nIn 2009, an orphan gene was discovered to regulate an internal biological network: the orphan gene, QQS, from \"Arabidopsis thaliana\" modifies plant composition. The QQS orphan protein interacts with a conserved transcription factor, these data explain the compositional changes (increased protein) that are induced when QQS is engineered into diverse species. In 2011, a comprehensive genome-wide study of the extent and evolutionary origins of orphan genes in plants was conducted in the model plant \"Arabidopsis thaliana\" \"\n\nGenes can be tentatively classified as orphans if no orthologous proteins can be found in nearby species.\n\nOne method used to estimate nucleotide or protein sequence similarity indicative of homology (i.e. similarity due to common origin) is the Basic Local Alignment Search Tool (BLAST). BLAST allows query sequences to be rapidly searched against large sequence databases. Simulations suggest that under certain conditions BLAST is suitable for detecting distant relatives of a gene. However, genes that are short and evolve rapidly can easily be missed by BLAST.\n\nThe systematic detection of homology to annotate orphan genes is called phylostratigraphy. Phylostratigraphy generates a phylogenetic tree in which the homology is calculated between all genes of a focal species and the genes of other species. The earliest common ancestor for a gene determines the age, or phylostratum, of the gene. The term \"orphan\" is sometimes used only for the youngest phylostratum containing only a single species, but when interpreted broadly as a taxonomically-restricted gene, it can refer to all but the oldest phylostratum, with the gene orphaned within a larger clade.\n\nOrphan genes arise from multiple sources, predominantly through de novo origination, duplication and rapid divergence, and horizontal gene transfer.\n\nNovel orphan genes continually arise de novo from non-coding sequences. These novel genes may be sufficiently beneficial to be swept to fixation by selection. Or, more likely, they will fade back into the non-genic background. This latter option is supported by research in Drosophila showing that young genes are more likely go extinct.\n\nDe novo genes were once thought to be a near impossibility due to the complex and potentially fragile intricacies of creating and maintaining functional polypeptides, but research from the past 10 years or so has found multiple examples of de novo genes, some of which are associated with important biological processes, particularly testes function. \n\nFor young orphan genes, it is sometimes possible to find homologous non-coding DNA sequences in sister taxa, which is generally accepted as strong evidence of de novo origin. However, the contribution of de novo origination to taxonomically-restricted genes of older origin, particularly in relation to the traditional gene duplication theory of gene evolution, remains contested.\n\nThe duplication and divergence model for orphan genes involves a new gene being created from some duplication or divergence event and undergoing a period of rapid evolution where all detectable similarity to the originally duplicated gene is lost. While this explanation is consistent with current understandings of duplication mechanisms, the number of mutations needed to lose detectable similarity is large enough as to be a rare event, and the evolutionary mechanism by which a gene duplicate could be sequestered and diverge so rapidly remains unclear.\n\nAnother explanation for how orphan genes arise is through a duplication mechanism called horizontal gene transfer, where the original duplicated gene derives from a separate, unknown lineage. This explanation for the origin of orphan genes is especially relevant in bacteria and archaea, where horizontal gene transfer is common.\n\nOrphans genes tend to be very short (~6 times shorter than mature genes), and some are weakly expressed, tissue specific and simpler in codon usage and amino acid composition. Orphan genes mostly encode intrinsically disordered proteins. \nOf the tens of thousands of enzymes of primary or specialized metabolism that have been characterized to date, none are orphans, or even of restricted lineage; apparently, catalysis requires hundreds of millions of years of evolution.\n\nWhile the prevalence of orphan genes has been established, the evolutionary role of orphans, and its resulting importance, is still being debated. One theory is that many orphans have no evolutionary role; genomes contain non-functional open reading frames (ORFs) that create spurious polypeptide products not maintained by selection, meaning that they are unlikely to be conserved between species and would likely be detected as orphan genes. However, a variety of other studies have shown that at least some orphans are functionally important and may help explain the emergence of novel phenotypes.\n", "id": "11241418", "title": "Orphan gene"}
{"url": "https://en.wikipedia.org/wiki?curid=268020", "text": "Evolutionary computation\n\nIn computer science, evolutionary computation is a family of algorithms for global optimization inspired by biological evolution, and the subfield of artificial intelligence and soft computing studying these algorithms. In technical terms, they are a family of population-based trial and error problem solvers with a metaheuristic or stochastic optimization character.\n\nIn evolutionary computation, an initial set of candidate solutions is generated and iteratively updated. Each new generation is produced by stochastically removing less desired solutions, and introducing small random changes. In biological terminology, a population of solutions is subjected to natural selection (or artificial selection) and mutation. As a result, the population will gradually evolve to increase in fitness, in this case the chosen fitness function of the algorithm.\n\nEvolutionary computation techniques can produce highly optimized solutions in a wide range of problem settings, making them popular in computer science. Many variants and extensions exist, suited to more specific families of problems and data structures. Evolutionary computation is also sometimes used in evolutionary biology as an \"in silico\" experimental procedure to study common aspects of general evolutionary processes.\n\nThe use of Evolutionary principles for automated problem solving originated in the 1950s. It was not until the 1960s that three distinct interpretations of this idea started to be developed in three different places.\n\nEvolutionary programming was introduced by Lawrence J. Fogel in the US, while John Henry Holland called his method a genetic algorithm. In Germany Ingo Rechenberg and Hans-Paul Schwefel introduced evolution strategies. These areas developed separately for about 15 years. From the early nineties on they are unified as different representatives (\"dialects\") of one technology, called evolutionary computing. Also in the early nineties, a fourth stream following the general ideas had emerged – genetic programming. Since the 1990s, nature-inspired algorithms are becoming an increasingly significant part of evolutionary computation.\n\nThese terminologies denote the field of evolutionary computing and consider evolutionary programming, evolution strategies, genetic algorithms, and genetic programming as sub-areas.\n\nSimulations of evolution using evolutionary algorithms and artificial life started with the work of Nils Aall Barricelli in the 1960s, and was extended by Alex Fraser, who published a series of papers on simulation of artificial selection. Artificial evolution became a widely recognised optimisation method as a result of the work of Ingo Rechenberg in the 1960s and early 1970s, who used evolution strategies to solve complex engineering problems. Genetic algorithms in particular became popular through the writing of John Holland. As academic interest grew, dramatic increases in the power of computers allowed practical applications, including the automatic evolution of computer programs. Evolutionary algorithms are now used to solve multi-dimensional problems more efficiently than software produced by human designers, and also to optimise the design of systems.\n\nEvolutionary computing techniques mostly involve metaheuristic optimization algorithms. Broadly speaking, the field includes:\n\n\nEvolutionary algorithms form a subset of evolutionary computation in that they generally only involve techniques implementing mechanisms inspired by biological evolution such as reproduction, mutation, recombination, natural selection and survival of the fittest. Candidate solutions to the optimization problem play the role of individuals in a population, and the cost function determines the environment within which the solutions \"live\" (see also fitness function). Evolution of the population then takes place after the repeated application of the above operators.\n\nIn this process, there are two main forces that form the basis of evolutionary systems: Recombination and mutation create the necessary diversity and thereby facilitate novelty, while selection acts as a force increasing quality.\n\nMany aspects of such an evolutionary process are stochastic. Changed pieces of information due to recombination and mutation are randomly chosen. On the other hand, selection operators can be either deterministic, or stochastic. In the latter case, individuals with a higher fitness have a higher chance to be selected than individuals with a lower fitness, but typically even the weak individuals have a chance to become a parent or to survive.\n\nThe list of active researchers is naturally dynamic and non-exhaustive. A network analysis of the community was published in 2007.\n\n\n\n", "id": "268020", "title": "Evolutionary computation"}
{"url": "https://en.wikipedia.org/wiki?curid=8746727", "text": "Level of support for evolution\n\nThe level of support for evolution among scientists, the public and other groups is a topic that frequently arises in the creation-evolution controversy and touches on educational, religious, philosophical, scientific and political issues. The subject is especially contentious in countries where significant levels of non-acceptance of evolution by general society exist although evolution is taught at school and university.\n\nThe overwhelming majority of the scientific community accepts evolution as the dominant scientific theory of biological diversity. Many scientific associations have rejected the challenges to evolution proposed by ID proponents.\n\nThere are religious sects and denominations in several countries for whom the theory of evolution is in conflict with creationism that is central to their dogma, and who therefore reject it: in the United States, South Africa, India, South Korea, Singapore, the Philippines, and Brazil, with smaller followings in the United Kingdom, the Republic of Ireland, the Netherlands, Japan, Italy, Germany, Israel, Australia, New Zealand, and Canada.\n\nSeveral publications discuss the subject of acceptance, including a document produced by the United States National Academy of Sciences.\n\nThe vast majority of the scientific community and academia supports evolutionary theory as the only explanation that can fully account for observations in the fields of biology, paleontology, molecular biology, genetics, anthropology, and others. One 1987 estimate found that \"700 scientists ... (out of a total of 480,000 U.S. earth and life scientists) ... give credence to creation-science\". A 1991 Gallup poll found that about 5% of American scientists (including those with training outside biology) identified themselves as creationists.\n\nAdditionally, the scientific community considers intelligent design, a neo-creationist offshoot, to be unscientific, pseudoscience, or junk science. The U.S. National Academy of Sciences has stated that intelligent design \"and other claims of supernatural intervention in the origin of life\" are not science because they cannot be tested by experiment, do not generate any predictions, and propose no new hypotheses of their own. In September 2005, 38 Nobel laureates issued a statement saying \"Intelligent design is fundamentally unscientific; it cannot be tested as scientific theory because its central conclusion is based on belief in the intervention of a supernatural agent.\" In October 2005, a coalition representing more than 70,000 Australian scientists and science teachers issued a statement saying \"intelligent design is not science\" and calling on \"all schools not to teach Intelligent Design (ID) as science, because it fails to qualify on every count as a scientific theory\".\n\nIn 1986, an \"amicus curiae\" brief, signed by 72 US Nobel Prize winners, 17 state academies of science and 7 other scientific societies, asked the US Supreme Court in \"Edwards v. Aguillard\", to reject a Louisiana state law requiring that where evolutionary science was taught in public schools, creation science must also be taught. The brief also stated that the term \"creation science\" as used that law embodied religious dogma, and that \"teaching religious ideas mislabeled as science is detrimental to scientific education\". This was the largest collection of Nobel Prize winners to sign anything up to that point. According to anthropologists Almquist and Cronin, the brief is the \"clearest statement by scientists in support of evolution yet produced.\"\n\nThere are many scientific and scholarly organizations from around the world that have issued statements in support of the theory of evolution. The American Association for the Advancement of Science, the world's largest general scientific society with more than 130,000 members and over 262 affiliated societies and academies of science including over 10 million individuals, has made several statements and issued several press releases in support of evolution. The prestigious United States National Academy of Sciences, which provides science advice to the nation, has published several books supporting evolution and criticising creationism and intelligent design.\n\nThere is a notable difference between the opinion of scientists and that of the general public in the United States. A 2009 poll by Pew Research Center found that \"Nearly all scientists (97%) say humans and other living things have evolved over time – 87% say evolution is due to natural processes, such as natural selection. The dominant position among scientists – that living things have evolved due to natural processes – is shared by only about a third (32%) of the public.\"\n\nOne of the earliest resolutions in support of evolution was issued by the American Association for the Advancement of Science in 1922, and readopted in 1929.\n\nAnother early effort to express support for evolution by scientists was organized by Nobel Prize–winning American biologist Hermann J. Muller in 1966. Muller circulated a petition entitled \"Is Biological Evolution a Principle of Nature that has been well established by Science?\" in May 1966:\n\nThis manifesto was signed by 177 of the leading American biologists, including George G. Simpson of Harvard University, Nobel Prize Winner Peter Agre of Duke University, Carl Sagan of Cornell, John Tyler Bonner of Princeton, Nobel Prize Winner George Beadle, President of the University of Chicago, and Donald F. Kennedy of Stanford University, formerly head of the United States Food and Drug Administration.\n\nThis was followed by the passing of a resolution by the American Association for the Advancement of Science (AAAS) in the fall of 1972 that stated, in part, \"the theory of creation ... is neither scientifically grounded nor capable of performing the rules required of science theories\". The United States National Academy of Sciences also passed a similar resolution in the fall of 1972. A statement on evolution called \"A Statement Affirming Evolution as a Principle of Science.\" was signed by Nobel Prize Winner Linus Pauling, Isaac Asimov, George G. Simpson, Caltech Biology Professor Norman H. Horowitz, Ernst Mayr, and others, and published in 1977. The governing board of the American Geological Institute issued a statement supporting resolution in November 1981.\nShortly thereafter, the AAAS passed another resolution supporting evolution and disparaging efforts to teach creationism in science classes.\n\nTo date, there are no scientifically peer-reviewed research articles that disclaim evolution listed in the scientific and medical journal search engine Pubmed.\n\nThe Discovery Institute announced that over 700 scientists had expressed support for intelligent design as of February 8, 2007. This prompted the National Center for Science Education to produce a \"light-hearted\" petition called \"Project Steve\" in support of evolution. Only scientists named \"Steve\" or some variation (such as Stephen, Stephanie, and Stefan) are eligible to sign the petition. It is intended to be a \"tongue-in-cheek parody\" of the lists of alleged \"scientists\" supposedly supporting creationist principles that creationist organizations produce. The petition demonstrates that there are more scientists who accept evolution with a name like \"Steve\" alone (over 1370) than there are in total who support intelligent design. This is, again, why the percentage of scientists who support evolution has been estimated by Brian Alters to be about 99.9 percent.\n\nMany creationists act as evangelists and their organizations are registered as tax-free religious organizations. Creationists have claimed that they represent the interests of true Christians, and evolution is only associated with atheism.\n\nHowever, not all religious organizations find support for evolution incompatible with their religious faith. For example, 12 of the plaintiffs opposing the teaching of creation science in the influential \"McLean v. Arkansas\" court case were clergy representing Methodist, Episcopal, African Methodist Episcopal, Catholic, Southern Baptist, Reform Jewish, and Presbyterian groups. There are several religious organizations that have issued statements advocating the teaching of evolution in public schools. In addition, the Archbishop of Canterbury, Dr. Rowan Williams, issued statements in support of evolution in 2006. The Clergy Letter Project is a signed statement by 12,808 (as of 28 May 2012) American Christian clergy of different denominations rejecting creationism organized in 2004. Molleen Matsumura of the National Center for Science Education found, of Americans in the twelve largest Christian denominations, at least 77% belong to churches that support evolution education (and that at one point, this figure was as high as 89.6%). These religious groups include the Catholic Church, as well as various denominations of Protestantism, including the United Methodist Church, National Baptist Convention, USA, Evangelical Lutheran Church in America, Presbyterian Church (USA), National Baptist Convention of America, African Methodist Episcopal Church, the Episcopal Church, and others. A figure closer to about 71% is presented by the analysis of Walter B. Murfin and David F. Beck.\n\nMichael Shermer argued in Scientific American in October 2006 that evolution supports concepts like family values, avoiding lies, fidelity, moral codes and the rule of law. Shermer also suggests that evolution gives more support to the notion of an omnipotent creator, rather than a tinkerer with limitations based on a human model.\n\nThe Ahmadiyya Movement universally accepts evolution and actively promotes it. Mirza Tahir Ahmad, Fourth Caliph of the Ahmadiyya Muslim Community has stated in his magnum opus \"Revelation, Rationality, Knowledge & Truth\" that evolution did occur but only through God being the One who brings it about. It does not occur itself, according to the Ahmadiyya Muslim Community. The Ahmadis do not believe Adam was the first human on earth, but merely the first prophet to receive a revelation of God.\n\nA fundamental part of `Abdul-Bahá's teachings on evolution is the belief that all life came from the same origin: \"the origin of all material life is one...\" He states that from this sole origin, the complete diversity of life was generated: \"Consider the world of created beings, how varied and diverse they are in species, yet with one sole origin\" He explains that a slow, gradual process led to the development of complex entities:\n\nThe 1950 encyclical \"Humani generis\" advocated scepticism towards evolution without explicitly rejecting it; this was substantially amended by Pope John-Paul II in 1996 in an address to the Pontifical Academy of Sciences in which he said, \"Today, almost half a century after publication of the encyclical, new knowledge has led to the recognition of the theory of evolution as more than a hypothesis.\" Between 2000 and 2002 the International Theological Commission found that \"Converging evidence from many studies in the physical and biological sciences furnishes mounting support for some theory of evolution to account for the development and diversification of life on earth, while controversy continues over the pace and mechanisms of evolution.\" This statement was published by the Vatican on July 2004 by the authority of Cardinal Ratzinger (who became Pope Benedict XVI) who was the president of the Commission at the time.\n\nThe Magisterium has not made an authoritative statement on intelligent design, and has permitted arguments on both sides of the issue. In 2005, Cardinal Christoph Schönborn of Vienna appeared to endorse intelligent design when he denounced philosophically materialist interpretations of evolution. In an op-ed in the New York Times he said \"Evolution in the sense of common ancestry might be true, but evolution in the neo-Darwinian sense - an unguided, unplanned process of random variation and natural selection - is not.\" This common line of reasoning among theologians is flawed, as evolution by natural selection is not random at all; only mutations occur in a stochastic manner, while natural selection establishes genes which aid survival in a particular environment.\n\nIn the January 16–17 2006 edition of the official Vatican newspaper \"L'Osservatore Romano\", University of Bologna evolutionary biology Professor Fiorenzo Facchini wrote an article agreeing with the judge's ruling in \"Kitzmiller v. Dover\" and stating that intelligent design was unscientific. Jesuit Father George Coyne, former director of the Vatican Observatory, has also denounced intelligent design.\n\nHindus believe in the concept of evolution of life on Earth. The concepts of Dashavatara—different incarnations of God starting from simple organisms and progressively becoming complex beings—and Day and Night of Brahma are generally cited as instances of Hindu acceptance of evolution.\n\nIn the United States, many Protestant denominations promote creationism, preach against evolution, and sponsor lectures and debates on the subject. Denominations that explicitly advocate creationism instead of evolution or \"Darwinism\" include the Assemblies of God, the Free Methodist Church, Lutheran Church–Missouri Synod, Pentecostal Churches, Seventh-day Adventist Churches, Wisconsin Evangelical Lutheran Synod, Christian Reformed Church, Southern Baptist Convention, and the Pentecostal Oneness churches. Jehovah's Witnesses produce Day-age creationism literature to refute evolution but reject the \"creationist\" label, which they consider to only apply to Young Earth creationism.\n\nA common complaint of creationists is that evolution is of no value, has never been used for anything, and will never be of any use. According to many creationists, nothing would be lost by getting rid of evolution, and science and industry might even benefit.\n\nIn fact, evolution is being put to practical use in industry and widely used on a daily basis by researchers in medicine, biochemistry, molecular biology, and genetics to both formulate hypotheses about biological systems for the purposes of experimental design, as well as to rationalise observed data and prepare applications. As of August 2017 there are 487,558 scientific papers in PubMed that mention 'evolution'. Pharmaceutical companies utilize biological evolution in their development of new products, and also use these medicines to combat evolving bacteria and viruses.\n\nBecause of the perceived value of evolution in applications, there have been some expressions of support for evolution on the part of corporations. In Kansas, there has been some widespread concern in the corporate and academic communities that a move to weaken the teaching of evolution in schools will hurt the state's ability to recruit the best talent, particularly in the biotech industry. Paul Hanle of the Biotechnology Institute warned that the United States risks falling behind in the biotechnology race with other nations if it does not do a better job of teaching evolution. James McCarter of Divergence Incorporated stated that the work of 2001 Nobel Prize winner Leland Hartwell relied heavily on the use of evolutionary knowledge and predictions, both of which have significant implications for the treatment of cancers. Furthermore, McCarter concluded that 47 of the last 50 Nobel Prizes in medicine or physiology depended on an understanding of evolutionary theory (according to McCarter's unspecified personal criteria).\n\nThere are also many educational organizations that have issued statements in support of the theory of evolution.\n\nRepeatedly, creationists and intelligent design advocates have lost suits in US courts. Here is a list of important court cases in which creationists have suffered setbacks:\n\n\nThere does not appear to be significant correlation between believing in evolution and understanding evolutionary science. In some countries, creationist beliefs (or a lack of support for evolutionary theory) are relatively widespread, even garnering a majority of public opinion. A study published in \"Science\" compared attitudes about evolution in the United States, 32 European countries (including Turkey) and Japan. The only country where acceptance of evolution was lower than in the United States was Turkey (25%). Public acceptance of evolution was most widespread (at over 80% of the population) in Iceland, Denmark and Sweden.\n\nAccording to a 2014 poll produced by the Pew Research Center, 71% of people in Argentina believe \"humans and other living things evolved over time\" while 23% believe they have \"always existed in the present form.\"\n\nA 2009 poll showed that almost a quarter of Australians believe \"the biblical account of human origins\" over the Darwinian account. 42 percent of Australians believe in a \"wholly scientific\" explanation for the origins of life, while 32 percent believe in an evolutionary process \"guided by God\".\n\nA 2010 survey conducted by Auspoll and the Australian Academy of Science found that 79% of Australians believe in evolution (71% believe it is currently occurring, 8% believe in evolution but do not think it is currently occurring), 11% were not sure and 10% stated they do not believe in evolution.\n\nAccording to a 2014 poll by the Pew Research Center, 44% of people in Bolivia believe \"humans and other living things evolved over time\" while 39% believe they have \"always existed in the present form.\"\n\nIn a 2010 poll, 59% of respondents said they believe in theistic evolution, or evolution guided by God. A further 8% believe in evolution without divine intervention, while 25% were creationists. Support for creationism was stronger among the poor and the least educated. According to a 2014 poll produced by the Pew Research Center, 66% of Brazilians agree that humans evolved over time and 29% think they have always existed in the present form.\n\nIn a 2012 poll, 61% of Canadians believe that humans evolved from less advanced life forms, while 22% believe that God created human beings in their present form within the last 10,000 years.\n\nAccording to a 2014 poll by the Pew Research Center, 69% of people in Chile believe \"humans and other living things evolved over time\" while 26% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 59% of people in Colombia believe \"humans and other living things evolved over time\" while 35% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 56% of people in Costa Rica believe \"humans and other living things evolved over time\" while 38% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 41% of people in Dominican Republic believe \"humans and other living things evolved over time\" while 56% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 50% of people in Ecuador believe \"humans and other living things evolved over time\" while 44% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 46% of people in El Salvador believe \"humans and other living things evolved over time\" while 45% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 55% of people in Guatemala believe \"humans and other living things evolved over time\" while 38% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 49% of people in Honduras believe \"humans and other living things evolved over time\" while 45% believe they have \"always existed in the present form.\"\n\nAmong those who had heard of Charles Darwin and knew something about the theory of evolution, 77% of people in India agree that enough scientific evidence exists to support Charles Darwin’s Theory of Evolution. Also, 85% of God believing Indians who know about evolution agree that life on earth evolved over time as a result of natural selection.\n\nIn a survey carried among 10 major nations, the highest proportion that agreed that evolutionary theories alone should be taught in schools was in India, at 49%.\n\nA 2009 survey conducted by the McGill researchers and their international collaborators found that 85% of Indonesian high school students agreed with the statement, \"Millions of fossils show that life has existed for billions of years and changed over time.\"\n\nAccording to a 2014 poll by the Pew Research Center, 64% of people in Mexico believe \"humans and other living things evolved over time\" while 32% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 47% of people in Nicaragua believe \"humans and other living things evolved over time\" while 48% believe they have \"always existed in the present form.\"\n\nAccording to a 2008 Norstat poll for NRK, 59% of the Norwegian population fully accept evolution, 24% somewhat agree with the theory, 4% somewhat disagree with the theory while 8% do not accept evolution. 4% did not know.\n\nA 2009 survey conducted by the McGill researchers and their international collaborators found that 86% of Pakistani high school students agreed with the statement, \"Millions of fossils show that life has existed for billions of years and changed over time.\"\n\nAccording to a 2014 poll by the Pew Research Center, 61% of people in Panama believe \"humans and other living things evolved over time\" while 34% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 59% of people in Paraguay believe \"humans and other living things evolved over time\" while 30% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 51% of people in Peru believe \"humans and other living things evolved over time\" while 39% believe they have \"always existed in the present form.\"\n\nA 2006 UK poll on the \"origin and development of life\" asked participants to choose between three different explanations for the origin of life: 22% chose (Young Earth) creationism, 17% opted for intelligent design (\"certain features of living things are best explained by the intervention of a supernatural being, e.g. God\"), 48% selected evolution theory (with a divine role explicitly excluded) and the rest did not know. A 2009 poll found that only 38% of Britons believe God played no role in evolution. In a 2012 poll, 69% of Britons believe that humans evolved from less advanced life forms, while 17% believe that God created human beings in their present forms within the last 10,000 years.\n\nUS courts have ruled in favor of teaching evolution in science classrooms, and against teaching creationism, in numerous cases such as Edwards v. Aguillard, Hendren v. Campbell, McLean v. Arkansas and Kitzmiller v. Dover Area School District.\n\nA prominent organization in the United States behind the intelligent design movement is the Discovery Institute, which, through its Center for Science and Culture, conducts a number of public relations and lobbying campaigns aimed at influencing the public and policy makers in order to advance its position in academia. The Discovery Institute claims that because there is a significant lack of public support for evolution, that public schools should, as their campaign states, \"Teach the Controversy\", although there is no controversy over the validity of evolution within the scientific community.\n\nThe US has one of the highest levels of public belief in biblical or other religious accounts of the origins of life on earth among industrialized countries.\n\nA 2017 Gallup creationism survey found that 38% of adults in the United States inclined to the view that \"God created humans in their present form at one time within the last 10,000 years\" when asked for their views on the origin and development of human beings, which was noted as being at the lowest level in 35 years. 19% believed that \"human beings have developed over millions of years from less advanced forms of life, but God had no part in this process\", despite 49% of respondents indicating they believed in evolution. Belief in creationism is inversely correlated to education; only 22% of those with post-graduate degrees believe in strict creationism. A 2000 poll for People for the American Way found 70% of the American public felt that evolution was compatible with a belief in God.\n\nA 2005 Pew Research Center poll found that 70% of evangelical Christians believed that living organisms have not changed since their creation, but only 31% of Catholics and 32% of mainline Protestants shared this opinion. A 2005 Harris Poll estimated that 63% of liberals and 37% of conservatives agreed that humans and other primates have a common ancestry.\n\nAccording to a 2014 poll produced by the Pew Research Center, 74% of people in Uruguay believe \"humans and other living things evolved over time\" while 20% believe they have \"always existed in the present form.\"\n\nAccording to a 2014 poll by the Pew Research Center, 63% of people in Venezuela believe \"humans and other living things evolved over time\" while 33% believe they have \"always existed in the present form.\"\n\nThe level of assent that evolution garners has changed with time. The trends in acceptance of evolution can be estimated.\n\nThe level of support for evolution in different communities has varied with time. Darwin's theory had convinced almost every naturalist within 20 years of its publication in 1858, and was making serious inroads with the public and the more liberal clergy. It had reached such extremes, that by 1880, one\nAmerican religious weekly publication estimated that \"perhaps a quarter, perhaps a half of the educated ministers in our leading Evangelical denominations\" felt \"that the story of the creation and fall of man, told in Genesis, is no more the record of actual occurrences than is the parable of the Prodigal Son.\"\n\nBy the late 19th century, many of the most conservative Christians accepted an ancient earth, and life on earth before Eden. Victorian Era Creationists were more akin to people who subscribe to theistic evolution today. Even fervent anti-evolutionist Scopes Trial prosecutor William Jennings Bryan interpreted the \"days\" of Genesis as ages of the earth, and acknowledged that biochemical evolution took place, drawing the line only at the story of Adam and Eve's creation. Prominent pre-World War II creationist Harry Rimmer allowed an Old Earth by slipping millions of years into putative gaps in the Genesis account, and claimed that the Noachian Flood was only a local phenomenon.\n\nIn the decades of the 20th century, George McCready Price and a tiny group of Seventh-day Adventist followers were the among the very few believers in a Young Earth and a worldwide flood, which Price championed in his \"new catastrophism\" theories. It was not until the publication of John C. Whitcomb, Jr., and Henry M. Morris’s book \"Genesis Flood\" in 1961 that Price's idea was revived. In the last few decades, many creationists have adopted Price's beliefs, becoming progressively more strict biblical literalists.\n\nIn a 1991 Gallup poll, 47% of the US population, and 25% of college graduates agreed with the statement, \"God created man pretty much in his present form at one time within the last 10,000 years.\"\n\nFourteen years\nlater, in 2005, Gallup found that 53% of Americans expressed the belief that \"God created human beings in their present form exactly the way the Bible describes it.\" About 2/3 (65.5%) of those surveyed thought that creationism was definitely or probably true. In 2005 a Newsweek poll discovered that 80 percent of the American public thought that \"God created the universe.\" and the Pew Research Center reported that \"nearly two-thirds of Americans say that creationism should be taught alongside evolution in public schools.\" Ronald Numbers commented on that with \"Most surprising of all was the discovery that large numbers of high-school biology teachers — from 30% in Illinois and 38% in Ohio to a whopping 69% in Kentucky — supported the teaching of creationism.\"\n\nThe National Center for Science Education reports that from 1985 to 2005, the number of Americans unsure about evolution increased from 7% to 21%, while the number rejecting evolution declined from 48% to 39%. Jon Miller of Michigan State University has found in his polls that the number of Americans who accept evolution has declined from 45% to 40% from 1985 to 2005.\n\nIn light of these somewhat contradictory results, it is difficult to know for sure what is happening to public opinion on evolution in the US. It does not appear that either side is making unequivocal progress. It does appear that uncertainty about the issue is increasing, however.\n\nAnecdotal evidence is that creationism is becoming more of an issue in the UK as well. One report in 2006 was that UK students are increasingly arriving ill-prepared to participate in medical studies or other advanced education.\n\nThe level of support for creationism among relevant scientists is minimal. Only 700 out of 480,000 U.S. earth and life scientists gave credence to creationism in 1987, representing about 0.146% of relevant scientists. In 2007 the Discovery Institute reported that about 600 scientists signed their \"A Scientific Dissent from Darwinism\" list, up from 100 in 2001. The actual statement of the Scientific Dissent from Darwinism is a relatively mild one that expresses skepticism about the absoluteness of 'Darwinism' (and is in line with the falsifiability required of scientific theories) to explain all features of life, and does not in any way represent an absolute denial or rejection of evolution. By contrast, a tongue-in-cheek response known as Project Steve, a list of scientists named Steve who agree that evolution is \"a vital, well-supported, unifying principle of the biological sciences,\" has 1,382 signatories . People named Steve make up approximately 1% of the total U.S. population.\n\nThe United States National Science Foundation statistics on US yearly science graduates demonstrate that from 1987 to 2001, the number of biological science graduates increased by 59% while the number of geological science graduates decreased by 20.5%. However, the number of geology graduates in 2001 was only 5.4% of the number of graduates in the biological sciences, while it was 10.7% of the number of biological science graduates in 1987. The Science Resources Statistics Division of the National Science Foundation estimated that in 1999, there were 955,300 biological scientists in the US (about 1/3 of who hold graduate degrees). There were also 152,800 earth scientists in the US as well.\n\nA large fraction of the Darwin Dissenters have specialties unrelated to research on evolution; of the dissenters, three-quarters are not biologists. As of 2006, the dissenter list was expanded to include non-US scientists.\n\nSome researchers are attempting to understand the factors that affect people's acceptance of evolution. Studies have yielded inconsistent results, explains associate professor of education at Ohio State University, David Haury. He recently performed a study that found people are likely to reject evolution if they have feelings of uncertainty, regardless of how well they understand evolutionary theory. Haury believes that teachers need to show students that their intuitive feelings may be misleading (for example, using the Wason selection task), and thus to exercise caution when relying on them as they judge the rational merits of ideas.\n\n\n", "id": "8746727", "title": "Level of support for evolution"}
{"url": "https://en.wikipedia.org/wiki?curid=44862806", "text": "Outline of evolution\n\nThe following outline is provided as an overview of and topical guide to evolution:\n\nEvolution – change in heritable traits of biological organisms over generations due to natural selection, mutation, gene flow, and genetic drift. Also known as descent with modification. Over time these evolutionary processes lead to formation of new species (speciation), changes within lineages (anagenesis), and loss of species (extinction). \"Evolution\" is also another name for evolutionary biology, the subfield of biology concerned with studying evolutionary processes that produced the diversity of life on Earth.\n\n\n\n\n\n\"See also Basic principles (above)\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "id": "44862806", "title": "Outline of evolution"}
{"url": "https://en.wikipedia.org/wiki?curid=22433196", "text": "Evolution of emotion\n\nThe study of the evolution of emotions dates back to the 19th century. Evolution and natural selection has been applied to the study of human communication, mainly by Charles Darwin in his 1872 work, \"The Expression of the Emotions in Man and Animals\". Darwin researched the expression of emotions in an effort to support his theory of evolution. He proposed that much like other traits found in animals, emotions also evolved and were adapted over time. His work looked at not only facial expressions in animals and specifically humans, but attempted to point out parallels between behaviors in humans and other animals.\n\nAccording to modern evolutionary theory, different emotions evolved at different times. Primal emotions, such as fear, are associated with ancient parts of the brain and presumably evolved among our premammal ancestors. Filial emotions, such as a human mother's love for her offspring, seem to have evolved among early mammals. Social emotions, such as guilt and pride, evolved among social primates. Sometimes, a more recently evolved part of the brain moderates an older part of the brain, such as when the cortex moderates the amygdala's fear response. Evolutionary psychologists consider human emotions to be best adapted to the life our ancestors led in nomadic foraging bands.\n\nDarwin's original plan was to include his findings about expression of emotions in a chapter of his work, \"The Descent of Man, and Selection in Relation to Sex\" (Darwin, 1871) but found that he had enough material for a whole book. It was based on observations, both those around him and of people in many parts of the world. One important observation he made was that even in individuals who were born blind, body and facial expressions displayed are similar to those of anyone else. The ideas found in his book on universality of emotions were intended to go against Sir Charles Bell's 1844 claim that human facial muscles were created to give them the unique ability to express emotions. The main purpose of Darwin's work was to support the theory of evolution by demonstrating that emotions in humans and other animals are similar. Most of the similarities he found were between species closely related, but he found some similarities between distantly related species as well. He proposed the idea that emotional states are adaptive, and therefore only those able to express certain emotions passed on their characteristics.\n\nIn the 1872 work, Darwin proposed three principles. The first of the three is the \"principle of serviceable habits,\" which he defined as useful habits reinforced previously, and then inherited by offspring. He used as an example contracting of eyebrows (furrowing the brow), which he noted is serviceable to prevent too much light from entering the eyes. He also said that the raising of eyebrows serves to increase the field of vision. He cited examples of people attempting to remember something and raising their brows, as though they could \"see\" what they were trying to remember.\n\nThe second of the principles is that of antithesis. While some habits are serviceable, Darwin proposed that some actions or habits are carried out merely because they are opposite in nature to a serviceable habit, but are not serviceable themselves. Shrugging of the shoulders is an example Darwin used of antithesis, because it has no service. Shoulder shrugging is a passive expression, and very opposite of a confident or aggressive expression.\n\nThe third of the principles is expressive habits, or nervous discharge from the nervous system. This principle proposes that some habits are performed because of a build-up to the nervous system, which causes a discharge of the excitement. Examples include foot and finger tapping, as well as vocal expressions and expressions of anger. Darwin noted that many animals rarely make noises, even when in pain, but under extreme circumstances they vocalize in response to pain and fear.\n\nPaul Ekman is most noted in this field for conducting research involving facial expressions of emotions. His work provided data to back up Darwin's ideas about universality of facial expressions, even across cultures. He conducted research by showing photographs exhibiting expressions of basic emotion to people and asking them to identify what emotion was being expressed. In 1971, Ekman and Wallace Friesen presented to people in a preliterate culture a story involving a certain emotion, along with photographs of specific facial expressions. The photographs had been previously used in studies using subjects from Western cultures. When asked to choose, from two or three photographs, the emotion being expressed in the story, the preliterate subjects' choices matched those of the Western subjects most of the time. These results indicated that certain expressions are universally associated with particular emotions, even in instances in which the people had little or no exposure to Western culture. The only emotions the preliterate people found hard to distinguish between were fear and surprise.\nEkman noted that while universal expressions do not necessarily prove Darwin's theory that they evolved, they do provide strong evidence of the possibility. He mentioned the similarities between human expressions and those of other primates, as well as an overall universality of certain expressions to back up Darwin's ideas. The expressions of emotion that Ekman noted as most universal based on research are: anger, fear, disgust, sadness, and enjoyment.\n\nA common view is that facial expressions initially served a non-communicative adaptive function. Thus, the widened eyes in the facial expression of fear have been shown to increase the visual field and the speed of moving the eyes which helps finding and following threats. The wrinkled nose and mouth of the facial expression of disgust limit the intake of foul-smelling and possibly dangerous air and particles. Later, such reactions, which could be observed by other members of the group, increasingly become more distinctive and exaggerated in order to fulfill a primarily socially communicative function. This communicative function can dramatically or subtly influence the behavior of other members in the group. Thus, rhesus monkeys or human infants can learn to fear potential dangers based on only the facial expressions of fear of other group members or parents. Seeing fear expressions increases the tendency for flight responses while seeing anger expressions increases the tendency for fight responses. Classical conditioning studies have found that it is easier to create a pairing between a negative stimulant and anger/fear expressions than between a negative stimulant and a happiness expression. Cross-cultural studies and studies on the congenitally blind have found that these groups display the same expressions of shame and pride in situations related to social status. These expressions have clear similarities to displays of submission and dominance by other primates. Humans viewing expression of pride automatically assign a higher social status to such individuals than to those expressing other emotions.\n\nRobert Zajonc, a University of Michigan psychologist, published two reviews in 1989 of the \"facial efference theory of emotion\", also known as facial feedback theory, which he had first introduced to the scientific literature in an article published in \"Science\" in 1985. This theory proposes that the facial musculature of mammals can control the temperature of the base of the brain (in particular the hypothalamus) by varying the degree of forward and backward flow through a vascular network (a so-called \"rete mirabile\"). The theory is based on the idea that increasing the temperature of portions of the hypothalamus can produce aggressive behavior, whereas cooling can produce relaxation. Our emotional language has comparable descriptors, such as \"hot-head\" and \"cool-breezy\". The theory offers an explanation for the evolution of common facial expressions of emotion in mammals. Little experimental work has been done to extend the theory, however.\n\nCarroll Izard, a psychologist who is known for his work with emotions, discussed gains and losses associated with the evolution of emotions. He said that discrete emotion experiences emerge in ontogeny before language or conceptual structures that frame the qualia known as discrete emotion feelings are acquired. He noted that in evolution, when humans gained the capability of expressing themselves with language, which contributed greatly to emotional evolution. Not only can humans articulate and share their emotions, they can use their experiences to foresee and take appropriate action in future experiences. He did, however, raise the question of whether or not humans have lost some of their empathy for one another, citing things such as murder and crime against one another as destructive.\n\nJoseph LeDoux focuses much of his research on the emotion fear. Fear can be evoked by two systems in the brain, both involving the thalamus and the amygdala: one old, short and fast, the other more recently evolved, more circuitous and slower. In the older system, sensory information travels directly and quickly from the thalamus to the amygdala where it elicits the autonomic and motor responses we call fear. In the younger system, sensory information travels from the thalamus to the relevant cortical sensory areas (touch to the somatosensory cortex, vision to the visual cortex, etc.) and on to frontal association areas, where appraisal occurs. These frontal areas communicate directly with the amygdala and, in light of appraisal, may reduce or magnify the amygdala's fear response. If you glimpse what looks like a snake, long before your younger frontal areas have had time to determine it is a stick, the old thalamus-amygdala system will have evoked fear. LeDoux hypothesizes that the old fast system persists because a behavioral response at the first hint of danger is of little consequence when mistaken but may mean the difference between life and death when appropriate.\n\n\n", "id": "22433196", "title": "Evolution of emotion"}
{"url": "https://en.wikipedia.org/wiki?curid=44701366", "text": "Directed evolution (transhumanism)\n\nThe term directed evolution is used within the transhumanist community to refer to the idea of applying the principles of directed evolution and experimental evolution to the control of human evolution. In this sense, it is distinct from the use of the term in biochemistry, which refers only to the evolution of proteins and RNA. Maxwell J. Melhmanh has described directed evolution of humans as the Holy Grail of transhumanism.\nOxford philosopher Julian Savulescu wrote that:\nAccording to UCLA biophysicist Gregory Stock:\nRiccardo Campa, from the Institute for Ethics and Emerging Technologies, wrote that \"self-directed evolution\" can be coupled with many different political, philosophical, and religious views.\n\nAndrew Askland, from the Sandra Day O'Connor College of Law claims that referring to transhumanism as directed evolution is problematic because evolution is ateleological and transhumanism is teleological.\n\nParticipant evolution is an alternative term that refers to the process of deliberately redesigning the human body and brain using technological means, rather than through the natural processes of mutation and natural selection, with the goal of removing \"biological limitations\" and human enhancement. The idea of participant evolution was first put forward by Manfred Clynes and Nathan S. Kline in the 1960s in their article \"Cyborgs and Space\", where they argued that the human species was already on a path of participant evolution. Science fiction writers have speculated what the next stage of such participant evolution will be.\n\nWhilst Clynes and Kline saw participant evolution as the process of creating cyborgs, the idea has been adopted and propounded by transhumanists who argue that individuals should have the choice of using human enhancement technologies on themselves and their children, to progressively become transhuman and ultimately posthuman, as part of a voluntary regimen of participant evolution.\n", "id": "44701366", "title": "Directed evolution (transhumanism)"}
{"url": "https://en.wikipedia.org/wiki?curid=46324244", "text": "Skeletal changes of organisms transitioning from water to land\n\nInnovations conventionally associated with terrestrially first appeared in aquatic elpistostegalians such as \"Panderichthys rhombolepis\", \"Elpistostege watsoni\", and \"Tiktaalik roseae\". Phylogenetic analyses distribute the features that developed along the tetrapod stem and display a stepwise process of character acquisition, rather than abrupt. The complete transition occurred over a period of 25 million years beginning with the tetrapodomorph diversification in the Middle Devonian (380 myr).\n\nBy the Upper Devonian period, the fin-limb transition as well as other skeletal changes such as gill arch reduction, opercular series loss, mid-line fin loss, and scale reduction were already completed in many aquatic organisms. As aquatic tetrapods began their transition to land, several skeletal changes are thought to have occurred to allow for movement and respiration on land. Some adaptations required to adjust to non-aquatic life include the movement and use of alternating limbs, the use of pelvic appendages as sturdy propulsors, and the use of a solid surface at the organism’s base to generate propulsive force required for walking.\n\nThe Osteolepiformes and Elpistostegalia are two crown groups of rhipidistians with respect to the tetrapods. The development of skull roof and cheekbone patterns in these organisms match those found in the first tetrapods. Palatal and nasal skeletal features like choanae are present in these groups and are also observed in modern amphibians. This indicates that incipient air breathing was developed, as well as modification of the hyoid arch towards stapes development. These characteristics account for why osteichthyans are accepted as the sister group of tetrapods.\n\nThe elpistostegalid fish are considered the most apomorphic of fish in comparison to tetrapods. From well-preserved fossils, it is observed that they share a paltybasic skull with eye ridges, and external nares situated on the margin of the mouth. Development of eye ridges and flatting of the skull are also observed in primitive fossil amphibians and reptiles. The most likely reason for the traits to be adaptive was for their use in aerial vision above the waterline. The traits enabled animals to check area on land for safe spots if being chased by a predator in water, as well as being useful for searching for prey items above the water. The water-based lateral line system was used substantially by these aquatic tetrapods to detect danger from predators. Within the Osteichthyan diversification, there were no changes related to respiration in the transition as can be seen by the nasal region and palatal morphology in elpistostegalid fishes. The primary change from basic ostelepiform ancestors to the first elpistostegalid in the middle Devonian was to the pre-existing roof skulls.\n\nIn \"Elginerpeton pancheni\", a prototetrapod from the late Frasnian, basic tetrapod characteristics in the lower jaw and the cranium are observed. The taxon is believed to fill the gap between elpistostegalid fishes and well-preserved Devonian tetrapods. The \"Elginerpeton\" is considered more derived than the elpistostegalid fishes due to presence of paired fangs on the parasymphysial toothplate, a slender shaped anterior coronoid, and in the loss of the intracranial joint and coronoid fossa. The loss of the intercranial joint was a direct functional necessity to strengthen the broad and long platybasic skull when the animal was out of the water. The tubular lower jaw of the \"Elginerpeton\", compared to the flat-lamina jaw shape of fishes gave it superior cross-sectional force, required when not supported in an aquatic setting – allowing for opening of the mouth outside of water. The adaptation may also be interpreted as a specialization for buccopharyngeal breathing. It is speculated to be the first step towards aerial respiration in the transition from fish to tetrapod.\n\nIn the tetrapod and higher clades from the lower-middle Famennian there are several defining changes on the basis of anatomy of \"Ichthyostega\", \"Tulerpeton\", and \"Acanthostega\". In the cranium, there is a stapes derived from the hyomandibular of fishes; a single bilateral pair of nasal bones, and a fenestra ovalis in the otic capsule of the braincase. The opening of the otic wall of the braincase can be considered a paedomorphic feature for tetrapods and is linked to the stapes functionally. The stapes was thought to be just a structural support between the palate and the stapedial plate of the braincase. In the \"Acanthostega\", it is likely that due to the otic capsule of the brain case being mesial to the stapedial plate, sound was picked up from the palate or the otic notch to allow for rudimentary hearing. It was able to perceive vibrations by opening its mouth by way of the palate. Other factors that caused aquatic tetrapods to spend more time on land caused the development of terrestrial hearing with the development of a tympanum within an otic notch and developed by convergent evolution at least three times.\nThere was also a change in the dermal bones of the skull in the aquatic tetrapods. It involved the enlargement of the jugal, ceasing the contact of the maxilla with the squamosal and the single bilateral pair of nasal bones. The feature allows for a stronger bite as well as increasing the strength of the skull.\n\nFeeding on land is a completely different task than feeding in water. Water is much more dense and viscous compared to air, causing hunting techniques adapted in water to be less successful when applied on land. The main technique used in water is suction feeding and is used by most aquatic vertebrates. This technique does not function in air so animals use methods of overtaking prey with jaws followed by biting down. Transitional forms prior to fully developed terrestrial tetrapods such as \"Acanthostega\", are thought to have captured prey in the water. Large coronoid fangs are present in the fishes \"Eusthenopteron\", \"Panderichthys\", and \"Tiktaalik\", and the early tetrapod, \"Ventasega\". In \"Acanthostega\", which is more derived, the large teeth are absent. In \"Eusthenopetron\" and \"Panderichthys\", an ossified operculum is exhibited unlike in the \"Tiktaalik\", \"Ventastega\", and \"Acanthostega\". These differences as well as reductions of the gill chamber and changes in the nature of the lower jaw are hypothesized to indicate a reduced reliance on suction feeding in early tetrapods in comparison to osteolepiform fish. This morphological data is not enough however to prove that suction feeding was less used as the morphological changes have been found in fish that use the suction feeding mechanism.\n\nCranial sutures are indicators of skull function and morphologies can be linked to specific feeding modes. Transitional feeding changes can be observed by examining cross sectional morphology of a suture in taxa of the fish-tetrapod transition. Comparing positionally comparable sutures in extant fish allows for the creation of a sutural morphospace. The main cause of sutural deformation is caused by strain during feeding activity, most prominent with feeding mechanisms involving sucking a prey into the mouth. There is a tension anteriorly, and compression posteriorly strain patterns are observed in \"Polypterus\", a prey-sucking predator. In terrestrial tetrapod \"Phonerpeton\", there is compression between the frontals and parietals and a complex loading between the post parietals. There is no evidence of tensile strain in any sutures. \"Acanthostega\" fossil records demonstrate that no strain pattern was exhibited that relate to prey capture by means of suction. The load compression is similar to extant tetrapods. It is most likely that the organism captured prey by biting in the water or near the edge of the water. This finding indicates that the terrestrial mode of feeding first emerged in an aquatic environment.\n\nThe cranial endoskeleton of \"T. roseae\" shares derived features with tetrapods. There was a loss of opercular and extrascapular elements, enhancing head mobility in \"T. roseae\" compared to other tetrapodomorph fish. The formation of the neck allowed for locomotion in shallow waters. This environment allows for less motility compared to the three-dimensional space that fish are able to orient themselves in. The body of the organism in these environments would be fixed in the shallow pools with appendages planted on a substrate.\n\nIn the \"Acanthostega\" and \"Ichthyostega\", which are considered to be more derived than other basal aquatic tetrapods, the pectoral girdle is decoupled from the skull. There is also a loss of the dorsal pectoral girdle bones, which permits a large degree of movement for the shoulder. This allowed for a greater degree of movement, and is a necessity for improving aquatic maneuveurs and terrestrial locomotion. This could have been driven by the need to lift the head to aid aerial respiration by using nostrils and choanae.\n\nLimbs in vertebrates are occasionally organized into stylopod (relating to the humerus and femur), zeugopod (relating to the radius and tibia, along with associated structures) and autopod (relating to digits) categories, although anatomically, the evolutionary differences between these groups in early tetrapods tends to be vague.\nThe transition from fins to limbs occurred once an endoskeleton entered the base of the fin, as seen in today's lungfish. This is thought to have originated in the group Sarcopterygians, including osteolipiforms like \"Eusthenopteron\", due to the homology of the tetrapod forelimb and the osteolepiform fin endoskeleton.\n\n\"Acanthostega\" is a partially aquatic tetrapod with developed limbs that shares features common with the earlier tetrapods, \"Panderichthys\" and \"Eusthenopteron\". Like \"Panderichthys\", the humerus of \"Acanthostega\" is flattened dorso-ventrally, the intermedium terminates level with the radius, and the endoskeleton can be divided into stylopodium, zeugopodium and autopodium segments. Similar to \"Eusthenopteron\", the radials do not articulate with the radius on the distal end. \"Acanthostega\" also has a 1:2 ratio of humerus to radius and ulna, a feature seen in all tetrapods higher than \"Acanthostega\" on the phylogeny.\n\nUnlike \"Panderichthys\", \"Acanthostega\" hind limbs are at least the size of its fore limbs, if not larger. This development of larger limbs is required to physically support the organism during emergence from an aquatic setting to land. The humerus and femur of \"Acanthostega\" also contain evidence of greater development of the appendicular muscles compared to more aquatic tetrapods, hinting at the presence of digits.\n\nSimilarly, \"Ossinodus\" has two hindlimbs located bilaterally and proximodistally aymmetrical. Due to the presence of a small femur during juvenile development, this Carboniferous- period tetrapod is thought to be aquatic during juvenile development; only emerging onto land once it reaches adulthood. \"Ossinodus\" also has a broad, flat tibia, akin to \"Acanthostega\", and is thought to be only partially terrestrial.\n\nThe development of the pelvic region was crucial for the adaptation from water to land, yet some features of tetrapod locomotion are thought to have arose before the origin of digited limbs or the transition from water to land. The fossil record of early tetrapods shows evidence of distinct pelvic development occurring in osteolepiforms, further supporting osteolepiform ancestry of terrestrial tetrapods.\n\n\"Acanthostega\" has a large pelvis, with the iliac region articulating with the axial skeleton and a broad ischial plate. It has a sacrum; a fundamental skeletal feature that allows the organism to transfer force produced in its hindlimbs to its axial skeleton, and move in a terrestrial environment. A pubo-ischiadic symphysis is also observed, uniting the two pelvic halves.\n\nIn contrast, \"Protopterus annectens\" (a member of lungfish, thought to be a sister group to tetrapods) has a small, anatomically simpler pelvis, a derived limb endoskeleton and a lack of digits. Yet, it shares the ability to lift itself using a solid surface as a base with its pelvic region with \"Acanthostega\" and is also observed to move with tetrapod-like locomotion in an aquatic environment. This illustrates that a fundamental innovation in tetrapods is also found in a lower, sister taxon, in which members lack a sacrum.\n\n\"Acanthostega\" is the earliest example of a digitized tetrapod. The humerus and femur of \"Acanthostega\" contain evidence of greater development of the appendicular muscles compared to more aquatic tetrapods. \"Acanthostega\" has a total lack of dermal fin rays and displays the presence of two or more spool-shaped bones or cartilages articulating individually in antero-posterial sets on the distal end of its limbs. This feature can now be distinguished as digits instead of the endoskeletal radials seen in earlier tetrapods.\n\n\"Pederpes\", a tetrapod from the Early Carboniferous period, also has hindlimbs containing 5 digits that are rotated to face anteriorly. Unlike previous tetrapods, who have been only partially adapted to land, \"Pederpes\" has the novel ability to bend its limbs and propel itself forwards in a terrestrial setting. This is attributed to the symmetry of the digits and limbs in \"Pederpes\", allowing it to rotate its hindlimbs to an anteriorly facing position and propel itself from the edge of the foot when moving forward. This morphological development of bendable wrists and ankles can distinguish \"Pederpes\" the first true terrestrial tetrapod.\n", "id": "46324244", "title": "Skeletal changes of organisms transitioning from water to land"}
{"url": "https://en.wikipedia.org/wiki?curid=46348501", "text": "Evolution of the cochlea\n\nThe word cochlea /ˈkɒklɪə/ is Latin for “snail, shell or screw” and originates from the Greek word \"kohlias\". The modern definition, the auditory portion of the inner ear, originated in the late 17th century. Within the mammalian cochlea exists the organ of Corti, which contains hair cells that are responsible for translating the vibrations it receives from surrounding fluid-filled ducts into electrical impulses that are sent to the brain to process sound. This spiral-shaped cochlea is estimated to have originated during the early Cretaceous Period, around 120 million years ago. Further, the auditory innervation of the spiral-shaped cochlea also traces back to the Cretaceous period. The evolution of the human cochlea is a major area of scientific interest because of its favourable representation in the fossil record. During the last century, many scientists such as evolutionary biologists and paleontologists strove to develop new methods and techniques to overcome the many obstacles associated with working with ancient, delicate artifacts. In the past, scientists were limited in their ability to fully examine specimens without causing damage to them. In more recent times, technologies such as micro-CT scanning became available. These technologies allow for the visual differentiation between fossilized animal materials and other sedimentary remains. With the use of X-ray technologies, it is possible to ascertain some information about the auditory capabilities of extinct creatures, giving insight to human ancestors as well as their contemporary species.\n\nWhile the basic structure of the inner ear in lepidosaurs (lizards and snakes), archosaurs (birds and crocodilians) and mammals is similar, and the organs are considered to be homologous, each group has a unique type of auditory organ. The hearing organ arose within the lagenar duct of stem reptiles, lying between the saccular and lagenar epithelia. In lepidosaurs, the hearing organ, the basilar papilla, is generally small, with at most 2000 hair cells, whereas in archosaurs the basilar papilla can be much longer (>10mm in owls) and contain many more hair cells that show two typical size extremes, the short and the tall hair cells. In mammals, the structure is known as the organ of Corti and shows a unique arrangement of hair cells and supporting cells. All mammalian organs of Corti contain a supporting tunnel made up of pillar cells, on the inner side of which there are inner hair cells and outer hair cells on the outer side. The definitive mammalian middle ear and the elongated cochlea allows for better sensitivity for higher frequencies.\n\nAs in all lepidosaurs and archosaurs, the single-ossicle (columellar) middle ear transmits sound to the footplate of the columella, which sends a pressure wave through the inner ear. In snakes, the basilar papilla is roughly 1mm long and only responds to frequencies below about 1 kHz. In contrast, lizards tend to have two areas of hair cells, one responding below and the other above 1 kHz. The upper frequency limit in most lizards is roughly 5–8 kHz. The longest lizard papillae are about 2mm long and contain 2000 hair cells and their afferent innervating fibers can be very sharply tuned to frequency.\n\nIn birds and crocodilians, the similarity of the structure of the basilar papilla betrays their close evolutionary relationship. The basilar papilla is up to about 10mm long and contains up to 16500 hair cells. While most birds have an upper hearing limit of only about 6 kHz, the barn owl can hear up to 12 kHz and thus close to the human upper limit.\n\nEgg-laying mammals, the monotremes (spiny anteater and platypus), do not have a spiral cochlea, but one shaped more like a banana, up to about 7 mm long. Like in lepidosaurs and archosaurs, it contains a lagena, a vestibular sensory epithelium, at its tip. Only in therian mammals (marsupials and placentals) is the cochlea truly coiled 1.5 to 3.5 times. Whereas in monotremes there are many rows of both inner and outer hair cells in the organ of Corti, in therian (marsupial and placental) mammals the number of inner hair-cell rows is one, and there are generally only three rows of outer hair cells.\n\nAmphibians have unique inner ear structures. There are two sensory papillae involved in hearing, the basilar (higher frequency) and amphibian (lower frequency) papillae, but it is uncertain whether either is homologous to the hearing organs of lepidosaurs, archosaurs and mammals and we have no idea when they arose.\n\nFish have no dedicated auditory epithelium, but use various vestibular sensory organs that respond to sound. In most teleost fishes it is the saccular macula that responds to sound. In some, such as goldfishes, there is also a special bony connection to the gas bladder that increases sensitivity allowing hearing up to about 4 kHz.\n\nThe size of cochlea has been measured throughout its evolution based on the fossil record. In one study, the basal turn of the cochlea was measured, and it was hypothesized that cochlear size correlates with body mass. The size of the basal turn of the cochlea was not different in Neanderthals and Holocene humans, however it became larger in early modern humans and Upper Paleolithic humans. Furthermore, the position and orientation of the cochlea is similar between Neanderthals and Holocene humans, relative to plane of the lateral canal, whereas early modern and upper Paleolithic humans have a more superiorly placed cochlea than Holocene humans. When comparing hominins of the Middle Pleistocene and Neanderthals and Holocene humans, the apex of the cochlea faces more inferiorly in the hominins than the latter two groups. Finally, the cochlea of European middle Pleistocene hominins faces more inferiorly than Neanderthals, modern humans, and Homo erectus.\nHuman beings, along with Apes, are the only mammals that do not have high frequency (>32 kHz) hearing. Humans have long cochleae, but the space devoted to each frequency range is quite large (2.5mm per octave), resulting in a comparatively reduced upper frequency limit. The human cochlea has approximately 2.5 turns around the modiolus (the axis). Humans, like many mammals and birds, are able to perceive auditory signals that displace the eardrum by a mere picometre.\n\nBecause of its prominence and preserved state in the fossil record, until recently, the ear had been used to determine phylogeny. The ear itself contains different portions, including the outer ear, the middle ear, and the inner ear and all of these show evolutionary changes that are often unique to each lineage [14]. It was the independent evolution of a tympanic middle ear in the Triassic era that produced strong selection pressures towards improved hearing organs in the separate lineages of land vertebrates.\n\nThe cochlea is the tri-chambered auditory detection portion of the ear, consisting of the scala media, the scala tympani, and the scala vestibuli. Regarding mammals, placental and marsupial cochleae have similar cochlear responses to auditory stimulation as well as DC resting potentials. This leads to the investigation of the relationship between these therian mammals and researching their ancestral species to trace the origin of the cochlea.\n\nThis spiral-shaped cochlea that is in both marsupial and placental mammals is traced back to approximately 120 million years ago. The development of the most basic basilar papilla (the auditory organ that later evolved into the Organ of Corti in mammals) happened at the same time as the water-to-land transition of vertebrates, approximately 380 million years ago. The actual coiling or spiral nature of the cochlea occurred to save space inside the skull. The longer the cochlea, the higher is the potential resolution of sound frequencies given the same hearing range. The oldest of the truly coiled mammalian cochleae were approximately 4 mm in length.\n\nThe earliest evidence available for primates depicts a short cochlea with prominent laminae, suggesting that they had good high-frequency sensitivity as opposed to low-frequency sensitivity. After this, over a period of around 60 million years, evidence suggests that primates developed longer cochleae and less prominent laminae, which means that they had an improvement in low-frequency sensitivity and a decrease in high-frequency sensitivity. By the early Miocene period, the cycle of the elongation of the cochleae and the deterioration of the laminae was completed. Evidence shows that primates have had an increasing cochlear volume to body mass ratio over time. These changes in the cochlear labyrinth volume negatively affect the highest and lowest audible frequencies, causing a downward shift. Non-primates appear to have smaller cochlear labyrinth volumes overall when compared to primates. Some evidence also suggests that selective forces for the larger cochlear labyrinth may have started after the basal primate node.\nMammals are the subject of a substantial amount of research not only because of the potential knowledge to be gained regarding humans, but also because of their rich and abundant representation in the fossil record. The spiral shape of the cochlea evolved later on in the evolutionary pathway of mammals than previously believed, just before the therians split into the two lineages marsupials and placentals, about 120 million years ago.\n\nParallel to the evolution of the cochlea, prestins show an increased rate of evolution in therian mammals. Prestins are located in the outer hair cells of mammalian cochlea and are considered motor proteins. They are found in the hair cells of all vertebrates, including fish, but are thought to have initially been membrane transporter molecules. A high concentration of prestins are found only in the lateral membranes of therian outer hair cells (there is uncertainty with regard to concentrations in monotremes). This high concentration is not found in inner hair cells, and is also lacking in all hair cell types of non-mammals. Prestin also has a role in motility, which evolved a greater importance in the motor function in land vertebrates, but this developed vastly differently in different lineages. In certain birds and mammals, prestins function as both transporters and motors, but the strongest evolution to robust motor dynamics only evolved in therian mammals. It is hypothesized that this motor system is significant to the therian cochlea at high frequencies because of the distinctive cellular and bony composition of the organ of Corti that allows the prestins to intensify movements of the whole structure.\nModern ultra-sound echolocating species such as bats and toothed whales show highly evolved prestins, and these prestins show identical sequence alterations over time. Unusually, the sequences thus apparently evolved independent from each other during different time periods. Furthermore, the evolution of neurotransmitter receptor systems (acetylcholine) that regulate the motor feedback of the outer hair cells coincides with prestin evolution in therians. This suggests that there was a parallel evolution of a control system and a motor system in the inner ear of therian mammals.\n\nLand vertebrates evolved middle ears independently in each major lineage, and are this the result of parallel evolution. The configurations of the middle ears of monotreme and therian mammals can thus be interpreted as convergent evolution or homoplasy. Thus evidence from fossils demonstrate homoplasies for the detachment of the ear from the jaw. Furthermore, it is apparent that the land-based eardrum, or tympanic membrane, and connecting structures such as the Eustachian tube evolved convergently in multiple different settings as opposed to being a defining morphology.\n", "id": "46348501", "title": "Evolution of the cochlea"}
{"url": "https://en.wikipedia.org/wiki?curid=4187880", "text": "Interlocus contest evolution\n\nInterlocus contest evolution (ICE) is a process of intergenomic conflict by which different loci within a single genome antagonistically coevolve. ICE supposes that the Red Queen process, which is characterized by a never-ending antagonistic evolutionary arms race, does not only apply to species but also to genes within the genome of a species.\n\nBecause sexual recombination allows different gene loci to evolve semi-autonomously, genes have the potential to coevolve antagonistically. ICE occurs when \"an allelic substitution at one locus selects for a new allele at the interacting locus, and vice versa.\" As a result, ICE can lead to a chain reaction of perpetual gene substitution at antagonistically interacting loci, and no stable equilibrium can be achieved. The rate of evolution thus increases at that locus.\n\nICE is thought to be the dominant mode of evolution for genes controlling social behavior. The ICE process can explain many biological phenomena, including intersexual conflict, parent-offspring conflict, and interference competition.\n\nA fundamental conflict between the sexes lies in differences in investment: males generally invest predominantly in fertilization while females invest predominantly in offspring. This conflict manifests itself in many traits associated with sexual reproduction. Genes expressed in only one sex are selectively neutral in the other sex; male- and female-linked genes can therefore be acted upon separated by selection and will evolve semi-autonomously. Thus, one sex of a species may evolve to better itself rather than better the species as a whole, sometimes with negative results for the opposite sex: loci will antagonistically coevolve to enhance male reproductive success at females’ expense on the one hand, and to enhance female resistance to male coercion on the other. This is an example of intralocus sexual conflict, and is unlikely to be resolved fully throughout the genome. However, in some cases this conflict may be resolved by the restriction of the gene’s expression to only the sex that it benefits, resulting in sexual dimorphism.\n\nThe ICE theory can explain the differentiation of the human X- and Y-chromosomes. Semi-autonomous evolution may have promoted genes beneficial to females in the X-chromosome even when detrimental to males, and genes beneficial to males in the Y-chromosome, even when detrimental to females. As the distribution of the X-chromosome is three times as large as the Y-chromosome (the X-chromosome occurs in 3/4 of offspring genes, while the Y-chromosome occurs in only 1/4), the Y-chromosome has a reduced opportunity for rapid evolution. Thus the Y-chromosome has \"shed\" its genes to leave only the essential ones (such as the SRY gene), which gives rise to the differences in the X- and Y-chromosomes.\n\nA father, mother and offspring may differ in the optimal resource allocation to the offspring. This co-evolutionary conflict can be considered in the context of ICE. Selection will favor genes in the male to maximize female investment in the current offspring, no matter the consequences to the female's reproduction later in life, while selection will favor genes in the female that increase her overall lifetime fitness. Genes expressed in the offspring will be selected to produce an intermediary level of resource allocation between the male-benefit and female-benefit loci. This three-way conflict again occurs when parents feed their offspring, as the optimum feeding rate and optimum point in time to discontinue feeding differ between father, mother and offspring.\n\nICE can also explain the theory of interference competition, which is most likely to be associated with opposing sets of genes that determine the outcome of competition between individuals. Different sets of genes may code for signal or receiver phenotypes, such as in the context of threat displays: when a competing male can win more contests by intimidation, rather than by fighting, selection will favor the accumulation of deceitful genes that may not be honest indicators of the male’s fighting capability.\n\nFor example, primitive male elephant seals may have used the lowest frequencies in the threat call of a rival as an indication of body size. The elephant seal's enormous nose may have evolved as a resonating device to amplify low frequencies, illustrating selection that favors the production of low-frequency threat vocalizations. However, this counter-selects for receptor systems that provide an increased threshold required for intimidation, which in turn selects for deeper threat vocalizations. The rapid divergence of threat displays among closely related species provides further evidence in support of the co-evolutionary arms race within the genome of a single species, driven by the ICE process.\n", "id": "4187880", "title": "Interlocus contest evolution"}
{"url": "https://en.wikipedia.org/wiki?curid=45682373", "text": "Character evolution\n\nCharacter evolution is the process by which a character or trait (a certain body part or property of an organism) evolves along the branches of an evolutionary tree. Character evolution usually refers to single changes within a lineage that make this lineage unique from others. These changes are called character state changes and they are often used in the study of evolution to provide a record of common ancestry. Character state changes can be phenotypic changes, nucleotide substitutions, or amino acid substitutions. These small changes in a species can be identifying features of when exactly a new lineage diverged from an old one. \nIn the study of phylogenetics or cladistics, researchers can look at the characters shared by a collection of species and then group them into what is called a clade. The term clade was coined in 1957 by the biologist Julian Huxley to refer to the result of cladogenesis, a concept Huxley borrowed from Bernhard Rensch. A clade is by definition monophyletic, meaning it contains one ancestor (which can be an organism, a population, or a species) and all its descendants.\n\nNatural Selection is the process by which organisms that are better adapted to their environment are selected to survive and reproduce more offspring. Natural selection selects for the phenotype or the characteristics of an organism that gives the organism a reproductive advantage in which it becomes the gene pool of a population. In addition, mutations also arise in the genome of an individual organism and offspring(s) can inherit such mutations. This genetic variation allows more organisms to adapt to a changing environment.\n\nIt is often the case in the study of phylogenies that the vast majority of organisms of interest are long extinct. It is therefore a matter of speculation to reconstruct what ancestral organisms existed long before the present time, and how the evolutionary process led from one organism to another, and which present-day organisms are most closely related. Character evolution and the character state changes that drive this type of evolution are what help researchers construct these trees in a fashion referred to as maximum parsimony. When talking about phylogenetics, maximum parsimony refers to a method of inferring a phylogenetic tree in a way that minimizes the number of implied character state transformations in the observed data (hence \"maximally parsimonious\"). The basic ideas were presented by James S. Farris in 1970.\n\nAlthough fairly effective, maximum parsimony (like any method of phylogenetic inference) may not recover the true course of evolution for a given feature. For a number of reasons, two organisms can possess a trait not present in their last common ancestor. The phenomena of convergent evolution, parallel evolution, and evolutionary reversals (collectively termed \"homoplasy\") are evolutionary forces that may disrupt the effectiveness of the maximum parsimony method of inferring phylogenetic relationships. However, Rindal and Brower showed that the vast majority of the time, parsimony and model-based phylogenetic analyses of the same data sets gave results that were not significantly different from one another, implying that if parsimony is producing false hypotheses of relationships due to homoplasy, then the Maximum Likelihood or Bayesian methods are doing so as well.\n\nLamarck is best known for his \"Theory of Inheritance of Acquired Characteristics\" in 1801. His theory states that the characteristics an organism acquires throughout its life in order to adapt to its environment are passed down to its offspring. For example, Lamarck believed that the long necks of giraffes evolved as generations of giraffes reached for ever higher leaves of a tree. Their offspring and later generations inherited the resulting long necks.\n\n", "id": "45682373", "title": "Character evolution"}
{"url": "https://en.wikipedia.org/wiki?curid=46661027", "text": "Phagomimicry\n\nPhagomimicry is a defensive behaviour of sea hares, in which the animal ejects a mixture of chemicals, which mimic food, giving the sea-hare a chance to escape. The typical defence response of the sea hare to a predator is to release two chemicals - ink from the ink gland and opaline from the opaline gland. While ink creates an dark, diffuse cloud in the water which disrupts the sensory perception of the predator by acting as a smokescreen and as a decoy, the opaline, which affects the senses dealing with feeding, causes the predator to instinctively attack the cloud of chemicals as if it were indeed food.\n", "id": "46661027", "title": "Phagomimicry"}
{"url": "https://en.wikipedia.org/wiki?curid=46333271", "text": "Vertebrate land invasion\n\nThe aquatic to terrestrial transition of vertebrate organisms occurred in the late Devonian era and was an important step in the evolutionary history of modern land vertebrates. The transition allowed animals to escape competitive pressure from the water and explore niche opportunities on land. Fossils from this period have allowed scientists to identify some of the species that existed during this transition, such as Tiktaalik and Acanthostega. Many of these species were also the first to develop adaptations suited to terrestrial over aquatic life, such as neck mobility and hindlimb locomotion.\n\nThe late Devonian vertebrate transition was not the only terrestrial invasion in evolutionary history. The vertebrate transition was preceded by the plant and invertebrate terrestrial invasion. These invasions allowed for the appropriate niche development that would ultimately facilitate the vertebrate invasion. Furthermore, the late Devonian event is only significant in that it was the first land invasion by vertebrate organisms, and that it resulted in an explosion of vertebrate biodiversity due to the many different niches that these species were able to exploit. However, aquatic species have continued to develop adaptations suited to terrestrial life (and vice versa) from the late Devonian to the Holocene.\n\nThe vertebrate species that were important to the initial water to land transition can be qualified as being one of five groups: Sarcopterygian fishes, prototetrapods, aquatic tetrapods, true tetrapods, and terrestrial tetrapods. Many morphological changes occurred throughout this transition. Mechanical support structures changed from fins to limbs, the method of locomotion changed from swimming to walking, respiratory structures changed from gills to lungs, feeding mechanisms changed from suction feeding to biting, and mode of reproduction changed from larval development to metamorphosis.\n\nLungfish appeared approximately 400 million years ago. It is a species that endured rapid evolution during the Devonian era, which became known as the dipnoan renaissance. The Acanthostega species, known as the fish with legs, is considered a tetrapod by structural findings but is postulated to have perhaps never left the aquatic environment. Its legs are not well-suited to support its weight. The bones of its forearm, the radius and ulna, are very thin at the wrist and also unable to support it on land. It also lacks a sacrum and strong ligaments at the hip, which would be integral to supporting the animal against gravity. In this sense, the species is considered a tetrapod but not one that has adapted well enough to walk on land. Furthermore, its gill bars have a supportive brace characterized for use as an underwater ear because it can pick up noise vibrations through the water. Tetrapods that adapted to terrestrial living adapted these gill bones to pick up sounds through air, and they later became the middle ear bones seen in mammalian tetrapods. Ichthyostega, on the other hand, is considered to be a fully terrestrial tetrapod that perhaps depended on water for its aquatic young. Comparisons between the skeletal features of Acanthostega and Ichthyostega reveal that they had different habits. Acanthostega is likely exclusive to an aquatic environment, while Ichthyostega is progressed in the aquatic to terrestrial transition by living dominantly on the shores.\n\nAn evolutionary timeline of the late Devonian vertebrate terrestrial invasion demonstrates the changes that took place. A group of fish from the Givetian stage began developing limbs, and eventually evolved into aquatic tetrapods in the Famennian stage. Pederpes, Westlothiana, Protogyrinus, and Crassigyrinus descended from these species into the carboniferous period and were the first land vertebrates.\n\nA particularly important transitional species is one known as Tiktaalik. It has a fin, but interestingly the fin has bones within it that are similar to mammalian tetrapods. It has an upper arm bone, a lower arm bone, forearm bones, a wrist, and fingerlike projections. Essentially, it is a fin that can support the animal. Similarly, it also has a neck that allows independent head movement from the body. Its ribs are also able to support the body in gravity. Its skeletal features exhibit its ability as a fish that can live in shallow water and also venture onto land.\n\nIt took many millions of years for vertebrates to transition out of water onto land. During this time, both the competitive pressures that would push species out of the water and the niche occupation incentives that would pull species onto land were slowly building. The culmination of these driving factors are what ultimately facilitated the vertebrate transition.\n\nScientists believe that a long period of time where biotic and abiotic factors in the aquatic environment were unfavourable to a subset of aquatic organisms is what initiated their transition to shallower waters. Some of these push factors are environmental hypoxia, unfavourable aquatic temperatures, and increased salinity. Other constantly present factors such as predation, competition, waterborne diseases and parasites also contributed to the transition.\n\nA theory put forth by Joseph Barrell possibly helps explain what may have initiated these push factors to become relevant in the late Devonian. The extensive oxidized sediments that were present in Europe and North America during the late Devonian are evidence of severe droughts during this time. These droughts would cause small ponds and lakes to dry out, forcing certain aquatic organisms to move on land to find other bodies of water. Natural selection on these organisms eventually led to the evolution of the first terrestrial vertebrates.\n\nThe pull factors were secondary to the push factors, and only became significant once the pressures to leave the aquatic environment became significant. These were largely the niches and opportunities that were available for exploitation in the terrestrial environment, and include higher environmental oxygen partial pressures, favourable temperatures, and the lack of competitors and predators on land. The plants and invertebrates that had preceded the vertebrate invasion also provided opportunities in the form of abundant prey and lack of predators.\n\nThere were many challenges that the first land vertebrates faced. These challenges allowed for rapid natural selection and niche domination, resulting in an adaptive radiation that produced many different vertebrate land species in a relatively short period of time.\n\nDepending on the water depth at which a species lives, the visual perception of many aquatic species is better suited to darker environments than those on land. Similarly, hearing in aquatic organisms is better optimized for sounds underwater, where the speed and amplitude of sound is greater than in air.\n\nHomeostasis was almost definitely a challenge for land invading vertebrates. Gas exchange and water balance are highly different in water and in air. Homeostasis mechanisms suitable for a terrestrial environment may have been necessary to develop before these organisms invaded land.\n\nThe primary anatomical barrier is the development of lungs for proper gas exchange, however other anatomical barriers also exist. The stressors of the musculoskeletal system are different in air than they are in water, and the muscles and bones must be strong enough to withstand the increased effects of gravity on land.\n\nMany behaviours, such as reproduction, are specifically optimized to a wet environment. Navigation and locomotion are also highly different in aquatic environments compared to terrestrial environments.\n\nThe ancestral species of tetrapods that lived entirely in water had tall and narrow skulls with eyes facing sideways and forwards to maximize visibility for predators and prey in the aquatic environment. As the ancestors of early tetrapods started inhabiting shallower waters, these species had flatter skulls with eyes at the tops of their heads, which made it possible to spot food above them. Once the tetrapods transitioned onto land, the lineages evolved to have tall and narrow skulls with eyes facing sideways and forwards again. This allowed them to navigate through the terrestrial environment and look for predators and prey.\n\nFish do not have necks, so the head is directly connected to the shoulders. In contrast, land animals use necks to move their heads so they can look down to see the food on the ground. The greater the mobility of the neck, the more visibility the land animal has. As lineages moved from completely aquatic environments to shallower waters and land, they gradually evolved vertebral columns that increased neck mobility. The first neck vertebra that evolved permitted the animals to have flexion and extension of the head so that they can see up and down. The second neck vertebra evolved to allow rotation of the neck for moving the head left and right. As tetrapod species continued to evolve on land, adaptations included seven or more vertebrae, allowing increasing neck mobility.\n\nThe sacrum connects the pelvis and hindlimbs and is useful for motion on land. The aquatic ancestors of tetrapods did not have a sacrum, so it was speculated to have evolved for locomotive function exclusive to terrestrial environments. However, the Acanthostega species is one of the earliest lineages to have a sacrum, even though it is a fully aquatic species. Once species moved onto land, the trait was adapted for terrestrial locomotion support, which is evidenced by additional vertebrae fusing similarly to permit additional support. This is an example of exaptation, where a trait performs a function that did not arise through natural selection for its current use.\n\nAs the lineages evolved to adapt to terrestrial environments, many lost traits that were better suited for the aquatic environment. Many lost their gills, which were only useful for obtaining oxygen in water. Their tail fins became smaller. They lost the lateral line system, a network of canals along the skull and jaw that are sensitive to vibration, which does not work outside of an aquatic environment.\n\nFor successful land invasion, the species had several pre-adaptations like air-breathing and limb-based locomotion. Aspects such as reproduction and swallowing, however, have bound these species to the aquatic environment. These pre-adaptations have allowed vertebrates to venture onto land hundreds of times, but were not able to accomplish the same degree of prolific radiation into diverse terrestrial species. To understand the potential of future invasions, studies must evaluate the models of evolutionary steps taken in past invasions. The commonalities to current and future invasions may then be elucidated to predict the effects of environmental changes.\n", "id": "46333271", "title": "Vertebrate land invasion"}
{"url": "https://en.wikipedia.org/wiki?curid=3079141", "text": "Deistic evolution\n\nDeistic evolution is a position in the origins debate which involves accepting the scientific evidence for evolution and age of the universe whilst advocating the view that a deistic God created the universe but has not interfered since. The position is a counterpoint to theistic evolution and is endorsed by those who believe in both deism and the veracity of science.\n\nIn \"Christian Theology\", by Millard J. Erickson, 2013, is written:\n\nThe psychologist Steve Stewart-Williams in his book \"Darwin, God and the Meaning of Life\" (2010) states:\n\nStewart-Williams further writes that deistic evolution strips God of what most religious believers consider central. Any deistic God is not around for prayers, miracles or to intervene in people's lives and that because of this it is unpopular with monotheistic religions.\n\nDeistic Evolution adheres to the concept of some form of God, but denies any personal God. A recent defender of deistic evolution was Michael Anthony Corey, author of the book \"Back to Darwin: The Scientific Case for Deistic Evolution\" (1994).\n\nSome scholars have written that Charles Darwin was an advocate of deistic evolution.\n\nDeistic evolution is similarly the operative idea in Pandeism, which has been counted amongst the handful of spiritual beliefs which \"are compatible with modern science.\" and specifically wherein it is noted that \"\"pandeistic\" belief systems ... [present] the inclusion of God as the ever unfolding expression of a complex universe with an identifiable beginning but no teleological direction necessarily present.\"\n\nDeistic evolution is not the same as theistic evolution, yet they are sometimes confused. The difference rests on the difference between a theistic god that is interested in, if not actively involved in, the outcome of his creation and humanity specifically and a deistic god that is either disinterested in the outcome, and holds no special place for humanity, or will not intervene. Often, there is no discernible difference between the two positions—the choice of terminology has more to do with the believer and her or his need for a god, than fitting into a mostly arbitrary dictionary or academic definition.\n\nDeistic evolution has been criticised by Christian creationists as being incompatible with Christianity since it contradicts a literal reading of the Bible and more importantly, leaves no role for the \"Christian personal God\".\n\nM. J. Erickson wrote that deistic evolution is in conflict with the scriptural doctrine of providence according to which \"God is personally and intimately concerned with and involved in what is going on in the specific events within his entire creation.\"\n\nCharles P. Grannan wrote in 1894, \"Another baseless assumption of negative critics is that the general principles of Atheistic and Deistic evolution, admitted by many scientists to account for the origin of the various species of plants and animals, should also be applied to explain the origin of the Christian religion.\"\n\nCharles Wesley Rishell criticized the concept in 1899, comparing it to the notion (false, in his view), that gravity was a property of matter instead of a continued action of God:\n\nDeistic evolution does not oppose or contradict evolution or come into conflict with science as it says that a God started the process and then left it to natural processes. However deism is still a religious philosophy.\n\nStewart-Williams wrote regarding deistic evolution and science:\n\nThere is considerable room for this \"god of the gaps\" view, since scientific observation is entirely unable to shed any light on what happened during the Planck epoch, the earliest 10 seconds in the history of the universe. All development since this initial creative act merely follows laws and principles which He created:\n\n\nThe Roman Catholic Church disagrees with the doctrine of deistic evolution. In November 2005, Pope Benedict addressed a general audience of 25,000 in St. Peter's Square:\n\n", "id": "3079141", "title": "Deistic evolution"}
{"url": "https://en.wikipedia.org/wiki?curid=47971685", "text": "Extended evolutionary synthesis\n\nThe extended evolutionary synthesis is a set of extensions of the earlier modern synthesis of evolutionary biology that took place between 1918 and 1942. The extended evolutionary synthesis was called for in the 1950s by C. H. Waddington, argued for on the basis of punctuated equilibrium by Stephen Jay Gould and Niles Eldredge in the 1980s, and relaunched in 2007 by Massimo Pigliucci.\n\nThe extended evolutionary synthesis revisits the relative importance of different factors at play, examining several assumptions of the earlier synthesis, and augmenting it with additional causative factors. It includes multilevel selection, transgenerational epigenetic inheritance, niche construction, and evolvability.\n\nNot all biologists have agreed on the need for, or the scope of, an extended synthesis. Many have collaborated on a different synthesis in evolutionary developmental biology, which integrates embryology with molecular genetics and evolution to understand how natural selection operated on developmental processes and deep homologies between organisms at the level of highly conserved genes.\n\nThe modern synthesis was the widely accepted early-20th-century synthesis reconciling Charles Darwin's theory of evolution by natural selection and Gregor Mendel's theory of genetics in a joint mathematical framework. It established evolution as biology's central paradigm. The 19th-century ideas of natural selection by Darwin and Mendelian genetics were united by researchers who included Ronald Fisher, one of the three founders of population genetics, and J. B. S. Haldane and Sewall Wright, between 1918 and 1932. Julian Huxley introduced the phrase \"modern synthesis\" in his 1942 book, \"\".\n\nDuring the 1950s, the English biologist C. H. Waddington called for an extended synthesis based from his research on epigenetics and genetic assimilation. An extended synthesis was also proposed by the Austrian zoologist Rupert Riedl, with the study of evolvability. In 1978, Michael J. D. White wrote about an extension of the modern synthesis based on new research from speciation.\n\nIn the 1980s, the American palaeontologists Stephen Jay Gould and Niles Eldredge argued for an extended synthesis. This was based on their idea of punctuated equilibrium, the role of species selection shaping large scale evolutionary patterns and natural selection working on multiple levels extending from genes to species.\nThe ethologist John Endler wrote a paper in 1988 discussing processes of evolution that he felt had been neglected.\n\nResearchers in the field of evolutionary developmental biology created a different extended synthesis. They argued that the modern and extended syntheses were mostly centered on genes and ignored the development of morphology. They collaborated on a different synthesis in evolutionary developmental biology, which integrates embryology with molecular genetics and evolution to understand how natural selection operated on developmental processes and deep homologies between organisms at the level of highly conserved genes.\n\nThe idea of an extended synthesis was relaunched in 2007 by Massimo Pigliucci, with a book in 2010 titled \"Evolution: The Extended Synthesis\", which has served as a launching point for work on the extended synthesis. This includes:\n\n\nOther processes such as evolvability, phenotypic plasticity, reticulate evolution, sex evolution and symbiogenesis are said by proponents to have been excluded or missed from the modern synthesis. The goal of Piglucci's extended synthesis is to take evolution beyond the gene-centered approach of population genetics to consider more organism- and ecology-centered approaches. Many of these causes are currently considered secondary in evolutionary causation, and proponents of the extended synthesis want them to be considered first-class evolutionary causes. The biologist Eugene Koonin wrote in 2009 that \"the new developments in evolutionary biology by no account should be viewed as refutation of Darwin. On the contrary, they are widening the trails that Darwin blazed 150 years ago and reveal the extraordinary fertility of his thinking.\"\n\nPigliucci and colleagues make the following predictions:\n\n\nBiologists disagree on the need for an extended synthesis. Opponents contend that the modern synthesis is able to fully account for the newer observations, while proponents think that the conceptions of evolution at the core of the modern synthesis are too narrow. Proponents argue that even when the modern synthesis allows for the ideas in the extended synthesis, using the modern synthesis affects the way that biologists think about evolution. For example, Denis Noble says that using terms and categories of the modern synthesis distort the picture of biology that modern experimentation has discovered. Proponents therefore claim that the extended synthesis is necessary to help expand the conceptions and framework of how evolution is considered throughout the biological disciplines.\n\nThe principal focus of dispute with Neo-Darwinists is over the source of variation on which Natural Selection can operate (supporters of the Extended Synthesis deny that these always arise from \"random copying errors\" in DNA replication). In fact they often assert that development of an improved organism by a non-standard mechanism - even occasionally - would violate Shannon's principles of Information Theory.\n\nDefend the extended synthesis\n\nCriticism of the extended synthesis\n\n", "id": "47971685", "title": "Extended evolutionary synthesis"}
{"url": "https://en.wikipedia.org/wiki?curid=48471703", "text": "GNC hypothesis\n\nGNC hypothesis or GNC-SNS primeval genetic code hypothesis refers to a hypothesis about the origin of genes. While almost all of the organisms on present Earth shares the universal genetic code, in GNC hypothesis it is argued that two primeval genetic codes preceded the present genetic code as follows.\nThis hypothesis was first proposed by Kenji Ikehara at Nara Women's University.\n\nGNC hypothesis is based on the following facts.\n\n", "id": "48471703", "title": "GNC hypothesis"}
{"url": "https://en.wikipedia.org/wiki?curid=48719972", "text": "Diversification rates\n\nDiversification rates are the rates at which new species form (the Speciation rate, λ) and living species go extinct (the extinction rate, μ). Diversification rates can be estimated from fossils, data on the species diversity of clades and their ages, or phylogenetic trees. Diversification rates are typically reported on a per-lineage basis (e.g. speciation rate per lineage per unit of time), and refer to the diversification dynamics expected under a birth-death process.\n\nA broad range of studies have demonstrated that diversification rates can vary tremendously both through time and across the tree of life. Current research efforts are focused on predicting diversification rates based on aspects of species or their environment.\n\nDiversification rates can be estimated time-series data on fossil occurrences. With perfect data, this would be an easy task; one could just count the number of speciation and extinction events in a given time interval, and then use these data to calculate per-lineage rates of speciation and extinction per unit time. However, the incomplete nature of the fossil record means that our calculations need to include the possibility that some fossil lineages were not sampled, and that we do not have precise estimates for the times of speciation and extinction of the taxa that are sampled. More sophisticated methods account for the probability of sampling any lineage, which might also depend on some properties of the lineage itself (e.g. whether it has any hard body parts that tend to fossilize) as well as the environment in which it lives.\n\nMany estimates of diversification rates for fossil lineages are for higher-level taxonomic groups like genera or families. Such rates are informative about general patterns and trends of diversification through time and across clades but can be difficult to compare directly to rates of speciation and extinction of individual species.\n\nDiversification rates can be estimated from data on the ages and diversities of monophyletic clades in the tree of life. For example, if a clade is 100 million years old and includes 1000 species, we can estimate the net diversification rate of that clade by using a formula derived from a birth-death model of diversification:\n\nEquations are also available for estimating speciation and extinction rates separately when one has ages and diversities for multiple clades.\n\nDiversification rates can be estimated using the information available in phylogenetic trees. To calculate diversification rates, such phylogenetic trees have to include branch lengths. Various methods are available to estimate speciation and extinction rates from phylogenetic trees using both maximum likelihood and Bayesian statistical approaches. One can also use phylogenetic trees to test for changing rates of speciation and/or extinction, both through time and across clades, and to associate rates of evolution with potential explanatory factors.\n\n", "id": "48719972", "title": "Diversification rates"}
{"url": "https://en.wikipedia.org/wiki?curid=49482835", "text": "Evolution of aerobic fermentation\n\nAerobic fermentation is a metabolic process by which cells metabolize sugars via fermentation in the presence of oxygen and occurs through the repression of normal respiratory metabolism (also referred to as the crabtree effect in yeast). This phenomenon is fairly rare and is primarily observed in yeasts. Aerobic fermentation evolved independently in at least three yeast lineages (\"Saccharomyces\", \"Dekkera\", \"Schizosaccharomyces\"). It has also been observed in plant pollen, trypanosomatids, mutated \"E. coli\", and tumor cells. Crabtree-positive yeasts will respire when grown with very low concentrations of glucose or when grown on most other carbohydrate sources. The Crabtree effect is a regulatory system whereby respiration is repressed by fermentation, except in low sugar conditions. When \"Saccharomyces cerevisiae\" is grown below the sugar threshold and undergoes a respiration metabolism, the fermentation pathway is still fully expressed, while the respiration pathway is only expressed relative to the sugar availability. This contrasts with the pasteur effect, which is the inhibition of fermentation in the presence of oxygen, and observed in most organisms.\n\nThe evolution of aerobic fermentation likely involved multiple successive molecular steps, which included the expansion of hexose transporter genes, copy number variation (CNV) and differential expression in metabolic genes, and regulatory reprogramming. Research is still needed to fully understand the genomic basis of this complex phenomenon. Many crabtree-positive yeast species are used for their fermentation ability in industrial processes in the production of wine, beer, sake, bread, and bioethanol. Through domestication, these yeast species have evolved, often through artificial selection, to better fit their environment. Strains evolved through mechanisms that include interspecific hybridization, horizontal gene transfer (HGT), gene duplication, pseudogenization, and gene loss.\n\nApproximately 100 million years ago (mya), within the yeast lineage there was a whole genome duplication (WGD). A majority of Crabtree-positive yeasts are post-WGD yeasts. It was believed that the WGD was a mechanism for the development of Crabtree effect in these species due to the duplication of alcohol dehydrogenase (ADH) encoding genes and hexose transporters. However, recent evidence has shown that aerobic fermentation originated before the WGD and evolved as a multi-step process, potentially aided by the WGD. The origin of aerobic fermentation, or the first step, in \"Saccharomyces\" crabtree-positive yeasts likely occurred in the interval between the ability to grow under anaerobic conditions, horizontal transfer of anaerobic DHODase (encoded by URA1 with bacteria), and the loss of respiratory chain Complex I. A more pronounced Crabtree effect, the second step, likely occurred near the time of the WGD event. Later evolutionary events that aided in the evolution of aerobic fermentation are better understood and outlined in the Genomic basis of the crabtree effect section.\n\nIt is believed that a major driving force in the origin of aerobic fermentation was its simultaneous origin with modern fruit (~125 mya). These fruit provided an abundance of simple sugar food source for microbial communities, including both yeast and bacteria. Bacteria, at that time, were able to produce biomass at a faster rate than the yeast. Producing a toxic compound, like ethanol, can slow the growth of bacteria, allowing the yeast to be more competitive. However, the yeast still had to use a portion of the sugar it consumes to produce ethanol. Crabtree-positive yeasts also have increased glycolytic flow, or increased uptake of glucose and conversion to pyruvate, which compensates for using a portion of the glucose to produce ethanol rather than biomass. Therefore, it is believed that the original driving force was to kill competitors. This is supported by research that determined the kinetic behavior of the ancestral ADH protein, which was found to be optimized to make ethanol, rather than consume it.\n\nFurther evolutionary events in the development of aerobic fermentation likely increased the efficiency of this lifestyle, including increased tolerance to ethanol and the repression of the respiratory pathway. In high sugar environments, \"S. cerevisiae\" outcompetes and dominants all other yeast species, except its closest relative \"Saccharomyces paradoxus\". The ability of \"S. cerevisiae\" to dominate in high sugar environments evolved more recently than aerobic fermentation and is dependent on the type of high-sugar environment. Other yeasts' growth is dependent on the pH and nutrients of the high-sugar environment.\n\nThe genomic basis of the crabtree effect is still being invested, and its evolution likely involved multiple successive molecular steps that increased the efficiency of the lifestyle.\n\nHexose transporters (HXT) are a group of proteins that are largely responsible for the uptake of glucose in yeast. In \"S. cerevisiae\", 20 \"HXT\" genes have been identified and 17 encode for glucose transporters (\"HXT1-HXT17\"), \"GAL2\" encodes for a galactose transporter, and \"SNF3\" and \"RGT2\" encode for glucose sensors. The number of glucose sensor genes have remained mostly consistent through the budding yeast lineage, however glucose sensors are absent from \"Schizosaccharomyces pombe\". \"Sch. pombe\" is a Crabtree-positive yeast, which developed aerobic fermentation independently from \"Saccharomyces\" lineage, and detects glucose via the cAMP-signaling pathway. The number of transporter genes vary significantly between yeast species and has continually increased during the evolution of the \"S. cerevisiae\" lineage. Most of the transporter genes have been generated by tandem duplication, rather than from the WGD. \"Sch. pombe\" also has a high number of transporter genes compared to its close relatives. Glucose uptake is believed to be a major rate-limiting step in glycolysis and replacing \"S. cerevisiae\"'s \"HXT1-17\" genes with a single chimera \"HXT\" gene results in decreased ethanol production or fully respiratory metabolism. Thus, having an efficient glucose uptake system appears to be essential to ability of aerobic fermentation. There is a significant positive correlation between the number of hexose transporter genes and the efficiency of ethanol production.\n\nAfter a WGD, one of the duplicated gene pair is often lost through fractionation; less than 10% of WGD gene pairs have remained in \"S. cerevisiae\" genome. A little over half of WGD gene pairs in the glycolysis reaction pathway were retained in post-WGD species, significantly higher than the overall retention rate. This has been associated with an increased ability to metabolize glucose into pyruvate, or higher rate of glycolysis. After glycolysis, pyruvate can either be further broken down by pyruvate decarboxylase (Pdc) or pyruvate dehydrogenase (Pdh). The kinetics of the enzymes are such that when pyruvate concentrations are high, due to a high rate of glycolysis, there is increased flux through Pdc and thus the fermentation pathway. The WGD is believed to have played a beneficial role in the evolution of the Crabtree effect in post-WGD species partially due to this increase in copy number of glycolysis genes.\n\nThe fermentation reaction only involves two steps. Pyruvate is converted to acetaldehyde by Pdc and then acetaldehyde is converted to ethanol by alcohol dehydrogenase (Adh). There is no significant increase in the number of \"Pdc\" genes in Crabtree-positive compared to Crabtree-negative species and no correlation between number of \"Pdc\" genes and efficiency of fermentation. There are five \"Adh\" genes in \"S. cerevisiae\". Adh1 is the major enzyme responsible for catalyzing the fermentation step from acetaldehyde to ethanol. Adh2 catalyzes the reverse reaction, consuming ethanol and converting it to acetaldehyde. The ancestral, or original, Adh had a similar function as Adh1 and after a duplication in this gene, Adh2 evolved a lower K for ethanol. Adh2 is believed to have increased yeast species' tolerance for ethanol and allowed Crabtree-positive species to consume the ethanol they produced after depleting sugars. However, Adh2 and consumption of ethanol is not essential for aerobic fermentation. \"Sch. pombe\" and other Crabtree positive species do not have the \"ADH2\" gene and consumes ethanol very poorly.\n\nIn Crabtree-negative species, respiration related genes are highly expressed in the presence of oxygen. However, when \"S. cerevisiae\" is grown on glucose in aerobic conditions, respiration-related gene expression is repressed. Mitochondrial ribosomal proteins expression is only induced under environmental stress conditions, specifically low glucose availability. Genes involving mitochondrial energy generation and phosphorylation oxidation, which are involved in respiration, have the largest expression difference between aerobic fermentative yeast species and respiratory species. In a comparative analysis between \"Sch. pombe\" and \"S. cerevisiae\", both of which evolved aerobic fermentation independently, the expression pattern of these two fermentative yeasts were more similar to each other than a respiratory yeast, \"C. albicans\". However, \"S. cerevisiae\" is evolutionarily closer to \"C. albicans\". Regulatory rewiring was likely important in the evolution of aerobic fermentation in both lineages.\n\nAerobic fermentation is also essential for multiple industries, resulting in human domestication of several yeast strains. Beer and other alcoholic beverages, throughout human history, have played a significant role in society through drinking rituals, providing nutrition, medicine, and uncontaminated water. During the domestication process, organisms shift from natural environments that are more variable and complex to simple and stable environments with a constant substrate. This often favors specialization adaptations in domesticated microbes, associated with relaxed selection for non-useful genes in alternative metabolic strategies or pathogenicity. Domestication might be partially responsible for the traits that promote aerobic fermentation in industrial species. Introgression and HGT is common in \"Saccharomyces\" domesticated strains. Many commercial wine strains have significant portions of their DNA derived from HGT of non-\"Saccharomyces\" species. HGT and introgression are less common in nature than is seen during domestication pressures. For example, the important industrial yeast strain \"Saccharomyces pastorianus\", is an interspecies hybrid of \"S. cerevisiae\" and the cold tolerant \"S. eubayanus.\" This hybrid is commonly used in lager-brewing, which requires slow, low temperature fermentation.\n\nAlcoholic fermentation is often used by plants in anaerobic conditions to produce ATP and regenerate NAD to allow for glycolysis to continue. For most plant tissues, fermentation only occurs in anaerobic conditions, but there are a few exceptions. In the pollen of maize (\"Zea mays\") and tobacco (\"Nicotiana tabacum\" & \"Nicotiana plumbaginifolia\"), the fermentation enzyme ADH is abundant, regardless of the oxygen level. In tobacco pollen, PDC is also highly expressed in this tissue and transcript levels are not influenced by oxygen concentration. Tobacco pollen, similar to Crabtree-positive yeast, perform high levels of fermentation dependent on the sugar supply, and not oxygen availability. In these tissues, respiration and alcoholic fermentation occur simultaneously with high sugar availability. Fermentation produces the toxic acetaldehyde and ethanol, that can build up in large quantities during pollen development. It has been hypothesized that acetaldehyde is a pollen factor that causes cytoplasmic male sterility. Cytoplasmic male sterility is a trait observed in maize, tobacco and other plants in which there is an inability to produce viable pollen. It is believed that this trait might be due to the expression of the fermentation genes, ADH and PDC, a lot earlier on in pollen development than normal and the accumulation of toxic aldehyde.\n\nWhen grown in glucose-rich media, trypanosomatid parasites degrade glucose via aerobic fermentation. In this group, this phenomenon is not a pre-adaptation to/or remnant of anaerobic life, shown through their inability to survive in anaerobic conditions. It is believed that this phenomenon developed due to the capacity for a high glycolytic flux and the high glucose concentrations of their natural environment. The mechanism for repression of respiration in these conditions is not yet known.\n\nA couple \"Escherichia coli\" mutant strains have been bioengineered to ferment glucose under aerobic conditions. One group developed the ECOM3 (\"E. coli\" cytochrome oxidase mutant) strain by removing three terminal cytochrome oxidases (cydAB, cyoABCD, and cbdAB) to reduce oxygen uptake. After 60 days of adaptive evolution on glucose media, the strain displayed a mixed phenotype. In aerobic conditions, some populations' fermentation solely produced lactate, while others did mixed-acid fermentation.\n\nOne of the hallmarks of cancer is altered metabolism or deregulating cellular energetics. Cancers cells often have reprogrammed their glucose metabolism to perform lactic acid fermentation, in the presence of oxygen, rather than send the pyruvate made through glycolysis to the mitochondria. This is referred to as the Warburg effect, and is associated with high consumption of glucose and a high rate of glycolysis. ATP production in these cancer cells is often only through the process of glycolysis and pyruvate is broken down by the fermentation process in the cell's cytoplasm. This phenomenon is often seen as counterintuitive, since cancer cells have higher energy demands due to the continued proliferation and respiration produces significantly more ATP than glycolysis alone (fermentation produces no additional ATP). Typically, there is an up-regulation in glucose transporters and enzymes in the glycolysis pathway (also seen in yeast). There are many parallel aspects of aerobic fermentation in tumor cells that are also seen in Crabtree-positive yeasts. Further research into the evolution of aerobic fermentation in yeast such as \"S. cerevisiae\" can be a useful model for understanding aerobic fermentation in tumor cells. This has a potential for better understanding cancer and cancer treatments.\n", "id": "49482835", "title": "Evolution of aerobic fermentation"}
{"url": "https://en.wikipedia.org/wiki?curid=1267220", "text": "Sympatry\n\nIn biology, two species or populations are considered sympatric when they exist in the same geographic area and thus frequently encounter one another. An initially interbreeding population that splits into two or more distinct species sharing a common range exemplifies sympatric speciation. Such speciation may be a product of reproductive isolation – which prevents hybrid offspring from being viable or able to reproduce, thereby reducing gene flow – that results in genetic divergence. Sympatric speciation does not imply secondary contact, which is speciation or divergence in allopatry followed by range expansions leading to an area of sympatry. Sympatric species or taxa in secondary contact may or may not interbreed.\n\nFour main types of population pairs exist in nature. Sympatric populations (or species) contrast with parapatric populations, which contact one another in adjacent but not shared ranges and do not interbreed; peripatric species, which are separated only by areas in which neither organism occurs; and allopatric species, which occur in entirely distinct ranges that are neither adjacent nor overlapping. Allopatric populations isolated from one another by geographical factors (e.g., mountain ranges or bodies of water) may experience genetic – and, ultimately, phenotypic – changes in response to their varying environments. These may drive allopatric speciation, which is arguably the dominant mode of speciation.\n\nThe lack of geographic isolation as a definitive barrier between sympatric species has yielded controversy among ecologists, biologists, and zoologists regarding the validity of the term. As such, researchers have long debated the conditions under which sympatry truly applies, especially with respect to parasitism. Because parasitic organisms often inhabit multiple hosts during a life cycle, evolutionary biologist Ernst Mayr stated that internal parasites existing within different hosts demonstrate allopatry, not sympatry. Today, however, many biologists consider parasites and their hosts to be sympatric (see examples below). Conversely, zoologist Michael J. D. White considered two populations sympatric if genetic interbreeding was viable within the habitat overlap. This may be further specified as sympatry occurring within one deme; that is, reproductive individuals must be able to locate one another in the same population in order to be sympatric.\n\nOthers question the ability of sympatry to result in complete speciation: until recently, many researchers considered it nonexistent, doubting that selection alone could create disparate, but not geographically separated, species. In 2003, biologist Karen McCoy suggested that sympatry can act as a mode of speciation only when \"the probability of mating between two individuals depend[s] [solely] on their genotypes, [and the genes are] dispersed throughout the range of the population during the period of reproduction\". In essence, sympatric speciation does require very strong forces of natural selection to be acting on heritable traits, as there is no geographic isolation to aid in the splitting process. Yet, recent research has begun to indicate that sympatric speciation is not as uncommon as was once assumed.\n\nSyntopy is a special case of sympatry. It means the joint occurrence of two species in the same habitat at the same time. Just as the broader term sympatry, \"syntopy\" is used especially for close species that might hybridise or even be sister species. \"Sympatric\" species occur together in the same region, but do not necessarily share the same localities as \"syntopic\" species do. Areas of syntopy are of interest because they allow to study how similar species may coexist without outcompeting each other.\n\nAs an example, the two bat species \"Myotis auriculus\" and \"M. evotis\" were found to be syntopic in North America. In contrast, the marbled newt and the northern crested newt have a large sympatric range in western France, but differ in their habitat preferences and only rarely occur syntopically in the same breeding ponds.\n\nThe lack of geographic constraint in isolating sympatric populations implies that the emerging species avoid interbreeding via other mechanisms. Before speciation is complete, two diverging populations may still produce viable offspring. As speciation progresses, isolating mechanisms – such as gametic incompatibility that renders fertilization of the egg impossible – are selected for in order to increase the reproductive divide between the two populations.\n\nSympatric groups frequently show a greater ability to discriminate between their own species and other closely related species than do allopatric groups. This is shown in the study of hybrid zones. It is also apparent in the differences in levels of prezygotic isolation (by factors that prevent formation of a viable zygote) in both sympatric and allopatric populations. There are two main theories regarding this process: 1) differential fusion, which suggests that only populations with a keen ability to discriminate between species will persist in sympatry; and 2) character displacement, which implies that distinguishing characteristics will be heightened in areas where the species co-occur in order to facilitate discrimination.\n\nReinforcement is the process by which natural selection reinforces reproductive isolation. In sympatry, reinforcement increases species discrimination and sexual adaptation in order to avoid maladaptive hybridization and encourage speciation. If hybrid offspring are either sterile or less-fit than non-hybrid offspring, mating between members of two different species will be selected against. Natural selection decreases the probability of such hybridization by selecting for the ability to identify mates of one's own species from those of another species.\n\nReproductive character displacement strengthens the reproductive barriers between sympatric species by encouraging the divergence of traits that are crucial to reproduction. Divergence is frequently distinguished by assortative mating between individuals of the two species. For example, divergence in the mating signals of two species will limit hybridization by reducing one's ability to identify an individual of the second species as a potential mate. Support for the reproductive character displacement hypothesis comes from observations of sympatric species in overlapping habitats in nature. Increased prezygotic isolation, which is associated with reproductive character displacement, has been observed in cicadas of genus \"Magicicada\", stickleback fish, and the flowering plants of the genus \"Phlox\".\n\nAn alternative explanation for species discrimination in sympatry is differential fusion. This hypothesis states that of the many species have historically come into contact with one another, the only ones that persist in sympatry (and thus are seen today) are species with strong mating discrimination. On the other hand, species lacking strong mating discrimination are assumed to have fused while in contact, forming one distinct species.\n\nDifferential fusion is less widely recognized than character displacement, and several of its implications are refuted by experimental evidence. For example, differential fusion implies greater postzygotic isolation among sympatric species, as this functions to prevent fusion between the species. However, Coyne and Orr found equal levels of postzygotic isolation among sympatric and allopatric species pairs in closely related \"Drosophila\". Nevertheless, differential fusion remains a possible, though not complete, contributor to species discrimination.\n\nSympatry has been increasingly evidenced in current research. Because of this, sympatric speciation – which was once highly debated among researchers – is progressively gaining credibility as a viable form of speciation.\n\nSeveral distinct types of killer whale (\"Orcinus orca\"), which are characterized by an array of morphological and behavioral differences, live in sympatry throughout the North Atlantic, North Pacific and Antarctic oceans. In the North Pacific, three whale populations – called \"transient\", \"resident\", and \"offshore\" – demonstrate partial sympatry, crossing paths with relative frequency. The results of recent genetic analyses using mtDNA indicate that this is due to secondary contact, in which the three types encountered one another following the bidirectional migration of \"offshore\" and \"resident\" whales between the North Atlantic and North Pacific. Partial sympatry in these whales is, therefore, not the result of speciation. Furthermore, killer whale populations that consist of all three types have been documented in the Atlantic, evidencing that interbreeding occurs among them. Thus, secondary contact does not always result in total reproductive isolation, as has often been predicted.\n\nThe parasitic great spotted cuckoo (\"Clamator glandarius\") and its magpie host, both native to Southern Europe, are completely sympatric species. However, the duration of their sympatry varies with location. For example, great spotted cuckoos and their magpie hosts in Hoya de Gaudix, southern Spain, have lived in sympatry since the early 1960s, while species in other locations have more recently become sympatric. Great spotted cuckoos, when in South Africa, are sympatric with at least 8 species of starling and 2 crows, pied crow and Cape crow.\n\nThe great spotted cuckoo exhibits brood parasitism by laying a mimicked version of the magpie egg in the magpie's nest. Since cuckoo eggs hatch before magpie eggs, magpie hatchlings must compete with cuckoo hatchlings for resources provided by the magpie mother. This relationship between the cuckoo and the magpie in various locations can be characterized as either recently sympatric or anciently sympatric. The results of an experiment by Soler and Moller (1990) showed that in areas of ancient sympatry (species in cohabitation for many generations), magpies were more likely to reject most of the cuckoo eggs, as these magpies had developed counter-adaptations that aid in identification of egg type. In areas of recent sympatry, magpies rejected comparatively fewer cuckoo eggs. Thus, sympatry can cause coevolution, by which both species undergo genetic changes due to the selective pressures that one species exerts on the other.\n\nLeafcutter ants protect and nourish various species of fungus as a source of food in a system known as ant-fungus mutualism. Leafcutter ants belonging to the genus \"Acromyrmex\" are known for their mutualistic relationship with Basidiomycete fungi. Ant colonies are closely associated with their fungus colonies, and may have co-evolved with a consistent vertical lineage of fungi in individual colonies. Ant populations defend against the horizontal transmission of foreign fungi to their fungal colony, as this transmission may lead to competitive stress on the local fungal garden. Invaders are identified and removed by the ant colony, inhibiting competition and fungal interbreeding. This active isolation of individual populations helps maintain the genetic purity of the fungal colony, and this mechanism may lead to sympatric speciation within a shared habitat.\n\n\n", "id": "1267220", "title": "Sympatry"}
{"url": "https://en.wikipedia.org/wiki?curid=1159407", "text": "Homeosis\n\nIn evolutionary developmental biology, homeosis is the transformation of one organ into another, arising from mutation in or misexpression of certain developmentally critical genes, specifically homeotic genes. In animals, these developmental genes specifically control the development of organs on their anteroposterior axis. In plants, however, the developmental genes affected by homeosis may control anything from the development of a stamen or petals to the development of chlorophyll. Homeosis may be caused by mutations in Hox genes, found in animals, or others such as the MADS-box family in plants. Homeosis is a characteristic that has helped insects become as successful and diverse as they are. \n\nHomeotic mutations work by changing segment identity during development. For example, the \"Ultrabithorax\" genotype gives a phenotype wherein metathoracic and first abdominal segments become mesothoracic segments. Another well-known example is \"Antennapedia\": a gain-of-function allele causes legs to develop in the place of antennae.\n\nIn botany, Rolf Sattler has revised the concept of homeosis (replacement) by his emphasis on partial homeosis in addition to complete homeosis; this revision is now widely accepted.\n\nHomeotic mutants in angiosperms are thought to be rare in the wild: in the annual plant \"Clarkia\" (Onagraceae), homeotic mutants are known where the petals are replaced by a second whorl of sepal-like organs, originating via a mutation governed by a single recessive gene. The absence of lethal or deleterious consequences in floral mutants resulting in distinct morphological expressions has been a factor in the evolution of Clarkia, and perhaps also in many other plant groups.\n\nFollowing the work on homeotic mutants by Ed Lewis, the phenomenology of homeosis in animals was further elaborated by discovery of a conserved DNA binding sequence present in many homeotic proteins. \nThus, the 60 amino acid DNA binding protein domain was named the homeodomain, while the 180 bp nucleotide sequence encoding it was named the homeobox. The homeobox gene clusters studied by Ed Lewis were named the Hox genes, although it should be noted that many more homeobox genes are encoded by animal genomes than those in the Hox gene clusters.\n\nThe homeotic-function of certain proteins was first postulated to be that of a \"selector\" as proposed by Antonio Garcia-Bellido. \nBy definition selectors were imagined to be (transcription factor) proteins that stably determined one of two possible cell fates for a cell and its cellular descendants in a tissue. \nWhile most animal homeotic functions are associated with homeobox-containing factors, not all homeotic proteins in animals are encoded by homeobox genes, and further not all homeobox genes are necessarily associated with homeotic functions or (mutant) phenotypes.\nThe concept of homeotic selectors was further elaborated or at least qualified by Michael Akam in a so-called \"post-selector gene\" model that incorporated additional findings and \"walked back\" the \"orthodoxy\" of selector-dependent stable binary switches.\n\nThe concept of tissue compartments is deeply intertwined with the selector model of homeosis because the selector-mediated maintenance of cell fate can be restricted into different organizational units of an animal's body plan.\nIn this context, newer insights into homeotic mechanisms were found by Albert Erives and colleagues by focusing on enhancer DNAs that are co-targeted by homeotic selectors and different combinations of developmental signals.\nThis work identifies a protein biochemical difference between the transcription factors that function as homeotic selectors versus the transcription factors that function as effectors of developmental signaling pathways, such as the Notch signaling pathway and the BMP signaling pathway.\nThis work proposes that homeotic selectors function to \"license\" enhancer DNAs in a restricted tissue compartment so that the enhancers are enabled to read-out developmental signals, which are then integrated via polyglutamine-mediated aggregation.\n\nLike the complex multicellularity seen in animals, the multicellularity of land plants is developmentally organized into tissue and organ units via transcription factor genes with homeotic effects.\nWhile plants do have homeobox-containing genes, plant homeotic factors tend to possess MADS-box DNA binding domains.\nInterestingly, animal genomes also possess a small number MADS-box factors.\nThus, in the independent evolution of multicellularity in plants and animals, different eukaryotic transcription factor families were co-opted to serve homeotic functions. \nMADS-domain factors have been proposed to function as co-factors to more specialized factors and thereby help to determine organ identity.\nThis has been proposed to correspond more closely to the interpretation of animal homeotics outlined by Michael Akam.\n\n", "id": "1159407", "title": "Homeosis"}
{"url": "https://en.wikipedia.org/wiki?curid=49021319", "text": "TALE-likes\n\nTranscription Activator Like Effector Likes (TALE-likes) are a group of bacterial DNA binding proteins named for the first and still best studied group, the TALEs of \"Xanthomonas\" bacteria. TALEs are important factors in the plant diseases caused by \"Xanthomonas\" bacteria, but are known primarily for their role in biotechnology as programmable DNA binding proteins, particularly in the context of TALE nucleases. TALE-likes have additionally been found in many strains of the \"Ralstonia solanacearum\" bacterial species complex, in \"Burkholderia rhizoxinica\" strain HKI 454, and in two unknown marine bacteria. Whether or not all these proteins from a single phylogenetic grouping is as yet unclear.\n\nThe unifying feature of the TALE-likes are their tandem arrays of DNA binding repeats. These repeats are, with few exceptions, 33-35 amino acids in length, and composed of two alpha-helices on either side of a flexible loop containing the DNA base binding residues and with neighbouring repeats joined by flexible linker loops. Evidence for this common structure comes in part from solved crystal structures of TALEs and a \"Burkholderia\" TALE-like, but also from the conservation of the code that all TALE-likes use to recognise DNA-sequences.\n\nTALEs are the first identified, best-studied and largest group within the TALE-likes. TALEs are found throughout the bacterial genus \"Xanthomonas\", comprising mostly plant pathogens. Those TALEs which have been studied have all been shown to be secreted as part of the Type III secretion system into host plant cells. Once inside the host cell they translocate to the nucleus, bind specific DNA sequences within host promoters and turn on downstream genes. Every part of this process is thought to be conserved across all TALEs. The single meaningful difference between individual TALEs, based on current understanding, is the specific DNA sequence that each TALE binds. TALEs from even closely related strains differ in the composition of repeats that make up their DNA binding domain. Repeat composition determines DNA binding preference. In particular position 13 of each repeat confers the DNA base preference of each repeat. During early research it was noted that almost all the differences between repeats of a single TALE repeat array are found in positions 12 and 13 and this finding led to the hypothesis that these residues determine base preference. In fact repeat positions 12 and 13, referred to jointly as the Repeat Variable Diresidue (RVD) are commonly said to confer base specificity despite clear evidence that position 13 is the base determining residue. In addition to the repeat domain TALEs also possess a number of conserved features in the domains flanking the repeats. These include domains for type-III-secretion, nuclear localization and transcriptional activation. This allows TALEs to carry out their biological role as effector proteins secreted into host plant cells to activate expression of specific host genes.\n\nDiversity and evolution\n\nWhilst the RVD positions are commonly the only variable positions within a single TALE repeat array it should be noted that there are more differences when comparing repeat arrays of different TALEs. The diversity of TALEs across the Xanthomonas genus is considerable, but a particularly striking finding is that the evolutionary history one arrives at by comparing repeat compositions differs from that found when comparing non-repeat sequences. Repeat arrays of TALEs are thought to evolve rapidly, with a number of recombinatorial processes suggested to shape repeat array evolution. Recombination of TALE repeat arrays has been demonstrated in a forced-selection experiment. This evolutionary dynamism is though to be made possible by the very high sequence identity of TALE repeats, which is a unique feature of TALEs as opposed to other TALE-likes.\n\nT-zero\n\nAnother unique feature of TALEs is a set of four repeat structures at the N-terminal flank of the core repeat array. These structures, termed non-canonical or degenerate repeats have been shown to be vital for DNA binding, though all but one do not contact DNA bases and thus make no contribution to sequence preference. The one exception is repeat -1, which encodes a fixed T-zero preference to all TALEs. This means that the target sequences of TALEs are always preceded by a thymine base. This is thought to be common to all TALEs, with the possible exception of TalC from \"Xanthomonas oryzae pv. oryzae\" strain AXO1947.\n\nDiscovery and molecular properties\n\nIt was noted in the 2002 publication of the genome of reference strain \"Ralstonia solanacearum\" GMI1000 that its genome encodes a protein similar to \"Xanthomonas\" TALEs. Based on similar domain structure and repeat sequences it was presumed that this gene and homologs in other \"Ralstonia\" strains would encode proteins with the same molecular properties as TALEs, including sequence-specific DNA binding. In 2013 this was confirmed by two studies. These genes and the proteins they encode are referred to as RipTALs (Ralstonia injected protein TALE-like) in line with the standard nomenclature of Ralstonia effectors. Whilst the DNA binding code of the core repeats is conserved with TALEs, RipTALs do not share the T-zero preference, instead they have a strict G-zero requirement. In addition repeats within a single RipTAL repeat array have multiple sequence differences beyond the RVD positions, unlike the near-identical repeats of TALEs.\n\nBiological role\n\nSeveral lines of evidence support the idea that RipTALs function as effector proteins, promoting bacterial growth or disease by manipulating the expression of plant genes. They are secreted into plant cells by the Type III secretion system, which is the main delivery system for effector proteins. They are able to function as sequence-specific transcription factors in plant cells. In addition a strain lacking its RipTAL was shown to grow slower inside eggplant leaf tissue than the wild type. Furthermore, a study based on DNA polymorphisms in \"ripTAL\" repeat domain sequences and host plants found a statistically significant connection between host plant and repeat domain variants. This is expected if the RipTALs of different strains are adapted to target genes in specific host plants. Despite this to date no target genes have been identified for any RipTAL.\n\nDiscovery\n\nThe publication of the genome of bacterial strain \"Bukrholderia rhizoxinica\" HKI 454, in 2011 led to the discovery of a set of TALE-like genes that differed considerably in nature from the TALEs and RipTALS. The proteins encoded by these genes were studied for their DNA binding properties by two groups independently and named the Bats (Burkholderia TALE-likes ) or BurrH. This research showed that the repeat units of the \"Burkholderia\" TALE-likes bind DNA with the same code as TALEs, governed by position 13 of each repeat. There are, however, a number of differences.\n\nBiological role\n\n\"Burkholderia\" TALE-likes are composed almost entirely of repeats, lacking the large non-repetitive domains found flanking the repeats in TALEs and RpTALs. Those domains are key to the functions of TALEs and RipTALs allowing them to infiltrate the plant nucleus and turn on gene expression. It is therefore currently unclear what the biological roles of \"Burkholderia\" TALE-likes are. What is clear is that they are not effector proteins secreted into plant cells to act as transcription factors, the biological role of TALEs and RipTALs. It is not unexpected that they may differ in biological roles from TALEs and RipTALs since the life style of the bacterium they derive from is very unlike that of TALE and RipTAL bearing bacteria. \"B. rhizoxinica\" is an endosymbiont, living inside a fungus, \"Rhizopus microsporus\", a plant pathogen. The same fungus is also an opportunistic human pathogen in immuno-compromised patients, but whereas \"B. rhizoxinica\" is necessary for pathogenicity on plant hosts it is irrelevant to human infection. It is unclear whether the \"Burkholderia\" TALE-likes are ever secreted either into the fungus, let alone into host plants.\nUses in Biotechnology\n\nAs noted in the publications on \"Burkholderia\" TALE-likes there may be some advantages to using these proteins as a scaffold for programmable DNA-binding proteins to function as transcription factors or designer-nucleases, compared to TALEs. These advantages are a shorter repeat size, more compact domain structure (no large non-repeat domains), greater repeat sequence diversity enabling the use of PCR on the genes encoding them and making them less vulnerable to recombinatorial repeat loss. In addition Burkholderia TALE-likes have no T-zero requirement relaxing the constraints on DNA target selection. However, to uses of Burkholderia TALE-likes as programmable DNA binding proteins have been published, outside of the original characterization publications.\n\nDiscovery\n\nIn 2007 the results of a sweep of the world's oceans by the J. Craig Venter Institute were made publicly available. The paper in 2014 on \"Burkholderia\" TALE-likes was also the first to report that two entries from that database resembled TALE-likes, based on sequence similarity. These were further characterized and assessed for their DNA-binding potential in 2015. The repeat units encoded by these sequences were found to mediate DNA binding with base preference matching the TALE code, and judged likely to form structures nearly identical to Bat1 repeats based on molecular dynamics simulations. The proteins encoded by these DNA sequences were therefore designated Marine Organism TALE-likes (MOrTLs) 1 and 2.\n\nEvolutionary relationship to other TALE-likes\n\nWhilst repeats of MOrTL1 and 2 both conform structurally and functionally to the TALE-like norm, they differ considerably at the sequence level both from all other TALE-likes and from one another. It is not known whether they are truly homologous to the other TALE-likes, and thus constitute together with the TALEs, RipTALs and Bats a true protein-family. Alternatively they may have evolved independently. It is particularly difficult to judge the relationship to the other TALE-likes because almost nothing is known of the organisms that MOrTL1 and MOrTL2 come from. It is known only that they were found in two separate sea-water samples from the Gulf of Mexico and are likely to be bacteria based on size-exclusion before DNA sequencing.\n", "id": "49021319", "title": "TALE-likes"}
{"url": "https://en.wikipedia.org/wiki?curid=7917758", "text": "Software evolution\n\nSoftware evolution is the term used in software engineering (specifically software maintenance) to refer to the process of developing software initially, then repeatedly updating it for various reasons.\n\nFred Brooks, in his key book \"The Mythical Man-Month\", states that over 90% of the costs of a typical system arise in the maintenance phase, and that any successful piece of software will inevitably be maintained.\n\nIn fact, Agile methods stem from maintenance-like activities in and around web based technologies, where the bulk of the capability comes from frameworks and standards.\n\nSoftware maintenance address bug fixes and minor enhancements and software evolution focus on adaptation and migration.\n\nThe aim of software evolution would be to implement (and revalidate) the possible major changes to the system without being able \"a priori\" to predict how user requirements will evolve. \nThe existing larger system is never complete and continues to evolve. As it evolves, the complexity of the system will grow unless there is a better solution available to solve these issues. The main objectives of software evolution are ensuring the reliability and flexibility of the system. During the 20 years past, the lifespan of a system could be on average 6–10 years. However, it was recently found that a system should be evolved once every few months to ensure it is adapted to the real-world environment. This is due to the rapid growth of World Wide Web and Internet Resources that make it easier for users to find related information. The idea of software evolution leads to open source development as anybody could download the source codes and hence modify it. The positive impact in this case is large amounts of new ideas would be discovered and generated that aims the system to have better improvement in variety choices.\n\nOver time, software systems, programs as well as applications, continue to develop. These changes will require new laws and theories to be created and justified. Some models as well would require additional aspects in developing future programs. Innovations and improvements do increase unexpected form of software development. The maintenance issues also would probably change as to adapt to the evolution of the future software. \nSoftware process and development are an ongoing experience that has a never-ending cycle. After going through learning and refinements, it is always an arguable issue when it comes to matter of efficiency and effectiveness of the programs.\n\nE.B. Swanson initially identified the \nthree categories of maintenance: corrective, adaptive, and perfective. Four categories of software were then catalogued by Lientz and Swanson (1980).\nThese have since been updated and normalized internationally in the ISO/IEC 14764:2006:\n\n\nAll of the preceding take place when there is a known requirement for change.\n\nAlthough these categories were supplemented by many authors like Warren et al. (1999) and Chapin (2001), the ISO/IEC 14764:2006 international standard has kept the basic four categories.\n\nMore recently the description of software maintenance and evolution has been done using ontologies (Kitchenham et al. (1999), Deridder (2002), Vizcaíno (2003), Dias (2003), and Ruiz (2004)), which enrich the description of the many evolution activities.\n\nCurrent trends and practices are projected forward using a new model of software evolution called the staged model [1]. Staged model was introduced to replace conventional analysis which is less suitable for modern software development is rapid changing due to its difficulties of hard to contribute in software evolution. There are five distinct stages contribute in simple staged model (Initial development, Evolution, Servicing, Phase-out, and Close-down). \n\nProf. Meir M. Lehman, who worked at Imperial College London from 1972 to 2002, and his colleagues have identified a set of behaviours in the evolution of proprietary software. These behaviours (or observations) are known as Lehman's Laws, and there are eight of them:\n\n\nIt is worth mentioning that the applicability of all of these laws for all types of software systems has been studied by several researchers. For example, see a presentation by Nanjangud C Narendra where he describes a case study of an enterprise Agile project in the light of Lehman’s laws of software evolution. Some empirical observations coming from the study of open source software development appear to challenge some of the laws .\n\nThe laws predict that the need for functional change in a software system is inevitable, and not a consequence of incomplete or incorrect analysis of requirements or bad programming. They state that there are limits to what a software development team can achieve in terms of safely implementing changes and new functionality.\n\nMaturity Models specific to software evolution have been developed to improve processes, and help to ensure continuous rejuvenation of the software as it evolves iteratively.\n\nThe \"global process\" that is made by the many stakeholders (e.g. developers, users, their managers) has many feedback loops. The evolution speed is a function of the feedback loop structure and other characteristics of the global system. Process simulation techniques, such as system dynamics can be useful in understanding and managing such global process.\n\nSoftware evolution is not likely to be Darwinian, Lamarckian or Baldwinian, but an important phenomenon on its own. Given the increasing dependence on software at all levels of society and economy, the successful evolution of software is becoming increasingly critical. This is an important topic of research that hasn't received much attention.\n\nThe evolution of software, because of its rapid path in comparison to other man-made entities, was seen by Lehman as the \"fruit fly\" of the study of the evolution of artificial systems.\n\n\n\n", "id": "7917758", "title": "Software evolution"}
{"url": "https://en.wikipedia.org/wiki?curid=32128", "text": "Uniformitarianism\n\nUniformitarianism, also known as the Doctrine of Uniformity, is the assumption that the same natural laws and processes that operate in the universe now have always operated in the universe in the past and apply everywhere. It refers to invariance in the principles underpinning science, such as the constancy of causality, or causation, throughout time, but it has also been used to describe invariance of physical laws through time and space. Though an unprovable postulate that cannot be verified using the scientific method, uniformitarianism has been a key first principle of virtually all fields of science.\n\nIn geology, uniformitarianism has included the gradualistic concept that \"the present is the key to the past\" (that events occur at the same rate now as they have always done); many geologists now, however, no longer hold to a strict theory of gradualism. Coined by William Whewell, the word was proposed in contrast to catastrophism by British naturalists in the late 18th century, starting with the work of the geologist James Hutton. Hutton's work was later refined by scientist John Playfair and popularised by geologist Charles Lyell's \"Principles of Geology\" in 1830. Today, Earth's history is considered to have been a slow, gradual process, punctuated by occasional natural catastrophic events.\n\nThe earlier conceptions likely had little influence on 18th-century European geological explanations for the formation of Earth. Abraham Gottlob Werner (1749–1817) proposed Neptunism, where strata represented deposits from shrinking seas precipitated onto primordial rocks such as granite. In 1785 James Hutton proposed an opposing, self-maintaining infinite cycle based on natural history and not on the Biblical account.\nHutton then sought evidence to support his idea that there must have been repeated cycles, each involving deposition on the seabed, uplift with tilting and erosion, and then moving undersea again for further layers to be deposited. At Glen Tilt in the Cairngorm mountains he found granite penetrating metamorphic schists, in a way which indicated to him that the presumed primordial rock had been molten after the strata had formed. He had read about angular unconformities as interpreted by Neptunists, and found an unconformity at Jedburgh where layers of greywacke in the lower layers of the cliff face have been tilted almost vertically before being eroded to form a level plane, under horizontal layers of Old Red Sandstone. In the spring of 1788 he took a boat trip along the Berwickshire coast with John Playfair and the geologist Sir James Hall, and found a dramatic unconformity showing the same sequence at Siccar Point. Playfair later recalled that \"the mind seemed to grow giddy by looking so far into the abyss of time\", and Hutton concluded a 1788 paper he presented at the Royal Society of Edinburgh, later rewritten as a book, with the phrase \"we find no vestige of a beginning, no prospect of an end\".\n\nBoth Playfair and Hall wrote their own books on the theory, and for decades robust debate continued between Hutton's supporters and the Neptunists. Georges Cuvier's paleontological work in the 1790s, which established the reality of extinction, explained this by local catastrophes, after which other fixed species repopulated the affected areas. In Britain, geologists adapted this idea into \"diluvial theory\" which proposed repeated worldwide annihilation and creation of new fixed species adapted to a changed environment, initially identifying the most recent catastrophe as the biblical flood.\n\nFrom 1830 to 1833 Charles Lyell's multi-volume \"Principles of Geology\" was published. The work's subtitle was \"An attempt to explain the former changes of the Earth's surface by reference to causes now in operation\". He drew his explanations from field studies conducted directly before he went to work on the founding geology text, and developed Hutton's idea that the earth was shaped entirely by slow-moving forces still in operation today, acting over a very long period of time. The terms \"uniformitarianism\" for this idea, and \"catastrophism\" for the opposing viewpoint, were coined by William Whewell in a review of Lyell's book. \"Principles of Geology\" was the most influential geological work in the middle of the 19th century.\n\nAccording to Reijer Hooykaas (1963), Lyell's uniformitarianism is a family of four related propositions, not a single idea:\n\nNone of these connotations requires another, and they are not all equally inferred by uniformitarians.\n\nGould explained Lyell's propositions in \"Time's Arrow, Time's Cycle\" (1987), stating that Lyell conflated two different types of propositions: a pair of methodological assumptions with a pair of substantive hypotheses. The four together make up Lyell's uniformitarianism.\n\nThe two methodological assumptions below are accepted to be true by the majority of scientists and geologists. Gould claims that these philosophical propositions must be assumed before you can proceed as a scientist doing science. \"You cannot go to a rocky outcrop and observe either the constancy of nature's laws or the working of unknown processes. It works the other way around.\" You first assume these propositions and \"then you go to the outcrop.\"\n\n\n\nThe substantive hypotheses were controversial and, in some cases, accepted by few. These hypotheses are judged true or false on empirical grounds through scientific observation and repeated experimental data. This is in contrast with the previous two philosophical assumptions that come before one can do science and so cannot be tested or falsified by science.\n\n\n\nGeoscientists support diverse systems of Earth history, the nature of which rest on on certain mixture of views about process, control, rate, and state which are preferred. Because geologists and geomorphologists tend to adopt opposite views over process, rate and state in the inorganic world, there are eight different systems of beliefs in the development of the terrestrial sphere. All geoscientists stand by the principle of uniformity of law. Most, but not all, are directed by the principle of simplicity. All make definite assertions about the quality of rate and state in the inorganic realm.\n\nStephen Jay Gould's first scientific paper, \"Is uniformitarianism necessary?\" (1965), reduced these four assumptions to two. He dismissed the first principle, which asserted spatial and temporal invariance of natural laws, as no longer an issue of debate. He rejected the third (uniformity of rate) as an unjustified limitation on scientific inquiry, as it constrains past geologic rates and conditions to those of the present. So, Lyellian uniformitarianism was unnecessary.\n\nUniformitarianism was proposed in contrast to catastrophism, which states that the distant past \"consisted of epochs of paroxysmal and catastrophic action interposed between periods of comparative tranquility\" Especially in the late 19th and early 20th centuries, most geologists took this interpretation to mean that catastrophic events are not important in geologic time; one example of this is the debate of the formation of the Channeled Scablands due to the catastrophic Missoula glacial outburst floods. An important result of this debate and others was the re-clarification that, while the same principles operate in geologic time, catastrophic events that are infrequent on human time-scales can have important consequences in geologic history.\nDerek Ager has noted that \"geologists do not deny uniformitarianism in its true sense, that is to say, of interpreting the past by means of the processes that are seen going on at the present day, so long as we remember that the periodic catastrophe is one of those processes. Those periodic catastrophes make more showing in the stratigraphical record than we have hitherto assumed.\"\n\nEven Charles Lyell thought that ordinary geological processes would cause Niagara Falls to move upstream to Lake Erie within 10,000 years, leading to catastrophic flooding of a large part of North America.\n\nModern geologists do not apply uniformitarianism in the same way as Lyell. They question if rates of processes were uniform through time and only those values measured during the history of geology are to be accepted. The present may not be a long enough key to penetrate the deep lock of the past. Geologic processes may have been active at different rates in the past that humans have not observed. \"By force of popularity, uniformity of rate has persisted to our present day. For more than a century, Lyell's rhetoric conflating axiom with hypotheses has descended in unmodified form. Many geologists have been stifled by the belief that proper methodology includes an a priori commitment to gradual change, and by a preference for explaining large-scale phenomena as the concatenation of innumerable tiny changes.\"\n\nThe current consensus is that Earth's history is a slow, gradual process punctuated by occasional natural catastrophic events that have affected Earth and its inhabitants. In practice it is reduced from Lyell's conflation, or blending, to simply the two philosophical assumptions. This is also known as the principle of geological actualism, which states that all past geological action was like all present geological action. The principle of actualism is the cornerstone of paleoecology.\n\n\n\n", "id": "32128", "title": "Uniformitarianism"}
{"url": "https://en.wikipedia.org/wiki?curid=52408383", "text": "Endogenosymbiosis\n\nEndogenosymbiosis is an evolutionary process, proposed by the evolutionary and environmental biologist Roberto Cazzolla Gatti, in which \"gene carriers\" (viruses, retroviruses and bacteriophages) and symbiotic prokaryotic cells (bacteria or archaea) could share parts or all of their genomes in an endogenous symbiotic relationship with their hosts.\n\nThe related process of symbiogenesis or endosymbiosis was proposed by Lynn Margulis in 1967. She argued that the internal symbiosis of bacteria-like organisms had formed organelles like chloroplasts and mitochondria. She proposed that this had created the eukaryotes, and thus driven the expansion of life on Earth. She had argued that this process of symbiotic collaboration had run alongside the classical Darwinian cycle of mutation, natural selection and adaptation.\n\nRoberto Cazzolla Gatti, Ph.D., associate professor at Tomsk State University (Russia), argued in his hypothesis that \"the main likely cause of the evolution of sexual reproduction, the parasitism, also represents the origin of biodiversity\". \nIn other terms, this theory suggests that sexual reproduction acts as a conservative system against the inclusion of new genetic variations into cells' DNA (supported by the DNA repair systems) and, instead, the evolution of species can take place only when this preservative system fails to contrast the inclusion, within the host genome, of hexogen parts of DNA (and RNA) coming from obliged \"parasitic\" elements (viruses and phages) that establish a symbiosis with their hosts. \n\"As two parallel evolutionary lines – Cazzolla Gatti wrote in his original paper – sexual reproduction seems to preserve what the endogenosymbiosis moves to diversify. Following the former process, the species can adapt slowly and indefinitely to the external factors, adjusting themselves, but not 'creating' novelty. The latter process, instead, leads to the speciation due to sudden changes in genes sequences. Not only organelles can be symbiotic with other cells, as suggested Lynn Margulis, but entire pieces of genetic material coming from symbiotic parasites, can be included in the host DNA, changing the gene expression and addressing the speciation process\".\n\nThis idea challenges the canonical natural selection models based on the gradualism of the mutation-adaptation pattern, providing more support to the punctuated equilibrium theory proposed by Stephen Jay Gould and Niles Eldredge.\n\nTwo independent studies provide support for the hypothesis. Jamie E. Henzy and Welkin E. Johnson demonstrated that the complex evolutionary history of the IFIT (Interferon Induced proteins with Tetratricopeptide repeats) family of antiviral genes has been shaped by continuous interactions between mammalian hosts and their many viruses.\n\nDavid Enard and colleagues estimated that viruses have driven close to 30% of all adaptive amino acid changes in the part of the human proteome conserved within mammals. Their results suggest that viruses are one of the most dominant drivers of evolutionary change across mammalian and human proteomes.\n\nPreviously, it was estimated that about 7–8% percent of the entire human genome carry about 100,000 pieces of DNA that came from endogenous retroviruses. This may be an underestimate.\n\nIn 2016 the biologists Sarah R. Bordestein and Seth R. Bordestein reported that genes are frequently transferred between hosts and parasites. Eukaryotic genes are often co-opted by viruses and bacterial genes are commonly found in bacteriophages. The presence of bacteriophages in symbiotic bacteria that obligately reside in eukaryotes may promote eukayotic DNA transfers to bacteriophages.\n", "id": "52408383", "title": "Endogenosymbiosis"}
{"url": "https://en.wikipedia.org/wiki?curid=53182010", "text": "Walter Aubrey Kidd\n\nDr Walter Aubrey Kidd MD FRSE MRCS FZS (1853-1929) was a British physician and medical author.\n\nHe was born in Blackheath, London on 20 July 1852, the son of Dr Joseph Kidd and his wife, Sophia McKern. His brothers included Dr Percy Kidd. He was educated at Rottingdean then Uppingham School.\n\nHe followed in the family tradition and studied Medicine at Corpus Christi College in Cambridge from 1870 later gaining his doctorate (MD) at the University of London. He worked at Guy's Hospital in London and as a GP in Blackheath.\n\nIn 1907 he was elected a Fellow of the Royal Society of Edinburgh. His proposers were Daniel John Cunningham, Charles E. S. Phillips, Ramsay Heatley Traquair and George Archdall O'Brien Reid.\n\nHis book Initiatives in Evolution looked at very odd elements of evolution such as the evolution of human hair.\n\nHe retired in 1915 and moved to Cheltenham in 1918. He moved to South Africa in 1927.\n\nHe died of heart failure on 21 February 1929 at Peak's View in the Rondebosch suburb of Cape Town in South Africa.\n\nHe married Alice Harriet Benn (d.1947) in 1881.\n\nThey had four children: Alice Sophie Kidd, Walter Shirley Kidd, Edward Aubrey Kidd, and Hubert John Kidd.\n\n", "id": "53182010", "title": "Walter Aubrey Kidd"}
{"url": "https://en.wikipedia.org/wiki?curid=1658929", "text": "Adrian Bejan\n\nAdrian Bejan is an American professor who has made contributions to modern thermodynamics and developed what he calls the constructal law. He is J. A. Jones Distinguished Professor of Mechanical Engineering at Duke University and author of the 2016 book \"The Physics of Life: The Evolution of Everything\".\n\nBejan was born in Galaţi, a port town located on the Danube in Romania.\nHis mother, Marioara Bejan (1914–1998), was a pharmacist. His father, Dr. Anghel Bejan (1910–1976), was a veterinarian. Bejan showed an early talent in drawing, and his parents enrolled him in art school. He also excelled in basketball, which earned him a position on the Romanian national basketball team.\n\nAt age 19 Bejan won a scholarship to the United States and entered Massachusetts Institute of Technology in Cambridge, Massachusetts. In 1972 he was awarded BS and MS degrees as a member of the Honors Course in Mechanical Engineering. He graduated in 1975 with a PhD from MIT with a thesis titled \"Improved thermal design of the cryogenic cooling system for a superconducting synchronous generator\". His advisor was Joseph L. Smith Jr.\n\nFrom 1976 to 1978 Bejan was a Miller research fellow in at the University of California Berkeley working with Chang-Lin Tien. In 1978 he moved to Colorado and joined the faculty of the Department of Mechanical Engineering at the University of Colorado in Boulder. In 1982 Bejan published his first book, \"Entropy Generation Through Heat and Fluid Flow\". The book is aimed at practical applications of the second law of thermodynamics, and presented his ideas on irreversibility, availability and exergy analysis in a form for engineers. In 1984 he published \"Convection Heat Transfer\"'. In an era when researchers did heat transfer calculations using numerical methods on supercomputers, the book emphasized new research methods such as intersection of asymptotes, heatlines, and scale analysis to solve problems.\n\nBejan was appointed full professor at Duke University in 1984. In 1988 he published the first edition of his textbook \"Advanced Engineering Thermodynamics\". The book combined thermodynamics theory with engineering heat transfer and fluid mechanics, and introduced entropy generation minimization as a method of optimization. In 1996 the ASME awarded him the Worcester Reed Warner Medal for \"originality, challenges to orthodoxy, and impact on thermodynamics and heat transfer, which were made through his first three books\".\n\nIn 1989 Bejan was appointed the J. A. Jones Distinguished Professor of Mechanical Engineering. In 1988 and 1989, his peers named two dimensionless groups \"Bejan number (Be),\" in two different fields: for the pressure difference group, in heat transfer by forced convection, and for the dimensionless ratio of fluid friction irreversibility divided by heat transfer irreversibility, in thermodynamics. From 1992 to 1996 he published four more books, \"Convection in Porous Media\", \"Heat Transfer\", \"Thermal Design and Optimization\" and \"Entropy Generation Minimization\".\n\nIn 1995 while reviewing entropy generation minimization for a symposium paper and writing another paper on the cooling of electronic components, Bejan formulated his self-described constructal law. Where electronic components are too small for convective cooling, they must be designed for efficient conduction. The paper provides a method for efficiently designing conductive paths, from smaller paths leading to larger ones. The similarity of the solution to the branching structures seen in multiple inanimate and living things led to his statement of what he calls a new law of nature: \"For a finite-size system to persist in time (to live), it must evolve in such a way that it provides easier access to the imposed (global) currents that flow through it.\" To emphasize the coming together of paths he called the theory \"constructal\" from the Latin \"to build\", in contrast with approaches using \"fractal\" geometry, from the Latin \"to break\".\n\nBejan incorporated his constructal law into the second edition of his textbook, \"Advanced Engineering Thermodynamics\" (1997). Since then he has concentrated on constructal law and its applications. In 2004 he published \"Porous and Complex Flow Structures in Modern Technologies\". The same year he and Sylvie Lorente were awarded the Edward F. Obert Award by the ASME for their paper \"Thermodynamic Formulation of the Constructal Law\" In 2008 he published \"Design with Constructal Theory\", a textbook for the course he developed with Lorente at Duke. In 2011 the American Society of Mechanical Engineers presented him with an honorary membership. He was cited for \"an extraordinary record of creative work, including the unification of thermodynamics and heat transfer; the conceptual development of design as a science that unites all fields; legendary contributions to engineering education; and, since 1996, the discovery and continued development of the constructal law.\"\n\nBejan has also written books for the general audience. In 2012 he published \"Design in Nature: How the Constructal Law Governs Evolution in Biology, Technology, and Social Organization\" and 2016 \"The Physics of Life: The Evolution of Everything\". He credits these books for his award of the Ralph Coats Roe Medal from the ASME in 2017. He was cited for \"permanent contributions to the public appreciation of the pivotal role of engineering in an advanced society through outstanding accomplishments as an engineering scientist and educator, renowned communicator and prolific writer\".\n\nIn November 2017 the Franklin Institute of Philadelphia announced that Bejan would be awarded the 2018 Benjamin Franklin Medal in Mechanical Engineering. He was cited for \"his pioneering interdisciplinary contributions in thermodynamics and convection heat transfer that have improved the performance of engineering systems, and for constructal theory, which predicts natural design and its evolution in engineering, scientific, and social systems.\" \n\nBejan has received multiple awards and honorary degrees.\n\n\n\n\n\n", "id": "1658929", "title": "Adrian Bejan"}
{"url": "https://en.wikipedia.org/wiki?curid=53850852", "text": "Phylosymbiosis\n\nIn the field of microbiome research, a group of species is said to show a pylosymbiotic signal if the degree of similarity between the species' microbiomes recapitulates to a significant extent their evolutionary history.\nIn other words, a phylosymbiotic signal among a group of species is evident if their microbiome similarity dendrogram could significantly\nrecapitulate their hosts phylogenic tree. For the analysis of the phylosymbiotic signal to be reliable, environmental differences\nthat could shape the host microbiome should be either eliminated or accounted for.\nOne plausible mechanistic explanation for such phenomena could be, for example, a result of host immune genes that rapidly evolve in a continues arms race with members of its microbiome.\n\n", "id": "53850852", "title": "Phylosymbiosis"}
{"url": "https://en.wikipedia.org/wiki?curid=53867278", "text": "Coloration evidence for natural selection\n\nAnimal coloration provided important early evidence for evolution by natural selection, at a time when little direct evidence was available. Three major functions of coloration were discovered in the second half of the 19th century, and subsequently used as evidence of selection: camouflage (protective coloration); mimicry, both Batesian and Müllerian; and aposematism.\n\nCharles Darwin's \"On the Origin of Species\" was published in 1859, arguing from circumstantial evidence that selection by human breeders could produce change, and that since there was clearly a struggle for existence, that natural selection must be taking place. But he lacked an explanation either for genetic variation or for heredity, both essential to the theory. Many alternative theories were accordingly considered by biologists, threatening to undermine Darwinian evolution.\n\nSome of the first evidence was provided by Darwin's contemporaries, the naturalists Henry Walter Bates and Fritz Müller. They described forms of mimicry that now carry their names, based on their observations of tropical butterflies. These highly specific patterns of coloration are readily explained by natural selection, since predators such as birds which hunt by sight will more often catch and kill insects that are less good mimics of distasteful models than those that are better mimics; but the patterns are otherwise hard to explain. \nDarwinists such as Alfred Russel Wallace and Edward Bagnall Poulton, and in the 20th century Hugh Cott and Bernard Kettlewell, sought evidence that natural selection was taking place. Wallace noted that snow camouflage, especially plumage and pelage that changed with the seasons, suggested an obvious explanation as an adaptation for concealment. Poulton's 1890 book, \"The Colours of Animals\", written during Darwinism's lowest ebb, used all the forms of coloration to argue the case for natural selection. Cott described many kinds of camouflage, and in particular his drawings of coincident disruptive coloration in frogs convinced other biologists that these deceptive markings were products of natural selection. Kettlewell experimented on peppered moth evolution, showing that the species had adapted as pollution changed the environment; this provided compelling evidence of Darwinian evolution.\n\nCharles Darwin published \"On the Origin of Species\" in 1859, arguing that evolution in nature must be driven by natural selection, just as breeds of domestic animals and cultivars of crop plants were driven by artificial selection.\nDarwin's theory radically altered popular and scientific opinion about the development of life. However, he lacked evidence and explanations for some critical components of the evolutionary process. He could not explain the source of variation in traits within a species, and did not have a mechanism of heredity that could pass traits faithfully from one generation to the next. This made his theory vulnerable; alternative theories were being explored during the eclipse of Darwinism; and so Darwinian field naturalists like Wallace, Bates and Müller looked for clear evidence that natural selection actually occurred. Animal coloration, readily observable, soon provided strong and independent lines of evidence, from camouflage, mimicry and aposematism, that natural selection was indeed at work. The historian of science Peter J. Bowler wrote that Darwin's theory\n\nIn his 1889 book \"Darwinism\", the naturalist Alfred Russel Wallace considered the white coloration of Arctic animals. He recorded that the Arctic fox, Arctic hare, ermine and ptarmigan change their colour seasonally, and gave \"the obvious explanation\", that it was for concealment. The modern ornithologist W. L. N. Tickell, reviewing proposed explanations of white plumage in birds, writes that in the ptarmigan \"it is difficult to escape the conclusion that cryptic brown summer plumage becomes a liability in snow, and white plumage is therefore another cryptic adaptation.\" All the same, he notes, \"in spite of winter plumage, many Ptarmigan in NE Iceland are killed by Gyrfalcons throughout the winter.\"\n\nIn the words of camouflage researchers I. C. Cuthill and A. Székely, the English zoologist and camouflage expert Hugh Cott's 1940 book \"Adaptive Coloration in Animals\" provided \"persuasive arguments for the survival value of coloration, and for adaptation in general, at a time when natural selection was far from universally accepted within evolutionary biology.\" In particular, they argue, \"Coincident Disruptive Coloration\" (one of Cott's categories) \"made Cott's drawings the most compelling evidence for natural selection enhancing survival through disruptive camouflage.\" Cott explained, while discussing \"a little frog known as \"Megalixalus fornasinii\"\" in his chapter on coincident disruptive coloration, that \"it is only when the pattern is considered in relation to the frog's normal attitude of rest that its remarkable nature becomes apparent... The attitude and very striking colour-scheme thus combine to produce an extraordinary effect, whose deceptive appearance depends upon the breaking up of the entire form into two strongly contrasted areas of brown and white. Considered separately, neither part resembles part of a frog. Together in nature the white configuration alone is conspicuous. This stands out and distracts the observer's attention from the true form and contour of the body and appendages on which it is superimposed\". Cott concluded that the effect was concealment \"so long as the false configuration is recognized in preference to the real one\". Such patterns embody, as Cott stressed, considerable precision as the markings must line up accurately for the disguise to work. Cott's description and in particular his drawings convinced biologists that the markings, and hence the camouflage, must have survival value (rather than occurring by chance); and further, as Cuthill and Székely indicate, that the bodies of animals that have such patterns must indeed have been shaped by natural selection.\n\nBetween 1953 and 1956, the geneticist Bernard Kettlewell experimented on peppered moth evolution. He presented results showing that in a polluted urban wood with dark tree trunks, dark moths survived better than pale ones, causing industrial melanism, whereas in a clean rural wood with paler trunks, pale moths survived better than dark ones. The implication was that survival was caused by camouflage against suitable backgrounds, where predators hunting by sight (insect-eating birds, such as the great tits used in the experiment) selectively caught and killed the less well-camouflaged moths. The results were intensely controversial, and from 2001 Michael Majerus carefully repeated the experiment. The results were published posthumously in 2012, vindicating Kettlewell's work as \"the most direct evidence\", and \"one of the clearest and most easily understood examples of Darwinian evolution in action\".\n\nBatesian mimicry, named for the 19th century naturalist Henry Walter Bates who first noted the effect in 1861, \"provides numerous excellent examples of natural selection\" at work. The evolutionary entomologist James Mallet noted that mimicry was \"arguably the oldest Darwinian theory not attributable to Darwin.\" Inspired by \"On the Origin of Species\", Bates realized that unrelated Amazonian butterflies resembled each other when they lived in the same areas, but had different coloration in different locations in the Amazon, something that could only have been caused by adaptation.\n\nMüllerian mimicry, too, in which two or more distasteful species that share one or more predators have come to mimic each other's warning signals, was clearly adaptive; Fritz Müller described the effect in 1879, in an account notable for being the first use of a mathematical argument in evolutionary ecology to show how powerful the effect of natural selection would be.\n\nIn 1867, in a letter to Darwin, Wallace described warning coloration. Mallet notes that this discovery \"rather illogically\" followed rather than preceded the accounts of Batesian and Müllerian mimicry, which both rely on the existence and effectiveness of warning coloration. The conspicuous colours and patterns of animals with strong defences such as toxins are advertised to predators, signalling honestly that the animal is not worth attacking. This directly increases the reproductive fitness of the potential prey, providing a strong selective advantage. The existence of unequivocal warning coloration is therefore clear evidence of natural selection at work.\n\nEdward Bagnall Poulton's 1890 book, \"The Colours of Animals\", renamed Wallace's concept of warning colours \"aposematic\" coloration, as well as supporting Darwin's then unpopular theories of natural selection and sexual selection. Poulton's explanations of coloration are emphatically Darwinian. For example, on aposematic colouration he wrote that\n\nPoulton summed up his allegiance to Darwinism as an explanation of Batesian mimicry in one sentence: \"Every step in the gradually increasing change of the mimicking in the direction of specially protected form, would have been an advantage in the struggle for existence\".\n\nThe historian of science Peter J. Bowler commented that Poulton used his book to complain about experimentalists' lack of attention to what field naturalists (like Wallace, Bates, and Poulton) could readily see were adaptive features. Bowler added that \"The fact that the adaptive significance of coloration \"was\" (sic) widely challenged indicates just how far anti-Darwinian feeling had developed. Only field naturalists such as Poulton refused to give in, convinced that their observations showed the validity of selection, whatever the theoretical problems.\"\n", "id": "53867278", "title": "Coloration evidence for natural selection"}
{"url": "https://en.wikipedia.org/wiki?curid=53918629", "text": "Evolutionary models of human drug use\n\nThe use of psychoactive substances is one of the most perplexing human behaviors. Psychoactive drugs can relieve the symptoms of mental disorders (e.g. lithium) or cause harm to individuals and societies (e.g. heroin). Psychoactive drugs can induce pleasure, increase energy (e.g. chocolate, coffee), relieve pain (Aspirin), or can impose a large social burden in the form of chronic illness (e.g. tobacco) and be a cause of mortality.\n\nWhy do humans seek out and at times even develop addictions to drugs that harm them? A number of attempts have been made to understand drug use and addiction from an evolutionary perspective. Evolutionary models of drug use are unique in that they emphasize the effect drugs had on fitness over human evolution.\n\nThe dominant paradigm of drug abuse focuses on human neurobiology and suggests that drug use is the result of reward-related behavior and that drug addiction is a consequence of drug interference with natural reward systems. Specifically, this tradition postulates that the chemical compounds humans seek out increase brain dopamine levels and thereby effectively usurp the mesolimbic pathway, a system originally intended to motivate/reward fitness enhancing behaviors such as those that increase access to food and sex.\n\nIdeas concerning the neural bases of motivation and reinforcement in behavior can be traced back to the 1950s. In 1953, Olds and Milner published findings implicating a brain region, specifically a cluster of dopamine neurons, with reward-based learning. Drugs of abuse were later discovered to increase dopamine in the region of the brain associated with reward-based-learning (see: brain stimulation reward).\n\nResearch on the molecular pathways of addiction suggests that drugs of abuse, despite their diverse chemical substrates, converge on a common circuitry in the brain’s limbic system. Specifically, drugs are thought to activate the mesolimbic dopamine pathway, facilitating dopamine transmission in the nucleus accumbens, via disinhibition, excitation, uptake blockade, etc. to produce a dopamine-like, yet dopamine independent effect.\n\nThe hijack model of drug abuse explains that drugs that elicit positive emotion mediate incentive motivation in the nucleus accumbens of the brain. Put another way, drugs of abuse act on ancient and evolutionarily conserved neural mechanisms associated with positive emotions that evolved to mediate incentive behavior. Psychoactive drugs induce emotions that in human evolutionary history signaled increases in fitness. Positive emotions such as euphoria and excitation are tools chosen by natural selection to help direct the behavior and physiology of an individual towards an increase in Darwinian fitness. For example, in the environment of evolutionary adaptation, humans would feel positive euphoric emotions in response to a successful foraging session or in the event of a successful breeding. Many psychoactive substances provide this same feeling and yet do not produce fitness benefits.\n\nResearchers have shown how emotional disposition is correlated with problematic use of alcohol, wherein if the reason for alcohol consumption is positive, the user is thought to drink to enhance positive feelings with greater control of the substance than if the user’s emotional disposition prior to alcoholic consumption was negative. In these cases, the individual is drinking to cope and is shown to have less control over his/her own use. Alcohol mediates negative feelings by their suppression but also encourages the habituated continuance of positive emotion. Recovering alcoholics often report that the reason for relapse is often related to the impulse to compensate for negative feelings, resulting in a motivation to cope and therefore drink.\n\nDespite being harmful, drugs such as nicotine, cocaine, THC, and opium artificially stimulate the emotions and neural circuits involved in the mesolimbic reward system thus encouraging drug consumption. Drugs of abuse are harmful, why do they increase dopamine like sugar and sex do? The hijack hypothesis suggests that drugs are effective hijackers of neural reward circuitry (e.g. the mesolimbic dopamine system) because they are evolutionarily novel. Specifically proposing that modern-day drug concentrations, methods of delivery, and the existence of certain drugs themselves were not available until recently on an evolutionary time scale, and thus human biology has been slow to adapt and is presently mismatched and susceptible.\n\nTo explain how drugs increase dopamine and cause positive emotions while at the same time lowering reproductive fitness, researchers posited that evolutionarily novel drugs hijack the brain’s mesolimbic dopamine system and generate a false positive signal of a fitness benefit as well as inhibiting negative effects, to signal a lack of negative fitness consequences. Modern drug addiction fundamentally indicates a false increase of fitness, leading to increasing drug abuse to continue gain, even if the gain is realized as being false. That these drugs create a signal in the brain that indicates, falsely, the arrival of a huge fitness benefit which changes behavioral propensities so that drug seeking increases in frequency and displaces adaptive behaviors. Proponents of the hijack hypothesis suggest that the paradox of drug reward is due to this evolutionary mismatch, that extant access to psychoactive drug concentrations and products are unmatched by those that existed in the past.\n\nWhy do humans seek out and consume drugs that harm them? The paradox of drug reward refers to the puzzling ability of drugs to induce both aversive and rewarding effects. Despite contention on the particulars of dopamine induced reward and behavior, there is agreement that dopamine plays an instrumental role in the processing of reward-related stimuli and that drug induced dopamine stimulation explains at least some part of drug abuse phenomena. And still, almost all major recreational drugs are plant secondary metabolites or a close chemical analog. The secondary plant compounds from which psychoactive drugs are derived are a form of interspecies defense chemicals that evolved to deter and/or mitigate consumption of the plant soma by herbivores/insects. The compounds from which psychoactive drugs are derived evolved to punish herbivore consumption, not reward it.\n\nAnimals evolved to exploit plant tissues and energy stores and in response plants developed a number of defenses, including neurotoxins. The presence and concentration of these toxins vary by plant tissue, with leaves and organs central to reproduction and energy conservation displaying high toxin concentrations (e.g. pistils/stamens and storage organs) and absent in tissue central to seed dispersion (e.g. fruit). The power and effectiveness of plant neurotoxic substances has been shaped by ~400 million years of evolution. Plant derived neurotoxins are not evolutionarily novel and human neurophysiology recognizes plant toxins and activates specialized xenobiotic defenses that involve genes, tissue barriers, neural circuits, organ systems and behaviors to protect against them.\n\nDrug toxicity and aversion exist in humans and are at odds with the theory of drug reward. Chronic drug use is harmful in humans and the human brain has evolved defenses to prevent, not reinforce, drug abuse. In response to the evolution of plant chemical defenses, herbivores have co-evolved a number of countermeasures, including (1) compounds that prevent or attenuate induction of plant chemical defenses; (2) detoxification mechanisms, including enzymes and symbiotic relationships with microbes to detoxify or extract nutrients from plant defenses, and cellular membrane carrier proteins for toxin transport; and (3) chemosensors and aversive learning mechanisms that permit selective feeding on less toxic tissues.\n\nHuman and plant neurotoxin coevolution is evidenced by features of the xenobiotic defense network. Tobacco activates defense mechanisms which researchers suggests it is recognized as toxic not a reward. Nicotine activates bitter taste receptors in the mouth and gut. Ingesting 4–8 mg of nicotine causes burning of the mouth and throat, nausea, aversion, vomiting and diarrhea. In higher doses the effects are more robust and can result in weakness, confusion, dizziness, convulsions, and coma. If consumed in high enough amounts, acute nicotine toxicity can trigger failure of the respiratory system and induce death in human adults within minutes. First-time users of tobacco especially report a variety of unpleasant effects upon administration of nicotine, including nausea, vomiting, gastrointestinal distress, headache, and sweating. This, when taken with the fact that nicotine is a plant toxin that evolved to deter herbivores, suggests instead that the human body naturally recognizes tobacco as a toxic substance, and not a reward.\n\nIn addition, research has found genetic evidence that humans have had a long evolutionary history to plant neurotoxins. Sullivan et al. (2008) has noted that humans, like other mammals, have ‘inherited’ the cytochrome P450 system, which functions to detoxify chemicals found in the environment, including plant neurotoxins. The ubiquity of CYP genes in humans worldwide, including CYP2A6 and CYP2B6, which metabolize nicotine, as well as other drugs, might suggest an evolutionary history with humans and plant neurotoxins. The mammalian body has also evolved to develop defenses against overtoxicity, such as exogenous substance metabolism and vomiting reflexes.\n\nThe neurotoxin regulation model of human drug use proposes that during the course of human evolution, plant consumption played a key role. The hypothesis suggests that the compulsory consumption of both the nutrients and neurotoxins in plants selected for a system capable of maximizing the benefits of plant energy extraction while mitigating the cost of plant toxicity. To do this, humans evolved a defense system in which plant consumption is mediated by cues of toxicity in a manner sensitive to the individual’s toxicity threshold, maintaining blood toxin concentrations below a critical level.\n\nResearch on herbivores supports the notion of a regulation pathway. Plant toxin concentration informs mammalian herbivore food choices, with herbivores moderating toxicity by capping daily plant intake to accommodate blood toxin concentrations. This mechanism exists across herbivore species and remains static in response to a range of plant toxins, even those that are evolutionarily novel. Similarly, in laboratory conditions, mice have been shown to moderate administration of drugs regardless of dose per injection or the number of lever presses required.\n\nEvidence of toxin regulation exists across drug types and is present in the case of nicotine. In humans, self-administration of nicotine is moderated such that steady blood concentrations of the toxin are maintained. Moreover, though nicotine is a potent neurotoxin, lethal overdoses are rare and smoking behavior is couched around titration, with number of cigarettes smoked directly tied to changes in nicotine blood concentration. In addition, although typical doses of recreational drugs are often only marginally below the lethal dose, overdose remains rare. For the most part, drug consumption is metered. Thus, proponents of the neurotoxin regulation model of drug use suggest it is highly unlikely that toxin consumption is controlled by the system that motivates and rewards the consumption of macronutrients. Arguing that If drugs and sugar (and other energetically dense foods) stimulate dopamine in the mesolimbic reward system with the same degree of efficiency, then, the drug overdose rates should be comparable in scale to the incidence of obesity.\n\nThe neurotoxin regulation hypothesis proposes that drug use is not novel because human brains and plant neurotoxins coevolved. Genetic evidence suggests that humans have had regular exposure to plant drugs throughout our evolutionary history. Archeological evidence indicates the presence of psychoactive plants and drug use in early hominid species about 200 million years ago. Paleogenetic evidence suggests that the first time human ancestors were exposed and adapted to substantial amount of dietary ethanol, was approximately 10 million years ago. Neurobiological evidence appears to corroborate this story. The fit of allelochemicals within the CNS indicates some coevolutionary activity between mammalian brains and psychoactive plants, meaning they interacted ecologically and therefore responded to one another evolutionarily. This would have only been possible with mammalian CNS exposure to these allelochemicals, therefore to ancient mammalian psychotropic substance use. For example, the mammalian brain has evolved receptor systems for plant substances, such as the opioid receptor system, not available to the mammalian body itself.\n\nThe neurotoxin regulation model of drug use is a response to proponents of the hijack hypothesis. Largely this is because the neurobiological reward model of drug use sees interactions between plant neurotoxins and human reward systems as novel and rewarding.\n\nThe neurotoxin regulation hypothesis emphasizes the evolutionary biology of plant-human coevolution and maintains that secondary plant metabolites, including alkaloids like nicotine, morphine, and cocaine, are potent neurotoxins that evolved to deter and punish herbivore consumption of the plant soma not encourage/reward it. Researchers highlight that it is evolutionarily disadvantageous for plants to produce toxins that plant predators (e.g. humans) are attracted to, and that it runs contrary to evolutionary logic that plant predators (e.g. humans) would evolve neurobiological systems unprotected from plant toxin consumption.\n\nProponents of the hijack hypothesis outline a path to addiction that involves drugs co-opting neural reward systems intended for food. However, research on murine models has shown that when the concentration is sufficiently high, sugar operates as a more robust reward than even cocaine. In laboratory conditions, where rats are presented with both a sugar and cocaine sipper, they choose sugar. Researchers use these findings to suggest that sugar reward might generate a stronger dopamine stimulation than cocaine and also may make use of neural mechanisms beyond dopamine stimulation.\n\nAlternative mechanisms explain continued tobacco use: The majority of first time users of cigarettes report adverse reactions, including nausea, dizziness, sickness, and headache. A study by DiFranza et al. (2004) found that 69% of subjects rated inhaling their first cigarette as bad, and nearly three-quarters (72%) reported that their first cigarette made them not want to smoke again. Given the above, opponents of the reward model of drug use suggest it is likely that a mechanism other than a false perception of an increased fitness benefit via hijacking of the brain’s mesolimbic dopamine system, is leading to continued tobacco use.\n\nThroughout the course of human evolution, the importance of psychoactive plant substances for health has been enormous. Since our earliest ancestors chewed on certain herbs to relieve pain, or wrapped leaves around wounds to improve healing, natural products have often been the only ways of treating disease and injury. Plants provide fitness benefits. Upwards of 25% of all pharmaceutical drugs are from plant-derived sources. The US National Cancer Institute has identified over 3,000 plants that are effective against cancer cells. Almost all major recreational drugs are secondary plant compounds or a close chemical analog. It is well established that in both present and past contexts plants have been used for medicinal purposes.\n\nA core premise of evolutionary theory is that a trait cannot evolve unless it contributes to an individual’s reproductive fitness. Proponents of the pharmacophagy hypothesis/medicinal model of drug use suggest that that pharmacophagy, the consumption of pharmacological substances for medicinal purposes, evolved in the backdrop of human-plant coevolution as a means of self-medication. Theorists propose that the reason humans learned to ignore the cues of plant toxicity (e.g. bitter taste) and consumed potentially lethal substances with little to no energetic content because ingesting the bioactive compounds of plants in small amounts was therapeutic.\n\nThough the long-term health costs of drug use use are undeniable, proponents of the medicinal model of drug use suggest it is possible that regulated consumption of plant neurotoxins was selected for. In this regard, researchers have argued that the human brain evolved to control and regulate intake of psychoactive plant toxins in order to promote reproductive fitness. Broadly, theorists suggest that plant toxins were deliberately ingested by human ancestors to combat macroparasites (e.g. parasitic worms) and/or to ward off disease carrying vectors (e.g. mosquitos).\n\nFor example, researchers have recently sought to understand why humans began using tobacco in spite of the consequences and adverse reactions commonly associated with its use. Hagen and colleagues propose that, as in other species, humans began using tobacco and other plant toxins as a way of controlling infection by parasitic diseases, including helminths. Tobacco, as well as arecoline and cannabis, two other plant neurotoxins that are widely used as recreational drugs in humans, have been found to be toxic to parasitic worms that affect humans and other mammals, as well as plants. Modern anthelminthics function as well by targeting nicotinic acetylcholine receptors (nAChRs) on somatic muscle cells of parasites, producing paralysis and expelling the parasite, the same receptors which are targeted by nicotine (Roulette et al., 2014). Moreover, it has also been found that nicotine is equally or more effective than commercial anthelmintics at killing leeches, including those that infect humans. Similarly, Roulette et al. (2014) found in a study comparing male smoking prevalence and parasite load among Aka hunter-gatherers that treatment with commercial anthelmintics was associated with a decrease in cotinine concentrations (a measure of current tobacco use), thereby supporting their theory that humans regulate the amount of tobacco used in response to current helminth infection. The study also found that men with higher initial tobacco use also had lower worm burdens one year later, suggesting that nicotine not only eliminates parasites, but also protects from reinfection.\n\nSome evolutionary psychological theories concerning drug use suggest individuals consume drugs to increase reproductive opportunities. Drug use can increase reproductive fitness because drug use can (1) advertise biological quality, sexual maturity, or availability, (2) decrease inhibitions in mating contexts, and/or (3) enhance associative learning behaviors that in turn increase mating opportunities. See Richardson et al., 2017 for a review.\n\nResearchers suggest that because variation in drug use susceptibility is in part due to genetic factors, drug consumption could potentially be a costly and honest signal of biological quality. The hypothesis being that humans engage in substance use despite health costs in part to evidence that they can afford to do so. To test the effects substance use had on indicators of mating success researchers tested the effect an individual’s fluctuating asymmetry had on the propensity/likelihood to use drugs and found no significant results.\n\nHagen et al. (2013) suggest that individuals use drug substances to signal maturity. They point out that sexually selected cues of quality often emerge in adolescence (e.g. the peacock’s tail) and reliably signal developmental maturity. The teratogenic effects of drugs of abuse are well documented, as is the fact that psychoactive substances are most harmful for individuals who are developmentally immature. Although this hypothesis remains untested, evidence in support comes from age at onset of drug use. Unequivocally, tobacco consumption does not occur prior to age 11 and in most all cases, this aligns with age at onset of drug use, as cigarette addicts report having first smoked in adolescence. Hagen et al. suggest the reason drug use most often occurs in adolescent populations is due to the developmental maturity of the adolescent nervous system as well as the increases competition to compete for mates. Consistent with these notions, researchers have found that adolescents with alcohol use disorders were more sexually active, had more sexual partners, and initiated sexual activity at slightly albeit younger ages.\n\nAnother possible explanation to the prevalence of substance use is that it contributes to mating success by altering brain functions that lower inhibitions. Generally, people seem to believe substance use will enhance their social behaviors in ways conducive to mating success. Research has shown that many drug types inhibit prefrontal cortex neural activity, the area of the brain responsible conducting long term gains and short term costs. Alcohol myopia theory suggest alcohol lowers inhibitions and amplifies the pre-drinking intention to have sex. Research has also shown that alcohol stimulates dopamine activity in the mesolimbic-dopamine system, which amplifies the salience of natural rewards (e.g. finding food and mates) in the present environment and boost associative learning.\n\nDrug use is not evenly distributed in the population. Research has shown that the prevalence of substance use problems varies in fairly reliable ways according to age, sex, and sociodemographic characteristics. Overall, and across drug categories—including alcohol, coffee, cannabis, and nicotine—men make up the primary drug demographic. Research has also shown that the prevalence of substance use disorders is highest among young adults (ages 18–29), and among individuals of low socioeconomic status.\n\nApplication of evolutionary theory to patterns of drug use suggest patterns can be explained in terms of the fundamental trade-offs that occur during different developmental periods as well as gender differences arising from reproductive asymmetry. According to life history theory, individuals have finite energetic resources and thus face energetic allocation decisions concerning investment in maintenance, growth, and reproduction. How resources are allocated to these different tasks in order to most effectively maximize reproductive fitness will depend on the age and sex of the individual and the environmental context the individual exists in.\n\nLife history predicts that men, especially if they are young, are most likely to engage in drug use because they are most likely to engage in risky behavior and discount the future. Young men have the most to gain from risk-taking behavior because competition for mates, status, and resources is greatest during late adolescence and young adulthood. As men age, they are more likely to develop long-term pair-bonds, accrue status, and have children, thus as men age life history theory predicts a decrease in risk taking behavior and a reallocation of energy to parenting rather than mating. Average age at drug initiation, occurs in adolescence (ages 15–25) and supports this shift. In contrast, life history theory predicts that women are less prone to engage in risk-taking behavior because they experience less variance in reproductive success and have more to lose from risk taking and more to gain from focusing effort on parenting.\n\nAlmost all major recreational drugs are secondary plant compounds or a close chemical analog and are thus teratogenic, substances known to cause congenital abnormalities and other reproductive harms (e.g. nicotine, carbon monoxide, hydrogen cyanide). Give sex-specific vulnerabilities and fitness costs, the fetal protection hypothesis proposes that selection for increased drug avoidance could have evolved in women to protect them from harming their developing fetuses and nursing infants.\n\nAncestral Women and Conditions: In the environment of evolutionary adaptation (EEA), selection pressures shaping avoidance of or defenses against teratogenic substances would have been high. Evidence from evolutionary anthropology suggest ancestral women, similar to women in extant hunter gatherer populations, experienced high fertility and high infant mortality. Importantly, high fertility is characterized by short inter-birth intervals, early age at first birth, and periods of breastfeeding spanning upwards of two years. Given such high reproductive costs, it is likely the fitness cost of ingesting neurotoxins is higher for women than men. One such hunter-gatherer population, the Aka, have incredibly high smoking prevalence rates among men (95%), but very low rates among women (5%).\n\nDrugs and Negative Fertility Endpoints: Studies have shown that fetal exposure to nicotine is associated with a range of negative outcomes before and during parturition as well as for the baby early and later in life. It has also been shown that cigarette smoking has a significant negative effect on the clinical outcome of assisted reproduction treatments, with smokers requiring higher mean gonadotropin doses for ovarian stimulation and requiring nearly twice the number of in vitro fertilization cycles to conceive.\n\nFemale Specific Defenses: Compared to men, women metabolize toxins at faster rates and detect the presence of toxins at lower concentrations. Ovarian hormones have been implicated in the activity of xenobiotic metabolism. Research on pregnant women has documented the differential effects of estrogen and progesterone on the expression of CYPs in response to cigarette smoke. Using hepatic cells, Choi et al., (2012) found that progesterone and estradiol altered drug metabolism but only when hormone concentrations reach that which is normal for pregnancy. Changes in xenobiotic metabolism in women using birth control also suggest hormonally mediated influences. Specifically, changes in hepatic drug elimination for CYP1A2, CYP2C19 and CYP2A6 are similar in pregnant women and non-pregnant women using hormonal contraception. Furthermore, in women across the menstrual cycle, smoking topography variables (e.g. total number of cigarettes smoked, mean puff volume, etc.) seem to be mediated by estradiol and progesterone. In various studies, regular smokers have been shown to produce approximately one-third less estrogens (including estradiol) during the postovulatory menstrual phase than non-smoking counterparts. Research suggests the reason for this relationship is due to a functional tradeoff, with the enzymes that metabolize estradiol instead metabolizing/detoxifying tobacco toxins.\n\n", "id": "53918629", "title": "Evolutionary models of human drug use"}
{"url": "https://en.wikipedia.org/wiki?curid=1174964", "text": "Technological evolution\n\nTechnological evolution is an innovation- and technology-related theory that describes the radical transformation of society through technological development. This theory originated with Czech philosopher Radovan Richta.\n\n\"Mankind In Transition; A View of the Distant Past, the Present and the Far Future\", Masefield Books, 1993. Technology (which Richta defines as \"a material entity created by the application of mental and physical effort to nature in order to achieve some value\") evolves in three stages: tools, machine, automation. This evolution, he says, follows two trends:\n\nThe pretechnological period, in which all other animal species remain today aside from some avian and primate species was a non-rational period of the early prehistoric man.\n\nThe emergence of technology, made possible by the development of the rational faculty, paved the way for the first stage: the tool. A tool provides a mechanical advantage in accomplishing a physical task, such as an arrow, plow, or hammer that augments physical labor to more efficiently achieve his objective. Later animal-powered tools such as the plow and the horse, increased the productivity of food production about tenfold over the technology of the hunter-gatherers. Tools allow one to do things impossible to accomplish with one's body alone, such as seeing minute visual detail with a microscope, manipulating heavy objects with a pulley and cart, or carrying volumes of water in a bucket.\n\nThe second technological stage was the creation of the machine. A machine (a powered machine to be more precise) is a tool that substitutes the element of human physical effort, and requires only to control its function. Machines became widespread with the industrial revolution, though windmills, a type of machine, are much older.\n\nExamples of this include cars, trains, computers, and lights. Machines allow humans to\nTremendously exceed the limitations of their bodies. Putting a machine on the farm, a tractor, increased food productivity at least tenfold over the technology of the plow and the horse.\n\nThe third, and final stage of technological evolution is the automation. The automation is a machine that removes the element of human control with an automatic algorithm. Examples of machines that exhibit this characteristic are digital watches, automatic telephone switches, pacemakers, and computer programs.\n\nIt's crucial to understand that the three stages outline the introduction of the fundamental types of technology, and so all three continue to be widely used today. A spear, a plow, a pen, a knife, a glove, a chicken and an optical microscope are all examples of tools.\n\nThe process of technological evolution culminates with the ability to achieve all the material values technologically possible and desirable by mental effort.\n\nAn economic implication of the above idea is that intellectual labour will become increasingly more important relative to physical labour. Contracts and agreements around information will become increasingly more common at the marketplace. Expansion and creation of new kinds of institutes that works with information such as universities, book stores, patent-trading companies, etc. is considered an indication that a civilization is in technological evolution.\n\nThis highlights the importance underlining the debate over intellectual property in conjunction with decentralized distribution systems such as today's internet. Where the price of information distribution is going towards zero with ever more efficient tools to distribute information is being invented. Growing amounts of information being distributed to an increasingly larger customer base as times goes by. With growing disintermediation in said markets and growing concerns over the protection of intellectual property rights it is not clear what form markets for information will take with the evolution of the information age.\n\n", "id": "1174964", "title": "Technological evolution"}
{"url": "https://en.wikipedia.org/wiki?curid=19349161", "text": "Cambrian explosion\n\nThe Cambrian explosion or Cambrian radiation was the event of approximately in the Cambrian period when most major animal phyla appeared in the fossil record. It resulted in the divergence of most modern metazoan phyla. The event was accompanied by major diversification of other organisms.\n\nBefore the Cambrian explosion, most organisms were simple, composed of individual cells occasionally organized into colonies. Over the following 70 to 80 million years, the rate of diversification accelerated, and the variety of life began to resemble that of today. Almost all present animal phyla appeared during this period.\n\nThe Cambrian explosion has generated extensive scientific debate.\n\nThe seemingly rapid appearance of fossils in the \"Primordial Strata\" was noted by William Buckland in the 1840s, and in his 1859 book \"On the Origin of Species\", Charles Darwin discussed the then inexplicable lack of earlier fossils as one of the main difficulties for his theory of descent with slow modification through natural selection. The long-running puzzlement about the appearance of the Cambrian fauna, seemingly abruptly, without precursor, centers on three key points: whether there really was a mass diversification of complex organisms over a relatively short period of time during the early Cambrian; what might have caused such rapid change; and what it would imply about the origin of animal life. Interpretation is difficult due to a limited supply of evidence, based mainly on an incomplete fossil record and chemical signatures remaining in Cambrian rocks.\n\nThe first discovered Cambrian fossils were trilobites, described by Edward Lhuyd, the curator of Oxford Museum, in 1698. Although their evolutionary importance was not known, on the basis of their old age, William Buckland (1784–1856) realised that a dramatic step-change in the fossil record had occurred around the base of what we now call the Cambrian. Nineteenth-century geologists such as Adam Sedgwick and Roderick Murchison used the fossils for dating rock strata, specifically for establishing the Cambrian and Silurian periods. By 1859, leading geologists including Roderick Murchison, were convinced that what was then called the lowest Silurian stratum showed the origin of life on Earth, though others, including Charles Lyell, differed. In \"On the Origin of Species\", Charles Darwin considered this sudden appearance of a solitary group of trilobites, with no apparent antecedents, and absence of other fossils, to be \"undoubtedly of the gravest nature\" among the difficulties in his theory of natural selection. He reasoned that earlier seas had swarmed with living creatures, but that their fossils had not been found due to the imperfections of the fossil record. In the sixth edition of his book, he stressed his problem further as:\n\nAmerican paleontologist Charles Walcott, who studied the Burgess Shale fauna, proposed that an interval of time, the \"Lipalian\", was not represented in the fossil record or did not preserve fossils, and that the ancestors of the Cambrian animals evolved during this time.\n\nEarlier fossil evidence has since been found. The earliest claim is that the history of life on earth goes back : Rocks of that age at Warrawoona, Australia, were claimed to contain fossil stromatolites, stubby pillars formed by colonies of microorganisms. Fossils (\"Grypania\") of more complex eukaryotic cells, from which all animals, plants, and fungi are built, have been found in rocks from , in China and Montana. Rocks dating from contain fossils of the Ediacara biota, organisms so large that they are likely multicelled, but very unlike any modern organism. In 1948, Preston Cloud argued that a period of \"eruptive\" evolution occurred in the Early Cambrian, but as recently as the 1970s, no sign was seen of how the 'relatively' modern-looking organisms of the Middle and Late Cambrian arose.\n\nThe intense modern interest in this \"Cambrian explosion\" was sparked by the work of Harry B. Whittington and colleagues, who, in the 1970s, reanalysed many fossils from the Burgess Shale and concluded that several were as complex as, but different from, any living animals. The most common organism, \"Marrella\", was clearly an arthropod, but not a member of any known arthropod class. Organisms such as the five-eyed \"Opabinia\" and spiny slug-like \"Wiwaxia\" were so different from anything else known that Whittington's team assumed they must represent different phyla, seemingly unrelated to anything known today. Stephen Jay Gould's popular 1989 account of this work, \"Wonderful Life\", brought the matter into the public eye and raised questions about what the explosion represented. While differing significantly in details, both Whittington and Gould proposed that all modern animal phyla had appeared almost simultaneously in a rather short span of geological period. This view led to the modernization of Darwin's tree of life and the theory of punctuated equilibrium, which Eldredge and Gould developed in the early 1970s and which views evolution as long intervals of near-stasis \"punctuated\" by short periods of rapid change.\n\nOther analyses, some more recent and some dating back to the 1970s, argue that complex animals similar to modern types evolved well before the start of the Cambrian.\n\nRadiometric dates for much of the Cambrian, obtained by analysis of radioactive elements contained within rocks, have only recently become available, and for only a few regions.\n\nRelative dating (\"A\" was before \"B\") is often assumed sufficient for studying processes of evolution, but this, too, has been difficult, because of the problems involved in matching up rocks of the same age across different continents.\n\nTherefore, dates or descriptions of sequences of events should be regarded with some caution until better data become available.\n\nFossils of organisms' bodies are usually the most informative type of evidence. Fossilization is a rare event, and most fossils are destroyed by erosion or metamorphism before they can be observed. Hence, the fossil record is very incomplete, increasingly so as earlier times are considered. Despite this, they are often adequate to illustrate the broader patterns of life's history. Also, biases exist in the fossil record: different environments are more favourable to the preservation of different types of organism or parts of organisms. Further, only the parts of organisms that were already mineralised are usually preserved, such as the shells of molluscs. Since most animal species are soft-bodied, they decay before they can become fossilised. As a result, although 30-plus phyla of living animals are known, two-thirds have never been found as fossils.\nThe Cambrian fossil record includes an unusually high number of lagerstätten, which preserve soft tissues. These allow paleontologists to examine the internal anatomy of animals, which in other sediments are only represented by shells, spines, claws, etc. – if they are preserved at all. The most significant Cambrian lagerstätten are the early Cambrian Maotianshan shale beds of Chengjiang (Yunnan, China) and Sirius Passet (Greenland); the middle Cambrian Burgess Shale (British Columbia, Canada); and the late Cambrian Orsten (Sweden) fossil beds.\n\nWhile lagerstätten preserve far more than the conventional fossil record, they are far from complete. Because lagerstätten are restricted to a narrow range of environments (where soft-bodied organisms can be preserved very quickly, e.g. by mudslides), most animals are probably not represented; further, the exceptional conditions that create lagerstätten probably do not represent normal living conditions. In addition, the known Cambrian lagerstätten are rare and difficult to date, while Precambrian lagerstätten have yet to be studied in detail.\n\nThe sparseness of the fossil record means that organisms usually exist long before they are found in the fossil record – this is known as the Signor–Lipps effect.\n\nTrace fossils consist mainly of tracks and burrows, but also include coprolites (fossil feces) and marks left by feeding. Trace fossils are particularly significant because they represent a data source that is not limited to animals with easily fossilized hard parts, and reflects organisms' behaviour. Also, many traces date from significantly earlier than the body fossils of animals that are thought to have been capable of making them. While exact assignment of trace fossils to their makers is generally impossible, traces may, for example, provide the earliest physical evidence of the appearance of moderately complex animals (comparable to earthworms).\n\nSeveral chemical markers indicate a drastic change in the environment around the start of the Cambrian. The markers are consistent with a mass extinction, or with a massive warming resulting from the release of methane ice.\nSuch changes may reflect a cause of the Cambrian explosion, although they may also have resulted from an increased level of biological activity – a possible result of the explosion. Despite these uncertainties, the geochemical evidence helps by making scientists focus on theories that are consistent with at least one of the likely environmental changes.\n\nCladistics is a technique for working out the \"family tree\" of a set of organisms. It works by the logic that, if groups B and C have more similarities to each other than either has to group A, then B and C are more closely related to each other than either is to A. Characteristics that are compared may be anatomical, such as the presence of a notochord, or molecular, by comparing sequences of DNA or protein. The result of a successful analysis is a hierarchy of clades – groups whose members are believed to share a common ancestor. The cladistic technique is sometimes problematic, as some features, such as wings or camera eyes, evolved more than once, convergently – this must be taken into account in analyses.\n\nFrom the relationships, it may be possible to constrain the date that lineages first appeared. For instance, if fossils of B or C date to X million years ago and the calculated \"family tree\" says A was an ancestor of B and C, then A must have evolved more than X million years ago.\n\nIt is also possible to estimate how long ago two living clades diverged – i.e. about how long ago their last common ancestor must have lived  – by assuming that DNA mutations accumulate at a constant rate. These \"molecular clocks\", however, are fallible, and provide only a very approximate timing: they are not sufficiently precise and reliable for estimating when the groups that feature in the Cambrian explosion first evolved, and estimates produced by different techniques vary by a factor of two. However, the clocks can give an indication of branching rate, and when combined with the constraints of the fossil record, recent clocks suggest a sustained period of diversification through the Ediacaran and Cambrian.\n\nA phylum is the highest level in the Linnaean system for classifying organisms. Phyla can be thought of as groupings of animals based on general body plan. Despite the seemingly different external appearances of organisms, they are classified into phyla based on their internal and developmental organizations. For example, despite their obvious differences, spiders and barnacles both belong to the phylum Arthropoda, but earthworms and tapeworms, although similar in shape, belong to different phyla. As chemical and genetic testing becomes more accurate, previously hypothesised phyla are often entirely reworked.\n\nA phylum is not a fundamental division of nature, such as the difference between electrons and protons. It is simply a very high-level grouping in a classification system created to describe all currently living organisms. This system is imperfect, even for modern animals: different books quote different numbers of phyla, mainly because they disagree about the classification of a huge number of worm-like species. As it is based on living organisms, it accommodates extinct organisms poorly, if at all.\n\nThe concept of stem groups was introduced to cover evolutionary \"aunts\" and \"cousins\" of living groups, and have been hypothesized based on this scientific theory. A crown group is a group of closely related living animals plus their last common ancestor plus all its descendants. A stem group is a set of offshoots from the lineage at a point earlier than the last common ancestor of the crown group; it is a relative concept, for example tardigrades are living animals that form a crown group in their own right, but Budd (1996) regarded them as also being a stem group relative to the arthropods.\n\nThe term \"Triploblastic\" means consisting of three layers, which are formed in the embryo, quite early in the animal's development from a single-celled egg to a larva or juvenile form. The innermost layer forms the digestive tract (gut); the outermost forms skin; and the middle one forms muscles and all the internal organs except the digestive system. Most types of living animal are triploblastic – the best-known exceptions are Porifera (sponges) and Cnidaria (jellyfish, sea anemones, etc.).\n\nThe bilaterians are animals that have right and left sides at some point in their life histories. This implies that they have top and bottom surfaces and, importantly, distinct front and back ends. All known bilaterian animals are triploblastic, and all known triploblastic animals are bilaterian. Living echinoderms (sea stars, sea urchins, sea cucumbers, etc.) 'look' radially symmetrical (like wheels) rather than bilaterian, but their larvae exhibit bilateral symmetry and some of the earliest echinoderms may have been bilaterally symmetrical. Porifera and Cnidaria are radially symmetrical, not bilaterian, and not triploblastic.\n\nThe term \"Coelomate\" means having a body cavity (coelom) containing the internal organs. Most of the phyla featured in the debate about the Cambrian explosion are coelomates: arthropods, annelid worms, molluscs, echinoderms, and chordates – the noncoelomate priapulids are an important exception. All known coelomate animals are triploblastic bilaterians, but some triploblastic bilaterian animals do not have a coelom – for example flatworms, whose organs are surrounded by unspecialized tissues.\n\nUnderstanding of the Cambrian explosion relies upon knowing what was there beforehand – did the event herald the sudden appearance of a wide range of animals and behaviours, or did such things exist beforehand?\n\nPhylogenetic analysis has been used to support the view that during the Cambrian explosion, metazoans (multi-celled animals) evolved monophyletically from a single common ancestor: flagellated colonial protists similar to modern choanoflagellates.\n\nChanges in the abundance and diversity of some types of fossil have been interpreted as evidence for \"attacks\" by animals or other organisms. Stromatolites, stubby pillars built by colonies of microorganisms, are a major constituent of the fossil record from about , but their abundance and diversity declined steeply after about . This decline has been attributed to disruption by grazing and burrowing animals.\n\nPrecambrian marine diversity was dominated by small fossils known as acritarchs. This term describes almost any small organic walled fossil – from the egg cases of small metazoans to resting cysts of many different kinds of green algae. After appearing around , acritarchs underwent a boom around , increasing in abundance, diversity, size, complexity of shape, and especially size and number of spines. Their increasingly spiny forms in the last 1 billion years may indicate an increased need for defence against predation. Other groups of small organisms from the Neoproterozoic era also show signs of antipredator defenses. A consideration of taxon longevity appears to support an increase in predation pressure around this time.\nIn general, the fossil record shows a very slow appearance of these lifeforms in the Precambrian, with many cyanobacterial species making up much of the underlying sediment.\n\nThe layers of the Doushantuo formation from around \nharbour microscopic fossils that may represent early bilaterians. Some have been described as animal embryos and eggs, although some may represent the remains of giant bacteria.\nAnother fossil, \"Vernanimalcula\", has been interpreted as a coelomate bilaterian,\nbut may simply be an infilled bubble.\n\nThese fossils form the earliest hard-and-fast evidence of animals, as opposed to other predators.\n\nThe traces of organisms moving on and directly underneath the microbial mats that covered the Ediacaran sea floor are preserved from the Ediacaran period, about . They were probably made by organisms resembling earthworms in shape, size, and how they moved. The burrow-makers have never been found preserved, but, because they would need a head and a tail, the burrowers probably had bilateral symmetry – which would in all probability make them bilaterian animals. They fed above the sediment surface, but were forced to burrow to avoid predators.\n\nAround the start of the Cambrian (about ), many new types of traces first appear, including well-known vertical burrows such as \"Diplocraterion\" and \"Skolithos\", and traces normally attributed to arthropods, such as \"Cruziana\" and \"Rusophycus\". The vertical burrows indicate that worm-like animals acquired new behaviours, and possibly new physical capabilities. Some Cambrian trace fossils indicate that their makers possessed hard exoskeletons, although they were not necessarily mineralised.\n\nBurrows provide firm evidence of complex organisms; they are also much more readily preserved than body fossils, to the extent that the absence of trace fossils has been used to imply the genuine absence of large, motile, bottom-dwelling organisms. They provide a further line of evidence to show that the Cambrian explosion represents a real diversification, and is not a preservational artefact.\n\nIndeed, as burrowing became established, it allowed an explosion of its own, for as burrowers disturbed the sea floor, they aerated it, mixing oxygen into the toxic muds. This made the bottom sediments more hospitable, and allowed a wider range of organisms to inhabit them – creating new niches and the scope for higher diversity.\n\nAt the start of the Ediacaran period, much of the acritarch fauna, which had remained relatively unchanged for hundreds of millions of years, became extinct, to be replaced with a range of new, larger species, which would prove far more ephemeral. This radiation, the first in the fossil record, is followed soon after by an array of unfamiliar, large, fossils dubbed the Ediacara biota, which flourished for 40 million years until the start of the Cambrian. Most of this \"Ediacara biota\" were at least a few centimeters long, significantly larger than any earlier fossils. The organisms form three distinct assemblages, increasing in size and complexity as time progressed.\n\nMany of these organisms were quite unlike anything that appeared before or since, resembling discs, mud-filled bags, or quilted mattresses – one palæontologist proposed that the strangest organisms should be classified as a separate kingdom, Vendozoa.\n\nAt least some may have been early forms of the phyla at the heart of the \"Cambrian explosion\" debate, having been interpreted as early molluscs (\"Kimberella\"), echinoderms (\"Arkarua\"); and arthropods (\"Spriggina\", \"Parvancorina\"). Still, debate exists about the classification of these specimens, mainly because the diagnostic features that allow taxonomists to classify more recent organisms, such as similarities to living organisms, are generally absent in the ediacarans. However, there seems little doubt that \"Kimberella\" was at least a triploblastic bilaterian animal. These organisms are central to the debate about how abrupt the Cambrian explosion was. If some were early members of the animal phyla seen today, the \"explosion\" looks a lot less sudden than if all these organisms represent an unrelated \"experiment\", and were replaced by the animal kingdom fairly soon thereafter (40M years is \"soon\" by evolutionary and geological standards).\n\nPaul Knauth, a geologist at Arizona State University, maintains that photosynthesizing organisms such as algae, may have grown over a 750- to 800-million-year-old formation in Death Valley known as the Beck Spring Dolomite. In the early 1990s, samples from this 1,000-foot thick layer of dolomite revealed that the region housed flourishing mats of photosynthesizing, unicellular life forms which antedated the Cambrian explosion.\n\nMicrofossils have been unearthed from holes riddling the otherwise barren surface of the dolomite. These geochemical and microfossil findings support the idea that during the Precambrian period, complex life evolved both in the oceans and on land. Knauth contends that animals may well have had their origins in freshwater lakes and streams, and not in the oceans.\n\nSome 30 years later, a number of studies have documented an abundance of geochemical and microfossil evidence showing that life covered the continents as far back as 2.2 billion years ago. Many paleobiologists now accept the idea that simple life forms existed on land during the Precambrian, but are opposed to the more radical idea that multicellular life thrived on land more than 600 million years ago.\n\nThe first Ediacaran and lowest Cambrian (Nemakit-Daldynian) skeletal fossils represent tubes and problematic sponge spicules. The oldest sponge spicules are monaxon siliceous, aged around , known from the Doushantou Formation in China and from deposits of the same age in Mongolia, although the interpretation of these fossils as spicules has been challenged. In the late Ediacaran-lowest Cambrian, numerous tube dwellings of enigmatic organisms appeared. It was organic-walled tubes (e.g. \"Saarina\") and chitinous tubes of the sabelliditids (e.g. \"Sokoloviina\", \"Sabellidites\", \"Paleolina\") that prospered up to the beginning of the Tommotian. The mineralized tubes of \"Cloudina\", \"Namacalathus\", \"Sinotubulites\", and a dozen more of the other organisms from carbonate rocks formed near the end of the Ediacaran period from , as well as the triradially symmetrical mineralized tubes of anabaritids (e.g. \"Anabarites\", \"Cambrotubulus\") from uppermost Ediacaran and lower Cambrian. Ediacaran mineralized tubes are often found in carbonates of the stromatolite reefs and thrombolites, i.e. they could live in an environment adverse to the majority of animals.\n\nAlthough they are as hard to classify as most other Ediacaran organisms, they are important in two other ways. First, they are the earliest known calcifying organisms (organisms that built shells from calcium carbonate). Secondly, these tubes are a device to rise over a substrate and competitors for effective feeding and, to a lesser degree, they serve as armor for protection against predators and adverse conditions of environment. Some \"Cloudina\" fossils show small holes in shells. The holes possibly are evidence of boring by predators sufficiently advanced to penetrate shells. A possible \"evolutionary arms race\" between predators and prey is one of the hypotheses that attempt to explain the Cambrian explosion.\n\nIn the lowest Cambrian, the stromatolites were decimated. This allowed animals to begin colonization of warm-water pools with carbonate sedimentation. At first, it was anabaritids and \"Protohertzina\" (the fossilized grasping spines of chaetognaths) fossils. Such mineral skeletons as shells, sclerites, thorns, and plates appeared in uppermost Nemakit-Daldynian; they were the earliest species of halkierids, gastropods, hyoliths and other rare organisms. The beginning of the Tommotian has historically been understood to mark an explosive increase of the number and variety of fossils of molluscs, hyoliths, and sponges, along with a rich complex of skeletal elements of unknown animals, the first archaeocyathids, brachiopods, tommotiids, and others. This sudden increase is partially an artefact of missing strata at the Tommotian type section, and most of this fauna in fact began to diversify in a series of pulses through the Nemakit-Daldynian and into the Tommotian.\n\nSome animals may already have had sclerites, thorns, and plates in the Ediacaran (e.g. \"Kimberella\" had hard sclerites, probably of carbonate), but thin carbonate skeletons cannot be fossilized in siliciclastic deposits. Older (~750 Ma) fossils indicate that mineralization long preceded the Cambrian, probably defending small photosynthetic algae from single-celled eukaryotic predators.\n\nTrace fossils (burrows, etc.) are a reliable indicator of what life was around, and indicate a diversification of life around the start of the Cambrian, with the freshwater realm colonized by animals almost as quickly as the oceans.\n\nFossils known as \"small shelly fauna\" have been found in many parts on the world, and date from just before the Cambrian to about 10 million years after the start of the Cambrian (the Nemakit-Daldynian and Tommotian ages; see timeline). These are a very mixed collection of fossils: spines, sclerites (armor plates), tubes, archeocyathids (sponge-like animals), and small shells very like those of brachiopods and snail-like molluscs – but all tiny, mostly 1 to 2 mm long.\n\nWhile small, these fossils are far more common than complete fossils of the organisms that produced them; crucially, they cover the window from the start of the Cambrian to the first lagerstätten: a period of time otherwise lacking in fossils. Hence, they supplement the conventional fossil record and allow the fossil ranges of many groups to be extended.\n\nThe earliest trilobite fossils are about 530 million years old, but the class was already quite diverse and worldwide, suggesting they had been around for quite some time.\nThe fossil record of trilobites began with the appearance of trilobites with mineral exoskeletons – not from the time of their origin.\n\nThe earliest generally accepted echinoderm fossils appeared a little bit later, in the Late Atdabanian; unlike modern echinoderms, these early Cambrian echinoderms were not all radially symmetrical.\n\nThese provide firm data points for the \"end\" of the explosion, or at least indications that the crown groups of modern phyla were represented.\n\nThe Burgess Shale and similar lagerstätten preserve the soft parts of organisms, which provide a wealth of data to aid in the classification of enigmatic fossils. It often preserved complete specimens of organisms only otherwise known from dispersed parts, such as loose scales or isolated mouthparts. Further, the majority of organisms and taxa in these horizons are entirely soft-bodied, hence absent from the rest of the fossil record. Since a large part of the ecosystem is preserved, the ecology of the community can also be tentatively reconstructed.\nHowever, the assemblages may represent a \"museum\": a deep-water ecosystem that is evolutionarily \"behind\" the rapidly diversifying fauna of shallower waters.\n\nBecause the lagerstätten provide a mode and quality of preservation that is virtually absent outside of the Cambrian, many organisms appear completely different from anything known from the conventional fossil record. This led early workers in the field to attempt to shoehorn the organisms into extant phyla; the shortcomings of this approach led later workers to erect a multitude of new phyla to accommodate all the oddballs. It has since been realised that most oddballs diverged from lineages before they established the phyla known today – slightly different designs, which were fated to perish rather than flourish into phyla, as their cousin lineages did.\n\nThe preservational mode is rare in the preceding Ediacaran period, but those assemblages known show no trace of animal life – perhaps implying a genuine absence of macroscopic metazoans.\n\nCrustaceans, one of the four great modern groups of arthropods, are very rare throughout the Cambrian. Convincing crustaceans were once thought to be common in Burgess Shale-type biotas, but none of these individuals can be shown to fall into the crown group of \"true crustaceans\". The Cambrian record of crown-group crustaceans comes from microfossils. The Swedish Orsten horizons contain later Cambrian crustaceans, but only organisms smaller than 2 mm are preserved. This restricts the data set to juveniles and miniaturised adults.\n\nA more informative data source is the organic microfossils of the Mount Cap formation, Mackenzie Mountains, Canada. This late Early Cambrian assemblage () consists of microscopic fragments of arthropods' cuticle, which is left behind when the rock is dissolved with hydrofluoric acid. The diversity of this assemblage is similar to that of modern crustacean faunas. Most interestingly, analysis of fragments of feeding machinery found in the formation shows that it was adapted to feed in a very precise and refined fashion. This contrasts with most other early Cambrian arthropods, which fed messily by shovelling anything they could get their feeding appendages on into their mouths. This sophisticated and specialised feeding machinery belonged to a large (about 30 cm) organism, and would have provided great potential for diversification; specialised feeding apparatus allows a number of different approaches to feeding and development, and creates a number of different approaches to avoid being eaten.\n\nAfter an extinction at the Cambrian–Ordovician boundary, another radiation occurred, which established the taxa that would dominate the Palaeozoic.\n\nDuring this radiation, the total number of orders doubled, and families tripled, increasing marine diversity to levels typical of the Palaeozoic, and disparity to levels approximately equivalent to today's.\n\nThe event lasted for about the next 20–25 million years. Different authors break the explosion down into stages in different ways.\n\nEd Landing recognizes three stages: Stage 1, spanning the Ediacaran-Cambrian boundary, corresponds to a diversification of biomineralizing animals and of deep and complex burrows; Stage 2, corresponding to the radiation of molluscs and stem-group Brachiopods (hyoliths and tommotiids), which apparently arose in intertidal waters; and Stage 3, seeing the Atdabanian diversification of trilobites in deeper waters, but little change in the intertidal realm.\n\nGraham Budd synthesises various schemes to produce a compatible view of the SSF record of the Cambrian explosion, divided slightly differently into four intervals: a \"Tube world\", lasting from , spanning the Ediacaran-Cambrian boundary, dominated by Cloudina, Namacalathus ans pseudoconodont-type element; a \"Sclerite world\", seeing the rise of halkieriids, tommotiids, and hyoliths, lasting to the end of the Fortunian (c. 525 Ma); a brachiopod world, perhaps corresponding to the as yet unratified Cambrian Stage 2; and Trilobite World, kicking off in Stage 3.\n\nComplementary to the shelly fossil record, trace fossils can be divided into five subdivisions: \"Flat world\" (late Ediacaran), with traces restricted to the sediment surface; Protreozoic III (after Jensen), with increasing complexity; \"pedum\" world, initiated at the base of the Cambrian with the base of the \"T.pedum\" zone (see discussion at Cambrian#Dating the Cambrian); \"Rusophycus\" world, spanning and thus corresponding exactly to the periods of Sclerite World and Brachiopod World under the SSF paradigm; and \"Cruziana\" world, with an obvious correspondence to Trilobite World.\n\nThere is strong evidence for species of Cnidaria and Porifera existing in the Ediacaran and possible members of Porifera even before that during the Cryogenian. Bryozoans don't appear in the fossil record until after the Cambrian, in the Lower Ordovician.\n\nThe fossil record as Darwin knew it seemed to suggest that the major metazoan groups appeared in a few million years of the early to mid-Cambrian, and even in the 1980s, this still appeared to be the case.\n\nHowever, evidence of Precambrian Metazoa is gradually accumulating. If the Ediacaran \"Kimberella\" was a mollusc-like protostome (one of the two main groups of coelomates), the protostome and deuterostome lineages must have split significantly before (deuterostomes are the other main group of coelomates). Even if it is not a protostome, it is widely accepted as a bilaterian. Since fossils of rather modern-looking cnidarians (jellyfish-like organisms) have been found in the Doushantuo lagerstätte, the cnidarian and bilaterian lineages must have diverged well over .\n\nTrace fossils and predatory borings in \"Cloudina\" shells provide further evidence of Ediacaran animals. Some fossils from the Doushantuo formation have been interpreted as embryos and one (\"Vernanimalcula\") as a bilaterian coelomate, although these interpretations are not universally accepted. Earlier still, predatory pressure has acted on stromatolites and acritarchs since around .\n\nSome say that the evolutionary change was accomplished by an order of magnitude, but the presence of Precambrian animals somewhat dampens the \"bang\" of the explosion; not only was the appearance of animals gradual, but their evolutionary radiation (\"diversification\") may also not have been as rapid as once thought. Indeed, statistical analysis shows that the Cambrian explosion was no faster than any of the other radiations in animals' history. However, it does seem that some innovations linked to the explosion – such as resistant armour – only evolved once in the animal lineage; this makes a lengthy Precambrian animal lineage harder to defend. Further, the conventional view that all the phyla arose in the Cambrian is flawed; while the phyla may have diversified in this time period, representatives of the crown groups of many phyla do not appear until much later in the Phanerozoic. Further, the mineralised phyla that form the basis of the fossil record may not be representative of other phyla, since most mineralised phyla originated in a benthic setting. The fossil record is consistent with a Cambrian explosion that was limited to the benthos, with pelagic phyla evolving much later.\n\nEcological complexity among marine animals increased in the Cambrian, as well later in the Ordovician. However, recent research has overthrown the once-popular idea that disparity was exceptionally high throughout the Cambrian, before subsequently decreasing. In fact, disparity remains relatively low throughout the Cambrian, with modern levels of disparity only attained after the early Ordovician radiation.\n\nThe diversity of many Cambrian assemblages is similar to today's, and at a high (class/phylum) level, diversity is thought by some to have risen relatively smoothly through the Cambrian, stabilizing somewhat in the Ordovician. This interpretation, however, glosses over the astonishing and fundamental pattern of basal polytomy and phylogenetic telescoping at or near the Cambrian boundary, as seen in most major animal lineages. Thus Harry Blackmore Whittington's questions regarding the abrupt nature of the Cambrian explosion remain, and have yet to be satisfactorily answered.\n\nDespite the evidence that moderately complex animals (triploblastic bilaterians) existed before and possibly long before the start of the Cambrian, it seems that the pace of evolution was exceptionally fast in the early Cambrian. Possible explanations for this fall into three broad categories: environmental, developmental, and ecological changes. Any explanation must explain both the timing and magnitude of the explosion.\n\nEarth's earliest atmosphere contained no free oxygen (O); the oxygen that animals breathe today, both in the air and dissolved in water, is the product of billions of years of photosynthesis. Cyanobacteria were the first organisms to evolve the ability to photosynthesize, introducing a steady supply of oxygen into the environment. Initially, oxygen levels did not increase substantially in the atmosphere. The oxygen quickly reacted with iron and other minerals in the surrounding rock and ocean water. Once a saturation point was reached for the reactions in rock and water, oxygen was able to exist as a gas in its diatomic form. Oxygen levels in the atmosphere increased substantially afterward. As a general trend, the concentration of oxygen in the atmosphere has risen gradually over about the last 2.5 billion years.\n\nOxygen levels seem to have a positive correlation with diversity in eukaryotes well before the Cambrian period. The last common ancestor of all extant eukaryotes is thought to have lived around 1.8 billion years ago. Around 800 million years ago, there was a notable increase in the complexity and number of eukaryotes species in the fossil record. Before the spike in diversity, eukaryotes are thought to have lived in highly sulfuric environments. Sulfide interferes with mitochondrial function in aerobic organisms, limiting the amount of oxygen that could be used to drive metabolism. Oceanic sulfide levels decreased around 800 million years ago, which supports the importance of oxygen in eukaryotic diversity.\n\nThe shortage of oxygen might well have prevented the rise of large, complex animals. The amount of oxygen an animal can absorb is largely determined by the area of its oxygen-absorbing surfaces (lungs and gills in the most complex animals; the skin in less complex ones); but, the amount needed is determined by its volume, which grows faster than the oxygen-absorbing area if an animal's size increases equally in all directions. An increase in the concentration of oxygen in air or water would increase the size to which an organism could grow without its tissues becoming starved of oxygen. However, members of the Ediacara biota reached metres in length tens of millions of years before the Cambrian explosion. Other metabolic functions may have been inhibited by lack of oxygen, for example the construction of tissue such as collagen, required for the construction of complex structures, or to form molecules for the construction of a hard exoskeleton. However, animals are not affected when similar oceanographic conditions occur in the Phanerozoic; there is no convincing correlation between oxygen levels and evolution, so oxygen may have been no more a prerequisite to complex life than liquid water or primary productivity.\n\nThe amount of ozone (O) required to shield Earth from biologically lethal UV radiation, wavelengths from 200 to 300 nanometers (nm), is believed to have been in existence around the Cambrian explosion. The presence of the ozone layer may have enabled the development of complex life and life on land, as opposed to life being restricted in the water.\n\nIn the late Neoproterozoic (extending into the early Ediacaran period), the Earth suffered massive glaciations in which most of its surface was covered by ice. This may have caused a mass extinction, creating a genetic bottleneck; the resulting diversification may have given rise to the Ediacara biota, which appears soon after the last \"Snowball Earth\" episode.\nHowever, the snowball episodes occurred a long time before the start of the Cambrian, and it is hard to see how so much diversity could have been caused by even a series of bottlenecks; the cold periods may even have \"delayed\" the evolution of large size organisms.\n\nNewer research suggests that volcanically active midocean ridges caused a massive and sudden surge of the calcium concentration in the oceans, making it possible for marine organisms to build skeletons and hard body parts.\nAlternatively a high influx of ions could have been provided by the widespread erosion that produced Powell's Great Unconformity.\n\nA range of theories are based on the concept that minor modifications to animals' development as they grow from embryo to adult may have been able to cause very large changes in the final adult form. The Hox genes, for example, control which organs individual regions of an embryo will develop into. For instance, if a certain \"Hox\" gene is expressed, a region will develop into a limb; if a different Hox gene is expressed in that region (a minor change), it could develop into an eye instead (a phenotypically major change).\n\nSuch a system allows a large range of disparity to appear from a limited set of genes, but such theories linking this with the explosion struggle to explain why the origin of such a development system should by itself lead to increased diversity or disparity. Evidence of Precambrian metazoans combines with molecular data to show that much of the genetic architecture that could feasibly have played a role in the explosion was already well established by the Cambrian.\n\nThis apparent paradox is addressed in a theory that focuses on the physics of development. It is proposed that the emergence of simple multicellular forms provided a changed context and spatial scale in which novel physical processes and effects were mobilized by the products of genes that had previously evolved to serve unicellular functions. Morphological complexity (layers, segments, lumens, appendages) arose, in this view, by self-organization.\n\nHorizontal gene transfer has also been identified as a possible factor in the rapid acquisition of the biochemical capability of biomineralization among organisms during this period, based on evidence that the gene for a critical protein in the process was originally transferred from a bacterium into sponges.\n\nThese focus on the interactions between different types of organism. Some of these hypotheses deal with changes in the food chain; some suggest arms races between predators and prey, and others focus on the more general mechanisms of coevolution. Such theories are well suited to explaining why there was a rapid increase in both disparity and diversity, but they must explain why the \"explosion\" happened when it did.\n\nEvidence for such an extinction includes the disappearance from the fossil record of the Ediacara biota and shelly fossils such as \"Cloudina\", and the accompanying perturbation in the record.\n\nMass extinctions are often followed by adaptive radiations as existing clades expand to occupy the ecospace emptied by the extinction. However, once the dust had settled, overall disparity and diversity returned to the pre-extinction level in each of the Phanerozoic extinctions.\n\nAndrew Parker has proposed that predator-prey relationships changed dramatically after eyesight evolved. Prior to that time, hunting and evading were both close-range affairs – smell, vibration, and touch were the only senses used. When predators could see their prey from a distance, new defensive strategies were needed. Armor, spines, and similar defenses may also have evolved in response to vision. He further observed that, where animals lose vision in unlighted environments such as caves, diversity of animal forms tends to decrease. Nevertheless, many scientists doubt that vision could have caused the explosion. Eyes may well have evolved long before the start of the Cambrian. It is also difficult to understand why the evolution of eyesight would have caused an explosion, since other senses, such as smell and pressure detection, can detect things at a greater distance in the sea than sight can; but the appearance of these other senses apparently did not cause an evolutionary explosion.\n\nThe ability to avoid or recover from predation often makes the difference between life and death, and is therefore one of the strongest components of natural selection. The pressure to adapt is stronger on the prey than on the predator: if the predator fails to win a contest, it loses a meal; if the prey is the loser, it loses its life.\n\nBut, there is evidence that predation was rife long before the start of the Cambrian, for example in the increasingly spiny forms of acritarchs, the holes drilled in \"Cloudina\" shells, and traces of burrowing to avoid predators. Hence, it is unlikely that the \"appearance\" of predation was the trigger for the Cambrian \"explosion\", although it may well have exhibited a strong influence on the body forms that the \"explosion\" produced. However, the intensity of predation does appear to have increased dramatically during the Cambrian as new predatory \"tactics\" (such as shell-crushing) emerged. This rise of predation during the Cambrian was confirmed by the temporal pattern of the median predator ratio at the scale of genus, in fossil communities covering the Cambrian and Ordovician periods, but this pattern is not correlated to diversification rate. This lack of correlation between predator ratio and diversification over the Cambrian and Ordovician suggests that predators did not trigger the large evolutionary radiation of animals during this interval. Thus the role of predators as triggerers of diversification may have been limited to the very beginning of the \"Cambrian explosion\".\n\nGeochemical evidence strongly indicates that the total mass of plankton has been similar to modern levels since early in the Proterozoic. Before the start of the Cambrian, their corpses and droppings were too small to fall quickly towards the seabed, since their drag was about the same as their weight. This meant they were destroyed by scavengers or by chemical processes before they reached the sea floor.\n\nMesozooplankton are plankton of a larger size. Early Cambrian specimens filtered microscopic plankton from the seawater. These larger organisms would have produced droppings and corpses that were large enough to fall fairly quickly. This provided a new supply of energy and nutrients to the mid-levels and bottoms of the seas, which opened up a huge range of new possible ways of life. If any of these remains sank uneaten to the sea floor they could be buried; this would have taken some carbon out of circulation, resulting in an increase in the concentration of breathable oxygen in the seas (carbon readily combines with oxygen).\n\nThe initial herbivorous mesozooplankton were probably larvae of benthic (seafloor) animals. A larval stage was probably an evolutionary innovation driven by the increasing level of predation at the seafloor during the Ediacaran period.\n\nMetazoans have an amazing ability to increase diversity through coevolution. This means that an organism's traits can lead to traits evolving in other organisms; a number of responses are possible, and a different species can potentially emerge from each one. As a simple example, the evolution of predation may have caused one organism to develop a defence, while another developed motion to flee. This would cause the predator lineage to split into two species: one that was good at chasing prey, and another that was good at breaking through defences. Actual coevolution is somewhat more subtle, but, in this fashion, great diversity can arise: three quarters of living species are animals, and most of the rest have formed by coevolution with animals.\n\nEvolving organisms inevitably change the environment they evolve in. The Devonian colonization of land had planet-wide consequences for sediment cycling and ocean nutrients, and was likely linked to the Devonian mass extinction. A similar process may have occurred on smaller scales in the oceans, with, for example, the sponges filtering particles from the water and depositing them in the mud in a more digestible form; or burrowing organisms making previously unavailable resources available for other organisms.\n\nThe explosion may not have been a significant evolutionary event. It may represent a threshold being crossed: for example a threshold in genetic complexity that allowed a vast range of morphological forms to be employed. This genetic threshold may have a correlation to the amount of oxygen available to organisms. Using oxygen for metabolism produces much more energy than anaerobic processes. Organisms that use more oxygen have the opportunity to produce more complex proteins, providing a template for further evolution. These proteins translate into larger, more complex structures that allow organisms better to adapt to their environments. With the help of oxygen, genes that code for these proteins could contribute to the expression of complex traits more efficiently. Access to a wider range of structures and functions would allow organisms to evolve in different directions, increasing the number of niches that could be inhabited. Furthermore, organisms had the opportunity to become more specialized in their own niches.\n\nThe \"Cambrian explosion\" can be viewed as two waves of metazoan expansion into empty niches: first, a coevolutionary rise in diversity as animals explored niches on the Ediacaran sea floor, followed by a second expansion in the early Cambrian as they became established in the water column. The rate of diversification seen in the Cambrian phase of the explosion is unparalleled among marine animals: it affected all metazoan clades of which Cambrian fossils have been found. Later radiations, such as those of fish in the Silurian and Devonian periods, involved fewer taxa, mainly with very similar body plans. Although the recovery from the Permian-Triassic extinction started with about as few animal species as the Cambrian explosion, the recovery produced far fewer significantly new types of animals.\n\nWhatever triggered the early Cambrian diversification opened up an exceptionally wide range of previously unavailable ecological niches. When these were all occupied, limited space existed for such wide-ranging diversifications to occur again, because strong competition existed in all niches and incumbents usually had the advantage. If a wide range of empty niches had continued, clades would be able to continue diversifying and become disparate enough for us to recognise them as different phyla; when niches are filled, lineages will continue to resemble one another long after they diverge, as limited opportunity exists for them to change their life-styles and forms.\n\nThere were two similar explosions in the evolution of land plants: after a cryptic history beginning about , land plants underwent a uniquely rapid adaptive radiation during the Devonian period, about . Furthermore, Angiosperms (flowering plants) originated and rapidly diversified during the Cretaceous period.\n\n\nTimeline References:\n\n", "id": "19349161", "title": "Cambrian explosion"}
{"url": "https://en.wikipedia.org/wiki?curid=55257518", "text": "Amity-enmity complex\n\nThe amity-enmity complex was a term introduced by Sir Arthur Keith. His work, \"A New Theory of Human Evolution\" (1948), posited that humans evolved as differing races, tribes, and cultures, exhibiting patriotism, morality, leadership and nationalism. Those who belong are part of the in-group, and tolerated; all others are classed as out-group, and subject to hostility; 'The code of enmity is a necessary part of the machinery of evolution. He who feels generous towards his enemy...has given up his place in the turmoil of evolutionary competition.' Conscience in humans evolved a duality; to protect and save friends,\nand also to hate and fight enemies. \nKeith's work summarized earlier opinions on human tribalism by Charles Darwin, Alfred Russel Wallace, and Herbert Spencer.\n\n\nThe amity-enmity complex maintains 'tribal spirit' and thus unity, of the community, 'as long as personal contact between its members is possible.' If the community grows beyond this limitation, then disruption, swarming and disintegration occur. Modern mass communication enables communities 'of 100 million' to remain intact.\n\nKeith expressed regret that this phenomenon, which explains so much, had not become common knowledge: \"[W]e eternally experience the misery... of each new manifestation of the complex, then invent some new 'ism' to categorise this behavior as an evil, dealing with a common behavioural trait piecemeal [instead of] finally grasping and understanding the phenomenon.\"\n\nColleges, sports teams, churches, trades unions, female fashions and political parties enable people to exhibit tribal loyalty within large, mass-communicating nations. 'In politics we have to take sides.' But all these 'petty manifestations' are cast aside in time of war.\nBismarck, Abraham Lincoln and Lloyd George are cited as statesmen who knew how to exploit the tribal spirit for political ends.\n\nRobert Ardrey pointed out that similar behavior can be observed in most primates, especially baboons and chimps. \"Nationalism as such is no more than a human expression of the animal drive to maintain and defend a territory... the mentality of the single Germanic tribe under Hitler differed in no way from that of early man or late baboon.\"\n\nThe amity-enmity complex is a serious obstacle to world peace and world government, and may even lead to nuclear holocaust: \"How can we get along without war?... if we fail to get along without war, the future will be as lacking in human problems as it will be remarkably lacking in men.\"\n\nDesmond Morris makes a prescriptive point: \"We must try to step outside our groups and look down on human battlefields with the unbiased eye of a hovering Martian.\" And he warns that \"the truly violent species all appear to have exterminated themselves, a lesson we should not overlook.\" The inherited aggression of the amity-enmity rivalry between communities is rationalized under a \"persistent cloak of ideology... a matter of ideals, moral principles, social philosophies or religious beliefs... [O]nly an immense amount of intellectual restraint will save the situation.\"\n\nAfter World War Two, a debate about the place of instinct and learning (the nature-versus-nurture debate) has occurred. According to Steven Pinker, the \"bitter lessons of lynchings, world wars, and the Holocaust\" have caused \"prevailing theories of mind\" to be \"refashioned to make racism and sexism as untenable as possible. The doctrine of the blank slate became entrenched in intellectual life.\"\n\nPinker makes the point that \"conflicts of interest are inherent to the human condition.\" Man is a product of nature, as much as malarial mosquitoes; both \"are doing exactly what evolution designed them to do, even if the outcome makes people suffer... [We] cannot call their behavior pathological... [T]he belief that violence is an aberration is dangerous.\"\n\n", "id": "55257518", "title": "Amity-enmity complex"}
{"url": "https://en.wikipedia.org/wiki?curid=55895027", "text": "Disposable soma theory of aging\n\nThe disposable soma theory of aging states that organisms age due to an evolutionary trade-off between growth, reproduction, and DNA repair maintenance. Formulated by Thomas Kirkwood, the disposable soma theory explains that an organism only has a limited amount of resources or \"soma\" that it can allocate to its various cellular processes. Therefore, a greater investment in growth and reproduction would result in reduced investment in DNA repair maintenance, leading to increased cellular damage, shortened telomeres, accumulation of mutations, compromised stem cells, and ultimately, senescence. Although many models, both animal and human, have appeared to support this theory, parts of it are still controversial. \nSpecifically, while the evolutionary trade-off between growth and aging has been well established, \nthe relationship between reproduction and aging is still without scientific consensus, and the cellular mechanisms largely undiscovered.\n\nBritish biologist Thomas Kirkwood first proposed the disposable soma theory of aging in a 1977 \"Nature\" review article. The theory was inspired by Leslie Orgel's Error Catastrophe Theory of Aging, which was published fourteen years earlier, in 1963. Orgel believed that the process of aging arose due to mutations acquired during the replication process, and Kirkwood developed the disposable soma theory in order to mediate Orgel's work with evolutionary genetics.\n\nThe disposable soma theory of aging acts on the premise that there is a tradeoff in resource allocation between somatic maintenance and reproductive investment. Too low an investment in self-repair would be evolutionarily unsound, as the organism would likely die before reproductive age. However, too high an investment in self-repair would also be evolutionarily unsound due to the fact that one's offspring would likely die before reproductive age. Therefore, there is a compromise and resources are partitioned accordingly. However, this compromise is thought to damage somatic repair systems, which can lead to progressive cellular damage and senescence. Repair costs can be categorized into three groups: (1) the costs of increased durability of nonrenewable parts; (2) the costs of maintenance involving cell renewal, and (3) the costs of intracellular maintenance. In a nutshell, aging and decline is essentially a tradeoff for increased reproductive robustness in youth.\n\nMuch research has been done on the antagonistic effects of increased growth on lifespan. Specifically, the hormone insulin-like growth factor 1 (IGF-1), binds to a cell receptor, leading to a phosphorylation cascade. This cascade results in kinases phosphorylating forkhead transcription factor (FOXO), deactivating it. Deactivation of FOXO results in an inability to express genes involved in responding to oxidative stress response, such as antioxidants, chaperones, and heat-shock proteins. \nAdditionally, uptake of IGF-1 stimulates the mTOR pathway, which activates protein synthesis (and therefore growth) through upregulation of the translation-promoting S6K1, and also inhibits autophagy, a process necessary for recycling of damaged cellular products. Decline of autophagy causes neurodegeneration, protein aggregation and premature aging. Lastly, studies have also indicated that the mTOR pathway also alters immune responses and stimulates cyclin-dependent kinase (CDK) inhibitors such as p16 and p21. This leads to alteration of the stem-cell niche and results in stem cell exhaustion, another theorized mechanism of aging.\n\nThe mechanism of why reproduction inhibits lifespan with regards to multicellular organisms is still unclear. Although many models do illustrate an inverse relationship, and the theory makes sense from an evolutionary perspective, the cellular mechanisms have yet to be explored. However, with regards to cellular replication, the progressive shortening of telomeres is a mechanism which limits the amount of generations of a single cell may undergo. Furthermore, in unicellular organisms like \"Saccharomyces cerevisiae\", the formation of extrachromosomal rDNA circles (ERCs) in mother cells (but not daughter cells) upon every subsequent division is an identifiable type of DNA damage that is associated with replication. These ERCs accumulate over time and eventually trigger replicative senescence and death of the mother cell.\n\nThere is a large body of evidence indicating the negative effects of growth on longevity across many species. As a general rule, individuals of a smaller size generally live longer than larger individuals of the same species.\n\nIn dwarf models of mice, such Snell or Ames mice, mutations have arisen, either rendering them incapable of producing IGF-1 or unable to have adequate receptors for IGF-1 uptake. Furthermore, mice injected with growth hormone have been shown to have progressive weight loss, roughing of the coat, curvature of the spine, enlargement of the organs, kidney lesions and increased cancer risk. This effect is also seen in different breeds of dogs, where smaller breeds of dogs typically live significantly longer compared to their larger counterparts. Selectively bred for their small size, smaller dog breeds like the Chihuahua (average lifespan of 15–20 years) have the B/B genotype for the IGF-1 haplotype, reducing the amount of IGF-1 produced. Conversely, large dogs like the Great Dane (average lifespan of 6–8 years) are homozygous for the IGF-1 I allele, which increases the amount of IGF-1 production.\n\nInitially, it was believed that growth hormone actually prolonged lifespan due to a 1990 study that indicated that injection of growth hormone to men over 60 years of age appeared to reverse various biomarkers implicated in aging, such as decreased muscle mass, bone density, skin thickness, and increased adipose tissue. However, a 1999 study found that administering growth hormone also significantly increased mortality rate. Recent genomic studies have confirmed that the genes involved in growth hormone uptake and signaling are largely conserved across a plethora of species, such as yeast, nematodes, fruit flies, mice and humans. These studies have also shown that individuals with Laron syndrome, an autosomal recessive disorder resulting in dwarfism due to defects in growth hormone receptors, have increased lifespan. Additionally, these individuals have much lower incidences of age-related diseases such as type 2 diabetes and cancer. Lastly, human centenarians around the world are disproportionately of short stature, and have low levels of IGF-1.\n\nNumerous studies have found that lifespan is inversely correlated with both the total amount of offspring birthed, as well as the age at which females first gives birth, also known as primiparity. Additionally, it has been found that reproduction is a costly mechanism that alters the metabolism of fat. Lipids invested in reproduction would be unable to be allocated to support mechanisms involved in somatic maintenance.\n\nThe disposable soma theory has been consistent with the majority of animal models. It was found in numerous animal studies that castration or genetic deformities of reproduction organs was correlated with increased lifespan. Moreover, in red squirrels, it was found that females with an early primiparity achieved the highest immediate and lifetime reproductive success. However, it was also found that these same individuals had a decreased median and maximum lifespan. Specifically squirrels who mated earlier had a 22.4% rate of mortality until two years of age compared to a 16.5% rate of mortality in late breeders. In addition, these squirrels had an average maximum lifespan of 1035 days compared to an average maximum lifespan of 1245 days for squirrels that bred later.\n\nIn another study, researchers selectively bred fruit flies over three years to develop two different strains, an early-reproducing strain and a late-reproducing strain. Interestingly, the late-reproducing line had a significantly longer lifespan than the early-reproducing line. Even more telling was that when the researchers introduced a mutation in the ovarian-associated gene \"ovoD1\", resulting in defective oogenesis, the differences in lifespan between the two lines disappeared. The researchers in this case concluded that \"aging has evolved primarily because of the damaging effects of reproduction earlier in life\".\n\nProminent aging researcher Steven Austad also performed a large-scale ecological study on the coast of Georgia in 1993. Austad isolated two opossum populations, one from the predator-infested mainland and one from the predator-absent nearby island of Sapelo. According to the disposable soma theory, a genetically isolated population subject to low environmentally-induced mortality would evolve delayed reproduction and aging. This is because without the pressure of predation, it would be evolutionarily advantageous to allocate more resources to somatic maintenance than reproduction, as early offspring mortality would be low. As predicted, even after controlling for predation, the isolated population had a longer lifespan, delayed primiparity, and reduced aging biomarkers such as tail collagen cross-linking.\n\nIn general, only a few studies exist in human models. It was found that castrated men live longer than their fertile counterparts. Further studies found that in British women, primiparity was earliest in women who died early and latest in women who died at the oldest ages. Furthermore, increased number of children birthed was associated with a decreased lifespan. A final study found that female centenarians were more likely to have children in later life compared average, especially past the age of 40. The researchers discovered that 19.2% of female centenarians had their first child after the age of 40, compared to 5.5% of the rest of the female population.\n\nThere are numerous studies that support cellular damage, often due to a lack of somatic maintenance mechanisms, as a primary determinant for aging, and these studies have given rise to the free radical theory of aging and the DNA damage theory of aging. One study found that the cells of short-living rodents \"in vitro\" show much greater mutation rates and a general lack of genome surveillance compared to human cells and are far more susceptible to oxidative stress. \nOther studies have been conducted on the naked mole rat, a rodent species with remarkable longevity (30 years), capable of outliving the brown rat (3 years) by ten-fold. Additionally, almost no incidence cancer has ever been detected in naked mole rats. Nearly all of the differences found between these two organisms, which are otherwise rather genetically similar, was in somatic maintenance. Naked mole rats were found to have higher levels of superoxide dismutase, a reactive oxygen species clearing antioxidant. In addition, naked mole rats had higher levels of base excision repair, DNA damage response signaling, homologous recombination repair, mismatch repair, nucleotide excision repair, and non-homologous end joining. In fact, many of these processes were near or exceeded human levels. Proteins from naked mole rats were also more resistant to oxidation, misfolding, ubiquitination, and had increased translational fidelity.\n\nFurther studies have been conducted on patients with Hutchinson-Gilford Progeria Syndrome (HGPS), a condition that leads to premature aging. Patients with HGPS typically age about seven times faster than average and usually succumb to the disease in their early teens. Patients with HGPS have cellular defects, specifically in the lamin proteins, which regulate the organization of the lamina and nuclear envelope for mitosis. \nLastly, as mentioned previously, it has been found that the suppression of autophagy is associated with reduced lifespan, while stimulation is associated with extended lifespan. Activated in times of caloric restriction, autophagy is a process that prevents cellular damage through clearance and recycling of damaged proteins and organelles.\n\nOne of the main weaknesses of the disposable soma theory is that it does not postulate any specific cellular mechanisms to which an organism shifts energy to somatic repair over reproduction. Instead, it only offers an evolutionary perspective on why aging may occur due to reproduction. Therefore, parts of it are rather limited outside of the field of evolutionary biology.\n\nCritics have pointed out the supposed inconsistencies of the disposable soma theory due to the observed effects of caloric restriction, which is correlated with increased lifespan. Although it activates autophagy, according to classical disposable soma principles, with less caloric intake, there would less total energy to be distributed towards somatic maintenance, and decreased lifespan would be observed (or at least the positive autophagic effects would be balanced out). However, Kirkwood, alongside his collaborator Darryl P. Shanley, assert that caloric restriction triggers an adaptive mechanism which causes the organism to shift a higher proportion of resources to somatic maintenance, away from reproduction. This theory is supported by multiple studies, which show that caloric restriction typically results in impaired fertility, but leave an otherwise healthy organism. Evolutionarily, an organism would want to delay reproduction to when resources were more plentiful. During a resource-barren period, it would evolutionarily unwise to invest resources in progeny that would be unlikely to survive in famine. Mechanistically, the NAD-dependent deacetylase Sirtuin 1 (SIRT-1) is upregulated during low-nutrient periods. SIRT-1 increases insulin sensitivity, decreases the amount of inflammatory cytokines, stimulates autophagy, and activates FOXO, the aforementioned protein involved in activating stress response genes. SIRT-1 is also found to result in decreased fertility.\n\nIn additional to differential partitioning of energy allocation during caloric restriction, less caloric intake would result in less metabolic waste in the forms of free radicals like hydrogen peroxide, superoxide and hydroxyl radicals, which damage important cellular components, particularly mitochondria. Elevated levels of free radicals in mice has been correlated with neurodegeneration, myocardial injury, severe anemia, and premature death.\n\nAnother primary criticism of the disposable soma theory is that it fails to account for why women tend to live longer than their male counterparts. Afterall, females invest significantly more resources into reproduction and according to the classical disposable soma principles, this would compromise energy diverted to somatic maintenance. However, this can be reconciled with the grandmother hypothesis. The Grandmother Hypothesis states that menopause comes about into older women in order to restrict the time of reproduction as a protective mechanism. This would allow women to live longer and increase the amount of care they could provide to their grandchildren, increasing their evolutionary fitness. And so, although women do invest a greater proportion of resources into reproduction during their fertile years, their overall reproductive period is significantly shorter than men, who are able of reproduction during and even beyond middle age. Additionally, males invest more resources into growth, and have significantly higher levels of IGF-1 compared to females, which is correlated with decreased lifespan. Other variables such as increased testosterone levels in males are not accounted for. Increased testosterone is often associated with reckless behaviour, which may lead to a high accidental death rate.\n\nA few contradicting animal models weaken the validity of the disposable soma theory. This includes studies done on the aforementioned naked mole rats. In these studies, it was found that reproductive naked mole rats actually show significantly increased lifespans compared to non-reproductive individuals, which contradicts the principles of diposable soma. However, although these naked mole rats are mammalian, they are highly atypical in terms of aging studies and may not serve as the best model for humans. For example, naked mole rats have a disproportionately high longevity quotient and live in eusocial societies, where breeding is usually designated to a queen.\n\nThe disposable soma theory is tested disproportionately on female organisms for the relationship between reproduction and aging, as females carry a greater burden in reproduction. Additionally, for the relationship between growth and aging, studies are disproportionately conducted on males, to minimize the hormonal fluctuations that occur with menstrual cycling. Lastly, genetic and environmental factors, rather than reproductive patterns, may explain the variations in human lifespan. For example, studies have shown that poorer individuals, to whom nutritious food and medical care is less accessible, typically have higher birth rates and earlier primiparity.\n\n\n", "id": "55895027", "title": "Disposable soma theory of aging"}
{"url": "https://en.wikipedia.org/wiki?curid=55968776", "text": "Reciprocal causation\n\nIn biology, reciprocal causation arises when developing organisms are both \"products\" of evolution as well as \"causes\" of evolution. Formally, reciprocal causation exists when process A is a cause of process B and, subsequently, process B is a cause of process A, with this feedback potentially repeated. Some researchers, particularly advocates of the extended evolutionary synthesis, promote the view that causation in biological systems is inherently reciprocal.\n\nHarvard evolutionary biologist Ernst Mayr (1961) suggested that there are two fundamentally different types of causation in biology, ‘ultimate’ and ‘proximate’. Ultimate causes (e.g. natural selection) were seen as (i) providing historical accounts for the existence of an organism’s features, and (ii) explaining the function or ‘goal-directedness’ of living beings. In contrast, proximate causes (e.g. physiology) were seen as explaining how biological systems work. According to Mayr, the evolutionary sciences study ultimate causes and the rest of biology studies proximate causes. In some of his works, Mayr considered these domains autonomous:\n“The clarification of the biochemical mechanism by which the genetic program is translated into the phenotype tells us absolutely nothing about the steps by which natural selection has built up the particular genetic program.”\nMayr, 1980\nThere has been widespread acceptance of the proximate-ultimate dichotomy within the evolutionary sciences. However, many biologists, psychologists and philosophers have taken issue with Mayr’s corollary that the proximate-ultimate distinction implies that development is irrelevant to evolution. For instance, evolutionary biologist Mary Jane West-Eberhard writes:\n“The proximate-ultimate distinction has given rise to a new confusion, namely, a belief that proximate causes of phenotypic variation have nothing to do with ultimate, evolutionary explanation.”\nWest-Eberhard, 2003\nMayr’s position implied a unidirectional or linear conception of causation for both development and evolution: genotypes cause phenotypes (proximate causation), whilst through natural selection, changes in environments cause changes in organisms (ultimate causation). Reciprocal causation was proposed as an alternative to this linear characterization. (see also ) It emphasizes how causation cycles through biological systems recursively, allowing proximate causes to feed back and thereby feature in ultimate explanations. \n\nReciprocal causation features in several explanations within contemporary evolutionary biology, including sexual selection theory, coevolution, habitat selection, and frequency-dependent selection. In these examples, the source of selection on a trait coevolves with the trait itself, therefore causation is reciprocal and developmental processes potentially become relevant to evolutionary accounts. For instance, a peacock’s tail evolves through mating preferences in peahens, and those preferences coevolve with the male trait. The ‘ultimate explanation’ for the male trait is the prior existence of female preferences, proximately manifest in differential peahen mate choice decisions, whilst the ‘ultimate explanation’ for the peahens’ mating preferences is the prior existence of variation in the peacock’s tail associated with fitness. This example illustrates how reciprocal causation is not a rejection of the proximate-ultimate distinction itself, but instead a rejection of the implication that developmental processes should not feature in evolutionary explanations.\n\nReciprocal causation also applies in other domains of evolutionary biology. The extended evolutionary synthesis emphasizes how developmental events, including both the causal effects of environments on organisms (for instance, arising through developmental plasticity, or epigenetic inheritance) and the causal effects of organisms on environments (e.g. niche construction), can direct the course of evolution. Developmental plasticity, niche construction, extra-genetic forms of inheritance and developmental bias are recognized as playing evolutionary roles that cannot be reduced to natural selection of genetically encoded characters or strategies. Proximate causes are not autonomous from natural selection, but rather feed back to influence the rate and direction of adaptive evolution. This goes beyond the recognition that ontogenetic processes can impose constraints on the action of selection, or that proximate and ultimate processes interact. Rather, developmental processes are also seen as a source of evolutionary novelty, initiators of evolutionary episodes, and co-directors of patterns of evolutionary change.\n\nAcceptance or rejection of Mayr’s proximate-ultimate distinction may lie at the centre of several major debates within contemporary biology, concerning evo devo (evolutionary developmental biology), niche construction, cultural evolution, human cooperation, and the evolution of language. According to some biologists and philosophers, these disputes share a common pattern. On one side are researchers who consider that interaction and feedback processes traditionally characterized as ‘proximate’ have explanatory value for ‘ultimate’ evolutionary questions. Their concern is that the proximate-ultimate distinction has discouraged consideration of the manner in which developmental processes can set the evolutionary agenda, for instance, by introducing innovations, channeling phenotypic variation, or initiating evolutionary episodes through modifying selection pressures. One the other side are researchers who largely adopt Mayr’s stance with a clean separation of proximate and ultimate causation. For the latter, a failure to respect Mayr’s dichotomy is considered a sign of confusing an evolutionary explanation with a mechanistic explanation.\n", "id": "55968776", "title": "Reciprocal causation"}
{"url": "https://en.wikipedia.org/wiki?curid=3592098", "text": "Industrial melanism\n\nIndustrial melanism is an evolutionary effect prominent in several arthropods, where dark pigmentation (melanism) has evolved in an environment affected by industrial pollution, including sulphur dioxide gas and dark soot deposits. Sulphur dioxide kills lichens, leaving tree bark bare where in clean areas it is boldly patterned, while soot darkens bark and other surfaces. Darker pigmented individuals have a higher fitness in those areas as their camouflage matches the polluted background better; they are thus favoured by natural selection. This change, extensively studied by Bernard Kettlewell, is a popular teaching example in Darwinian evolution, providing evidence for natural selection. Kettlewell's results have been challenged by zoologists, creationists and the journalist Judith Hooper, but later researchers have upheld Kettlewell's findings.\nIndustrial melanism is widespread in the Lepidoptera (butterflies and moths), involving over 70 species such as \"Odontopera bidentata\" (scalloped hazel) and \"Lymantria monacha\" (dark arches), but the most studied is the evolution of the peppered moth, \"Biston betularia\". It is also seen in a beetle, \"Adalia bipunctata\" (two-spot ladybird), where camouflage is not involved as the insect has conspicuous warning coloration, and in the seasnake \"Emydocephalus annulatus\" where the melanism may help in excretion of trace elements through sloughing of the skin. The rapid decline of melanism that has accompanied the reduction of pollution, in effect a natural experiment, makes natural selection for camouflage \"the only credible explanation\".\n\nOther explanations for the observed correlation with industrial pollution have been proposed, including strengthening the immune system in a polluted environment, absorbing heat more rapidly when sunlight is reduced by air pollution, and the ability to excrete trace elements into melanic scales and feathers.\n\nIndustrial melanism was first noticed in 1900 by the geneticist William Bateson; he observed that the colour morphs were inherited, but did not suggest an explanation for the polymorphism.\n\nIn 1906, the geneticist Leonard Doncaster described the increase in frequency of the melanic forms of several moth species from about 1800 to 1850 in the heavily industrialised north-west region of England.\n\nIn 1924, the evolutionary biologist J. B. S. Haldane constructed a mathematical argument showing that the rapid growth in frequency of the \"carbonaria\" form of the peppered moth, \"Biston betularia\", implied selective pressure.\n\nFrom 1955 onwards, the geneticist Bernard Kettlewell conducted a series of experiments exploring the evolution of melanism in the peppered moth. He used a capture-mark-recapture technique to show that dark forms survived better than light ones.\n\nBy 1973, pollution in England had begun to decrease, and the dark \"carbonaria\" form had declined in frequency. This provided convincing evidence, gathered and analysed by Kettlewell and others such as the entomologist and geneticist Michael Majerus and the population geneticist Laurence M. Cook, that its rise and fall had been caused by natural selection in response to the changing pollution of the landscape.\n\nIndustrial melanism is known from over 70 species of moth that Kettlewell found in England, and many others from Europe and North America.\nAmong these, \"Apamea crenata\" (clouded border brindle moth) and \"Acronicta rumicis\" (knot grass moth) are always polymorphic, though the melanic forms are more common in cities and (like those of the peppered moth) are declining in frequency as those cities become less polluted.\n\nAmong other insects, industrial melanism has been observed in a beetle, \"Adalia bipunctata\", the two-spot ladybird.\n\nIn the vertebrates, industrial melanism is known from the turtle-headed seasnake \"Emydocephalus annulatus\", and may be present in urban feral pigeons.\n\nOriginally, peppered moths lived where light-colored lichens covered the trees. For camouflage from predators against that clean background, they had generally light coloration. During the Industrial Revolution in England, sulphur dioxide pollution in the atmosphere reduced the lichen cover, while soot blackened the bark of urban trees, making the light-colored moths more vulnerable to predation. This provided a selective advantage to the gene responsible for melanism, and the darker-colored moths increased in frequency. The melanic phenotype of \"Biston betularia\" has been calculated to give a fitness advantage as great as 30 per cent. By the end of the 19th century it almost completely replaced the original light-coloured type (var. \"typica\"), forming a peak of 98% of the population in 1895.\n\nMelanic \"B. betularia\" have been widely observed in North America. In 1959, 90% of \"B. betularia\" in Michigan and Pennsylvania were melanic. By 2001, melanism dropped to 6% of the population, following clean air legislation. The drop in melanism was correlated with an increase in species diversity of lichens, a decrease in the atmospheric pollutant sulphur dioxide, and an increase in the pale phenotype. The return of lichens is in turn directly correlated with the reduction in atmospheric sulphur dioxide.\n\nKettlewell's experiments were criticised by the zoologist Theodore David Sargent, who failed to reproduce Kettlewell's results between 1965 and 1969, and argued that Kettlewell had specially trained his birds to give the desired results. \nMichael Majerus however found that Kettlewell was basically correct in concluding that differential bird predation in a polluted environment was the primary cause of industrial melanism in the peppered moth. The story was in turn taken up in a 2002 book \"Of Moths and Men\", by the journalist Judith Hooper, asserting that Kettlewell's findings were fraudulent. The story was picked up by creationists who repeated the assertions of fraudulence. Zoologists including L. M. Cook, B. S. Grant, Majerus and David Rudge however all upheld Kettlewell's account, finding that each of Hooper's and the creationists' claims collapsed when the facts were examined. \n\nIt has been suggested that the demonstrated relationship between melanism and pollution can not be fully proven because the exact reason for increase in survivability can not be tracked and pin-pointed. However, as air quality has improved in industrial areas of America and Britain, through improved regulation, offering the conditions for a natural experiment, melanism has sharply declined in moths including \"B. betularia\" and \"Odontopera bidentata\". Cook and J. R. G. Turner have concluded that \"natural selection is the only credible explanation for the overall decline\", and other biologists working in the area concur with this judgement.\n\nIn 1921, the evolutionary biologist Richard Goldschmidt argued that the observed increase in the melanic form of the black arches moth, \"Lymantria monacha\", could not have been caused by mutation pressure alone, but required a selective advantage from an unknown cause: he did not consider camouflage as an explanation.\n\nNearly a century later, it was suggested that the moth's industrial melanism might, in addition (pleiotropy) to providing camouflage with \"the well-known protective dark coloration\", also confer better immunity to toxic chemicals from industrial pollution. The darker forms have a stronger immune response to foreign objects; these are encapsulated by haemocytes (insect blood cells), and the capsule so formed is then hardened with deposits of the dark pigment, melanin.\n\nA non-camouflage mechanism has been suggested for some vertebrates. In tropical ocean regions subject to industrial pollution the turtle-headed seasnake \"Emydocephalus annulatus\" is more likely to be melanic. These snakes shed their skin every two to six weeks. Sloughed skin contains toxic minerals, higher for dark skin, so industrial melanism could be selected for through improved excretion of trace elements. The same may apply in the case of urban feral pigeons, which have the ability to remove trace metals such as zinc to their feathers. However, toxic lead was not found to accumulate in feathers, so the putative mechanism is limited in its range.\n\nMelanic forms of the two-spot ladybird \"Adalia bipunctata\" are very frequent in and near cities, and rare in unpolluted countryside, so they appear to be industrial. Ladybirds are aposematic (with conspicuous warning coloration), so camouflage cannot explain the distribution. A proposed explanation is that the melanic forms have a thermal advantage directly linked to the pollution aspect of industrialization, since smoke and particulates in the air reduce the amount of sunlight that reaches the habitats of these species. Melanic phenotypes should then be favoured by natural selection, as the dark coloration absorbs the limited sunlight better. A possible explanation might be that in colder environments, the thermal advantages of industrial melanism might increase activity and the likelihood to mate. In the Netherlands, melanic \"A. bipunctata\" had a distinct mating advantage over the non-melanic form.\n\nHowever, thermal melanism failed to explain the distribution of the species near Helsinki where the city forms a relatively warm 'heat island', while near the Finnish coast there is more sunlight as well as more melanism, so the selective pressure driving melanism requires a different explanation. A study in Birmingham similarly found no evidence of thermal melanism but a strong correlation with smoke pollution; melanism declined from 1960 to 1978 as the city became cleaner. Further, the same study found that a related species, \"Adalia decempunctata\", experienced no change in frequency of melanism in the same places in that period.\n", "id": "3592098", "title": "Industrial melanism"}
{"url": "https://en.wikipedia.org/wiki?curid=425636", "text": "Christmas Bird Count\n\nThe Christmas Bird Count (CBC) is a census of birds in the Western Hemisphere, performed annually in the early Northern-hemisphere winter by volunteer birdwatchers and administered by the National Audubon Society. The purpose is to provide population data for use in science, especially conservation biology, though many people participate for recreation.\n\nUp through the 19th century, many North Americans participated in the tradition of Christmas \"side hunts\", in which they competed at how many birds they could kill, regardless of whether they had any use for the carcasses and of whether the birds were beneficial, beautiful, or rare. In December 1900, the U.S. ornithologist Frank Chapman, founder of Bird-Lore (which became Audubon magazine), proposed counting birds on Christmas instead of killing them. \n\nThat year, 27 observers took part in the first count in 25 places in the United States and Canada, 15 of them in the northeastern U.S. from Massachusetts to Philadelphia. Since then the counts have been held every winter, usually with increasing numbers of observers. For instance, the 101st count, in the winter of 2000–2001, involved 52,471 people in 1,823 places in 17 countries (but mostly in the U.S. and Canada). During the 113th count (winter 2012–2013), 71,531 people participated in 2,369 locations. The National Audubon Society now partners with Bird Studies Canada, the Gulf Coast Bird Observatory of Texas (responsible for CBCs in Mexico), and the Red Nacional de Observadores de Aves (RNOA, National Network of Bird Observers) and the Instituto Alexander von Humboldt of Colombia.\n\nThe greatest number of bird species ever reported by any U.S. location in a single count is 250, observed on December 19, 2005 in the Matagorda County-Mad Island Marsh count circle around Matagorda and Palacios, Texas. The greatest number of bird species ever reported by a CBC circle in the world is 529, observed on December 21, 2013 in the Cosanga-Narupa count (previously known as the Yanayacu count) on the eastern slope of the Andes in Ecuador. \n\nEach individual count is performed in a \"count circle\" with a diameter of 15 miles or 24 kilometres. At least ten volunteers, including a compiler to manage things, count in each circle. They break up into small parties and follow assigned routes, which change little from year to year, counting every bird they see. In most count circles, some people also watch feeders instead of following routes.\n\nCounts can be held on any day from December 14 to January 5 inclusive.\n\nThe results are by no means as accurate as a human census. Not all the area in the count circles is covered, and not every bird along the routes is seen or identified. Big flocks can't be counted precisely. Also, telling whether a bird has been counted twice can be difficult. The rules address this problem by prohibiting counting birds when retracing one's route, except for species that the party hasn't seen before. Also, when a large roost of some species occurs in a count circle, an expert estimates the number for that species during the morning or evening and usually no individuals are counted at other times. Observers can attempt to keep track of flocks of mobile birds such as crows, and can use their judgement, even sometimes recognizing an individual bird or at least that two birds of the same species are different individuals.\n\nThe results, providing data on winter ranges of birds, are complementary to those of the Breeding Bird Surveys.\n\nParticipation is open to all and, as of 2012, free.\n\n\n", "id": "425636", "title": "Christmas Bird Count"}
{"url": "https://en.wikipedia.org/wiki?curid=2580529", "text": "Tucson Bird Count\n\nThe Tucson Bird Count (TBC) is a community-based program that monitors bird populations in and around the Tucson, Arizona, United States metropolitan area. With nearly 1000 sites monitored annually, the Tucson Bird Count is among the largest urban biological monitoring programs in the world.\n\nEach spring, TBC participants collect data on bird abundance and distribution at hundreds of point count locations arrayed across the Tucson basin. The TBC is an example of citizen science, drawing on the combined efforts of hundreds of volunteers. So that data are of suitable quality for scientific analysis and decisionmaking, all TBC volunteers are skilled birdwatchers; many are also professional field guides or biologists. TBC methods are similar to those employed by the North American Breeding Bird Survey, although the TBC uses more closely spaced sites (one site per 1-km square) over a smaller total area (approximately 1000 km). For full details of TBC methods, see the TBC web page at tucsonbirds.org or Turner (2003). The TBC's spatially systematic monitoring is complemented by a TBC park monitoring program that surveys parks, watercourses, or other areas of particular interest multiple times throughout the year.\n\nUses of Tucson Bird Count data include monitoring the status of the Tucson-area bird community over time, finding the areas and land-use practices that are succeeding at sustaining native birds, and investigating the ecology of birds in human-dominated landscapes. Tucson Bird Count results have led to scientific publications, informed Tucson-area planning, and contributed a variety of projects, from locating populations of imperiled species to estimating risk to humans from West Nile Virus. The TBC and several associated research projects are examples of reconciliation ecology, in that they investigate how native species can be sustained in and around the places people live, work, and play. Researchers have also used TBC data to explore the extent to which urban humans are separated from nature (Turner et al. 2004)\n\nRecently, the city of Ottawa, Canada has initiated an urban bird survey that is largely modeled on the Tucson Bird Count. The Ottawa Breeding Bird Count will conduct its inaugural season in 2007. By collaborating among cities, urban surveys like the Tucson Bird Count and the Ottawa Breeding Bird Count, will help researchers discover ways to create habitat for biodiversity in the places where people live and work.\n\nThe Tucson Bird Count began in spring 2001 and is ongoing. As of summer 2005, the TBC had recorded 192,000 individual birds belonging to 212 distinct species. About 115 of these species are known or suspected to breed in the Tucson area; the remainder are migrants (either to higher latitudes, or to higher elevations in nearby mountain ranges such as the Catalinas) or vagrants.\n\nBecause the data are entered directly by participants into the Tucson Bird Count web site, results are publicly available on-line with little delay after observations are made. Among the project's more basic, yet striking results are distribution maps for species in the Tucson area. Many species show strong patterns with respect to development intensity, presence of various habitats, or other factors. For example, the non-native rock pigeon, common in urban areas worldwide, is generally restricted to Tucson's urban core. In contrast, Gambel's quail, a characteristic species of Sonoran Desert upland habitats, shows the inverse pattern, common near Tucson's periphery yet absent from most of the more heavily developed central portion of the city.\n\n\n", "id": "2580529", "title": "Tucson Bird Count"}
{"url": "https://en.wikipedia.org/wiki?curid=5540602", "text": "Australian Bird Count\n\nThe Australian Bird Count (ABC) was a project of the Royal Australasian Ornithologists Union (RAOU). Following the first and successful Atlas of Australian Birds project, which led to the publication of a book on the distribution of Australian birds in 1984, it was suggested by Ken Rogers that the RAOU should next look at bird migration and other movements in Australia. Methodology for a suitable project involving volunteers was worked out through experimental fieldwork and a workshop on ‘Monitoring the Populations and Movements of Australian Birds’.\n\nA project manager, [Stephen Ambrose], was appointed and project fieldwork ran from January 1989 to August 1995. Some 950 volunteer observers carried out 79,000 surveys, for fixed 20-minute periods in 1700 three-hectare locations over Australia.\n\nProject management started at the Australian Museum in Sydney and was later moved to the RAOU National Office in Melbourne. Financial support came at first from the Australian Nature Conservation Agency and subsequently from BP Australia which pledged A$260,000 to the project over five years.\n\nWhile much of the data has yet to be analysed, significant seasonal movements of several species of birds, (demonstrated through geographical shifts in seasonal abundance) have been quantified. A report on some of the findings of the project was published as a supplement to the RAOU's magazine \"Wingspan\" in 1999.\n\n", "id": "5540602", "title": "Australian Bird Count"}
{"url": "https://en.wikipedia.org/wiki?curid=1033516", "text": "Breeding bird survey\n\nA breeding bird survey monitors the status and trends of bird populations. Data from the survey are an important source for the range maps found in field guides. The North American Breeding Bird Survey is a joint project of the United States Geological Survey (USGS) and the Canadian Wildlife Service. The UK Breeding Bird Survey is administered by the British Trust for Ornithology, the Joint Nature Conservation Committee, and the Royal Society for the Protection of Birds.\n\nThe results of the BBS are valuable in evaluating the increasing and decreasing range of bird population which can be a key point to bird conservation. The BBS was designed to provide a continent-wide perspective of population change.\n\nThe North American Breeding Bird Survey was launched in 1966 after the concept of a continental monitoring program for all breeding birds had been developed by Chandler Robbins and his associates from the Migratory Bird Population Station. The program was developed in Laurel, Maryland. In the first year of its existence there were nearly 600 surveys conducted in the east part of the Mississippi River. One year later, in 1967, the survey spread to the Great Plains states and by 1968 almost 2000 routes had been established across southern Canada and 48 American states. As more birders were finding out about this program, the activity of BBS kept on increasing. In the 1980s, Breeding Bird Survey included areas such as Yukon, Northwest Territories of Canada and Alaska. Moreover, the number of routes placed in a number of states has had increased. Nowadays, BBS counts approximately 3700 active routes in the United States and Canada. From all the BBS routes, approximately 2900 are surveyed on a regular basis, each year. The density of the routes varies greatly across the continent and the largest number of routes can be found in New England and Mid-Atlantic states, in which there are more skilled birders to study the behavior of birds. Many bird watchers participate in these surveys as they find the experience rewarding. Currently, the BBS is planned to be expanded to parts of central and western North America as well as northern Mexico.\n\nThe surveys conducted by BBS take place during the peak of the nesting season, June, or May in countries with warmer temperatures. The BBS routes are 24.5 miles long and there are 50 stops at every 0.5 mile along the route. Routes are randomly located in order to sample habitats that are representative of the entire region. \n\nBBS data is quite difficult to analyze given that the survey does not produce a complete counting of the breeding bird populations but more like a relative abundance index. And yet, these surveys have proved to be of great value in studying the bird population trends.\n\nBBS data can also be used to produce continental-scale relative abundance maps. When analyzed at larger scales, the relative abundance maps can offer a clear indication of the relative abundances of bird species that are observed by the BBS. However, maybe the most effective use of these surveys is the opportunity to analyze population change, even though they do not provide information on the factors that cause these changes in the population trends.\n\nThe BTO/JNCC/RSPB Breeding Bird Survey (BBS) is a national project aimed at keeping track of changes in the breeding populations of widespread bird species in the UK. In the UK, there are over 3200 active routes and more than 3000 individuals involved in monitoring the population trends of more than 100 bird species.\n\nThe program started in 1992, and after being tested for two years, it was officially launched. As of 1994 the BBS data has been successfully used by Governments and different non-Governmental organizations for bird conservation purposes.\n\n\n", "id": "1033516", "title": "Breeding bird survey"}
{"url": "https://en.wikipedia.org/wiki?curid=2390787", "text": "Seabird Colony Register\n\nThe Seabird Colony Register (SCR) is a database, managed by the British Joint Nature Conservation Committee, which contains counts of breeding seabirds at British seabird colonies made between 1969 and 1998, which is used for analysing past changes in breeding seabird numbers and changes in their colony size in Britain and Ireland. \n\nData included in the SCR include results of two complete seabird censuses of Britain and Ireland: Operation Seafarer (1969/70) and the Seabird Colony Register Census (1985-87), as well as ad hoc counts and counts from other surveys. Data are held for all 25 species of seabird breeding throughout Britain and Ireland. \n\nThe SCR has been partially superseded by the Seabird 2000 database.\n", "id": "2390787", "title": "Seabird Colony Register"}
{"url": "https://en.wikipedia.org/wiki?curid=45531696", "text": "Great Backyard Bird Count\n\nGreat(Global) Backyard Bird Count (GBBC) is citizen science project in ornithology. It is conducted annually in mid February. This event is supported by Cornell Lab of Ornithology and National Audubon Society. During this four-day event birdwatchers all around the world are invited to count and report details of birds in the area they live. This data is logged at ebird.org server in specific format. The rich data collected during this event is used for scientific research.\n\nGBBC was first launched in 1998 in USA.. It is conducted in the second week of February. In 2013 the event went global and bird watchers around the world can now participate in this event. In 2015 nearly half of known bird species worldwide were reported. Many wildlife and conservation institutes around the world are supporting and participating in this event. Data collected during this event is subjected to scrutiny be experts since this event involves lot of amateurs and enthusiasts who may not be as skilled.\n\nData collected in this event have created awareness and raised alarm about the changes in population and habitats of common birds.\n\nParticipation in this event is increasing rapidly with use of social media like Facebook. There are lot of bird count events organized around the globe by various NGO's and Nature clubs. Many of these institutions are now organizing their events during the GBBC period so the local efforts can contribute to the global database.\n\n", "id": "45531696", "title": "Great Backyard Bird Count"}
{"url": "https://en.wikipedia.org/wiki?curid=51615720", "text": "Great Cocky Count\n\nThe Great Cocky Count, also known as the Great Cockatoo Count, is an annual census designed to provide accurate data about the number and distribution of black cockatoos. It is the largest single survey of black cockatoos in Western Australia.\n\nThe count is a citizen science survey and is conducted at sunset one night in autumn, usually in early April. It was first held in 2009 and has been conducted each year since.\n\nOver 450 volunteers participated in the 2014 count at hundreds of locations between Geraldton and Esperance, Western Australia.\n\nThe 2015 count had over 600 volunteers surveying over 300 sites, with the endangered Carnaby's black cockatoo being the main focus of the count but the vulnerable Baudin's black cockatoo and forest red-tailed black cockatoo also being counted.\n\nIn 2016 a total of 426 roost sites were surveyed by approximately 700 volunteers. The results included:\nIt was estimated that 27% of the black cockatoos that inhabit the south west of Western Australia were counted in a single night.\n\nThe long-term results from the surveys, which have been conducted since 2009, have found that the Carnaby’s black cockatoo population of the Perth-Peel coastal plain declined at a rate of roughly 15 per cent each year. There has been a reduction in flock size and fewer occupied roost sites around Perth, mostly as a result of increased urban sprawl. 46% of the population are found in the Gnangara pine plantation, which is scheduled to be cleared by 2030. The reduction in numbers is mostly a result of clearing breeding grounds and reducing their range. Currently the birds are thought to be using all available habitat, which is barely enough to support the population.\n", "id": "51615720", "title": "Great Cocky Count"}
{"url": "https://en.wikipedia.org/wiki?curid=64045", "text": "Chromosomal crossover\n\nChromosomal crossover (or crossing over) is the exchange of genetic material between homologous chromosomes that results in recombinant chromosomes during sexual reproduction. It is one of the final phases of genetic recombination, which occurs in the \"pachytene\" stage of prophase I of meiosis during a process called synapsis. Synapsis begins before the synaptonemal complex develops and is not completed until near the end of prophase I. Crossover usually occurs when matching regions on matching chromosomes break and then reconnect to the other chromosome.\n\nCrossing over was described, in theory, by Thomas Hunt Morgan. He relied on the discovery of Frans Alfons Janssens who described the phenomenon in 1909 and had called it \"chiasmatypie\". The term \"chiasma\" is linked, if not identical, to chromosomal crossover. Morgan immediately saw the great importance of Janssens' cytological interpretation of chiasmata to the experimental results of his research on the heredity of \"Drosophila\". The physical basis of crossing over was first demonstrated by Harriet Creighton and Barbara McClintock in 1931.\n\nThe linked frequency of crossing over between two gene loci (markers) is the \"crossing-over value\" . For fixed set of genetic and environmental conditions, recombination in a particular region of a linkage structure (chromosome) tends to be constant and the same is then true for the crossing-over value which is used in the production of genetic maps.\n\nThere are two popular and overlapping theories that explain the origins of crossing-over, coming from the different theories on the origin of meiosis. The first theory rests upon the idea that meiosis evolved as another method of DNA repair, and thus crossing-over is a novel way to replace possibly damaged sections of DNA. The second theory comes from the idea that meiosis evolved from bacterial transformation, with the function of propagating \ndiversity.\n\nCrossing over and DNA repair are very similar processes, which utilize many of the same protein complexes. Recombinases and primases lay a foundation of nucleotides along the DNA sequence. One such particular protein complex that is conserved between processes is RAD51, a well conserved recombinase protein that has been shown to be crucial in DNA repair as well as cross over. Several other genes in \"D. melanogaster\" have been linked as well to both processes, by showing that mutants at these specific loci cannot undergo DNA repair or crossing over. Such genes include mei-41, mei-9, hdm, spnA, and brca2. This large group of conserved genes between processes supports the theory of a close evolutionary relationship.\nFurthermore, DNA repair and crossover have been found to favor similar regions on chromosomes. In an experiment using radiation hybrid mapping on wheat’s (\"Triticum aestivum L.\") 3B chromosome, crossing over and DNA repair were found to occur predominantly in the same regions. Furthermore, crossing over has been correlated to occur in response to stressful, and likely DNA damaging, conditions \n\nThe process of bacterial transformation also shares many similarities with chromosomal cross over, particularly in the formation of overhangs on the sides of the broken DNA strand, allowing for the annealing of a new strand. Bacterial transformation itself has been linked to DNA repair many times. The second theory comes from the idea that meiosis evolved from bacterial transformation, with the function of propagating genetic diversity. \n. Thus, this evidence suggests that it is a question of whether cross over is linked to DNA repair or bacterial transformation, as the two do not appear to be mutually exclusive. It is likely that crossing over may have evolved from bacterial transformation, which in turn developed from DNA repair, thus explaining the links between all three processes.\n\nMeiotic recombination may be initiated by double-stranded breaks that are introduced into the DNA by exposure to DNA damaging agents or the Spo11 protein. One or more exonucleases then digest the 5’ ends generated by the double-stranded breaks to produce 3’ single-stranded DNA tails (see diagram). The meiosis-specific recombinase Dmc1 and the general recombinase Rad51 coat the single-stranded DNA to form nucleoprotein filaments. The recombinases catalyze invasion of the opposite chromatid by the single-stranded DNA from one end of the break. Next, the 3’ end of the invading DNA primes DNA synthesis, causing displacement of the complementary strand, which subsequently anneals to the single-stranded DNA generated from the other end of the initial double-stranded break. The structure that results is a \"cross-strand exchange\", also known as a Holliday junction. The contact between two chromatids that will soon undergo crossing-over is known as a \"chiasma\". The Holliday junction is a tetrahedral structure which can be 'pulled' by other recombinases, moving it along the four-stranded structure.\nThe MSH4 and MSH5 proteins form a hetero-oligomeric structure (heterodimer) in yeast and humans. In the yeast \"Saccharomyces cerevisiae\" MSH4 and MSH5 act specifically to facilitate crossovers between homologous chromosomes during meiosis. The MSH4/MSH5 complex binds and stabilizes double Holliday junctions and promotes their resolution into crossover products. An MSH4 hypomorphic (partially functional) mutant of \"S. cerevisiae\" showed a 30% genome wide reduction in crossover numbers, and a large number of meioses with non exchange chromosomes. Nevertheless, this mutant gave rise to spore viability patterns suggesting that segregation of non-exchange chromosomes occurred efficiently. Thus in \"S. cerevisiae\" proper segregation apparently does not entirely depend on crossovers between homologous pairs.\n\nThe grasshopper \"Melanoplus femur-rubrum\" was exposed to an acute dose of X-rays during each individual stage of meiosis, and chiasma frequency was measured. Irradiation during the leptotene-zygotene stages of meiosis (that is, prior to the pachytene period in which crossover recombination occurs) was found to increase subsequent chiasma frequency. Similarly, in the grasshopper \"Chorthippus brunneus\", exposure to X-irradiation during the zygotene-early pachytene stages caused a significant increase in mean cell chiasma frequency. Chiasma frequency was scored at the later diplotene-diakinesis stages of meiosis. These results suggest that X-rays induce DNA damages that are repaired by a crossover pathway leading to chiasma formation.\n\nIn most eukaryotes, a cell carries two versions of each gene, each referred to as an allele. Each parent passes on one allele to each offspring. An individual gamete inherits a complete haploid complement of alleles on chromosomes that are independently selected from each pair of chromatids lined up on the metaphase plate. Without recombination, all alleles for those genes linked together on the same chromosome would be inherited together. Meiotic recombination allows a more independent segregation between the two alleles that occupy the positions of single genes, as recombination shuffles the allele content between homologous chromosomes.\n\nRecombination results in a new arrangement of maternal and paternal alleles on the same chromosome. Although the same genes appear in the same order, some alleles are different. In this way, it is theoretically possible to have any combination of parental alleles in an offspring, and the fact that two alleles appear together in one offspring does not have any influence on the statistical probability that another offspring will have the same combination. This principle of \"independent assortment\" of genes is fundamental to genetic inheritance.\nHowever, the frequency of recombination is actually not the same for all gene combinations. This leads to the notion of \"genetic distance\", which is a measure of recombination frequency averaged over a (suitably large) sample of pedigrees. Loosely speaking, one may say that this is because recombination is greatly influenced by the proximity of one gene to another. If two genes are located close together on a chromosome, the likelihood that a recombination event will separate these two genes is less than if they were farther apart. Genetic linkage describes the tendency of genes to be inherited together as a result of their location on the same chromosome. Linkage disequilibrium describes a situation in which some combinations of genes or genetic markers occur more or less frequently in a population than would be expected from their distances apart. This concept is applied when searching for a gene that may cause a particular disease. This is done by comparing the occurrence of a specific DNA sequence with the appearance of a disease. When a high correlation between the two is found, it is likely that the appropriate gene sequence is really closer.\n\nCrossovers typically occur between homologous regions of matching chromosomes, but similarities in sequence and other factors can result in mismatched alignments. Most DNA is composed of base pair sequences repeated very large numbers of times. These repetitious segments, often referred to as satellites, are fairly homogenous among a species. During DNA replication, each strand of DNA is used as a template for the creation of new strands using a partially-conserved mechanism; proper functioning of this process results in two identical, paired chromosomes, often called sisters. Sister chromatid crossover events are known to occur at a rate of several crossover events per cell per division in eukaryotes. Most of these events involve an exchange of equal amounts of genetic information, but unequal exchanges may occur due to sequence mismatch. These are referred to by a variety of names, including non-homologous crossover, unequal crossover, and unbalanced recombination, and result in an insertion or deletion of genetic information into the chromosome. While rare compared to homologous crossover events, these mutations are drastic, affecting many loci at the same time. They are considered the main driver behind the generation of gene duplications and are a general source of mutation within the genome.\n\nThe specific causes of non-homologous crossover events are unknown, but several influential factors are known to increase the likelihood of an unequal crossover. One common vector leading to unbalanced recombination is the repair of double-strand breaks (DSBs). DSBs are often repaired using non-homologous end joining, a process which involves invasion of a template strand by the DSB strand (see figure below). Nearby homologous regions of the template strand are often used for repair, which can give rise to either insertions or deletions in the genome if a non-homologous but complementary part of the template strand is used. Sequence similarity is a major player in crossover – crossover events are more likely to occur in long regions of close identity on a gene. This means that any section of the genome with long sections of repetitive DNA is prone to crossover events.\n\nThe presence of transposable elements is another influential element of non-homologous crossover. Repetitive regions of code characterize transposable elements; complementary but non-homologous regions are ubiquitous within transposons. Because chromosomal regions composed of transposons have large quantities of identical, repetitious code in a condensed space, it is thought that transposon regions undergoing a crossover event are more prone to erroneous complementary match-up; that is to say, a section of a chromosome containing a lot of identical sequences, should it undergo a crossover event, is less certain to match up with a perfectly homologous section of complementary code and more prone to binding with a section of code on a slightly different part of the chromosome. This results in unbalanced recombination, as genetic information may be either inserted or deleted into the new chromosome, depending on where the recombination occurred.\n\nWhile the motivating factors behind unequal recombination remain obscure, elements of the physical mechanism have been elucidated. Mismatch repair (MMR) proteins, for instance, are a well-known regulatory family of proteins, responsible for regulating mismatched sequences of DNA during replication and escape regulation. The operative goal of MMRs is the restoration of the parental genotype. One class of MMR in particular, MutSβ, is known to initiate the correction of insertion-deletion mismatches of up to 16 nucleotides. Little is known about the excision process in eukaryotes, but \"E. coli\" excisions involve the cleaving of a nick on either the 5’ or 3’ strand, after which DNA helicase and DNA polymerase III bind and generate single-stranded proteins, which are digested by exonucleases and attached to the strand by ligase. Multiple MMR pathways have been implicated in the maintenance of complex organism genome stability, and any of many possible malfunctions in the MMR pathway result in DNA editing and correction errors. Therefore, while it is not certain precisely what mechanisms lead to errors of non-homologous crossover, it is extremely likely that the MMR pathway is involved.\n\n", "id": "64045", "title": "Chromosomal crossover"}
{"url": "https://en.wikipedia.org/wiki?curid=2197855", "text": "Dentinogenesis\n\nDentinogenesis is the formation of dentin, a substance that forms the majority of teeth. Dentinogenesis is performed by odontoblasts, which are a special type of biological cell on the outer wall of dental pulps, and it begins at the late bell stage of a tooth development. The different stages of dentin formation after differentiation of the cell result in different types of dentin: mantle dentin, primary dentin, secondary dentin, and tertiary dentin.\n\nOdontoblasts differentiate from cells of the dental papilla. This is an expression of signaling molecules and growth factors of the inner enamel epithelium (IEE).\n\nThey begin secreting an organic matrix around the area directly adjacent to the IEE, closest to the area of the future cusp of a tooth. The organic matrix contains collagen fibers with large diameters (0.1-0.2 μm in diameter). The odontoblasts begin to move toward the center of the tooth, forming an extension called the odontoblast process. Thus, dentin formation proceeds toward the inside of the tooth. The odontoblast process causes the secretion of hydroxyapatite crystals and mineralization of the matrix. This area of mineralization is known as mantle dentin and is a layer usually about 20-150 μm thick.\n\nWhereas mantle dentin forms from the preexisting ground substance of the dental papilla, primary dentin forms through a different process. Odontoblasts increase in size, eliminating the availability of any extracellular resources to contribute to an organic matrix for mineralization. Additionally, the larger odontoblasts cause collagen to be secreted in smaller amounts, which results in more tightly arranged, heterogeneous nucleation that is used for mineralization. Other materials (such as lipids, phosphoproteins, and phospholipids) are also secreted. There is some dispute about the control of mineralization during dentinogenesis.\n\nThe dentin in the root of a tooth forms only after the presence of Hertwig epithelial root sheath (HERS), near the cervical loop of the enamel organ. Root dentin is considered different from dentin found in the crown of the tooth (known as coronal dentin) because of the different orientation of collagen fibers, as well as the possible decrease of phosphophoryn levels and less mineralization.\n\nMaturation of dentin or mineralization of predentin occurs soon after its apposition, which takes place two phases: primary and secondary. Initially, the calcium hydroxyapatite crystals form as globules, or calcospherules, in the collagen fibers of the predentin, which allows for both the expansion and fusion during the primary mineralization phase. Later, new areas of mineralization occur as globules form in the partially mineralized predentin during the secondary mineralization phase. These new areas of crystal formation are more or less regularly layered on the initial crystals, allowing them to expand, although they fuse incompletely.\n\nIn areas where both primary and secondary mineralization have occurred with complete crystalline fusion, these appear as lighter rounded areas on a stained section of dentin and are considered globular dentin. In contrast, the darker arclike areas in a stained section of dentin are considered interglobular dentin. In these areas, only primary mineralization has occurred within the predentin, and the globules of dentin do not fuse completely. Thus, interglobular dentin is slightly less mineralized than globular dentin. Interglobular dentin is especially evident in coronal dentin, near the DEJ, and in certain dental anomalies, such as in dentin dysplasia.\n\nSecondary dentin is formed after root formation is finished and occurs at a much slower rate. It is not formed at a uniform rate along the tooth, but instead forms faster along sections closer to the crown of a tooth. This development continues throughout life and accounts for the smaller areas of pulp found in older individuals.\n\nTertiary dentin is deposited at specific sites in response to injury by odontoblasts or replacement odontoblasts from the pulp depending on the severity of the injury. Tertiary dentin can be divided into reactionary or reparative dentin. Reactionary dentin is formed by odontoblasts when the injury does not damage the odontoblast layer. Reparative dentin is formed by replacement odontoblasts when the injury is so severe that it damages a part of the primary odontoblast layer. Thus a type of tertiary dentin forms in reaction to stimuli, such as attrition or dental caries.\n\n", "id": "2197855", "title": "Dentinogenesis"}
{"url": "https://en.wikipedia.org/wiki?curid=4014228", "text": "Filamentation\n\nFilamentation is the anomalous growth of certain bacteria, such as \"E. coli\", in which cells continue to elongate but do not divide (no septa formation). The cells that result from elongation without division have multiple chromosomal copies. Bacterial filamentation is often observed as a result of bacteria responding to various stresses, including DNA damage or inhibition of replication. This may happen, for example, while responding to extensive DNA damage through the SOS response system. Nutritional changes may also cause bacterial filamentation. Some of the key genes involved in filamentation in E.coli include \"sulA\" and \"minCD\". The following genes have been connected to virulence using the G. mellonella infection model: BCR1,FLO8, KEM1, SUV3 and TEC1. These genes are required for biofilm development from filamentation. \nFilamentation properties are argued to be necessary in virulence. The biofilm of bacteria is also connected to the organism’s virulence. Filamentation is a survival strategy that protects bacteria from stressors such as host effectors and protist predators. The strategy of filamentation is known to protect bacteria from antibiotic medicines taken by the host.\n\n", "id": "4014228", "title": "Filamentation"}
{"url": "https://en.wikipedia.org/wiki?curid=500995", "text": "Invagination\n\nInvagination is the infolding of one part within another part of a structure, a folding that creates a pocket. The term, originally used in embryology, has been adopted in other disciplines as well. It has many meanings in each term or subject.\n\n\nThe term is used to explain a special kind of metanarrative. It was first used by Maurice Merleau-Ponty () to describe the dynamic self-differentiation of the 'flesh'. It was later used by Rosalind E. Krauss and Jacques Derrida (\"The Law of Genre\", \"Glyph 7\", 1980); for Derrida, an invaginated text is a narrative that folds upon itself, \"endlessly swapping outside for inside and thereby producing a structure \"en abyme\"\". He applies the term to such texts as Immanuel Kant's \"Critique of Judgment\" and Maurice Blanchot's \"La Folie du Jour\". Invagination is an aspect of différance, since according to Derrida it opens the \"inside\" to the \"other\" and denies both inside and outside a stable identity.\n", "id": "500995", "title": "Invagination"}
{"url": "https://en.wikipedia.org/wiki?curid=903117", "text": "Karyorrhexis\n\nKaryorrhexis (from Greek κάρυον \"karyon\", \"kernel, seed or nucleus\", and ῥῆξις \"rhexis\", \"bursting\") is the destructive fragmentation of the nucleus of a dying cell whereby its chromatin is distributed irregularly throughout the cytoplasm. It is usually preceded by pyknosis and can occur as a result of either programmed cell death (apoptosis), senescence, or necrosis.\n\nIn apoptosis, the cleavage of DNA is done by Ca and Mg -dependent endonucleases.\n\n", "id": "903117", "title": "Karyorrhexis"}
{"url": "https://en.wikipedia.org/wiki?curid=903171", "text": "Pyknosis\n\nPyknosis, or karyopyknosis, is the irreversible condensation of chromatin in the nucleus of a cell undergoing necrosis or apoptosis. It is followed by karyorrhexis, or fragmentation of the nucleus.\nPyknosis (from Greek \"pyknono\" meaning \"to thicken up, to close or to condense\") is also observed in the maturation of erythrocytes (a red blood cell) and the neutrophil (a type of white blood cell). The maturing metarubricyte (a stage in RBC maturation) will condense its nucleus before expelling it to become a reticulocyte. The maturing neutrophil will condense its nucleus into several connected lobes that stay in the cell until the end of its cell life.\n\nPyknotic nuclei are often found in the zona reticularis of the adrenal gland. They are also found in the keratinocytes of the outermost layer in parakeratinised epithelium.\n\n", "id": "903171", "title": "Pyknosis"}
{"url": "https://en.wikipedia.org/wiki?curid=4128827", "text": "Dynamin\n\nDynamin is a GTPase responsible for endocytosis in the eukaryotic cell. Dynamins are principally involved in the scission of newly formed vesicles from the membrane of one cellular compartment and their targeting to, and fusion with, another compartment, both at the cell surface (particularly caveolae internalization) as well as at the Golgi apparatus. Dynamin also plays a role in many processes including division of organelles, cytokinesis and microbial pathogen resistance.\n\nDynamin is part of the \"dynamin superfamily,\" which includes classical dynamins, dynamin-like proteins, Mx proteins, OPA, mitofusins, and GBPs. Dynamin itself is a 96 kDa enzyme, and was first isolated when researchers were attempting to isolate new microtubule-based motors from the bovine brain. Dynamin has been extensively studied in the context of clathrin-coated vesicle budding from the cell membrane.\n\nAs a vesicle invaginates, dynamin polymerizes into a helical tube around the neck of the vesicle. The polymer constricts the underlying membrane upon GTP binding and hydrolysis. Constriction around the vesicle neck leads to membrane fission and results in the pinching off of the vesicle from the parent membrane. An example of this process is clathrin-mediated endocytosis.\n\nTo view a ‘cartoon’ image of the non-constricted and constricted state of dynamin spirals, please follow this link: http://dynamin.niddk.nih.gov/figure5.html. The first structure on the left is dynamin in its relaxed state. The structure on the right is dynamin in its constricted state. This shows the extent to which dynamin tightens and changes when GTP is converted to GDP.\n\nThis constriction is in part the result of the twisting activity of dynamin. This twisting required GTP hydrolysis. Dynamin is the only molecular motor known to have a twisting activity. Dynamin is a right-handed helix and has a right-handed twisting activity that explains its tightening and the reduction in the pitch of the helix described above.\n\nIn mammals, three different dynamin genes have been identified:\n\n\nMutations in Dynamin II have been found to cause dominant intermediate Charcot-Marie-Tooth disease.\nEpileptic encephalopathy–causing de novo mutations in dynamin have been suggested to cause dysfunction of vesicle scission during synaptic vesicle endocytosis.\n", "id": "4128827", "title": "Dynamin"}
{"url": "https://en.wikipedia.org/wiki?curid=10095073", "text": "Site-specific recombination\n\nSite-specific recombination, also known as conservative site-specific recombination, is a type of genetic recombination in which DNA strand exchange takes place between segments possessing at least a certain degree of sequence homology. Site-specific recombinases (SSRs) perform rearrangements of DNA segments by recognizing and binding to short DNA sequences (sites), at which they cleave the DNA backbone, exchange the two DNA helices involved and rejoin the DNA strands. While in some site-specific recombination systems just a recombinase enzyme and the recombination sites is enough to perform all these reactions, in other systems a number of accessory proteins and/or accessory sites are also needed. Multiple genome modification strategies, among these recombinase-mediated cassette exchange (RMCE), an advanced approach for the targeted introduction of transcription units into predetermined genomic \"loci\", rely on the capacities of SSRs.\n\nSite-specific recombination systems are highly specific, fast and efficient, even when faced with complex eukaryotic genomes. They are employed in a variety of cellular processes, including bacterial genome replication, differentiation and pathogenesis, and movement of mobile genetic elements (Nash 1996). For the same reasons, they present a potential basis for the development of genetic engineering tools.\n\nRecombination sites are typically between 30 and 200 nucleotides in length and consist of two motifs with a partial inverted-repeat symmetry, to which the recombinase binds, and which flank a central crossover sequence at which the recombination takes place. The pairs of sites between which the recombination occurs are usually identical, but there are exceptions (e.g. attP and attB of λ integrase, see lambda phage).\n\nBased on amino acid sequence homology and mechanistic relatedness most site-specific recombinases are grouped into one of two families: the tyrosine recombinase family or the serine recombinase family. The names stem from the conserved nucleophilic amino acid residue that they use to attack the DNA and which becomes covalently linked to it during strand exchange. Early members of the serine recombinase family were known as resolvase / DNA invertases, while the founding member of the tyrosine recombinases, lambda- integrase, using attP/B recognition sites) differs from the now well known enzymes such as Cre (from the P1 phage) and FLP (from yeast \"S. cerevisiae\") while famous serine recombinases include enzymes such as: gamma-delta resolvase (from the Tn\"1000\" transposon), Tn3 resolvase (from the Tn3 transposon) and \"φ\"C31 integrase (from the \"φ\"C31 phage).\"\n\nAlthough the individual members of the two recombinase families can perform reactions with same practical outcomes, the two families are unrelated to each other, having different protein structures and reaction mechanisms. Unlike tyrosine recombinases, serine recombinases are highly modular as was first hinted by biochemical studies, and later shown by crystallographic structures. Knowledge of these protein structures could prove useful when attempting to reengineer recombinase proteins as tools for genetic manipulation.\n\nRecombination between two DNA sites begins by the recognition and binding of these sites by the recombinase protein. This is followed by synapsis, i.e. bringing the sites together to form the synaptic complex. It is within this synaptic complex that the strand exchange takes place, as the DNA is cleaved and rejoined by controlled transesterification reactions. During strand exchange, the DNA cut at fixed points within the crossover region of the site releases a deoxyribose hydroxyl group, while the recombinase protein forms a transient covalent bond to a DNA backbone phosphate. This phosphodiester bond between the hydroxyl group of the nucleophilic serine or tyrosine residue conserves the energy that was expended in cleaving the DNA. Energy stored in this bond is subsequently used for the rejoining of the DNA to the corresponding deoxyribose hydroxyl group on the other site. The entire process therefore goes through without the need for external energy-rich cofactors such as ATP.\n\nAlthough the basic chemical reaction is the same for both tyrosine and serine recombinases, there are marked differences.\" \nTyrosine recombinases, such as Cre or Flp, cleave one DNA strand at a time at points that are staggered by 6-8bp, linking the 3’ end of the strand to the hydroxyl group of the tyrosine nucleophile (Fig. 1). Strand exchange then proceeds via a crossed strand intermediate analogous to the Holliday junction in which only one pair of strands has been exchanged.\n\nThe mechanism and control of serine recombinases is much less well understood. This group of enzymes was only discovered in the mid-1990s and is still relatively small. The now classical members gamma-delta and Tn3 resolvase, but also new additions like φC31-, Bxb1-, and R4 integrases, cut all four DNA strands simultaneously at points that are staggered by 2bp (Fig. 2). During cleavage, a protein-DNA bond is formed via a transesterification reaction in which a phosphodiester bond is replaced by a phosphoserine bond between a 5’ phosphate at the cleavage site and the hydroxyl group of the conserved serine residue (S10 in resolvase).\n\nIt is still not entirely clear how the strand exchange occurs after the DNA has been cleaved. However, it has been shown that the strands are exchanged while covalently linked to the protein, with a resulting net rotation of 180°. The most quoted (but not the only) model accounting for these facts is the \"subunit rotation model\" (Fig. 2). Independent of the model, DNA duplexes are situated outside of the protein complex, and large movement of the protein is needed to achieve the strand exchange. In this case the recombination sites are slightly asymmetric, which allows the enzyme to tell apart the left and right ends of the site. When generating products, left ends are always joined to the right ends of their partner sites, and vice versa. This causes different recombination hybrid sites to be reconstituted in the recombination products. Joining of left ends to left or right to right is avoided due to the asymmetric “overlap” sequence between the staggered points of top and bottom strand exchange, which is in stark contrast to the mechanism employed by tyrosine recombinases.\n\nThe reaction catalysed, for instance, by Cre-recombinase may lead to excision of the DNA segment flanked by the two sites (Fig. 3A), but may also lead to integration or inversion of the orientation of the flanked DNA segment (Fig. 3B). What the outcome of the reaction will be is dictated mainly by the relative location and the orientation of sites that are to be recombined, but also by the innate specificity of the site-specific system in question. Excisions and inversions occur if the recombination takes place between two sites that are found on the same molecule (intramolecular recombination), and if the sites are in the same (direct repeat) or in an opposite orientation (inverted repeat), respectively. Insertions, on the other hand, take place if the recombination occurs on sites that are situated on two different DNA molecules (intermolecular recombination), provided that at least one of these molecules is circular. Most site-specific systems are highly specialised, catalysing only one of these different types of reaction, and have evolved to ignore the sites that are in the ‘wrong’ orientation.\n\n", "id": "10095073", "title": "Site-specific recombination"}
{"url": "https://en.wikipedia.org/wiki?curid=10270766", "text": "Autophagin\n\nAutophagin-1 (Atg4/Apg4) is a unique cysteine protease responsible for the cleavage of the carboxyl terminus of Atg8/Apg8/Aut7, a reaction essential for its lipidation during autophagy. Human Atg4 homologues cleave the carboxyl termini of the three human Atg8 homologues, microtubule-associated protein light chain 3 (LC3), GABARAP, and GATE-16. \n\nThe rapid advancement in our understanding of the mechanisms and regulation of autophagy has placed this process in the center of current research in major human disorders. The future challenge is to develop easy methods to separately manipulate the activity of each of the autophagic pathways. This would allow researchers to further understand their contribution to disease such as cancer, neurodegeneration, infectious disease, muscular disorders and possibly will provide therapeutic tools.\n\n", "id": "10270766", "title": "Autophagin"}
{"url": "https://en.wikipedia.org/wiki?curid=8574103", "text": "Residual body\n\nIn lysosomal digestion, residual bodies are vesicles containing indigestible materials. Residual bodies are either secreted by the cell via exocytosis (this generally only occurs in macrophages), or they become lipofuscin granules that remain in the cytosol indefinitely. Longer-living cells like neurons and muscle cells usually have a higher concentration of lipofuscin than other more rapidly proliferating cells.\n\n", "id": "8574103", "title": "Residual body"}
{"url": "https://en.wikipedia.org/wiki?curid=8299558", "text": "Septin\n\nSeptins are a group of GTP-binding proteins found primarily in eukaryotic cells of fungi and animals, but also in some green algae. Different septins form protein complexes with each other. These complexes can further assemble into filaments, rings and gauzes. Assembled as such, septins function in cells by localizing other proteins, either by providing a scaffold to which proteins can attach, or by preventing diffusion of molecules from one compartment of the cell to another.\n\nSeptins have been implicated in the localization of cellular processes at the site of cell division, at the plasma membrane, at sites where specialized structures like cilia or flagella are attached to the cell body. In yeast cells, they compartmentalize parts of the cell and build scaffolding to provide structural support during cell division at the septum, from which they derive their name. Recent research in human cells suggests that septins build cages around bacterial pathogens, immobilizing the harmful microbes and preventing them from invading other cells.\n\nAs filament forming proteins, septins can be considered part of the cytoskeleton. Apart from forming non-polar filaments, septins associate with cell membranes, actin filaments and microtubules. Although present in most eukaryotes, septins have not been observed in green plants, but they have been reported in red algae.\n\nSeptins are P-Loop-NTPase proteins that range in weight from 30-65 kDa. Septins are highly conserved between different eukaryotic species. They are composed of a variable-length proline rich N-terminus with a basic phosphoinositide binding motif important for membrane association, a GTP-binding domain, a highly conserved Septin Unique Element domain, and a C-terminal extension including a coiled coil domain of varying length.\n\nSeptins interact either via their respective GTP-binding domains, or via both their N- and C-termini. Different organisms express a different number of septins, and from those symmetric oligomeres are formed. For example, in humans Sept7-Sept6-Sept2-Sept2-Sept6-Sept7 form one complex, and in yeast Cdc11-Cdc12-Cdc3-Cdc10-Cdc10-Cdc3-Cdc12-Cdc11 form another one. These complexes then associate to form non-polar filaments, filament bundles, cages or ring structures in cells.\n\nSeptins are found in fungi, animals, some green algae, and some red algae, but not in higher plants.\n\nThere are seven different septins in Saccharomyces cerevisiae. Five of those are involved in mitosis, while two (Spr3 and Spr28) are specific to sporulation. Mitotic septins (Cdc3, Cdc10, Cdc11, Cdc12, Shs1) form a ring structure at the bud neck during cell division. They are involved in the selection of the bud-site, the positioning of the mitotic spindle, polarized growth, and cytokinesis. The sporulating septins (Spr3, Spr28) localize together with Cdc3 and Cdc11 to the edges of prospore membranes.\n\nThe septin cortex undergoes several changes throughout the cell cycle: The first visible septin structure is a distinct ring which appears ~15 min before bud emergence. After bud emergence, the ring broadens to assume the shape of an hourglass around the mother-bud neck. During cytokinesis, the septin cortex splits into a double ring which eventually disappears. How can the septin cortex undergo such dramatic changes, although some of its functions may require it to be a stable structure? FRAP analysis has revealed that the turnover of septins at the neck undergoes multiple changes during the cell cycle. The predominant, functional conformation is characterized by a low turnover rate (frozen state), during which the septins are phosphorylated. Structural changes require a destabilization of the septin cortex (fluid state) induced by dephosphorylation prior to bud emergence, ring splitting and cell separation.\n\nThe composition of the septin cortex does not only vary throughout the cell cycle but also along the mother-bud axis. This polarity of the septin network allows concentration of some proteins primarily to the mother side of the neck, some to the center and others to the bud site.\n\nThe septins act as a scaffold, recruiting many proteins. These protein complexes are involved in cytokinesis, chitin deposition, cell polarity, spore formation, in the morphogenesis checkpoint, spindle alignment checkpoint and bud site selection.\n\nBudding yeast cytokinesis is driven through two septin dependent, redundant processes: recruitment and contraction of the actomyosin ring and formation of the septum by vesicle fusion with the plasma membrane. In contrast to septin mutants, disruption of one single pathway only leads to a delay in cytokinesis, not complete failure of cell division. Hence, the septins are predicted to act at the most upstream level of cytokinesis.\n\nAfter the isotropic-apical switch in budding yeast, cortical components, supposedly of the exocyst and polarisome, are delocalized from the apical pole to the entire plasma membrane of the bud, but not the mother cell. The septin ring at the neck serves as a cortical barrier that prevents membrane diffusion of these factors between the two compartments. This asymmetric distribution is abolished in septin mutants.\n\nSome conditional septin mutants do not form buds at their normal axial location. Moreover, the typical localization of some bud-site-selection factors in a double ring at the neck is lost or disturbed in these mutants. This indicates that the septins may serve as anchoring site for such factors in axially budding cells.\n\nSince their discovery in \"S. cerevisiae,\" septin homologues have been found in other eukaryotic species, including filamentous fungi. Septins in filamentous fungi display a variety of different shapes within single cells, where they control aspects of filamentous morphology.\n\nThe genome of \"C. albicans\" encodes homologues to all \"S. cerevisiae\" septins. Without Cdc3 and Cdc12 genes \"Candida albicans\" cannot proliferate, other septins affect morphology and chitin deposition, but are not essential. \"Candida albicans\" can display different morphologies of vegetative growth, which determines the appearance of septin structures. Newly forming hyphae form a septin ring at the base, Double rings form at sites of hyphal septation, and a septin cap forms at hyphal tips. Elongated septin-filaments encircle the spherical chlamydospores. Double rings of septins at the septation site also bear growth polarity, with the growing tip ring disassembling, while the basal ring remaining intact.\n\nFive septins are found in \"A. nidulans\" (AnAspAp, AnAspBp, AnAspCp, AnAspDp, AnAspEp). AnAspBp forms single rings at septation sites that eventually split into double rings. Additionally, AnAspBp forms a ring at sites of branch emergence which broadens into a band as the branch grows. Like in \"C. albicans,\" double rings reflect polarity of the hypha. In the case of \"Aspergillus nidulans\" polarity is conveyed by disassembly of the more basal ring (the ring further away from the hyphal growth tip), leaving the apical ring intact, potentially as a growth guidance cue.\n\nThe \"ascomycete A. gossypii\" possesses homologues to all \"S. cerevisiae\" septins, with one being duplicated (\"AgCDC3, AgCDC10, AgCDC11A, AgCDC11B, AgCDC12, AgSEP7\"). \"In vivo\" studies of AgSep7p-GFP have revealed that septins assemble into discontinuous hyphal rings close to growing tips and sites of branch formation, and into asymmetric structures at the base of branching points. Rings are made of filaments which are long and diffuse close to growing tips and short and compact further away from the tip. During septum formation, the septin ring splits into two to form a double ring. \"Agcdc3Δ, Agcdc10Δ \"and \"Agcdc12Δ \"deletion mutants display aberrant morphology and are defective for actin-ring formation, chitin-ring formation, and sporulation. Due to the lack of septa, septin deletion mutants are highly sensitive, and damage of a single hypha can result in complete lysis of a young mycelium.\n\nIn contrast to septins in yeast, and in contrast to other cytoskeletal components of metazoa, septins do not form a continuous network in metazoan cells, but several dispersed ones in the cortical cytoplasm. These are integrated with actin bundles and microtubules. For example, the actin bundling protein anillin is required for correct spatial control of septin organization. In the sperm cells of mammals, septins form a stable ring called annulus in the tail. In mice (and potentially in humans, too), defective annulus formation leads to male infertility.\n\nIn humans, septins are involved in cytokinesis, cilium formation and neurogenesis through the capability to recruit other proteins or serve as a diffusion barrier. There are 13 different human genes coding for septins. The septin proteins produced by these genes are grouped into four subfamilies each named after its founding member: (i) SEPT2 (SEPT1, SEPT4, SEPT5), (ii) SEPT3 (SEPT9, SEPT12), (iii) SEPT6 (SEPT8, SEPT10, SEPT11, SEPT14), and (iv) SEPT7. Septin protein complexes are assembled to form either hetero-hexamers (incorporating monomers selected from three different groups and the monomer from each group is present in two copies; 3 x 2 = 6) or hetero-octamers (monomers from four different groups, each monomer present in two copies; 4 x 2 = 8). These hetero-oligomers in turn form higher-order structures such as filaments and rings.\n\nSeptins form cage-like structures around bacterial pathogens, immobilizing harmful microbes and preventing them from invading healthy cells. This cellular defence system could potentially be exploited to create therapies for dysentery and other illnesses. For example, \"Shigella\" is a bacterium that causes lethal diarrhoea in humans. To propagate from cell to cell, \"Shigella\" bacteria develop actin-polymer 'tails', which propel the microbes and allow them to gain entry into neighbouring host cells. As part of the immune response, human cells produce a cell-signalling protein called TNF-α which trigger thick bundles of septin filaments to encircle the microbes within the infected host cell. Microbes that become trapped in these septin cages are broken down by autophagy. Disruptions in septins and mutations in the genes that code for them could be involved in causing leukaemia, colon cancer and neurodegenerative conditions such as Parkinson's disease and Alzheimer's disease. Potential therapies for these, as well as for bacterial conditions such as dysentery caused by \"Shigella\", might bolster the body’s immune system with drugs that mimic the behaviour of TNF-α and allow the septin cages to proliferate.\n\nIn the nematode worm \"Caenorhabditis elegans\" there are two genes coding for septins, and septin complexes contain the two different septins in a tetrameric UNC59-UNC61-UNC61-UNC59 complex. Septins in \"C.elegans\" concentrate at the cleavage furrow and the spindle midbody during cell division. Septins are also involved in cell migration and axon guidance in \"C.elegans\".\n\nThe septin localized in the mitochondria is called mitochondrial septin (M-septin). It was identified as a CRMP/CRAM-interacting protein in developing mouse brain.\n\nThe septins were discovered in 1970 by Leland H. Hartwell and colleagues in a screen for temperature-sensitive mutants affecting cell division (cdc mutants) in yeast (\"Saccharomyces cerevisiae\"). The screen revealed four mutants which prevented cytokinesis at restrictive temperature. The corresponding genes represent the four original septins, \"ScCDC3, ScCDC10, ScCDC11,\" and \"ScCDC12\". Despite disrupted cytokinesis, the cells continued budding, DNA synthesis, and nuclear division, which resulted in large multinucleate cells with multiple, elongated buds. In 1976, analysis of electron micrographs revealed ~20 evenly spaced striations of 10-nm filaments around the mother-bud neck in wild-type but not in septin-mutant cells. Immunofluorescence studies revealed that the septin proteins colocalize into a septin ring at the neck. The localization of all four septins is disrupted in conditional \"Sccdc3\" and \"Sccdc12\" mutants, indicating interdependence of the septin proteins. Strong support for this finding was provided by biochemical studies: The four original septins co-purified on affinity columns, together with a fifth septin protein, encoded by \"ScSEP7\" or \"ScSHS1\". Purified septins from budding yeast, Drosophila, Xenopus, and mammalian cells are able to self associate \"in vitro\" to form filaments. How the septins interact \"in vitro\" to form heteropentamers that assemble into filaments was studied in detail in \"S. cerevisiae\".\n\nMicrographs of purified filaments raised the possibility that the septins are organized in parallel to the mother-bud axis. The 10-nm striations seen on electron micrographs may be the result of lateral interaction between the filaments. Mutant strains lacking factors important for septin organization support this view. Instead of continuous rings, the septins form bars oriented along the mother-bud axis in deletion mutants of \"ScGIN4, ScNAP1\" and \"ScCLA4\".\n\n", "id": "8299558", "title": "Septin"}
{"url": "https://en.wikipedia.org/wiki?curid=13138964", "text": "Formins\n\nFormins (formin homology proteins) are a group of proteins that are involved in the polymerization of actin and associate with the fast-growing end (barbed end) of actin filaments. Most formins are Rho-GTPase effector proteins. Formins regulate the actin and microtubule cytoskeleton \n\nFormins have been found in all eukaryotes studied. In humans, 15 different formin proteins are present that have been classified in 7 subgroups. By contrast, yeasts contain only 2-3 formins.\n\nFormins are characterised by the presence of three formin homology (FH) domains (FH1, FH2 and FH3), although members of the formin family do not necessarily contain all three domains. In addition, other domains are usually present, such as PDZ, DAD, WH2, or FHA domains.\n\nThe proline-rich FH1 domain mediates interactions with a variety of proteins, including the actin-binding protein profilin, SH3 (Src homology 3) domain proteins, and WW domain proteins. The actin nucleation-promoting activity of \"S. cerevisiae\" formins has been localized to the FH2 domain. The FH2 domain is required for the self-association of formin proteins through the ability of FH2 domains to directly bind each other, and may also act to inhibit actin polymerisation. The FH3 domain is less well conserved and is required for directing formins to the correct intracellular location, such the mitotic spindle, or the projection tip during conjugation. In addition, some formins can contain a GTPase-binding domain (GBD) required for binding to Rho small GTPases, and a C-terminal conserved DRF autoregulatory domain (Dia-autoregulatory domain) (DAD). The GBD domain is a bifunctional autoinhibitory domain that interacts with and is regulated by activated Rho family members. Mammalian Drf3 contains a CRIB-like motif within its GBD for binding to Cdc42, which is required for Cdc42 to activate and guide Drf3 towards the cell cortex where it remodels the actin skeleton. The DRF autoregulatory domain binds the N-terminal GTPase-binding domain; this link is broken when GTP-bound Rho binds to the GBD and activates the protein. The addition of the DAD to mammalian cells induces actin filament formation, stabilises microtubules, and activates serum-response mediated transcription. Another commonly found domain is an armadillo repeat region (ARR) located in the FH3 domain.\n\nThe FH2 domain, has been shown by X-ray crystallography to have an elongated, crescent shape containing three helical subdomains.\n\nFormins also directly bind to microtubules via their FH2 domain. This interaction is important in promoting the capture and stabilization of a subset of microtubules oriented towards the leading edge of migrating cells. Formins also promote the capture of microtubules by the kinetochore during mitosis and for aligning microtubules along actin filaments.\n\n", "id": "13138964", "title": "Formins"}
{"url": "https://en.wikipedia.org/wiki?curid=14252918", "text": "Afterhyperpolarization\n\nAfterhyperpolarization, or AHP, describes the hyperpolarizing phase of a neuron's action potential where the cell's membrane potential falls below the normal resting potential. This is also commonly referred to as an action potential's undershoot phase. AHPs have been segregated into \"fast\", \"medium\", and \"slow\" components that appear to have distinct ionic mechanisms and durations. While fast and medium AHPs can be generated by single action potentials, slow AHPs generally develop only during trains of multiple action potentials.\n\nDuring single action potentials, transient depolarization of the membrane opens more voltage-gated K channels than are open in the resting state, many of which do not close immediately when the membrane returns to its normal resting voltage. This can lead to an \"undershoot\" of the membrane potential to values that are more polarized (\"hyperpolarized\") than was the original resting membrane potential. Ca-activated K channels that open in response to the influx of Ca during the action potential carry much of the K current as the membrane potential becomes more negative. The K permeability of the membrane is transiently unusually high, driving the membrane voltage \"V\" even closer to the K equilibrium voltage \"E\". Hence, hyperpolarization persists until the membrane K permeability returns to its usual value.\n\nMedium and slow AHP currents also occur in neurons. The ionic mechanisms underlying medium and slow AHPs are not yet well understood, but may also involve M current and HCN channels for medium AHPs, and ion-dependent currents and/or ionic pumps for slow AHPs.\n", "id": "14252918", "title": "Afterhyperpolarization"}
{"url": "https://en.wikipedia.org/wiki?curid=2741772", "text": "Karyolysis\n\nKaryolysis (from Greek κάρυον \"karyon\", \"kernel, seed or nucleus\", and λύσις \"lysis\" from λύειν \"lyein\", \"to separate\") is the complete dissolution of the chromatin of a dying cell due to the enzymatic degradation by endonucleases. The whole cell will eventually stain uniformly with eosin after karyolysis. It is usually associated with karyorrhexis and occurs mainly as a result of necrosis, while in apoptosis after karyorrhexis the nucleus usually dissolves into \"apoptotic bodies\".\n\nDisintegration of the cytoplasm, pyknosis of the nuclei, and karyolysis of the nuclei of scattered transitional cells may be seen in urine from healthy individuals as well as in urine containing malignant cells. Cells with an attached tag of partially preserved cytoplasm were initially described by Papanicolaou and are sometimes called comet or decoy cells. They may have some of the characteristics of malignancy, and it is therefore important that they be recognized for what they are.\n\n", "id": "2741772", "title": "Karyolysis"}
{"url": "https://en.wikipedia.org/wiki?curid=14942693", "text": "RAP6\n\nRAP6 is the abbreviation for Rab5-activating protein 6, a novel endosomal protein with a role in endocytosis. RAP6 was discovered by Alejandro Barbieri and his group of researchers (Christine Hunker, Adriana Galvis, Ivan Kruk, Hugo Giambini, Lina Torres and Maria Luisa Veisaga) working at Florida International University.\n\nThis novel human protein has been reported to be involved in membrane trafficking. It has been shown that RAP6 has a guanine nucleotide exchange factor (GEF) activity specific to Rab5 and a GTPase activating protein (GAP) activity specific to RAS.\n\nThe original GeneBank Identifications (GIs) have been published in the NCBI Nucleotide databases with GIs 77176718 and 77176720. Since then, many names have been coined to the validated protein such as RabGEF1, GeneID: 27342. RAP6 belongs to the family of the GAPVD1, GeneID: 26130\n", "id": "14942693", "title": "RAP6"}
{"url": "https://en.wikipedia.org/wiki?curid=20733528", "text": "Endoexocytosis\n\nEndoexocytosis is a cellular process. It is a unique internalization process in which the two opposing gap junction plaque membranes are internalized into one of two contacting cells, to form pentilaminar-annular gap junction vesicles. In non-gap junctional membrane internalization, endocytosis can occur independent of the conditions in adjacent cells. However in the case of gap junction plaque internalization the membrane of the adjacent cell is internalized into its contacting cell pair, and is thus an “endoexocytic” process. It has been previously suggested that gap junction plaque internalization is mediated by clathrin and its associated proteins. This hypothesis was supported initially, by the presence of the bristle-like coat surrounding annular gap junction vesicles and later by the demonstration of the light microscopic immunocytochemical and transmission electron microscopic quantum dot colocalization of clathrin and Cx43-GFP structures. The reports of a decreased number of annular gap junctions following treatments that disrupt clathrin activity provided additional support for a role of clathrin in gap junction plaque internalization and annular gap junction formation.\n", "id": "20733528", "title": "Endoexocytosis"}
{"url": "https://en.wikipedia.org/wiki?curid=2538686", "text": "Autolysis (biology)\n\nIn biology, autolysis, more commonly known as self-digestion, refers to the destruction of a cell through the action of its own enzymes. It may also refer to the digestion of an enzyme by another molecule of the same enzyme.\n\nThe term derives from the Greek words αὐτο- (\"self\") and λύσις (\"splitting\").\n\nAutolytic cell destruction is uncommon in living adult organisms and usually occurs in injured cells and dying tissue. Autolysis is initiated by the cells' lysosomes releasing digestive enzymes into the cytoplasm. These enzymes are released due to the cessation of active processes in the cell, not as an active process. In other words, though autolysis resembles the active process of digestion of nutrients by live cells, the dead cells are not actively digesting themselves as is often claimed and as the synonym \"self-digestion\" of autolysis seems to imply. Autolysis of individual cell organelles can be lessened if the organelle is stored in ice-cold isotonic buffer after cell fractionation.\n\nIn the healing of wounds, autolytic debridement can be a helpful process, where the body breaks down and liquifies dead tissue so that it can be washed or carried away. Modern wound dressings that help keep the wound moist can assist in this process.\n\nIn the food industry, autolysis involves killing yeast and encouraging breakdown of its cells by various enzymes. The resulting autolyzed yeast is used as a flavoring or flavor enhancer. For yeast extract, when this process is triggered by the addition of salt, it is known as plasmolysis.\n\nIn bread baking, the term (or, more commonly, its French cognate \"autolyse\") is described as a period of rest following initial mixing of flour and water, before other ingredients (such as salt and yeast) are added to the dough. The term was coined by French baking professor Raymond Calvel, who recommended the procedure as a means of reducing kneading time, thereby improving the flavor and color of bread. Long kneading times subject bread dough to atmospheric oxygen, which bleaches the naturally occurring carotenoids in bread flour, robbing the flour of its natural creamy color and flavor. An autolyse also makes the dough easier to shape and improves structure.\n\nIn the making of fermented beverages, autolysis can occur when the must or wort is left on the lees for a long time.\nIn beer brewing, autolysis causes undesired off-flavors. Autolysis in winemaking is often undesirable, but in the case of the best Champagnes it is a vital component in creating flavor and mouth feel.\n\n", "id": "2538686", "title": "Autolysis (biology)"}
{"url": "https://en.wikipedia.org/wiki?curid=21588604", "text": "Potocytosis\n\nPotocytosis is a type of receptor-mediated endocytosis in which small molecules are transported across the plasma membrane of a cell. The molecules are transported by caveolae (rather than clathrin-coated vesicles) and are deposited directly into the cytosol.\n\nLike other types of receptor-mediated endocytosis, potocytosis typically begins when an extracellular ligand binds to a receptor protein on the surface of a cell, thus beginning the formation of an endocytotic vesicle. The ligand is usually of low molecular mass (e.g. vitamins), but some larger molecules (such as lipids) can also act as ligands.\n", "id": "21588604", "title": "Potocytosis"}
{"url": "https://en.wikipedia.org/wiki?curid=22328498", "text": "Efferocytosis\n\nIn cell biology, efferocytosis (from \"efferre\", Latin for 'to take to the grave', 'to bury') is the process by which dying/dead cells (e.g. apoptotic or necrotic) are removed by phagocytic cells. It can be regarded as the 'burying of dead cells'.\n\nDuring efferocytosis, the cell membrane of phagocytic cells engulfs the apoptotic cell, forming a large fluid-filled vesicle containing the dead cell. This ingested vesicle is called an efferosome (in analogy to the term phagosome). This process is similar to macropinocytosis.\n\nFor apoptosis, the effect of efferocytosis is that dead cells are removed before their membrane integrity is breached and their contents leak into the surrounding tissue. This prevents exposure of tissue to toxic enzymes, oxidants and other intracellular components such as proteases and caspases.\n\nEfferocytosis can be performed not only by 'professional' phagocytic cells such as macrophages or dendritic cells, but also by many other cell types including epithelial cells and fibroblasts. To distinguish them from living cells, apoptotic cells carry specific 'eat me' signals, such as the presence of phosphatidyl serine (resulting from phospholipid flip-flop) or calreticulin on the outer leaflet of the cell membrane.\n\nEfferocytosis triggers specific downstream intracellular signal transduction pathways, for example resulting in anti-inflammatory, anti-protease and growth-promoting effects. Conversely, impaired efferocytosis has been linked to autoimmune disease and tissue damage. Efferocytosis results in production by the ingesting cell of mediators such as hepatocyte- and vascular endothelial growth factor, which are thought to promote replacement of the dead cells.\n\nDefective efferocytosis has been demonstrated in such diseases as cystic fibrosis and bronchiectasis, COPD, asthma and idiopathic pulmonary fibrosis, rheumatoid arthritis, systemic lupus erythematosus, glomerulonephritis and atherosclerosis.\n\nSpecialized pro-resolving mediators are cell-derived metabolites of certain polyunsaturated fatty acids viz.: arachidonic acid which is metabolized to the lipoxins; eicosapentaenoic acid which is metabolized to the Resolvin E's; docosahexaenoic acid which is metablized to the Resolvin D's, Maresins, and Neuroprotectins; and n-3 docosapentaenoic acid which is metabolized to the n-3 docosapentaenoic acid-derived resolvins and n-3 docosapentaenoic acid-derived neuroprotectins (See Specialized pro-resolving mediators). These mediators possess a broad range of overlapping activities which act to resolve inflammation; one of the important activities which many of these mediators possess is the stimulation of efferocytosis in inflamed tissues. Failure to form sufficient amounts of these mediators is proposed to be one cause of chronic and pathological inflammatory responses (see Specialized pro-resolving mediators#SPM and inflammation).\n", "id": "22328498", "title": "Efferocytosis"}
{"url": "https://en.wikipedia.org/wiki?curid=4440593", "text": "Histone methylation\n\nHistone methylation is a process by which methyl groups are transferred to amino acids of histone proteins that make up nucleosomes, which the DNA double helix wraps around to form chromosomes. Methylation of histones can either increase or decrease transcription of genes, depending on which amino acids in the histones are methylated, and how many methyl groups are attached. Methylation events that weaken chemical attractions between histone tails and DNA increase transcription, because they enable the DNA to uncoil from nucleosomes so that transcription factor proteins and RNA polymerase can access the DNA. This process is critical for the regulation of gene expression that allows different cells to express different genes.\n\nHistone methylation, as a mechanism for modifying chromatin structure is associated with stimulation of neural pathways known to be important for formation of long-term memories and learning. Animal models have shown methylation and other epigenetic regulation mechanisms to be associated with conditions of aging, neurodegenerative diseases, and intellectual disability (Rubinstein–Taybi Syndrome, X-linked mental retardation).\nThis modification alters the properties of the nucleosome and affects its interactions with other proteins, particularly in regards to gene transcription processes. \n\nThe fundamental unit of chromatin, called a nucleosome, contains DNA wound around a protein octamer. This octamer consists of two copies each of four histone proteins: H2A, H2B, H3, and H4. Each one of these proteins has a tail extension, and these tails are the targets of nucleosome modification by methylation. DNA activation or inactivation is largely dependent on the specific tail residue methylated and its degree of methylation. Histones can be methylated on lysine (K) and arginine (R) residues only, but methylation is most commonly observed on lysine residues of histone tails H3 and H4. The tail end furthest from the nucleosome core is the N-terminal (residues are numbered starting at this end). Common sites of methylation associated with gene activation include H3K4, H3K48, and H3K79. Common sites for gene inactivation include H3K9 and H3K27. Studies of these sites have found that that methylation of histone tails at different residues serve as markers for the recruitment of various proteins or protein complexes that serve to regulate chromatin activation or inactivation.\n\nLysine and arginine residues both contain amino groups, which confer basic and hydrophobic characteristics. Lysine is able to be mono-, di-, or trimethylated with a methyl group replacing each hydrogen of its NH3+ group. With a free NH2 and NH2+ group, arginine is able to be mono- or dimethylated. This dimethylation can occur symmetrically on the NH2 group or asymmetrically with one methylation on each group. Each addition of a methyl group on each residue requires a specific set of protein enzymes with various substrates and cofactors. Generally, methylation of an arginine residue requires a complex including protein arginine methyltransferase (PRMT) while lysine requires a specific histone methyltransferase (HMT), usually containing an evolutionarily conserved SET domain.\n\nDifferent degrees of residue methylation can confer different functions, as exemplified in the methylation of the commonly studied H4K20 residue. Monomethylated H4K20 (H4K20me1) is involved in the compaction of chromatin and therefore transcriptional repression. However, H4K20me2 is vital in the repair of damaged DNA. When dimethylated, the residue provides a platform for the binding of protein 53BP1 involved in the repair of double-stranded DNA breaks. H4K20me3 is observed to be concentrated in heterochromatin and reductions in this trimethylation are observed in cancer progression. Therefore, H4K20me3 serves an additional role in chromatin repression.\n\nThe genome is tightly condensed into chromatin, which needs to be loosened for transcription to occur. In order to halt the transcription of a gene the DNA must be wound tighter. This can be done by modifying histones at certain sites by methylation. Histone methyltransferases are enzymes which transfer methyl groups from S-Adenosyl methionine onto the lysine or arginine residues of the H3 and H4 histones. There are instances of the core globular domains of histones being methylated as well.\n\nThe histone methyltransferases are specific to either lysine or arginine. The lysine-specific transferases are further broken down into whether or not they have a SET domain or a non-SET domain. These domains specify exactly how the enzyme catalyzes the transfer of the methyl from SAM to the transfer protein and further to the histone residue. The methyltransferases can add 1-3 methyls on the target residues.\n\nThese methyls that are added to the histones act to regulate transcription by blocking or encouraging DNA access to transcription factors. In this way the integrity of the genome and epigenetic inheritance of genes are under the control of the actions of histone methyltransferases. Histone methylation is key in distinguishing the integrity of the genome and the genes that are expressed by cells, thus giving the cells their identities.\n\nMethylated histones can either repress or activate transcription. For example, while H3K4me2/3 and H3K79me3 are generally associated with transcriptional activity, methylation of H3K9me2/3, H3K27me2/3, and H4K20me3 are associated with repression.\n\nModifications made on the histone have an effect on the genes that are expressed in a cell and this is the case when methyls are added to the histone residues by the histone methyltransferases. Histone methylation plays an important role on the assembly of the heterochromatin mechanism and the maintenance of gene boundaries between genes that are transcribed and those that aren’t. These changes are passed down to progeny and can be affected by the environment that the cells are subject to. Epigenetic alterations are reversible meaning that they can be targets for therapy.\n\nThe activities of histone methyltransferases are offset by the activity of histone demethylases. This allows for the switching on or off of transcription by reversing pre-existing modifications. It is necessary for the activities of both histone methyltrasnsferases and histone demethylases to be regulated tightly. Misregulation of either can lead to gene expression that leads to increased susceptibility to disease. Many cancers arise from the inappropriate epigenetic effects of misregulated methylation. However, because these processes are at times reversible, there is interest in utilizing their activities in concert with anti-cancer therapies.\n\nIn female organisms, a sperm containing an X chromosome fertilizes the egg, giving the embryo two copies of the X chromosome. Females, however, do not initially require both copies of the X chromosome as it would only double the amount of protein products transcribed as shown by the hypothesis of dosage compensation. The paternal X chromosome is quickly inactivated during the first few divisions. This inactive X chromosome (Xi) is packed into an incredibly tight form of chromatin called heterochromatin. This packing occurs due to the methylation of the different lysine residues that help form different histones. In humans X inactivation is a random process, that is mediated by the non-coding RNA XIST.\n\nAlthough methylation of lysine residues occurs on many different histones, the most characteristic of Xi occurs on the ninth lysine of the third histone (H3K9). While a single methylation of this region allows for the genes bound to remain transcriptionally active, in heterochromatin this lysine residue is often methylated twice or three times, H3K9me2 or H3K9me3 respectively, to ensure that the DNA bound is inactive. More recent research has shown that H3K27me3 and H4K20me1 are also common in early embryos. Other methylation markings associated with transcriptionally active areas of DNA, H3K4me2 and H3K4me3, are missing from the Xi chromosome along with many acetylation markings. Although it was known that certain Xi histone methylation markings stayed relatively constant between species, it has recently been discovered that different organisms and even different cells within a single organism can have different markings for their X inactivation. Through histone methylation, there is genetic imprinting, so that the same X homolog stays inactivated through chromosome replications and cell divisions.\n\nDue to the fact that histone methylation regulates much of what genes become transcribed, even slight changes to the methylation patterns can have dire effects on the organism. Mutations that occur to increase and decrease methylation have great changes on gene regulation, while mutations to enzymes such as methyltransferase and demethyltransferase can completely alter which proteins are transcribed in a given cell. Over methylation of a chromosome can cause certain genes that are necessary for normal cell function, to become inactivated. In a certain yeast strain, Saccharomyces cerevisiae, a mutation that causes three lysine residues on the third histone, H3K4, H3K36, and H3K79, to become methylated causes a delay in the mitotic cell cycle, as many genes required for this progression are inactivated. This extreme mutation leads to the death of the organism. It has been discovered that the deletion of genes that will eventually allow for the production of histone methyltransferase allows this organism to live as its lysine residues are not methylated.\n\nIn recent years it has come to the attention of researchers that many types of cancer are caused largely due to epigenetic factors. Cancer can be caused in a variety of ways due to differential methylation of histones. Since the discovery of oncogenes as well as tumor suppressor genes it has been known that a large factor of causing and repressing cancer is within our own genome. If areas around oncogenes become unmethylated these cancer-causing genes have the potential to be transcribed at an alarming rate. Opposite of this is the methylation of tumor suppressor genes. In cases where the areas around these genes were highly methylated, the tumor suppressor gene was not active and therefore cancer was more likely to occur. These changes in methylation pattern are often due to mutations in methyltransferase and demethyltransferase. Other types of mutations in proteins such as isocitrate dehydrogenase 1 (IDH1) and isocitrate dehydrogenase 2 (IDH2) can cause the inactivation of histone demethyltransferase which in turn can lead to a variety of cancers, gliomas and leukemias, depending on in which cells the mutation occurs.\n\n\n", "id": "4440593", "title": "Histone methylation"}
{"url": "https://en.wikipedia.org/wiki?curid=24039902", "text": "Ectopic recombination\n\nEctopic recombination is an aberrant form of recombination in which crossing over occurs at non-homologous, rather than along homologous, loci. Such recombination often results in dramatic chromosomal rearrangement, which is generally deleterious. Some research, however, has suggested that ectopic recombination can result in mutated chromosomes that benefit the organism. Ectopic recombination can occur during both meiosis and mitosis, although it is more likely to occur during meiosis. It occurs relatively frequently—in at least one yeast species (Saccharomyces cerevisiae) the frequency of ectopic recombination is roughly on par with that of allelic (or traditional) recombination. If the alleles at two loci are heterozygous, then ectopic recombination is relatively likely to occur, whereas if the alleles are homozygous, they will almost certainly undergo allelic recombination. Ectopic recombination does not require loci involved to be close to one another; it can occur between loci that are widely separated on a single chromosome, and has even been known to occur across chromosomes. Neither does it require high levels of homology between sequences—the lower limit required for it to occur has been estimated at as low as 2.2 kb of homologous stretches of DNA nucleotides.\n\nThe role of transposable elements in ectopic recombination is an area of active inquiry. Transposable elements—repetitious sequences of DNA that can insert themselves into any part of the genome—can encourage ectopic recombination at repeated homologous sequences of nucleotides. However, according to one proposed model, ectopic recombination might serve as an inhibitor of high transposable element copy numbers. The frequency of ectopic recombination of transposable elements has been linked to both higher copy numbers of transposable elements and the longer lengths of those elements. Since ectopic recombination is generally deleterious, anything that increases its odds of occurring is selected against, including the aforementioned higher copy numbers and longer lengths. This model, however, can only be applied to single families of transposable elements in the genome, as the probability of ectopic recombination occurring in one TE family is independent of it occurring in another. It follows that transposable elements that are shorter, transpose themselves less often, and have mutation rates high enough to disrupt the homology between transposable element sequences sufficiently to prevent ectopic recombination from occurring are selected for.\n", "id": "24039902", "title": "Ectopic recombination"}
{"url": "https://en.wikipedia.org/wiki?curid=1501977", "text": "Receptor-mediated endocytosis\n\nReceptor-mediated endocytosis (RME), also called clathrin-mediated endocytosis, is a process by which cells absorb metabolites, hormones, other proteins – and in some cases viruses – by the inward budding of plasma membrane vesicles containing proteins with receptor sites specific to the molecules being absorbed (endocytosis).\n\nAlthough receptors and their ligands can be brought into the cell through a few mechanisms (e.g. caveolin), clathrin-mediated endocytosis remains the best studied. Clathrin-mediated endocytosis of many receptor types begins with ligand binding its receptor. The ligand and receptor (sometimes bound to an adaptor protein) then diffuse through the plasma membrane until captured by a preformed or forming clathrin-coated pit. Other receptors can nucleate a clathrin-coated pit allowing formation around the receptor. A mature pit will pinch off from the plasma membrane forming a clathrin-coated vesicle that then uncoats and typically fuses to an early endosome. Once fused the endocytosed cargo (receptor and/or ligand) can then be sorted to lysosomal, recycling, or other trafficking pathways.\n\nThe function of receptor-mediated endocytosis is diverse. It is widely used for the specific uptake of certain substances required by the cell (examples include LDL via the LDL receptor or iron via transferrin). The role of receptor-mediated endocytosis is well recognized in the downregulation of transmembrane signal transduction but can also promote sustained signal transduction. The activated receptor becomes internalised and is transported to late endosomes and lysosomes for degradation. However, receptor-mediated endocytosis is also actively implicated in transducing signals from the cell periphery to the nucleus. This became apparent when it was found that the association and formation of specific signaling complexes is required for the effective signaling of hormones (e.g. EGF). Additionally it has been proposed that the directed transport of active signaling complexes to the nucleus might be required to enable signaling as random diffusion is too slow and mechanisms permanently downregulating incoming signals are strong enough to shut down signaling completely without additional signal-transducing mechanisms.\n\nUsing fluorescent dyes to stain the plasma membrane, it is possible to follow the internalization of plasma membrane fragments by microscopy.\n\nSince the process is non-specific, the ligand can be a carrier for larger molecules. If the target cell has a known specific pinocytotic receptor, drugs can be attached and will be internalized.\n\nTo achieve internalisation of nanoparticles into cells, such as T cells, antibodies can be used to target the nanoparticles to specific receptors on the cell surface (such as CCR5). This is one method of improving drug delivery to immune cells.\n\n\n\n", "id": "1501977", "title": "Receptor-mediated endocytosis"}
{"url": "https://en.wikipedia.org/wiki?curid=6389071", "text": "Endocytic cycle\n\nThe whole cycle of endocytosis plus exocytosis is known as the endocytic cycle. Most animal cells engage portions of their surface plasma membranes in a process called endocytosis. The main route of endocytosis is the coated pit, which buds into a cell to form a cytoplasmic vesicle—clathrin-coated vesicle. The membrane so internalised is processed in a series of intracellular organelles, which include endosomes and lysosomes. Some of this membrane is returned to the cell surface by a process called exocytosis. \n\nThe endocytic cycle is crucial for the survival of individual cells and multicellular organisms.\n\nLow-density lipoprotein (LDL) originates in the liver and is transported around the body by the blood stream. From there it is taken up by other cells, such as fibroblasts, and degraded: this provides a source of cholesterol for the growth of these other cells. LDL in the blood binds to LDL receptors on the surface of fibroblasts; these receptors concentrate in coated pits (they are about 200x as concentrated here as along the rest of the cell’s plasma membrane) and are internalised when the pit becomes a coated vesicle. The itinerary of the LDL receptor inside the cell is complex, but it spends little time there. Within a fraction of a minute, it has released its LDL cargo and is returned to the cell surface by exocytosis. It is now ready for another round of LDL clearance.\n\nHigh levels of LDL in the blood are observed in atherosclerosis and associated with the disease; the endocytic cycle reduces LDL through consuming it. This may or may not be useful in regulating increased levels of LDL, although it may have limitations, or not increase at all in the presence of extra LDL molecules.\n\nTransferrin is a plasma protein that is able to combine with iron ions: It is the vehicle with which iron is carried around the body. Free ferric ions are toxic; but cells need iron to build many of their proteins including cytochromes and hemoglobin. Ferric ions are carried in the blood tightly bound to transferrin as ferritransferrin. Dividing cells, which need the iron, gain it by binding the ferritransferrin to transferrin receptors on their surfaces. These receptors also have a high affinity for coated pits. Like the LDL receptor, the transferrin receptor is internalised into a coated vesicle. The iron is released inside the cell and the receptor returned to the cell surface. The route this receptor takes inside the cell appears to be different from that taken by the LDL receptor, because it takes about 10 minutes before it is exocytosed.\n\nImpulses between neurons are transmitted by the release of neurotransmitters at the junction between the two cells, a region called a synapse. This release is effected by exocytosis at the presynaptic terminal. A vesicle full of transmitter, acetylcholine (for example), in the presynaptic terminal fuses with its neighbouring plasma membrane and thereby releases a burst of acetylcholine into the synaptic space. The acetylcholine is rapidly degraded here, but before this happens it activates acetylcholine receptors on the postsynaptic terminal and triggers an electrical impulse in that cell. The membrane added to the presynaptic terminal is recovered by endocytosis and recycled to form fresh vesicles full of neurotransmitter, ready for another cycle of postsynaptic excitation.\n\nThus, the function of the nervous system is dependent on this endocytic cycle. An example of this dependence is found in fruit flies. A key protein required for endocytosis is dynamin: It assists in budding a coated pit into a cell to form a coated vesicle. Mutations in the dynamin gene in which the activity of the dynamin protein is lost at above-normal temperatures (for the fly) exist: These are called temperature-sensitive mutations. Such mutant flies have the property that, when the fly is brought from its normal 22°C to 30°C, the dynamin function is lost. However, when the flies are cooled to 22°C, it is regained. In other words, in these mutant flies, the endocytic cycle can be turned off at 30°C, and turned back on by cooling. What one observes is that, within seconds of warming to 30°C, the fruit flies become paralysed: They drop out of the air and appear near-dead. On cooling, they slowly get up, flutter their wings and fly away. The endocytic cycle has been temporarily suspended with drastic effects.\n\nAnimal cells, such as fibroblasts, as grown in culture in the laboratory are usually stationary; they grow and divide, but rarely move about. They have a normal endocytic cycle: coated pits 'bud in' from all over the cell’s surface in a random fashion and the returned membrane is exocytosed at the cell’s surface, also at random.\n\nMoving cells, such as fibroblasts, are arranged quite differently. Endocytosis by coated pits occurs, as in stationary cells, at random. But in motile cells exocytosis now occurs at the front of the cell: It is here that both LDL- and transferrin-receptors emerge from inside the cell and return to the cell surface. As the sites of endocytosis (at random on the cell surface) and exocytosis (at the front edge of the cell) are separated in space, it follows that (within the cell’s context) there is a flow of membrane from the front rearward.\n\nThe consequences of this polarised endocytic cycle are profound:\n\n• The membrane added at the front of the cell is believed to provide the surface there for the cell to extend itself forward as it moves.\n\n• Evidence indicates that those molecules in the cell’s surface that act as the feet of the cell — the integrins, which attach the cell to the substratum — can also be endocytosed and transported through the cell. In this way, fresh adhesion sites (see cell adhesion) are provided at the cell’s front.\n\n• The flow of membrane from the front rearward is not a flow of average plasma membrane proteins: The membrane internalised is a subset of plasma membrane proteins, such as LDL or transferrin receptors and the lipid bilayer in which they sit. These recycling molecules appear at the front surface of the cell and diffuse about, drifting rearwards until they are recaptured by a coated pit and transported back, through the cell, to the front. \nOther proteins do not participate in this cycle: Therefore, they experience a slow rearward flow of the lipid bilayer in which they reside. Thus, they are subject to two different influences: (a) they tend to get swept backward in this lipid flow and (b) they tend to randomise their distribution on the cell surface by Brownian motion. Calculation shows that, for non-cycling proteins, diffusion is the more important influence, so these molecules would be expected to have a near-random distribution on the cell’s surface. However, if a large object were attached to the cell surface that were unable to diffuse against the flow, it would be expected to be swept backward by the flow toward the trailing end of the cell. Indeed, it would act as a marker for that flow. This is the reason why carbon particles attached to the dorsal surface of a moving cell, or why aggregates of surface proteins, are seen to move to the back of a cell. This process is known as cap formation. \n\nFurther support for this scheme comes from a study of yeast cells (S. cerevisiae), which undergo a primitive form of movement called shmooing (after Al Capp's shmoo). A protein that cycles rapidly would be expected to be most concentrated near the shmoo tip of such a cell, whereas a non-cycling protein would be expected to be swept backward slightly. Both these distributions have been observed for a cycling and non-cycling variant of the same protein in the plasma membrane of yeast cells as they shmoo.\n\n• The molecular feet of a cell (usually integrins), when bound to the substratum, cannot diffuse about. Like any other macroscopic object sitting in the lipid flow, they are thus pushed backward. However, they cannot move backward (they are attached to the fixed substratum)) and therefore push the cell forward. This may be the force against the substratum, which enables a cell to move forward.\n\n• Polarised endocytic cycles are believed to exist in other cellular contexts, but the evidence for them at present is less clear.\n\n", "id": "6389071", "title": "Endocytic cycle"}
{"url": "https://en.wikipedia.org/wiki?curid=39936", "text": "Necrosis\n\nNecrosis (from the Greek νέκρωσις \"death, the stage of dying, the act of killing\" from νεκρός \"dead\") is a form of cell injury which results in the premature death of cells in living tissue by autolysis.\n\nNecrosis is caused by factors external to the cell or tissue, such as infection, toxins, or trauma which result in the unregulated digestion of cell components.\n\nIn contrast, apoptosis is a naturally occurring programmed and targeted cause of cellular death.\n\nWhile apoptosis often provides beneficial effects to the organism, necrosis is almost always detrimental and can be fatal.\n\nCellular death due to necrosis does not follow the apoptotic signal transduction pathway, but rather various receptors are activated, and result in the loss of cell membrane integrity and an uncontrolled release of products of cell death into the extracellular space.\n\nThis initiates in the surrounding tissue an inflammatory response which attracts leukocytes and nearby phagocytes which eliminate the dead cells by phagocytosis. However, microbial damaging substances released by leukocytes would create collateral damage to surrounding tissues. This excess collateral damage inhibits the healing process. Thus, untreated necrosis results in a build-up of decomposing dead tissue and cell debris at or near the site of the cell death. A classic example is gangrene. For this reason, it is often necessary to remove necrotic tissue surgically, a procedure known as debridement.\n\nStructural signs that indicate irreversible cell injury and the progression of necrosis include dense clumping and progressive disruption of genetic material, and disruption to membranes of cells and organelles.\n\nThere are six distinctive morphological patterns of necrosis:\n\n\n\nNecrosis may occur due to external or internal factors.\n\nExternal factors may involve mechanical trauma (physical damage to the body which causes cellular breakdown), damage to blood vessels (which may disrupt blood supply to associated tissue), and ischemia. Thermal effects (extremely high or low temperature) can result in necrosis due to the disruption of cells.\n\nIn frostbite, crystals form, increasing the pressure of remaining tissue and fluid causing the cells to burst. Under extreme conditions tissues and cells die through an unregulated process of destruction of membranes and cytosol.\nInternal factors causing necrosis include: trophoneurotic disorders; injury and paralysis of nerve cells. Pancreatic enzymes (lipases) are the major cause of fat necrosis.\n\nNecrosis can be activated by components of the immune system, such as the complement system; bacterial toxins; activated natural killer cells; and peritoneal macrophages. Pathogen-induced necrosis programs in cells with immunological barriers (intestinal mucosa) may alleviate invasion of pathogens through surfaces affected by inflammation. Toxins and pathogens may cause necrosis; toxins such as snake venoms may inhibit enzymes and cause cell death. Necrotic wounds have also resulted from the stings of \"Vespa mandarinia.\"\n\nPathological conditions are characterized by inadequate secretion of cytokines. Nitric oxide (NO) and reactive oxygen species (ROS) are also accompanied by intense necrotic death of cells. A classic example of a necrotic condition is ischemia which leads to a drastic depletion of oxygen, glucose, and other trophic factors and induces massive necrotic death of endothelial cells and non-proliferating cells of surrounding tissues (neurons, cardiomyocytes, renal cells, etc.). Recent cytological data indicates that necrotic death occurs not only during pathological events but it is also a component of some physiological process.\n\nActivation-induced death of primary T-lymphocytes and other important constituents of the immune response are caspase-independent and necrotic by morphology; hence, current researchers have demonstrated that the occurrence of necrotic cell death can not only occur during pathological processes but also during normal processes such as tissue renewal, embryogenesis, and immune response.\n\nUntil recently, necrosis was thought to be an unregulated process. There are two broad pathways in which necrosis may occur in an organism.\n\nThe first of these two pathways initially involves oncosis, where swelling of the cells occur. The cell then proceeds to blebbing, and this is followed by pyknosis, in which nuclear shrinkage transpires. In the final step of this pathway the nucleus is dissolved into the cytoplasm, which is referred to as karyolysis.\n\nThe second pathway is a secondary form of necrosis that is shown to occur after apoptosis and budding. Cellular changes of necrosis occur in this secondary form of apoptosis, where the nucleus breaks into fragments, which is known as karyorrhexis.\n\nThe nucleus changes in necrosis, and characteristics of this change are determined by manner in which its DNA breaks down:\n\n\nPlasma alterations are also seen in necrosis. Plasma membranes appear discontinuous when viewed with an electron microscope. This discontinuous membrane is caused by cell blebbing and the loss of microvilli.\n\nThere are many causes of necrosis, and as such treatment is based upon how the necrosis came about. Treatment of necrosis typically involves two distinct processes: Usually, the underlying cause of the necrosis must be treated before the dead tissue itself can be dealt with.\n\n\nEven after the initial cause of the necrosis has been halted, the necrotic tissue will remain in the body. The body's immune response to apoptosis, which involves the automatic breaking down and recycling of cellular material, is not triggered by necrotic cell death due to the apoptotic pathway being disabled.\n\nIf calcium is deficient, pectin cannot be synthesized, and therefore the cell walls cannot be bonded and thus an impediment of the meristems. This will lead to necrosis of stem and root tips and leaf edges. For example, necrosis of tissue can occur in \"Arabidopsis thaliana\" due to plant pathogens.\n\n\n", "id": "39936", "title": "Necrosis"}
{"url": "https://en.wikipedia.org/wiki?curid=854294", "text": "DNA repair\n\nDNA repair is a collection of processes by which a cell identifies and corrects damage to the DNA molecules that encode its genome. In human cells, both normal metabolic activities and environmental factors such as radiation can cause DNA damage, resulting in as many as 1 million individual molecular lesions per cell per day. Many of these lesions cause structural damage to the DNA molecule and can alter or eliminate the cell's ability to transcribe the gene that the affected DNA encodes. Other lesions induce potentially harmful mutations in the cell's genome, which affect the survival of its daughter cells after it undergoes mitosis. As a consequence, the DNA repair process is constantly active as it responds to damage in the DNA structure. When normal repair processes fail, and when cellular apoptosis does not occur, irreparable DNA damage may occur, including double-strand breaks and DNA crosslinkages (interstrand crosslinks or ICLs). This can eventually lead to malignant tumors, or cancer as per the two hit hypothesis.\n\nThe rate of DNA repair is dependent on many factors, including the cell type, the age of the cell, and the extracellular environment. A cell that has accumulated a large amount of DNA damage, or one that no longer effectively repairs damage incurred to its DNA, can enter one of three possible states:\n\nThe DNA repair ability of a cell is vital to the integrity of its genome and thus to the normal functionality of that organism. Many genes that were initially shown to influence life span have turned out to be involved in DNA damage repair and protection.\nThe 2015 Nobel Prize in Chemistry was awarded to Tomas Lindahl, Paul Modrich, and Aziz Sancar for their work on the molecular mechanisms of DNA repair processes.\nThere are two types: nucleotide excision repair and base excision repair.\n\nDNA damage, due to environmental factors and normal metabolic processes inside the cell, occurs at a rate of 10,000 to 1,000,000 molecular lesions per cell per day. While this constitutes only 0.000165% of the human genome's approximately 6 billion bases (3 billion base pairs), unrepaired lesions in critical genes (such as tumor suppressor genes) can impede a cell's ability to carry out its function and appreciably increase the likelihood of tumor formation and contribute to tumour heterogeneity.\n\nThe vast majority of DNA damage affects the primary structure of the double helix; that is, the bases themselves are chemically modified. These modifications can in turn disrupt the molecules' regular helical structure by introducing non-native chemical bonds or bulky adducts that do not fit in the standard double helix. Unlike proteins and RNA, DNA usually lacks tertiary structure and therefore damage or disturbance does not occur at that level. DNA is, however, supercoiled and wound around \"packaging\" proteins called histones (in eukaryotes), and both superstructures are vulnerable to the effects of DNA damage.\n\nDNA damage can be subdivided into two main types:\n\n\nThe replication of damaged DNA before cell division can lead to the incorporation of wrong bases opposite damaged ones. Daughter cells that inherit these wrong bases carry mutations from which the original DNA sequence is unrecoverable (except in the rare case of a back mutation, for example, through gene conversion).\n\nThere are several types of damage to DNA due to endogenous cellular processes:\n\nDamage caused by exogenous agents comes in many forms. Some examples are:\n\nUV damage, alkylation/methylation, X-ray damage and oxidative damage are examples of induced damage. Spontaneous damage can include the loss of a base, deamination, sugar ring puckering and tautomeric shift.\n\nIn human cells, and eukaryotic cells in general, DNA is found in two cellular locations — inside the nucleus and inside the mitochondria. Nuclear DNA (nDNA) exists as chromatin during non-replicative stages of the cell cycle and is condensed into aggregate structures known as chromosomes during cell division. In either state the DNA is highly compacted and wound up around bead-like proteins called histones. Whenever a cell needs to express the genetic information encoded in its nDNA the required chromosomal region is unravelled, genes located therein are expressed, and then the region is condensed back to its resting conformation. Mitochondrial DNA (mtDNA) is located inside mitochondria organelles, exists in multiple copies, and is also tightly associated with a number of proteins to form a complex known as the nucleoid. Inside mitochondria, reactive oxygen species (ROS), or free radicals, byproducts of the constant production of adenosine triphosphate (ATP) via oxidative phosphorylation, create a highly oxidative environment that is known to damage mtDNA. A critical enzyme in counteracting the toxicity of these species is superoxide dismutase, which is present in both the mitochondria and cytoplasm of eukaryotic cells.\n\nSenescence, an irreversible process in which the cell no longer divides, is a protective response to the shortening of the chromosome ends. The telomeres are long regions of repetitive noncoding DNA that cap chromosomes and undergo partial degradation each time a cell undergoes division (see Hayflick limit). In contrast, quiescence is a reversible state of cellular dormancy that is unrelated to genome damage (see cell cycle). Senescence in cells may serve as a functional alternative to apoptosis in cases where the physical presence of a cell for spatial reasons is required by the organism, which serves as a \"last resort\" mechanism to prevent a cell with damaged DNA from replicating inappropriately in the absence of pro-growth cellular signaling. Unregulated cell division can lead to the formation of a tumor (see cancer), which is potentially lethal to an organism. Therefore, the induction of senescence and apoptosis is considered to be part of a strategy of protection against cancer.\n\nIt is important to distinguish between DNA damage and mutation, the two major types of error in DNA. DNA damage and mutation are fundamentally different. Damage results in physical abnormalities in the DNA, such as single- and double-strand breaks, 8-hydroxydeoxyguanosine residues, and polycyclic aromatic hydrocarbon adducts. DNA damage can be recognized by enzymes, and thus can be correctly repaired if redundant information, such as the undamaged sequence in the complementary DNA strand or in a homologous chromosome, is available for copying. If a cell retains DNA damage, transcription of a gene can be prevented, and thus translation into a protein will also be blocked. Replication may also be blocked or the cell may die.\n\nIn contrast to DNA damage, a mutation is a change in the base sequence of the DNA. A mutation cannot be recognized by enzymes once the base change is present in both DNA strands, and thus a mutation cannot be repaired. At the cellular level, mutations can cause alterations in protein function and regulation. Mutations are replicated when the cell replicates. In a population of cells, mutant cells will increase or decrease in frequency according to the effects of the mutation on the ability of the cell to survive and reproduce.\n\nAlthough distinctly different from each other, DNA damage and mutation are related because DNA damage often causes errors of DNA synthesis during replication or repair; these errors are a major source of mutation.\n\nGiven these properties of DNA damage and mutation, it can be seen that DNA damage is a special problem in non-dividing or slowly-dividing cells, where unrepaired damage will tend to accumulate over time. On the other hand, in rapidly-dividing cells, unrepaired DNA damage that does not kill the cell by blocking replication will tend to cause replication errors and thus mutation. The great majority of mutations that are not neutral in their effect are deleterious to a cell's survival. Thus, in a population of cells composing a tissue with replicating cells, mutant cells will tend to be lost. However, infrequent mutations that provide a survival advantage will tend to clonally expand at the expense of neighboring cells in the tissue. This advantage to the cell is disadvantageous to the whole organism, because such mutant cells can give rise to cancer. Thus, DNA damage in frequently dividing cells, because it gives rise to mutations, is a prominent cause of cancer. In contrast, DNA damage in infrequently-dividing cells is likely a prominent cause of aging.\n\nCells cannot function if DNA damage corrupts the integrity and accessibility of essential information in the genome (but cells remain superficially functional when non-essential genes are missing or damaged). Depending on the type of damage inflicted on the DNA's double helical structure, a variety of repair strategies have evolved to restore lost information. If possible, cells use the unmodified complementary strand of the DNA or the sister chromatid as a template to recover the original information. Without access to a template, cells use an error-prone recovery mechanism known as translesion synthesis as a last resort.\n\nDamage to DNA alters the spatial configuration of the helix, and such alterations can be detected by the cell. Once damage is localized, specific DNA repair molecules bind at or near the site of damage, inducing other molecules to bind and form a complex that enables the actual repair to take place.\n\nCells are known to eliminate three types of damage to their DNA by chemically reversing it. These mechanisms do not require a template, since the types of damage they counteract can occur in only one of the four bases. Such direct reversal mechanisms are specific to the type of damage incurred and do not involve breakage of the phosphodiester backbone. The formation of pyrimidine dimers upon irradiation with UV light results in an abnormal covalent bond between adjacent pyrimidine bases. The photoreactivation process directly reverses this damage by the action of the enzyme photolyase, whose activation is obligately dependent on energy absorbed from blue/UV light (300–500 nm wavelength) to promote catalysis. Photolyase, an old enzyme present in bacteria, fungi, and most animals no longer functions in humans, who instead use nucleotide excision repair to repair damage from UV irradiation. Another type of damage, methylation of guanine bases, is directly reversed by the protein methyl guanine methyl transferase (MGMT), the bacterial equivalent of which is called ogt. This is an expensive process because each MGMT molecule can be used only once; that is, the reaction is stoichiometric rather than catalytic. A generalized response to methylating agents in bacteria is known as the adaptive response and confers a level of resistance to alkylating agents upon sustained exposure by upregulation of alkylation repair enzymes. The third type of DNA damage reversed by cells is certain methylation of the bases cytosine and adenine.\n\nWhen only one of the two strands of a double helix has a defect, the other strand can be used as a template to guide the correction of the damaged strand. In order to repair damage to one of the two paired molecules of DNA, there exist a number of excision repair mechanisms that remove the damaged nucleotide and replace it with an undamaged nucleotide complementary to that found in the undamaged DNA strand.\n\nDouble-strand breaks, in which both strands in the double helix are severed, are particularly hazardous to the cell because they can lead to genome rearrangements. PVN Acharya noted that double-strand breaks and a \"cross-linkage joining both strands at the same point is irreparable because neither strand can then serve as a template for repair. The cell will die in the next mitosis or in some rare instances, mutate.\" Three mechanisms exist to repair double-strand breaks (DSBs): non-homologous end joining (NHEJ), microhomology-mediated end joining (MMEJ), and homologous recombination. In an \"in vitro\" system, MMEJ occurred in mammalian cells at the levels of 10–20% of HR when both HR and NHEJ mechanisms were also available.\nIn NHEJ, DNA Ligase IV, a specialized DNA ligase that forms a complex with the cofactor XRCC4, directly joins the two ends. To guide accurate repair, NHEJ relies on short homologous sequences called microhomologies present on the single-stranded tails of the DNA ends to be joined. If these overhangs are compatible, repair is usually accurate. NHEJ can also introduce mutations during repair. Loss of damaged nucleotides at the break site can lead to deletions, and joining of nonmatching termini forms insertions or translocations. NHEJ is especially important before the cell has replicated its DNA, since there is no template available for repair by homologous recombination. There are \"backup\" NHEJ pathways in higher eukaryotes. Besides its role as a genome caretaker, NHEJ is required for joining hairpin-capped double-strand breaks induced during V(D)J recombination, the process that generates diversity in B-cell and T-cell receptors in the vertebrate immune system.\n\nHomologous recombination requires the presence of an identical or nearly identical sequence to be used as a template for repair of the break. The enzymatic machinery responsible for this repair process is nearly identical to the machinery responsible for chromosomal crossover during meiosis. This pathway allows a damaged chromosome to be repaired using a sister chromatid (available in G2 after DNA replication) or a homologous chromosome as a template. DSBs caused by the replication machinery attempting to synthesize across a single-strand break or unrepaired lesion cause collapse of the replication fork and are typically repaired by recombination.\n\nMMEJ starts with short-range end resection by MRE11 nuclease on either side of a double-strand break to reveal microhomology regions. In further steps, Poly (ADP-ribose) polymerase 1 (PARP1) is required and may be an early step in MMEJ. There is pairing of microhomology regions followed by recruitment of flap structure-specific endonuclease 1 (FEN1) to remove overhanging flaps. This is followed by recruitment of XRCC1–LIG3 to the site for ligating the DNA ends, leading to an intact DNA. MMEJ is always accompanied by a deletion, so that MMEJ is a mutagenic pathway for DNA repair.\n\nThe extremophile \"Deinococcus radiodurans\" has a remarkable ability to survive DNA damage from ionizing radiation and other sources. At least two copies of the genome, with random DNA breaks, can form DNA fragments through annealing. Partially overlapping fragments are then used for synthesis of homologous regions through a moving D-loop that can continue extension until they find complementary partner strands. In the final step there is crossover by means of RecA-dependent homologous recombination.\n\nTopoisomerases introduce both single- and double-strand breaks in the course of changing the DNA's state of supercoiling, which is especially common in regions near an open replication fork. Such breaks are not considered DNA damage because they are a natural intermediate in the topoisomerase biochemical mechanism and are immediately repaired by the enzymes that created them.\n\nTranslesion synthesis (TLS) is a DNA damage tolerance process that allows the DNA replication machinery to replicate past DNA lesions such as thymine dimers or AP sites. It involves switching out regular DNA polymerases for specialized translesion polymerases (i.e. DNA polymerase IV or V, from the Y Polymerase family), often with larger active sites that can facilitate the insertion of bases opposite damaged nucleotides. The polymerase switching is thought to be mediated by, among other factors, the post-translational modification of the replication processivity factor PCNA. Translesion synthesis polymerases often have low fidelity (high propensity to insert wrong bases) on undamaged templates relative to regular polymerases. However, many are extremely efficient at inserting correct bases opposite specific types of damage. For example, Pol η mediates error-free bypass of lesions induced by UV irradiation, whereas Pol ι introduces mutations at these sites. Pol η is known to add the first adenine across the T^T photodimer using Watson-Crick base pairing and the second adenine will be added in its syn conformation using Hoogsteen base pairing. From a cellular perspective, risking the introduction of point mutations during translesion synthesis may be preferable to resorting to more drastic mechanisms of DNA repair, which may cause gross chromosomal aberrations or cell death. In short, the process involves specialized polymerases either bypassing or repairing lesions at locations of stalled DNA replication. For example, Human DNA polymerase eta can bypass complex DNA lesions like guanine-thymine intra-strand crosslink, G[8,5-Me]T, although can cause targeted and semi-targeted mutations. Paromita Raychaudhury and Ashis Basu studied the toxicity and mutagenesis of the same lesion in \"Escherichia coli\" by replicating a G[8,5-Me]T-modified plasmid in \"E. coli\" with specific DNA polymerase knockouts. Viability was very low in a strain lacking pol II, pol IV, and pol V, the three SOS-inducible DNA polymerases, indicating that translesion synthesis is conducted primarily by these specialized DNA polymerases.\nA bypass platform is provided to these polymerases by Proliferating cell nuclear antigen (PCNA). Under normal circumstances, PCNA bound to polymerases replicates the DNA. At a site of lesion, PCNA is ubiquitinated, or modified, by the RAD6/RAD18 proteins to provide a platform for the specialized polymerases to bypass the lesion and resume DNA replication. After translesion synthesis, extension is required. This extension can be carried out by a replicative polymerase if the TLS is error-free, as in the case of Pol η, yet if TLS results in a mismatch, a specialized polymerase is needed to extend it; Pol ζ. Pol ζ is unique in that it can extend terminal mismatches, whereas more processive polymerases cannot. So when a lesion is encountered, the replication fork will stall, PCNA will switch from a processive polymerase to a TLS polymerase such as Pol ι to fix the lesion, then PCNA may switch to Pol ζ to extend the mismatch, and last PCNA will switch to the processive polymerase to continue replication.\n\nCells exposed to ionizing radiation, ultraviolet light or chemicals are prone to acquire multiple sites of bulky DNA lesions and double-strand breaks. Moreover, DNA damaging agents can damage other biomolecules such as proteins, carbohydrates, lipids, and RNA. The accumulation of damage, to be specific, double-strand breaks or adducts stalling the replication forks, are among known stimulation signals for a global response to DNA damage. The global response to damage is an act directed toward the cells' own preservation and triggers multiple pathways of macromolecular repair, lesion bypass, tolerance, or apoptosis. The common features of global response are induction of multiple genes, cell cycle arrest, and inhibition of cell division.\n\nThe packaging of eukaryotic DNA into chromatin presents a barrier to all DNA-based processes that require recruitment of enzymes to their sites of action. To allow DNA repair, the chromatin must be remodeled. In eukaryotes, ATP dependent chromatin remodeling complexes and histone-modifying enzymes are two predominant factors employed to accomplish this remodeling process.\n\nChromatin relaxation occurs rapidly at the site of a DNA damage. In one of the earliest steps, the stress-activated protein kinase, c-Jun N-terminal kinase (JNK), phosphorylates SIRT6 on serine 10 in response to double-strand breaks or other DNA damage. This post-translational modification facilitates the mobilization of SIRT6 to DNA damage sites, and is required for efficient recruitment of poly (ADP-ribose) polymerase 1 (PARP1) to DNA break sites and for efficient repair of DSBs. PARP1 protein starts to appear at DNA damage sites in less than a second, with half maximum accumulation within 1.6 seconds after the damage occurs. PARP1 synthesizes polymeric adenosine diphosphate ribose (poly (ADP-ribose) or PAR) chains on itself. Next the chromatin remodeler ALC1 quickly attaches to the product of PARP1 action, a poly-ADP ribose chain, and ALC1 completes arrival at the DNA damage within 10 seconds of the occurrence of the damage. About half of the maximum chromatin relaxation, presumably due to action of ALC1, occurs by 10 seconds. This then allows recruitment of the DNA repair enzyme MRE11, to initiate DNA repair, within 13 seconds.\n\nγH2AX, the phosphorylated form of H2AX is also involved in the early steps leading to chromatin decondensation after DNA double-strand breaks. The histone variant H2AX constitutes about 10% of the H2A histones in human chromatin. γH2AX (H2AX phosphorylated on serine 139) can be detected as soon as 20 seconds after irradiation of cells (with DNA double-strand break formation), and half maximum accumulation of γH2AX occurs in one minute. The extent of chromatin with phosphorylated γH2AX is about two million base pairs at the site of a DNA double-strand break. γH2AX does not, itself, cause chromatin decondensation, but within 30 seconds of irradiation, RNF8 protein can be detected in association with γH2AX. RNF8 mediates extensive chromatin decondensation, through its subsequent interaction with CHD4, a component of the nucleosome remodeling and deacetylase complex NuRD.\n\nDDB2 occurs in a heterodimeric complex with DDB1. This complex further complexes with the ubiquitin ligase protein CUL4A and with PARP1. This larger complex rapidly associates with UV-induced damage within chromatin, with half-maximum association completed in 40 seconds. The PARP1 protein, attached to both DDB1 and DDB2, then PARylates (creates a poly-ADP ribose chain) on DDB2 that attracts the DNA remodeling protein ALC1. Action of ALC1 relaxes the chromatin at the site of UV damage to DNA. This relaxation allows other proteins in the nucleotide excision repair pathway to enter the chromatin and repair UV-induced cyclobutane pyrimidine dimer damages.\n\nAfter rapid chromatin remodeling, cell cycle checkpoints are activated to allow DNA repair to occur before the cell cycle progresses. First, two kinases, ATM and ATR are activated within 5 or 6 minutes after DNA is damaged. This is followed by phosphorylation of the cell cycle checkpoint protein Chk1, initiating its function, about 10 minutes after DNA is damaged.\n\nAfter DNA damage, cell cycle checkpoints are activated. Checkpoint activation pauses the cell cycle and gives the cell time to repair the damage before continuing to divide. DNA damage checkpoints occur at the G1/S and G2/M boundaries. An intra-S checkpoint also exists. Checkpoint activation is controlled by two master kinases, ATM and ATR. ATM responds to DNA double-strand breaks and disruptions in chromatin structure, whereas ATR primarily responds to stalled replication forks. These kinases phosphorylate downstream targets in a signal transduction cascade, eventually leading to cell cycle arrest. A class of checkpoint mediator proteins including BRCA1, MDC1, and 53BP1 has also been identified. These proteins seem to be required for transmitting the checkpoint activation signal to downstream proteins.\n\nDNA damage checkpoint is a signal transduction pathway that blocks cell cycle progression in G1, G2 and metaphase and slows down the rate of S phase progression when DNA is damaged. It leads to a pause in cell cycle allowing the cell time to repair the damage before continuing to divide.\n\nCheckpoint Proteins can be separated into four groups: phosphatidylinositol 3-kinase (PI3K)-like protein kinase, proliferating cell nuclear antigen (PCNA)-like group, two serine/threonine(S/T) kinases and their adaptors. Central to all DNA damage induced checkpoints responses is a pair of large protein kinases belonging to the first group of PI3K-like protein kinases-the ATM (Ataxia telangiectasia mutated) and ATR (Ataxia- and Rad-related) kinases, whose sequence and functions have been well conserved in evolution. All DNA damage response requires either ATM or ATR because they have the ability to bind to the chromosomes at the site of DNA damage, together with accessory proteins that are platforms on which DNA damage response components and DNA repair complexes can be assembled.\n\nAn important downstream target of ATM and ATR is p53, as it is required for inducing apoptosis following DNA damage. The cyclin-dependent kinase inhibitor p21 is induced by both p53-dependent and p53-independent mechanisms and can arrest the cell cycle at the G1/S and G2/M checkpoints by deactivating cyclin/cyclin-dependent kinase complexes.\n\nThe SOS response is the changes in gene expression in \"Escherichia coli\" and other bacteria in response to extensive DNA damage. The prokaryotic SOS system is regulated by two key proteins: LexA and RecA. The LexA homodimer is a transcriptional repressor that binds to operator sequences commonly referred to as SOS boxes. In \"Escherichia coli\" it is known that LexA regulates transcription of approximately 48 genes including the lexA and recA genes. The SOS response is known to be widespread in the Bacteria domain, but it is mostly absent in some bacterial phyla, like the Spirochetes.\nThe most common cellular signals activating the SOS response are regions of single-stranded DNA (ssDNA), arising from stalled replication forks or double-strand breaks, which are processed by DNA helicase to separate the two DNA strands. In the initiation step, RecA protein binds to ssDNA in an ATP hydrolysis driven reaction creating RecA–ssDNA filaments. RecA–ssDNA filaments activate LexA autoprotease activity, which ultimately leads to cleavage of LexA dimer and subsequent LexA degradation. The loss of LexA repressor induces transcription of the SOS genes and allows for further signal induction, inhibition of cell division and an increase in levels of proteins responsible for damage processing.\n\nIn \"Escherichia coli\", SOS boxes are 20-nucleotide long sequences near promoters with palindromic structure and a high degree of sequence conservation. In other classes and phyla, the sequence of SOS boxes varies considerably, with different length and composition, but it is always highly conserved and one of the strongest short signals in the genome. The high information content of SOS boxes permits differential binding of LexA to different promoters and allows for timing of the SOS response. The lesion repair genes are induced at the beginning of SOS response. The error-prone translesion polymerases, for example, UmuCD'2 (also called DNA polymerase V), are induced later on as a last resort. Once the DNA damage is repaired or bypassed using polymerases or through recombination, the amount of single-stranded DNA in cells is decreased, lowering the amounts of RecA filaments decreases cleavage activity of LexA homodimer, which then binds to the SOS boxes near promoters and restores normal gene expression.\n\nEukaryotic cells exposed to DNA damaging agents also activate important defensive pathways by inducing multiple proteins involved in DNA repair, cell cycle checkpoint control, protein trafficking and degradation. Such genome wide transcriptional response is very complex and tightly regulated, thus allowing coordinated global response to damage. Exposure of yeast \"Saccharomyces cerevisiae\" to DNA damaging agents results in overlapping but distinct transcriptional profiles. Similarities to environmental shock response indicates that a general global stress response pathway exist at the level of transcriptional activation. In contrast, different human cell types respond to damage differently indicating an absence of a common global response. The probable explanation for this difference between yeast and human cells may be in the heterogeneity of mammalian cells. In an animal different types of cells are distributed among different organs that have evolved different sensitivities to DNA damage.\n\nIn general global response to DNA damage involves expression of multiple genes responsible for postreplication repair, homologous recombination, nucleotide excision repair, DNA damage checkpoint, global transcriptional activation, genes controlling mRNA decay, and many others. A large amount of damage to a cell leaves it with an important decision: undergo apoptosis and die, or survive at the cost of living with a modified genome. An increase in tolerance to damage can lead to an increased rate of survival that will allow a greater accumulation of mutations. Yeast Rev1 and human polymerase η are members of [Y family translesion DNA polymerases present during global response to DNA damage and are responsible for enhanced mutagenesis during a global response to DNA damage in eukaryotes.\n\nExperimental animals with genetic deficiencies in DNA repair often show decreased life span and increased cancer incidence. For example, mice deficient in the dominant NHEJ pathway and in telomere maintenance mechanisms get lymphoma and infections more often, and, as a consequence, have shorter lifespans than wild-type mice. In similar manner, mice deficient in a key repair and transcription protein that unwinds DNA helices have premature onset of aging-related diseases and consequent shortening of lifespan. However, not every DNA repair deficiency creates exactly the predicted effects; mice deficient in the NER pathway exhibited shortened life span without correspondingly higher rates of mutation.\n\nIf the rate of DNA damage exceeds the capacity of the cell to repair it, the accumulation of errors can overwhelm the cell and result in early senescence, apoptosis, or cancer. Inherited diseases associated with faulty DNA repair functioning result in premature aging, increased sensitivity to carcinogens, and correspondingly increased cancer risk (see below). On the other hand, organisms with enhanced DNA repair systems, such as \"Deinococcus radiodurans\", the most radiation-resistant known organism, exhibit remarkable resistance to the double-strand break-inducing effects of radioactivity, likely due to enhanced efficiency of DNA repair and especially NHEJ.\n\nA number of individual genes have been identified as influencing variations in life span within a population of organisms. The effects of these genes is strongly dependent on the environment, in particular, on the organism's diet. Caloric restriction reproducibly results in extended lifespan in a variety of organisms, likely via nutrient sensing pathways and decreased metabolic rate. The molecular mechanisms by which such restriction results in lengthened lifespan are as yet unclear (see for some discussion); however, the behavior of many genes known to be involved in DNA repair is altered under conditions of caloric restriction. Several agents reported to have anti-aging properties have been shown to attenuate constitutive level of mTOR signaling, an evidence of reduction of metabolic activity, and concurrently to reduce constitutive level of DNA damage] induced by endogenously generated reactive oxygen species. \nFor example, increasing the gene dosage of the gene SIR-2, which regulates DNA packaging in the nematode worm \"Caenorhabditis elegans\", can significantly extend lifespan. The mammalian homolog of SIR-2 is known to induce downstream DNA repair factors involved in NHEJ, an activity that is especially promoted under conditions of caloric restriction. Caloric restriction has been closely linked to the rate of base excision repair in the nuclear DNA of rodents, although similar effects have not been observed in mitochondrial DNA.\n\nThe \"C. elegans\" gene AGE-1, an upstream effector of DNA repair pathways, confers dramatically extended life span under free-feeding conditions but leads to a decrease in reproductive fitness under conditions of caloric restriction. This observation supports the pleiotropy theory of the biological origins of aging, which suggests that genes conferring a large survival advantage early in life will be selected for even if they carry a corresponding disadvantage late in life.\n\nDefects in the NER mechanism are responsible for several genetic disorders, including:\n\n\nMental retardation often accompanies the latter two disorders, suggesting increased vulnerability of developmental neurons.\n\nOther DNA repair disorders include:\n\n\nAll of the above diseases are often called \"segmental progerias\" (\"accelerated aging diseases\") because their victims appear elderly and suffer from aging-related diseases at an abnormally young age, while not manifesting all the symptoms of old age.\n\nOther diseases associated with reduced DNA repair function include Fanconi anemia, hereditary breast cancer and hereditary colon cancer.\n\nBecause of inherent limitations in the DNA repair mechanisms, if humans lived long enough, they would all eventually develop cancer. There are at least 34 Inherited human DNA repair gene mutations that increase cancer risk. Many of these mutations cause DNA repair to be less effective than normal. In particular, Hereditary nonpolyposis colorectal cancer (HNPCC) is strongly associated with specific mutations in the DNA mismatch repair pathway. \"BRCA1\" and \"BRCA2\", two famous genes whose mutations confer a hugely increased risk of breast cancer on carriers, are both associated with a large number of DNA repair pathways, especially NHEJ and homologous recombination.\n\nCancer therapy procedures such as chemotherapy and radiotherapy work by overwhelming the capacity of the cell to repair DNA damage, resulting in cell death. Cells that are most rapidly dividing — most typically cancer cells — are preferentially affected. The side-effect is that other non-cancerous but rapidly dividing cells such as progenitor cells in the gut, skin, and hematopoietic system are also affected. Modern cancer treatments attempt to localize the DNA damage to cells and tissues only associated with cancer, either by physical means (concentrating the therapeutic agent in the region of the tumor) or by biochemical means (exploiting a feature unique to cancer cells in the body). …; in the context of therapies targeting DNA damage response genes, the latter approach has been termed ‘synthetic lethality’.\n\nPerhaps the most well-known of these 'synthetic lethality' drugs is the poly(ADP-ribose) polymerase 1 (PARP1) inhibitor olaparib, which was approved by the Food and Drug Administration in 2015 for the treatment in women of BRCA-defective ovarian cancer. Tumor cells with partial loss of DNA damage response (specifically, homologous recombination repair) are dependent on another mechanism – single-strand break repair – which is a mechanism consisting, in part, of the PARP1 gene product. Olaparib is combined with chemotherapeutics to inhibit single-strand break repair induced by DNA damage caused by the co-administered chemotherapy. Tumor cells relying on this residual DNA repair mechanism are unable to repair the damage and hence are not able to survive and proliferate, whereas normal cells can repair the damage with the functioning homologous recombination mechanism.\n\nMany other drugs for use against other residual DNA repair mechanisms commonly found in cancer are currently under investigation. However, synthetic lethality therapeutic approaches have been questioned due to emerging evidence of acquired resistance, achieved through rewiring of DNA damage response pathways and reversion of previously-inhibited defects.\n\nIt has become apparent over the past several years that the DNA damage response acts as a barrier to the malignant transformation of preneoplastic cells. Previous studies have shown an elevated DNA damage response in cell-culture models with oncogene activation and preneoplastic colon adenomas. DNA damage response mechanisms trigger cell-cycle arrest, and attempt to repair DNA lesions or promote cell death/senescence if repair is not possible. Replication stress is observed in preneoplastic cells due to increased proliferation signals from oncogenic mutations. Replication stress is characterized by: increased replication initiation/origin firing; increased transcription and collisions of transcription-replication complexes; nucleotide deficiency; increase in reactive oxygen species (ROS).\n\nReplication stress, along with the selection for inactivating mutations in DNA damage response genes in the evolution of the tumor, leads to downregulation and/or loss of some DNA damage response mechanisms, and hence loss of DNA repair and/or senescence/programmed cell death. In experimental mouse models, loss of DNA damage response-mediated cell senescence was observed after using a short hairpin RNA (shRNA) to inhibit the double-strand break response kinase ataxia telangiectasia (ATM), leading to increased tumor size and invasiveness. Humans born with inherited defects in DNA repair mechanisms (for example, Li-Fraumeni syndrome) have a higher cancer risk.\n\nThe prevalence of DNA damage response mutations differs across cancer types; for example, 30% of breast invasive carcinomas have mutations in genes involved in homologous recombination. In cancer, downregulation is observed across all DNA damage response mechanisms (base excision repair (BER), nucleotide excision repair (NER), DNA mismatch repair (MMR), homologous recombination repair (HR), non-homologous end joining (NHEJ) and translesion DNA synthesis (TLS). As well as mutations to DNA damage repair genes, mutations also arise in the genes responsible for arresting the cell cycle to allow sufficient time for DNA repair to occur, and some genes are involved in both DNA damage repair and cell cycle checkpoint control, for example ATM and checkpoint kinase 2 (CHEK2) – a tumor suppressor that is often absent or downregulated in non-small cell lung cancer.\n\nTable: Genes involved in DNA damage response pathways and frequently mutated in cancer (HR = homologous recombination; NHEJ = non-homologous end joining; SSA = single-strand annealing; FA = fanconi anemia pathway; BER = base excision repair; NER = nucleotide excision repair; MMR = mismatch repair)\n\nClassically, cancer has been viewed as a set of diseases that are driven by progressive genetic abnormalities that include mutations in tumour-suppressor genes and oncogenes, and chromosomal aberrations. However, it has become apparent that cancer is also driven by\nepigenetic alterations.\n\nEpigenetic alterations refer to functionally relevant modifications to the genome that do not involve a change in the nucleotide sequence. Examples of such modifications are changes in DNA methylation (hypermethylation and hypomethylation) and histone modification, changes in chromosomal architecture (caused by inappropriate expression of proteins such as HMGA2 or HMGA1) and changes caused by microRNAs. Each of these epigenetic alterations serves to regulate gene expression without altering the underlying DNA sequence. These changes usually remain through cell divisions, last for multiple cell generations, and can be considered to be epimutations (equivalent to mutations).\n\nWhile large numbers of epigenetic alterations are found in cancers, the epigenetic alterations in DNA repair genes, causing reduced expression of DNA repair proteins, appear to be particularly important. Such alterations are thought to occur early in progression to cancer and to be a likely cause of the genetic instability characteristic of cancers.\n\nReduced expression of DNA repair genes causes deficient DNA repair. When DNA repair is deficient DNA damages remain in cells at a higher than usual level and these excess damages cause increased frequencies of mutation or epimutation. Mutation rates increase substantially in cells defective in DNA mismatch repair or in homologous recombinational repair (HRR). Chromosomal rearrangements and aneuploidy also increase in HRR defective cells.\n\nHigher levels of DNA damage not only cause increased mutation, but also cause increased epimutation. During repair of DNA double strand breaks, or repair of other DNA damages, incompletely cleared sites of repair can cause epigenetic gene silencing.\n\nDeficient expression of DNA repair proteins due to an inherited mutation can cause increased risk of cancer. Individuals with an inherited impairment in any of 34 DNA repair genes (see article DNA repair-deficiency disorder) have an increased risk of cancer, with some defects causing up to a 100% lifetime chance of cancer (e.g. p53 mutations). However, such germline mutations (which cause highly penetrant cancer syndromes) are the cause of only about 1 percent of cancers.\n\nDeficiencies in DNA repair enzymes are occasionally caused by a newly arising somatic mutation in a DNA repair gene, but are much more frequently caused by epigenetic alterations that reduce or silence expression of DNA repair genes. For example, when 113 colorectal cancers were examined in sequence, only four had a missense mutation in the DNA repair gene MGMT, while the majority had reduced MGMT expression due to methylation of the MGMT promoter region (an epigenetic alteration). Five different studies found that between 40% and 90% of colorectal cancers have reduced MGMT expression due to methylation of the MGMT promoter region.\n\nSimilarly, out of 119 cases of mismatch repair-deficient colorectal cancers that lacked DNA repair gene PMS2 expression, PMS2 was deficient in 6 due to mutations in the PMS2 gene, while in 103 cases PMS2 expression was deficient because its pairing partner MLH1 was repressed due to promoter methylation (PMS2 protein is unstable in the absence of MLH1). In the other 10 cases, loss of PMS2 expression was likely due to epigenetic overexpression of the microRNA, miR-155, which down-regulates MLH1.\n\nIn further examples (tabulated in Table 4 of this reference), epigenetic defects were found at frequencies of between 13%–100% for the DNA repair genes BRCA1, WRN, FANCB, FANCF, MGMT, MLH1, MSH2, MSH4, ERCC1, XPF, NEIL1 and ATM. These epigenetic defects occurred in various cancers (e.g. breast, ovarian, colorectal and head and neck). Two or three deficiencies in the expression of ERCC1, XPF or PMS2 occur simultaneously in the majority of the 49 colon cancers evaluated by Facista et al.\n\nThe chart in this section shows some frequent DNA damaging agents, examples of DNA lesions they cause, and the pathways that deal with these DNA damages. At least 169 enzymes are either directly employed in DNA repair or influence DNA repair processes. Of these, 83 are directly employed in repairing the 5 types of DNA damages illustrated in the chart.\n\nSome of the more well studied genes central to these repair processes are shown in the chart. The gene designations shown in red, gray or cyan indicate genes frequently epigenetically altered in various types of cancers. Wikipedia articles on each of the genes highlighted by red, gray or cyan describe the epigenetic alteration(s) and the cancer(s) in which these epimutations are found. Two review articles, and two broad experimental survey articles also document most of these epigenetic DNA repair deficiencies in cancers.\n\nRed-highlighted genes are frequently reduced or silenced by epigenetic mechanisms in various cancers. When these genes have low or absent expression, DNA damages can accumulate. Replication errors past these damages (see translesion synthesis) can lead to increased mutations and, ultimately, cancer. Epigenetic repression of DNA repair genes in accurate DNA repair pathways appear to be central to carcinogenesis.\n\nThe two gray-highlighted genes \"RAD51\" and \"BRCA2\", are required for homologous recombinational repair. They are sometimes epigenetically over-expressed and sometimes under-expressed in certain cancers. As indicated in the Wikipedia articles on RAD51 and BRCA2, such cancers ordinarily have epigenetic deficiencies in other DNA repair genes. These repair deficiencies would likely cause increased unrepaired DNA damages. The over-expression of \"RAD51\" and \"BRCA2\" seen in these cancers may reflect selective pressures for compensatory \"RAD51\" or \"BRCA2\" over-expression and increased homologous recombinational repair to at least partially deal with such excess DNA damages. In those cases where \"RAD51\" or \"BRCA2\" are under-expressed, this would itself lead to increased unrepaired DNA damages. Replication errors past these damages (see translesion synthesis) could cause increased mutations and cancer, so that under-expression of \"RAD51\" or \"BRCA2\" would be carcinogenic in itself.\n\nCyan-highlighted genes are in the microhomology-mediated end joining (MMEJ) pathway and are up-regulated in cancer. MMEJ is an additional error-prone inaccurate repair pathway for double-strand breaks. In MMEJ repair of a double-strand break, an homology of 5–25 complementary base pairs between both paired strands is sufficient to align the strands, but mismatched ends (flaps) are usually present. MMEJ removes the extra nucleotides (flaps) where strands are joined, and then ligates the strands to create an intact DNA double helix. MMEJ almost always involves at least a small deletion, so that it is a mutagenic pathway. FEN1, the flap endonuclease in MMEJ, is epigenetically increased by promoter hypomethylation and is over-expressed in the majority of cancers of the breast, prostate, stomach, neuroblastomas, pancreas, and lung. PARP1 is also over-expressed when its promoter region ETS site is epigenetically hypomethylated, and this contributes to progression to endometrial cancer, BRCA-mutated ovarian cancer, and BRCA-mutated serous ovarian cancer. Other genes in the MMEJ pathway are also over-expressed in a number of cancers (see MMEJ for summary), and are also shown in cyan.\n\nDifferential activity of DNA repair pathways across various regions of the human genome causes mutations to be very unevenly distributed within tumor genomes. In particular, the gene-rich, early-replicating regions of the human genome exhibit lower mutation frequencies than the gene-poor, late-replicating heterochromatin. One mechanism underlying this involves the histone modification H3K36me3, which can recruit mismatch repair proteins, thereby lowering mutation rates in H3K36me3-marked regions. Another important mechanism concerns nucleotide excision repair, which can be recruited by the transcription machinery, lowering somatic mutation rates in active genes and other open chromatin regions.\n\nThe basic processes of DNA repair are highly conserved among both prokaryotes and eukaryotes and even among bacteriophages (viruses which infect bacteria); however, more complex organisms with more complex genomes have correspondingly more complex repair mechanisms. The ability of a large number of protein structural motifs to catalyze relevant chemical reactions has played a significant role in the elaboration of repair mechanisms during evolution. For an extremely detailed review of hypotheses relating to the evolution of DNA repair, see.\n\nThe fossil record indicates that single-cell life began to proliferate on the planet at some point during the Precambrian period, although exactly when recognizably modern life first emerged is unclear. Nucleic acids became the sole and universal means of encoding genetic information, requiring DNA repair mechanisms that in their basic form have been inherited by all extant life forms from their common ancestor. The emergence of Earth's oxygen-rich atmosphere (known as the \"oxygen catastrophe\") due to photosynthetic organisms, as well as the presence of potentially damaging free radicals in the cell due to oxidative phosphorylation, necessitated the evolution of DNA repair mechanisms that act specifically to counter the types of damage induced by oxidative stress.\n\nOn some occasions, DNA damage is not repaired, or is repaired by an error-prone mechanism that results in a change from the original sequence. When this occurs, mutations may propagate into the genomes of the cell's progeny. Should such an event occur in a germ line cell that will eventually produce a gamete, the mutation has the potential to be passed on to the organism's offspring. The rate of evolution in a particular species (or, in a particular gene) is a function of the rate of mutation. As a consequence, the rate and accuracy of DNA repair mechanisms have an influence over the process of evolutionary change. DNA damage protection and repair does not influence the rate of adaptation by gene regulation and by recombination and selection of alleles. On the other hand, DNA damage repair and protection does influence the rate of accumulation of irreparable, advantageous, code expanding, inheritable mutations, and slows down the evolutionary mechanism for expansion of the genome of organisms with new functionalities. The tension between evolvability and mutation repair and protection needs further investigation.\n\nA technology named clustered regularly interspaced short palindromic repeat shortened to CRISPR-Cas9 was discovered in 2012. The new technology allows anyone with molecular biology training to alter the genes of any species with precision.<br>\nIt is a cheaper, more efficient and precise than other technologies. With the help of CRISPR–Cas9,parts of a genome can be edited by scientists by removing or adding or altering parts in a DNA sequence.\n\n", "id": "854294", "title": "DNA repair"}
{"url": "https://en.wikipedia.org/wiki?curid=1803894", "text": "Coagulative necrosis\n\nCoagulative necrosis is a type of accidental cell death typically caused by ischemia or infarction. In coagulative necrosis the architecture of dead tissue is preserved for at least a couple of days. It is believed that the injury denatures structural proteins as well as lysosomal enzymes thus blocking the proteolysis of the damaged cells. The lack of lysosomal enzymes allows it to maintain a \"coagulated\" morphology for some time. Like most types of necrosis if enough viable cells are present around the affected area regeneration will usually occur.\n\nCoagulative necrosis can also be induced by high local temperature; it is a desired effect of treatments such as high intensity focused ultrasound applied to cancerous cells.\n\nCoagulative necrosis is most commonly caused by conditions that do not involve severe trauma, toxins or an acute or chronic immune response. The lack of oxygen (hypoxia) causes cell death in a localised area which is perfused by blood vessels failing to deliver primarily oxygen, but also other important nutrients. It is important to note that while ischemia in most tissues of the body will cause coagulative necrosis, in the central nervous system ischemia causes liquefactive necrosis, as there is very little structural framework in neural tissue.\n\nThe macroscopic appearance of an area of coagulative necrosis is a pale segment of tissue contrasting against surrounding well vascularised tissue and is dry on cut surface. The tissue may later turn red due to inflammatory response. The surrounding surviving cells can aid in regeneration of the affected tissue unless they are stable or permanent.\n\nThe microscopic anatomy shows a lighter staining tissue (when stained with H&E) containing no nuclei with very little structural damage, giving the appearance often quoted as 'ghost cells'. The decreased staining is due to digested nuclei which no longer show up as dark purple when stained with hematoxylin, and removed cytoplasmic structures giving reduced amounts of intracellular protein, reducing the usual dark pink staining cytoplasm with eosin.\n\nAs the majority of the structural remnants of the necrotic tissue remains, labile cells adjacent to the affected tissue will replicate and replace the cells which have been killed during the event. Labile cells are constantly undergoing mitosis and can therefore help reform the tissue, whereas nearby stable and permanent cells (e.g. neurons and cardiomyocytes) do not undergo mitosis and will not replace the tissue affected. Fibroblasts will also migrate to the affected area depositing fibrous tissue producing fibrosis or scarring in areas where viable cells do not replicate and replace tissue.\n\n", "id": "1803894", "title": "Coagulative necrosis"}
{"url": "https://en.wikipedia.org/wiki?curid=564779", "text": "Cell growth\n\nThe term cell growth is used in the contexts of biological cell development and cell division (reproduction). When used in the context of cell development, the term refers to increase in cytoplasmic and organelle volume (G1 phase), as well as increase in genetic material (G2 phase) following the replication during S phase.\nThis is not to be confused with growth in the context of cell division, referred to as proliferation, where a cell, known as the \"mother cell\", grows and divides to produce two \"daughter cells\" (M phase). \nCell populations go through a particular type of exponential growth called doubling. Thus, each generation of cells should be twice as numerous as the previous generation. However, the number of generations only gives a maximum figure as not all cells survive in each generation.\n\nCell size is highly variable among organisms, with some algae such as \"Caulerpa taxifolia\" being a single cell several meters in length. Plant cells are much larger than animal cells, and protists such as \"Paramecium\" can be 330 μm long, while a typical human cell might be 10 μm. How these cells \"decide\" how big they should be before dividing is an open question. Chemical gradients are known to be partly responsible, and it is hypothesized that mechanical stress detection by cytoskeletal structures is involved. Work on the topic generally requires an organism whose cell cycle is well-characterized.\n\nThe relationship between cell size and cell division has been extensively studied in yeast. For some cells, there is a mechanism by which cell division is not initiated until a cell has reached a certain size. If the nutrient supply is restricted (after time t = 2 in the diagram, below), and the rate of increase in cell size is slowed, the time period between cell divisions is increased. Yeast cell-size mutants were isolated that begin cell division before reaching a normal/regular size (\"wee\" mutants).\nWee1 protein is a tyrosine kinase that normally phosphorylates the Cdc2 cell cycle regulatory protein (the homolog of CDK1 in humans), a cyclin-dependent kinase, on a tyrosine residue. Cdc2 drives entry into mitosis by phosphorylating a wide range of targets. This covalent modification of the molecular structure of Cdc2 inhibits the enzymatic activity of Cdc2 and prevents cell division. Wee1 acts to keep Cdc2 inactive during early G2 when cells are still small. When cells have reached sufficient size during G2, the phosphatase Cdc25 removes the inhibitory phosphorylation, and thus activates Cdc2 to allow mitotic entry. A balance of Wee1 and Cdc25 activity with changes in cell size is coordinated by the mitotic entry control system. It has been shown in Wee1 mutants, cells with weakened Wee1 activity, that Cdc2 becomes active when the cell is smaller. Thus, mitosis occurs before the yeast reach their normal size. This suggests that cell division may be regulated in part by dilution of Wee1 protein in cells as they grow larger.\n\nThe protein kinase Cdr2 (which negatively regulates Wee1) and the Cdr2-related kinase Cdr1 (which directly phosphorylates and inhibits Wee1 \"in vitro\") are localized to a band of cortical nodes in the middle of interphase cells. After entry into mitosis, cytokinesis factors such as myosin II are recruited to similar nodes; these nodes eventually condense to form the cytokinetic ring. A previously uncharacterized protein, Blt1, was found to colocalize with Cdr2 in the medial interphase nodes. Blt1 knockout cells had increased length at division, which is consistent with a delay in mitotic entry. This finding connects a physical location, a band of cortical nodes, with factors that have been shown to directly regulate mitotic entry, namely Cdr1, Cdr2, and Blt1.\n\nFurther experimentation with GFP-tagged proteins and mutant proteins indicates that the medial cortical nodes are formed by the ordered, Cdr2-dependent assembly of multiple interacting proteins during interphase. Cdr2 is at the top of this hierarchy and works upstream of Cdr1 and Blt1. Mitosis is promoted by the negative regulation of Wee1 by Cdr2. It has also been shown that Cdr2 recruits Wee1 to the medial cortical node. The mechanism of this recruitment has yet to be discovered. A Cdr2 kinase mutant, which is able to localize properly despite a loss of function in phosphorylation, disrupts the recruitment of Wee1 to the medial cortex and delays entry into mitosis. Thus, Wee1 localizes with its inhibitory network, which demonstrates that mitosis is controlled through Cdr2-dependent negative regulation of Wee1 at the medial cortical nodes.\n\nCell polarity factors positioned at the cell tips provide spatial cues to limit Cdr2 distribution to the cell middle. In fission yeast \"Schizosaccharomyces pombe\" (\"S. Pombe\"), cells divide at a defined, reproducible size during mitosis because of the regulated activity of Cdk1. The cell polarity protein kinase Pom1, a member of the dual-specificity tyrosine-phosphorylation regulated kinase (DYRK) family of kinases, localizes to cell ends. In Pom1 knockout cells, Cdr2 was no longer restricted to the cell middle, but was seen diffusely through half of the cell. From this data it becomes apparent that Pom1 provides inhibitory signals that confine Cdr2 to the middle of the cell. It has been further shown that Pom1-dependent signals lead to the phosphorylation of Cdr2. Pom1 knockout cells were also shown to divide at a smaller size than wild-type, which indicates a premature entry into mitosis.\n\nPom1 forms polar gradients that peak at cell ends, which shows a direct link between size control factors and a specific physical location in the cell. As a cell grows in size, a gradient in Pom1 grows. When cells are small, Pom1 is spread diffusely throughout the cell body. As the cell increases in size, Pom1 concentration decreases in the middle and becomes concentrated at cell ends. Small cells in early G2 which contain sufficient levels of Pom1 in the entirety of the cell have inactive Cdr2 and cannot enter mitosis. It is not until the cells grow into late G2, when Pom1 is confined to the cell ends that Cdr2 in the medial cortical nodes is activated and able to start the inhibition of Wee1. This finding shows how cell size plays a direct role in regulating the start of mitosis. In this model, Pom1 acts as a molecular link between cell growth and mitotic entry through a Cdr2-Cdr1-Wee1-Cdk1 pathway. The Pom1 polar gradient successfully relays information about cell size and geometry to the Cdk1 regulatory system. Through this gradient, the cell ensures it has reached a defined, sufficient size to enter mitosis.\n\nMany different types of eukaryotic cells undergo size-dependent transitions during the cell cycle. These transitions are controlled by the cyclin-dependent kinase Cdk1. Though the proteins that control Cdk1 are well understood, their connection to mechanisms monitoring cell size remains elusive.\nA postulated model for mammalian size control situates mass as the driving force of the cell cycle. A cell is unable to grow to an abnormally large size because at a certain cell size or cell mass, the S phase is initiated. The S phase starts the sequence of events leading to mitosis and cytokinesis. A cell is unable to get too small because the later cell cycle events, such as S, G2, and M, are delayed until mass increases sufficiently to begin S phase.\n\nMany of the signal molecules that convey information to cells during the control of cellular differentiation or growth are called growth factors. The protein mTOR is a serine/threonine kinase that regulates\ntranslation and cell division. Nutrient availability influences mTOR\nso that when cells are not able to grow to normal size they will not\nundergo cell division.\nThe details of the molecular mechanisms of mammalian cell size control\nare currently being investigated. The size of post-mitotic neurons\ndepends on the size of the cell body, axon and dendrites. In\nvertebrates, neuron size is often a reflection of the number of\nsynaptic contacts onto the neuron or from a neuron onto other cells.\nFor example, the size of motoneurons usually reflects the size of\nthe motor unit that is controlled by the motoneuron.\nInvertebrates often have giant neurons and axons that provide\nspecial functions such as rapid action potential propagation.\nMammals also use this trick for increasing the speed of signals in the\nnervous system, but they can also use myelin to accomplish this, so\nmost human neurons are relatively small cells.\n\nOne common means to produce very large cells is by cell fusion to form syncytia. For example, very long (several inches) skeletal muscle cells are formed by fusion of thousands of myocytes. Genetic studies of the fruit fly \"Drosophila\" have revealed several genes that are required for the formation of multinucleated muscle cells by fusion of myoblasts. Some of the key proteins are important for cell adhesion between myocytes and some are involved in adhesion-dependent cell-to-cell signal transduction that allows for a cascade of cell fusion events.\n\nOocytes can be unusually large cells in species for which embryonic development takes place away from the mother's body. Their large size can be achieved either by pumping in cytosolic components from adjacent cells through cytoplasmic bridges (\"Drosophila\") or by internalization of nutrient storage granules (yolk granules) by endocytosis (frogs).\n\nIncreases in the size of plant cells are complicated by the fact that almost all plant cells are inside of a solid cell wall. Under the influence of certain plant hormones the cell wall can be remodeled, allowing for increases in cell size that are important for the growth of some plant tissues.\n\nMost unicellular organisms are microscopic in size, but there are some giant bacteria and protozoa that are visible to the naked eye. See: Table of cell sizes —Dense populations of a giant sulfur bacterium in Namibian shelf sediments— Large protists of the genus \"Chaos\", closely related to the genus \"Amoeba\"\n\nIn the rod-shaped bacteria \"E. coli\", \"Caulobacter crescentus\" and \"B. subtilis\" cell size is controlled by a simple mechanisms in which cell division occurs after a constant volume has been added since the previous division. By always growing by the same amount, cells born smaller or larger than average naturally converge to an average size equivalent to the amount added during each generation.\n\nThe process of cell division, called cell cycle, has four major parts called phases. The first part, called G phase is marked by synthesis of various enzymes that are required for DNA replication.\nThe second part of the cell cycle is tCell reproduction is asexual. For most of the constituents of the cell, growth is a steady, continuous process, interrupted only briefly at M phase when the nucleus and then the cell divide in two.\nhe S phase, where DNA replication produces two identical sets of chromosomes. The third part is the G phase in which a significant protein synthesis occurs, mainly involving the production of microtubules that are required during the process of division, called mitosis.\nThe fourth phase, M phase, consists of nuclear division (karyokinesis) and cytoplasmic division (cytokinesis), accompanied by the formation of a new cell membrane. This is the physical division of \"mother\" and \"daughter\" cells. The M phase has been broken down into several distinct phases, sequentially known as prophase, prometaphase, metaphase, anaphase and telophase leading to cytokinesis.\n\nCell division is more complex in eukaryotes than in other organisms. Prokaryotic cells such as bacterial cells reproduce by binary fission, a process that includes DNA replication, chromosome segregation, and cytokinesis. Eukaryotic cell division either involves mitosis or a more complex process called meiosis. Mitosis and meiosis are sometimes called the two \"nuclear division\" processes. Binary fission is similar to eukaryote cell reproduction that involves mitosis. Both lead to the production of two daughter cells with the same number of chromosomes as the parental cell. Meiosis is used for a special cell reproduction process of diploid organisms. It produces four special daughter cells (gametes) which have half the normal cellular amount of DNA. A male and a female gamete can then combine to produce a zygote, a cell which again has the normal amount of chromosomes.\n\nThe rest of this article is a comparison of the main features of the three types of cell reproduction that either involve binary fission, mitosis, or meiosis. The diagram below depicts the similarities and differences of these three types of cell reproduction.\n\nThe DNA content of a cell is duplicated at the start of the cell reproduction process. Prior to DNA replication, the DNA content of a cell can be represented as the amount Z (the cell has Z chromosomes). After the DNA replication process, the amount of DNA in the cell is 2Z (multiplication: 2 x Z = 2Z). During Binary fission and mitosis the duplicated DNA content of the reproducing parental cell is separated into two equal halves that are destined to end up in the two daughter cells. The final part of the cell reproduction process is cell division, when daughter cells physically split apart from a parental cell. During meiosis, there are two cell division steps that together produce the four daughter cells.\n\nAfter the completion of binary fission or cell reproduction involving mitosis, each daughter cell has the same amount of DNA (Z) as what the parental cell had before it replicated its DNA. These two types of cell reproduction produced two daughter cells that have the same number of chromosomes as the parental cell. Chromosomes duplicate prior to cell division when forming new skin cells for reproduction. After meiotic cell reproduction the four daughter cells have half the number of chromosomes that the parental cell originally had. This is the haploid amount of DNA, often symbolized as N. Meiosis is used by diploid organisms to produce haploid gametes. In a diploid organism such as the human organism, most cells of the body have the diploid amount of DNA, 2N. Using this notation for counting chromosomes we say that human somatic cells have 46 chromosomes (2N = 46) while human sperm and eggs have 23 chromosomes (N = 23). Humans have 23 distinct types of chromosomes, the 22 autosomes and the special category of sex chromosomes. There are two distinct sex chromosomes, the X chromosome and the Y chromosome. A diploid human cell has 23 chromosomes from that person's father and 23 from the mother. That is, your body has two copies of human chromosome number 2, one from each of your parents.\n\nImmediately after DNA replication a human cell will have 46 \"double chromosomes\". In each double chromosome there are two copies of that chromosome's DNA molecule. During mitosis the double chromosomes are split to produce 92 \"single chromosomes\", half of which go into each daughter cell. During meiosis, there are two chromosome separation steps which assure that each of the four daughter cells gets one copy of each of the 23 types of chromosome.\n\nThough cell reproduction that uses mitosis can reproduce eukaryotic cells, eukaryotes bother with the more complicated process of meiosis because sexual reproduction such as meiosis confers a selective advantage. Notice that when meiosis starts, the two copies of sister chromatids number 2 are adjacent to each other. During this time, there can be genetic recombination events. Parts of the chromosome 2 DNA gained from one parent (red) will swap over to the chromosome 2 DNA molecule that received from the other parent (green). Notice that in mitosis the two copies of chromosome number 2 do not interact. It is these new combinations of parts of chromosomes that provide the major advantage for sexually reproducing organisms by allowing for new combinations of genes and more efficient evolution.\nHowever, in organisms with more than one set of chromosomes at the main life cycle stage, sex may also provide an advantage because, under random mating, it produces homozygotes and heterozygotes according to the Hardy-Weinberg ratio.\n\nA series of growth disorders can occur at the cellular level and these consequently underpin much of the subsequent course in cancer, in which a group of cells display uncontrolled growth and division beyond the normal limits, \"invasion\" (intrusion on and destruction of adjacent tissues), and sometimes \"metastasis\" (spread to other locations in the body via lymph or blood).\n\nThe cell growth can be detected by a variety of methods.\nThe cell size growth can be visualized by microscopy, using suitable stains. But the increase of cells number is usually more significant. It can be measured by manual counting of cells under microscopy observation, using the dye exclusion method (i.e. trypan blue) to count only viable cells. Less fastidious, scalable, methods include the use of cytometers, while flow cytometry allows combining cell counts ('events') with other specific parameters: fluorescent probes for membranes, cytoplasm or nuclei allow distinguishing dead/viable cells, cell types, cell differentiation, expression of a biomarker such as Ki67.\n\nBeside the increasing number of cells, one can be assessed regarding the metabolic activity growth, that is, the CFDA and calcein-AM measure (fluorimetrically) not only the membrane functionality (dye retention), but also the functionality of cytoplasmic enzymes (esterases). The MTT assays (colorimetric) and the resazurin assay (fluorimetric) dose the mitochondrial redox potential.\n\nAll these assays may correlate well, or not, depending on cell growth conditions and desired aspects (activity, proliferation). The task is even more complicated with populations of different cells, furthermore when combining cell growth interferences or toxicity.\n\n\n", "id": "564779", "title": "Cell growth"}
{"url": "https://en.wikipedia.org/wiki?curid=21992863", "text": "Fungating lesion\n\nA fungating lesion is a skin lesion that fungates, that is, becomes like a fungus in its appearance or growth rate. It is marked by ulcerations (breaks on the skin or surface of an organ) and necrosis (death of living tissue) and usually presents a foul odor. This kind of lesion may occur in many types of cancer, including breast cancer, melanoma, and squamous cell carcinoma, and especially in advanced disease. The characteristic malodorous smell is caused by dimethyl trisulfide. It is usually not literally a fungal infection but rather a neoplastic growth with necrosing portions.\n\n", "id": "21992863", "title": "Fungating lesion"}
{"url": "https://en.wikipedia.org/wiki?curid=5406477", "text": "Interkinesis\n\nInterkinesis or interphase II is a period of rest that cells of some species enter during meiosis, between meiosis I and meiosis II. No DNA replication occurs during interkinesis, however it does occur during the interphase I stage of meiosis (See meiosis I). During interkinesis the single spindle of the first meiotic division disassembles and the microtubules reassemble into two new spindles for the second meiotic division. Interkinesis follows telophase I and is where Many plants skip telophase I and interkinesis, going immediately into prophase II. Each chromosome still consists of two chromatids.\n", "id": "5406477", "title": "Interkinesis"}
{"url": "https://en.wikipedia.org/wiki?curid=29328565", "text": "Leptotene stage\n\nThe leptotene stage, also known as the leptonema, is the first of five substages of prophase I in meiosis. The term \"leptonema\" derives from Greek words meaning \"thin threads\". A cell destined to become a gamete enters the leptotene stage after its chromosomes are duplicated during interphase. During the leptotene stage those duplicated chromosomes—each consisting of two sister chromatids—condense from diffuse chromatin into long, thin strands that are more visible within the nucleoplasm. The next stage of prophase I in meiosis is the zygotene stage.\n\nDuring this stage, the chromosomes attach themselves by their ends (telomeres) to the inner membrane of the nuclear envelope. At the transition to the zygotene stage the telomeres usually aggregate at a nuclear envelope sector, thereby forming a meiotic bouquet. Lateral (axial) elements of the synaptonemal complex are also formed. It is the first stage of Prophase 1 in Meiosis 1. \n", "id": "29328565", "title": "Leptotene stage"}
{"url": "https://en.wikipedia.org/wiki?curid=3650941", "text": "S phase index\n\nS-phase index (SPI), is a measure of cell growth and viability, especially the capacity of tumor cells to proliferate. It is defined as the number of BrdU-incorporating cells relative to the volume of DNA staining determined from whole mount confocal analyses.\n\nOnly cells in the S phase will incorporate BrdU into their DNA structure, which assists in determining length of the cell cycle.\n\n", "id": "3650941", "title": "S phase index"}
{"url": "https://en.wikipedia.org/wiki?curid=3190871", "text": "Intraflagellar transport\n\nIntraflagellar transport or IFT is a bidirectional motility along axonemal microtubules that is essential for the formation (ciliogenesis) and maintenance of most eukaryotic cilia and flagella. It is thought to be required to build all cilia that assemble within a membrane projection from the cell surface. \"Plasmodium falciparum\" cilia and the sperm flagella of Drosophila are examples of cilia that assemble in the cytoplasm and do not require IFT. The process of IFT involves movement of large protein complexes called IFT particles or trains from the cell body to the ciliary tip and followed by their return to the cell body. The outward or anterograde movement is powered by kinesin-2 while the inward or retrograde movement is powered by cytoplasmic dynein 2/1b. The IFT particles are composed of about 20 proteins organized in two subcomplexes called complex A and B.\n\nIFT was first reported in 1993 by graduate student Keith Kozminski while working in the lab of Dr. Joel Rosenbaum at Yale University. The process of IFT has been best characterized in the biflagellate alga \"Chlamydomonas reinhardtii\" as well as the sensory cilia of the nematode \"Caenorhabditis elegans\".\n\nIt has been suggested based on localization studies that IFT proteins also function outside of cilia.\n\nIFT describes the bi-directional movement of non-membrane-bound particles along the doublet microtubules of the flagellar axoneme, between the axoneme and the plasma membrane. Studies have shown that the movement of IFT particles along the microtubule is carried out by two different microtubule-based motors; the anterograde (towards the flagellar tip) motor is heterotrimeric kinesin-2, and the retrograde (towards the cell body) motor is cytoplasmic dynein 1b. IFT particles carry axonemal subunits to the site of assembly at the tip of the axoneme; thus, IFT is necessary for axonemal growth. Therefore, since the axoneme needs a continually fresh supply of proteins, an axoneme with defective IFT machinery will slowly shrink in the absence of replacement protein subunits. In healthy flagella, IFT particles reverse direction at the tip of the axoneme, and are thought to carry used proteins, or \"turnover products,\" back to the base of the flagellum.\n\nThe IFT particles themselves consist of two sub-complexes, each made up of several individual IFT proteins. The two complexes, known as 'A' and 'B,' are separable via sucrose centrifugation (both complexes at approximately 16S, but under increased ionic strength complex B sediments more slowly, thus segregating the two complexes). The many subunits of the IFT complexes have been named according to their molecular weights:\n\n\nThe biochemical properties and biological functions of these IFT subunits are just beginning to be elucidated, for example they interact with components of the basal body like CEP170 or proteins which are required for cilium formation like tubulin chaperone and membrane proteins.\n\nDue to the importance of IFT in maintaining functional cilia, defective IFT machinery has now been implicated in many disease phenotypes generally associated with non-functional (or absent) cilia. IFT88, for example, encodes a protein also known as Tg737 or Polaris in mouse and human, and the loss of this protein has been found to cause an autosomal-recessive polycystic kidney disease model phenotype in mice. Other human diseases such as retinal degeneration, situs inversus (a reversal of the body's left-right axis), Senior-Loken syndrome, liver disease, primary ciliary dyskinesia, nephronophthisis, Alstrom syndrome, Meckel-Gruber syndrome, Sensenbrenner syndrome, Jeune syndrome, and Bardet-Biedl syndrome, which causes both cystic kidneys and retinal degeneration, have been linked to the IFT machinery. This diverse group of genetic syndromes and genetic diseases are now understood to arise due to malfunctioning cilia, and the term \"ciliopathy\" is now used to indicate their common origin.\nThese and possibly many more disorders may be better understood via study of IFT.\n\nOne of the most recent discoveries regarding IFT is its potential role in signal transduction. IFT has been shown to be necessary for the movement of other signaling proteins within the cilia, and therefore may play a role in many different signaling pathways. Specifically, IFT has been implicated as a mediator of Sonic Hedgehog signaling, one of the most important pathways in embryogenesis.\n\n\n", "id": "3190871", "title": "Intraflagellar transport"}
{"url": "https://en.wikipedia.org/wiki?curid=11035490", "text": "Nemosis\n\nNemosis is a process of cell activation and death in human fibroblasts.\n\nInitially discovered as programmed necrosis, the name nemosis, is a derivative from the Goddess Nemesis in Greek mythodology. This name was adopted for fibroblast activation based on its initiation by direct cell–cell interactions as opposed to preference for extracellular matrix, ECM, contacts. Contacts between normal diploid fibroblasts induce cell activation leading to programmed cell death, PCD. This type of PCD has features of necrosis rather than apoptosis.\n\nNemosis of fibroblasts, or mesenchymal cells in general, generates large amounts of mediators of inflammation, such as prostaglandins, as well as growth factors such as hepatocyte growth factor. It is thus indicated to contribute to processes like acute and chronic inflammation, and cancer. Factors secreted by nemotic fibroblasts also\nbreak down the ECM. Such factors include several matrix metalloproteinases, and plasminogen activation.\n\n", "id": "11035490", "title": "Nemosis"}
{"url": "https://en.wikipedia.org/wiki?curid=4750568", "text": "Necrobiosis\n\nNecrobiosis is the physiological death of a cell, and can be caused by conditions such as basophilia, erythema, or a tumor. It is identified both with and without necrosis.\n\nIt is associated with necrobiosis lipoidica and granuloma annulare.\n\nNecrobiosis differs from apoptosis, which kills a damaged cell to protect the body from harm. \n", "id": "4750568", "title": "Necrobiosis"}
{"url": "https://en.wikipedia.org/wiki?curid=25368725", "text": "Biochemical switches in the cell cycle\n\nA series of biochemical switches control transitions between and within the various phases of the cell cycle. The cell cycle is a series of complex, ordered, sequential events that control how a single cell divides into two cells, and involves several different phases. The phases include the G1 and G2 phases, DNA replication or S phase, and the actual process of cell division, mitosis or M phase. During the M phase, the chromosomes separate and cytokinesis occurs.\n\nThe switches maintain the orderly progression of the cell cycle and act as checkpoints to ensure that each phase has been properly completed before progression to the next phase. For example, Cdk, or cyclin dependent kinase, is a major control switch for the cell cycle and it allows the cell to move from G1 to S or G2 to M by adding phosphate to protein substrates. Such multi-component (involving multiple inter-linked proteins) switches have been shown to generate decisive, robust (and potentially irreversible) transitions and trigger stable oscillations. As a result, they are a subject of active research that tries to understand how such complex properties are wired into biological control systems.\n\nMany biological circuits produce complex outputs by exploiting one or more feedback loops. In a sequence of biochemical events, feedback would refer to a downstream element in the sequence (B in the image on the left) affecting some upstream component (A in the image on the left) to affect its own production or activation (output) in the future. If this element acts to enhance its own output, then it engages in positive feedback (blue arrow). A positive feedback loop is also known as a self-reinforcing loop, and it is possible that these loops can be part of a larger loop, as this is characteristic of regulatory circuits.\n\nConversely, if this element leads to its own inhibition through upstream elements, this is canonically negative feedback (red blunt arrow). A negative feedback loop is also known as a balancing loop, and it may be common to see oscillations in which a delayed negative feedback signal is used to maintain homeostatic balance in the system.\n\nFeedback loops can be used for amplification (positive) or self-correction (negative). The right combination of positive and negative feedback loops can generate ultrasensitivity and bistability, which in turn can generate decisive transitions and oscillations.\n\nPositive and negative feedback loops do not always operate distinctly. In the mechanism of biochemical switches, they work together to create a flexible system. For example, according to Pfeuty & Kaneko (2009), to overcome a drawback in biochemical systems, positive feedback regulation loops may interact with negative regulation loops to facilitate escape from stable states. The coexistence of two stable states is known as bistability, which is often the result of positive feedback regulations.\n\nAn example that reveals the interaction of the multiple negative and positive feedback loops is the activation of cyclin-dependent protein kinases, or Cdks14. Positive feedback loops play a role by switching cells from low to high Cdk-activity. The interaction between the two types of loops is evident in mitosis. While positive feedback initiates mitosis, a negative feedback loop promotes the inactivation of the cyclin-dependent kinases by the anaphase-promoting complex. This example clearly shows the combined effects that positive and negative feedback loops have on cell-cycle regulation.\n\nAn \"all-or-none\" response to a stimulus is termed ultrasensitivity. In other words, a very small change in stimulus causes a very large change in response, producing a sigmoidal dose-response curve. An ultrasensitive response is described by the general equation V = S/(S + K), known as the Hill equation, when n, the Hill coefficient, is more than 1. The steepness of the sigmoidal curve depends on the value of n. A value of n = 1 produces a hyperbolic or Michaelian response. Ultrasensitivity is achieved in a variety of systems; a notable example is the cooperative binding of the enzyme hemoglobin to its substrate. Since an ultrasensitive response is almost ‘digital’, it can be used to amplify a response to a stimulus or cause a decisive sharp transition (between ‘off’ and ‘on’ states).\n\nUltrasensitivity plays a large role in cell-cycle regulation. For example, Cdk1 and Wee1 are mitotic regulators, and they are able to inactivate each other through inhibitory phosphorylation. This represents a double negative feedback loop in which both regulators inactivate each other. According to Kim et al. (2007), there must be an ultrasensitive element to generate a bistable response. It turns out that Wee1 has an ultrasensitive response to Cdk1, and this likely arises because of substrate competition among the various phosphorylation sites on Wee1.\n\nBistability implies hysteresis, and hysteresis implies multistability. Multistability indicates the presence of two or more stable states for a given input. Therefore, bistability is the ability of a system to exist in two steady states. In other words, there is a range of stimulus values for which the response can have two steady-state values. Bistability is accompanied by hysteresis, which means that the system approaches one of the two steady states preferentially depending on its history. Bistability requires feedback as well as an ultrasensitive circuit element.\n\nUnder the proper circumstances, positive and negative feedback loops can provide the conditions for bistability; for example, by having positive feedback coupled to an ultrasensitive response element with the circuit. A hysteretic bistable system can act as a robust reversible switch because it is harder for the system to transition between ‘on’ and ‘off’ states (compared to the equivalent monostable ultransensitive response). The system could also be poised such that one of the transitions is physically unattainable; for example, no amount of reduction in the stimulus will return the system to the ‘off’-state once it is already in the ‘on’ state. This would form a robust irreversible switch.\n\nThere is no one-to-one correspondence between network topology, since many networks have a similar input and output relationship. A network topology does not imply input or output, and similarly input or output does not imply network topology. It is for this reason that parameterization is very important for circuit function. If the dynamics of the input are comparable or faster than the response of the system, the response may appear hysteretic.\n\nThree cell cycle switches are described below that achieve abrupt and/or irreversible transitions by exploiting some of the mechanisms described above.\n\nThe G1/S transition, more commonly known as the Start checkpoint in budding yeast (the restriction point in other organisms) regulates cell cycle commitment. At this checkpoint, cells either arrest before DNA replication (due to limiting nutrients or a pheromone signal), prolong G1 (size control), or begin replication and progress through the rest of the cell cycle. The G1/S regulatory network or regulon in budding yeast includes the G1 cyclins Cln1, Cln2 and Cln3, Cdc28 (Cdk1), the transcription factors SBF and MBF, and the transcriptional inhibitor Whi5. Cln3 interacts with Cdk1 to initiate the sequence of events by phosphorylating a large number of targets, including SBF, MBF and Whi5. Phosphorylation of Whi5 causes it to translocate out of the nucleus, preventing it from inhibiting SBF and MBF. Active SBF/MBF drive the G1/S transition by turning on the B-type cyclins and initiating DNA replication, bud formation and spindle body duplication. Moreover, SBF/MBF drives expression of Cln1 and Cln2, which can also interact with Cdk1 to promote phosphorylation of its targets.\n\nThis G1/S switch was initially thought to function as a linear sequence of events starting with Cln3 and ending in S phase. However, the observation that any one of the Clns was sufficient to activate the regulon indicated that Cln1 and Cln2 might be able to engage positive feedback to activate their own transcription. This would result in a continuously accelerating cycle that could act as an irreversible bistable trigger. Skotheim et al. used single-cell measurements in budding yeast to show that this positive feedback does indeed occur. A small amount of Cln3 induces Cln1/2 expression and then the feedback loop takes over, leading to rapid and abrupt exit of Whi5 from the nucleus and consequently coherent expression of G1/S regulon genes. In the absence of coherent gene expression, cells take longer to exit G1 and a significant fraction even arrest before S phase, highlighting the importance of positive feedback in sharpening the G1/S switch.\n\nThe G1/S cell cycle checkpoint controls the passage of eukaryotic cells from the first gap phase, G1, into the DNA synthesis phase, S. In this switch in mammalian cells, there are two cell cycle kinases that help to control the checkpoint: cell cycle kinases CDK4/6-cyclin D and CDK2-cyclin E. The transcription complex that includes Rb and E2F is important in controlling this checkpoint. In the first gap phase, the Rb-HDAC repressor complex binds to the E2F-DP1 transcription factors, therefore inhibiting the downstream transcription. The phosphorylation of Rb by CDK4/6 and CDK2 dissociates the Rb-repressor complex and serves as an on/off switch for the cell cycle. Once Rb is phosphorylated, the inhibition is released on the E2F transcriptional activity. This allows for the transcription of S phase genes encoding for proteins that amplify the G1 to S phase switch.\n\nMany different stimuli apply checkpoint controls including TGFb, DNA damage, contact inhibition, replicative senescence, and growth factor withdrawal. The first four act by inducing members of the INK4 or Kip/Cip families of cell cycle kinase inhibitors. TGFb inhibits the transcription of Cdc25A, a phosphatase that activates the cell cycle kinases, and growth factor withdrawal activates GSK3b, which phosphorylates cyclin D. This leads to its rapid ubiquitination.\n\nG2 is commenced by E2F-mediated transcription of cyclin A, which forms the cyclin A-Cdk2 complex. In order to proceed into mitosis, the cyclin B-Cdk1 complex (first discovered as MPF or M-phase promoting factor; Cdk1 is also known as Cdc2 in fission yeast and Cdc28 in budding yeast) is activated by Cdc25, a protein phosphatase. As mitosis starts, the nuclear envelope disintegrates, chromosomes condense and become visible, and the cell prepares for division. Cyclin B-Cdk1 activation results in nuclear envelope breakdown, which is a characteristic of the initiation of mitosis.\n\nThe cyclin B-Cdk1 complex participates in a regulatory circuit in which Cdk1 can phosphorylate and activate its activator, Cdc25 (positive feedback), and phosphorylate and inactivate its inactivator, the kinase Wee1 (double-negative feedback). This circuit could act as a bistable trigger with one stable steady state in G2 (Cdk1 and Cdc25 off, Wee1 on) and a second stable steady state in M phase (Cdk1 and Cdc25 active, Wee1 off). However, Wee1 is itself regulated by other factors, such as Cdr2.\n\nIt was suggested and defended by Jin et al. in their series of experiments with the human HeLa cell line in 1998 that it is the spatial location of cyclin B within the cell that initiates mitosis. Known from previous experiments in both human cells and starfish oocytes, Jin et al. summarize that cyclin B1 is abundant in the cytoplasm during non-dividing phases of mitosis, but is identified in the nucleus, in complex with Cdk1, immediately before the cell enters mitosis. Other experimenters showed that cells would not divide if cyclin B remains in the cytoplasm. In order to further investigate the effect of spatial location of cyclin B on cell division and cycle control, Jin et al. tagged cyclin B with a nuclear localization signal (NLS) that would keep the cyclin within the nucleus. Initially, this NLS cyclin B did not induce the expected effect of accelerated mitotic entry. This result is due to the inhibition detailed in the figure below. Wee1, an inhibitor on the cyclin B-Cdk1 complex, is localized in the nucleus, and likely phosphorylating the NLS cyclin B, rendering it unable to perform as predicted. This postulation was confirmed when Jin et al. employed Cdc2AF, an unphosphorylatable mutant of Cdk1, and saw accelerated entry to cell division due to the nuclear localization of the cyclin B. Therefore, nuclear localization of cyclin B is necessary but not sufficient to trigger cell division.\n\nIn investigation of cell cycle regulation, Jin et al. manipulated cells in order to evaluate the localization of cyclin B in cells with DNA damage. Through combination of DNA damage and nuclear localization of exogenous cyclin B, they were able to determine that cells would divide even with DNA damage if the cyclin B were forced to be expressed in the nucleus. This suggests that spatial localization of cyclin B may play a role as a checkpoint of mitosis. If the cells, under normal circumstances, don’t divide when their genetic information is damaged, but will enter mitosis if endogenous cyclin B is expressed in the nucleus, it is likely that the translocation of the cyclin B to the cytoplasm is a mechanism that prevents immature mitotic entry. This hypothesis was further supported by Jin et al.’s analysis of cells arrested in G2 due to DNA damage. In these cells, Jin et al. observed high levels of cyclin B-Cdc2 complex activity in the cytoplasm. This is supporting evidence for the previously mentioned theory because it shows that the Cdc2 can activate the cyclin without immediate translocation to the nucleus. Additionally, the accumulation of cyclin B-Cdk1 complexes in the cytoplasm of cells that are not dividing due to DNA damage supports the theory that it is nuclear localization of cyclin B that initiates mitotic entry.\n\nTo conclude, spatial localization of cyclin B plays a role in mitotic entry. Translocation of cyclin B from the cytoplasm to the nucleus is necessary for cell division, but not sufficient, as its inhibitors do not allow the cell to enter mitosis prematurely. In addition to the back up inhibition of the cyclin B-Cdk1 complex, premature cellular division is prevented by the translocation of the cyclin B itself. The cyclin B-Cdk1 complex will remain in the cytoplasm in cells with DNA damage, rather than translocate to the nucleus, keeping the cell inhibiting the cell from entering mitosis. The next question addressed by researchers in this field is by which specific mechanism is this translocation regulated.\n\nSantos et al. hypothesized that the translocation of cyclin B is regulated by a mechanism of positive feedback, similar to that which regulates the activation of the cyclin B-Cdk1 complex. They believed that the positive feedback loop involves the phosphorylation of the cyclin B and its translocation to the nucleus. To begin to investigate this, they first reconfirmed some of the results of the Jin et al. experiments, utilizing immunofluorescence to show cyclin B in the cytoplasm prior to division, and translocation to the nucleus to initiate mitosis, which they operationalized by comparing relative to nuclear envelope breakdown (NEB). Using nuclear cyclin that cannot be inactivated by Wee1 or Myt1, Santos et al. observed that active nuclear cyclin recruits more cyclin from the cytoplasm to be translocated to the nucleus. They confirmed this observation by employing a rapamycin treatment, iRap. iRap induces translocation of tagged cyclin B from the cytoplasm to the nucleus. Remarkably, Santos et al. saw that untagged cyclin B migrated with the cyclin B influenced by iRap. The untagged cyclin is insentive to the treatment, and moves independently from the treated cyclin. This supports the first part of the positive feedback loop, that nuclear localization of cyclin B, which leads to mitotic entry, promotes increased translocation of cytoplasmic cyclin B to the nucleus, further promoting the remaining cytoplasmic cyclin B to migrate to the nucleus, etc.\n\nSantos et al. further hypothesize that phosphorylation of the cyclin B is another component of the positive feedback loop. They observed that the cyclin B naturally enters the nucleus before NEB. In contrast, mutated, unphosphorylatable cyclin B enters the nucleus during NEB. This is unexpected because it is characteristic of the cell cycle for the cyclin to translocate to the nucleus prior to NEB in order to induce cell cycle progression into mitotic division. Therefore, Santos et al. conclude that the phosphorylation of the cyclin B promotes the translocation to the nucleus. However, in addition, translocation to the nucleus promotes phosphorylation of the cyclin. It is noted by the authors that phosphorylation of cyclin B is nineteen times more favorable in the nucleus than in the cytoplasm, due to the smaller overall volume of the nucleus, allowing a faster phosphorylation rate. The increased translocation due to phosphorylation and increased phosphorylation due to translocation exemplify the positive feedback loop that resembles that previously discovered, which activates the cyclin B-Cdk1 complex.\n\nIn conclusion, nuclear localization of cyclin B is necessary for cellular entry into mitosis. The translocation of the cyclin from the cytoplasm to the nucleus, which allows for cellular division, is regulated by a positive feedback loop. Active cyclin B translocates to the nucleus and promotes activation and translocation of additional units of cyclin residing in the nucleus. This phenomenon is enhanced when considering phosphorylation. Phosphorylation of cyclin B promotes translocation to the nucleus, and cyclin B in the nucleus is much more likely to be phosphorylated, so nuclear localization promotes cyclin B phosphorylation in return.\n\nOnce cells are in mitosis, cyclin B-Cdk1 activates the anaphase-promoting complex (APC), which in turn inactivates cyclin B-Cdk1 by degrading cyclin B, eventually leading to exit from mitosis. Coupling the bistable Cdk1 response function to the negative feedback from the APC could generate what is known as a relaxation oscillator, with sharp spikes of Cdk1 activity triggering robust mitotic cycles. However, in a relaxation oscillator, the control parameter moves slowly relative to the system’s response dynamics which may be an accurate representation of mitotic entry, but not necessarily mitotic exit.\n\nIt is necessary to inactivate the cyclin B-Cdk1 complex in order to exit the mitotic stage of the cell cycle. The cells can then return to the first gap phase G1 and wait until the cycle proceeds yet again.\n\nIn 2003 Pomerening et al. provided strong evidence for this hypothesis by demonstrating hysteresis and bistability in the activation of Cdk1 in the cytoplasmic extracts of Xenopus oocytes. They first demonstrated a discontinuous sharp response of Cdk1 to changing concentrations of non-destructible Cyclin B (to decouple the Cdk1 response network from APC-mediated negative feedback). However, such a response would be consistent with both a monostable, ultrasensitive transition and a bistable transition. To distinguish between these two possibilities, they measured the steady-state levels of active Cdk1 in response to changing cyclin levels, but in two separate experiments, one starting with an interphase extract and one starting with an extract already in mitosis. At intermediate concentrations of cyclin they found two steady-state concentrations of active Cdk1. Which of the two steady states was occupied depended on the history of the system, i.e.whether they started with interphase or mitotic extract, effectively demonstrating hysteresis and bistability.\n\nIn the same year, Sha et al. independently reached the same conclusion revealing the hysteretic loop also using Xenopus laevis egg extracts. In this article, three predictions of the Novak-Tyson model were tested in an effort to conclude that hysteresis is the driving force for \"cell-cycle transitions into and out of mitosis\". The predictions of the Novak-Tyson model are generic to all saddle-node bifurcations. Saddle-node bifurcations are extremely useful bifurcations in an imperfect world because they help describe biological systems which are not perfect. The first prediction was that the threshold concentration of cyclin to enter mitosis is higher than the threshold concentration of cyclin to exit mitosis, and this was confirmed by supplementing cycling egg extracts with non-degradable cyclin B and measuring the activation and inactivation threshold after the addition of cycloheximide (CHX), which is a protein synthesis inhibitor. Furthermore, the second prediction of the Novak-Tyson model was also validated: unreplicated deoxyribonucleic acid, or DNA, increases the threshold concentration of cyclin that is required to enter mitosis. In order to arrive at this conclusion, cytostatic factor released extracts were supplemented with CHX, APH (a DNA polymerase inhibitor), or both, and non-degradable cyclin B was added. The third and last prediction that was tested and proven true in this article was that the rate of Cdc2 activation slows down near the activation threshold concentration of cyclin. These predictions and experiments demonstrate the toggle-like switching behavior that can be described by hysteresis in a dynamical system.\n\nIn the transition from metaphase to anaphase, it is crucial that sister chromatids are properly and simultaneously separated to opposite ends of the cell. Separation of sister-chromatids is initially strongly inhibited to prevent premature separation in late mitosis, but this inhibition is relieved through destruction of the inhibitory elements by the anaphase-promoting complex (APC) once sister-chromatid bi-orientation is achieved. One of these inhibitory elements is securin, which prevents the destruction of cohesin, the complex that holds the sister-chromatids together, by binding the protease separase which targets Scc1, a subunit of the cohesin complex, for destruction. In this system, the phosphatase Cdc14 can remove an inhibitory phosphate from securin, thereby facilitating the destruction of securin by the APC, releasing separase. As shown by Uhlmann et al., during the attachment of chromosomes to the mitotic spindle the chromatids remain paired because cohesion between the sisters prevents separation. Cohesion is established during DNA replication and depends on cohesin, which is a multisubunit complex composed of Scc1, Scc3, Smc2, and Smc3. In yeast at the metaphase-to-anaphase transition, Scc1 dissociates from the chromosomes and the sister chromatids separate. This action is controlled by the Esp1 protein, which is tightly bound by the anaphase inhibitor Pds1 that is destroyed by the anaphase-promoting complex. In order to verify that Esp1 does play a role in regulating Scc1 chromosome association, cell strains were arrested in G1 with an alpha factor. These cells stayed in arrest during the development. Esp1-1 mutant cells were used and the experiment was repeated, and Scc1 successfully bound to the chromosomes and remained associated even after the synthesis was terminated. This was crucial in showing that with Esp1, Scc1 is hindered in its ability to become stably associated with chromosomes during G1, and Esp1 can in fact directly remove Scc1 from chromosomes.\n\nIt has been shown by Holt et al. that separase activates Cdc14, which in turn acts on securin, thus creating a positive feedback loop that increases the sharpness of the metaphase to anaphase transition and coordination of sister-chromatid separation. Holt et al. probed the basis for the effect of positive feedback in securin phosphorylation by using mutant 'securin' strains of yeast, and testing how changes in the phosphoregulation of securin affects the synchrony of sister chromatid separation. Their results indicate that interfering with this positive securin-separase-cdc14 loop decreases sister chromatid separation synchrony. This positive feedback can hypothetically generate bistability in the transition to anaphase, causing the cell to make the irreversible decision to separate sister-chromatids.\n\nMitotic exit is an important transition point that signifies the end of mitosis and the onset of new G1 phase for a cell, and the cell needs to rely on specific control mechanisms to ensure that once it exits mitosis, it never returns to mitosis until it has gone through G1, S, and G2 phases and passed all the necessary checkpoints. Many factors including cyclins, cyclin-dependent kinases (CDKs), ubiquitin ligases, inhibitors of cyclin-dependent kinases, and reversible phosphorylations regulate mitotic exit to ensure that cell cycle events occur in correct order with least amount of errors. The end of mitosis is characterized by spindle breakdown, shortened kinetochore microtubules, and pronounced outgrowth of astral (non-kinetochore) microtubules. For a normal eukaryotic cell, mitotic exit is irreversible.\n\nMany speculations were made with regard to the control mechanisms employed by a cell to promote the irreversibility of mitotic exit in a eukaryotic model organism, the budding yeast \"Saccharomyces cerevisiae\". Proteolytic degradation of cell cycle regulators and corresponding effects on the levels of cyclin-dependent kinases were proposed as a mechanism that promotes eukaryotic cell cycle and metaphase-to-anaphase transition in particular.\nIn this theory, anaphase promoting complex (APC), a class of ubiquitin ligase, facilitates degradation of mitotic cyclins (Clb2) and anaphase-inhibiting factors (PDS1, CUT2) to promote mitotic exit. APC ubiquitinates nine-amino acid motif known as the destruction box (D box) in the NH2-terminal domain of mitotic cyclins for degradation by proteasome. APC in association with Cdc20 (APC-Cdc20) ubiquitinates and targets mitotic cyclins (Clb2) for degradation at initial phase. Simultaneously, APC-Cdc20 mediates degradation of securins, which inhibit separases through binding, at anaphase onset. Released and active separase cleaves cohesin that held sister chromatids together, facilitating separation of sister chromatids and initiates mitotic exit by promoting release of Cdc14 from nucleolus. At later phase, downregulation of Cdk1 and activation of Cdc14, a Cdh1-activating phosphatase, promotes formation of APC in association with Cdh1 (APC-Cdh1) to degrade Clb2s. Cdc20 and Cdh1, which are the activators of APC, recruit substrates such as securin and B-type cyclins(Clb) for ubiquitination. Without Cdk1-Clb2 complexes to phosphorylate proteins that are involved in spindle dynamics such as Sli15, Ase1, and Ask1, spindle elongation and chromosomal segregation are promoted, facilitating mitotic exit.\nThe importance of proteolytic degradation in eukaryotic cell cycle changed the view of cell division as a simple kinase cascade to a more complex process in which interactions among phosphorylation, ubiquitination, and proteolysis are necessary. However, experiments using budding yeast cells with cdc28-as1, a INM-PP1 (ATP analog)-sensitive Cdk allele, proved that destruction of B-type cyclins (Clb) is not necessary for triggering irreversible mitotic exit. Clb2 degradation did shorten the Cdk1-inhibition period required for triggering irreversible mitotic exit indicating that cyclin proteolysis contributes to the dynamic nature of the eukaryotic cell cycle due to slower timescale of its action but is unlikely to be the major determining factor in triggering irreversible cell cycle transitions.\n\nDiscoveries were made which indicated the importance of the level of the inhibitors of cyclin-dependent kinases in regulating eukaryotic cell cycle. In particular, the level of Sic1, a stoichiometric inhibitor of Clb-CDK complexes in budding yeast, was shown to be particularly important in irreversible G1-S transition by irreversibly activating S phase kinases. Sic1 level was shown to play a major role in triggering irreversible mitotic exit (M-G1 transition) as well as in G1-S transition. During mitosis, decreasing levels of Cdk1 leads to the activation of Cdc14, a phosphatase that counteracts Cdk1 via activation of Cdh1 and Swi5, a transcriptional activator of Sic1 proteins. While degradation of Sic1 to a certain low level triggered the onset of S phase, accumulation of Sic1 to a certain high level was required to trigger irreversible mitotic exit. Cdk1-inhibitors could induce mitotic exit even when degradation of B-type cyclins was blocked by expression of non-degradable Clbs or proteasome inhibitors. However, sister chromatids failed to segregate, and cells reverted to mitosis once the inhibitors were washed away, indicating that a threshold level of the inhibitors needs to be achieved to trigger irreversible mitotic exit independently of cyclin degradations. Despite different thresholds of Sic1 level that are required to trigger mitotic exit compared to G1-S transition, the level of Sic1 was shown to play a key role in regulating eukaryotic cell cycle by inhibiting the activity of CDKs.\n\nBecause eukaryotic cell cycle involves a variety of proteins and regulatory interactions, dynamical systems approach can be taken to simplify a complex biological circuit into a general framework for better analysis. Among the four possible input/output relationships, the relationship between Sic1 level and mitotic exit seems to show the characteristics of an irreversible bistable switch, driven by feedback between APC-Cdh1, Sic1, and Clb2-Cdk1. Bistability is known to control biological functions such as cell cycle control and cellular differentiation and play a key role in many cellular regulatory networks. Bistable input/output relationship is characterized by two stable states with two bifurcation points. Multiple outputs are possible for one specific input in the region of bistability, marked by two bifurcation points. In addition, the bistable relationship displays hysteresis: the final state/output depends on the history of the input as well as the current value of input because the system has a memory. One bifurcation point has a negative control parameter value (the bifurcation point is on the other side of the axis), resulting in disconnection between the two stable states and irreversibility of the transition from one state to the other. With regard to mitotic exit, the two stable states are defined by mitosis and G1 phase. Once Sic1 level (input) accumulates beyond the threshold, irreversible transition occurs from mitosis (stable state I) to G1 phase (stable state II).\nIn the imperfect environment, the only bifurcation that remains intact is saddle-node bifurcation. Saddle-node bifurcation does not break down (saddle-node is the expected generic behavior), while transcritical and pitchfork bifurcations break down in the presence of imperfections. Thus, the only one-dimensional bifurcation that can exist in imperfect biological world is the saddle-node bifurcation. The bistable relation between M-G1 transition and Sic1 level can be represented as a diagram of two saddle-node bifurcations in which the system’s behavior changes qualitatively with a small change in control parameter, the amount of Sic1.\n\n Because the behavior of cell cycle critically depends on the amount of Sic1 at the M-G1 transition state, the amount of Sic1 is tightly regulated by systems-level feedbacks. Because Cdk1-Clb2 inhibits Sic1 by phosphorylating Sic1 and making Sic1 available for degradation via ubiquitylation, APC-Cdh1-dependent degradation of Cdk1-Clb2 not only decreases the level of available Cdk1-Clb2 complexes but also increases the level of Sic1 which in turn further inhibits the function of Cdk1-Clb2. This activation of the double negative feedback loop is initiated from APC-Cdc20-dependent degradation of Cdk1-Clb2 and release of Cdc14 from nucleolar protein Net1/Cfi1. FEAR (Cdc14 early anaphase release) pathway facilitates Clb2-Cdk1-dependent phosphorylation of Net1 which transiently releases Cdc14 from Net1. The released Cdc14 and Clb2-Cdk1 complexes go onto form spindles that activates mitotic exit network (MEN). MEN allows sustained release of Cdc14 from the nucleolus, and Cdc14 counters the activity of Clb2-Cdk1 by activating Cdh1 and stabilizing Sic1 through activation of Sic1-transcriptional activator Swi5. Sic1 positively regulates itself by inhibiting Cdk1-Clb2 to release inhibition of Swi5, and Cdh1 also positively regulates itself by inhibiting Clb2-Cdk1 to release inhibition of MEN which can activate Cdc14 and subsequently Cdh1 itself. The double-negative feedback loop, formed by APC-Cdh1 and Sic1, is required to maintain low Clb2-Cdk1 activity because Clb2 auto-activates its synthesis by activating transcriptional factors, Fkh2–Mcm1 Ndd1 complex.\n\nEukaryotic cell cycle consists of various checkpoints and feedback loops to ensure faithful and successful cell division. During mitosis for example, when duplicated chromosomes are improperly attached to mitotic spindle, spindle assembly checkpoint (SAC) proteins including Mad and Bub inhibit APC-Cdc20 to delay entry into anaphase and B-type cyclin degradations. In addition, when mitotic spindles are misaligned, MEN and subsequently Cdc14 are inhibited in a Bub2 and Bfa1-dependent manner to prevent degradation of mitotic cyclins and anaphase entry.\nSic1 is a nice example demonstrating how systems-level feedbacks interact to sense the environmental conditions and trigger cell cycle transitions. Even though actual M-G1 transition is vastly complex with numerous proteins and regulations involved, dynamical systems approach allows simplification of this complex system into bistable input/output relation with two saddle-node bifurcations in which the output (mitotic exit) depends on critical concentration of Sic1. Using one-dimensional analysis, it might be possible to explain many of the irreversible transition points in the eukaryotic cell cycle that are governed by systems-level control and feedback. Other examples of irreversible transition points include Start (irreversible commitment to a new cell division cycle) that can be explained by irreversible bistable switch whose control parameter is tightly regulated by the systemic feedbacks involving Cln2, Whi5, and SBF.\n\n\n", "id": "25368725", "title": "Biochemical switches in the cell cycle"}
{"url": "https://en.wikipedia.org/wiki?curid=6332274", "text": "Cap formation\n\nWhen molecules on the surface of cell are crosslinked, they are moved to one end of the cell to form a \"cap\". This phenomenon, the process of which is called cap formation, was discovered in 1971 on lymphocytes and is a property of amoebae and all locomotory animal cells except sperm. The crosslinking is most easily achieved using a polyvalent antibody to a surface antigen on the cell. Cap formation can be visualised by attaching a fluorophore, such as fluorescein, to the antibody.\n\n\nCapping only occurs on motile cells and is therefore believed to reflect an intrinsic property of how cells move. It is an energy dependent process and in lymphocytes is partially inhibited by cytochalasin B (which disrupts microfilaments) but unaffected by colchicine (which disrupts microtubules). However, a combination of these drugs eliminates capping. A key feature of capping is that only those molecules that are crosslinked cap: Others do not.\n\nCap formation is now seen as closely related to the carbon particle experiments of Abercrombie. In this case, crawling fibroblasts were held in a medium containing small (~1 micrometre in size) carbon particles. On occasion, these particles attached to the front leading edge of these cells: When they did so, the particles were observed to move rearward on the cell’s dorsal surface. They did so in a roughly straight line, with the particle remaining initially stationary with respect to the substratum. The cell seemed to ooze forward under the particle. In view of what we know of capping, this phenomenon is now interpreted as follows: The particle is presumably stuck to many surface molecules, crosslinking them and forming a patch. As in capping, the particle moves toward the back of the cell.\n\nAbercrombie thought that the carbon particle is a marker on the cell surface and its behaviour reflects what the surface is doing. This led him to propose that, as a cell moves, membrane from internal stores is added at the front of the cell—enabling the cell to extend forward—and retrieved toward the rear of the cell. This process of exocytosis at the front of the cell and endocytosis elsewhere has been modified by Bretscher. He and Hopkins showed that the specific membrane endocytosed by coated pits on motile cells is returned by exocytosis to the cell surface at the leading edge. The spatial difference between the sites of exocytosis (at the front) and endocytosis (everywhere on the surface) leads to a flow of the matrix of the plasma membrane—lipids—from the front toward the rear. Large objects, such as patches, would be swept along with this flow, whereas non-crosslinked small molecules would be able to diffuse by Brownian motion against the flow and so evade being swept backward. Hence, in this theory, the need for crosslinking. Bretscher proposed that on stationary cells exocytosis is random—and therefore a major difference between motile and nonmotile cells.\n\nAn alternative view is that the patches are moved to the rear of the cell by direct attachment to the actin cytoskeleton. The molecular mechanism for how this could be achieved is unclear, since, when glycolipids or GPI-bound proteins (in the outer monolayer of the cell's surface bilayer) are crosslinked, they cap, just like any surface protein. As these molecules cannot themselves interact directly with the cytoplasmic actin cytoskeleton, this scheme seems unlikely.\n\nA third scheme, by de Petris, suggests that a motile cell is continuously raking its surface from front to back: Any aggregates (but not uncrosslinked molecles) caught in the teeth of the rake are moved to the back of the cell. In this scheme, the nature of the tines of the rake are not specified but could, for example, be surface integrins that often act as the feet of the cell to attach it to the substrate. The force required to rake the surface could be provided by the actin cytoskeleton.\n\nA fourth scheme, by Hewitt, suggests that motile cells have rearward waves on their surfaces: patches, but not single molecules, become entrained in these waves and are thus moved to the back of the cell.\n", "id": "6332274", "title": "Cap formation"}
{"url": "https://en.wikipedia.org/wiki?curid=11368321", "text": "CDK7 pathway\n\nCDK7 is a cyclin-dependent kinase shown to be not easily classified. CDK7 is both a CDK-activating kinase (CAK) and a component of the general transcription factor TFIIH.\n\nAn intricate network of cyclin-dependent kinases (CDKs) are organized in a pathway to ensure that each cell accurately replicates its DNA and segregate it equally between the two daughter cells. One CDK–the CDK7 complex–cannot be so easily classified. CDK7 is both a CDK-activating kinase (CAK), which phosphorylates cell-cycle CDKs within the activation segment (T-loop), and a component of the general transcription factor TFIIH, which phosphorylates the C-terminal domain (CTD) of the largest subunit of Pol II. A proposed mode of CDK7 inhibition is the phosphorylation of cyclin H by CDK7 itself or by another kinase.\n\nIn order to be active, most CDKs require not only a cyclin partner but also phosphorylation at one particular site, which corresponds to Thr161 in human CDK1, and which is located within the so-called T-loop of kinase subdomain VIII. CDKl, CDK2 and CDK4 all require T-loop phosphorylation for maximum activity.\n\nAn entirely new perspective on CDK7 function was opened when CDK7 was identified as a subunit of transcription factor IIH (TFIIH) and shown to phosphorylate the carboxy-terminal domain (CTD) of RNA polymerase II (RNAPII). TFIIH is a multiprotein complex required not only for class II transcription but also for nucleotide-excision repair. Its associated CTD-kinase activity is considered important for the promoter-clearance step of transcription, but the precise structural consequences of the phosphorylation of the CTD remain the subject of debate. Cyclin H and MAT1 are also present in TFIIH, and it is not known what, if anything, distinguishes the TFIIH-associated form of CDK7 from the quantitatively predominant free form. Whether CDK7 really displays dual-substrate specificity remains to be further explored, but there is no question that the CDK7-cyclin H-MAT1 complex is able to phosphorylate both the T-loop of CDKs and the YSPTSPS (single-letter code for amino acids) repeats of the RNAPII CTD \"in vitro\".\n\nIt has been demonstrated that TFIIH is a rate-limiting factor for HIV transcription in unactivated T-cells by using a combination of in vivo ChIP experiments and cell-free transcription studies. The ability of NF-κB to rapidly recruit TFIIH during HIV activation in T-cells is an unexpected discovery; however, there are several precedents in the literature of cellular genes that are activated through the recruitment of TFIIH. In an early and influential paper, demonstrated that type I activators such as Sp1 and CTF, which were able to support initiation but were unable to support efficient elongation, were also unable to bind TFIIH. By contrast, type II activators such as VP16, p53 and E2F1, which supported both initiation and elongation, were able to bind to TFIIH. In one of the most thoroughly characterized transcription systems, have studied the temporal order of recruitment of transcription factors during the activation of the major histocompatibility class II (MHC II) DRA gene by IFN-gamma. Following induction of the CIITA transcription factor by IFN-gamma, there was recruitment of both CDK7 and CDK9 causing RNAP CTD phosphorylation and elongation. Finally, Nissen and Yamamoto (2000) in their studies of the activation of the IL-8 and ICAM-1 promoters observed enhanced CDK7 recruitment and RNAP II CTD phosphorylation in response to NF-κB activation by TNF.\n\n", "id": "11368321", "title": "CDK7 pathway"}
{"url": "https://en.wikipedia.org/wiki?curid=2428938", "text": "Cell migration\n\nCell migration is a central process in the development and maintenance of multicellular organisms. Tissue formation during embryonic development, wound healing and immune responses all require the orchestrated movement of cells in particular directions to specific locations. Cells often migrate in response to specific external signals, including chemical signals and mechanical signals. Errors during this process have serious consequences, including intellectual disability, vascular disease, tumor formation and metastasis. An understanding of the mechanism by which cells migrate may lead to the development of novel therapeutic strategies for controlling, for example, invasive tumour cells.\n\nDue to the highly viscous environment (low Reynolds number), cells need to permanently produce forces in order to move. Cells achieve active movement by very different mechanisms. Many less complex prokaryotic organisms (and sperm cells) use flagella or cilia to propel themselves. Eukaryotic cell migration typically is far more complex and can consist of combinations of different migration mechanisms. It generally involves drastic changes in cell shape which are driven by the cytoskeleton. Two very distinct migration scenarios are crawling motion (most commonly studied) and blebbing motility. A paradigmatic example of crawling motion is the case of fish epidermal keratocytes (videomicroscopy of crawling cultured epidermal fish keratocytes), which have been extensively used in research and teaching.\n\nThe migration of cultured cells attached to a surface is commonly studied using microscopy.\nAs cell movement is very slow, a few µm/minute, time-lapse microscopy videos are recorded of the migrating cells to\nspeed up the movement. \nSuch videos (Figure 1) reveal that the leading cell front is very active, with a characteristic behavior of successive contractions and expansions.\nIt is generally accepted that the leading front is the main motor that pulls the cell forward.\n\nThe processes underlying mammalian cell migration are believed to be consistent with those of (non-spermatozooic) locomotion. Observations in common include:\nThe latter feature is most easily observed when aggregates of a surface molecule are cross-linked with a fluorescent antibody or when small beads become artificially bound to the front of the cell.\n\nOther eukaryotic cells are observed to migrate similarly. The amoeba Dictyostelium discoideum is useful to researchers because they consistently exhibit chemotaxis in response to cyclic AMP; they move more quickly than cultured mammalian cells; and they have a haploid genome that simplifies the process of connecting a particular gene product with its effect on cellular behaviour.\n\nThere are two main theories for how the cell advances its front edge: the cytoskeletal model and membrane flow model. It is possible that both underlying processes contribute to cell extension.\n\nExperimentation has shown that there is rapid actin polymerisation at the cell's front edge. This observation has led to the hypothesis that formation of actin filaments \"push\" the leading edge forward and is the main motile force for advancing the cell’s front edge. In addition, cytoskeletal elements are able to interact extensively and intimately with a cell's plasma membrane.\n\nOther cytoskeletal components (like microtubules) have important functions in cell migration. It has been found that microtubules act as “struts” that counteract the contractile forces that are needed for trailing edge retraction during cell movement. When microtubules in the trailing edge of cell are dynamic, they are able to remodel to allow retraction. When dynamics are suppressed, microtubules cannot remodel and, therefore, oppose the contractile forces. The morphology of cells with suppressed microtubule dynamics indicate that cells can extend the front edge (polarized in the direction of movement), but have difficulty retracting their trailing edge. On the other hand high drug concentrations, or microtubule mutations that depolymerize the microtubules, can restore cell migration but there is a loss of directionality. It can be concluded that microtubules act both to restrain cell movement and to establish directionality.\n\nStudies have also shown that the front is the site at which membrane is returned to the cell surface from internal membrane pools at the end of the endocytic cycle. This has led to the hypothesis that extension of the leading edge occurs primarily by addition of membrane at the front of the cell. If so, the actin filaments that form at the front might stabilize the added membrane so that a structured extension, or lamella, is formed rather than a bubble-like structure (or bleb) at its front. For a cell to move, it is necessary to bring a fresh supply of \"feet\" (proteins called integrins, which attach a cell to the surface on which it is crawling) to the front. It is likely that these feet are endocytosed toward the rear of the cell and brought to the cell's front by exocytosis, to be reused to form new attachments to the substrate.\n\nBased on some mathematical models, recent studies hypothesize a novel biological model for collective biomechanical and molecular mechanism of cell motion. It is proposed that microdomains weave the texture of cytoskeleton and their interactions mark the location for formation of new adhesion sites. According to this model, microdomain signaling dynamics organizes cytoskeleton and its interaction with substratum. As microdomains trigger and maintain active polymerization of actin filaments, their propagation and zigzagging motion on the membrane generate a highly interlinked network of curved or linear filaments oriented at a wide spectrum of angles to the cell boundary. It is also proposed that microdomain interaction marks the formation of new focal adhesion sites at the cell periphery. Myosin interaction with the actin network then generate membrane retraction/ruffling, retrograde flow, and contractile forces for forward motion. Finally, continuous application of stress on the old focal adhesion sites could result in the calcium-induced calpain activation, and consequently the detachment of focal adhesions which completes the cycle.\n\nMigrating cells have a polarity—a front and a back. Without it, they would move in all directions at once, i.e. spread. How this polarity is formulated at a molecular level inside a cell is unknown. In a cell that is meandering in a random way, the front can easily give way to become passive as some other region, or regions, of the cell form(s) a new front. In chemotaxing cells, the stability of the front appears enhanced as the cell advances toward a higher concentration of the stimulating chemical. This polarity is reflected at a molecular level by a restriction of certain molecules to particular regions of the inner cell surface. Thus, the phospholipid PIP3 and activated Rac and CDC42 are found at the front of the cell, whereas Rho GTPase and PTEN are found toward the rear.\n\nIt is believed that filamentous actins and microtubules are important for establishing and maintaining a cell’s polarity. Drugs that destroy actin filaments have multiple and complex effects, reflecting the wide role that these filaments play in many cell processes. It may be that, as part of the locomotory process, membrane vesicles are transported along these filaments to the cell’s front. In chemotaxing cells, the increased persistence of migration toward the target may result from an increased stability of the arrangement of the filamentous structures inside the cell and determine its polarity. In turn, these filamentous structures may be arranged inside the cell according to how molecules like PIP3 and PTEN are arranged on the inner cell membrane. And where these are located appears in turn to be determined by the chemoattractant signals as these impinge on specific receptors on the cell’s outer surface.\n\nAlthough microtubules have been known to influence cell migration for many years, the mechanism by which they do so has remained controversial. On a planar surface, microtubules are not needed for the movement, but they are required to provide directionality to cell movement and efficient protrusion of the leading edge. When present, microtubules retard cell movement when their dynamics are suppressed by drug treatment or by tubulin mutations.\n\n\nIn a series of recent works, a new area of research called inverse problems in cell motility has been established. \nThis approach is based on the idea that behavioral or shape changes of a cell bear information about the underlying mechanisms that generate these changes. Reading cell motion, namely, understanding the underlying biophysical and mechanochemical processes, is of paramount importance.\nThe mathematical models developed in these works determine some physical features and material properties of the cells locally through analysis of live cell image sequences and uses this information to make further inferences about the molecular structures, dynamics, and processes within the cells, such as the actin network, microdomains, chemotaxis, adhesion, and retrograde flow.\n\n\n", "id": "2428938", "title": "Cell migration"}
